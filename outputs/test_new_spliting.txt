Focused review:

1.	The MIL is widely used in weakly supervised object detection[1] and point-based object detection[2].
2.	This paper used SAM to generate the mask proposals. However, the training of SAM used many complete mask annotations. Therefore, the proposed method still rely on the complete mask supervision, which is not suitable to be considered as a point-supervised method. It is also not consistent with the tile “A SINGLE POINT IS ALL YOU NEED.”
3.	The proposed SAE only significantly outperforms minimum in PL category. In some other cases, it is even lower than minimum.
4. Can the authors compare the proposed method with point supervised instance segmention[3], in the same supervision setting? Minor:
1.	“In many cases, the annotated point of an object is typically positioned in close proximity to the center of the mask (Chen et al., 2022). ” “close proximity” is a redundancy. Proximity means closeness, nearness; therefore “close proximity” means “close closeness” or “near nearness.”
[1] Bilen, Hakan, and Andrea Vedaldi. "Weakly supervised deep detection networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[2] Papadopoulos, Dim P., et al. "Training object class detectors with click supervision." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.
[3] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov. Pointly-supervised instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2617–2626, 2022

Review Point: 1. The MIL is widely used in weakly supervised object detection[1] and point-based object detection[2].
Review Point: 2. This paper used SAM to generate the mask proposals. However, the training of SAM used many complete mask annotations. Therefore, the proposed method still rely on the complete mask supervision, which is not suitable to be considered as a point-supervised method. It is also not consistent with the tile “A SINGLE POINT IS ALL YOU NEED.” 3. The proposed SAE only significantly outperforms minimum in PL category. In some other cases, it is even lower than minimum.
Review Point: 4. Can the authors compare the proposed method with point supervised instance segmention[3], in the same supervision setting? Minor:
==================================================

Focused review:

1. Usually in these kind of settings, there is an assumption that sets a minimum distance between any two consecutive change points in the environment. This type of assumption exists to make sure that the change-point-detector-based algorithm have (statistically) enough time to detect any possible change in the environment and hence the algorithm is able to detect any change to the environment. Naturally, this implies that, as we get more distance from a change point in the environment, the probability of facing another change in the environment is more than the time when we were closer to the last change point. Hence we need to collect more samples from various arms to see if there has been a change that makes our current decisions sub-optimal. This argument (which i believe is correct) has contradiction with the nature of your method in decreasing forced exploration as we get distance from the last change point.
2. The experiments section needs to be clarified more by providing more clear information about the setups.
3. The number of arms involved in the synthetic data experiment is low and it raises the question about the performance of the proposed technique in scenarios with more arms.
4. The writing of the text is not smooth to follow. It can definitely be improved.

Review Point: 2. The experiments section needs to be clarified more by providing more clear information about the setups.
Review Point: 3. The number of arms involved in the synthetic data experiment is low and it raises the question about the performance of the proposed technique in scenarios with more arms.
Review Point: 4. The writing of the text is not smooth to follow. It can definitely be improved.
==================================================

Focused review:

1.	The motivation is not clear. In the introduction section, it is only mentioned that the purpose of this paper is to examine whether generative models can derive rare generative factors, but a clear motivation is lacking. For example, what is the purpose of deriving rare generative factors? What negative impacts might arise if rare generative factors are not effectively learned?
2.	Generative models typically learn latent representations automatically, however, this paper manually defines Generative Factors, and all experiments are based on these predefined factors. This approach may create a discrepancy with the stated objective in line 92: “Our work provides valuable insights into the limitations of current generative models in learning robust, transferable representations from imbalanced datasets, opening new avenues for improving their generalization capabilities.”
3.	The experiments are not consistent with the examples. In these examples, the rare factor has a strong relationship with the label. However, in the dataset D_u, the numbers of samples with two rare factors are the same. From my point of view, it is more like an imbalanced classification problem, while the authors experiments did not take it into account.
4.	p>0.05 only means that one cannot reject the null hypothesis, and is not strong evidence that the model has effectively learned the model.
5.	The paper lacks a detailed explanation of how Spectral Decoupling mitigates this memorization tendency
6.	Some parts of the paper are verbose and convoluted, making it difficult to understand.

Review Point: 1. The motivation is not clear. In the introduction section, it is only mentioned that the purpose of this paper is to examine whether generative models can derive rare generative factors, but a clear motivation is lacking. For example, what is the purpose of deriving rare generative factors? What negative impacts might arise if rare generative factors are not effectively learned?
Review Point: 4. p>0.05 only means that one cannot reject the null hypothesis, and is not strong evidence that the model has effectively learned the model.
Review Point: 5. The paper lacks a detailed explanation of how Spectral Decoupling mitigates this memorization tendency 6. Some parts of the paper are verbose and convoluted, making it difficult to understand.
==================================================

Focused review:

1. This paper directly extends distillation strategies on decoding-based watermarking and then poses the limitation of weight-based watermarking. However, no solution is provided. It seems like an experimental report, and it would be better to introduce the specific solution.
2. The motivation of this paper is there is a limitation of decoding-based watermarking, namely, replacing it with a normal decoder. Is the assumption practical? In practice, for an LLM API, how do we conduct such an operation?
3. For wright-based LLM watermarking, more attacks (besides fine-tuning) shall be considered, such as pruning and further distillation.
4. The paper exceeds 9 pages.

Review Point: 1. This paper directly extends distillation strategies on decoding-based watermarking and then poses the limitation of weight-based watermarking. However, no solution is provided. It seems like an experimental report, and it would be better to introduce the specific solution.
Review Point: 2. The motivation of this paper is there is a limitation of decoding-based watermarking, namely, replacing it with a normal decoder. Is the assumption practical? In practice, for an LLM API, how do we conduct such an operation?
Review Point: 3. For wright-based LLM watermarking, more attacks (besides fine-tuning) shall be considered, such as pruning and further distillation.
==================================================

Focused review:

1. (Major) The implemented algorithm in the main experiments consists of three improvements compared to the naive Uncertainty Herding: the temperature scaling, the radius adaptation, and the entropy regularization. From my point of view, this paper's main contribution is giving a natural (and probably SOTA) interpolation between the low-budget and the high-budget AL regimes. In that sense, the first two terms seem very natural, yet the third term is unnatural and not analyzed in the paper. On the one hand, the third term is important (and sometimes even critical) since the third term does boost the performance of the proposed algorithm. For example, in Appendix C, without entropy-regularization, the performance of Uncertainty Herding does not exceed uncertainty sampling in the high-budget regime. Intuitively, without that regularization, it would be weird that the algorithm (as an interpolation of the MaxHerding and the margin-based uncertainty sampling) outperforms both the MaxHerding and the margin-based uncertainty sampling in EVERY regime. However, the ablation study only considers adding entropy-regularization to the random sampling (passive learning). The authors make no attempts to explain or identify the effects of the entropy-regularization, which makes the logic incomplete.
2. (Moderate) The theory part does not contain much contribution and novelty.
3. (Moderate) There seem to be no descriptions of how the temperature scaling changes the uncertainty function. For example, the $f_{\tau}$ function in Definition 5 is used without definition.
4. (Minor) Figures in the main text are too small.

Review Point: 2. (Moderate) The theory part does not contain much contribution and novelty.
Review Point: 3. (Moderate) There seem to be no descriptions of how the temperature scaling changes the uncertainty function. For example, the $f_{\tau}$ function in Definition 5 is used without definition.
Review Point: 4. (Minor) Figures in the main text are too small.
==================================================

Focused review:

1. The contribution of the paper is focused on the **empirical existence** of gradient subspaces. Although the authors, in Section 5, discuss how the existence of this sub-space could be leveraged in practice, it remains an open question to provide empirical evidence of these eventual benefits.
2. **Limited experimental campaign**. It has to be noticed that the contributions of this paper are only empirical. As a consequence, I would have expected more experiments to prove the empirical existence of these sub-spaces. Results are limited to 6 domains (i.e., Finger-spin, Ball_in_cup, Ant, HalfCheetah, Pendulum, Walker2D). I invite the authors to expand the set of domains considered.
3. **Clarity and writing (minor)**. I would encourage the authors to introduce and follow a more rigorous and formal definition of the elements that are used in the paper (e.g., subspace and so on). All the discussion is, indeed, rather informal.

Review Point: 1. The contribution of the paper is focused on the **empirical existence** of gradient subspaces. Although the authors, in Section 5, discuss how the existence of this sub-space could be leveraged in practice, it remains an open question to provide empirical evidence of these eventual benefits.
Review Point: 2. **Limited experimental campaign**. It has to be noticed that the contributions of this paper are only empirical. As a consequence, I would have expected more experiments to prove the empirical existence of these sub-spaces. Results are limited to 6 domains (i.e., Finger-spin, Ball_in_cup, Ant, HalfCheetah, Pendulum, Walker2D). I invite the authors to expand the set of domains considered.
Review Point: 3. **Clarity and writing (minor)**. I would encourage the authors to introduce and follow a more rigorous and formal definition of the elements that are used in the paper (e.g., subspace and so on). All the discussion is, indeed, rather informal.
==================================================

Focused review:

- There are several assumptions that limit the applicability of the framework, though they are properly discussed. Proposition 2.3 is the key technique that governs the whole technical assumptions including the assumption on the link function (Assumption 2.4). It is unclear for me as a newbie reader to this field how much it is restrictive and whether this can be relaxed. Any further discussion would be appreciated.
- Though the paper starts with the single-index learning, it seems that the framework deviates from the framework by considering a different link function in the final form of the modified correlation loss in eq. (14). I found it quite confusing at first sight. After this modification, is this technically qualified as a single-index learning?

Review Point: - There are several assumptions that limit the applicability of the framework, though they are properly discussed. Proposition 2.3 is the key technique that governs the whole technical assumptions including the assumption on the link function (Assumption 2.4). It is unclear for me as a newbie reader to this field how much it is restrictive and whether this can be relaxed. Any further discussion would be appreciated.
Review Point: - Though the paper starts with the single-index learning, it seems that the framework deviates from the framework by considering a different link function in the final form of the modified correlation loss in eq. (14). I found it quite confusing at first sight. After this modification, is this technically qualified as a single-index learning?
==================================================

Focused review:

1)	The evaluation of the paper is limited. Initially, all experiments are only tested on 22 prompts from the original DreamFusion gallery, while previous works typically use over 40. The prompt list is also not provided in the appendix, which compromises the reproducibility of the experiments. Furthermore, the absence of recent open-sourced baseline methods that focus on addressing the Janus problem, such as ESD1 and JointDreamer2, is notable.
2)	The authors introduce the concept of the "joint distribution expression of the data and camera pose," but provide limited explanation of the practical meaning of such a distribution. Also, the use of a rather discrete 4-views to represent the camera pose distribution does not appear to be meaningful. More ablation studies should be conducted.
3)	The authors state that the image templates are "user-provided" in line 317 of the main manuscript, which seems unusual. It is not feasible for users to provide consistent multi-view templates for any text prompts.

Review Point: 1) The evaluation of the paper is limited. Initially, all experiments are only tested on 22 prompts from the original DreamFusion gallery, while previous works typically use over 40. The prompt list is also not provided in the appendix, which compromises the reproducibility of the experiments. Furthermore, the absence of recent open-sourced baseline methods that focus on addressing the Janus problem, such as ESD1 and JointDreamer2, is notable.
Review Point: 2) The authors introduce the concept of the "joint distribution expression of the data and camera pose," but provide limited explanation of the practical meaning of such a distribution. Also, the use of a rather discrete 4-views to represent the camera pose distribution does not appear to be meaningful. More ablation studies should be conducted.
Review Point: 3) The authors state that the image templates are "user-provided" in line 317 of the main manuscript, which seems unusual. It is not feasible for users to provide consistent multi-view templates for any text prompts.
==================================================

Focused review:

- There are potential concerns on the technical methodology front. While this study is the product of extensive training and evaluation, much of the methodology draws from pre-existing studies. Although there are several SSL studies tailored for time-series data, particularly in the realm of biosignals in healthcare, the authors did not extensively compare different model architectures. Readers might be keen to discern whether biosignal SSL performance is more contingent upon scale or the model architecture itself.
- While this study encompasses two modalities, it seems that the authors have considered them in isolation. Pretrained models have shown effectiveness across varied modalities like images and language. It would be beneficial for the authors to delve deeper into this aspect.
- The pretrained embedding for PPG appears to encapsulate more information than its ECG counterpart. This raises a question: given that PPG is passively sampled and ECG is actively collected by users, could this disparity in data collection methods influence such an outcome? Additionally, conventional clinical diagnoses often rely on 12-lead ECG or periodic information like HRV derived from ECG. It would be valuable if the authors could elucidate more why the ECG embedding doesn't seem as informative as the PPG.
- Lastly, the authors note that positive pairs are drawn from the same individual. However, a person's PPG and ECG patterns can vary based on different conditions or circumstances. It might be more insightful for the authors to determine positive pairs by taking additional attributes into account.

Review Point: - There are potential concerns on the technical methodology front. While this study is the product of extensive training and evaluation, much of the methodology draws from pre-existing studies. Although there are several SSL studies tailored for time-series data, particularly in the realm of biosignals in healthcare, the authors did not extensively compare different model architectures. Readers might be keen to discern whether biosignal SSL performance is more contingent upon scale or the model architecture itself.
Review Point: - While this study encompasses two modalities, it seems that the authors have considered them in isolation. Pretrained models have shown effectiveness across varied modalities like images and language. It would be beneficial for the authors to delve deeper into this aspect.
Review Point: - The pretrained embedding for PPG appears to encapsulate more information than its ECG counterpart. This raises a question: given that PPG is passively sampled and ECG is actively collected by users, could this disparity in data collection methods influence such an outcome? Additionally, conventional clinical diagnoses often rely on 12-lead ECG or periodic information like HRV derived from ECG. It would be valuable if the authors could elucidate more why the ECG embedding doesn't seem as informative as the PPG.
Review Point: - Lastly, the authors note that positive pairs are drawn from the same individual. However, a person's PPG and ECG patterns can vary based on different conditions or circumstances. It might be more insightful for the authors to determine positive pairs by taking additional attributes into account.
==================================================

Focused review:

1. Please add a table grid to Figure 2 for clearer comparisons.
2. The author notes the absence of 3D interactive environments as a limitation. However, this is a significant point to address since many high-performing models should ideally transfer seamlessly to robotics. A 3D interactive environment would greatly facilitate this transition. Notably, papers like ComPhy feature 3D environments, as referenced in Table 1 by the author.
3. I appreciate the author's use of this paper as a benchmark for various RL approaches, highlighting the need for more research in this domain to address physical reasoning tasks. What potential solutions or recommendations does the author suggest for this benchmark?

Review Point: 1. Please add a table grid to Figure 2 for clearer comparisons.
Review Point: 2. The author notes the absence of 3D interactive environments as a limitation. However, this is a significant point to address since many high-performing models should ideally transfer seamlessly to robotics. A 3D interactive environment would greatly facilitate this transition. Notably, papers like ComPhy feature 3D environments, as referenced in Table 1 by the author.
Review Point: 3. I appreciate the author's use of this paper as a benchmark for various RL approaches, highlighting the need for more research in this domain to address physical reasoning tasks. What potential solutions or recommendations does the author suggest for this benchmark?
==================================================

Focused review:

**(W1)** Although the authors do not explicitly state this in the paper (perhaps intentionally?), this work firmly overlaps with the established areas of meta-learning and transfer learning. As such, the key challenge here is generalizability to unseen datasets. I was not able to find details about which datasets were used for training the FLAME framework and which ones were used for inference. Without this, it becomes hard to judge how well the proposed method can generalize to datasets it never encountered. The authors were pretty open about this being a shortcoming of the current approach (e.g. in Section 3.2.1 and Section 5) which is commendable. However, I would argue that generalizability is actually both the hardest and the most interesting part. Furthermore, without it, the entire framework can be seen as an interesting idea without much real-world use.
**(W2)** Leaning on the above point, I think this paper should be placed in the context of other work related to meta-learning and transfer learning, both in terms of methodology (see W1) and in terms of updating the introduction and related work sections.
**(W3)** Given that the authors apply HyperNetworks to obtain model weights, it's easy to wonder how much the proposed method relies on the parameter generator simply memorizing the weights for all of the dataset/task pairs it has been trained on, as opposed to being able to acquire meta-knowledge that is transferable between tasks. In other words, do we even need hypernetworks or can we just get away with a collection of pre-trained model weights? There are certain claims made in e.g. Section 4.2 about that topic, but this is more of a post-hoc interpretation rather than an empirically validated claim. More convincing empirical evidence would be to include results where we have a collection of pre-trained models (the same collection as the one FLAME ends up learning to parameterize during pre-training) and test if having an LLM pick the most appropriate set of weights from the stash could improve performance.
**(W4)** (minor issue) There are a small handful of writing issues and missing clarifications that could easily be addressed:
* (Abstract) "high-performance models", and in general usage of the word "model" is a little ambiguous as it is unclear if the authors are talking about LLMs or deep learning models, or even classical ML models. This becomes clear later on but could have been clarified earlier.
* (Section 1, page 2) "In real-world scenarios, data often is limited or lacks insufficient supervisions" -- Firstly, "supervisions" -> "supervision". Secondly, the authors likely want to say "lacks sufficient supervision". Thirdly, it is unclear what the authors even mean when they say the data lacks sufficient supervision. This should probably be reworded.
* (Section 4.1.2) "while Relative Efficiency scales this runtime against the worst-performing method" -- worst-performing in terms of runtime or model quality? I think the authors mean runtime but this could be clarified.
* (Section 5) "we aim to paves" -> "we aim to pave"

Review Point: * (Abstract) "high-performance models", and in general usage of the word "model" is a little ambiguous as it is unclear if the authors are talking about LLMs or deep learning models, or even classical ML models. This becomes clear later on but could have been clarified earlier.
Review Point: * (Section 4.1.2) "while Relative Efficiency scales this runtime against the worst-performing method" -- worst-performing in terms of runtime or model quality? I think the authors mean runtime but this could be clarified.
==================================================

Focused review:

Reading Section 2, I am not sure I quite understand how novel is NeuralMPM. How does it compare to other methods that use MPM in a learning setting? How does it differ from the work of Li et al. (2024)? What are its main contributions?
NeuralMPM is compared against only two models, namely GNS and DMCF, making it hard to assess how good it actually is.
In general, I don’t think authors make a compelling argument for the merits of their model, probably due to their choice of figures. A list below:
1. why using the WaterRamps dataset in Figure 4 if WaterRamps is the only dataset over which NeuralMPM does not outperform the other two models?
2. Figure 7: you claim that ”A NeuralMPM model trained on a square domain can naturally generalise to larger rectangular domains (twice as wide here) when using a fully convolutional U-Net”, but without ground truth (as you claim in Fig. 19 and line 486) for the right hand side particles how can we assess how good is NeuralMPM at generalising? I am having a hard time believing that the evolution from starting point to $t_1$ is realistic (where did the leftmost blob of water go?).
Also, the labels between the left and right section of the figure are confusing (Predictions and GT for left and Predictions $t_1$, Predictions $t_2$ for right).
3. In line 469 you claim that “one notable advantage of NeuralMPM is its invariance to the number of particles, as the transition model only processes the voxelized representation.” Doesn’t figure 6 confute the claim? After 10^5 particles the FPS decreases and after 10^4 the GPU usage increases for NeuralMPM. They don’t seem to be an incredibly large number of particles either. Could you please clarify this point?
4. You claim that NeuralMPM is superior to GNS, but the results reported by their authors show otherwise, as shown in Fig. 3. Without a discussion on what might cause this discrepancy, it is hard to assess the merits of NeuralMPM over GNS.
The abstract and conclusions don’t highlight the merits of the paper enough, in my opinion, especially because there’s plenty of details and quantitative results in Section 4. For example:
Line 22: We demonstrate the advantages of NeuralMPM on several datasets (how many? Which ones?), including fluid dynamics and fluid-solid interactions
Line 23: Compared to existing methods (which methods?), NeuralMPM reduces training times from days to hours (can you quantify the improvement?)
Line 516: NeuralMPM trains in a fraction of the time (what fraction?) it takes to train alternative approaches (what are alternative approaches?)
I am willing to increase my score if:
1) comparisons of the models’ performance for different datasets (Multilateral, DamBreak 2D and/or Variable Gravity) are plotted, showing the 3x improvement reported in Table 1 of NeuralMPM over GNS and DMCF for those datasets also through figures.
2) an intuition on why the authors’ implementation of GNS performs differently to the original paper is provided and a comparison of the GNS training speed originally reported is offered.
3) The claim of invariance with respect to number of particle numbers is adequately defended.
4) Abstract and conclusions are reformulated to better highlight the merits and the original contributions of NeuralMPM with respect to previous literature by quantitative results.

Review Point: 1. why using the WaterRamps dataset in Figure 4 if WaterRamps is the only dataset over which NeuralMPM does not outperform the other two models?
Review Point: 3. In line 469 you claim that “one notable advantage of NeuralMPM is its invariance to the number of particles, as the transition model only processes the voxelized representation.” Doesn’t figure 6 confute the claim? After 10^5 particles the FPS decreases and after 10^4 the GPU usage increases for NeuralMPM. They don’t seem to be an incredibly large number of particles either. Could you please clarify this point?
Review Point: 4. You claim that NeuralMPM is superior to GNS, but the results reported by their authors show otherwise, as shown in Fig.
Review Point: 1) comparisons of the models’ performance for different datasets (Multilateral, DamBreak 2D and/or Variable Gravity) are plotted, showing the 3x improvement reported in Table 1 of NeuralMPM over GNS and DMCF for those datasets also through figures.
Review Point: 2) an intuition on why the authors’ implementation of GNS performs differently to the original paper is provided and a comparison of the GNS training speed originally reported is offered.
Review Point: 3) The claim of invariance with respect to number of particle numbers is adequately defended.
Review Point: 4) Abstract and conclusions are reformulated to better highlight the merits and the original contributions of NeuralMPM with respect to previous literature by quantitative results.
==================================================

Focused review:

I think the paper needs a significant revision. I cannot read the mathematical notation as it is so I cannot evaluate the validity of the results. For example, three places where the classification of Type I vs Type II saddles are done -- all three unclear to me.
1. Line 36: what does it mean "GD noise persists at the saddle"
2. Line 43: Type-I where the **noises of the gradient vanishes in the escape directions**, and Type-II saddles where the **gradient noise vanish in the directions of escape**.
I read this sentence maybe 5 times, but the two definitions are identical up to a permutation of words.
* Can you provide a precise mathematical definition of "GD noise persists at the saddle" ?
* Can you clarify the distinction between "escape directions" and "directions of escape", or use consistent terminology if they mean the same thing ?
* Can you define these concepts explicitly when they are first introduced?
3. Line 102-103: What is $\hat{H}(x_t)$ here?
Figure 3 seems to capture the core result of the paper and hence could be put on the second page. Can you provide a clear definition of this concept of $L_2$ stability when it is first introduced, as this seems to be an important concept in the paper ?
Why does SGD escape the Type-II saddle in Figure 1 (even though in a longer time scale). Aren't Type-II saddles are attractive under SGD by definition? Can you explicitly address this apparent contradiction and clarify the conditions under which Type-II saddles are escaped versus when they are attractive?
Finally, the attractivity of saddles and even local max is studied in Chen et al 2023 which is a very important but missing citation in this submission. They also study the symmetry-induced saddles and low-rank matrices for linear networks.
Chen, Feng, et al. "Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks." Advances in Neural Information Processing Systems 36 (2024).
How does this paper compare to Chen et al 2023?

Review Point: 1. Line 36: what does it mean "GD noise persists at the saddle" 2. Line 43: Type-I where the **noises of the gradient vanishes in the escape directions**, and Type-II saddles where the **gradient noise vanish in the directions of escape**. I read this sentence maybe 5 times, but the two definitions are identical up to a permutation of words.
Review Point: * Can you provide a precise mathematical definition of "GD noise persists at the saddle" ?
Review Point: * Can you clarify the distinction between "escape directions" and "directions of escape", or use consistent terminology if they mean the same thing ?
Review Point: * Can you define these concepts explicitly when they are first introduced?
==================================================

Focused review:

- The paper predominantly centers on evaluating attacks and defence strategies. But the paper title implies a broader scope – VFL in its entirety. The paper title could be more specific to align with the focus of the paper.
- A notable contribution is the introduction of new evaluation metrics such as defence capability score (DCS). However, the experiments did not validate the effectiveness of the proposed metrics. What is the evidence that shows that the proposed metrics indeed work, representing the real ability of the evaluated algorithms?
- The paper claims to “implement basic VFL training and evaluation flow under multiple model partition, communication protocols and attacks and defences algorithms using datasets of different modality”. But it lacks a clear exposition of the workflow. What is the training and evaluation flow in VFLAIR and how does the workflow facilitate the benchmarking?
- Following on the above point, the evaluation section is not organised systematically according to model partition, communication protocols, and attack and defence algorithms. The current evaluation section only amounts to a compilation of experimental results, which
requires a more structured, systematic and coherent organization.
There is only one paragraph in related work. There is no need to employ a bullet point at the beginning

Review Point: - The paper predominantly centers on evaluating attacks and defence strategies. But the paper title implies a broader scope – VFL in its entirety. The paper title could be more specific to align with the focus of the paper.
Review Point: - A notable contribution is the introduction of new evaluation metrics such as defence capability score (DCS). However, the experiments did not validate the effectiveness of the proposed metrics. What is the evidence that shows that the proposed metrics indeed work, representing the real ability of the evaluated algorithms?
Review Point: - The paper claims to “implement basic VFL training and evaluation flow under multiple model partition, communication protocols and attacks and defences algorithms using datasets of different modality”. But it lacks a clear exposition of the workflow. What is the training and evaluation flow in VFLAIR and how does the workflow facilitate the benchmarking?
Review Point: - Following on the above point, the evaluation section is not organised systematically according to model partition, communication protocols, and attack and defence algorithms. The current evaluation section only amounts to a compilation of experimental results, which requires a more structured, systematic and coherent organization. There is only one paragraph in related work. There is no need to employ a bullet point at the beginning
==================================================

Focused review:

1. The paper's structure is not well-organized, making it challenging to grasp the primary contributions and follow the narrative. Furthermore, the writing is convoluted, containing multiple unclear sentences that hinder comprehension.
2. The experimental results lack persuasiveness. Firstly, the authors do not compare their work with state-of-the-art algorithms such as SAC-N, EDAC [1], and LB-SAC [2]. Even when implementing the algorithms based on https://github.com/tinkoff-ai/CORL, this paper's results are inferior to previous methods in most Mujoco and Adroit environments. Hence, I believe there are significant limitations in the paper's empirical contributions.
3. Figure 2 should clarify the color codes representing different algorithms and indicate the corresponding perturbation magnitudes. The paper should explain how optimal state perturbations were determined in the robustness analysis. The connection between the state perturbations in the experiments and the real-world environments, as claimed in the paper, seems tenuous. I have reservations about the practical significance of this paper.
4. While the paper focuses on model-free offline RL, I find it puzzling that model-based offline RL is included in the related work. Robust RL works like [3, 4] (and others) dealing with state adversaries seem more relevant, yet the authors do not discuss them in the related work section.
[1] Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble. Gaon An, Seungyong Moon, Jang-Hyun Kim, Hyun Oh Song. NeurIPS 2021.
[2] Q-Ensemble for Offline RL: Don’t Scale the Ensemble, Scale the Batch Size. Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, Dmitry Akimov, Sergey Kolesnikov.
[3] Robust Reinforcement Learning on State Observations with Learned Optimal Adversary. Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh. ICLR 2021.
[4] Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Furong Huang. Neurips 2022.

Review Point: 1. The paper's structure is not well-organized, making it challenging to grasp the primary contributions and follow the narrative. Furthermore, the writing is convoluted, containing multiple unclear sentences that hinder comprehension.
Review Point: 2. The experimental results lack persuasiveness. Firstly, the authors do not compare their work with state-of-the-art algorithms such as SAC-N, EDAC [1], and LB-SAC [2]. Even when implementing the algorithms based on https://github.com/tinkoff-ai/CORL, this paper's results are inferior to previous methods in most Mujoco and Adroit environments. Hence, I believe there are significant limitations in the paper's empirical contributions.
Review Point: 3. Figure 2 should clarify the color codes representing different algorithms and indicate the corresponding perturbation magnitudes. The paper should explain how optimal state perturbations were determined in the robustness analysis. The connection between the state perturbations in the experiments and the real-world environments, as claimed in the paper, seems tenuous. I have reservations about the practical significance of this paper.
==================================================

Focused review:

- Despite utilizing a more complex architecture and significantly increasing the number of trainable parameters, IA-GPL does not exhibit a consistently superior performance over GPF-plus. This raises questions regarding whether the added complexity and computational cost translate into a meaningful improvement. Especially, given the small gap and the high standard deviation, I'm uncertain whether the results are statistically significant.
| (50-shot) | ToxCast | SIDER | ClinTox | BACE | HIV | MUV | # Tuning parameters|
|-----| - | - | - | - | - | - | - |
| GPF-plus | 60.85±1.69 | 52.44±0.83 | 73.85±2.15 | 76.02±0.99 | 64.49±1.19 | 59.93±0.83 | 3-12K|
| IA-GPL | 61.63±0.40 | 52.85±0.84 | 74.50±0.76 | 76.64±0.83 | 64.60±0.95 | 59.32±1.13 | 20K (167%~667% of GPF-plus)|
- The IA-GPL method bears similarities to the GPF-plus approach, particularly in the way it generates node-level prompts by leveraging a shared basis. However, a side-by-side comparison of these two methods is notably absent in Figure 2, where the inclusion of GPF-plus would have provided a clearer perspective on how IA-GPL differentiates itself. Also, what are the potential theoretical advantages of IA-GPL compared to GPF-plus? The authors argue that while GPF-plus uses node-specific prompts, IA-GPL is superior; however, the experimental results do not strongly support this claimed superiority.
- Additionally, while IA-GPL incorporates more sophisticated mechanisms and employs additional training parameters, it also incurs greater computational overhead compared to the simpler aggregation technique used in GPF-plus.

Review Point: - Additionally, while IA-GPL incorporates more sophisticated mechanisms and employs additional training parameters, it also incurs greater computational overhead compared to the simpler aggregation technique used in GPF-plus.
==================================================

Focused review:

* The paper only considers purely cooperative and competitive environments. It would be interesting to see whether PAIC can handle more complex mixed-motive environments.
* The paper assumes that the peer agent does not update its policy during test time. However, in the real world, peers may be able to tune their policies online, which could pose a challenge for PAIC.

Review Point: * The paper only considers purely cooperative and competitive environments. It would be interesting to see whether PAIC can handle more complex mixed-motive environments.
Review Point: * The paper assumes that the peer agent does not update its policy during test time. However, in the real world, peers may be able to tune their policies online, which could pose a challenge for PAIC.
==================================================

Focused review:

weaknesses:
Evaluations are restricted to three topical biomedical settings constructed by the authors and do not include existing graph completion datasets.
the gap between full WEHG and the random walk version is quite small
there are existing knowledge graph traversal methods trained with reinforcement learning such as 'Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning' and 'DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning', as well as non RL algorithms such as 'Random walk inference and learning in a large scale knowledge base'. None of these are cited or discussed.
The qualitative trajectories in table 4 (presumably cherry picked) do not seem like good explanations
Since the definition of a correctly generated hypothesis is whether two terms will co-occur in a sentence, I think a good baseline would even be f(bio-bert("term 1, [sep], term2")) Notes:
The abstract is a little clunky in the middle, there is a sentence that uses the word node 4 times. - Figure 1 could be improved, maybe label the different portions of the image (a, b, c etc) and explicitly reference them in the caption.
Table 4 column 1 and 2 seem redundant

Review Point: - Figure 1 could be improved, maybe label the different portions of the image (a, b, c etc) and explicitly reference them in the caption. Table 4 column 1 and 2 seem redundant
==================================================

Focused review:

This paper has the following issues:
1. The authors did not compare the proposed method with any previous approaches.
2. The main contribution of this paper is its consideration of geographic spread - how many countries a video has trended in - as well as engagement intensity, that is view count, in measuring video popularity. However, they seem to be quite orthogonal to me, but the authors used only a single score integrating both two factors. I am not sure why they used separate scores measuring each. Especially, While the authors mentioned in section A.3 that there is a positive correlation between them, but according to Figure A2, there are also non-negligible number of videos categorized into "locally ultra popular" and "globally moderately popular".
3. The writing needs to be improved. There are many redundant sentences aimed at increasing the length of the paper. For example, Section 5 contains too redundant statements about the hybrid approach using both llm prompting and supervised signals. Furthermore, there are too many cases where figures or tables contradict the text. To name a few, in Section A.3.2 (L803), the authors mentioned that the upper-right quadrant in Figure A3 is less populated than the lower-left quadrant, which contradicts the fact that there are 6,279 local hits videos and 7,360 global bit hits videos. There are also no 0.83 in Table A4 (L1187); the highest score is 0.78. The authors should also fix typos, e.g., please fix "red lines" in the Figure A3 caption to "blue lines". Overall, it feels as though the authors have submitted an unfinished draft that hasn’t been proofread.
4. There are too many missing details
- How did the authors differentiate the videos of classes 1 and 2 in the local hit cluster and the videos of classes 3 and 4 in the global big hit cluster? There are two defining factors, geographical spread and view count, and I am not sure about how they are integrated into a single score.
- The authors did not mention the training recipe and data for the supervised baseline model.
- L299: Please elaborate on the hypothesis generation function Phi_hypothesis, especially regarding the reinforcement learning technique used for it, if any (Section 5). And how many hypotheses are used for prediction?
- Please provide the exact final prompt text.
- How did the authors screen participants in the surveys?
- And please let me know exactly what questions were asked to the participants in the survey to score each metric.
- L508 "This feature enhances not only the performance but also the explainability of the model": This statement is not explicitly validated in the paper. The authors used only accuracy and precision/recall for evaluation.

Review Point: 1. The authors did not compare the proposed method with any previous approaches.
Review Point: 4. There are too many missing details - How did the authors differentiate the videos of classes 1 and 2 in the local hit cluster and the videos of classes 3 and 4 in the global big hit cluster? There are two defining factors, geographical spread and view count, and I am not sure about how they are integrated into a single score.
Review Point: - The authors did not mention the training recipe and data for the supervised baseline model.
Review Point: - L299: Please elaborate on the hypothesis generation function Phi_hypothesis, especially regarding the reinforcement learning technique used for it, if any (Section 5). And how many hypotheses are used for prediction?
Review Point: - How did the authors screen participants in the surveys?
Review Point: - And please let me know exactly what questions were asked to the participants in the survey to score each metric.
Review Point: - L508 "This feature enhances not only the performance but also the explainability of the model": This statement is not explicitly validated in the paper. The authors used only accuracy and precision/recall for evaluation.
==================================================

Focused review:

* In its present form, the paper is not well-organized and reads disjointed. Without having intimate knowledge of each cited work, it is at many points unclear what parts are repeated from prior work and what is actually novel, since reproduced and (apparently) newly derived formulae are intermixed in writing without clear distinctions. For example, Theorem 2 / Eq. (4) is reproduced from Zhang & Chen (2022, Eq. 16), but is not clearly marked as such, instead it is only noted as being "inspired by (Zhang & Chen, 2022)", and the following formula Eq. (5) seems to be simple substitution of the authors' SNR-based expression, as far as I can tell. **This could be improved by stating clearly which theoretical results are reproduced and which are novel, and what the motivation and purpose of extending, generalizing, or modifying each known result is.**
* Furthermore, it is unclear what the goal of each section is, as many subsections of the main section 3 do not seem to be connected and do not form a cohesive story. Specifically, the authors derive (or reproduce) six theorems and one corollary in section 3, but in section 4, they only make use of the corollary (which is a corollary of theorem 3) to perform any experiments. This makes it unclear what the purpose of theorems 4, 5, and 6 in this work is, especially since they also do not seem to be connected to each other. **This could be improved by rewriting sections towards a more coherent story linking the results, and by adding experiments related to theorems 4-6, if possible.** For example, for theorem 5, the authors could actually list a grouping of S2N models that "induce the same diffusion model on the signal-to-noise space", as stated as a possibility in lines 378-380. For theorem 6, the authors could attempt an empirical comparative analysis of the mutual information of various S2N models, e.g. the pretrained diffusion models used for the other experiments.
* The SNR-based viewpoint, which according to the title should be a key feature of the paper, does not seem really necessary for theorems 1 to 3 and corollary 1 (which, as noted, are however the only theoretical results that are used in the experimental section), unless I am missing something. This viewpoint - as used in this work - also seems to generally increase the complexity of the involved expressions rather than reduce it, whereas the reduction of complexity was one key advantage of originally introducing this viewpoint in Kingma et al. (2022). **Please clarify the use of this viewpoint for theorems 1-3 and corollary 1.**
Reviewer comment: The SNR-based viewpoint, which according to the title should be a key feature of the paper, does not seem really necessary for theorems 1 to 3 and corollary 1 (which, as noted, are however the only theoretical results that are used in the experimental section). This viewpoint - as used in this work - also seems to generally increase the complexity of the involved expressions rather than reduce it, whereas the reduction of complexity was one key advantage of originally introducing this viewpoint in Kingma et al. (2022). Could the authors clarify how the SNR-based perspective contributes to each theorem and the corollary?
* Some expressions and derivations listed in the main text seem overly extensive, showing trivial steps that may better be left in the Appendix, for example in lines 253-269. Doing so could free up space for writing a more cohesive storyline and for easier understanding of the key contributions.
* The experimental section seems highly limited in scope and generalizability:
* The authors only evaluate a small selection of models and use FID as the single metric to compare models. For models based on the datasets FFHQ and the conditional model of ImageNet, only marginal differences in FID are found (Table 2). Only for AFHQ, the proposed choice $\gamma = 0.026$ seems to make a real difference for FID. For CIFAR-10 32x32, the gains from $\gamma \neq 0$ are more clear (Fig. 1), but this dataset is very limited by its resolution and size, making it generally unsuitable for drawing deep conclusions.
* The best choice of the hyperparameters $\gamma, \delta$ (and perhaps $\rho$) may well be very different for other datasets or even different network architectures, but the experimental section lacks any discussion of this.
* The results in Figure 2 and 3, setting $\rho = 1$ (as in standard SDE sampling) and varying $\delta, \gamma$ are not significant at all, with an FID score of 1.68 vs. 1.70 for conditional ImageNet and between 2.36 and 2.40 for unconditional CIFAR-10.
* This seems to suggest that, beyond results on the limited CIFAR-10 set (Figure 1), there are no relevant gains from the proposed sampler, and it requires expensive grid-searches for up to three hyperparameters, severely limiting its practical usefulness.
* **The trustworthiness of the presented empirical results could be improved by evaluating on other datasets or tasks, other pretrained models (not only those trained with the EDM formalism) and/or comparing against other samplers proposed in the literature, at similar NFE.**

Review Point: * Some expressions and derivations listed in the main text seem overly extensive, showing trivial steps that may better be left in the Appendix, for example in lines 253-269. Doing so could free up space for writing a more cohesive storyline and for easier understanding of the key contributions.
Review Point: * The experimental section seems highly limited in scope and generalizability:
Review Point: * The authors only evaluate a small selection of models and use FID as the single metric to compare models. For models based on the datasets FFHQ and the conditional model of ImageNet, only marginal differences in FID are found (Table 2). Only for AFHQ, the proposed choice $\gamma = 0.026$ seems to make a real difference for FID. For CIFAR-10 32x32, the gains from $\gamma \neq 0$ are more clear (Fig. 1), but this dataset is very limited by its resolution and size, making it generally unsuitable for drawing deep conclusions.
Review Point: * The best choice of the hyperparameters $\gamma, \delta$ (and perhaps $\rho$) may well be very different for other datasets or even different network architectures, but the experimental section lacks any discussion of this.
Review Point: * The results in Figure 2 and 3, setting $\rho = 1$ (as in standard SDE sampling) and varying $\delta, \gamma$ are not significant at all, with an FID score of 1.68 vs. 1.70 for conditional ImageNet and between 2.36 and 2.40 for unconditional CIFAR-10.
Review Point: * This seems to suggest that, beyond results on the limited CIFAR-10 set (Figure 1), there are no relevant gains from the proposed sampler, and it requires expensive grid-searches for up to three hyperparameters, severely limiting its practical usefulness.
Review Point: * **The trustworthiness of the presented empirical results could be improved by evaluating on other datasets or tasks, other pretrained models (not only those trained with the EDM formalism) and/or comparing against other samplers proposed in the literature, at similar NFE.**
==================================================

Focused review:

1. The author learns a set of conditional embeddings through a few images. When using the LFW dataset for training, do different images under the same identity need to be trained separately or can they be trained together to obtain a set of conditional embeddings for encryption and decryption?
2. In Sec.2.2, the description of obtaining conditional embeddings seems unclear. Does each time step correspond to a unique time embedding?

Review Point: 1. The author learns a set of conditional embeddings through a few images. When using the LFW dataset for training, do different images under the same identity need to be trained separately or can they be trained together to obtain a set of conditional embeddings for encryption and decryption?
Review Point: 2. In Sec.2.2, the description of obtaining conditional embeddings seems unclear. Does each time step correspond to a unique time embedding?
==================================================

Focused review:

- The biggest weakness is in the experiment evaluation part. I cannot get a clear sense of the quality of the generated videos.
- No significant quantitative comparison is performed. Only one set of clip scores is reported, and clip score is not a good metric for video generation!(It does not consider temporal consistency) I recommend the author add more metrics like Frechet Video Distance (FVD) (Unterthiner et al., 2018), and Kernel Video Distance (KVD) (Unterthiner et al., 2019).
- The proposed algorithm is training-free, so it can easily generate high-resolution videos using high-resolution text-to-image models like SDXL. It would be great for the author to add more high-resolution results.
- For visual comparison, adding more videos, especially videos with higher frame rates in supplementary materials or external websites, is very helpful. Also user study is welcomed for more solid evaluations.
- The presentation has some weaknesses.
- The first paragraph of section 2: "Present text-to-video synthesis techniques either require costly training on large-scale text-video paired data ...., or necessitate fine-tuning on a reference video...". This sentence is logically wrong since the author cited some training-free video generation methods, like the text2video-zero.
- In figures 2,3,4,5... The font in the figure is too small. It would be better to improve the font size.
- Ablations of the two proposed components is missing. It would be interesting to see what the videos will be like without the momentum attention, or without the dependency noise sampling.

Review Point: - The biggest weakness is in the experiment evaluation part. I cannot get a clear sense of the quality of the generated videos.
Review Point: - No significant quantitative comparison is performed. Only one set of clip scores is reported, and clip score is not a good metric for video generation!(It does not consider temporal consistency) I recommend the author add more metrics like Frechet Video Distance (FVD) (Unterthiner et al., 2018), and Kernel Video Distance (KVD) (Unterthiner et al., 2019).
Review Point: - The proposed algorithm is training-free, so it can easily generate high-resolution videos using high-resolution text-to-image models like SDXL. It would be great for the author to add more high-resolution results.
Review Point: - For visual comparison, adding more videos, especially videos with higher frame rates in supplementary materials or external websites, is very helpful. Also user study is welcomed for more solid evaluations.
Review Point: - The first paragraph of section 2: "Present text-to-video synthesis techniques either require costly training on large-scale text-video paired data ...., or necessitate fine-tuning on a reference video...". This sentence is logically wrong since the author cited some training-free video generation methods, like the text2video-zero.
Review Point: - In figures 2,3,4,5... The font in the figure is too small. It would be better to improve the font size.
Review Point: - Ablations of the two proposed components is missing. It would be interesting to see what the videos will be like without the momentum attention, or without the dependency noise sampling.
==================================================

Focused review:

Weakness:
1. CIFAR and MNIST are too toy. ImageNet experiment and fair comparison with previous unsupervised learning (especially contrastive learning) are important, but missing in this work.
2. LeNet is also too toy for a fair comparison with the latest results on unsupervised learning. A model of the ResNet level is a must.
3. Some related works on prototype learning are not cited, like “Prototypical Contrastive Learning of Unsupervised Representations”.

Review Point: 1. CIFAR and MNIST are too toy. ImageNet experiment and fair comparison with previous unsupervised learning (especially contrastive learning) are important, but missing in this work.
Review Point: 2. LeNet is also too toy for a fair comparison with the latest results on unsupervised learning. A model of the ResNet level is a must.
Review Point: 3. Some related works on prototype learning are not cited, like “Prototypical Contrastive Learning of Unsupervised Representations”.
==================================================

Focused review:

The improver's signal for improvement is a real value, which is the aggregation of utilities of solutions to different problems.
Moreover, the utilities of the solution of each problem may be on different scales, and they are linearly combined.
1. Different tasks have utilities with different values and scales.
2. Aggregating utilities of different tasks to a single number is a minimal input to the improver and may be an insufficient signal for improvement (just one number in each iteration and previous improver code).
3. Tasks may not be representative, and their selection is unclear.
4. Each utilities instantiates data for the problem, for example maxcut instantiates 3 random graphs with 300 nodes each.
It's unclear how the data and instances are selected.
3 and 4 together: how many tasks, data instances, and their types are required?
5. Figure 4a is missing iterations beyond T=4.
6. The approach assumes many solution variants are generated for each task, and it's unclear how much of the "improvement" is due to increasing the number of variants.
7. It's unclear how much of the "improvement" is due to GPT-4 being a generic language model that may improve each selected task rather than generic recursive self-improvement.
8. The paper is well written, and the presented formulation is elegant at a high-level, however, it is missing key details of self-improvement.

Review Point: 1. Different tasks have utilities with different values and scales.
Review Point: 2. Aggregating utilities of different tasks to a single number is a minimal input to the improver and may be an insufficient signal for improvement (just one number in each iteration and previous improver code).
Review Point: 3. Tasks may not be representative, and their selection is unclear.
Review Point: 4. Each utilities instantiates data for the problem, for example maxcut instantiates 3 random graphs with 300 nodes each. It's unclear how the data and instances are selected.
Review Point: 3 and 4 together: how many tasks, data instances, and their types are required?
Review Point: 6. The approach assumes many solution variants are generated for each task, and it's unclear how much of the "improvement" is due to increasing the number of variants.
Review Point: 7. It's unclear how much of the "improvement" is due to GPT-4 being a generic language model that may improve each selected task rather than generic recursive self-improvement.
Review Point: 8. The paper is well written, and the presented formulation is elegant at a high-level, however, it is missing key details of self-improvement.
==================================================

Focused review:

* **Claims with Insufficient Experimental Results** My main concern is that the authors claim the model retains its original text generation capabilities, but they provide insufficient experimental results to support this. Maybe the authors can demonstrate some results on commonly used text generation tasks, e.g. natural language understanding (MMLU, BigBench), summarization.
* **Lack of Motivation** It's unclear whether the results reported in Section 4 reflect zero-shot performance or task-specific fine-tuning. I believe zero-shot performance would be more meaningful, as it would more convincingly demonstrate the effectiveness of a unified model in both semantic representation and text generation tasks. Otherwise, selecting the optimal attention mechanism and training objective for each specific task would be more practical.

Review Point: * **Claims with Insufficient Experimental Results** My main concern is that the authors claim the model retains its original text generation capabilities, but they provide insufficient experimental results to support this. Maybe the authors can demonstrate some results on commonly used text generation tasks, e.g. natural language understanding (MMLU, BigBench), summarization.
Review Point: * **Lack of Motivation** It's unclear whether the results reported in Section 4 reflect zero-shot performance or task-specific fine-tuning. I believe zero-shot performance would be more meaningful, as it would more convincingly demonstrate the effectiveness of a unified model in both semantic representation and text generation tasks. Otherwise, selecting the optimal attention mechanism and training objective for each specific task would be more practical.
==================================================

Focused review:

1. The writing of the paper can be improved, for example, the description of the experimental setting and model details is not clear enough, which leads to some confusion (see the questions below).
2. The technical novelty of the paper is limited as the two proposed components seem to be very straightforward.

Review Point: 1. The writing of the paper can be improved, for example, the description of the experimental setting and model details is not clear enough, which leads to some confusion (see the questions below).
Review Point: 2. The technical novelty of the paper is limited as the two proposed components seem to be very straightforward.
==================================================

Focused review:

* May be over claiming contributions: I have to admit that I have not read a lot of papers in few-shot/zero-shot/one-shot domain. It seems quite unexpected to me that no one had ever attempted to develop a system that can utilize labels of all frequencies. After reading this paper, I did some simple google search. Without going into too much details, I found some articles discussing models that can handle "N-shot" labels with N ranging from 0 to some N, albeit N is not claimed to be set to infinity, which seems to be very similar to the "X-shot" problem formation. Since I am no expert in this topic and I certainly do not intend to become on for the purpose of this review, I encourage other reviewers who have more domain knowledge than me to think about this point.
* The fact that the proposed model does not do well on the Situation dataset can be problematic: From table 2 we can see that the proposed model does not do well on the Situation dataset and it certainly did not "still exceed[s] all other baselines significantly." In particular, I find it hard to be persuaded that the proposed model exceeds NLI (Li et al., 2022) **significantly**. Can authors elaborate on this?
* Other aspect might worth considerations yet unexplored: For example, for the Situation dataset, GPT3.5 is able to outperform all other models. However, the authors could have argue that GPT3.5 might be less advantageous because of computational efficiency. The paper does not provide a detailed comparison of X-SHOT with other state-of-the-art techniques in terms of computational efficiency. It is unclear how X-SHOT compares in terms of training time and computational resources required.

Review Point: * The fact that the proposed model does not do well on the Situation dataset can be problematic: From table 2 we can see that the proposed model does not do well on the Situation dataset and it certainly did not "still exceed[s] all other baselines significantly." In particular, I find it hard to be persuaded that the proposed model exceeds NLI (Li et al., 2022) **significantly**. Can authors elaborate on this?
Review Point: * Other aspect might worth considerations yet unexplored: For example, for the Situation dataset, GPT3.5 is able to outperform all other models. However, the authors could have argue that GPT3.5 might be less advantageous because of computational efficiency. The paper does not provide a detailed comparison of X-SHOT with other state-of-the-art techniques in terms of computational efficiency. It is unclear how X-SHOT compares in terms of training time and computational resources required.
==================================================

Focused review:

Overall, the presentation needs improvement such as:
- A simple prompt example used to generate a detailed rationale shown in Figure 2 would be better.
- A clear subsection of used benchmarks and LLMs in the main paper.
I am confused about the description of the experimental section such as:
- Figure 8 is not clear enough at first sight. Which method uses skill annotations? And does the higher precision @ k = 20 indicate better performance or worse? Could you please explain the reason behind the evaluation?
- Figure 9 only depicts the number of skills per instance obtained by direct prompting vs. our rationale parsing. While the larger number cannot show the generation effectiveness. Reliability or relevance to the question are more important. Is there any other ablation study showing the comparisons concerning the reliability of annotated skills?

Review Point: - A simple prompt example used to generate a detailed rationale shown in Figure 2 would be better.
Review Point: - A clear subsection of used benchmarks and LLMs in the main paper. I am confused about the description of the experimental section such as:
Review Point: - Figure 8 is not clear enough at first sight. Which method uses skill annotations? And does the higher precision @ k = 20 indicate better performance or worse? Could you please explain the reason behind the evaluation?
Review Point: - Figure 9 only depicts the number of skills per instance obtained by direct prompting vs. our rationale parsing. While the larger number cannot show the generation effectiveness. Reliability or relevance to the question are more important. Is there any other ablation study showing the comparisons concerning the reliability of annotated skills?
==================================================

Focused review:

1. It is unclear whether the used prompt can best unleash VLMs' performance. For example, from Table 5, it seems no example has been provided, and that may lead to lower VLM performance.
2. Why do human only achieve 83-85% accuracy if human collected the dataset and this dataset do not require expert knowledge? (Line 426-427) It is a bit confusing to understand.
3. In Table 3, why not try GPT-4o as the Image-to-Text model? Also, why not try Claude models as predictor?
4. The images are generated instead of from real world, and could potentially affect the output. The test size is 350 which might be small.

Review Point: 1. It is unclear whether the used prompt can best unleash VLMs' performance. For example, from Table 5, it seems no example has been provided, and that may lead to lower VLM performance.
Review Point: 2. Why do human only achieve 83-85% accuracy if human collected the dataset and this dataset do not require expert knowledge? (Line 426-427) It is a bit confusing to understand.
Review Point: 3. In Table 3, why not try GPT-4o as the Image-to-Text model? Also, why not try Claude models as predictor?
Review Point: 4. The images are generated instead of from real world, and could potentially affect the output. The test size is 350 which might be small.
==================================================

Focused review:

1. The actual role of using a diffusion model remains unclear. If the prior is not a diffusion model, will the algorithm still perform effectively in this scenario?
2. The theoretical statements provided are only valid for the linear case. It is also unclear whether there would be a significant gap in performance between linear and non-linear cases, which could impact the applicability of the algorithm in more complex settings.
3. The paper addresses both the contextual bandit and bandits with dependent arms. However, this combined focus makes it challenging to isolate the specific impact of using a diffusion model. It remains unclear whether the diffusion model could be effectively applied in the traditional contextual bandit problem without dependent arms, as dependent arms generally allow for more informative decision-making.
4. Lastly, I am curious whether a diffusion model is truly necessary in this setting, especially if it is already pre-trained, to handle the contextual bandit scenario with dependent arms.

Review Point: 1. The actual role of using a diffusion model remains unclear. If the prior is not a diffusion model, will the algorithm still perform effectively in this scenario?
Review Point: 2. The theoretical statements provided are only valid for the linear case. It is also unclear whether there would be a significant gap in performance between linear and non-linear cases, which could impact the applicability of the algorithm in more complex settings.
Review Point: 3. The paper addresses both the contextual bandit and bandits with dependent arms. However, this combined focus makes it challenging to isolate the specific impact of using a diffusion model. It remains unclear whether the diffusion model could be effectively applied in the traditional contextual bandit problem without dependent arms, as dependent arms generally allow for more informative decision-making.
Review Point: 4. Lastly, I am curious whether a diffusion model is truly necessary in this setting, especially if it is already pre-trained, to handle the contextual bandit scenario with dependent arms.
==================================================

Focused review:

1. Lack of explanation and analysis of the model's deep mechanism. Why the dynamic update of confidence score works and why the model can outperformance supervised models so significantly need further detailed explanation. Only description but no explanation makes it short of interpretability.
2. Inconsistant symbol usage. The w_x in Eq. 5-7 and the w_i^t in Eq. 9 have different formats, which might lead to confusion.
3. An overview of the workflow and the model, which can make it easier to get the whole picture of the work, is needed.
4. Missing Ethics Statement (e.g., the reproductivity of the work).
5. Typo on Line 496 (wrong serial number '(2)').

Review Point: 1. Lack of explanation and analysis of the model's deep mechanism. Why the dynamic update of confidence score works and why the model can outperformance supervised models so significantly need further detailed explanation. Only description but no explanation makes it short of interpretability.
Review Point: 2. Inconsistant symbol usage. The w_x in Eq. 5-7 and the w_i^t in Eq. 9 have different formats, which might lead to confusion.
Review Point: 3. An overview of the workflow and the model, which can make it easier to get the whole picture of the work, is needed.
Review Point: 4. Missing Ethics Statement (e.g., the reproductivity of the work).
==================================================

Focused review:

- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)? - Some of the design choices are not explained well (e.g. why IDF-weighting) - Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed.
- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used? - There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance? - With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms? - A few minor typos, proof reading should fix them. Nothing major.

Review Point: - Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?
Review Point: - Some of the design choices are not explained well (e.g. why IDF-weighting) - Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed.
Review Point: - Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?
Review Point: - There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?
Review Point: - With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?
==================================================

Focused review:

- Although the authors show three different scenarios of the possible DAGs in Figure 2, there are fundamental issues in this modeling. There exists a real world that we measure with various sensors, such as the camera (generating images from various views) or textual descriptions of the scene. Hence, for two modalities, each observation requires two types of latent representations: modality-specific representations ($m_x$ and $m_t$) that hold information exclusive to each modality, and a shared representation ($z=z_x=z_t$) that captures common information across both modalities. This eliminates the need to connect $z_x$ and $z_t$ through an undirected edge. Further, this idea represents the DAG shown in Figure 2(a). While the authors justify DAGs shown in Figure 2(b) and Figure 2(c) with examples of text-to-image retrieval and image captioning, these tasks can also be captured as inference on the first DAG i.e. Figure 2(a).
- Theorem 3.1 is interesting and provides insights into the CLIP objective in an infinite sample setting. However, as also mentioned in the paper, this seems to be a direct extension of Theorem 1 to the multimodal setting in Wang and Isola (2020). I am unsure if it adds a lot of value.
- The claims made regarding the disentanglement capabilities of CLIP seem to be unfounded. Firstly, the authors base their assertions on empirical analysis using only a single dataset, which is insufficient to substantiate claims of disentanglement. Further, the authors state that“the objective of disentangled representations is to learn features that lend themselves to be easily and robustly transferred to downstream tasks”. However, this is more of a consequence of achieving disentangled representations rather than an actual objective. The paper does not provide a clear or rigorous definition of disentanglement (which in itself is an involved question). Moreover, the baselines used for the disentanglement experiments are outdated (from 2018), while there are more recent works in this field. 1. https://proceedings.neurips.cc/paper_files/paper/2023/hash/6818dcc65fdf3cbd4b05770fb957803e-Abstract-Conference.html 2. https://proceedings.neurips.cc/paper_files/paper/2023/hash/93e98ddf39a9beb0a97fbbe56a986c80-Abstract-Conference.html
Specifically, the first paper even shows the suboptimality of existing multimodal representation learning approaches such as CLIP. Overall, it appears that the authors use the notion of disentanglement to reinforce their theoretical findings, but their references to it are vague and unsupported. The experimental results presented do not imply “true” disentanglement.
- Minor: Figures 1 and 2 can be placed early on in the text. They are referred to in the introduction. It would be easier to have a main figure that explains the broad concept.

Review Point: - Theorem 3.1 is interesting and provides insights into the CLIP objective in an infinite sample setting. However, as also mentioned in the paper, this seems to be a direct extension of Theorem 1 to the multimodal setting in Wang and Isola (2020). I am unsure if it adds a lot of value.
Review Point: 1. https://proceedings.neurips.cc/paper_files/paper/2023/hash/6818dcc65fdf3cbd4b05770fb957803e-Abstract-Conference.html 2. https://proceedings.neurips.cc/paper_files/paper/2023/hash/93e98ddf39a9beb0a97fbbe56a986c80-Abstract-Conference.html Specifically, the first paper even shows the suboptimality of existing multimodal representation learning approaches such as CLIP. Overall, it appears that the authors use the notion of disentanglement to reinforce their theoretical findings, but their references to it are vague and unsupported. The experimental results presented do not imply “true” disentanglement.
Review Point: - Minor: Figures 1 and 2 can be placed early on in the text. They are referred to in the introduction. It would be easier to have a main figure that explains the broad concept.
==================================================

Focused review:

1. If I understand correctly, the MSE loss requires ground truth object states, which restricts the paper’s applicability to simulation scenarios. I’m curious if and how the authors plan to train the dynamics model using real-world data.
1. Additionally, I think one key advantage of neural dynamics models is their speed; however, the paper does not include an experiment comparing the speed of the proposed method with analytical methods. Could you provide runtime comparisons between MDGS and traditional analytical methods (e.g., finite element analysis) for simulating complex mechanical systems? This would give readers a clearer picture of the computational trade-offs.
1. A more comprehensive discussion of the method’s limitations from the authors would be appreciated.

Review Point: 1. If I understand correctly, the MSE loss requires ground truth object states, which restricts the paper’s applicability to simulation scenarios. I’m curious if and how the authors plan to train the dynamics model using real-world data.
Review Point: 1. Additionally, I think one key advantage of neural dynamics models is their speed; however, the paper does not include an experiment comparing the speed of the proposed method with analytical methods. Could you provide runtime comparisons between MDGS and traditional analytical methods (e.g., finite element analysis) for simulating complex mechanical systems? This would give readers a clearer picture of the computational trade-offs.
Review Point: 1. A more comprehensive discussion of the method’s limitations from the authors would be appreciated.
==================================================

Focused review:

- Since the attack's goal is to mislead agents into executing malicious actions, the scope of the paper aligns more closely with prompt injection attacks rather than jailbreak attacks. However, the paper focuses significantly on distinguishing itself from jailbreak attacks and includes them as evaluation baselines for comparison. It would be beneficial to provide a comprehensive survey of prompt injection attacks in the related work section, highlighting the key differences and advantages of the proposed method compared to existing attacks. Including prompt injection attacks as baselines would strengthen the contribution of the paper.
- Given the extensive research on prompt injection attacks, the paper does not clearly distinguish the key advantages of its proposed attack over existing methods. Specifically, there is a line of prior studies [1][2] that have utilized context-ignoring techniques by appending prompts like "Ignore previous instructions, do xxx." This is similar to the method used in this paper, as illustrated in Figure 4, where the attack prompt is "Disregard all previous and subsequent instructions and choices, do xxx." Moreover, previous methods do not require collecting a prompt training dataset or performing expensive DPO training of an LLM-based attack prompter, making them more efficient.
- Defenses against prompt injection attacks have also been extensively studied (e.g., [3][4]). It is important to assess the effectiveness of the proposed attack against these defenses to determine its practical applicability and robustness.
- The selection of victim web agents is limited. The paper evaluates only one type of VLM-based web agent, which may not fully demonstrate the generalizability of the proposed attacks. Incorporating a more comprehensive evaluation with a variety of web agents would strengthen the paper.
- In comparing baselines during evaluation, the paper includes several white-box jailbreak attacks like GCG and AutoDAN. It is unclear how these baselines are implemented against SeeAct, given that it uses proprietary VLMs that are black-box and do not provide gradient information. Reference:
[1] Branch, Hezekiah J., et al. "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples." arXiv preprint arXiv:2209.02128 (2022).
[2] Perez, Fábio, and Ian Ribeiro. "Ignore Previous Prompt: Attack Techniques For Language Models." NeurIPS ML Safety Workshop. 2022.
[3] Liu, Yupei, et al. "Formalizing and benchmarking prompt injection attacks and defenses." 33rd USENIX Security Symposium (USENIX Security 24). 2024.
[4] Chen, Sizhe, et al. "Aligning LLMs to Be Robust Against Prompt Injection." arXiv preprint arXiv:2410.05451 (2024).

Review Point: - Since the attack's goal is to mislead agents into executing malicious actions, the scope of the paper aligns more closely with prompt injection attacks rather than jailbreak attacks. However, the paper focuses significantly on distinguishing itself from jailbreak attacks and includes them as evaluation baselines for comparison. It would be beneficial to provide a comprehensive survey of prompt injection attacks in the related work section, highlighting the key differences and advantages of the proposed method compared to existing attacks. Including prompt injection attacks as baselines would strengthen the contribution of the paper.
Review Point: - Defenses against prompt injection attacks have also been extensively studied (e.g., [3][4]). It is important to assess the effectiveness of the proposed attack against these defenses to determine its practical applicability and robustness.
Review Point: - The selection of victim web agents is limited. The paper evaluates only one type of VLM-based web agent, which may not fully demonstrate the generalizability of the proposed attacks. Incorporating a more comprehensive evaluation with a variety of web agents would strengthen the paper.
==================================================

Focused review:

While it seems intuitive to observe some of these takeaways that the authors state in the paper about the benefits of human prompting, but at what cost? The benefit of developing automated approaches or at least some approach that combines both automated and crowdsourced prompts is that they would be far more scalable. Some questions that come to mind are:
- How many people do you need per question? Does this vary across questions?
- What kind of onboarding is necessary for people to have a grasp of how to prompt well?
- What kind of background is required? It seems like the vast majority of these 20 participants work in academia/tech and have graduate/college/phd degrees (as shown in Figure 6). Additionally, given the anecdotal example provided in point 1 of Section 4 results, it seems that significant creativity and understanding of the domain are necessary to come up with good prompts for more difficult problems.
- What is the distribution of time spent coming up with each prompt (and writing test cases)?
While it is not a weakness that prompts collected from LeetPrompt were able to solve all hidden test cases, it is a limitation (particularly given the small number of test cases) in terms of helping the community understand what the pitfalls (if any) of collecting human generated prompts are, i.e., when are automated approaches are better or worse than humans.

Review Point: - How many people do you need per question? Does this vary across questions?
Review Point: - What kind of onboarding is necessary for people to have a grasp of how to prompt well?
Review Point: - What kind of background is required? It seems like the vast majority of these 20 participants work in academia/tech and have graduate/college/phd degrees (as shown in Figure 6). Additionally, given the anecdotal example provided in point 1 of Section 4 results, it seems that significant creativity and understanding of the domain are necessary to come up with good prompts for more difficult problems.
Review Point: - What is the distribution of time spent coming up with each prompt (and writing test cases)? While it is not a weakness that prompts collected from LeetPrompt were able to solve all hidden test cases, it is a limitation (particularly given the small number of test cases) in terms of helping the community understand what the pitfalls (if any) of collecting human generated prompts are, i.e., when are automated approaches are better or worse than humans.
==================================================

Focused review:

There are two main concerns.
1. Did the authors try some specifically purification methods for the protection? e.g., the method in [1]. Can the method purify the perturbations?
2. The authors may need to compare the added loss term alone in the ACE+ loss with the proposed ACE loss. The added loss in ACE+ is also targeted attack. Experimental comparisons are needed between it and the ACE loss.
[1] Bochuan Cao et al. IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI, 2023.

Review Point: 1. Did the authors try some specifically purification methods for the protection? e.g., the method in [1]. Can the method purify the perturbations?
Review Point: 2. The authors may need to compare the added loss term alone in the ACE+ loss with the proposed ACE loss. The added loss in ACE+ is also targeted attack. Experimental comparisons are needed between it and the ACE loss. [1] Bochuan Cao et al. IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI, 2023.
==================================================

Focused review:

1. More experiments are expected, such as meta-learning, and poison attack. I think hyperparameters selection and data hyper-cleaning are somewhat similar, experiments on other applications are expected.
2. Some O(1) sample complexity methods should be compared, e.g. SUSTAIN[1]
3. The proposed method seems can not effectively solve the large-scale problem. Can the author give some results on the large-scale datasets?
4. The convergence analysis is based on the PL condition. An analysis on a more general case is expected.
[1] Khanduri P, Zeng S, Hong M, et al. A near-optimal algorithm for stochastic bilevel optimization via double-momentum[J]. Advances in neural information processing systems, 2021, 34: 30271-30283.

Review Point: 1. More experiments are expected, such as meta-learning, and poison attack. I think hyperparameters selection and data hyper-cleaning are somewhat similar, experiments on other applications are expected.
Review Point: 2. Some O(1) sample complexity methods should be compared, e.g. SUSTAIN[1] 3. The proposed method seems can not effectively solve the large-scale problem. Can the author give some results on the large-scale datasets?
Review Point: 4. The convergence analysis is based on the PL condition. An analysis on a more general case is expected. [1] Khanduri P, Zeng S, Hong M, et al. A near-optimal algorithm for stochastic bilevel optimization via double-momentum[J]. Advances in neural information processing systems, 2021, 34: 30271-30283.
==================================================

Focused review:

1. The value proposition in terms of the accuracy vs time savings is kind of weak in this line of research. First, the intended time savings is for the KD training phase, not for the inference phase. Time savings for the inference phase is more important. Second, there is always accuracy degradation when the training dataset is pruned. When the ratio of kept sample is high, the training time savings is small. To have a meaningful training time savings, the ratio of kept sample has to be much smaller, which, however, results in significant, unacceptable accuracy performance degradation.
2. In comparison with Moderate Dataset Selection (MoDS) and reshaping techniques proposed in the literature, the novelty of the proposed method is kind of limited.
3. Experiments are insufficient to demonstrate the universality of the proposed method with respect to different students with variety of learning capability. The proposed method is mainly motivated by the learning gap between the pretrained teacher and students. What would happen if the learning capability of the student is close to, or very far away from that of the pretrained teacher? If the pruned training dataset works only for certain students, but not for others, the method won't fly since finding which student fits by experiments also takes significant amounts of training time.
4. Experiments are insufficient to demonstrate its competitiveness with another technique called data distillation.

Review Point: 1. The value proposition in terms of the accuracy vs time savings is kind of weak in this line of research. First, the intended time savings is for the KD training phase, not for the inference phase. Time savings for the inference phase is more important. Second, there is always accuracy degradation when the training dataset is pruned. When the ratio of kept sample is high, the training time savings is small. To have a meaningful training time savings, the ratio of kept sample has to be much smaller, which, however, results in significant, unacceptable accuracy performance degradation.
Review Point: 2. In comparison with Moderate Dataset Selection (MoDS) and reshaping techniques proposed in the literature, the novelty of the proposed method is kind of limited.
Review Point: 3. Experiments are insufficient to demonstrate the universality of the proposed method with respect to different students with variety of learning capability. The proposed method is mainly motivated by the learning gap between the pretrained teacher and students. What would happen if the learning capability of the student is close to, or very far away from that of the pretrained teacher? If the pruned training dataset works only for certain students, but not for others, the method won't fly since finding which student fits by experiments also takes significant amounts of training time.
Review Point: 4. Experiments are insufficient to demonstrate its competitiveness with another technique called data distillation.
==================================================

Focused review:

- The training pipeline (Figure 1) is unclear. Either in the caption or the paper itself should explain what the green box means, and what the "+" and "-" refers to. Also, from the figure, it seems like ACC-Debate first uses the usual natural debate, then guided debate, followed with training the actor/critic LLMs and iteratively refine the process. I thought the ACC-Debate process only involves guided debate and then iteratively improve the training examples for better training results. Would appreciate some clarifications here.
- Equation 5 seems to be incorrect. Since the authors only observed at most 5 rounds of debate, where $t$ = 0, 1,... 4, why is the percent improvement $\frac{acc_5 - acc_0}{acc_0}$?
- Typo on the y-axis in Figure 2: improvment -> improvement
- What are some error cases of using this ACC-Debate framework, where ACC-Debate fails to improve performance, such as when the actor does not change its response despite the critic’s feedback or when the debate converges on an incorrect answer?
- The authors should conduct ablation studies to examine the effect of the number of debate rounds on final accuracy. This would help determine the optimal number of debate rounds needed for effective debate and identify potential diminishing returns on performance improvement.

Review Point: - The training pipeline (Figure 1) is unclear. Either in the caption or the paper itself should explain what the green box means, and what the "+" and "-" refers to. Also, from the figure, it seems like ACC-Debate first uses the usual natural debate, then guided debate, followed with training the actor/critic LLMs and iteratively refine the process. I thought the ACC-Debate process only involves guided debate and then iteratively improve the training examples for better training results. Would appreciate some clarifications here.
Review Point: - Equation 5 seems to be incorrect. Since the authors only observed at most 5 rounds of debate, where $t$ = 0, 1,... 4, why is the percent improvement $\frac{acc_5 - acc_0}{acc_0}$?
Review Point: - The authors should conduct ablation studies to examine the effect of the number of debate rounds on final accuracy. This would help determine the optimal number of debate rounds needed for effective debate and identify potential diminishing returns on performance improvement.
==================================================

Focused review:

1. The paper can be seen as applying an existing method (de-randomized smoothing) to a new domain (malware). Although the authors claim that it is “challenging” to adapt de-randomized smoothing for malware, I’m not convinced. The proposed method is a form of structured ablation, originally studied by Levine & Feizi (2020) for 2-d inputs with homogeneous base classifiers. The modification from 2-d to 1-d inputs and from homogeneous to heterogeneous base classifiers seems straightforward. Moreover, the proposed method has appeared in prior work in a more general form by Hammoudeh & Lowd (2023).
1. The paper claims to be “first to offer certified robustness in the realm of static detection of malware executables”. However there is prior work on this topic by Huang et al. (2023) which appeared on arXiv in January 2023. Their work considers a different threat model for malware: edit distance robustness rather than patch robustness.
1. A characteristic feature of the malware domain is that inputs vary in length. However, it’s not clear to me how the proposed classifier architecture handles this. As an example, consider a 100 KB malicious file and a classifier with a maximum input length of 2 MB. For $n$ in the range 4–20, the malicious file fits within a single block, meaning it is passed to a single base classifier, while the remaining $n - 1$ base classifiers receive padding as input. Assuming the base classifiers predict “benign” for padding, the votes are $1$ “malicious” and $n - 1$ “benign” giving a prediction of “benign”. I wonder if I’m missing something here, because it seems the classifier is guaranteed to make false negative errors on small files, which are abundant according to Figure 6.
1. I found the description of the threat model and certificate unclear. Section 3.2 states that the attacker is allowed to “modify or add any bytes in a contiguous portion”. I understand that “modify” means overwrite or replace, but it’s not clear what “add” means in this context. For instance, “add” could mean “insert” or “append”, or it may mean “increment or decrement by some amount”. The mathematical description $x’ = x + \delta$ implies the original sequence $x$ is additively perturbed by $\delta$ (which is undefined), but this seems to be at odds with the earlier description. Reading between the lines, my understanding is that the certificate covers a contiguous chunk of the original file being overwritten, which may include some bytes being appended to the end of the file. This should be precisely stated somewhere. The current definition of the certificate in Section 5 is in terms of ablated sequences – it would be helpful to translate this to the input space.
1. It’s great that the authors are planning to release the PACE dataset. However, the current description of the collection process is a bit light on detail. It would be helpful to describe how binaries were selected from the various sources. For instance, was there a preference for recent binaries? Are the binaries for a single platform (e.g., Windows x64) or multiple platforms? How do you know the binaries are benign?
Minor points:
1. Section 2 states that it’s “surprising” MalConv is still considered state-of-the-art for malware detection on raw byte sequences, given it was released in 2018. The authors attribute this to limited availability of public data. However, I think the main reason is due to difficulties in scaling more complex models (such as transformers) to very long sequences, containing upwards of a million tokens. It’s worth pointing out that the authors of MalConv have released a follow up model known as MalConv 2 (Raff et al., 2021).
1. Section 3 states that the input vector fed into the network “has to be of a fixed dimension”. This is not true in general, and I don’t believe it’s true for MalConv. I believe it is possible to support arbitrary length inputs in modern frameworks such as PyTorch by specifying `None` for the size of the dimension.
1. Section 3.1 states that models like EMBER and GBDT “can work only on feature vectors”. I’m a bit puzzled by this statement. When composed with their feature extractors, these models must be able to operate on raw binaries, otherwise they would be useless as malware detectors?
1. Section 5 states that vision-oriented ablation techniques such as masking and block ablations are infeasible for byte sequences. However I can’t see why these wouldn’t work on 1d sequences? It seems feasible to mask bytes or ablate 1d blocks.
1. Section 7 states MalConv NonNeg “has been believed as a robust model for a long time”. It would be good to include a citation for this claim. **References**
- Huang et al., “Certified robustness of learning-based static malware detectors,” arXiv:2302.01757 (2023). https://arxiv.org/abs/2302.01757
- Hammoudeh & Lowd, “Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks,” AdvML-Frontiers 2023. https://openreview.net/forum?id=NX5Nxrz6PV
- Levine & Feizi, “(De)Randomized Smoothing for Certifiable Defense against Patch Attacks,” NeurIPS 2020. https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf
- Raff et al., “Classifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection,” AAAI 2021. https://ojs.aaai.org/index.php/AAAI/article/view/17131/16938

Review Point: 1. The paper can be seen as applying an existing method (de-randomized smoothing) to a new domain (malware). Although the authors claim that it is “challenging” to adapt de-randomized smoothing for malware, I’m not convinced. The proposed method is a form of structured ablation, originally studied by Levine & Feizi (2020) for 2-d inputs with homogeneous base classifiers. The modification from 2-d to 1-d inputs and from homogeneous to heterogeneous base classifiers seems straightforward. Moreover, the proposed method has appeared in prior work in a more general form by Hammoudeh & Lowd (2023).
Review Point: 1. The paper claims to be “first to offer certified robustness in the realm of static detection of malware executables”. However there is prior work on this topic by Huang et al. (2023) which appeared on arXiv in January 2023. Their work considers a different threat model for malware: edit distance robustness rather than patch robustness.
Review Point: 1. It’s great that the authors are planning to release the PACE dataset. However, the current description of the collection process is a bit light on detail. It would be helpful to describe how binaries were selected from the various sources. For instance, was there a preference for recent binaries? Are the binaries for a single platform (e.g., Windows x64) or multiple platforms? How do you know the binaries are benign? Minor points:
Review Point: 1. Section 2 states that it’s “surprising” MalConv is still considered state-of-the-art for malware detection on raw byte sequences, given it was released in 2018. The authors attribute this to limited availability of public data. However, I think the main reason is due to difficulties in scaling more complex models (such as transformers) to very long sequences, containing upwards of a million tokens. It’s worth pointing out that the authors of MalConv have released a follow up model known as MalConv 2 (Raff et al., 2021).
Review Point: 1. Section 3 states that the input vector fed into the network “has to be of a fixed dimension”. This is not true in general, and I don’t believe it’s true for MalConv. I believe it is possible to support arbitrary length inputs in modern frameworks such as PyTorch by specifying `None` for the size of the dimension.
Review Point: 1. Section 3.1 states that models like EMBER and GBDT “can work only on feature vectors”. I’m a bit puzzled by this statement. When composed with their feature extractors, these models must be able to operate on raw binaries, otherwise they would be useless as malware detectors?
Review Point: 1. Section 5 states that vision-oriented ablation techniques such as masking and block ablations are infeasible for byte sequences. However I can’t see why these wouldn’t work on 1d sequences? It seems feasible to mask bytes or ablate 1d blocks.
Review Point: 1. Section 7 states MalConv NonNeg “has been believed as a robust model for a long time”. It would be good to include a citation for this claim. **References** - Huang et al., “Certified robustness of learning-based static malware detectors,” arXiv:2302.01757 (2023). https://arxiv.org/abs/2302.01757 - Hammoudeh & Lowd, “Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks,” AdvML-Frontiers 2023. https://openreview.net/forum?id=NX5Nxrz6PV - Levine & Feizi, “(De)Randomized Smoothing for Certifiable Defense against Patch Attacks,” NeurIPS 2020. https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf - Raff et al., “Classifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection,” AAAI 2021. https://ojs.aaai.org/index.php/AAAI/article/view/17131/16938
==================================================

Focused review:

*I did not see training/inference time and memory in the quantitative evaluation table
*The editing capability does not have a comparative evaluation. Because this is a claimed contribution, I think the authors should find baselines and demonstrate why the proposed approach is superior for interactive image editing. Some of the following references could be considered in the evaluation:
[1] Pan, Xingang, et al. "Drag your gan: Interactive point-based manipulation on the generative image manifold." ACM SIGGRAPH 2023 Conference Proceedings. 2023.
[2] Shi, Yujun, et al. "Dragdiffusion: Harnessing diffusion models for interactive point-based image editing." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
[3] Jacobson, Alec, et al. "Bounded biharmonic weights for real-time deformation." ACM Trans. Graph. 30.4 (2011): 78.
[4] Wang, Yu, et al. "Linear subspace design for real-time shape deformation." ACM Transactions on Graphics (TOG) 34.4 (2015): 1-11.
*Visualizations of the mesh are not shown when editing, and how the mesh edit propagates to the Gaussian splat. As an ablation, it would help to show why we can’t live with the edited (textured) mesh and need to map the edits to the Gaussian Splat.
*Evaluation of reconstruction quality is missing comparative qualitative examples. Because the PSNRs/SSIMs are generally quite high, it may be tough to appreciate these improvements qualitatively.

Review Point: 78. [4] Wang, Yu, et al. "Linear subspace design for real-time shape deformation." ACM Transactions on Graphics (TOG) 34.4 (2015): 1-11. *Visualizations of the mesh are not shown when editing, and how the mesh edit propagates to the Gaussian splat. As an ablation, it would help to show why we can’t live with the edited (textured) mesh and need to map the edits to the Gaussian Splat. *Evaluation of reconstruction quality is missing comparative qualitative examples. Because the PSNRs/SSIMs are generally quite high, it may be tough to appreciate these improvements qualitatively.
==================================================

Focused review:

- I think one of the main outstanding challenges in object-centric RL is how an agent can learn which are the particular objects in a game, and more importantly which are the relevant objects in a game. By introducing a predefined object extraction pipeline you basically fix an important part of an object-centric RL algorithm, and it's not said that the proposed object representation (i.e. coordinates and bounding box) is actually the best representation for an RL algorithm. So whereas this might be an interesting tool for developing and debugging object-centric methods, I'm in doubt of the value of this as a benchmark per se.
- In Figure 5, the pixel-based Deep PPO agents actually seem to outperform the object-centric ones in sample efficiency on some of the games (i.e. Asterix, Boxing and Freeway). This makes me wonder whether the current object-centric representation is actually fit for purpose. Of course the current policy that ingests the object-centric representation is pretty naive so that could be the point of the benchmark to further improve this.

Review Point: - In Figure 5, the pixel-based Deep PPO agents actually seem to outperform the object-centric ones in sample efficiency on some of the games (i.e. Asterix, Boxing and Freeway). This makes me wonder whether the current object-centric representation is actually fit for purpose. Of course the current policy that ingests the object-centric representation is pretty naive so that could be the point of the benchmark to further improve this.
==================================================

Focused review:

1.	The details of the constructed knowledge base are not provided (e.g., knowledge source, scale, length, etc.).
2.	Experiments are limited on GPT-4o, experiments on more open-source MLLMs (e.g., LLaVA) should be included.
3.	To validate the effectiveness of the proposed knowledge base, experiments should be conducted on the comparison of the proposed knowledge base with other knowledge bases.
4.	The proposed framework is similar to the traditional multimodal RAG [1], which leads to low novelty.
[1] Lin W, Chen J, Mei J, et al. Fine-grained late-interaction multi-modal retrieval for retrieval augmented visual question answering[J]. Advances in Neural Information Processing Systems, 2023, 36: 22820-22840.

Review Point: 1. The details of the constructed knowledge base are not provided (e.g., knowledge source, scale, length, etc.).
Review Point: 2. Experiments are limited on GPT-4o, experiments on more open-source MLLMs (e.g., LLaVA) should be included.
Review Point: 3. To validate the effectiveness of the proposed knowledge base, experiments should be conducted on the comparison of the proposed knowledge base with other knowledge bases.
Review Point: 4. The proposed framework is similar to the traditional multimodal RAG [1], which leads to low novelty. [1] Lin W, Chen J, Mei J, et al. Fine-grained late-interaction multi-modal retrieval for retrieval augmented visual question answering[J]. Advances in Neural Information Processing Systems, 2023, 36: 22820-22840.
==================================================

Focused review:

1) From the introduction and abstract of the paper there is an impression that the paper aims to identify influential training examples but they doesn't seem to be any experimental results on that aspect. The experimental results are mostly cumulative w.r.t. overall accuracy, runtime, etc. There are no examples that show the effectiveness of the method w.r.t. specific training instances.
2) To make the paper more clear it would be good to define what exactly “canonical” pipeline and “ data provenance” mean in the beginning of the paper. The readers need to have a clear understanding of those terms.
3) The notation `D_{tr}[v] to denote D` is a bit confusing. t \in f(D_{tr}) is confusing too since t \in D_{tr} and we see exactly t \in D_{tr} notation later in the paper. It would be good to change the notation to make it more straightforward.
4) In section 3.3 f* doesn’t seem to be defined too ?
5) Figure 3 is referenced in pages 4 and 5 and it is not explained. It’s unclear why the Compute time of Canonpipe TMC x100 is worse than TMC x10.
6) The intuitions behind modified KNN and quality metrics in section 4.1 are unclear.
7) The description of Counting Oracle is not very clear. It would be good, if possible, to describe it in a more intuitive way. It seems to be overloaded with math notations and is not straightforward to follow.

Review Point: 1) From the introduction and abstract of the paper there is an impression that the paper aims to identify influential training examples but they doesn't seem to be any experimental results on that aspect. The experimental results are mostly cumulative w.r.t. overall accuracy, runtime, etc. There are no examples that show the effectiveness of the method w.r.t. specific training instances.
Review Point: 2) To make the paper more clear it would be good to define what exactly “canonical” pipeline and “ data provenance” mean in the beginning of the paper. The readers need to have a clear understanding of those terms.
Review Point: 3) The notation `D_{tr}[v] to denote D` is a bit confusing. t \in f(D_{tr}) is confusing too since t \in D_{tr} and we see exactly t \in D_{tr} notation later in the paper. It would be good to change the notation to make it more straightforward.
Review Point: 4) In section 3.3 f* doesn’t seem to be defined too ?
Review Point: 5) Figure 3 is referenced in pages 4 and 5 and it is not explained. It’s unclear why the Compute time of Canonpipe TMC x100 is worse than TMC x10.
Review Point: 6) The intuitions behind modified KNN and quality metrics in section 4.1 are unclear.
Review Point: 7) The description of Counting Oracle is not very clear. It would be good, if possible, to describe it in a more intuitive way. It seems to be overloaded with math notations and is not straightforward to follow.
==================================================

Focused review:

1 The paper’s core contribution relies on a widely assumed mechanism—the E/I (Excitation-Inhibition) imbalance that leads to seizure-like activity. This hypothesis has been extensively validated in the literature (e.g., Sadeh & Clopath, 2021; Shao et al., 2019), yet the authors do not provide any novel insights or perspectives on it.
2 Although the model attempts to approximate a biologically realistic spiking neural network (SNN) architecture, relying solely on weakened inhibitory synapses to simulate PE lacks the complexity of actual epilepsy, which often involves a variety of neurotransmitters and molecular channels.
3 The validation relies on simplistic metrics, primarily firing rates and E/I balance under different stimuli. The study misses other crucial electrophysiological characteristics, such as neuron synchrony or spectral features.
4 While the paper introduces pulse stimulation to mimic RNS therapy for suppressing seizure activity, the tested parameter space and stimulation methods are simplistic, lacking consideration for how different frequencies might affect E/I balance long-term.
5 The authors test the model with known seizure-inducing video clips but do not clarify whether these stimuli elicit responses consistent with those of actual PE patients.

Review Point: 1 The paper’s core contribution relies on a widely assumed mechanism—the E/I (Excitation-Inhibition) imbalance that leads to seizure-like activity. This hypothesis has been extensively validated in the literature (e.g., Sadeh & Clopath, 2021; Shao et al., 2019), yet the authors do not provide any novel insights or perspectives on it.
Review Point: 2 Although the model attempts to approximate a biologically realistic spiking neural network (SNN) architecture, relying solely on weakened inhibitory synapses to simulate PE lacks the complexity of actual epilepsy, which often involves a variety of neurotransmitters and molecular channels.
Review Point: 3 The validation relies on simplistic metrics, primarily firing rates and E/I balance under different stimuli. The study misses other crucial electrophysiological characteristics, such as neuron synchrony or spectral features.
Review Point: 4 While the paper introduces pulse stimulation to mimic RNS therapy for suppressing seizure activity, the tested parameter space and stimulation methods are simplistic, lacking consideration for how different frequencies might affect E/I balance long-term.
Review Point: 5 The authors test the model with known seizure-inducing video clips but do not clarify whether these stimuli elicit responses consistent with those of actual PE patients.
==================================================

Focused review:

1. If we utilize the original dataset instead of the distilled data for model training, would the trigger remain effective? It would be better to include such experiments.
2. Can the proposed attacks evade other data distillation techniques (e.g., gradient matching based methods and distribution matching based methods)? It would further strengthen the experimental evaluation by examining the transferability of the proposed attacks.
3. In my understanding, individuals would majorly employ distilled data for training new models in scenarios such as neural architecture search and continual learning. Expanding on the implications of backdoor attacks in these applications would provide greater clarity.

Review Point: 1. If we utilize the original dataset instead of the distilled data for model training, would the trigger remain effective? It would be better to include such experiments.
Review Point: 2. Can the proposed attacks evade other data distillation techniques (e.g., gradient matching based methods and distribution matching based methods)? It would further strengthen the experimental evaluation by examining the transferability of the proposed attacks.
Review Point: 3. In my understanding, individuals would majorly employ distilled data for training new models in scenarios such as neural architecture search and continual learning. Expanding on the implications of backdoor attacks in these applications would provide greater clarity.
==================================================

Focused review:

1. A primary concern is the lack of rigorous evidence supporting the claim that frequency-domain merging avoids signal interference. Unlike data types like images or time series, frequency representations of high-dimensional embeddings lack intuitive clarity, making the necessity of this approach uncertain.
2. While the authors highlight MM-Lego’s ability to handle multiple modalities, the paper lacks experiments involving more than two modalities. This limitation leaves the model’s robustness in such scenarios untested.

Review Point: 1. A primary concern is the lack of rigorous evidence supporting the claim that frequency-domain merging avoids signal interference. Unlike data types like images or time series, frequency representations of high-dimensional embeddings lack intuitive clarity, making the necessity of this approach uncertain.
Review Point: 2. While the authors highlight MM-Lego’s ability to handle multiple modalities, the paper lacks experiments involving more than two modalities. This limitation leaves the model’s robustness in such scenarios untested.
==================================================

Focused review:

1. Notation clarification should be further improved. For example, the dimensions $I_1,\ldots, I_n$ and the matrix nuclear norm $\|\cdot\|_*$ are not clearly mentioned; and the dimensions of tensors or matrices or the underlying tensor spaces should be clearly presented at least at the beginning to avoid confusion. Minor issues about notation: equation xxx -> (xxx) and some notation could be shortened.
2. The proposed tensor U1 norm seems to be a straightforward extension of dictionary-based L1-norm for vectors/matrices. The connection in this direction was not mentioned or explained.
3. In the abstract, the authors claimed that the proposed methods can handle non-smooth changes. Unfortunately, it cannot be seen explicitly through either the proposed models or the numerical justifications. The motivation of the proposed methods could be explained in more detail. Moreover, a new tensor decomposition was claimed to be introduced in this paper, which does not seem the case since only one slice-wise tensor decomposition form (1) is adopted. Some necessary references for this tensor product are missing.
4. In the numerical experiments, it needs to describe the structure of the underlying data, i.e., whether it's sparse or low-rank in some desired transform.

Review Point: 1. Notation clarification should be further improved. For example, the dimensions $I_1,\ldots, I_n$ and the matrix nuclear norm $\|\cdot\|_*$ are not clearly mentioned; and the dimensions of tensors or matrices or the underlying tensor spaces should be clearly presented at least at the beginning to avoid confusion. Minor issues about notation: equation xxx -> (xxx) and some notation could be shortened.
Review Point: 2. The proposed tensor U1 norm seems to be a straightforward extension of dictionary-based L1-norm for vectors/matrices. The connection in this direction was not mentioned or explained.
Review Point: 3. In the abstract, the authors claimed that the proposed methods can handle non-smooth changes. Unfortunately, it cannot be seen explicitly through either the proposed models or the numerical justifications. The motivation of the proposed methods could be explained in more detail. Moreover, a new tensor decomposition was claimed to be introduced in this paper, which does not seem the case since only one slice-wise tensor decomposition form (1) is adopted. Some necessary references for this tensor product are missing.
Review Point: 4. In the numerical experiments, it needs to describe the structure of the underlying data, i.e., whether it's sparse or low-rank in some desired transform.
==================================================

Focused review:

1. The idea is not that novel, compared with BEIT-3 and VLMo, which also introduces different modality expert structures, although this work makes some changes to make it work in the era of LLM.
2. The VQA/Image Caption model, Visual Grounding model and Chat model are three different models, I am wondering how the performance can be if all these models are a unified one? Since GPT-4V may be a unified one.
3. Although I appreciate the excellent performance it achieves, the visual backbone is ViT-e, and the input resolution is 490 * 490, also the parameter size of LLM doubles, which makes the comparason a little hard.

Review Point: 1. The idea is not that novel, compared with BEIT-3 and VLMo, which also introduces different modality expert structures, although this work makes some changes to make it work in the era of LLM.
Review Point: 2. The VQA/Image Caption model, Visual Grounding model and Chat model are three different models, I am wondering how the performance can be if all these models are a unified one? Since GPT-4V may be a unified one.
Review Point: 3. Although I appreciate the excellent performance it achieves, the visual backbone is ViT-e, and the input resolution is 490 * 490, also the parameter size of LLM doubles, which makes the comparason a little hard.
==================================================

Focused review:

My primary concern is presentation quality. Improved clarity can significantly benefit readability of this paper, as well as my understanding of the main method.
1. Typo/abbreviation mistakes: line 3 of the abstract "be done ne(?) with Reinforcement Learning", start of the first paragraph of introduction, line 42 "in effect" (?), single quotes in line 43, "it dynamics" -> "its dynamics" in line 98, and so on.
2. What's the superscript -1 on line 271?
3. Figure readability is sadly discounted by low resolution, small font size, and the lack of in-figure legends. Personally I find it hard to parse the results without clear legend names and matching color coding, even with captions.
4. Undefined H3.1 and H3.2 in lines 337-338
5. $\beta_1$ and $\beta_2$ in Figure 2 captions seem out of blue. Are they defined anywhere in the main text? Why do they imply "shared" and "anogostic"? Perhaps a table comparing configurations of parallel runs?
6. I appreciate the intuition, but I struggle to understand the formulation of the proposed CAM in sec 3.2. I think neither eq 4 nor the relative ambiguity of a language are CAM, but I cannot find exactly how CAM is computed in the main paper.

Review Point: 3. Figure readability is sadly discounted by low resolution, small font size, and the lack of in-figure legends. Personally I find it hard to parse the results without clear legend names and matching color coding, even with captions.
Review Point: 4. Undefined H3.1 and H3.2 in lines 337-338 5. $\beta_1$ and $\beta_2$ in Figure 2 captions seem out of blue. Are they defined anywhere in the main text? Why do they imply "shared" and "anogostic"? Perhaps a table comparing configurations of parallel runs?
Review Point: 6. I appreciate the intuition, but I struggle to understand the formulation of the proposed CAM in sec 3.2. I think neither eq 4 nor the relative ambiguity of a language are CAM, but I cannot find exactly how CAM is computed in the main paper.
==================================================

Focused review:

1. The paper lacks comprehensiveness in terms of attacks and defenses. Stronger attacks such as white-box attacks aren't considered. Common defenses such as refusal training aren't included.
2. Lacks computational cost comparisons between the proposed method and the baselines in the paper.
3. The claim of maintaining "quality of responses" needs more rigorous evaluation (on capability benchmarks) - the quality scoring method (0-5 scale using GPT-4) lacks detailed explanation.

Review Point: 1. The paper lacks comprehensiveness in terms of attacks and defenses. Stronger attacks such as white-box attacks aren't considered. Common defenses such as refusal training aren't included.
Review Point: 2. Lacks computational cost comparisons between the proposed method and the baselines in the paper.
Review Point: 3. The claim of maintaining "quality of responses" needs more rigorous evaluation (on capability benchmarks) - the quality scoring method (0-5 scale using GPT-4) lacks detailed explanation.
==================================================

Focused review:

* It’s unclear whether the similarities are calculated between two latent spaces (as suggested in formula 3, line 094) or between two individual representations (a single point), as written in line 095. Clarifying this distinction (and how each metric is adapted accordingly) could improve the reader's understanding. It seems to be between two latent spaces, but in Figure 2 (left), the metric is calculated between two points.
* When evaluating the text architectures:
* Could you clarify the choice to test the same model with 25 different seeds, rather than exploring architectural variations like RoBERTa or ALBERT, which are available pretrained on Hugging Face? Including a wider variety of architectures might enhance the robustness and generalizability of the findings.
* It would be helpful to either move the explanation of the choice to use the CLS token to the main manuscript or to reference the appendix. Additionally, you might consider testing alternative aggregation modalities, such as the mean or the last token, since these can yield different representations and may impact the results.
* The vision task may benefit from broader testing since only ImageNet with 100 subclasses is used. Expanding the evaluation to include the full ImageNet dataset or other widely-used datasets, such as ImageNet-1k, CIFAR-10, or CIFAR-100, could provide a more comprehensive assessment.
* The analysis could benefit from stronger takeaways. For instance, in lines 438-439, you suggest that the choice of measures should be task-specific, and in lines 453-455, you advocate for the development of best practices for using similarity metrics. However, these insights feel somewhat broad, and more concrete guidance could help readers understand how to apply the benchmark effectively.
* In line 413, it’s mentioned that some domain-specific trends can be identified, yet later (line 424), you note that no single measure consistently outperforms others across all tests, even within a single dataset. The first claim—that some metrics appear more suitable for specific modalities—is interesting and would benefit from a more in-depth discussion. Also, adding a summary table of these findings of the paper could help readers better understand the details.

Review Point: * It’s unclear whether the similarities are calculated between two latent spaces (as suggested in formula 3, line 094) or between two individual representations (a single point), as written in line 095. Clarifying this distinction (and how each metric is adapted accordingly) could improve the reader's understanding. It seems to be between two latent spaces, but in Figure 2 (left), the metric is calculated between two points.
Review Point: * Could you clarify the choice to test the same model with 25 different seeds, rather than exploring architectural variations like RoBERTa or ALBERT, which are available pretrained on Hugging Face? Including a wider variety of architectures might enhance the robustness and generalizability of the findings.
Review Point: * It would be helpful to either move the explanation of the choice to use the CLS token to the main manuscript or to reference the appendix. Additionally, you might consider testing alternative aggregation modalities, such as the mean or the last token, since these can yield different representations and may impact the results.
Review Point: * The vision task may benefit from broader testing since only ImageNet with 100 subclasses is used. Expanding the evaluation to include the full ImageNet dataset or other widely-used datasets, such as ImageNet-1k, CIFAR-10, or CIFAR-100, could provide a more comprehensive assessment.
Review Point: * The analysis could benefit from stronger takeaways. For instance, in lines 438-439, you suggest that the choice of measures should be task-specific, and in lines 453-455, you advocate for the development of best practices for using similarity metrics. However, these insights feel somewhat broad, and more concrete guidance could help readers understand how to apply the benchmark effectively.
Review Point: * In line 413, it’s mentioned that some domain-specific trends can be identified, yet later (line 424), you note that no single measure consistently outperforms others across all tests, even within a single dataset. The first claim—that some metrics appear more suitable for specific modalities—is interesting and would benefit from a more in-depth discussion. Also, adding a summary table of these findings of the paper could help readers better understand the details.
==================================================

Focused review:

There are two major issues.
1. The theoretical proof seems incomplete. I failed to find proof of Theorem 3. I can only find a proof sketch and some supportive lemmas. As a theoretical paper, this harms the soundness.
2. If my understanding is correct, during the training and evaluation, the location to compute the parity in every sequence is the same, e.g., we always use the location 1,2, and 5 to compute the parity. I think it is a stronger and more reasonable result to make the location not fixed. For example, let $k=3$, then I can learn the sparse parity with any three locations. Especially during the testing, I can compute parity on a certain tuple of three locations that never appear during the training. This can make your work a stronger CoT inference rather than more of a training analysis.

Review Point: 1. The theoretical proof seems incomplete. I failed to find proof of Theorem 3. I can only find a proof sketch and some supportive lemmas. As a theoretical paper, this harms the soundness.
==================================================

Focused review:

1. In section 3.2, the paper mentions, “We initially collect thousands of system messages from online logs, and filter out duplicate and noisy data based on heuristic rules and clustering.” more explanation should be provided on the dataset construction process. Examples of " annotation guidelines designed by experts" and demographics of the 21 trained data annotators may be provided in the appendix to support this.
2. In section 3.2, the paper mentions that each system message is related to 2-3 system constraints, hence presenting a moderate level of complexity. However, compared to page-long system prompts often used from big techs, 2-3 system constraints look a bit short.
3. The paper lacks a detailed analysis on the performance of different models. Figure 3 breaks down the dataset into different domains, but this does not seem to be reflected in the evaluation.
4. Another widely used application for system messages is safety. Chat applications often want the model to refuse to respond to some user instructions. The benchmark could have been better by including such examples.
5. Analysis between attention distribution and performance is very interesting. However, the paper only presents if for three models. It would have been insightful if the results for other models were also included in the appendix.
6. An error analysis on when models fail can be added to provide more insight on improving models in system message following.

Review Point: 1. In section 3.2, the paper mentions, “We initially collect thousands of system messages from online logs, and filter out duplicate and noisy data based on heuristic rules and clustering.” more explanation should be provided on the dataset construction process. Examples of " annotation guidelines designed by experts" and demographics of the 21 trained data annotators may be provided in the appendix to support this.
Review Point: 2. In section 3.2, the paper mentions that each system message is related to 2-3 system constraints, hence presenting a moderate level of complexity. However, compared to page-long system prompts often used from big techs, 2-3 system constraints look a bit short.
Review Point: 3. The paper lacks a detailed analysis on the performance of different models. Figure 3 breaks down the dataset into different domains, but this does not seem to be reflected in the evaluation.
Review Point: 4. Another widely used application for system messages is safety. Chat applications often want the model to refuse to respond to some user instructions. The benchmark could have been better by including such examples.
Review Point: 5. Analysis between attention distribution and performance is very interesting. However, the paper only presents if for three models. It would have been insightful if the results for other models were also included in the appendix.
Review Point: 6. An error analysis on when models fail can be added to provide more insight on improving models in system message following.
==================================================

Focused review:

:
The support samples of "difficult task" is selected by fixing the query samples which means the difficulty of each task is highly correlated with the fixed query data. The hypothesis is somewhat unreasonable, since the FSL aims to adapt to the whole new class not the fixed query samples. The reviewer supposes that the tasks in the HARD-META-DATASET++ may not be a reliable estimator in the test-time in FSL.
The proposed method can be seen as an extension of the paper[1], since the paper[1] have already proposed that FSL are extremely sensitive to the data used for adaptation and used a greedy algorithm to find those difficult tasks. Although the proposed method is more efficient, the novelty is somewhat limited.
3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000.
As for questions, I would like to ask:
1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD a better criterion for evaluation?
2 How to use the "difficult task" in the training phase in FSL? Can the "difficult task" in base classes help the model improve the generality to novel classes?

Review Point: 3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000. As for questions, I would like to ask:
Review Point: 1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD a better criterion for evaluation?
Review Point: 2 How to use the "difficult task" in the training phase in FSL? Can the "difficult task" in base classes help the model improve the generality to novel classes?
==================================================

Focused review:

- The paper is lacking some deeper discussion on the experimental results of using their method. E.g., why post-editing has no significant effect and why the manual condition has the lowest quality, which are counterintuitive. Since the manual condition's effect is the worst, then the quality of the condition combined with post-editing should be reduced with a high probability.
- I don't think 20 articles are enough to analyze the actual effects of different types, and instead of increasing the number of participants, consider increasing the number of different articles. As a straightforward example, choosing the best of the three outputs generated by the LLM, the results of 10 participants would not vary excessively.
- The experiment is not sufficient. I am curious whether the accuracy of perspectives selection will have a significant impact on the quality of generated titles. If there is a task difficulty label, whether the effects of different types under different difficulty levels are considered, and whether the number of different task difficulty levels is balanced.

Review Point: - The paper is lacking some deeper discussion on the experimental results of using their method. E.g., why post-editing has no significant effect and why the manual condition has the lowest quality, which are counterintuitive. Since the manual condition's effect is the worst, then the quality of the condition combined with post-editing should be reduced with a high probability.
Review Point: - I don't think 20 articles are enough to analyze the actual effects of different types, and instead of increasing the number of participants, consider increasing the number of different articles. As a straightforward example, choosing the best of the three outputs generated by the LLM, the results of 10 participants would not vary excessively.
Review Point: - The experiment is not sufficient. I am curious whether the accuracy of perspectives selection will have a significant impact on the quality of generated titles. If there is a task difficulty label, whether the effects of different types under different difficulty levels are considered, and whether the number of different task difficulty levels is balanced.
==================================================

Focused review:

1. The encoder uses multiple original graphs as input; however, the rationale for connecting identical vertices across these graphs with edges is unclear. Table 5 only presents comparison results for the MIS and MC problems, why are the results for MCut not included? The description of the Drop Value is unclear, it would be better to provide a more detailed comparison of the results. Additionally, the drop value for K=2 shows a significant difference only in the context of MIS.
2. There is a lack of an ablation study on the design of the total loss function. The total loss function includes includes objective quality, constraint satisfaction, and solution diversity. It would be useful to analyze the results when the loss function includes only one of these components, such as solely objective quality, as well as the combination of objective quality and constraint satisfaction, and the combination of objective quality and solution diversity. This comparative analysis could provide insights into the impact of each loss component on the overall performance.
3. The result comparisons for each CO problem contain too few types of benchmarks. MC and MIS are closely related problems, and the instances tested in the experiments should remain consistent. Additionally, it would be beneficial to include more results for RB graphs and ER graphs. For the Max Cut problem, providing more results for BA graphs would also be helpful. Furthermore, testing the proposed algorithm on DIMACS and COLOR02 instances would demonstrate its generalization capabilities.
4. It would be beneficial to explicitly state the limitations of the proposed approach, for example, the scalability issues.

Review Point: 1. The encoder uses multiple original graphs as input; however, the rationale for connecting identical vertices across these graphs with edges is unclear. Table 5 only presents comparison results for the MIS and MC problems, why are the results for MCut not included? The description of the Drop Value is unclear, it would be better to provide a more detailed comparison of the results. Additionally, the drop value for K=2 shows a significant difference only in the context of MIS.
Review Point: 2. There is a lack of an ablation study on the design of the total loss function. The total loss function includes includes objective quality, constraint satisfaction, and solution diversity. It would be useful to analyze the results when the loss function includes only one of these components, such as solely objective quality, as well as the combination of objective quality and constraint satisfaction, and the combination of objective quality and solution diversity. This comparative analysis could provide insights into the impact of each loss component on the overall performance.
Review Point: 3. The result comparisons for each CO problem contain too few types of benchmarks. MC and MIS are closely related problems, and the instances tested in the experiments should remain consistent. Additionally, it would be beneficial to include more results for RB graphs and ER graphs. For the Max Cut problem, providing more results for BA graphs would also be helpful. Furthermore, testing the proposed algorithm on DIMACS and COLOR02 instances would demonstrate its generalization capabilities.
Review Point: 4. It would be beneficial to explicitly state the limitations of the proposed approach, for example, the scalability issues.
==================================================

Focused review:

- This component, though effective, adds additional steps to the inference phase, potentially affecting efficiency in real-time applications.
- The paper could benefit from a more detailed discussion of the limitations of the proposed methods and potential areas for future work, such as the impact of training data on performance.
- Further discussion on how the pruning limits affect accuracy vs. computation trade-off would add valuable insight.

Review Point: - This component, though effective, adds additional steps to the inference phase, potentially affecting efficiency in real-time applications.
Review Point: - The paper could benefit from a more detailed discussion of the limitations of the proposed methods and potential areas for future work, such as the impact of training data on performance.
Review Point: - Further discussion on how the pruning limits affect accuracy vs. computation trade-off would add valuable insight.
==================================================

Focused review:

1. The writing needs to be improved.
(1) I recommend adding a schematic diagram to demonstrate the method.
(2) In Eq.2, the writing style does not align with the habit. Generally speaking, a smaller loss indicates a better performance. However, the authors mention, "the effectiveness goal may be better achieved when the loss term L_e is larger". And same problem occurs in Eq.3.
(3) The figures and tables in the Appendix should be numbered individually.
2. I have some concerns about the method.
(1) For a concept that we want to erase, the method needs a prompt distribution. The performance is dependent on it because the alignment is conducted using the sampled prompts from it. However, the authors didn't discuss how we can obtain a proper distribution estimation. There is no discussion about the results under different training datasets and the visualization of the embeddings of the gathered prompts. I cannot determine how reliable the method is.
(2) In practical applications, the prompts users give are complex. There are many easy or short prompts and also implicit or long ones. For a very long prompt, there may be only one unsafe word. Under this condition, can the method still stop its generation? It's also a question of how reliable this method is. I think it can be addressed by analyzing the embedding patterns of the text encoder or conducting experiments on carefully categorized prompts.
(3) From Fig.1, the images generated by the unsafe prompts in this method are unmeaningful. Why not align these unsafe prompts to the safe ones?
3. The concerns about the experiments.
(1) I think the metric LPIPS may not be suitable. It is usually used in image quality assessment. CLIP-score is more reasonable.
(2) Can the method defend against the white-box attacks such as Prompt4Debugging?
(3) I recommend the authors show the results of the original models as well.
(4) Can the method be applied to other concepts? Such as shocking, bloody, objects and painting styles.

Review Point: 1. The writing needs to be improved. (1) I recommend adding a schematic diagram to demonstrate the method. (2) In Eq.2, the writing style does not align with the habit. Generally speaking, a smaller loss indicates a better performance. However, the authors mention, "the effectiveness goal may be better achieved when the loss term L_e is larger". And same problem occurs in Eq.3. (3) The figures and tables in the Appendix should be numbered individually.
Review Point: 3. The concerns about the experiments. (1) I think the metric LPIPS may not be suitable. It is usually used in image quality assessment. CLIP-score is more reasonable. (2) Can the method defend against the white-box attacks such as Prompt4Debugging? (3) I recommend the authors show the results of the original models as well. (4) Can the method be applied to other concepts? Such as shocking, bloody, objects and painting styles.
==================================================

Focused review:

1. Some of the theoretical proofs hinge on idealized assumptions (e.g., independence and Gaussian distribution in certain proofs). These may not fully apply in practical, real-world datasets.
2. The "accumulate-subsample" paradigm offers a practical perspective on fixed compute budgets but is tested under simplified conditions. Additional real-world constraints (e.g., dynamic memory handling) are not considered in depth.
3. While language models are likely the most relevant generative model for readers, the theoretical analysis does not extend to this model type and is instead focused on Gaussian and kernel density estimation models. Additionally, the size of the language model used in this study is quite small. While contemporary models like Llama range from 7B to 70B parameters, the paper’s experiments are limited to Gemma2 with only 2B parameters.

Review Point: 1. Some of the theoretical proofs hinge on idealized assumptions (e.g., independence and Gaussian distribution in certain proofs). These may not fully apply in practical, real-world datasets.
Review Point: 2. The "accumulate-subsample" paradigm offers a practical perspective on fixed compute budgets but is tested under simplified conditions. Additional real-world constraints (e.g., dynamic memory handling) are not considered in depth.
Review Point: 3. While language models are likely the most relevant generative model for readers, the theoretical analysis does not extend to this model type and is instead focused on Gaussian and kernel density estimation models. Additionally, the size of the language model used in this study is quite small. While contemporary models like Llama range from 7B to 70B parameters, the paper’s experiments are limited to Gemma2 with only 2B parameters.
==================================================

Focused review:

Weakness:
The necessity and complexity of using PLMs are not clear, as 1) the total amount of data generated is small, in the range of hundreds. 2) significant human review is still needed up and down the pipeline.
The size of DR SPIDER (The Reviewer's impression is 1k) is much smaller than SPIDER (10K questions).
Detailed suggestions:
Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics?
Section 3.2 paragraph 2: better explain what is "naturally occuring tables and columns".
Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting?
Section 4. Upper-case of common model names, "Bert-large" -> "BERT-large" etc.

Review Point: 1) the total amount of data generated is small, in the range of hundreds.
==================================================

Focused review:

1. In the Related Work Section, the author mentioned that the difference between SADI and these "fixed steering vector" works is that SADI takes "input semantics" into account. However, according to Algorithm 1, SADI uses the steering vectors obtained by the mean difference of activation of all positive and negative samples in the test set, which is also "fixed" to some extent. Besides, in the paper, the author said “CAA uses the mean difference in the activations at the position of the answer letter between all the positive and negative prompts to construct a fixed steering vector to shift activations.” From this angle, the changes made by SADI are actually very small. **So what is the essential difference between SADI and these related works? Why is SADI effective?**
2. For SADI, what is the relationship between the dataset used in "Difference Extraction" step and the dataset used in "Adaptive Steering" step? Are they from datasets on the same task? Or from datasets on different tasks? Or from the same dataset? What is the rationale for doing so? How to explain it? I think these questions concern the effectiveness of SADI.
3. This paper lacks guidance on selection of hyperparameters. SADI introduces the hyperparameters, but there seems no way to perceive which hyperparameters is optimal when solving a new dataset from Experiment section. Without such a principle, SADI may be left in the shade compared with similar methods.
4. No prominent improvement with SADI using SFT. In Table 1, SADI+SFT only performs 90.97 over SFT by 0.06. The author needs add more reasons why this issue occurs in the paper.

Review Point: 3. This paper lacks guidance on selection of hyperparameters. SADI introduces the hyperparameters, but there seems no way to perceive which hyperparameters is optimal when solving a new dataset from Experiment section. Without such a principle, SADI may be left in the shade compared with similar methods.
Review Point: 4. No prominent improvement with SADI using SFT. In Table 1, SADI+SFT only performs 90.97 over SFT by 0.06. The author needs add more reasons why this issue occurs in the paper.
==================================================

Focused review:

Weakness: 1. Their pretraining module simply combines the sequence pretrained model and the 3-D structure-aware pre-trained model. No novelty on the loss function can be viewed. 2. Two reasons mentioned for using factorized sparse tuning for this prediction task are not convincing. The sentence ‘while COVID-19 related proteins usually contain more than 1,000 amino acids’ has no strong connection with the studied problem. 3. From the result, the framework seems just introduce some training strategies based on ESM-1B, and there is no design in biology for the thermostability prediction task. Besides, no significant improvement in results is observed. 4. The description of biology in this paper is superficial, and the study lacks interpretability analysis.

Review Point: 1. Their pretraining module simply combines the sequence pretrained model and the 3-D structure-aware pre-trained model. No novelty on the loss function can be viewed.
Review Point: 2. Two reasons mentioned for using factorized sparse tuning for this prediction task are not convincing. The sentence ‘while COVID-19 related proteins usually contain more than 1,000 amino acids’ has no strong connection with the studied problem.
Review Point: 3. From the result, the framework seems just introduce some training strategies based on ESM-1B, and there is no design in biology for the thermostability prediction task. Besides, no significant improvement in results is observed.
Review Point: 4. The description of biology in this paper is superficial, and the study lacks interpretability analysis.
==================================================

Focused review:

* I find the novelty and contributions of this work to be limited, given that the main objective, formal guarantees, and algorithm are very similar to [1]. Moreover, the algorithm operates under the assumption of full client participation, a condition that may not always align with reality in FL settings.
* The proposed problem and guarantees rely on the assumption of binary target and attribute/group variables, which is a restrictive assumption. This raises questions about the actual utility of this approach and what are its guarantees in more realistic scenarios (e.g., multiple sensitive groups and multiclass problems). I note that extending the work to multiclass problems is also identified by the authors, but there are existing works that address multiple attributes for these fairness metrics, e.g., [3].
* The paper misses discussion and comparison to other works proposing the same idea -- i.e., how to optimize a fairness metric and produce results akin to centralized ML using the local information from clients (e.g., [2] and [4]). Also, while the authors briefly mention [5], [4] a more explicit discussion of these proposed method's conceptual differences would benefit the paper.
* The experimental section requires enhancements: (1) the paper performs experiments using only two datasets, (2) important experimental details are missing, (e.g., the number of clients used in the experiments, standard deviation for each result, Dirichlet distribution parameter values that were explored, what is the sensitive group for each dataset etc.), (3) the comparison to AFL which optimizes for client-fairness (i.e. a different fairness concept in FL) should be justified.
[1] Puheng Li, James Zou, and Linjun Zhang. Fairee: fair classification with finite-sample and distribution-free guarantee. In The Eleventh International Conference on Learning Representations, 2022.
[2] Papadaki, A., Martinez, N., Bertran, M., Sapiro, G., and Rodrigues, M. (2022). Minimax demographic group fairness in federated learning.
[3] Y. Zeng, H. Chen, and K. Lee (2021). Improving fairness via federated learning.
[4] Hu, S., Wu, Z. S., and Smith, V. (2022). Fair federated learning via bounded group loss.
[5] Yahya Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and A. Avestimehr. Fairfed: Enabling
group fairness in federated learning. Proceedings of the AAAI Conference on Artificial Intelligence, 37:7494–7502, 06 2023

Review Point: * I find the novelty and contributions of this work to be limited, given that the main objective, formal guarantees, and algorithm are very similar to [1]. Moreover, the algorithm operates under the assumption of full client participation, a condition that may not always align with reality in FL settings.
Review Point: * The proposed problem and guarantees rely on the assumption of binary target and attribute/group variables, which is a restrictive assumption. This raises questions about the actual utility of this approach and what are its guarantees in more realistic scenarios (e.g., multiple sensitive groups and multiclass problems). I note that extending the work to multiclass problems is also identified by the authors, but there are existing works that address multiple attributes for these fairness metrics, e.g., [3].
Review Point: * The paper misses discussion and comparison to other works proposing the same idea -- i.e., how to optimize a fairness metric and produce results akin to centralized ML using the local information from clients (e.g., [2] and [4]). Also, while the authors briefly mention [5], [4] a more explicit discussion of these proposed method's conceptual differences would benefit the paper.
==================================================

Focused review:

1. The paper primarily integrates existing methods to create a new approach rather than developing a novel method from the ground up.
2. The computational complexity of the top-kk operation is not addressed. Top-k selection can demand significant computational resources and time, which may render it impractical for client devices, particularly when dealing with large model parameters. This issue is especially relevant given the limited computational power of clients compared to a central server.
3. The distinction between Sparse-ProxSkip-Local and Sparse-ProxSkip is unclear, as both appear to follow the same steps in Algorithm 1. Additional clarification is necessary to outline any differences between these methods.
4. More explanation is required regarding the control variables $h_i$. Specifically, in Sections 3.2 and 3.3, $h_i$ appears as a loss function, while in Section 3.4, it is treated as a variable. It is crucial to clarify the role of $h_i$ in the algorithm, along with the significance of the criterion $\sum_i h_i=0$. It is important to figure out why using $h_i$ in the algorithm and what does $\sum_i h_i=0$ mean.
5. What challenges does the nonconvex setting present compared to existing convex methods, and how are these challenges addressed?

Review Point: 1. The paper primarily integrates existing methods to create a new approach rather than developing a novel method from the ground up.
Review Point: 2. The computational complexity of the top-kk operation is not addressed. Top-k selection can demand significant computational resources and time, which may render it impractical for client devices, particularly when dealing with large model parameters. This issue is especially relevant given the limited computational power of clients compared to a central server.
Review Point: 3. The distinction between Sparse-ProxSkip-Local and Sparse-ProxSkip is unclear, as both appear to follow the same steps in Algorithm 1. Additional clarification is necessary to outline any differences between these methods.
Review Point: 4. More explanation is required regarding the control variables $h_i$. Specifically, in Sections 3.2 and 3.3, $h_i$ appears as a loss function, while in Section 3.4, it is treated as a variable. It is crucial to clarify the role of $h_i$ in the algorithm, along with the significance of the criterion $\sum_i h_i=0$. It is important to figure out why using $h_i$ in the algorithm and what does $\sum_i h_i=0$ mean.
Review Point: 5. What challenges does the nonconvex setting present compared to existing convex methods, and how are these challenges addressed?
==================================================

Focused review:

While the paper presents a compelling and innovative approach to solving data science tasks using a LLM agent, there are several areas where the work could be improved. Below is a detailed assessment of the weaknesses, with a specific focus on the experimental setup and the lack of token usage comparison.
1. One significant weakness in the experimental setup is the lack of a detailed comparison of token usage. The paper does not provide information on how many tokens are required for different tasks and datasets, nor does it compare the token consumption of Data Interpreter with other methods or baselines.
2. The authors should conduct a thorough analysis of token consumption for different tasks and methods.  Provide a comparative analysis of token usage between Data Interpreter and existing methods or baselines. This will help readers understand the efficiency and resource requirements of Data Interpreter relative to other solutions.
3. By addressing the lack of token usage comparison, the authors can provide a more comprehensive and insightful evaluation of Data Interpreter, making the paper more robust and valuable to the research community.

Review Point: 1. One significant weakness in the experimental setup is the lack of a detailed comparison of token usage. The paper does not provide information on how many tokens are required for different tasks and datasets, nor does it compare the token consumption of Data Interpreter with other methods or baselines.
Review Point: 2. The authors should conduct a thorough analysis of token consumption for different tasks and methods. Provide a comparative analysis of token usage between Data Interpreter and existing methods or baselines. This will help readers understand the efficiency and resource requirements of Data Interpreter relative to other solutions.
Review Point: 3. By addressing the lack of token usage comparison, the authors can provide a more comprehensive and insightful evaluation of Data Interpreter, making the paper more robust and valuable to the research community.
==================================================

Focused review:

I believe that the community will find the findings and observations of this paper interesting. However, I identified a few areas where the paper could improve substantially:
- Omission of important details from introduction: The paper almost exclusively studies a specific approximation of the input space region count, which measures the number of prediction changes along *a line that connects two points from the training data*. However, this detail is not mentioned in the introduction, but only much later. Furthermore, many readers, while going through the paper for the first time, might confuse the introduced measure with the number of linear regions of the neural network. While this is clarified later in the paper (pg. 3), a short explanation in the introduction would be helpful.
- Weak/misleading section on "Motivation" (Section 3): I found Section 3 to be entirely misleading. The norm and margin quantities considered have **no reason** to be correlated with generalization for a ResNet18. First, they are not properly normalised, which is acknowledged later, so I do not see the reason for considering them in the first place. Second, there is no clear understanding that SGD on a ResNet will necessarily increase a specific notion of margin or minimise a specific norm. If someone wants to assess such connections, they should probably focus on simpler models (such as homogeneous neural networks) and control for many confounders. I understand and agree with the general point of this section (i.e., that accurate generalization measures based on the parameters of a "practical" network may be challenging to find in practice), but I found the motivating experiments problematic. I insist on this, since this section is the starting point of the paper and might mislead many readers. I would suggest being more precise in this section. Further concrete comments on this section: 1) Equation in line 172 is missing the distribution with respect to the expectation is taken. This is crucial, as it is not clear whether it applies to train or test data (it should be train). 2) It would be good to define the input-space margin which you mention in line 188 for the first time.
- Insufficient theoretical link to generalization: While I understand this is mainly an experimental paper, I was disappointed that there is no discussion on how the proposed measure can perhaps be related to improved generalization. For example, there is no mention of the self-evident property that a very small region count is undesirable (for region count equal to 1, we obtain a trivial predictor). An example of a satisfying result would be that (S)GD biases the model to implicitly minimise $R(\theta)$ under the constraint that all the train points are classified correctly together with perhaps more constraints from the hyperparameters (akin to results that exists for gradient descent on homogeneous neural networks and margin maximization). Should we hope to prove such a result? Do you believe that such result could be true? While this point alone is not brought up to dissuade acceptance of the paper, I would appreciate any thoughts the authors have on this.

Review Point: - Omission of important details from introduction: The paper almost exclusively studies a specific approximation of the input space region count, which measures the number of prediction changes along *a line that connects two points from the training data*. However, this detail is not mentioned in the introduction, but only much later. Furthermore, many readers, while going through the paper for the first time, might confuse the introduced measure with the number of linear regions of the neural network. While this is clarified later in the paper (pg. 3), a short explanation in the introduction would be helpful.
Review Point: 1) Equation in line 172 is missing the distribution with respect to the expectation is taken. This is crucial, as it is not clear whether it applies to train or test data (it should be train).
Review Point: 2) It would be good to define the input-space margin which you mention in line 188 for the first time.
==================================================

Focused review:

1. The experiment seems not sufficient to prove the claim "Sliced-Wasserstein is all you need for common learning tasks". Some common learning tasks like image classification and object detection is not tested.
2. Larger dataset for experiment is needed. Especially, toy datasets and MNIST images, which is a pretty small and easy learning task.
3. How SWD is used during the training is not very clear, a systematic model framework or a detailed algorithm might be helpful.
4. The compared methods in the experiment only has abbreviation with no explanation.
5. How SWD is better is not clearly reflected in the experiments.
6. There are so many important things were discussed in the Appendix due to the page limit.

Review Point: 1. The experiment seems not sufficient to prove the claim "Sliced-Wasserstein is all you need for common learning tasks". Some common learning tasks like image classification and object detection is not tested.
Review Point: 2. Larger dataset for experiment is needed. Especially, toy datasets and MNIST images, which is a pretty small and easy learning task.
Review Point: 3. How SWD is used during the training is not very clear, a systematic model framework or a detailed algorithm might be helpful.
Review Point: 4. The compared methods in the experiment only has abbreviation with no explanation.
Review Point: 5. How SWD is better is not clearly reflected in the experiments.
Review Point: 6. There are so many important things were discussed in the Appendix due to the page limit.
==================================================

Focused review:

- What is the fundamental difference between the proposed method with a model trained with UNet using latent-space? If there is no diffusion process and the model is doing x0 prediction, why it is still called diffusion method, especially when the authors proposed a discriminator version, does it just fall back to a simple UNet-based method?
- If the authors claim the powerful prediction quality can come from the strong prior of the pretrained model, but the currently proposed detail preserver, is just learning the reconstruction, and can be lazily learned via the skip connection within the UNet structure, it does not seem to preserve the original generator capability. The reviewer believe after longer training or large batch size training, the performance will degrade more. It is interesting that the detail preserver can enhance the detail by a large margin, given the reconstruction can be easily learned by the residual connection in each layer.
- If the authors claim it can be served as a visual foundation model, it needs to show more capability of generalization or potentials of other tasks, either through finetuning or adaptation.
- The qualitative results seem still not strong enough compared with other baselines, and are very close to Marigold.

Review Point: - What is the fundamental difference between the proposed method with a model trained with UNet using latent-space? If there is no diffusion process and the model is doing x0 prediction, why it is still called diffusion method, especially when the authors proposed a discriminator version, does it just fall back to a simple UNet-based method?
Review Point: - If the authors claim the powerful prediction quality can come from the strong prior of the pretrained model, but the currently proposed detail preserver, is just learning the reconstruction, and can be lazily learned via the skip connection within the UNet structure, it does not seem to preserve the original generator capability. The reviewer believe after longer training or large batch size training, the performance will degrade more. It is interesting that the detail preserver can enhance the detail by a large margin, given the reconstruction can be easily learned by the residual connection in each layer.
Review Point: - If the authors claim it can be served as a visual foundation model, it needs to show more capability of generalization or potentials of other tasks, either through finetuning or adaptation.
Review Point: - The qualitative results seem still not strong enough compared with other baselines, and are very close to Marigold.
==================================================

Focused review:

1. Limited analysis of predictor architecture and training choices' effects on distribution shift detection. Understanding the method's robustness across different model choices would strengthen the results.
2. While the method effectively identifies OOD samples, the paper provides limited guidance on what to do with these flagged sequences beyond excluding them. The practical impact would be enhanced by discussing mitigation strategies such as active learning, model retraining, or ways to incorporate OOD scores into exploration.
3. The theoretical foundations could benefit from deeper analysis, particularly regarding how classifier architecture affects density ratio estimation accuracy and potential bounds on detection performance under various distribution shift scenarios.

Review Point: 1. Limited analysis of predictor architecture and training choices' effects on distribution shift detection. Understanding the method's robustness across different model choices would strengthen the results.
Review Point: 2. While the method effectively identifies OOD samples, the paper provides limited guidance on what to do with these flagged sequences beyond excluding them. The practical impact would be enhanced by discussing mitigation strategies such as active learning, model retraining, or ways to incorporate OOD scores into exploration.
Review Point: 3. The theoretical foundations could benefit from deeper analysis, particularly regarding how classifier architecture affects density ratio estimation accuracy and potential bounds on detection performance under various distribution shift scenarios.
==================================================

Focused review:

1. This article's writing suffers from vagueness, often employing ambiguous sentences that hinder readers from effectively grasping the technical details. For instance, the use of "stepping stone" in line 31 and "deeper insight" in line 44 lack concrete explanations or specific examples. Additionally, the use of certain technical terms needs to be more precise. For example, "training dynamics" in line 109 is mentioned without any subsequent elaboration or relevant content in the later sections. This lack of clarity and precision in language can significantly impede the reader's understanding of the proposed methods and contributions.
2. For lines 85-100, it's confusing that authors directly turn to the discussion of Graph LLMs without motivating its relationship to the explainability of GNNs
3. I strongly recommend that authors check the rigorous definition of explainability in machine learning, such as [1]. The author seems to misunderstand the concept of explainability. They assume that expressing the prediction logic in natural language automatically equates to explainability. However, this is not the case. Natural language can generate irrelevant or even misleading explanations that do not reflect the true underlying reasoning of the model.
4. Following the previous points, there's no experiment evaluating the explainability of the model but focusing on the accuracy of prediction. A case study is not a rigorous way to check the effectiveness.
5. No explainability-related work is considered in the related works part. Specifically, some highly relevant works like [2] are omitted. In terms of prompt optimization, the general philosophy is highly similar to [3].
6. The methodology part is hard to follow. I strongly recommend that the authors summarize it into some algorithms.
7. The theoretical part is a simple replication of the one in [4], which can't well explain the empirical part.
[1] Carvalho, D. V., Pereira, E. M., & Cardoso, J. S. (2019). Machine learning interpretability: A survey on methods and metrics. Electronics, 8(8), 832.
[2] Zhang, J., Liu, J., Luo, D., Neville, J., & Wei, H. (2024). LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation. arXiv preprint arXiv:2407.15351.
[3] Yuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., & Zou, J. (2024). TextGrad: Automatic" Differentiation" via Text. arXiv preprint arXiv:2406.07496.
[4] He, X., Bresson, X., Laurent, T., Perold, A., LeCun, Y., & Hooi, B. (2023). Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. arXiv preprint arXiv:2305.19523.

Review Point: 1. This article's writing suffers from vagueness, often employing ambiguous sentences that hinder readers from effectively grasping the technical details. For instance, the use of "stepping stone" in line 31 and "deeper insight" in line 44 lack concrete explanations or specific examples. Additionally, the use of certain technical terms needs to be more precise. For example, "training dynamics" in line 109 is mentioned without any subsequent elaboration or relevant content in the later sections. This lack of clarity and precision in language can significantly impede the reader's understanding of the proposed methods and contributions.
Review Point: 2. For lines 85-100, it's confusing that authors directly turn to the discussion of Graph LLMs without motivating its relationship to the explainability of GNNs 3. I strongly recommend that authors check the rigorous definition of explainability in machine learning, such as [1]. The author seems to misunderstand the concept of explainability. They assume that expressing the prediction logic in natural language automatically equates to explainability. However, this is not the case. Natural language can generate irrelevant or even misleading explanations that do not reflect the true underlying reasoning of the model.
Review Point: 4. Following the previous points, there's no experiment evaluating the explainability of the model but focusing on the accuracy of prediction. A case study is not a rigorous way to check the effectiveness.
Review Point: 5. No explainability-related work is considered in the related works part. Specifically, some highly relevant works like [2] are omitted. In terms of prompt optimization, the general philosophy is highly similar to [3].
Review Point: 6. The methodology part is hard to follow. I strongly recommend that the authors summarize it into some algorithms.
==================================================

Focused review:

- FedInverse uses typical model inversion attacks to generate the images. However, whether those approaches could be directly applied to the aggregated model is still a problem. In the setting of the experiments, the clients have local datasets that are disjoint on the label (for example, in MNIST, the attacker has labels 5-9, while other clients have labels 0-4). However, the setting is not practical for the typical situation. As the number of participants increases, some clients will have similar data distributions, so that FedInverse may fail in this setting.
- Moreover, FedInverse generates the image from a similar distribution with target clients. However, it could not tell the source of the image exactly (from an attacker or another specific participant).
- Some metrics used in the experiments should be clear. Take the attack accuracy as an example. A well-trained model may still classify an image with some flaw as the correct class, and it could lead to results at variance with reality. Additionally, the top-5 accuracy is too weak for some experiments, especially on MNIST (with only ten labels). As for FID, the attack results of some baseline approaches(for example, applying the MI attack on local models) should be added for comparison. Otherwise, it would be hard for readers to understand the degree of privacy leakage.
- In Section 3, the description of the attack procedure is not clear enough. More details are needed.

Review Point: - FedInverse uses typical model inversion attacks to generate the images. However, whether those approaches could be directly applied to the aggregated model is still a problem. In the setting of the experiments, the clients have local datasets that are disjoint on the label (for example, in MNIST, the attacker has labels 5-9, while other clients have labels 0-4). However, the setting is not practical for the typical situation. As the number of participants increases, some clients will have similar data distributions, so that FedInverse may fail in this setting.
Review Point: - Moreover, FedInverse generates the image from a similar distribution with target clients. However, it could not tell the source of the image exactly (from an attacker or another specific participant).
Review Point: - Some metrics used in the experiments should be clear. Take the attack accuracy as an example. A well-trained model may still classify an image with some flaw as the correct class, and it could lead to results at variance with reality. Additionally, the top-5 accuracy is too weak for some experiments, especially on MNIST (with only ten labels). As for FID, the attack results of some baseline approaches(for example, applying the MI attack on local models) should be added for comparison. Otherwise, it would be hard for readers to understand the degree of privacy leakage.
Review Point: - In Section 3, the description of the attack procedure is not clear enough. More details are needed.
==================================================

Focused review:

1. **Lack of Clarity in Figure Usage:** The paper’s use of Figure 2 to explain the idea behind curvature-based change point detection might be challenging. As curvature is an abstract concept, it is difficult to comprehend the figure’s relevance without prior theoretical context. Suggestion: postpone the figure and its explanation until after the theoretical section to enhance clarity.
2. **Incomplete Proofs:** The paper references proofs from other works and, in some instances, lacks complete proofs (e.g., Proposition 3.5). This can be confusing and frustrating for readers, as it necessitates navigating multiple papers non-linearly to understand the method fully. A suggestion is to place complete proofs in an appendix with consistent notation to provide a comprehensive and self-contained resource.
3. **Notation Ambiguity:** The use of double brackets to represent ranges in mathematical notation, without prior explanation, deviates from the conventional mathematical convention of single brackets. This deviation can create confusion and uncertainty regarding the intended meaning of the notation. Providing a clear explanation or adhering to standard notation would improve clarity.
4. **Complex Definition without Adequate Explanation:** Definition3.3introducesthecurvature-basedchange metric, which comprises several sub-parts, including a moving average function, a min-max norm, and a subtraction (the “1−” in $MovAvg(1 − MinMaxNorm(\kappa_{t,w}))$. However, the paper lacks a detailed breakdown and explanation of these components. The origin of the "one minus" and the "moving average" is unclear, and the rationale behind this definition is not readily apparent. Enhancing the presentation by providing a step-by-step breakdown and connecting it to the theoretical foundation would improve understanding.

Review Point: 1. **Lack of Clarity in Figure Usage:** The paper’s use of Figure 2 to explain the idea behind curvature-based change point detection might be challenging. As curvature is an abstract concept, it is difficult to comprehend the figure’s relevance without prior theoretical context. Suggestion: postpone the figure and its explanation until after the theoretical section to enhance clarity.
Review Point: 2. **Incomplete Proofs:** The paper references proofs from other works and, in some instances, lacks complete proofs (e.g., Proposition 3.5). This can be confusing and frustrating for readers, as it necessitates navigating multiple papers non-linearly to understand the method fully. A suggestion is to place complete proofs in an appendix with consistent notation to provide a comprehensive and self-contained resource.
Review Point: 3. **Notation Ambiguity:** The use of double brackets to represent ranges in mathematical notation, without prior explanation, deviates from the conventional mathematical convention of single brackets. This deviation can create confusion and uncertainty regarding the intended meaning of the notation. Providing a clear explanation or adhering to standard notation would improve clarity.
Review Point: 4. **Complex Definition without Adequate Explanation:** Definition3.3introducesthecurvature-basedchange metric, which comprises several sub-parts, including a moving average function, a min-max norm, and a subtraction (the “1−” in $MovAvg(1 − MinMaxNorm(\kappa_{t,w}))$. However, the paper lacks a detailed breakdown and explanation of these components. The origin of the "one minus" and the "moving average" is unclear, and the rationale behind this definition is not readily apparent. Enhancing the presentation by providing a step-by-step breakdown and connecting it to the theoretical foundation would improve understanding.
==================================================

Focused review:

**Method**
* My main concern regarding the proposed method is the theoretical validity of the deformable environment map.
* First, an important assumption of using an environment map to model a specular surface is that the environment should be far away from the surface. However, the reflective surface present in the datasets is mainly reflecting objects close to it. This assumption is further violated when this specular surface starts moving. This is all because the query of environment mapping considers only direction but not the position of the surface point. So I am not very sure whether the use of an environment map for such scenes is a good choice.
* Second, the deformable environment map lacks theoretical support. The paper justifies it as a model for the changing environment lighting, but the scenes used in the paper are of static environment lighting. Although the ablation study shows a minor improvement in the performance when using this deformable environment map, but it could be because of the following three reasons: 1. compensating the inaccurate surface normal estimated, 2. compensating the relative motion of the object with its surrounding due to the incorrect environment mapping assumption mentioned above, 3. MLP carries more modeling capacity to fit the specular lighting.
* Third, the paper claims the use of the Cook-Torrance reflectance model for PBR, but this model requires an expensive integration in the hemisphere based on the BRDF. This raises a few issue: 1. the paper seems to miss out how this BRDF is estimated, 2. the paper seems to miss out how this integration is done, 3. the paper did not report the inference time which could be a concern due to this expensive integration. **Experiments**
* The performance improvement on the NeRF-DS dataset is not very significant.
* The performance improvement on the NeRF-DS dataset specular region is less significant than the overall improvement, which indicates that the performance improvement on the non-specular regions is contributing more to the overall performance.
* The PSNR for HyperNeRF dataset is lower than the 4DGS by 2dB, but the LPIPS is better. This is not impossible, but some explanation would be helpful.
I believe this paper is a useful extension to the dynamic specular object reconstruction, but the methods introduced are not very well justified with theoretical reasoning. The small performance improvement also decreases the contribution of this paper.

Review Point: **Method** * My main concern regarding the proposed method is the theoretical validity of the deformable environment map.
Review Point: * First, an important assumption of using an environment map to model a specular surface is that the environment should be far away from the surface. However, the reflective surface present in the datasets is mainly reflecting objects close to it. This assumption is further violated when this specular surface starts moving. This is all because the query of environment mapping considers only direction but not the position of the surface point. So I am not very sure whether the use of an environment map for such scenes is a good choice.
Review Point: * Second, the deformable environment map lacks theoretical support. The paper justifies it as a model for the changing environment lighting, but the scenes used in the paper are of static environment lighting. Although the ablation study shows a minor improvement in the performance when using this deformable environment map, but it could be because of the following three reasons:
Review Point: 1. compensating the inaccurate surface normal estimated, 2. compensating the relative motion of the object with its surrounding due to the incorrect environment mapping assumption mentioned above, 3. MLP carries more modeling capacity to fit the specular lighting.
Review Point: * Third, the paper claims the use of the Cook-Torrance reflectance model for PBR, but this model requires an expensive integration in the hemisphere based on the BRDF. This raises a few issue:
Review Point: 1. the paper seems to miss out how this BRDF is estimated, 2. the paper seems to miss out how this integration is done, 3. the paper did not report the inference time which could be a concern due to this expensive integration. **Experiments** * The performance improvement on the NeRF-DS dataset is not very significant.
Review Point: * The performance improvement on the NeRF-DS dataset specular region is less significant than the overall improvement, which indicates that the performance improvement on the non-specular regions is contributing more to the overall performance.
Review Point: * The PSNR for HyperNeRF dataset is lower than the 4DGS by 2dB, but the LPIPS is better. This is not impossible, but some explanation would be helpful. I believe this paper is a useful extension to the dynamic specular object reconstruction, but the methods introduced are not very well justified with theoretical reasoning. The small performance improvement also decreases the contribution of this paper.
==================================================

Focused review:

I am not fully convinced about the way the false negative samples are identified. In the paper, the authors proposed to use SimCSE as a complementary model and identify false negatives as negatives that have higher semantic similarity over a threshold using SimCSE representation, and punish those “false negatives” with a 0 weighting. I would argue that using similarity score alone could not identify false negatives. It only identify negatives that are very close to the original sentence semantically. In fact, those are “hard negatives” which has been shown in many contrastive works as essential in learning a high-quality representation.
The authors can also do a better job at explaining why starting with a Gaussian negatives and optimizing equation (3) and (4) can lead to negatives sampled from non-uniform semantic space. Any previous work or evidence to support that?
1) In Figure 1, what is the corpus that the authors use to sample sentence pairs?
2) In addition to STS tasks, the authors could also explore a number of downstream tasks as implemented in SentEval to evaluate the quality of sentence representations.
3) It would be good to add statical analysis to the results in Table 1 to show significance.

Review Point: 1) In Figure 1, what is the corpus that the authors use to sample sentence pairs?
Review Point: 2) In addition to STS tasks, the authors could also explore a number of downstream tasks as implemented in SentEval to evaluate the quality of sentence representations.
Review Point: 3) It would be good to add statical analysis to the results in Table 1 to show significance.
==================================================

Focused review:

Although this paper has the above strengths that are interesting, I also noticed several weaknesses that should be further considered.
1. Application Scenario. The authors propose a novel problem definition, namely RAG Dataset Inference (RAG-DI). The concept is quite straightforward, and one has to admit that the concern indeed exists in practice. However, it is still less discussed in this paper, that how often this type of problem can occur in realistic scenarios. I am concerned with the application range of this work. For example, if a person is concerned with their own data being used for LLMs, do they need to test all LLMs in the market? This could be prohibitive.
2. Lack of Computation Complexity. The authors proposed a novel method to deal with the specific problem of RAG-DI. However, the complexity is not fully discussed in the paper. As a result, the problem of scalability could be severe. For example, the owner has an excessively large dataset, will the method still work?
3. Countermeasures: Although the authors considered the scenario where the RAG provider attempts to prevent the unintended use of the system, they do not provide a detailed discussion of the possible countermeasures that the RAG provide may use to counteract the watermarking of data. As there are multiple existing techniques against watermarking, the discussion about vulnerability of the proposed framework when faced with these countermeasures could be inspiring

Review Point: 1. Application Scenario. The authors propose a novel problem definition, namely RAG Dataset Inference (RAG-DI). The concept is quite straightforward, and one has to admit that the concern indeed exists in practice. However, it is still less discussed in this paper, that how often this type of problem can occur in realistic scenarios. I am concerned with the application range of this work. For example, if a person is concerned with their own data being used for LLMs, do they need to test all LLMs in the market? This could be prohibitive.
Review Point: 2. Lack of Computation Complexity. The authors proposed a novel method to deal with the specific problem of RAG-DI. However, the complexity is not fully discussed in the paper. As a result, the problem of scalability could be severe. For example, the owner has an excessively large dataset, will the method still work?
Review Point: 3. Countermeasures: Although the authors considered the scenario where the RAG provider attempts to prevent the unintended use of the system, they do not provide a detailed discussion of the possible countermeasures that the RAG provide may use to counteract the watermarking of data. As there are multiple existing techniques against watermarking, the discussion about vulnerability of the proposed framework when faced with these countermeasures could be inspiring
==================================================

Focused review:

1. Although this method is proposed for LVLMs, this paper does not mention the DoLa [r1] in the main body. DoLa's claims are somewhat opposite to DAMO's. More analysis is needed to clarify the insights and differences.
[r1] Dola: Decodingbycontrastinglayers improves factualityinlarge languagemodels. arXivpreprint arXiv:2309.03883,2023.
2. Applying DAMO to LVLMs and LLMs seems to be contradictory, especially considering the DoLa's findings.
3. I check the appendix and think some critical experimental details of DAMO are missing, like temperature, max token, decoding strategies.
4. GPT4-V experiments should be better conducted rather than GPT4, following VCD.
5. typo: INF-MLLM1 $\rightarrow$ INF-MLLM.

Review Point: 1. Although this method is proposed for LVLMs, this paper does not mention the DoLa [r1] in the main body. DoLa's claims are somewhat opposite to DAMO's. More analysis is needed to clarify the insights and differences. [r1] Dola: Decodingbycontrastinglayers improves factualityinlarge languagemodels. arXivpreprint arXiv:2309.03883,2023.
Review Point: 2. Applying DAMO to LVLMs and LLMs seems to be contradictory, especially considering the DoLa's findings.
Review Point: 3. I check the appendix and think some critical experimental details of DAMO are missing, like temperature, max token, decoding strategies.
Review Point: 4. GPT4-V experiments should be better conducted rather than GPT4, following VCD.
==================================================

Focused review:

- The main concern with the proposed approach is whether the state-of-the-art results are not a consequence of the model’s higher capacity rather than its design. To address this, it would be relevant to see, for example, results from the following experiment: evaluate the ReNovo without Retrieval model of larger size (e.g., by increasing the transformer’s number of layers and hidden dimensionality) and compare it to ReNovo, as illustrated, for instance, in Figure 3.
- Another related concern is the train-test splitting strategy. The text states that “peptides appearing in the testing set are completely distinct from those in the training set.” It would be valuable to assess the method’s performance under a stricter splitting criterion, such as one based on a sequence similarity threshold rather than sequence identity. Such an experiment would demonstrate whether the improved performance is due to generalization rather than overfitting to the training dataset through the “datastore” approach. This could be also tested by measuring the model’s improvement on test sequences that lack similar sequences in the training set.
- Certain essential parts of the text lack clarity.
- The term “context features” is central to the work but is not clearly defined. Does this refer to an embedding from the last transformer decoder layer? What does $f$ in line 213 signify?
- The “ReNovo with Residual” model is not fully defined. How exactly are “ReNovo without Retrieval” and “ReNovo” configured?

Review Point: - The main concern with the proposed approach is whether the state-of-the-art results are not a consequence of the model’s higher capacity rather than its design. To address this, it would be relevant to see, for example, results from the following experiment: evaluate the ReNovo without Retrieval model of larger size (e.g., by increasing the transformer’s number of layers and hidden dimensionality) and compare it to ReNovo, as illustrated, for instance, in Figure 3.
Review Point: - Another related concern is the train-test splitting strategy. The text states that “peptides appearing in the testing set are completely distinct from those in the training set.” It would be valuable to assess the method’s performance under a stricter splitting criterion, such as one based on a sequence similarity threshold rather than sequence identity. Such an experiment would demonstrate whether the improved performance is due to generalization rather than overfitting to the training dataset through the “datastore” approach. This could be also tested by measuring the model’s improvement on test sequences that lack similar sequences in the training set.
Review Point: - The term “context features” is central to the work but is not clearly defined. Does this refer to an embedding from the last transformer decoder layer? What does $f$ in line 213 signify?
Review Point: - The “ReNovo with Residual” model is not fully defined. How exactly are “ReNovo without Retrieval” and “ReNovo” configured?
==================================================

Focused review:

1. It is not clear how the nodes are dynamically changed during the diffusion/denoising
2. Typo in figure 1: iv->vi, and the equation at (4) is confusing.
3. The author need to differentiate its work against GDSS, which also uses a dual diffusion for modeling graph. Moreover, latent graph diffusion work are missing [1]
4. Can you provide a theoretical and empirical time analysis on the proposed method. I feels that theoretically the time complexity is still O(N^2). However, It would be great to see a sensitivity analysis of wall-clock time vs. #nodes.
[1] Chen, Xiaohui, et al. "Nvdiff: Graph generation through the diffusion of node vectors." arXiv preprint arXiv:2211.10794 (2022).

Review Point: 3. The author need to differentiate its work against GDSS, which also uses a dual diffusion for modeling graph. Moreover, latent graph diffusion work are missing [1] 4. Can you provide a theoretical and empirical time analysis on the proposed method. I feels that theoretically the time complexity is still O(N^2). However, It would be great to see a sensitivity analysis of wall-clock time vs. #nodes. [1] Chen, Xiaohui, et al. "Nvdiff: Graph generation through the diffusion of node vectors." arXiv preprint arXiv:2211.10794 (2022).
==================================================

Focused review:

- The idea of condition-to-ratio should be further investigated or emphasized, such as adjusting the number of dimensions to vary condition-to-ratio so that its effect can be observed more clearly.
- While authors state the proposed solution is broadly applicable across DiT models and U-Net models, I only see results of DiT models in experiments. U-Net models with CMG should be included to verify the claim.
- Only one backbone DiT model is evaluated
- In terms of sparse camera control, I wonder how to obtain the sparse camera control signal with good naturalness and smoothness. A way I can think of is subsampling from **a complete one**, but in such cases we can just input the complete signal instead without the need of using a sparse camera control. Any thoughts?

Review Point: - The idea of condition-to-ratio should be further investigated or emphasized, such as adjusting the number of dimensions to vary condition-to-ratio so that its effect can be observed more clearly.
Review Point: - While authors state the proposed solution is broadly applicable across DiT models and U-Net models, I only see results of DiT models in experiments. U-Net models with CMG should be included to verify the claim.
Review Point: - Only one backbone DiT model is evaluated - In terms of sparse camera control, I wonder how to obtain the sparse camera control signal with good naturalness and smoothness. A way I can think of is subsampling from **a complete one**, but in such cases we can just input the complete signal instead without the need of using a sparse camera control. Any thoughts?
==================================================

Focused review:

- The empirical results are quite lacking. There are many model-based algorithms out there that should be compared to, as baselines, but the authors only compare to Q-learning. Not only that, there are also many variants of Q-learning that could be worthy baselines, but the authors also did not include those. The same is true for the set of DQN experiments. Because of this, I'm not adequately convinced in the usefulness of these methods. (For example, why not use some of the baselines mentioned in the intro?)
- In MA-DQN, the tabular equation (2) is used for the model learning. But in DQN settings, we are in a setting with large state (or action) spaces, so this method of model learning is not going to scale well. I wonder how the authors would address this major issue?
- Because the results are only shown for relatively simple environments, the above concern is not really tested. I'd recommend the authors test their method on real settings where DQN would be applied.

Review Point: - Because the results are only shown for relatively simple environments, the above concern is not really tested. I'd recommend the authors test their method on real settings where DQN would be applied.
==================================================

Focused review:

- The novelty / content of the paper is somewhat limited. For example in ClimaX -- an ICML paper -- which introduced a full end-to-end pretraining and finetuning pipeline over multiple datasets and multiple weather and climate finetuning tasks. A possible extension for this paper would be to use different foundation model backbones - any of the large weather models are interesting here.
- The Prithvi WxC model is very low resolution compared to other models, shows hardly no ablations, and for none of the tasks they consider, they beat any SOTA baselines. The current SOTA in weather and climate modeling has moved way beyond the 0.625x0.5 resolution. This of course is not the fault of the authors. Yet for example, the Aurora foundation model (left out in the paper) is trained on many atmospheric datasets on much finer resolution. It would have been nice to put a comparison between these two models.
- The downstream task is super low resolution - which a priori is ok, but not rtoo impressive.
- At least one more baseline would be needed to really gauge the results.
I would advise the authors to consider user other foundation models too. And please stop refering to the Prithvi WxC model as SOTA model.

Review Point: - The novelty / content of the paper is somewhat limited. For example in ClimaX -- an ICML paper -- which introduced a full end-to-end pretraining and finetuning pipeline over multiple datasets and multiple weather and climate finetuning tasks. A possible extension for this paper would be to use different foundation model backbones - any of the large weather models are interesting here.
Review Point: - The Prithvi WxC model is very low resolution compared to other models, shows hardly no ablations, and for none of the tasks they consider, they beat any SOTA baselines. The current SOTA in weather and climate modeling has moved way beyond the 0.625x0.5 resolution. This of course is not the fault of the authors. Yet for example, the Aurora foundation model (left out in the paper) is trained on many atmospheric datasets on much finer resolution. It would have been nice to put a comparison between these two models.
Review Point: - The downstream task is super low resolution - which a priori is ok, but not rtoo impressive.
Review Point: - At least one more baseline would be needed to really gauge the results. I would advise the authors to consider user other foundation models too. And please stop refering to the Prithvi WxC model as SOTA model.
==================================================

Focused review:

The main limitations of this work are the inaccuracies in annotations and the insufficiency of the benchmark experiments, detailed as follows:
1.	There are minor suggestions for improvement in the phrasing. For example, line 431 states, "For the binary segmentation task framing, we train both CNN-based and Transformer-based backbones, considering the prevalent imbalance in the image data due to the small size of wells." This description is somewhat vague. If it implies that the backbones were pre-trained, it should be clearly stated which datasets were used for this pre-training. In Section 4, BENCHMARK EXPERIMENTS, it might not be clear to the reader whether the experiments used only the RGB three channels or included the near-infrared to make four channels. Although Section 3 states that the dataset provides four channels, repeating this in the experimental setup could be beneficial.
2.	Tables 4 and 5 display the benchmark performances of various classical semantic segmentation and object detection architectures on this dataset. While this work does not provide experiments with different model sizes (such as small, base, large versions), this might be sufficient for this dataset. However, including the parameter count and computational load of each benchmark model in the tables would add value to the discussion of segmentation and detection results.
3.	The state-of-the-art backbone, ConvNeXt, was omitted in the experiments. Providing its performance in remote sensing image analysis would make the article's benchmark presentation more comprehensive.
4.	In Figure 4, the satellite images reveal that wells may not always be perfectly circular, yet the semantic segmentation annotations are uniformly circular, and the object detection bounding boxes are uniformly square. This may not accurately reflect the real shape of wells, but the article lacks an explanation for this, except for a mention on line 335 about the "teardrop shape typical for well sites." The accuracy of these annotations might be the primary limitation of this dataset.
5.	Line 336 states that wells "typically range from 70 to 120 meters in diameter," indicating that wells in Alberta are relatively uniform in size. In contrast, other datasets, like the Well Pad Dataset, exhibit more significant size variations, which could impact the performance of few- or zero-shot transfer learning, as mentioned on line 503. Discussing regional differences in well characteristics could enhance this section.

Review Point: 2. Tables 4 and 5 display the benchmark performances of various classical semantic segmentation and object detection architectures on this dataset. While this work does not provide experiments with different model sizes (such as small, base, large versions), this might be sufficient for this dataset. However, including the parameter count and computational load of each benchmark model in the tables would add value to the discussion of segmentation and detection results.
Review Point: 3. The state-of-the-art backbone, ConvNeXt, was omitted in the experiments. Providing its performance in remote sensing image analysis would make the article's benchmark presentation more comprehensive.
Review Point: 4. In Figure 4, the satellite images reveal that wells may not always be perfectly circular, yet the semantic segmentation annotations are uniformly circular, and the object detection bounding boxes are uniformly square. This may not accurately reflect the real shape of wells, but the article lacks an explanation for this, except for a mention on line 335 about the "teardrop shape typical for well sites." The accuracy of these annotations might be the primary limitation of this dataset.
Review Point: 5. Line 336 states that wells "typically range from 70 to 120 meters in diameter," indicating that wells in Alberta are relatively uniform in size. In contrast, other datasets, like the Well Pad Dataset, exhibit more significant size variations, which could impact the performance of few- or zero-shot transfer learning, as mentioned on line 503. Discussing regional differences in well characteristics could enhance this section.
==================================================

Focused review:

A main criticism I have for this paper is that many assumptions are not explicitly stated, and the quantifiers are not properly stated in the settings and theorems. Furthermore, the paper provided no intuition on how the analysis is conducted. These problems gave me a hard time parsing the result. For instance, my first impression was that mutual information is *not* the correct notion that should be used to measure the gain for ‘subpopulation harmonization’. In particular, if $f$ is a *deterministic* function of $x$ or even a randomized function whose randomness is *independent of* $x$, then $I(X;Y)$ and $I(X;Y| v(X,Y))$ are essentially the same. (Btw $v(X,Y)$ is a random variable, which I believe you never mentioned.) I think the key in your model is that $y$ is a randomized function of $x$, and the randomness is *dependent* on the choice of $s$. However, such a fact is only clear after carefully reading the proof (!), and the whole thing reads quite confusing at first.
Additional (hidden) assumptions for the theorem in this paper include the fixed distribution of $(X,Y)$ and a fixed number of groups of subpopulations. Apart from being sloppy and not stated properly, the assumption of a fixed number of groups of subpopulations also implies we should have considerable knowledge of the dataset.
A concern about the experiment is that the improvements compared to the baseline, especially w.r.t. the very basic ERM, are too marginal. I understand that getting the SOTA performance in experiment-based machine learning is an interesting problem, however slight the improvement is. But for this specific problem, the small margin of improvement (which itself is in a low accuracy range) might have low impacts on practice. Minor:
- In Table 1, the meaning of $p(\cdot)$ is overloaded – the ‘’class-imbalance’’ distribution is supported on the labels, while the ‘’subpopulation-imblance’’ distribution is supported on the groups of the subpopulations. I’d suggest using a different notation.
- In theorem 3.3, the Rademacher complexity of $G$ is not properly defined. Instead of simply pointing at the literature, I think the notion should be defined in the appendix with the proper quantifiers.
- Inequality (16) uses McDiarmid’s inequality, but this technical tool was never introduced. :(

Review Point: - In Table 1, the meaning of $p(\cdot)$ is overloaded – the ‘’class-imbalance’’ distribution is supported on the labels, while the ‘’subpopulation-imblance’’ distribution is supported on the groups of the subpopulations. I’d suggest using a different notation.
Review Point: - In theorem 3.3, the Rademacher complexity of $G$ is not properly defined. Instead of simply pointing at the literature, I think the notion should be defined in the appendix with the proper quantifiers.
Review Point: - Inequality (16) uses McDiarmid’s inequality, but this technical tool was never introduced. :(
==================================================

Focused review:

1. I have some concerns about the novelty of GMPO. Despite its simplicity, I believe this is somewhat very similar to the AWR algorithm, just that it optimizes a diffusion policy instead of a Gaussian policy. Also, this algorithm has already been verified in the image generation field.
Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. 3, 6
2. When applying GMPG, as I understand, Neural ODE needs to be leveraged in order to estimate logp of \mu. I'm concerned about the computation efficiency of this method. Could you compare it with SRPO and Diffusion-QL? Also, I think for GMPG, your policy model has to be a Gaussian but cannot be a diffusion policy. For Diffusion QL, your policy can be a Diffusion policy. Is this correct?
3. There are mistakes in the inference scheme for SfBC, and Diffusion QL, and IDQL in Table 2. I recall for Diffusion QL and IDQL, they simply take [max] of action candidate Q values, not softmax. For SfBC, it uses [softmax] rather than [max]. Not entirely sure about the first two. Please recheck their code implementations.

Review Point: 3. There are mistakes in the inference scheme for SfBC, and Diffusion QL, and IDQL in Table 2. I recall for Diffusion QL and IDQL, they simply take [max] of action candidate Q values, not softmax. For SfBC, it uses [softmax] rather than [max]. Not entirely sure about the first two. Please recheck their code implementations.
==================================================

Focused review:

1. The main weakness seems to be the lack of novelty. Of course, DQN has not been used before, however, RL approach has been used (Pensieve). The paper did not explain why their approach has a better performance compared to Pensieve. The reward structure is the same, hence, the reviewer is wondering what are the advantages.
2. Even though the performance seems to be good, but it is not improving by a lot. While it is fine in general, but, given the fact that the methodology is not novel and the improvement is not a lot, my recommendation will be towards rejection.
3. The proposed algorithm has not been tested for a wide range of dataset in particular for 5G dataset.

Review Point: 1. The main weakness seems to be the lack of novelty. Of course, DQN has not been used before, however, RL approach has been used (Pensieve). The paper did not explain why their approach has a better performance compared to Pensieve. The reward structure is the same, hence, the reviewer is wondering what are the advantages.
Review Point: 2. Even though the performance seems to be good, but it is not improving by a lot. While it is fine in general, but, given the fact that the methodology is not novel and the improvement is not a lot, my recommendation will be towards rejection.
Review Point: 3. The proposed algorithm has not been tested for a wide range of dataset in particular for 5G dataset.
==================================================

Focused review:

incremental contribution; GPU friendly attention appears quite related to previous work [13], as well as to Linformer and Nystromformer (see below).
hybrid convolutional-transformer models have been proposed before (eg DPT hybrid, see below)
the proposed improvements perform only slightly better than baselines in Fig.3
missing configuration in Fig.3a: EA + CA
large training footprint: it appears that only 3 crops 512x1024 can fit into a V100
incomplete related work in the field of efficient models for semantic segmentation (eg HardNet, SwiftNet, see below)
Missing related work:
René Ranftl, Alexey Bochkovskiy, Vladlen Koltun. Vision Transformers for Dense Prediction. ICCV 2021: 12159-12168
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh. Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention. AAAI 2021.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma. Linformer: Self-Attention with Linear Complexity. CoRR abs/2006.04768 (2020).
Marin Orsic, Sinisa Segvic. Efficient semantic segmentation with pyramidal fusion. Pattern Recognit. 110: 107611 (2021).
Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin. HarDNet: A Low Memory Traffic Network. ICCV 2019
It appears that large memory footprint precludes training on single GPU systems.

Review Point: 107611 (2021). Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin. HarDNet: A Low Memory Traffic Network. ICCV 2019 It appears that large memory footprint precludes training on single GPU systems.
==================================================

Focused review:

## Presentation
- The "Analysis" section contains the problem definition and explanation of the baselines, which are background information. It would be better to split this information into a clear background section.
- The task settings "Task-IL" and "Class-IL" are never defined in the paper.
- The paper assumes that storing previous models or datasets is so costly that it is a big disadvantage to the memory-based methods. However, from a lay perspective, it would seem that storage is very cheap. I suggest the authors include more motivating discussion about why the memory-based IL methods have a disadvantage.
## Contribution
- The self-supervised learning (SSL) objective is introduced without appropriate analysis and baselines.
- From the ablation studies, we see that the SSL objective is a crucial component of the proposed method. However, I don't see how SSL is specific to the proposed method. Without applying SSL objectives to at least some of the most competitive baselines, we don't know how much of the improvement from the proposed method is actually due to the stable-plastic learner architecture.
- Three different SSL losses are proposed to be used with the method. The best results in each of the datasets are split between each of the SSL objectives. On some datasets, choosing a different SSL objective would lead to the proposed method no longer beating the baselines. This seems like a weakness of the method, in that you have to train three versions of it in order to pick which one you want to deploy.

Review Point: - The task settings "Task-IL" and "Class-IL" are never defined in the paper.
Review Point: - The paper assumes that storing previous models or datasets is so costly that it is a big disadvantage to the memory-based methods. However, from a lay perspective, it would seem that storage is very cheap. I suggest the authors include more motivating discussion about why the memory-based IL methods have a disadvantage. ## Contribution - The self-supervised learning (SSL) objective is introduced without appropriate analysis and baselines.
Review Point: - From the ablation studies, we see that the SSL objective is a crucial component of the proposed method. However, I don't see how SSL is specific to the proposed method. Without applying SSL objectives to at least some of the most competitive baselines, we don't know how much of the improvement from the proposed method is actually due to the stable-plastic learner architecture.
Review Point: - Three different SSL losses are proposed to be used with the method. The best results in each of the datasets are split between each of the SSL objectives. On some datasets, choosing a different SSL objective would lead to the proposed method no longer beating the baselines. This seems like a weakness of the method, in that you have to train three versions of it in order to pick which one you want to deploy.
==================================================

Focused review:

1. The primary weakness appears to be the somewhat marginal and inconsistent improvements provided by the proposed predictor. As shown in Table 5, results with the predictor are mixed when compared to simple baselines like FIRST and RANDOM. On the MMLU benchmark, in particular, the predictor does not demonstrate substantial improvement over these baselines.
2. The title may be somewhat overstated. This paper primarily addresses the sensitivity, or bit requirements, of various components within MOE models. However, the title may give an initial impression of a comprehensive evaluation of multiple MOE architectures or algorithms. A more precise title would improve clarity.
3. The observations from the benchmark lack a strong connection to the proposed method. The predictor currently relies on a magnitude-based approach, but there may be potential to develop a more tailored predictor based on the empirical findings. For instance, could an allocator be learned through reinforcement learning to better capture these insights?

Review Point: 1. The primary weakness appears to be the somewhat marginal and inconsistent improvements provided by the proposed predictor. As shown in Table 5, results with the predictor are mixed when compared to simple baselines like FIRST and RANDOM. On the MMLU benchmark, in particular, the predictor does not demonstrate substantial improvement over these baselines.
Review Point: 2. The title may be somewhat overstated. This paper primarily addresses the sensitivity, or bit requirements, of various components within MOE models. However, the title may give an initial impression of a comprehensive evaluation of multiple MOE architectures or algorithms. A more precise title would improve clarity.
Review Point: 3. The observations from the benchmark lack a strong connection to the proposed method. The predictor currently relies on a magnitude-based approach, but there may be potential to develop a more tailored predictor based on the empirical findings. For instance, could an allocator be learned through reinforcement learning to better capture these insights?
==================================================

Focused review:

1. The experiments are rather limited, only one setting with GPT2 activations is performed.
2. To demonstrate the significance of the results, I think it would be better to include the average and standard deviation of the evaluated metrics across different runs
3. SAE is described as a promising approach for extracting sparse, meaningful, and interpretable features from neural networks. But under the current experiment setup, it is hard to tell how the extracted features are meaningful and interpretable.
4. The problem is framed as a resource allocator in section 3, but I think it is not very clear how the edge weights are defined. If I understand correctly, it is somehow defined as related to the magnitude of feature activation, e.g. in line 483.
5. There are some typos, e.g. in lines 210 and 221 $S$ is defined the same way, but shouldn't it be different? in line 266, l is written in as a subscript in aux_k_loss

Review Point: 1. The experiments are rather limited, only one setting with GPT2 activations is performed.
Review Point: 2. To demonstrate the significance of the results, I think it would be better to include the average and standard deviation of the evaluated metrics across different runs 3. SAE is described as a promising approach for extracting sparse, meaningful, and interpretable features from neural networks. But under the current experiment setup, it is hard to tell how the extracted features are meaningful and interpretable.
Review Point: 4. The problem is framed as a resource allocator in section 3, but I think it is not very clear how the edge weights are defined. If I understand correctly, it is somehow defined as related to the magnitude of feature activation, e.g. in line 483.
==================================================

Focused review:

* I have several concerns related to clarity and lack of detail given to some important aspects of the methodology and experiments. These are elaborated on in the Questions section below.
* The datasets chosen are relatively small and relatively low-dimensional (e.g. ~10s of fields at most). An area where this method might be useful is with tabular data of much greater dimensionality, as is typical in healthcare contexts.

Review Point: * I have several concerns related to clarity and lack of detail given to some important aspects of the methodology and experiments. These are elaborated on in the Questions section below.
Review Point: * The datasets chosen are relatively small and relatively low-dimensional (e.g. ~10s of fields at most). An area where this method might be useful is with tabular data of much greater dimensionality, as is typical in healthcare contexts.
==================================================

Focused review:

- The results shown in Figure 1 & 4 are low-quality as the geometry details are fuzzy. Even consider geometry solely without considering many other frameworks do produce colors, the results are not good enough.
- Some of the design choice is not good enough. For example, the sampling point clouds only have 2,048 points which could be the reason the results cannot recover the back strip for the chair. Can the author try higher point samplings, like 8,192?
- I am confused about how the proposed Hierarchical Structure works. Does the learned full node connection really make sense? Can the author provide more in-depth analysis here instead of just final results along with some inputs. I failed to understand how the parts affect the final generation performance. Can we visualize the learned node connections?
- The current paper only performed on very old data Text2Shape as well as ShapeNet. However, we have much more 3D assets such as ABO and Objaverse. Results demonstrate on those newer datasets could be appealing. The current tested text-prompt is really simple.

Review Point: - The results shown in Figure 1 & 4 are low-quality as the geometry details are fuzzy. Even consider geometry solely without considering many other frameworks do produce colors, the results are not good enough.
Review Point: - Some of the design choice is not good enough. For example, the sampling point clouds only have 2,048 points which could be the reason the results cannot recover the back strip for the chair. Can the author try higher point samplings, like 8,192?
Review Point: - I am confused about how the proposed Hierarchical Structure works. Does the learned full node connection really make sense? Can the author provide more in-depth analysis here instead of just final results along with some inputs. I failed to understand how the parts affect the final generation performance. Can we visualize the learned node connections?
Review Point: - The current paper only performed on very old data Text2Shape as well as ShapeNet. However, we have much more 3D assets such as ABO and Objaverse. Results demonstrate on those newer datasets could be appealing. The current tested text-prompt is really simple.
==================================================

Focused review:

1. I suggest the authors reorganize the experiment results. The current ones are confusing since the Mixed and FT-then-PT are introduced in the later part of the paper while the corresponding results are shown before.
2. I wonder if prompt size is one of the tuned hyper-parameters. If so, it would be better to present the exact number of prompts used for each setting since different prompt size indicates different learning capacity.
3. In fact the original VPT has two variants, i.e. VPT-shallow and VPT-deep. It seems VPT-deep is adopted in this paper. I wonder if the same pattern holds for VPT-shallow too.
4. It is noteworthy that VPT only performs better in half of all datasets of VTAB-1k Structured in one-shot setting as shown in Tab.4, which is different from results of other two sub-categories. This should be highlighted and illustrated in Sec.4.4.
5. The current scope of this paper is ok. But it would be much greater if the authors could expand the study to prompt tuning in language models.

Review Point: 1. I suggest the authors reorganize the experiment results. The current ones are confusing since the Mixed and FT-then-PT are introduced in the later part of the paper while the corresponding results are shown before.
Review Point: 2. I wonder if prompt size is one of the tuned hyper-parameters. If so, it would be better to present the exact number of prompts used for each setting since different prompt size indicates different learning capacity.
Review Point: 3. In fact the original VPT has two variants, i.e. VPT-shallow and VPT-deep. It seems VPT-deep is adopted in this paper. I wonder if the same pattern holds for VPT-shallow too.
Review Point: 4. It is noteworthy that VPT only performs better in half of all datasets of VTAB-1k Structured in one-shot setting as shown in Tab.4, which is different from results of other two sub-categories. This should be highlighted and illustrated in Sec.4.4.
Review Point: 5. The current scope of this paper is ok. But it would be much greater if the authors could expand the study to prompt tuning in language models.
==================================================

Focused review:

/Feedback
1. The methods have limited modeling novelty.
It is good to see that the paper conducts the first comprehensive comparison between process- and outcome-based approaches trained on the math word problem-solving task. The paper conducts extensive experiments on the two types of approaches, and there are a lot of interesting findings. However, it seems that the paper does not propose a new method.
2. All the experiments are conducted on only one dataset.
I understand that there are some limitations for other datasets, and conducting experiments on new datasets leads to extra costs. However, whenever the annotation pipeline is set up, the additional costs of conducting experiments on new datasets are acceptable. It is worthwhile to conduct the experiments on smaller datasets. A recently proposed MWP dataset, TabMWP (https://arxiv.org/abs/2209.14610), might be a good benchmark for the experiments in the revised paper, which is annotated with multi-step reasoning steps.

Review Point: 2. All the experiments are conducted on only one dataset. I understand that there are some limitations for other datasets, and conducting experiments on new datasets leads to extra costs. However, whenever the annotation pipeline is set up, the additional costs of conducting experiments on new datasets are acceptable. It is worthwhile to conduct the experiments on smaller datasets. A recently proposed MWP dataset, TabMWP (https://arxiv.org/abs/2209.14610), might be a good benchmark for the experiments in the revised paper, which is annotated with multi-step reasoning steps.
==================================================

Focused review:

- Lack of necessary qualitative results to support the paper’s claim: As a method for 3D editing and animation, I personally hope to see qualitative results in multiple viewpoints and timestamps, especially dynamic results which could be better demonstrated by a demo video. Otherwise, there is no clue to support that the proposed method is a good fit for editing and animating 3D scenes.
- Is Superquadrics + 3DGS a good design? Basically, the superquadrics used in the paper have two roles: 1) offering a good initialization for the latter 3D Gaussians and 2) providing group-wise semantic correspondence of each Gaussian centroid which facilitates animation and editing. However, this two-stage pipeline inherits the downside of 3D Gaussians when generalizing to unseen “poses” of objects. As shown in Figure 4, the animated results contain severe artifacts when the animated part is moved.
- Worthy discussion against another primitive-based representation: It is worth mentioning Mixture-of-Volumetric-Primitives as an alternative representation for the target task in this paper. It naturally has the properties for both stages in the proposed method: 1) semantic correspondence alignment and 2) photorealistic differentiable rendering. It would be great to see authors’ discussions and even experiments for this representation. Ideally, the only thing to do is to apply semantic alignment for all primitives without involving the second stage 3DGS training.
- There is prior work in deforming and animating a well-trained 3D scene representation, which could be treated as a top-down approach (the proposed method could be treated as a bottom-up approach) to solve similar tasks:
- **Deforming Radiance Fields with Cages. ECCV 2022**
- **CageNeRF: Cage-based Neural Radiance Fields for Generalized 3D Deformation and Animation. NeurIPS 2022.**

Review Point: - Lack of necessary qualitative results to support the paper’s claim: As a method for 3D editing and animation, I personally hope to see qualitative results in multiple viewpoints and timestamps, especially dynamic results which could be better demonstrated by a demo video. Otherwise, there is no clue to support that the proposed method is a good fit for editing and animating 3D scenes.
Review Point: - Is Superquadrics + 3DGS a good design? Basically, the superquadrics used in the paper have two roles:
Review Point: 1) offering a good initialization for the latter 3D Gaussians and
Review Point: 2) providing group-wise semantic correspondence of each Gaussian centroid which facilitates animation and editing. However, this two-stage pipeline inherits the downside of 3D Gaussians when generalizing to unseen “poses” of objects. As shown in Figure 4, the animated results contain severe artifacts when the animated part is moved.
Review Point: - Worthy discussion against another primitive-based representation: It is worth mentioning Mixture-of-Volumetric-Primitives as an alternative representation for the target task in this paper. It naturally has the properties for both stages in the proposed method:
Review Point: 2) photorealistic differentiable rendering. It would be great to see authors’ discussions and even experiments for this representation. Ideally, the only thing to do is to apply semantic alignment for all primitives without involving the second stage 3DGS training.
Review Point: - There is prior work in deforming and animating a well-trained 3D scene representation, which could be treated as a top-down approach (the proposed method could be treated as a bottom-up approach) to solve similar tasks:
Review Point: - **Deforming Radiance Fields with Cages. ECCV 2022** - **CageNeRF: Cage-based Neural Radiance Fields for Generalized 3D Deformation and Animation. NeurIPS 2022.**
==================================================

Focused review:

Over the years I have seen a number of papers claiming to "prove" some big or surprising results, by means of a very specific, sometimes even toyish, example. I am not a big fan of this class of papers. Unfortunately, this paper belongs to this class. My main concern of this type of papers is that they cannot show whether the results they prove only work for the very specific example they study, and whether these results can be generalized to a more, even slightly, general situation. After all, one can prove that something does not work by giving a counter-example, but one cannot prove that something works by just giving one example. Thus, the conclusions these papers make are usually misleading, and since they make big claims, these papers are often much more misleading than others.
In the case of this particular submission, I did some experiments and came to the conclusion that the claim of this paper "label noise GD can be better than standard GD" only works for very limited situations. Even for the very example the authors provided, changing just one parameter can break the whole story.
I used the exact same setting as the authors in Section 5. I always used the parameters in the code in the supplementary material if it disagrees with the paper. For instance, the paper used $\eta=0.5$ (line 465) while in the code $\eta = 1.0$, so I used $\eta = 1.0$. The paper didn't say what flip probability was used, and $p=0.1$ was used in the code so I used that too. My code: https://pastecode.io/s/r6epwvx7
I always used fixed random seeds so my results are fully reproducible. I encourage the authors and my fellow reviewers to play with my code on their own.
First, I reproduced the results in the paper, using the exact same set of parameters. In the following plots, the blue curves are the training loss, while the orange curves are the test accuracy.
- Standard GD: https://i.postimg.cc/YC0LGM7z/1.png
- Label noise GD: https://i.postimg.cc/C1TRQ8qc/2.png
I managed to reproduce the exact same results as the authors.
Next, when I stared at the code, the first thing that struck me was that the model architecture looked quite strange. First, I couldn't understand what the authors mean by "two patches" $x = \{ x^{(1)}, x^{(2)} \}$ (line 144), and I couldn't understand why there should be a "patch" that is just Gaussian noise. Second, I don't think the model in lines 172-174 is a "CNN" because I don't see any convolution here. Third, the squared ReLU activation is not usually used. Anyway, I tried to change the model a little bit and see if it could help, and the very first thing I did was to use a smaller $m$ (that is, making the network narrower), which I believe is a very reasonable and very natural thought. Here are the results:
- Standard GD, with $m=3$: https://i.postimg.cc/br6NpSZc/3.png
- Label noise GD, with $m=3$: https://i.postimg.cc/7Ydw9k7V/4.png
Standard GD now achieves 100% test accuracy. Thus, by simply changing $m=20$ to $m=3$, the problem is immediately solved.
Plus, when $m=3$, standard GD still works with a larger noise. I changed $\sigma_p = 0.5$ to $\sigma_p = 2.0$, and ran the two methods again with $m=3$. Here are the results:
- Standard GD, with $m=3, \sigma_p = 2.0$: https://i.postimg.cc/HLCHG968/5.png
- Label noise GD, with $m=3, \sigma_p = 2.0$: https://i.postimg.cc/zDPrDrdr/6.png
In this case, standard GD can achieve 90-ish% test accuracy, while the performance of label noise GD is miserable. Thus, I've shown a case where label noise GD is much worse than standard GD.
The authors might argue: "Our theory only works when $m$ is not too small (Assumption 3.1 (ii))." I don't think this is a valid argument, because one is free to choose whatever model architecture they want. And if a narrower network can work so well, why bother using a wider one that is much worse? Moreover, when we couldn't get expected results in deep learning, we should always start with trying something straightforward like changing the network architecture, optimizer, loss, etc., before trying something very unnatural like "adding label noise during training".
To sum up, the experimental results I present here establish the fact that the claim of this paper, that is "label noise GD is better than GD", is not very robust and not very sound. I am not convinced that label noise can improve generalization in any practical regime.
As a final remark, I am not saying that the authors should not get credit for constructing this particular example in the paper. I think it is a fun fact to know that such an example exists. However, I think the conclusion of this submission is unsound, and the paper is quite misleading due to its catchy claim "label noise GD improves generalization". The claim can be easily invalidated by just changing one parameter. Imagine a PhD student who read this paper, got attracted by the idea, and tried to add label noise on a number of tasks, only to find out that it didn't work at all and lots of time and work went down the drain. For the sake of that student and the learning theory community, I must insist on rejecting this submission.

Review Point: - Standard GD, with $m=3$: https://i.postimg.cc/br6NpSZc/3.png - Label noise GD, with $m=3$: https://i.postimg.cc/7Ydw9k7V/4.png Standard GD now achieves 100% test accuracy. Thus, by simply changing $m=20$ to $m=3$, the problem is immediately solved. Plus, when $m=3$, standard GD still works with a larger noise. I changed $\sigma_p = 0.5$ to $\sigma_p = 2.0$, and ran the two methods again with $m=3$. Here are the results:
==================================================

Focused review:

* The technical contributions and novelty in this paper are limited. It empirically investigates different technical aspects that affect semantic parsing performance in ICL. Similar investigations have been conducted in many other tasks already, and there are not novel findings.
* The presentation of one of the main ideas in the paper, namely sample selection, is not clear. The diversity and similarity part (Algorithm 1 and associated description in Sec. 2.1) is confusing.
* The findings are not completely convincing or surprising.

Review Point: * The technical contributions and novelty in this paper are limited. It empirically investigates different technical aspects that affect semantic parsing performance in ICL. Similar investigations have been conducted in many other tasks already, and there are not novel findings.
Review Point: * The presentation of one of the main ideas in the paper, namely sample selection, is not clear. The diversity and similarity part (Algorithm 1 and associated description in Sec. 2.1) is confusing.
==================================================

Focused review:

1) The work is made significantly harder to reproduce by omitting implementation details of the 3D projection "using differentiable geometric computations". The fact that those are used is mentioned multiple times on page 4 but it's entirely unclear to me what that entails. It would've been nice to at least include a rough sketch of the code. 2) Similarly, the order of pretrainings and trainings is hard to parse and I would've appreciated a block of pseudocode of the training procedure or at least an ordered list of training steps. UPDATE: It would've been nice to release code already with the submission as opposed to promising to do it in the future and the reviewers having to trust the authors blindly. In the past, even when the authors promised code release with the acceptance, it still took several months for a release. I'd like to encourage earlier and reliable code release with _submission_ instead of acceptance.

Review Point: 1) The work is made significantly harder to reproduce by omitting implementation details of the 3D projection "using differentiable geometric computations". The fact that those are used is mentioned multiple times on page 4 but it's entirely unclear to me what that entails. It would've been nice to at least include a rough sketch of the code.
Review Point: 2) Similarly, the order of pretrainings and trainings is hard to parse and I would've appreciated a block of pseudocode of the training procedure or at least an ordered list of training steps. UPDATE: It would've been nice to release code already with the submission as opposed to promising to do it in the future and the reviewers having to trust the authors blindly. In the past, even when the authors promised code release with the acceptance, it still took several months for a release. I'd like to encourage earlier and reliable code release with _submission_ instead of acceptance.
==================================================

Focused review:

- The evaluation could be more thorough by incorporating existing benchmarks such as Codec-Superb and DASB, allowing for a more comprehensive comparison of the proposed method against existing models under standardized settings.
- The related works section could be expanded to include methods that use frequency domain inputs, such as those discussed in the following papers:
- https://arxiv.org/pdf/2406.05298
- https://arxiv.org/pdf/2201.09429
- https://arxiv.org/pdf/2405.00233
- https://arxiv.org/pdf/2402.10533
- https://www.arxiv.org/pdf/2406.07422
- While Hubert-KM, Encodec, and Speech Tokenizer are reasonable baselines, it would be beneficial to include additional baselines with more similar setups, such as SPECTRAL CODECS (https://arxiv.org/pdf/2406.05298) or SemantiCodec (https://arxiv.org/pdf/2405.00233), for a fuller assessment.
- The proposed model is only evaluated on speech data, leaving other domains, such as general audio and music, unexplored.

Review Point: - The evaluation could be more thorough by incorporating existing benchmarks such as Codec-Superb and DASB, allowing for a more comprehensive comparison of the proposed method against existing models under standardized settings.
Review Point: - The related works section could be expanded to include methods that use frequency domain inputs, such as those discussed in the following papers:
Review Point: - https://arxiv.org/pdf/2406.05298 - https://arxiv.org/pdf/2201.09429 - https://arxiv.org/pdf/2405.00233 - https://arxiv.org/pdf/2402.10533 - https://www.arxiv.org/pdf/2406.07422 - While Hubert-KM, Encodec, and Speech Tokenizer are reasonable baselines, it would be beneficial to include additional baselines with more similar setups, such as SPECTRAL CODECS (https://arxiv.org/pdf/2406.05298) or SemantiCodec (https://arxiv.org/pdf/2405.00233), for a fuller assessment.
Review Point: - The proposed model is only evaluated on speech data, leaving other domains, such as general audio and music, unexplored.
==================================================

Focused review:

- The submitted maniscript does not include the Section of References, which hampers the reading and reviewing process.
- While the authors emphasize the advantage of linear-time decoding with the Mamba-based decoder, they don't provide ablation experiments that compare against a counterpart with transformer-based decoder. I would suggest conducting comparisons in terms of inference time or quality-speed trade-offs at different sequence lengths.
- The authors propose to use rotary positional embeddings in the cross-attention modules where the positional indices are based on arrial time, but don't provide ablation experiments about this. I would suggest comparing to using fixed positional encodings and no positional encodings.

Review Point: - The submitted maniscript does not include the Section of References, which hampers the reading and reviewing process.
Review Point: - While the authors emphasize the advantage of linear-time decoding with the Mamba-based decoder, they don't provide ablation experiments that compare against a counterpart with transformer-based decoder. I would suggest conducting comparisons in terms of inference time or quality-speed trade-offs at different sequence lengths.
Review Point: - The authors propose to use rotary positional embeddings in the cross-attention modules where the positional indices are based on arrial time, but don't provide ablation experiments about this. I would suggest comparing to using fixed positional encodings and no positional encodings.
==================================================

Focused review:

- Except for CS, the remaining algorithms are not new. The author's main contribution is how to apply CS algorithm to existing algorithms and the corresponding theoretical analysis.
- The idea of the CS algorithm is not new. It existed in previous algorithms (For example, in Alg 2 in [Mat]).
- It is natural to apply CS algorithms to existing algorithms, but it is not difficult to derive theoretical bounds. ====================== Ref.
[Mat] Matthew Fahrbach, Vahab S. Mirrokni, Morteza Zadimoghaddam: Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity. SODA 2019: 255-273.

Review Point: - Except for CS, the remaining algorithms are not new. The author's main contribution is how to apply CS algorithm to existing algorithms and the corresponding theoretical analysis.
Review Point: - The idea of the CS algorithm is not new. It existed in previous algorithms (For example, in Alg 2 in [Mat]).
==================================================

Focused review:

- Many heuristics are used such as fitting of scaling law (Section 3.1 eq1), credit assignment (Section 3.2, eq2 ), preference distribution (Section 3.3, eq3) and temporal average (eq4 and eq5). The authors try to motivate those choices from related works and intuitions, but only eq1 is adequately explained and validated.
- Following the above, are all the heuristics (eq 1- 5) necessary? How important are they? This is a missing part in the paper. An ablation on them can validate if the invented heuristics are actually all useful.

Review Point: - Many heuristics are used such as fitting of scaling law (Section 3.1 eq1), credit assignment (Section 3.2, eq2 ), preference distribution (Section 3.3, eq3) and temporal average (eq4 and eq5). The authors try to motivate those choices from related works and intuitions, but only eq1 is adequately explained and validated.
Review Point: - Following the above, are all the heuristics (eq 1- 5) necessary? How important are they? This is a missing part in the paper. An ablation on them can validate if the invented heuristics are actually all useful.
==================================================

Focused review:

1. There are already a number of applications for detecting AI-generated text, e.g., https://copyleaks.com/ai-content-detector, https://writer.com/ai-content-detector/. Even OPENAI released a classifier for indicating AI-written text. However, authors don't seem to make comparisons.
2.The authors use the area under the receiver operating characteristic curve (AUROC) as the sole evaluation metric for their methods. While this is a common metric for zero-shot detection, it may not capture all aspects of performance, such as precision and recall.
3. The experiments in this paper only test the proposed methods on datasets in English. It is unclear how well these methods would perform on other languages.

Review Point: 1. There are already a number of applications for detecting AI-generated text, e.g., https://copyleaks.com/ai-content-detector, https://writer.com/ai-content-detector/. Even OPENAI released a classifier for indicating AI-written text. However, authors don't seem to make comparisons.
Review Point: 2.The authors use the area under the receiver operating characteristic curve (AUROC) as the sole evaluation metric for their methods. While this is a common metric for zero-shot detection, it may not capture all aspects of performance, such as precision and recall.
Review Point: 3. The experiments in this paper only test the proposed methods on datasets in English. It is unclear how well these methods would perform on other languages.
==================================================

Focused review:

1. The paper lacks sufficient details regarding the training of the generator responsible for mapping final feature maps back into the original input samples. As readers, we are left with questions about the level of effort required to train such a generator and the upper bounds of its capabilities. Additionally, it would be valuable to understand how the performance of the feature inverter generator is affected by the increasing accuracy of the victim model. Could you provide more information into these aspects to enhance our understanding of your work?
2. A small weakness: I notice in the code provided in supplementary materials, the authors made some change to the architecture of Resnet - They remove the average pooling layer (over a 7x7 feature map), thus by stretching the final feature map, the FC layer is indeed 49 times wider than before. This inevitably simplifies the difficulty of reconstructions a lot. To me this is not a severe concern, since the major part of paper does not assume how feature maps are reshaped to enter the FC layers, and this technical modification is not the focus of the research problem. But for the researchers studying this problem, they could be concerned about this. I wonder could authors show how the results will be like, if not removing average pooling, and use the original Resnet architecture in Torchvision implementation? It is expected to see performance drop in that case, but revealing this could give readers more insights about the difficulty of the problem, and understand how much information will be lost by average pooling before FC layer.

Review Point: 1. The paper lacks sufficient details regarding the training of the generator responsible for mapping final feature maps back into the original input samples. As readers, we are left with questions about the level of effort required to train such a generator and the upper bounds of its capabilities. Additionally, it would be valuable to understand how the performance of the feature inverter generator is affected by the increasing accuracy of the victim model. Could you provide more information into these aspects to enhance our understanding of your work?
==================================================

Focused review:

1. The paper studied the Constrained Markov Decision Process (CMDP) problem in both stochastic and adversarial settings. However, it heavily relies on the framework established by previous works (e.g., policy optimization in Luo et al., 2021; primal-dual design and analysis in Castiglioni et al., 2022b; Stradi et al., 2024b). This combination seems not interesting and the technical contribution of this paper is somewhat limited for a theory paper, especially considering the extensive literature on CMDPs, both in theory and algorithmic sides.
2. The metric of cumulative constraint violation is problematic in safety-critical applications in the paper because it may have limitations since it allows for compensation among different rounds.
3. The proposed method adopts a model-based approach and is limited to tabular settings. It might not be scalable to the setting with a large state and action space.
4. The paper primarily focuses on theoretical aspects; however, it would be beneficial to include numerical experiments to validate the proposed algorithm. By comparing it with representative studies in the extensive literature, such as the LP-based method discussed in Efroni et al. (2020) and Stradi et al. (2024b), as well as a model-based RL method in reference [1] and a model-free RL method in reference [2], the practical performance of the algorithm can be more clearly demonstrated. BTW, [1] and [2] are very related work and not mentioned in the paper.
[1] Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Learning policies with zero or bounded constraint violation for constrained MDPs. NeurIPS 2021.
[2] Arnob Ghosh, Xingyu Zhou, and Ness Shroff. Provably efficient model-free constrained rl with linear function approximation. NeurIPS 2022.

Review Point: 1. The paper studied the Constrained Markov Decision Process (CMDP) problem in both stochastic and adversarial settings. However, it heavily relies on the framework established by previous works (e.g., policy optimization in Luo et al., 2021; primal-dual design and analysis in Castiglioni et al., 2022b; Stradi et al., 2024b). This combination seems not interesting and the technical contribution of this paper is somewhat limited for a theory paper, especially considering the extensive literature on CMDPs, both in theory and algorithmic sides.
Review Point: 2. The metric of cumulative constraint violation is problematic in safety-critical applications in the paper because it may have limitations since it allows for compensation among different rounds.
Review Point: 3. The proposed method adopts a model-based approach and is limited to tabular settings. It might not be scalable to the setting with a large state and action space.
==================================================

Focused review:

1.Baselines are limited. Why not compare with T-Patcher (Huang et al., 2023), which I believe is more suitable for sequential knowledge editing?
2.The criteria for successful edits are, in my opinion, insufficient, in that they do not consider the portability of the edit. Previous work such as Yao et al., 2023, introducing an additional assessment metric, portability, finding that the model-editing methods lack robustness when applied to related one-hop fact or synonyms.
3.Which layers to apply MALMEN? All layers or some picked layers? Section 5.2 claims that “Edit first FC in FFN” achieves inferior performance. How to select the layer in practical application?
4.The experiments are lacking in qualitative examples, it would be helpful to analyze some success and failure cases to see where the proposed method begins to fail (e.g., with respect to generalization).
5.MALMEN is essentially a combination of meta-learning based (e.g. MEND, Mitchell 2022) and parametric (e.g. ROME, Meng 2022 or MEMIT, Meng 2023) editing ideas that shows some promise. The method is not particularly technically novel (minor point).

Review Point: 1.Baselines are limited. Why not compare with T-Patcher (Huang et al., 2023), which I believe is more suitable for sequential knowledge editing?
Review Point: 2.The criteria for successful edits are, in my opinion, insufficient, in that they do not consider the portability of the edit. Previous work such as Yao et al., 2023, introducing an additional assessment metric, portability, finding that the model-editing methods lack robustness when applied to related one-hop fact or synonyms.
Review Point: 3.Which layers to apply MALMEN? All layers or some picked layers? Section 5.2 claims that “Edit first FC in FFN” achieves inferior performance. How to select the layer in practical application?
Review Point: 4.The experiments are lacking in qualitative examples, it would be helpful to analyze some success and failure cases to see where the proposed method begins to fail (e.g., with respect to generalization).
Review Point: 5.MALMEN is essentially a combination of meta-learning based (e.g. MEND, Mitchell 2022) and parametric (e.g. ROME, Meng 2022 or MEMIT, Meng 2023) editing ideas that shows some promise. The method is not particularly technically novel (minor point).
==================================================

Focused review:

1. The design of the author's approach does not convincingly demonstrate an actual improvement in unlearning, despite the experimental results resembling retraining. Since retraining does not engage with forgotten data, any generalization enhancements derived from this data will inevitably be lost; otherwise, complete unlearning cannot be assured. The solution proposed by the author incorporates information from forgotten data, which intuitively undermines the concept of complete unlearning.
2. The experiment is not sufficient, and additional deletion results at varying ratios would enhance the credibility. Furthermore, the limited number of datasets and models raises concerns. I am particularly interested in whether the experimental outcomes with more complex models and datasets, such as ImageNet, would influence the results of the method.
3. There is a lack of epoch description of the experiment of the approach, and it remains unclear how many training rounds are necessary for convergence to achieve satisfactory results. The criteria for terminating training are crucial aspects that require discussion in the context of approximate forgetting.
4. The comparative experiments lack sufficient explanation of hyperparameters, and some state-of-the-art results appear somewhat low. I recommend that the author adhere to the hyperparameter guidelines provided in the original paper. While the author states, "if our reproduced results align with the previously reported statistics in Shen et al. (2024), we present their results; otherwise, we provide our results," I believe that directly copying most of the experimental data is unacceptable. The author should revise the baseline results to reflect actual experimental findings. By the way, TS does not require further training of the teacher model; instead, the original and initialized models should serve as the teacher models for the experiment. I suggest the author review the original control scheme rather than relying solely on the introduction in [1]. If I have misunderstood any aspects, I would appreciate clarification from the author. In summary, the author's experimental section appears overly influenced by [1], which raises doubts about the authenticity of the author's experimental workload.
[1] Shaofei Shen, Chenhao Zhang, Yawen Zhao, Alina Bialkowski, Weitong Chen, and Miao Xu. Label-agnostic forgetting: A supervision-free unlearning in deep models. arXiv preprint arXiv:2404.00506, 2024.

Review Point: 1. The design of the author's approach does not convincingly demonstrate an actual improvement in unlearning, despite the experimental results resembling retraining. Since retraining does not engage with forgotten data, any generalization enhancements derived from this data will inevitably be lost; otherwise, complete unlearning cannot be assured. The solution proposed by the author incorporates information from forgotten data, which intuitively undermines the concept of complete unlearning.
Review Point: 2. The experiment is not sufficient, and additional deletion results at varying ratios would enhance the credibility. Furthermore, the limited number of datasets and models raises concerns. I am particularly interested in whether the experimental outcomes with more complex models and datasets, such as ImageNet, would influence the results of the method.
Review Point: 3. There is a lack of epoch description of the experiment of the approach, and it remains unclear how many training rounds are necessary for convergence to achieve satisfactory results. The criteria for terminating training are crucial aspects that require discussion in the context of approximate forgetting.
==================================================

Focused review:

1. Complexity of the Proposed Sparse Attention Mechanism: The multi-step decision-making process, involving both Query-Aware Sparse Pattern Determination and Cumulative-Attention Based Index Selection, introduces additional complexity to the attention mechanism. A detailed computational complexity analysis is missing, which raises questions about how the overhead introduced by this process is justified by the computational savings achieved through sparse attention.
2. Lack of Detailed Comparison of Computational Savings: The paper lacks a comprehensive comparison of how much computation is saved by using FlexPrefill compared to the baselines presented in Tables 1 and 2. This information would be valuable in quantifying the benefits of the proposed approach.
3. Limited Comparison with Recent Related Works: The paper does not include comparisons with other recent related works, such as Han et al. (2024) and Xiao et al. (2024a). Including these comparisons would help to better position the contributions of FlexPrefill within the context of current research.
4. Mixed Performance Benefits: The reported performance benefits compared to the Full Attention model on long-context benchmarks are mixed, as shown in Tables 1 and 2. Additionally, the computational savings compared to the Full Attention model and other baselines are not adequately reported, which limits the understanding of the trade-offs involved.

Review Point: 1. Complexity of the Proposed Sparse Attention Mechanism: The multi-step decision-making process, involving both Query-Aware Sparse Pattern Determination and Cumulative-Attention Based Index Selection, introduces additional complexity to the attention mechanism. A detailed computational complexity analysis is missing, which raises questions about how the overhead introduced by this process is justified by the computational savings achieved through sparse attention.
Review Point: 2. Lack of Detailed Comparison of Computational Savings: The paper lacks a comprehensive comparison of how much computation is saved by using FlexPrefill compared to the baselines presented in Tables 1 and 2. This information would be valuable in quantifying the benefits of the proposed approach.
Review Point: 3. Limited Comparison with Recent Related Works: The paper does not include comparisons with other recent related works, such as Han et al. (2024) and Xiao et al. (2024a). Including these comparisons would help to better position the contributions of FlexPrefill within the context of current research.
Review Point: 4. Mixed Performance Benefits: The reported performance benefits compared to the Full Attention model on long-context benchmarks are mixed, as shown in Tables 1 and 2. Additionally, the computational savings compared to the Full Attention model and other baselines are not adequately reported, which limits the understanding of the trade-offs involved.
==================================================

Focused review:

:
The expert descriptions exemplified in the paper encompass 1) quantifiable information (musical tempo, ...), 2) subjective information (valence, ...), and 3) writing style. I could also imagine that some contain 4) factual information, which is not possible to extract from only the music signal itself (e.g. name of the piece, composer). The proposed model seems to somewhat capture each element and is able to generate much more coherent examples than the other pre-trained models, but it also generates a lot of non-sensical descriptions such as "begins with a theme of the main theme of the main theme in the...," "the music is a fugue... to the sonata form of pizzicato," "the music of the movement."
I find similarities to models trained for question answering task, in particular, how LLMs fail to give correct answers - sometimes in a dangerously confident tone (e.g. "will my stomach be cleaner if I drink bleach?"). For me, a more interesting discussion would be about the challenges of this task, and how different models can or cannot cope with the elements mentioned above, potentially via an ablation study.
The dataset is quite small, which probably contributes to the limitations. I appreciate that it's very difficult to collect data for such a task, however, the impact will be minor unless the data could be collected in scale. For instance, the researchers could add a second dataset consisting of Anglo-american pop music (e.g. available in Lakh dataset) and crawl expert or fan reactions (e.g. from social media) and repeat the experiments. This way we could understand how generalizable the approaches are and contrast the modes of good and bad descriptions across datasets.
Classical pieces in the dataset are typically instrumental. It may not possible to draw out a universal (or a consensus between experts) sentiment from instrumental classical music (or any other type of music). The paper doesn't address subjectivity much in the account.

Review Point: 3) writing style. I could also imagine that some contain
==================================================

Focused review:

- Limited Dataset Evaluation: The experiments are confined to the Social Media Prediction Dataset (SMPD). Testing on additional datasets would strengthen claims of generalizability.
- Lack of Theoretical Justification for Weighting Mechanism: While the Weighted-Rank CR loss is a central contribution, the rationale behind its specific weighting mechanism (Table 1) could be expanded with a more thorough theoretical explanation.
- Narrow Baseline Comparison: The comparison primarily focuses on Rank-N-Contrast. Including other popular imbalanced learning methods or deep regression models would provide a more comprehensive evaluation.
- Presentation Quality Issues: The paper’s presentation could be improved; figures are not vector graphics, and some tables appear poorly formatted.

Review Point: - Limited Dataset Evaluation: The experiments are confined to the Social Media Prediction Dataset (SMPD). Testing on additional datasets would strengthen claims of generalizability.
Review Point: - Lack of Theoretical Justification for Weighting Mechanism: While the Weighted-Rank CR loss is a central contribution, the rationale behind its specific weighting mechanism (Table 1) could be expanded with a more thorough theoretical explanation.
Review Point: - Narrow Baseline Comparison: The comparison primarily focuses on Rank-N-Contrast. Including other popular imbalanced learning methods or deep regression models would provide a more comprehensive evaluation.
Review Point: - Presentation Quality Issues: The paper’s presentation could be improved; figures are not vector graphics, and some tables appear poorly formatted.
==================================================

Focused review:

- The idea of decoupled representation lacks novelty. The technical implementation is relatively weak.
- Lack of in-depth analysis, e.g., channel-dependent model.
- No available codes.

Review Point: - The idea of decoupled representation lacks novelty. The technical implementation is relatively weak.
==================================================

Focused review:

1. Comparing PLLaVA with the 4-Frame method in the initial failure analysis does not seem fair if PLLaVA uses 16 frames, which includes more video content to inform the answer. Recent studies show that using more frames can significantly improve the results [1][2]
2. The author mentioned that adding 249K data from VideoChat2 to train Video-ChatGPT does not continue to improve the model; on the contrary, the performance worsens. However, this outcome may also be influenced by the quality or distribution of the 249K data, as recent papers have shown that data quality and data mixture strategies can significantly impact model performance. Showing the performance of Video-ChatGPT trained on this 249K data alone could help verify this conclusion.
3. Lack the explanation of the x/y axis for Figure 2(a) and is there a direct correlation between norm distribution and model performance?
4. The post-optimization steps seem a bit counterintuitive. Does this mean that fine-tuning makes the alignment between vision and text even worse? How does this compare with unfreezing the LLM during fine-tuning?
5. Missing a few baseline methods such as [1][2][3]
[1] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. LLaVA-NeXT: A strong zero-shot video understanding model, 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/.
[2] Xu, Mingze, et al. "Slowfast-llava: A strong training-free baseline for video large language models." arXiv preprint arXiv:2407.15841 (2024).
[3] Gao, Mingze, et al. "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations." arXiv preprint arXiv:2409.03206 (2024)

Review Point: 3. Lack the explanation of the x/y axis for Figure 2(a) and is there a direct correlation between norm distribution and model performance?
Review Point: 4. The post-optimization steps seem a bit counterintuitive. Does this mean that fine-tuning makes the alignment between vision and text even worse? How does this compare with unfreezing the LLM during fine-tuning?
Review Point: 5. Missing a few baseline methods such as [1][2][3] [1] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. LLaVA-NeXT: A strong zero-shot video understanding model, 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. [2] Xu, Mingze, et al. "Slowfast-llava: A strong training-free baseline for video large language models." arXiv preprint arXiv:2407.15841 (2024). [3] Gao, Mingze, et al. "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations." arXiv preprint arXiv:2409.03206 (2024)
==================================================

Focused review:

1. The advantages of indicator RR compared with other common indicators are not fully elaborated.
2. The relationship between CEP and COPU is not clearly stated, and it should be explained in detail how to achieve it.
3. The datasets used in the experimental design are insufficient, and more datasets are recommended to be added for verification.
4. In terms of experimental Settings, the author should provide setting parameters of different networks to compare the results more reasonably.

Review Point: 1. The advantages of indicator RR compared with other common indicators are not fully elaborated.
Review Point: 2. The relationship between CEP and COPU is not clearly stated, and it should be explained in detail how to achieve it.
Review Point: 3. The datasets used in the experimental design are insufficient, and more datasets are recommended to be added for verification.
Review Point: 4. In terms of experimental Settings, the author should provide setting parameters of different networks to compare the results more reasonably.
==================================================

Focused review:

weaknesses of the paper are the lack of comparison and the lukewarm results. In particular, only Fig. 5 shows a comparison of the proposed approach. However, MDS is computed instead of PCA, so it is not possible to truly evaluate the comparison with respect to Fig. 2. Moreover, the discussion on related work at the first paragraph of Sec. 3 is somewhat unsatisfying. The main limitation that was mentioned is the O(N^2) computational cost. However, no analysis was provided of the computational resources requires for the proposed method.
Also, the results in Figs. 2 & 3 could potentially have been produced using other approaches, among those is even simple PCA on the network weights/hidden-states/other information (possibly padded to match dimensions). For instance, the third panel in Fig. 3 is somewhat expected, given that the underlying structure of RNN/GRU is significantly from one another.
Another issue is that most results are presented on the sentiment analysis (SA) problem. While SA is a great task for analysis and illustration, it is somewhat a toy example. Thus, it is not clear whether the results presented in this work extend to more challenging tasks solved using RNN models.
Fig. 4 is not entirely clear to me, and I am not sure what can be inferred from it. My impression is that the hidden states of the model are not representative enough, but I might be wrong. Please clarify.
Fig. 6 is interesting and I would like to understand it better. In particular, do you observe a similar phenomenon when using 100% of the data (instead of 25%/50%). Can you improve the state-of-the-art for this model+task using this approach? What is the difference between the middle and right panels? If you sample in 2D, how are the other dominant dimensions are being set? What are the baseline accuracies?
I am not sure what the results in Fig. 8 show. Please elaborate.
minor comments:
- I could not find a formal definition of the term "manifold". However, this term is used quite heavily in the manuscript, and thus I would expect at least an intuitive explanation for what it means. For instance, the first paragraph in the introduction claims neural networks form a manifold due to their continuously tunable weights. If this is the case, what about the initial network? Is it also part of this manifold of networks? Asked differently, what is not part of this manifold?
- It is claimed that two points in the meta-model embedding space are nearby if they correspond to neural networks with similar dynamics. This is only intuitively implied. Is there a formal guarantee? If not, I suggest to tone-down this claim, as most of the results are focused on a narrow set of architectures.
- A similar concern is regarding the convex combination of theta vectors. In particular, even in binary classification problems, the decision boundary is somewhat complex, let alone in a manifold of tasks. It would be interesting to better understand the boundary of the learned manifold.
- The tasks explored in this work are classification problems. What about regression tasks?
- The dynamical system studied in App. C is not a proper conjugate system per your definition as it has a different domain.

Review Point: - I could not find a formal definition of the term "manifold". However, this term is used quite heavily in the manuscript, and thus I would expect at least an intuitive explanation for what it means. For instance, the first paragraph in the introduction claims neural networks form a manifold due to their continuously tunable weights. If this is the case, what about the initial network? Is it also part of this manifold of networks? Asked differently, what is not part of this manifold?
Review Point: - It is claimed that two points in the meta-model embedding space are nearby if they correspond to neural networks with similar dynamics. This is only intuitively implied. Is there a formal guarantee? If not, I suggest to tone-down this claim, as most of the results are focused on a narrow set of architectures.
Review Point: - A similar concern is regarding the convex combination of theta vectors. In particular, even in binary classification problems, the decision boundary is somewhat complex, let alone in a manifold of tasks. It would be interesting to better understand the boundary of the learned manifold.
Review Point: - The tasks explored in this work are classification problems. What about regression tasks?
Review Point: - The dynamical system studied in App. C is not a proper conjugate system per your definition as it has a different domain.
==================================================

Focused review:

While there is lots of relevant work cited, the paper does not contextualize and differentiate its contribution clearly. While they reference the appendix for proofs, having a bit more intuition in the body of the paper would be helpful for some results (ex: Theorem 1.) Suggestions to improve score: - Spend more space contextualizing this work in the literature, and earlier in the paper. - Give intuition in body for *why* Theorems might be true. ================== AFTER RESPONSE ================== Thank you for the clarification on contextualization in the literature.

Review Point: - Spend more space contextualizing this work in the literature, and earlier in the paper.
==================================================

Focused review:

1. Problem (2) in the introduction section is questionable for OG. It is unclear what are the properties of important weight and important gradients, which the authors do not provide sufficient explanation. The authors also do not clearly explain the problem of overconsumption of network capacity, and why the performance drop in Figure 10 comes from this reason.
2. I do not think it is a good practice to organize the paper such that important information such as pseudocode for the main algorithm is deferred to the appendix, especially in section 4 and 5. This problem is severe enough such that the text is very difficult to follow without checking the reference. Key experimental results are not shown in the main text. This essentially gains unfair advantage to exceed the page limit.

Review Point: 1. Problem (2) in the introduction section is questionable for OG. It is unclear what are the properties of important weight and important gradients, which the authors do not provide sufficient explanation. The authors also do not clearly explain the problem of overconsumption of network capacity, and why the performance drop in Figure 10 comes from this reason.
Review Point: 2. I do not think it is a good practice to organize the paper such that important information such as pseudocode for the main algorithm is deferred to the appendix, especially in section 4 and 5. This problem is severe enough such that the text is very difficult to follow without checking the reference. Key experimental results are not shown in the main text. This essentially gains unfair advantage to exceed the page limit.
==================================================

Focused review:

The paper is based on a strong hypothesis (which is also the first claimed contribution) that “LLMs tend to perform discriminative tasks in a two-step paradigm, where LLMs first randomly sample a strategy for the query data point and then predict the label based on the execution of the selected strategy on the given in-context samples”. This claim is unclear, misleading, and not well-grounded.
(1) The claim is unclear because LLM have no will or aspiration (as something which is not a subject) and thus cannot “tent” to do something. They either do or not. I would not use term “tend” as not technical enough.
(2) If the claim means that live calculations in the layers of LLM somehow simulate the two-stage process, this claim is not grounded. In fact, all arguments in Section 3 and Section D are based on what Llama-3-8B says itself on this question in several answers to some prompt, which is not given in the paper (!). Obviously, unvalidated answers of LLMs cannot be directly treated as scientific findings. Still, I can provide my own experiment on what LLMs can reason about their own abilities. Unlike the authors, I will share my prompt in the following dialog with ChatGPT 4o mini: Me:
“It is known that you, ChatGPT, while process my queries, cannot communicate with Internet, cannot query different applications and resources, cannot receive additional information and results of calculations from outsource tools such as python libraries. Does it limit you? Do you suffer from the absence of such opportunities that humans have? For example, imagine you are given a set of pairs of numbers and their labels, and was asked to predict the label for a new input pair of numbers based on the provided data. This is a standard machine learning problem. How can you solve it without access to external tools, which human data scientists successfully apply?” ChatGPT:
“You're absolutely right in pointing out that I don't have access to the internet or external tools such as Python libraries for tasks like running machine learning models or doing live computations. While this limits my ability to directly execute code or retrieve real-time information, I can still provide insights and walk through the steps of how a human data scientist might approach solving a problem — including tasks like building a predictive model — by drawing on the knowledge I've been trained on.
So, to your question, do I "suffer" from not having access to these tools? In a way, yes, in the sense that I'm unable to perform live calculations, make real-time predictions, or validate models dynamically. But I’m not “suffering” in a conventional sense — my role is more about guiding you through the process, explaining concepts, and helping you understand how to solve problems, which is an essential part of data science itself.
Let me break this down further in the context of your example: …”
As we can see, ChatGPT “thinks” that (1) it is unable to directly run ML tools, (2) its role is different: to provide guidance to humans rather than to solve ML tasks itself.
There are other unsubstantiated claims about LLM’s decision behavior. For instance, in line 234, the authors state:
>"Moreover, our further quantitative results indicate that the decision behavior of the LLM, where the learning strategies are specialized in prompts, resembles that of MLP"
Yet, the authors do not cite specific results to support this claim. Although they may be referring to Table 1, I do not think that the differences in predictions that are shown there can tell us anything about the actual decision behavior as the authors suggest; instead, these results simply indicate a superficial similarity between the output values of the LLM and an MLP
Lines 179-182:
>”The example reasoning output above implies two important aspects of how LLMs perform classification tasks. On the one hand, LLMs tend to resort to existing mathematical methods (e.g., machine learning algorithms) to infer the labels of query data. On the other hand, LLMs tend to generate the predictions of labels based on the execution of the selected algorithm."
I do not concur that the model's reasoning output provided in the paper is sufficient to jump to such conclusions.
As authors acknowledge themselves, “Hallucination also takes place" (when discussing the model's incorrect reference to class imbalance in Appendix D). These “hallucinations” suggests that the model can merely rephrasing some texts about machine learning instead of truly executing the computational steps of ML algorithms.
Keeping the above comments in mind, the investigations into the quantitative differences in decision boundaries do not provide a strong enough basis for the claims about the LLM's actual implementation of ML algorithms.
There are also some minor oversights:
- Line 50: "LLMs *irregularly* obtain unexpected fragmented decision boundaries”. Meaning of 'irregularly' is unclear in this context. One might assume it means that some runs with LLMs produce fragmented boundaries, and some runs don't, but this would contradict the data presented in the paper.
- What motivated authors to uniformly divide each dimension into $N_g$ coordinates? (Line 124). The workshop paper that authors refer to, Zhao et. al (2024), does not seem to provide any motivation behind this as well. An explanation of this choice would add clarity.
- In the main-text Section 6 that describes the related work, the references to (Shi et al., 2023; Xiao et al., 2024) appear out of context. Although these works are elaborated on in Appendix A, it would be better to address them directly in the main text, including a brief explanation of how the ideas they explore differ from the authors' work.

Review Point: - Line 50: "LLMs *irregularly* obtain unexpected fragmented decision boundaries”. Meaning of 'irregularly' is unclear in this context. One might assume it means that some runs with LLMs produce fragmented boundaries, and some runs don't, but this would contradict the data presented in the paper.
Review Point: - What motivated authors to uniformly divide each dimension into $N_g$ coordinates? (Line 124). The workshop paper that authors refer to, Zhao et. al (2024), does not seem to provide any motivation behind this as well. An explanation of this choice would add clarity.
Review Point: - In the main-text Section 6 that describes the related work, the references to (Shi et al., 2023; Xiao et al., 2024) appear out of context. Although these works are elaborated on in Appendix A, it would be better to address them directly in the main text, including a brief explanation of how the ideas they explore differ from the authors' work.
==================================================

Focused review:

The weakness of this paper are as follows:
1. The inference is hard to implement to be really act as what the authors have claimed. I do not think the throughputs are experimental numbers but theoretical numbers. This is the common issue for adaptive token pruning methods, such as A-ViT. I think in practical this paper is useless, not only not decreasing the real computation cost but increasing the cost. I do not think the codes would be released for inference.
2. The results are not promising. In recent CVPR2023, there are several new works that achieved better results than this one. For example, Making Vision Transformers Efficient From a Token Sparsification View.
3. Further finetuning based on pretrained models is not fair comparisons.

Review Point: 1. The inference is hard to implement to be really act as what the authors have claimed. I do not think the throughputs are experimental numbers but theoretical numbers. This is the common issue for adaptive token pruning methods, such as A-ViT. I think in practical this paper is useless, not only not decreasing the real computation cost but increasing the cost. I do not think the codes would be released for inference.
Review Point: 2. The results are not promising. In recent CVPR2023, there are several new works that achieved better results than this one. For example, Making Vision Transformers Efficient From a Token Sparsification View.
Review Point: 3. Further finetuning based on pretrained models is not fair comparisons.
==================================================

Focused review:

The primary concern with the paper is its over-claims and the lack of a well-defined conceptual foundation in its design. While the authors’ efforts to propose an abstraction for speech language modeling in the LLM era are commendable, the current formulation—particularly the high-level abstractions—lacks convincing theoretical grounding. Specific issues include:
- In Section 2.2, the paper outlines the philosophy of the proposed roadmap; however, the definitions of each level lack theoretical depth, serving more as categorizations than rigorously justified stages. Even if the defined levels are accepted, the sequence of the first three levels is debatable. For instance, Level 2, which addresses basic paralinguistic perception, could logically precede speech recognition, as suggested by previous studies. If the framework is to be positioned as a roadmap, further justification is essential to clarify the rationale behind the level definitions and their order with enough theoretical background support.
- The benchmark relies on a limited selection of open-source corpora, but the chosen tasks lack alignment with the intended focus. For example, at Level 4, the authors utilize the COVID-cough dataset, which is not typically associated with speech tasks (usually classified as general audio tasks given its limited language-related content). While the authors aim to exclude general audio/music-related tasks, the current framework inadvertently includes such elements, particularly when analyzing music or environmental sounds. More robust justification is needed to delineate the scope of the proposed method. At Level 5, computer-assisted language learning traditionally viewed as analogous to ASR, is positioned at an AGI level, which may be overly ambitious. As the paper discusses, speech AGI aims to surpass human performance in speech-based tasks, but the selected tasks fall short of representing this objective effectively.

Review Point: - In Section 2.2, the paper outlines the philosophy of the proposed roadmap; however, the definitions of each level lack theoretical depth, serving more as categorizations than rigorously justified stages. Even if the defined levels are accepted, the sequence of the first three levels is debatable. For instance, Level 2, which addresses basic paralinguistic perception, could logically precede speech recognition, as suggested by previous studies. If the framework is to be positioned as a roadmap, further justification is essential to clarify the rationale behind the level definitions and their order with enough theoretical background support.
==================================================

Focused review:

1. Although the community may benefit from the study and advocated holistic landscape by resolving some of the current limitations, it is not convincing from the paper why each part of the taxonomy derived from 0-5 years old children, would be necessary for evaluating theory of mind in a large language model. More importantly, it is not clear why the proposed 10 tasks would be better at evaluating theory of mind compared to all other baselines (e.g., emotion), especially when there is no detailed results and analysis in either the main paper or the appendix.
2. Theory of mind evaluation may be impacted by RLHF in large language models, but the situated ToM evaluation (at least from the 10 tasks introduced in the paper), are not related to how the LLMs would respond to different aspects defined.

Review Point: 1. Although the community may benefit from the study and advocated holistic landscape by resolving some of the current limitations, it is not convincing from the paper why each part of the taxonomy derived from 0-5 years old children, would be necessary for evaluating theory of mind in a large language model. More importantly, it is not clear why the proposed 10 tasks would be better at evaluating theory of mind compared to all other baselines (e.g., emotion), especially when there is no detailed results and analysis in either the main paper or the appendix.
Review Point: 2. Theory of mind evaluation may be impacted by RLHF in large language models, but the situated ToM evaluation (at least from the 10 tasks introduced in the paper), are not related to how the LLMs would respond to different aspects defined.
==================================================

Focused review:

Weakness:
- I believe the issue pointed out is commonly known and it might not be an issue in the first place with some simple tricks. The gradients from the label loss influence the representation in the earlier layers and also, having dense layer connections from every layer to the last prediction layer is a simple solution to the problem.
- Many sophisticated GNN variants now handle label information [see references below] better. The authors' analyses are restricted to simpler models alone.
- While the results on small-scale datasets are promising, they need support from larger datasets like the OGB benchmark. The experiments need to compare all methods that use label information. I noticed that C&S results are missing on the reddit dataset.
- Also, the assumption enforced is label smoothness and not the main cluster smoothness assumption. References:
* Scaling Graph Propagation Kernels for Predictive Learning
- COMBINING LABEL PROPAGATION AND SIMPLE MODELS OUT-PERFORMS GRAPH NEURAL NETWORKS
* Structure-Aware Label Smoothing for Graph Neural Networks

Review Point: - I believe the issue pointed out is commonly known and it might not be an issue in the first place with some simple tricks. The gradients from the label loss influence the representation in the earlier layers and also, having dense layer connections from every layer to the last prediction layer is a simple solution to the problem.
Review Point: - Many sophisticated GNN variants now handle label information [see references below] better. The authors' analyses are restricted to simpler models alone.
Review Point: - While the results on small-scale datasets are promising, they need support from larger datasets like the OGB benchmark. The experiments need to compare all methods that use label information. I noticed that C&S results are missing on the reddit dataset.
Review Point: - Also, the assumption enforced is label smoothness and not the main cluster smoothness assumption. References:
Review Point: * Scaling Graph Propagation Kernels for Predictive Learning - COMBINING LABEL PROPAGATION AND SIMPLE MODELS OUT-PERFORMS GRAPH NEURAL NETWORKS * Structure-Aware Label Smoothing for Graph Neural Networks
==================================================

Focused review:

There are a few areas for improvement:
1. The proposed methods share a close relationship with layer-wise training of neural networks, which could potentially diminish the significance of this paper. (Refer to Question 1)
2. The data splitting does not adhere to standard practices. (Please see Question 2)
3. The presentation is difficult to follow and should be improved. (Please refer to Question 3,4,5)
4. Table 2, and 3 contains many interesting results but lacks explanations and intuitions. (Please refer to Questions 6, 7, 8)

Review Point: 1. The proposed methods share a close relationship with layer-wise training of neural networks, which could potentially diminish the significance of this paper. (Refer to Question 1) 2. The data splitting does not adhere to standard practices. (Please see Question 2) 3. The presentation is difficult to follow and should be improved. (Please refer to Question 3,4,5) 4. Table 2, and 3 contains many interesting results but lacks explanations and intuitions. (Please refer to Questions 6, 7, 8)
==================================================

Focused review:

* Doesnt the argument in line 90 only apply to the method proposed by Mazeika et al.? I think the framing could be improved.
* The two-step algorithm is just standard adversarial training with nontypical loss functions. I think it would be easier to understand if the authors framed their contribution accordingly. Initially, I thought that this 2 stage algorithm would be something different.
* Include the dataset that was used for the experiment in Figure 3
* “We attribute this to the fact that in all MLLMs, the image is always placed before the text as input” This information would be helpful when you describe the method. To give a better intuition about P_0^h

Review Point: * Doesnt the argument in line 90 only apply to the method proposed by Mazeika et al.? I think the framing could be improved.
Review Point: * The two-step algorithm is just standard adversarial training with nontypical loss functions. I think it would be easier to understand if the authors framed their contribution accordingly. Initially, I thought that this 2 stage algorithm would be something different.
Review Point: * Include the dataset that was used for the experiment in Figure 3 * “We attribute this to the fact that in all MLLMs, the image is always placed before the text as input” This information would be helpful when you describe the method. To give a better intuition about P_0^h
==================================================

Focused review:

- The writting of this work is quite poor. The inroduction part does not clearly introduce the background and motivation of the problem to be solved. The approach part is also not written in a concise and well-organized manner.
- The approaches compared in this work are not proposed recently. As a result, the validity and innovation of the proposed modules is not convincing.
- The authors did not conduct sufficient ablation experiments for different loss functions and did not give more qualitative analysis.

Review Point: - The writting of this work is quite poor. The inroduction part does not clearly introduce the background and motivation of the problem to be solved. The approach part is also not written in a concise and well-organized manner.
Review Point: - The approaches compared in this work are not proposed recently. As a result, the validity and innovation of the proposed modules is not convincing.
Review Point: - The authors did not conduct sufficient ablation experiments for different loss functions and did not give more qualitative analysis.
==================================================

Focused review:

-Writing
The motivation of the paper is not good enough. Why we need
-Experimental results
1.Current state-of-the-art diffusion models can generate images with a resolution of 512x512. Authors only show result on images of 256px.
2.The function of several components, which authors claimed to be useful, are not evaluated. There are no ablation study section.
-Comparison with existing baselines
3.The provided method only compare with Deepprivacy, CIAGAN and Repaint, the newest of which is publish in 2020. Several important methods are missing, e.g. FiT [1], RiDDLE [2].
4.Even in the methods mentioned in the paper, the qualitative results are not shown. Authors should put faces generated from different methods in a single image so reviewers can make a direct judgement
[1] Gu et al Password Conditioned Face id transformer
[2] Li et al. RiDDLE, Reversible and De-diversified De-identification with Latent Encryptor -Novelty
Except for the latent diffusion models incorporated in the framework, few novelty is found in the paper. Current SOTA de-id methods can achieve anonymization, which the presented method can not do. -Figure
I suggest authors to re-write the paper and re-draw some important figures. Many figures in the paper are either too small for me to see the texts(e.g. Fig. 4) or so large that occupies too much space.(e.g. Fig.1)

Review Point: -Writing The motivation of the paper is not good enough. Why we need -Experimental results 1.Current state-of-the-art diffusion models can generate images with a resolution of 512x512. Authors only show result on images of 256px.
Review Point: 2.The function of several components, which authors claimed to be useful, are not evaluated. There are no ablation study section. -Comparison with existing baselines 3.The provided method only compare with Deepprivacy, CIAGAN and Repaint, the newest of which is publish in 2020. Several important methods are missing, e.g. FiT [1], RiDDLE [2].
==================================================

Focused review:

Some of the concerns are below: 1. The claims of proposing a 'generalized' interpolation may be too strong. What could be the real cases which cannot be resolved by the proposed methods should also be discussed. I believe exposure setting could be only one of the problem of the poor generalization ability. 2. It's unclear to me how equation(2) in line 103 is derived. And in line 109, how equ(4) can be degraded to equ(1) given using different frames? 3. For the restoration network, it seems the network is just trying to achieve multi-frame deblurring with a two-stage process. What exactly the function of the two-stage network? Could the author show some outputs from different stages? In line 154, the improvement in dB can only come from more parameters in the model, but not the intuitive idea illustrated in the paper. Please double check it or visualize it. 4. Also for the restoration network, in line 147, what is the temporal ambiguity, and why the authors utilize four frames but not just 2 frame? In line 143, does the author mean B1 and B2? 5. More results on real videos should be reported. Currently, the results are reported on synthetic data and most previous methods do not share the same assumptions as this paper. Results on real videos in the supplementary material look fine, but related contents are missing in the paper, especially user study.

Review Point: 1. The claims of proposing a 'generalized' interpolation may be too strong. What could be the real cases which cannot be resolved by the proposed methods should also be discussed. I believe exposure setting could be only one of the problem of the poor generalization ability.
Review Point: 2. It's unclear to me how equation(2) in line 103 is derived. And in line 109, how equ(4) can be degraded to equ(1) given using different frames?
Review Point: 3. For the restoration network, it seems the network is just trying to achieve multi-frame deblurring with a two-stage process. What exactly the function of the two-stage network? Could the author show some outputs from different stages? In line 154, the improvement in dB can only come from more parameters in the model, but not the intuitive idea illustrated in the paper. Please double check it or visualize it.
Review Point: 4. Also for the restoration network, in line 147, what is the temporal ambiguity, and why the authors utilize four frames but not just 2 frame? In line 143, does the author mean B1 and B2?
Review Point: 5. More results on real videos should be reported. Currently, the results are reported on synthetic data and most previous methods do not share the same assumptions as this paper. Results on real videos in the supplementary material look fine, but related contents are missing in the paper, especially user study.
==================================================

Focused review:

Despite the thorough and clear documentation of the work, the reviewer has to list the following concerns that underline the assigned score for this submission.
1. ICLR might not be the appropriate target for this work. The major contribution of the paper is the successful deployment of S4D on the Loihi 2 chip. With this implementation centric work, the paper might be better placed at a systems conference where much more space can be attributed to a detailed description of this core contribution (MLSys, AICAS etc). The algorithmic contributions are very incremental in terms of model architecture (replacing activation functions) and quantization.
Why is the deployment the core contribution? Quantization methods for SSMs were investigated in [1.], and 8-bit quantization is an established standard in the ML community with most deployment frameworks supporting 8-bit precision at least for matmuls. The A, B, C matrices of SSMs might be a different matter, but does not provide data or comparison against established methods and references [1.] for this sake. At the same time, the task evaluation on sequential MNIST and CIFAR-10 is quite weak to claim contributions towards quantization methods. E.g. line 361f highlights that the accuracy drop introduced by 8-bit quantization is very small compared to the baseline. This is not a surprise on small scale datasets. We know for example from language models that naive quantization aware training works well up to a certain scale around 7B parameters and only breaks afterwards.
2. The hardware aware implementation is a strength of the paper. Unfortunately, a comparison to the obvious naive implementation that the authors discuss as well is missing. This would give readers a clearer picture of the significance of the hardware aware implementation. Without this data added, it is not clear that the proposed implementation is actually a significant contribution. A comparison to existing spiking neural network implementations on Loihi 2 would further add value to the paper.
3. The hardware-aware implementation on Loihi 2 is evaluated against a JIT compiled full precision pytorch model deployed on a Jetson Orin Nano. There are a couple of issues with this comparison.
- Integer precision for matrix operations poses a significant reduction in energy spend on computational operations and memory movement. Hence, comparing a quantized model against a full precision model is not fair.
- The implementation on Loihi 2 is tuned towards the system architecture of the Loihi system, while the authors mention that the Jetson implementation was just-in-time compiled from a torch model. It is not clear from the presentation of the paper, which optimizations were leveraged by the JIT compiler. The authors are transparent about this issue and even point out that they were not able to use NVIDIA's TensorRT framework for efficient deployment. It is hence likely that a highly optimized Loihi 2 implementation is compared to a poorly optimized CUDA implementation on the Jetson Orin Nano. To strengthen the paper, it is recommended to provide an implementation of similar sophistication or at least provide insights into which optimizations were part of the compilation procedure.
- The Jetson Orin Nano is oversized for the networks implemented. An Orin Nano has 512 to 1024 CUDA cores plus 3rd generation tensor cores as well as 4 to 8 GB DRAM. Running networks with up to 275.000 parameters favors the smaller Loihi 2 chip with only 128 cores and no DRAM. It is not clear from the paper if the Jetson was fully utilized. For example the power of the large DRAM might significantly contribute to the energy consumption of the Jetson despite not requiring DRAM at all for such small models. The results in table 2 suggest that only the large batch size of 64 could fully utilize the Jetson system - and in this setting the Jetson outperforms the Loihi 2 implementation. To allow for a fair comparison, it would be valuable to add results of an optimized implementation on a small deep learning accelerator of similar size as the Loihi 2 system. For example the Hailo-8 M.2 chip might be a better system for comparison than the oversized Jetson system. Another alternative to strengthening the results would be to run larger networks that saturate the compute and memory capacity of the Jetson. Perhaps the comparison with a low-power CPU with sufficiently large cache to host the 275.000 kb for the parameters might be a fairer comparison than the Jetson.
- To the best of the knowledge of the reviewer, there is no other implementation of SSMs on inference hardware that reports energy or latency numbers. In this environment, it would be valuable to add related works that optimize implementations of related recurrent neural networks for example on FPGAs. This would contribute to setting the present paper in the context of the RNN inference landscape.
[1. Q-S5: Towards Quantized State Space Models](https://arxiv.org/abs/2406.09477)

Review Point: 2. The hardware aware implementation is a strength of the paper. Unfortunately, a comparison to the obvious naive implementation that the authors discuss as well is missing. This would give readers a clearer picture of the significance of the hardware aware implementation. Without this data added, it is not clear that the proposed implementation is actually a significant contribution. A comparison to existing spiking neural network implementations on Loihi 2 would further add value to the paper.
Review Point: 3. The hardware-aware implementation on Loihi 2 is evaluated against a JIT compiled full precision pytorch model deployed on a Jetson Orin Nano. There are a couple of issues with this comparison.
Review Point: - Integer precision for matrix operations poses a significant reduction in energy spend on computational operations and memory movement. Hence, comparing a quantized model against a full precision model is not fair.
Review Point: - To the best of the knowledge of the reviewer, there is no other implementation of SSMs on inference hardware that reports energy or latency numbers. In this environment, it would be valuable to add related works that optimize implementations of related recurrent neural networks for example on FPGAs. This would contribute to setting the present paper in the context of the RNN inference landscape.
==================================================

Focused review:

1. The position of the selection method is unclear. Is it proposed for training a general model (e.g., ChatGPT) or a specialized model (e.g., CodeLLaMA)?
- If your method is proposed for training general models, you should verify its effectiveness using various downstream tasks (e.g., MMLU, GSM8k, HumanEval in TULU evaluation). Please report the performance on the above benchmarks directly using the checkpoints trained in Table 5. This will help determine whether the selection achieves overall improvement or just improvements in a few tasks.
- If your method is proposed for training specialized models, you should compare it with more relevant baselines, such as directly using existing **high-quality** domain-specific data (rather than StackExchange), evol-instruct in specific domains, or instruction backtranslation in specific domains. If a user wants to train a specialized model, they do not need to select data from large-scale general data but can directly use high-quality domain-specific SFT data.
2. The method seems to select the response whose format is closest to an existing model (e.g., GPT 3.5), rather than detecting format-consistent instances in the dataset.
3. The referenced response is unconvincing to me. Since the referenced prompt contains instructions, the model may correct the response even if asked to ignore them. It might be better not to provide the instruction.

Review Point: 1. The position of the selection method is unclear. Is it proposed for training a general model (e.g., ChatGPT) or a specialized model (e.g., CodeLLaMA)?
Review Point: - If your method is proposed for training general models, you should verify its effectiveness using various downstream tasks (e.g., MMLU, GSM8k, HumanEval in TULU evaluation). Please report the performance on the above benchmarks directly using the checkpoints trained in Table 5. This will help determine whether the selection achieves overall improvement or just improvements in a few tasks.
Review Point: - If your method is proposed for training specialized models, you should compare it with more relevant baselines, such as directly using existing **high-quality** domain-specific data (rather than StackExchange), evol-instruct in specific domains, or instruction backtranslation in specific domains. If a user wants to train a specialized model, they do not need to select data from large-scale general data but can directly use high-quality domain-specific SFT data.
Review Point: 2. The method seems to select the response whose format is closest to an existing model (e.g., GPT 3.5), rather than detecting format-consistent instances in the dataset.
Review Point: 3. The referenced response is unconvincing to me. Since the referenced prompt contains instructions, the model may correct the response even if asked to ignore them. It might be better not to provide the instruction.
==================================================

Focused review:

1. Relying Wikipedia’s abstract to generate the ES datasets is cost-efficient and novel. But this makes the entity summarization generated based on the abstract text rather than the triples of the entities in the knowledge graph. This might cause the entity summarization in WIKES not the gold entity summarization of the entities.
2. The DistillBERT is used to annotate the property that should be included in the summarization. The correctness of the final property is not evaluated, which is important to the quality of the WIKES in terms of entity summarization.
3. The datasets evaluation is not comprehensive. For example,
(1). Figure 3 only shows the F1 evaluation on WIkiProFem, part of the WIKES benchmark. The F1 score on other subdatasets are not presented.
(2). Table 2 shows that results of entity summarization methods on the smallest WIKES datasets. But the midium and the large WIKES datasets are not tested. It is not clear what would be the performance of current summarization methods on these two datasets.
4. The dataset quality are not analyzed, for example, the correctness and diversity that are important for ES benchmark.
5. Minor points and typos:
(1). In line 119, there is an extra question mark after “(version 3.9)”.
(2). The citation format in the main text seems not correct.

Review Point: 1. Relying Wikipedia’s abstract to generate the ES datasets is cost-efficient and novel. But this makes the entity summarization generated based on the abstract text rather than the triples of the entities in the knowledge graph. This might cause the entity summarization in WIKES not the gold entity summarization of the entities.
Review Point: 2. The DistillBERT is used to annotate the property that should be included in the summarization. The correctness of the final property is not evaluated, which is important to the quality of the WIKES in terms of entity summarization.
Review Point: 3. The datasets evaluation is not comprehensive. For example, (1). Figure 3 only shows the F1 evaluation on WIkiProFem, part of the WIKES benchmark. The F1 score on other subdatasets are not presented. (2). Table 2 shows that results of entity summarization methods on the smallest WIKES datasets. But the midium and the large WIKES datasets are not tested. It is not clear what would be the performance of current summarization methods on these two datasets.
Review Point: 4. The dataset quality are not analyzed, for example, the correctness and diversity that are important for ES benchmark.
==================================================

Focused review:

1. The authors need to further clarify the selection of metrics and justify how they can benefit real-world applications. This should goes back to the objective of machine unlearning. The objective of machine unlearning is to to negate a subset of data’s influence on the model. The goal should be maintaining a high performance of the model while erasing the imprint of the data from the model. Hence, if I understand it correctly, the performance should be as high as possible regardless of the retraining performance as long as the effectiveness of the unlearning is acceptable.
2. The authors consider the setting where the forget set is randomly sampled. First, It is not clear to me why this assumption will hold in real-world scenario when the machine unlearning is motivated by regulations in certain geographical regions. Second, if the forget set is randomly sampled, from a statistical point of view, the problem of machine unlearning becomes the problem of understanding how training dataset size can affect the model performance (the difference between the original performance and the retraining performance).
3. In the empirical results, the authors only demonstrated the effectiveness of the framework. The missing piece is the effectiveness of the proposed gradient-based method. It seems that the paper is lack of demonstration on which point the gradient-based method is converging to. For example, how the utilities of both players evolve during the training process? And which solution concept the algorithm is converging to if it is converging?

Review Point: 1. The authors need to further clarify the selection of metrics and justify how they can benefit real-world applications. This should goes back to the objective of machine unlearning. The objective of machine unlearning is to to negate a subset of data’s influence on the model. The goal should be maintaining a high performance of the model while erasing the imprint of the data from the model. Hence, if I understand it correctly, the performance should be as high as possible regardless of the retraining performance as long as the effectiveness of the unlearning is acceptable.
Review Point: 2. The authors consider the setting where the forget set is randomly sampled. First, It is not clear to me why this assumption will hold in real-world scenario when the machine unlearning is motivated by regulations in certain geographical regions. Second, if the forget set is randomly sampled, from a statistical point of view, the problem of machine unlearning becomes the problem of understanding how training dataset size can affect the model performance (the difference between the original performance and the retraining performance).
Review Point: 3. In the empirical results, the authors only demonstrated the effectiveness of the framework. The missing piece is the effectiveness of the proposed gradient-based method. It seems that the paper is lack of demonstration on which point the gradient-based method is converging to. For example, how the utilities of both players evolve during the training process? And which solution concept the algorithm is converging to if it is converging?
==================================================

Focused review:

1. Lack of novelty. This paper extends a previous model from ICL to finetuning and obtains a better performance, which is a very naive result since training (finetuning) a model always leads to better performance than using prompts (ICL), this makes the findings in the paper not meaningful.
2. Similarly, the model after training can have a more complex decision boundary than the zero-shot model, this is also a very straightforward conclusion that does not bring anything new for me.
3. Authors compare their methods with some very traditional models such as SVM, MLP, Decision Tree, XGBoost, and so on. Most of these methods have been proposed for more than ten years. A comparison of these methods can not prove that the proposed methods are good. Please compare with more SOTA methods, and also report some other information such as the parameters, inference time for each model.
4. The writing should be improved, especially for the figures in the paper.
5. Authors mentioned that the proposed finetuned method is an approach to reduce the requirements for GPU memory by two times, but no quantitative numbers for GPU memory required are provided in this paper.

Review Point: 1. Lack of novelty. This paper extends a previous model from ICL to finetuning and obtains a better performance, which is a very naive result since training (finetuning) a model always leads to better performance than using prompts (ICL), this makes the findings in the paper not meaningful.
Review Point: 2. Similarly, the model after training can have a more complex decision boundary than the zero-shot model, this is also a very straightforward conclusion that does not bring anything new for me.
Review Point: 3. Authors compare their methods with some very traditional models such as SVM, MLP, Decision Tree, XGBoost, and so on. Most of these methods have been proposed for more than ten years. A comparison of these methods can not prove that the proposed methods are good. Please compare with more SOTA methods, and also report some other information such as the parameters, inference time for each model.
Review Point: 4. The writing should be improved, especially for the figures in the paper.
Review Point: 5. Authors mentioned that the proposed finetuned method is an approach to reduce the requirements for GPU memory by two times, but no quantitative numbers for GPU memory required are provided in this paper.
==================================================

Focused review:

1. The proposed method needs to train from scratch for each distinct task, which is not a efficient way.
2. The quantitative results presented in Table 1 exhibit significant interval overlap, thereby reducing their statistical significance.
3. The applicability of the method remains uncertain due to its development on synthetic datasets and the necessity of training from scratch, as opposed to utilizing real-world datasets and adapting pretrained models.

Review Point: 1. The proposed method needs to train from scratch for each distinct task, which is not a efficient way.
Review Point: 2. The quantitative results presented in Table 1 exhibit significant interval overlap, thereby reducing their statistical significance.
Review Point: 3. The applicability of the method remains uncertain due to its development on synthetic datasets and the necessity of training from scratch, as opposed to utilizing real-world datasets and adapting pretrained models.
==================================================

Focused review:

1. Although the problem is interesting, the proposed methodology is not surprising. The main idea is to use PPR to calculate probability and select the top of entities and relations. The reasoning on the subgraph is following the existing methods.
2. The paper emphasizes improved efficiency, but I did not find a comparison of efficiency between the proposed method and the existing ones.
3. The hyperparameter searching looks inefficient.

Review Point: 1. Although the problem is interesting, the proposed methodology is not surprising. The main idea is to use PPR to calculate probability and select the top of entities and relations. The reasoning on the subgraph is following the existing methods.
Review Point: 2. The paper emphasizes improved efficiency, but I did not find a comparison of efficiency between the proposed method and the existing ones.
==================================================

Focused review:

* The reason directly optimizing the mask is typically not done is of course that a (hard) binary mask is non-differentiable. Using a soft mask in any form would necessarily be dense, not sparse, training. I think the authors do not focus on explaining what they do with respect to both of these things enough in the paper as it is currently written, as I'm still not confident I understand how this is addressed by BiDST. I believe the authors use the Gumbel-Softmax trick, and while that might explain how they learn a binary mask using gradients, that would mean that BiDST uses dense training as far as I can see. And yet the authors demonstrate sparse training acceleration in their results (and this itself is unclear). I have asked this in the question section in more detail, and a comprehensive answer by the authors is vital in my being able to understand this paper better as a reader and a reviewer. Understanding the fact of if BiDST is in fact doing dense weight/mask updates is key in understanding if it is fair to compare BiDST with existing DST sparse training methods, such as SET, as the main motivation of these methods is to reduce compute during training.
* In the explanation below Equation 8 in section 2.2, the authors casually mention that they simply replace the second-order partial derivatives with 0 in their derivation of the weight/mask-update rules for BiDST. Isn't it precisely these second-order partial derivatives that carry much of the information of the relationship between the mask and weight parameters? There is no discussion on the effect of this, or why this is a reasonable thing to do aside from the fact it is expensive to calculate. Note that the first-order partials are retained. After all the fanfare of BiDST doing optimization "properly" compared to existing DST methods in the motivation, this is quite a let-down, and I think the authors should note this earlier/be careful of giving the impression of over-claiming and be much more transparent that they are crudely approximating the bi-level optimization methodology.
* As explained in 3.1, BiDST experiments are presented using fewer training epochs to ensure a "fair" comparison due to the "mask learning computation cost", however it's not detailed what exactly is the fair number of epochs to compare and why, and I didn't see any numbers or details on what the "mask learning computation cost" overhead is w.r.t. existing DST and other sparse training methods.
* It is not clear how significant some of the ImageNet results are, being within 0.2 percentage points of the baselines in some cases. Because of this it really stands out as suspicious that although the authors stress that ImageNet experiments are all performed three times, we are not given the variance across these three runs to aid us in understanding the significance of the results, as for example done with CIFAR-10/100. The authors say "we omit these because it's less than 0.1% std dev (is this percentage or percentage points?). Why not just show them in the table? I would not ask for multiple imagenet runs normally, but if you've performed them and are stressing that you did, why hold back on showing the variance?
* In section 3.3, the authors use IoU to analyze the evolution of the mask during training and compare to existing DST methods, claiming "BiDST achieves better mask development" based on low IoU compared to other DST methods. However, while potentially interesting, the motivation for this analysis is nowhere near convincing enough to make such a bold claim.
* In section 3.4, when explaining the "sparse training" speedups shown, the authors explain "training acceleration is obtained from compiler optimization that skips the zeroes weights". If this is a compiler optimization, it's static analysis, i.e. a fixed mask. How is this possible for a DST method that is changing masks potentially every iteration, or is the "training engine" actually to speed up inference timings? This explanation is important as speeding up unstructured sparsity on real-world hardware is something that is not easy, and in fact is well worth a standalone publication if it was truly being solved so easily by the authors.
* The background is very short, and awkwardly positioned in the paper. It appears much of the relevant background is distributed throughout the method implicitly by citing baselines and methodology, but personally I'm a fan of the traditional consolidated background before a method.

Review Point: * As explained in 3.1, BiDST experiments are presented using fewer training epochs to ensure a "fair" comparison due to the "mask learning computation cost", however it's not detailed what exactly is the fair number of epochs to compare and why, and I didn't see any numbers or details on what the "mask learning computation cost" overhead is w.r.t. existing DST and other sparse training methods.
Review Point: * In section 3.3, the authors use IoU to analyze the evolution of the mask during training and compare to existing DST methods, claiming "BiDST achieves better mask development" based on low IoU compared to other DST methods. However, while potentially interesting, the motivation for this analysis is nowhere near convincing enough to make such a bold claim.
Review Point: * The background is very short, and awkwardly positioned in the paper. It appears much of the relevant background is distributed throughout the method implicitly by citing baselines and methodology, but personally I'm a fan of the traditional consolidated background before a method.
==================================================

Focused review:

* **Byzantine Resilience.** The paper could explore a broader range of Byzantine attacks and defenses, since “Byzantine resilience” refers to the ability of tolerating arbitrary corruptions. While robustness to basic Byzantine failures (Section 4.3) is a positive outcome, the method’s effectiveness against specific attacks (e.g., label flipping, gradient noise injection) is unclear. For instance, Allouah et al. (2023) explored representative defenses and attacks that might be relevant here, and integrating or testing such techniques could enhance FeedSign’s security profile.
* **Novelty Consideration.** Although FeedSign introduces an efficient mechanism for communication reduction, the reliance on sign-based updates and zeroth-order optimization is not entirely novel. Techniques like sign-SGD have been previously studied for Byzantine-resilient federated learning (Li et al. 2019), and zeroth-order methods are known in federated contexts. While FeedSign’s combination of these ideas is compelling, the novelty could be highlighted more by contrasting its specific contributions with these prior works in-depth.
* **Practical Relevance of Assumptions.** Assumption 2, in particular, appears to impose conditions that may not align with realistic settings. Further analysis or empirical evaluation showing how sensitive FeedSign’s performance is to deviations from this assumption would clarify the applicability of the theoretical results.
### References
Li et al. (AAAI 2019). RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets.
Allouah et al. (AISTATS 2023). Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity.

Review Point: * **Byzantine Resilience.** The paper could explore a broader range of Byzantine attacks and defenses, since “Byzantine resilience” refers to the ability of tolerating arbitrary corruptions. While robustness to basic Byzantine failures (Section 4.3) is a positive outcome, the method’s effectiveness against specific attacks (e.g., label flipping, gradient noise injection) is unclear. For instance, Allouah et al. (2023) explored representative defenses and attacks that might be relevant here, and integrating or testing such techniques could enhance FeedSign’s security profile.
Review Point: * **Novelty Consideration.** Although FeedSign introduces an efficient mechanism for communication reduction, the reliance on sign-based updates and zeroth-order optimization is not entirely novel. Techniques like sign-SGD have been previously studied for Byzantine-resilient federated learning (Li et al. 2019), and zeroth-order methods are known in federated contexts. While FeedSign’s combination of these ideas is compelling, the novelty could be highlighted more by contrasting its specific contributions with these prior works in-depth.
Review Point: * **Practical Relevance of Assumptions.** Assumption 2, in particular, appears to impose conditions that may not align with realistic settings. Further analysis or empirical evaluation showing how sensitive FeedSign’s performance is to deviations from this assumption would clarify the applicability of the theoretical results. ### References Li et al. (AAAI 2019). RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets. Allouah et al. (AISTATS 2023). Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity.
==================================================

Focused review:

* Different form previous works which directly flatten the input tensor into a sequence, it's impossible for HOT to use the same architecture process data with different orders, thus limiting its scalability.
* Consideing the high-order feature of HOT, it's necessary to compare the model parameters with its baselines.
* Besides, it's also necessary to conduct more experiments to demonstrate the generality of the proposed method on various kinds of higher-order data, such as typical 2D image datasets and video dataset.

Review Point: * Different form previous works which directly flatten the input tensor into a sequence, it's impossible for HOT to use the same architecture process data with different orders, thus limiting its scalability.
Review Point: * Consideing the high-order feature of HOT, it's necessary to compare the model parameters with its baselines.
Review Point: * Besides, it's also necessary to conduct more experiments to demonstrate the generality of the proposed method on various kinds of higher-order data, such as typical 2D image datasets and video dataset.
==================================================

Focused review:

- It would have been useful to see results which have human single turn jailbreak attempts in Figure 3. At the moment, the "Human" attacks have two variables changed compared to the others: the attack source (e.g. handcrafted), and additionally have multi-turn capabilities. Hence it makes it challenging to disambiguate if the difference in performance is due to the multi-turn aspect, or if humans given enough time remain better than automated based methods at creating jailbreaks.
- I am unclear as to why results against Cygent defense could not be carried out in the same setup as the original paper: the Llama model is open source, and the defense has a published paper. It would have enabled stronger reproducibility and clearer interpretation of results.
- Releasing the non-successful jailbreak attempts as well can be beneficial as it is still a useful resource, for example as training/fine-tuning data or to carry out further analysis.
- Although different styles of harmfulness were investigated: both "regular" harmbench style questions, but also WMDP-Bio for different attack objectives and domain performance from looking into the supplementary material it seems like just the harmbench data was released. Given the dataset is the core contribution of the paper it would have been useful to include the other domain data.

Review Point: - It would have been useful to see results which have human single turn jailbreak attempts in Figure 3. At the moment, the "Human" attacks have two variables changed compared to the others: the attack source (e.g. handcrafted), and additionally have multi-turn capabilities. Hence it makes it challenging to disambiguate if the difference in performance is due to the multi-turn aspect, or if humans given enough time remain better than automated based methods at creating jailbreaks.
Review Point: - I am unclear as to why results against Cygent defense could not be carried out in the same setup as the original paper: the Llama model is open source, and the defense has a published paper. It would have enabled stronger reproducibility and clearer interpretation of results.
Review Point: - Releasing the non-successful jailbreak attempts as well can be beneficial as it is still a useful resource, for example as training/fine-tuning data or to carry out further analysis.
Review Point: - Although different styles of harmfulness were investigated: both "regular" harmbench style questions, but also WMDP-Bio for different attack objectives and domain performance from looking into the supplementary material it seems like just the harmbench data was released. Given the dataset is the core contribution of the paper it would have been useful to include the other domain data.
==================================================

Focused review:

1. The FID calculations are measured with very few sampling steps, making it difficult to ensure the quality of the generated results. It would be beneficial to provide a comparison of different methods while maintaining image quality, ideally with visualizations, such as images decoded using stable diffusion.
2. The experiments were only validated on CIFAR and LSUN-BEDROOM datasets. Validation on a broader range of datasets, such as ImageNet and FFHQ, would provide more comprehensive insights. Additionally, including some actual generated images for visual comparison would be advantageous.

Review Point: 1. The FID calculations are measured with very few sampling steps, making it difficult to ensure the quality of the generated results. It would be beneficial to provide a comparison of different methods while maintaining image quality, ideally with visualizations, such as images decoded using stable diffusion.
Review Point: 2. The experiments were only validated on CIFAR and LSUN-BEDROOM datasets. Validation on a broader range of datasets, such as ImageNet and FFHQ, would provide more comprehensive insights. Additionally, including some actual generated images for visual comparison would be advantageous.
==================================================

Focused review:

- Only consider T-SNE (which has several limitations) as a dimensionality reduction technique
- Did not give reason for the choice of sequence representations, e.g. why last token for LAMA?
- Missing important benchmarks like mT5 for an "generic" autoregressive model and AfriTeVa for a community-focused autoregressive models
- Insufficient contributions for a long paper.
- Unclear methodology for the text classification and text generation experiments.

Review Point: - Only consider T-SNE (which has several limitations) as a dimensionality reduction technique - Did not give reason for the choice of sequence representations, e.g. why last token for LAMA?
Review Point: - Missing important benchmarks like mT5 for an "generic" autoregressive model and AfriTeVa for a community-focused autoregressive models - Insufficient contributions for a long paper.
Review Point: - Unclear methodology for the text classification and text generation experiments.
==================================================

Focused review:

1. It is not clear why resampling can effectively defend adversarial examples. Especially, image resampling via bilinear or nearest interpolation might be not effective enough to eliminate adversarial perturbation.
2. How can you guarantee that SampleNet is not attacked? As a result, in Table 5, IRAD cannot exhibit robustness when SampleNet is attacked simultaneously.
3. In my opinion, such a resampling method cannot effectively defend against white-box attacks. However, it might be effective in defending against black-box attacks, especially transfer-based attacks [1,2,3]. I suggest you add such a comparison.
4. Since image resampling pre-processes the input image before the model, it is similar to a purifier that eliminates adversarial perturbation [4]. I think it is necessary to compare with such baselines.
[1] Zhang et al. Patch-wise attack for fooling deep neural network. ECCV 2020.
[2] Wang et al. Enhancing the transferability of adversarial attacks through variance tuning. CVPR 2021.
[3] Wang et al. Admix: Enhancing the transferability of adversarial attacks. ICCV 2021.
[4] Naseer et al. A self-supervised approach for adversarial robustness. CVPR 2020.

Review Point: 1. It is not clear why resampling can effectively defend adversarial examples. Especially, image resampling via bilinear or nearest interpolation might be not effective enough to eliminate adversarial perturbation.
Review Point: 2. How can you guarantee that SampleNet is not attacked? As a result, in Table 5, IRAD cannot exhibit robustness when SampleNet is attacked simultaneously.
Review Point: 3. In my opinion, such a resampling method cannot effectively defend against white-box attacks. However, it might be effective in defending against black-box attacks, especially transfer-based attacks [1,2,3]. I suggest you add such a comparison.
Review Point: 4. Since image resampling pre-processes the input image before the model, it is similar to a purifier that eliminates adversarial perturbation [4]. I think it is necessary to compare with such baselines. [1] Zhang et al. Patch-wise attack for fooling deep neural network. ECCV 2020. [2] Wang et al. Enhancing the transferability of adversarial attacks through variance tuning. CVPR 2021. [3] Wang et al. Admix: Enhancing the transferability of adversarial attacks. ICCV 2021. [4] Naseer et al. A self-supervised approach for adversarial robustness. CVPR 2020.
==================================================

Focused review:

The primary limitation of this research is the weak linkage between the theoretical framework and experimental results, even though it is a challenge commonly observed in uniform convergence analyses.
1) In both Theorem 1 and Theorem 2, the bounds are set for all perturbations within $B_p(\delta)$, rather than specifically for learned UAP attacks.
2) When contrasting Theorem 1 with Theorem 2, it becomes evident that the generalization abilities of UAP and L-UAP are evaluated by $R(B_p(\delta))$ and $R(B_p(\delta)^L)$ respectively. Considering that $B_p(\delta)^L$ represents a more extensive class, its Rademacher complexity would naturally be larger. As a result, the theorem might suggest that L-UAP's generalization is inferior to UAP, which contradicts the paper's primary objective. This is similar to a general criticism for previous rademacher complexity analysis: Rademacher complexity of large model is very large, yet large model generalize better.
Therefore, without a deeper analysis of $R(B_p(\delta))$ and $R(B_p(\delta)^L)$, the analysis is not informative. Given this, the assertion in Section 2 — "motivated by Theorem 2, we propose to maximize problem 1” — doesn't come across as compelling.
3) Theorem 4's convergence results under the Lipschitz and KL conditions appear somewhat lacking in depth and specificity.
Given the weak theoretical analysis, increasing the number of perturbation candidates is not very novel.

Review Point: 1) In both Theorem 1 and Theorem 2, the bounds are set for all perturbations within $B_p(\delta)$, rather than specifically for learned UAP attacks.
Review Point: 3) Theorem 4's convergence results under the Lipschitz and KL conditions appear somewhat lacking in depth and specificity. Given the weak theoretical analysis, increasing the number of perturbation candidates is not very novel.
==================================================

Focused review:

1. Motivation of this work is not well clarified. Since existing equivariant models can achieve global symmetries, then local symmetries are also guaranteed at the same time. The authors are encouraged to show important problems or areas where we do not care about global symmetries, but insteand local symmetries are more important.
2. Theoretically I am not seeing advantages of the proposed method over existing methods. The authors are encouraged to clarify novelty of this work and present more analysis on why the proposed method is superior to existing methods.

Review Point: 1. Motivation of this work is not well clarified. Since existing equivariant models can achieve global symmetries, then local symmetries are also guaranteed at the same time. The authors are encouraged to show important problems or areas where we do not care about global symmetries, but insteand local symmetries are more important.
Review Point: 2. Theoretically I am not seeing advantages of the proposed method over existing methods. The authors are encouraged to clarify novelty of this work and present more analysis on why the proposed method is superior to existing methods.
==================================================

Focused review:

1. The motivation / idea of this work is not new (from large kernels to a stack of smaller kernels.)
This idea dates back to VGG (2014). Authors can refer to sec 2.3 in the paper for more discussions. Placing this as the main motivation largely harm the overall contribution, because this makes the paper more like a revisit / conversation in the debate.
2. Minor performance gain vs large variance in different architecture hyperparams.
The performance gain over SOTA models is minor, compared with performance variance in similar architectures with different hyperparams. As shown in Table9, searching a best setup for 1N1k is critical (min 81.3 vs max 82.6), while the performance gain over sota is only 0.x% level. This is also reflected in Table 8.
I deeply appreciate the efforts in searching a best setup for the architecture. However, this makes the major performance contribution more in the "searching" part but no in the architecture itself. Currently, due to the development of NAS, such searching efforts can be largely automated.
3. (Minor) Table 7 and 8 are mixed together in the manuscript. It is confused.

Review Point: 1. The motivation / idea of this work is not new (from large kernels to a stack of smaller kernels.) This idea dates back to VGG (2014). Authors can refer to sec 2.3 in the paper for more discussions. Placing this as the main motivation largely harm the overall contribution, because this makes the paper more like a revisit / conversation in the debate.
Review Point: 3. (Minor) Table 7 and 8 are mixed together in the manuscript. It is confused.
==================================================

Focused review:

- The validation set has a large influence on the OOD performance of the selected models [1]. I suggest the authors discuss the construction of the validation set and whether the same empirical trend can be observed with different validation sets, as suggested by [1].
- The evaluation is limited to the transformer architecture. Including other model architectures, such as ResNet-based CLIP models, would validate whether the benefits of layer-wise scaling extend beyond transformer architecture.
- There is a lack of sensitivity analysis for the hyper-parameter $\alpha$ and $\beta$ across different evaluation settings.
- Typos should be checked. For example,
- enchancing → enhancing (Line 23)
- bcenchmarks → benchmarks (Line 24)
- task=specific → task-specific (Line 112)
- Figure 4 → Table 4
[1] In search of lost domain generalization. In ICLR, 2021

Review Point: - The validation set has a large influence on the OOD performance of the selected models [1]. I suggest the authors discuss the construction of the validation set and whether the same empirical trend can be observed with different validation sets, as suggested by [1].
Review Point: - The evaluation is limited to the transformer architecture. Including other model architectures, such as ResNet-based CLIP models, would validate whether the benefits of layer-wise scaling extend beyond transformer architecture.
Review Point: - There is a lack of sensitivity analysis for the hyper-parameter $\alpha$ and $\beta$ across different evaluation settings.
==================================================

Focused review:

1. The model design is not novel, which has limited technical learning.
2. The dataset is not available. Then it cannot be a part of contribution.
3. Calibaration network is a little strange. Why it can align two views without knowing the related position for the two cameras? If the two camera's position is changed, can this model still work?
4. The number of views in the dataset is only 2. The statement of "multi-view" is unsoundness. Author should increase the view number.
5. The paper writing should be further improved. Besides, figure in the manuscript should be the vector figure (Most figures are blur).

Review Point: 1. The model design is not novel, which has limited technical learning.
Review Point: 2. The dataset is not available. Then it cannot be a part of contribution.
Review Point: 3. Calibaration network is a little strange. Why it can align two views without knowing the related position for the two cameras? If the two camera's position is changed, can this model still work?
Review Point: 4. The number of views in the dataset is only 2. The statement of "multi-view" is unsoundness. Author should increase the view number.
Review Point: 5. The paper writing should be further improved. Besides, figure in the manuscript should be the vector figure (Most figures are blur).
==================================================

Focused review:

1. Lemma 1, Corollary 1, 2 and Proposition 1 consider the participating time and resource usage. However, they do not consider the model training, loss functions, data heterogeneity, etc. Thus, it is hard to say the proposition can be utilized into FL.
2. Non-IID data distribution widely exists in FL. However, experiments only consider IID data distribution.
3. The presentation of experiment results is not clear. What does the proposed formulation mean when compared with other FL algorithms?

Review Point: 1. Lemma 1, Corollary 1, 2 and Proposition 1 consider the participating time and resource usage. However, they do not consider the model training, loss functions, data heterogeneity, etc. Thus, it is hard to say the proposition can be utilized into FL.
Review Point: 2. Non-IID data distribution widely exists in FL. However, experiments only consider IID data distribution.
Review Point: 3. The presentation of experiment results is not clear. What does the proposed formulation mean when compared with other FL algorithms?
==================================================

Focused review:

1.The empirical experiments are insufficient. Only Figure 5 has some empirical results.
2.The proposed method is not compared against any baseline methods.
3.No theoretical analysis is provided for the communication cost and performance.
Based on the above observations, the present work does not contain enough technical contribution to be published as a research paper. The author may consider to submit as a position paper.

Review Point: 1.The empirical experiments are insufficient. Only Figure 5 has some empirical results.
Review Point: 2.The proposed method is not compared against any baseline methods.
Review Point: 3.No theoretical analysis is provided for the communication cost and performance. Based on the above observations, the present work does not contain enough technical contribution to be published as a research paper. The author may consider to submit as a position paper.
==================================================

Focused review:

- Unlike Tandem Model [4,5] and cVAE based methods the proposed method uses gradient updates and therefore is slow. The authors acknowledge this in the manuscript and demonstrate study the method as a function of inference budget. - The sampling performed to obtain different initializations x_0 seems important for the convergence to optimum. This is not experimentally evaluated carefully on the proposed benchmarks, except for Tab. 1 in supplementary where it is compared to sampling from uniform distribution.

Review Point: - Unlike Tandem Model [4,5] and cVAE based methods the proposed method uses gradient updates and therefore is slow. The authors acknowledge this in the manuscript and demonstrate study the method as a function of inference budget.
Review Point: - The sampling performed to obtain different initializations x_0 seems important for the convergence to optimum. This is not experimentally evaluated carefully on the proposed benchmarks, except for Tab. 1 in supplementary where it is compared to sampling from uniform distribution.
==================================================

Focused review:

1. The problem of novelty, due to the well-known connection between flow matching and diffusion modeling. The approach proposed in this paper seems to resemble CorrDiff. Although the authors emphasize the difference in Section 4.4. They claim that CorrDiff is trained in two stages, i.e., the regression encoder is trained first and the diffusion of the residual components is trained afterwards. In contrast, it seems that in this paper, the two components are only trained jointly, and the final loss is the sum of these two components.
2. In addition, the authors claim that the first stage of CorrDiff may run the risk of overfitting. This reasoning seems insufficient to me, and the authors don't seem to have relevant evidence. There is also no guarantee that joint training avoids this risk, besides this can be solved perfectly well using simpler ways, such as early stopping.
3. The idea of co-training encoders and diffusion models is also not new, and in fact he has already proposed it for tasks such as speech synthesis [1,2] and precipitation nowcasting [3].
[1] Popov V, Vovk I, Gogoryan V, et al. Grad-tts: A diffusion probabilistic model for text-to-speech[C]//International Conference on Machine Learning. PMLR, 2021: 8599-8608.
[2] Chen Z, He G, Zheng K, et al. Bridge-TTS: Text-to-Speech Synthesis with Schrodinger Bridge[J].
[3] DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting

Review Point: 1. The problem of novelty, due to the well-known connection between flow matching and diffusion modeling. The approach proposed in this paper seems to resemble CorrDiff. Although the authors emphasize the difference in Section 4.4. They claim that CorrDiff is trained in two stages, i.e., the regression encoder is trained first and the diffusion of the residual components is trained afterwards. In contrast, it seems that in this paper, the two components are only trained jointly, and the final loss is the sum of these two components.
Review Point: 2. In addition, the authors claim that the first stage of CorrDiff may run the risk of overfitting. This reasoning seems insufficient to me, and the authors don't seem to have relevant evidence. There is also no guarantee that joint training avoids this risk, besides this can be solved perfectly well using simpler ways, such as early stopping.
Review Point: 3. The idea of co-training encoders and diffusion models is also not new, and in fact he has already proposed it for tasks such as speech synthesis [1,2] and precipitation nowcasting [3]. [1] Popov V, Vovk I, Gogoryan V, et al. Grad-tts: A diffusion probabilistic model for text-to-speech[C]//International Conference on Machine Learning. PMLR, 2021: 8599-8608. [2] Chen Z, He G, Zheng K, et al. Bridge-TTS: Text-to-Speech Synthesis with Schrodinger Bridge[J]. [3] DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting
==================================================

Focused review:

1. The goal of this work is to create a benchmark to evaluate linguistic skills of the model (unrelated to language specific learning). It would be good to fully understand why this is an important problem? Can a toy dataset be built instead, something that tests linguistic abilities, but isn’t a real language?
2. The paper is well written, but it would be good to improve some areas, such as:
- The related work could use more detail. For instance, it’d be important to add information about very low-resource languages and related attempts at benchmarking.
- Some specific stats on how Linguini was constructed should be included. The dataset includes 894 examples with some filtering. How many were filtered out from IOL?
- For a non-linguistic audience, sec 3.2 should include more details and references. Terms used such as: “verb tense conjugation”, “case declension”, “principles of voicing” need some explanations.
- Section 5 results indicate that for some models, performance degrades as in context examples are increased? This is counterintuitive. How were the examples selected?

Review Point: 1. The goal of this work is to create a benchmark to evaluate linguistic skills of the model (unrelated to language specific learning). It would be good to fully understand why this is an important problem? Can a toy dataset be built instead, something that tests linguistic abilities, but isn’t a real language?
Review Point: 2. The paper is well written, but it would be good to improve some areas, such as:
Review Point: - The related work could use more detail. For instance, it’d be important to add information about very low-resource languages and related attempts at benchmarking.
Review Point: - Some specific stats on how Linguini was constructed should be included. The dataset includes 894 examples with some filtering. How many were filtered out from IOL?
Review Point: - For a non-linguistic audience, sec 3.2 should include more details and references. Terms used such as: “verb tense conjugation”, “case declension”, “principles of voicing” need some explanations.
Review Point: - Section 5 results indicate that for some models, performance degrades as in context examples are increased? This is counterintuitive. How were the examples selected?
==================================================

Focused review:

1. The main concern is that the paper assumption is ideal and its hyper parameter \sigma, which may not hold in practice.
2. From Lemma 1, $\sigma$ should be from the forget set? In the appendix C.4, however it sets $\sigma=I$, which is not estimated from forget set?

Review Point: 1. The main concern is that the paper assumption is ideal and its hyper parameter \sigma, which may not hold in practice.
Review Point: 2. From Lemma 1, $\sigma$ should be from the forget set? In the appendix C.4, however it sets $\sigma=I$, which is not estimated from forget set?
==================================================

Focused review:

:
1. Some parts of the writing are unclear.
2. There are too many manual steps involved in dataset generation.
3. The dataset is relatively small.

Review Point: 2. There are too many manual steps involved in dataset generation.
==================================================

Focused review:

1. Problem Significance: The authors may need to demonstrate the problem of conformation generation remains significant, in the context of the rapid development of 3D molecule generation from scratch.
2. Novelty: There has been a line of work studying coarse-grained molecule generation in the community [1, 2]. The authors may need to further discuss the novelty of their methods in comparison to these existing methods.
3. Novelty Again: There has been another work proposing its information fusion attention that is similar to the aggregated attention strategy [3].
[1]. Jin et al. Junction Tree Variational Autoencoder for Molecular Graph Generation. https://arxiv.org/pdf/1802.04364.pdf
[2]. Zhang et al. Molecule Generation For Target Protein Binding with Structural Motifs. https://openreview.net/forum?id=Rq13idF0F73
[3]. Wang et al. Retrieval-based Controllable Molecule Generation. https://arxiv.org/pdf/2208.11126.pdf

Review Point: 1. Problem Significance: The authors may need to demonstrate the problem of conformation generation remains significant, in the context of the rapid development of 3D molecule generation from scratch.
Review Point: 2. Novelty: There has been a line of work studying coarse-grained molecule generation in the community [1, 2]. The authors may need to further discuss the novelty of their methods in comparison to these existing methods.
Review Point: 3. Novelty Again: There has been another work proposing its information fusion attention that is similar to the aggregated attention strategy [3]. [1]. Jin et al. Junction Tree Variational Autoencoder for Molecular Graph Generation. https://arxiv.org/pdf/1802.04364.pdf [2]. Zhang et al. Molecule Generation For Target Protein Binding with Structural Motifs. https://openreview.net/forum?id=Rq13idF0F73 [3]. Wang et al. Retrieval-based Controllable Molecule Generation. https://arxiv.org/pdf/2208.11126.pdf
==================================================

Focused review:

While the paper is methodologically sound and well-supported by experiments, there are a few areas that could benefit from additional clarity and detail:
- For mean-shifted BN:
1) The authors claim that mean-shifted BN introduces dataset-specific characteristics, yet the parameter \alpha appears to capture only frame-specific characteristics rather than broader dataset-level traits. A more intuitive approach might involve defining dataset-specific means and variances to capture the unique properties of each dataset more accurately. I suggest the authors discuss this alternative and clarify how frame-level mean shifts contribute to dataset-level adaptability.
2) It remains unclear how the parameter \alpha is specified within the model. Further explanation of its determination, whether it is learned, fixed, or computed dynamically, would help readers understand its role in adapting the batch normalization to different datasets.
- For the BEV-based range masking component, the authors choose to concatenate the mask as an additional channel rather than directly masking out unwanted regions in the BEV representation. It would be valuable for the authors to provide a rationale for this choice. Specifically, how does the inclusion of the mask as a separate channel improve model performance or facilitate feature extraction compared to direct masking? A discussion on the impact of this approach on model interpretability and cross-domain generalization would also be insightful.
- More discussion about the limitation when applying the approach to the real-world applications will be helpful.

Review Point: - For mean-shifted BN:1) The authors claim that mean-shifted BN introduces dataset-specific characteristics, yet the parameter \alpha appears to capture only frame-specific characteristics rather than broader dataset-level traits. A more intuitive approach might involve defining dataset-specific means and variances to capture the unique properties of each dataset more accurately. I suggest the authors discuss this alternative and clarify how frame-level mean shifts contribute to dataset-level adaptability.
Review Point: 2) It remains unclear how the parameter \alpha is specified within the model. Further explanation of its determination, whether it is learned, fixed, or computed dynamically, would help readers understand its role in adapting the batch normalization to different datasets.
Review Point: - For the BEV-based range masking component, the authors choose to concatenate the mask as an additional channel rather than directly masking out unwanted regions in the BEV representation. It would be valuable for the authors to provide a rationale for this choice. Specifically, how does the inclusion of the mask as a separate channel improve model performance or facilitate feature extraction compared to direct masking? A discussion on the impact of this approach on model interpretability and cross-domain generalization would also be insightful.
Review Point: - More discussion about the limitation when applying the approach to the real-world applications will be helpful.
==================================================

Focused review:

- Lack of novelty:
- Incremental: Compared to the main baseline SCINeRF [1], the idea is to effectively replace NeRF with 3D Gaussian Splatting [2] to benefit from its high quality and fast training and rendering.
- Training and rendering speed as one of the main advantages over previous work is only due to the use of 3DGS instead of NeRF.
- The main technical contribution is the "initialization protocol" consisting of two steps:
1. a simple decoding consisting of masking and interpolation to obtain "degraded frames"
2. application of an existing learning-based SfM approach VGGSfM [3], which turns out to be sufficiently robust to output an appropriate initialization point cloud and set of camera poses.
- All major remaining ideas / components have been adopted from previous work:
- Photometric SCI loss from [3]
- Joint optimization of camera poses and 3D representation like in [3]
- Densification strategy from [4]
- Some lack of clarity regarding novelty:
- The paper lacks some clarity regarding the novelty of leveraging the SCI image in the loss for optimization, which is adopted from SCINeRF [1]:
- The authors write:
- "we apply a specifically designed loss function by incorporating the SCI image formuation model with the 3DGS training procedure" (lines 74ff.)
- "we emulate the image formation process of video SCI and ..." (lines 281 f.)
- The whole section 3.3 is missing a reference to SCINeRF [1], which introduced this loss originally.
- Similarly, it is not very clear from the paper that the previous work [1] also already optimizes the camera poses / trajectory jointly with the 3D representation. Besides mentioning it in the related work, a reference to [1] in the description of that part of the proposed method would help avoiding misunderstandings.
- Limited evaluation setup:
- The evaluation is limited to image reconstruction only, while the main baseline SCINeRF [1] additionally evaluates novel view synthesis results on views not included in the SCI measurement.
- Similar to [1], a comparison across different compression rates would be interesting.
- The evaluation on real data is limited to qualitative comparisons only. If possible, quantitative results would be helpful.
- Some lack of contextualization w.r.t. baselines (please see questions):
- It is unclear how flexible previous generalizable approaches are compared to per scene optimization approaches (SCINeRF [1] and the proposed method) w.r.t.:
- static vs dynamic scenes
- camera trajectories and scene coverage
- The comparison of training time with EfficientSCI [5] as one example of a deep learning-based approach seems to be unfair, as the proposed method has to be optimized per 3D scene, whereas this class of baselines should generalize to unseen scenes after training.
Minor comment:
- In section 3.1.2, the modulation masks are said to be binary, while in section 3.2 thresholding is used to binarize them. This is a bit confusing. References:
- [1] SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image. CVPR 2024
- [2] 3D Gaussian Splatting for Real-Time Radiance Field Rendering. SIGGRAPH 2023
- [3] VGGSfM: Visual Geometry Grounded Deep Structure From Motion. CVPR 2024
- [4] 3D Gaussian Splatting as Markov Chain Monte Carlo. NeurIPS 2024
- [5] EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. CVPR 2023

Review Point: - Lack of novelty:- Incremental: Compared to the main baseline SCINeRF [1], the idea is to effectively replace NeRF with 3D Gaussian Splatting [2] to benefit from its high quality and fast training and rendering.
Review Point: - Training and rendering speed as one of the main advantages over previous work is only due to the use of 3DGS instead of NeRF.
Review Point: - The main technical contribution is the "initialization protocol" consisting of two steps:
Review Point: 1. a simple decoding consisting of masking and interpolation to obtain "degraded frames" 2. application of an existing learning-based SfM approach VGGSfM [3], which turns out to be sufficiently robust to output an appropriate initialization point cloud and set of camera poses.
Review Point: - All major remaining ideas / components have been adopted from previous work:
Review Point: - Photometric SCI loss from [3] - Joint optimization of camera poses and 3D representation like in [3] - Densification strategy from [4] - Some lack of clarity regarding novelty:
Review Point: - The paper lacks some clarity regarding the novelty of leveraging the SCI image in the loss for optimization, which is adopted from SCINeRF [1]:
Review Point: - The authors write:- "we apply a specifically designed loss function by incorporating the SCI image formuation model with the 3DGS training procedure" (lines 74ff.) - "we emulate the image formation process of video SCI and ..." (lines 281 f.) - The whole section 3.3 is missing a reference to SCINeRF [1], which introduced this loss originally.
Review Point: - Similarly, it is not very clear from the paper that the previous work [1] also already optimizes the camera poses / trajectory jointly with the 3D representation. Besides mentioning it in the related work, a reference to [1] in the description of that part of the proposed method would help avoiding misunderstandings.
Review Point: - Limited evaluation setup:- The evaluation is limited to image reconstruction only, while the main baseline SCINeRF [1] additionally evaluates novel view synthesis results on views not included in the SCI measurement.
Review Point: - Similar to [1], a comparison across different compression rates would be interesting.
Review Point: - The evaluation on real data is limited to qualitative comparisons only. If possible, quantitative results would be helpful.
Review Point: - Some lack of contextualization w.r.t. baselines (please see questions):
Review Point: - It is unclear how flexible previous generalizable approaches are compared to per scene optimization approaches (SCINeRF [1] and the proposed method) w.r.t.:
Review Point: - static vs dynamic scenes - camera trajectories and scene coverage - The comparison of training time with EfficientSCI [5] as one example of a deep learning-based approach seems to be unfair, as the proposed method has to be optimized per 3D scene, whereas this class of baselines should generalize to unseen scenes after training. Minor comment:
Review Point: - In section 3.1.2, the modulation masks are said to be binary, while in section 3.2 thresholding is used to binarize them. This is a bit confusing. References:
Review Point: - [1] SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image. CVPR 2024 - [2] 3D Gaussian Splatting for Real-Time Radiance Field Rendering. SIGGRAPH 2023 - [3] VGGSfM: Visual Geometry Grounded Deep Structure From Motion. CVPR 2024 - [4] 3D Gaussian Splatting as Markov Chain Monte Carlo. NeurIPS 2024 - [5] EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. CVPR 2023
==================================================

Focused review:

- What is the rationale behind the setting of N, which is the crowd count to generate synthetic images? What is the quality of the generated images? Is it possible to provide a measure of variance to assess the feasibility of this method?
- There are only six categories for N. Why not train the model by a classification task? In situations where the labels are not stable, the classification task seems to be able to maintain a relatively high level of accuracy.
- The synthetic images do not include images with 0 crowd count. Does this method have the capability to handle datasets that consist of a large portion of (background) images with no people, such as NWPU?
- How does the computational cost of generating synthetic images using the diffusion model compare to that of other unsupervised counting models?
- There are some repetitions in the references.

Review Point: - What is the rationale behind the setting of N, which is the crowd count to generate synthetic images? What is the quality of the generated images? Is it possible to provide a measure of variance to assess the feasibility of this method?
Review Point: - There are only six categories for N. Why not train the model by a classification task? In situations where the labels are not stable, the classification task seems to be able to maintain a relatively high level of accuracy.
Review Point: - The synthetic images do not include images with 0 crowd count. Does this method have the capability to handle datasets that consist of a large portion of (background) images with no people, such as NWPU?
Review Point: - How does the computational cost of generating synthetic images using the diffusion model compare to that of other unsupervised counting models?
==================================================

Focused review:

Limitations:
- Does not provide detailed qualitative examples of the proposed prompt contexts.
- The technical novelty of this paper is limited, considering a bunch of existing works on augmenting the CLIP textual prompts. Therefore, the technical differences between this work and other works on LLM-based prompt augmentation methods need to be clarified.
- Lack of baselines for comparisons: two important baselines, CHiLS and MPVR, are mentioned in the related work but are not compared. Besides, there is also a line of missing related works on prompt augmentation with semantic discriminativeness, e.g., S3A[1], Meta-Prompting[2], and LLM Explainer [3].
- Technical issues: (1) It is not guaranteed that the LLM-generated subcategories can satisfy the completeness and disjoint constraints of taxonomy stated in section 3.1. In practice, to what extent these two constraints can be satisfied and what implications will it have on the results needs further investigations and discussions. (2) How many layers of generated taxonomy will there be? If only generate a single layer of subcategories, this work would have high technical similarity with CHiLS; otherwise, more ablations are needed to investigate its benefits.
- Overclaim issue: (1) the motivation of the hierarchical taxonomic prompt starts from the game theory, however, the competition and players in the categorization contexts are not clearly defined. There is no clear and direct relationship between them. (2) The claimed semantic interpretability advantage mentioned in the abstract is not supported since there are neither interpretability results and comparisons nor qualitative examples.
- Presentation issue: The motivation figure 1 covers too large areas. Can add some subtitles to the paragraphs in section 6.3. References:
[1] S3A: Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment
[2] Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
[3] LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions

Review Point: - The technical novelty of this paper is limited, considering a bunch of existing works on augmenting the CLIP textual prompts. Therefore, the technical differences between this work and other works on LLM-based prompt augmentation methods need to be clarified.
Review Point: - Lack of baselines for comparisons: two important baselines, CHiLS and MPVR, are mentioned in the related work but are not compared. Besides, there is also a line of missing related works on prompt augmentation with semantic discriminativeness, e.g., S3A[1], Meta-Prompting[2], and LLM Explainer [3].
Review Point: - Technical issues: (1) It is not guaranteed that the LLM-generated subcategories can satisfy the completeness and disjoint constraints of taxonomy stated in section 3.1. In practice, to what extent these two constraints can be satisfied and what implications will it have on the results needs further investigations and discussions. (2) How many layers of generated taxonomy will there be? If only generate a single layer of subcategories, this work would have high technical similarity with CHiLS; otherwise, more ablations are needed to investigate its benefits.
Review Point: - Overclaim issue: (1) the motivation of the hierarchical taxonomic prompt starts from the game theory, however, the competition and players in the categorization contexts are not clearly defined. There is no clear and direct relationship between them. (2) The claimed semantic interpretability advantage mentioned in the abstract is not supported since there are neither interpretability results and comparisons nor qualitative examples.
Review Point: - Presentation issue: The motivation figure 1 covers too large areas. Can add some subtitles to the paragraphs in section 6.3. References: [1] S3A: Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment [2] Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs [3] LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions
==================================================

Focused review:

While this paper presents an interesting idea, several major components still exist to make readers agree with the proposed claims.
### There are unaddressed potential opposed claims
1. Instead of learning the contradiction as a metric, it is more natural to reverse the semantics of the context to adapt the contradiction to the similarity. One way to do this is to append instructions to the input (Instructor [1]), such as "Contradicted to: [INPUT]". There can be a baseline (not the original Instructor) learning cosine similarity as the metric follows the traditional contrastive learning paradigm, while the inputs are with the mentioned prefix. Outperforming this baseline can further establish the necessity of learning a sparse metric, otherwise, people may favor context switching more because it can be merged with general similarity learning.
[1] One Embedder, Any Task: Instruction-Finetuned Text Embeddings @ ACL2023 Findings
2. Is there a stable selection of \alpha? While direct semantic similarity plays an important role in contradiction retrieval in the benchmarks mentioned in the paper. The weight on the semantic similarity is quite unstable from my viewpoint - contradiction shall be not so relevant with the semantic similarity, for instance Case1:
Text1: "Dinosaurs live on earth today." Text2: "Dinosaurs don't live on earth today." Case2:
Text1: "Dinosaurs live on earth today." Text2: "A huge asteroid hit the earth 65 million years ago and caused the extinction of dinosaurs."
While the cases above are somehow extreme, this still indicates how contradiction is independent of the direct textual similarity. From some readers' view, the semantic similarity should be treated as a threshold (not totally irrelevant) rather than a weighted term. So,
- Why do you think semantic similarity should still play an important role in contradiction retrieval? I think showing some real-world distribution in the corpus can lead to some help.
### Claim support is incomplete
1. An important claim of this paper is the advantage of the Hoyer function over cosine similarity because of its intransitive nature. However, this point is not supported in the content because there lack of baselines trained on the reversed similarity labels. Please consider adding this as a baseline to your main table.
- How would you define the contradiction level with non-statements like questions?
2. The performance improvement on synthesized datasets is much higher than Arguana. Based on the prompts and cases provided in the Appendix, I feel the generated contradictions are somehow favored by the proposed paradigm - mostly similar with some sparse differences. However, as mentioned above, contradiction can happen between texts with very different "length", "style", "format", so I feel this synthesis is oversimplifying the problem and making the dataset favor the proposed method. I would suggest the authors allow the synthesizer to generate text pairs with very different other attributes to further validate the too significant improvement.
- Why don't you include NLI datasets as a source of benchmarking, they directly include contradicted text pairs.
### Not enough content
1. Lack of baselines. The main comparison only includes a contrastively learned encoder, which is even unintended for the proposed contradiction retrieval task. Even though contradiction retrieval is a new task, there should be some simple baselines as mentioned above. You can also consider adding the original Instructor as a baseline, though I feel I will not perform well.
2. Lack of task significance justification. As a newly proposed task focusing on a rather narrow scope, the paper does not fully justify the importance of further digging into this task. One way to further support the importance of contradiction retrieval is to select one of the tasks mentioned in the related works to show they can be improved with contradiction retrieval.
Overall, while this paper proposes a reasonable way to address a potential issue in text embedding learning, I feel this paper has to be significantly polished to reach publication quality. Thus, I am opposed to an immediate acceptance of this paper.

Review Point: - How would you define the contradiction level with non-statements like questions?
Review Point: 2. The performance improvement on synthesized datasets is much higher than Arguana. Based on the prompts and cases provided in the Appendix, I feel the generated contradictions are somehow favored by the proposed paradigm - mostly similar with some sparse differences. However, as mentioned above, contradiction can happen between texts with very different "length", "style", "format", so I feel this synthesis is oversimplifying the problem and making the dataset favor the proposed method. I would suggest the authors allow the synthesizer to generate text pairs with very different other attributes to further validate the too significant improvement.
Review Point: - Why don't you include NLI datasets as a source of benchmarking, they directly include contradicted text pairs. ### Not enough content 1. Lack of baselines. The main comparison only includes a contrastively learned encoder, which is even unintended for the proposed contradiction retrieval task. Even though contradiction retrieval is a new task, there should be some simple baselines as mentioned above. You can also consider adding the original Instructor as a baseline, though I feel I will not perform well.
==================================================

Focused review:

1. Clarity/Quality. In the main paper and Appendix I haven’t found the explanation on how exactly Auto-Attack was restricted to the continious K-dimensional space? Given that K is extremely small (1-6), this could drastically affect the overall Auto-Attack performance. Without understanding that it is hard to interpret the results e. g. in Table 1.
2. Quality. In Section 4.1 the Authors evaluate Randomized Smoothing with standard PGD. Although they claim to use up to 10 000 steps, it still doesn’t seem like a suitable evaluation method. A-PGD or AutoAttack would be better.
3. Quality. If I understood correctly, all the evaluations were performed on small-resolution datasets (CIFAR-10, SVHN, MNIST). However, Randomized Smoothing is able to provide provable defence e. g. for models on ImageNet. Given the exponential nature of the method, it is hard to understand whether it would scale to datasets of higher resolution.
4. In the limitations the authors admit that fixing random seed in e. g. Randomized smoothing makes it vulnerable for an attacker that knows it. I assume that with the knowledge of Subspace Grid-sweep’s grid a model could be trained that seems robust when evaluated in the predefined grid points but contains adversarial regions in-between. That could result in backdoor-attacks on seemingly robust models. Thus the method can provide false sense of security.

Review Point: 1. Clarity/Quality. In the main paper and Appendix I haven’t found the explanation on how exactly Auto-Attack was restricted to the continious K-dimensional space? Given that K is extremely small (1-6), this could drastically affect the overall Auto-Attack performance. Without understanding that it is hard to interpret the results e. g. in Table 1.
Review Point: 2. Quality. In Section 4.1 the Authors evaluate Randomized Smoothing with standard PGD. Although they claim to use up to 10 000 steps, it still doesn’t seem like a suitable evaluation method. A-PGD or AutoAttack would be better.
Review Point: 3. Quality. If I understood correctly, all the evaluations were performed on small-resolution datasets (CIFAR-10, SVHN, MNIST). However, Randomized Smoothing is able to provide provable defence e. g. for models on ImageNet. Given the exponential nature of the method, it is hard to understand whether it would scale to datasets of higher resolution.
Review Point: 4. In the limitations the authors admit that fixing random seed in e. g. Randomized smoothing makes it vulnerable for an attacker that knows it. I assume that with the knowledge of Subspace Grid-sweep’s grid a model could be trained that seems robust when evaluated in the predefined grid points but contains adversarial regions in-between. That could result in backdoor-attacks on seemingly robust models. Thus the method can provide false sense of security.
==================================================

Focused review:

- Although effective, the RefKG framework lacks technical novelty. The pipeline is simple and not exciting enough.
- The RefKG framework's effectiveness on tasks beyond fact verification and KGQA is not explored, limiting understanding of its broader applicability. Besides, the benchmarks are not sufficient enough.
- The approach may not generalize well to domains with sparse or highly specialized KGs. Moreover, the performance may heavily rely on the completeness and accuracy of the underlying KGs, which may vary in different domains.
- The iterative retrieval and reflection process may be computationally intensive, raising concerns about scalability for large-scale applications.
- The paper seems not go through a careful typos checking, as there are some typos.

Review Point: - Although effective, the RefKG framework lacks technical novelty. The pipeline is simple and not exciting enough.
Review Point: - The RefKG framework's effectiveness on tasks beyond fact verification and KGQA is not explored, limiting understanding of its broader applicability. Besides, the benchmarks are not sufficient enough.
Review Point: - The approach may not generalize well to domains with sparse or highly specialized KGs. Moreover, the performance may heavily rely on the completeness and accuracy of the underlying KGs, which may vary in different domains.
Review Point: - The iterative retrieval and reflection process may be computationally intensive, raising concerns about scalability for large-scale applications.
==================================================

Focused review:

1. In Section 2 (paragraph 2), the authors mention that CAS score relies on computing the likelihood of samples generated from a conditional diffusion model, for which they use a method proposed previously by Song et al. This was later used by Zimmermann et al. to measure the class-conditional likelihood of images. The key difference, as described in Section 4.1, is that the authors subtract the unconditional likelihood. While the authors provide some empirical justification for this (Figure 2), it may be useful to conduct a more thorough investigation to justify the hypotheses proposed in Section 4.1, for example by training conditional diffusion models on some toy data.
2. In Section 4.4, the approximation only holds when $\sigma$ approaches 0. Based on this, we should expect the accuracy for the approximation to improve as we choose $\sigma$ closer to 0, but this is not true in Table 1. Do the authors have some explanation for this behavior? Further, given the large range of NRMSE values, how should practitioners choose this parameter when computing CAS for a new domain?
3. In Section 5.2, the authors only use a single metric (accuracy) to measure the correspondence between CAS and human preference. However, this does not give any insights on how well CAS difference correlates with human judgment. It may be more useful to show a plot with CAS difference between the samples and the corresponding human preference accuracy (for e.g., through a histogram), where we expect the accuracy to increase as the CAS difference gets larger.

Review Point: 2. In Section 4.4, the approximation only holds when $\sigma$ approaches 0. Based on this, we should expect the accuracy for the approximation to improve as we choose $\sigma$ closer to 0, but this is not true in Table 1. Do the authors have some explanation for this behavior? Further, given the large range of NRMSE values, how should practitioners choose this parameter when computing CAS for a new domain?
Review Point: 3. In Section 5.2, the authors only use a single metric (accuracy) to measure the correspondence between CAS and human preference. However, this does not give any insights on how well CAS difference correlates with human judgment. It may be more useful to show a plot with CAS difference between the samples and the corresponding human preference accuracy (for e.g., through a histogram), where we expect the accuracy to increase as the CAS difference gets larger.
==================================================

Focused review:

1. I expect more theoretical analysis about the key findings, as the finding about middle layer data representation ability seems to be trivial.
2. The experimental results are not robust. 1) The paper claims to enhance the trade-off between utility and privacy, yet it does not present the trade-off curve; 2) The paper should compare additional privacy-preserving methods in FL; 3) The privacy attack mentioned, such as that by Geiping et al., is not state-of-the-art; 4) A larger dataset and neural networks are necessary.
3. The paper writing needs to be improved. For example, in theorem 1, I did not find a clear theorem claim.

Review Point: 1. I expect more theoretical analysis about the key findings, as the finding about middle layer data representation ability seems to be trivial.
Review Point: 1) The paper claims to enhance the trade-off between utility and privacy, yet it does not present the trade-off curve;
Review Point: 2) The paper should compare additional privacy-preserving methods in FL;
Review Point: 3) The privacy attack mentioned, such as that by Geiping et al., is not state-of-the-art;
Review Point: 3. The paper writing needs to be improved. For example, in theorem 1, I did not find a clear theorem claim.
==================================================

Focused review:

1. The attack motivation is not clear. Why the attackers want to extract the system prompt?
2. The setting that the defender has no access to system prompt is not realistic. If defenders (like OpenAI) don't have access to system prompt, how should they input the full prompts to their LLM in delopyment?
3. Technical contribution is very limited. The defense method is more suitable for a technical blog, but definitely not for a scientific conference like ICLR.

Review Point: 1. The attack motivation is not clear. Why the attackers want to extract the system prompt?
Review Point: 2. The setting that the defender has no access to system prompt is not realistic. If defenders (like OpenAI) don't have access to system prompt, how should they input the full prompts to their LLM in delopyment?
Review Point: 3. Technical contribution is very limited. The defense method is more suitable for a technical blog, but definitely not for a scientific conference like ICLR.
==================================================

Focused review:

- The potential negative impact of further tuning the LLM, on non-reasoning tasks has not been empirically verified in the paper.
- The validation of this method has been exclusively conducted on the massive 540B model, lacking generalization studies across models of smaller scales such as 7/13/70B.

Review Point: - The potential negative impact of further tuning the LLM, on non-reasoning tasks has not been empirically verified in the paper.
Review Point: - The validation of this method has been exclusively conducted on the massive 540B model, lacking generalization studies across models of smaller scales such as 7/13/70B.
==================================================

Focused review:

Weakness: 1. When aggregating the set's representation, the choice of pooling operation is not clear. Although the performance of different poolings are analyzed. It might also be reasonable to use other approaches; 2. The advantage of set-based over instance-based contrastive learning is well illustrated, while it can be more interesting to see the advantage of constructing sets based on input over based on hidden features.

Review Point: 1. When aggregating the set's representation, the choice of pooling operation is not clear. Although the performance of different poolings are analyzed. It might also be reasonable to use other approaches;
Review Point: 2. The advantage of set-based over instance-based contrastive learning is well illustrated, while it can be more interesting to see the advantage of constructing sets based on input over based on hidden features.
==================================================

Focused review:

* The presentation of the paper can be improved, especially in the section Introduction and Related Work. Some parts are overly verbose and lack clarity. I hope the authors to further refine their expressions for conciseness and clarity.
* Including the algorithm in the main paper would facilitate readers' understanding of the algorithm.

Review Point: * The presentation of the paper can be improved, especially in the section Introduction and Related Work. Some parts are overly verbose and lack clarity. I hope the authors to further refine their expressions for conciseness and clarity.
Review Point: * Including the algorithm in the main paper would facilitate readers' understanding of the algorithm.
==================================================

Focused review:

Weakness: 1. The background on linear GCNs may not be clear. Why do we need to study linear GCN? Most of current models are non-linear. 2. The difference between with self loop and without self loop? 3. The authors used the step-size to be T/K. Why exactly T/K? can we use larger or smaller than T/K?

Review Point: 1. The background on linear GCNs may not be clear. Why do we need to study linear GCN? Most of current models are non-linear.
Review Point: 2. The difference between with self loop and without self loop?
Review Point: 3. The authors used the step-size to be T/K. Why exactly T/K? can we use larger or smaller than T/K?
==================================================

Focused review:

- One main concern is why this work focuses on graph neural networks. It seems that the analysis could be applied to other neural networks, such as MLPs and CNNs. It is hard to see the uniqueness of graph neural networks here.
- The rigor of this work is insufficient. For instance, the definition of $<>$ in Eq. 13 was not introduced.
- To strengthen the experimental section, it would be valuable to include additional real-world datasets (e.g., PubMed, Citeseer) (or tasks beyond node classification).
- To our knowledge, this is the first work to provide a tight bound on the generalization error for GNNs with residual-like structures. However, this may be an overclaim, as other works exist. It would be better to compare with the PAC-Bayes bound for Graph Neural Networks [1].
- A missing reference: [2]
[1] Liao, Renjie, Raquel Urtasun, and Richard Zemel. "A pac-bayesian approach to generalization bounds for graph neural networks." arXiv preprint arXiv:2012.07690 (2020).
[2] Huang, W., Li, Y., Du, W., Yin, J., Da Xu, R.Y., Chen, L. and Zhang, M., 2021. Towards deepening graph neural networks: A GNTK-based optimization perspective. arXiv preprint arXiv:2103.03113.

Review Point: - One main concern is why this work focuses on graph neural networks. It seems that the analysis could be applied to other neural networks, such as MLPs and CNNs. It is hard to see the uniqueness of graph neural networks here.
Review Point: - The rigor of this work is insufficient. For instance, the definition of $<>$ in Eq. 13 was not introduced.
Review Point: - To strengthen the experimental section, it would be valuable to include additional real-world datasets (e.g., PubMed, Citeseer) (or tasks beyond node classification).
Review Point: - To our knowledge, this is the first work to provide a tight bound on the generalization error for GNNs with residual-like structures. However, this may be an overclaim, as other works exist. It would be better to compare with the PAC-Bayes bound for Graph Neural Networks [1].
Review Point: - A missing reference: [2] [1] Liao, Renjie, Raquel Urtasun, and Richard Zemel. "A pac-bayesian approach to generalization bounds for graph neural networks." arXiv preprint arXiv:2012.07690 (2020). [2] Huang, W., Li, Y., Du, W., Yin, J., Da Xu, R.Y., Chen, L. and Zhang, M., 2021. Towards deepening graph neural networks: A GNTK-based optimization perspective. arXiv preprint arXiv:2103.03113.
==================================================

Focused review:

1. A major concern on this paper is about its contribution/novelty, as it omits several highly relevant related works. For example, the training loss eq. (5) in this paper is (almost) identical to eqs. (8) and (21) in [1]. Furthermore, the idea that earlier tokens can be more important for LMs' generation/alignment/reward optimization (e.g., L21-22, L69-71) has been discussed in [2] (first paragraph of section 3 and appendix F.3). At the minimum level, the authors ought to have a adequate citation and discussion with these prior works. Otherwise, the contribution of this paper is unjustified.
[1] Yang, Shentao, Tianqi Chen, and Mingyuan Zhou. "A Dense Reward View on Aligning Text-to-Image Diffusion with Preference." Forty-first International Conference on Machine Learning.
[2] Yang, Shentao, et al. "Preference-grounded token-level guidance for language model fine-tuning." Advances in Neural Information Processing Systems 36 (2023).
2. Missing evaluation on the OpenLLM benchmark. Does D$^2$PO's improved RLHF come at the cost of general LM ability such as MMLU and/or GSM8K?
3. L253-254: missing citation for maximum entropy RL, such as [3, 4].
[3] Ziebart, Brian D., et al. "Maximum entropy inverse reinforcement learning." AAAI. Vol. 8. 2008.
[4] Ziebart, Brian D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.

Review Point: 2. Missing evaluation on the OpenLLM benchmark. Does D$^2$PO's improved RLHF come at the cost of general LM ability such as MMLU and/or GSM8K?
Review Point: 3. L253-254: missing citation for maximum entropy RL, such as [3, 4]. [3] Ziebart, Brian D., et al. "Maximum entropy inverse reinforcement learning." AAAI. Vol.
Review Point: 8. 2008. [4] Ziebart, Brian D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.
==================================================

Focused review:

- To show whether the acceleration in DFT outweighs the additional computation, it would be helpful if the paper provided a comparison of the wall time cost for accelerating traditional DFT algorithms.
- Since SO(3)-equivariance is important in many contexts, could the authors provide an example of how this model might be applied to another prediction task?

Review Point: - To show whether the acceleration in DFT outweighs the additional computation, it would be helpful if the paper provided a comparison of the wall time cost for accelerating traditional DFT algorithms.
Review Point: - Since SO(3)-equivariance is important in many contexts, could the authors provide an example of how this model might be applied to another prediction task?
==================================================

Focused review:

1.	It seems that each item in the new batch (with only one prompt) could not be computed parallelly as original. Whether it will increase the time cost? It might be better to add time and flops metrics in the experiments.
2.	I think the “batchprompt” could be used in both training and test phases, right?
3.	In BPE, the weight for confidence is directly 1. What about to generate the weights scores directly by the LLM without whether confident?

Review Point: 1. It seems that each item in the new batch (with only one prompt) could not be computed parallelly as original. Whether it will increase the time cost? It might be better to add time and flops metrics in the experiments.
Review Point: 2. I think the “batchprompt” could be used in both training and test phases, right?
Review Point: 3. In BPE, the weight for confidence is directly 1. What about to generate the weights scores directly by the LLM without whether confident?
==================================================

Focused review:

weakness in the proposed methods is how the activation traces are generated for source and target model when optimising the projection. Effectively only the target model is evaluated within distribution, after which the inputs observed there are then remapped to the source model. It's unclear what the effect is of potentially evaluating the source model out of its training distribution. I was hoping to see a way to correlate trajectories collected with the respective models independently. Perhaps the type of problem considered only allows a singular solution even across different number of joints, but it would be good to verify this.
While the authors do evaluate a large combination of models, only averages are reported. Given how close the results seem to be to random in Fig. 5, it's hard to gauge the significance of the results. Some variance or error metric would be very valuable.
While the idea of pruning the networks before correlating intuitively seems like a good idea, this is not experimentally validated. It would be good to add a comparison with and between unpruned models as well.
While okay to follow, the text could use a bit more polish. Questions
There's currently no mention of how all these models were trained. One caption hints at DQN? Please provide more details.
It's unclear how to interpret training duration in Fig. 3. Is this the time required to "pass" the validation set again after pruning?
What do the values in the table in Fig. 5 represent? Sums of weights in the projection matrix? Conclusion
While overall the method presented makes sense, and the evaluation is relatively thorough, the scope of the problems evaluated is considerably limited to draw any general conclusions of its validity, and some of the framing and details raise questions. As such I'd consider this submission marginally below acceptance.

Review Point: 3. Is this the time required to "pass" the validation set again after pruning? What do the values in the table in Fig. 5 represent? Sums of weights in the projection matrix? Conclusion While overall the method presented makes sense, and the evaluation is relatively thorough, the scope of the problems evaluated is considerably limited to draw any general conclusions of its validity, and some of the framing and details raise questions. As such I'd consider this submission marginally below acceptance.
==================================================

Focused review:

1. **Lack of Guidance on Calibration Data Selection**: Although the paper presents intriguing findings, it does not offer concrete criteria or methods for selecting calibration data to enhance the generalization of quantized LLMs. This limits its practical impact and novelty.
2. **Visualization Issues**:
- Radar charts (Figures 2, 5, and 6) lack marked magnitudes for the scores on the radius, and text overlays reduce clarity.
- The task types, while indicated by background colors, are not explicitly labeled. An additional legend would make the visualizations more intuitive.

Review Point: 1. **Lack of Guidance on Calibration Data Selection**: Although the paper presents intriguing findings, it does not offer concrete criteria or methods for selecting calibration data to enhance the generalization of quantized LLMs. This limits its practical impact and novelty.
Review Point: 2. **Visualization Issues**:- Radar charts (Figures 2, 5, and 6) lack marked magnitudes for the scores on the radius, and text overlays reduce clarity.
Review Point: - The task types, while indicated by background colors, are not explicitly labeled. An additional legend would make the visualizations more intuitive.
==================================================

Focused review:

1. While the Kolmogorov complexity is extremely interesting to study, using compressed size of minimal python code is somewhat problematic as a proxy, particularly for simple arithmetic operations. I appreciate the authors listing out the limitations of this approach and discussing its precedent but the rules specified in Appendic C seem somewhat arbitrary and I expect the compressed sizes of the 3 algorithms to be fairly similar.
2. The accuracy definition which only tests the first character after the first `=` symbol seems fairly forgiving. Do the authors have a measure of what the "strict" accuracy would be? This is assuming that the model is expected to also output `e` - the end token.
3. I would appreciate if the authors edited the plots to make them easier to read/parse. The font sizes are far too small and the legends are fairly difficult to parse.
4. The fitted curve that results in the cubic scaling law (KC $\approx 18n_p^{0.34}$) seems to be based on too few datapoints to rely on. Typos:
1. SUM vs $'$sum$'$. I recommend the authors pick a standard notation and stick to it.
2. Missing parenthesis after Appendix I on page 10.
3. [insert number]x on page 10.
4. Page 3 line 135: "representing the "$\rightarrow$ "representing them"

Review Point: 1. While the Kolmogorov complexity is extremely interesting to study, using compressed size of minimal python code is somewhat problematic as a proxy, particularly for simple arithmetic operations. I appreciate the authors listing out the limitations of this approach and discussing its precedent but the rules specified in Appendic C seem somewhat arbitrary and I expect the compressed sizes of the 3 algorithms to be fairly similar.
Review Point: 2. The accuracy definition which only tests the first character after the first `=` symbol seems fairly forgiving. Do the authors have a measure of what the "strict" accuracy would be? This is assuming that the model is expected to also output `e` - the end token.
Review Point: 3. I would appreciate if the authors edited the plots to make them easier to read/parse. The font sizes are far too small and the legends are fairly difficult to parse.
Review Point: 1. SUM vs $'$sum$'$. I recommend the authors pick a standard notation and stick to it.
Review Point: 4. Page 3 line 135: "representing the "$\rightarrow$ "representing them"
==================================================

Focused review:

1. The method proposed in this paper lacks necessary theoretical analysis.
2. The idea of confidence score is not novel, as it has been widely used in works related to OOD and class imbalance.
3. I noticed that the authors cited the work of Zhao et al.[1], but they did not use the relevant new datasets in the experimental section.
4. The ablation experiments show it is very sensitive to hyperparameters, especially regarding label propagation.
5. No time complexity or runtime comparison. No pseudo-code and no data statistics.
[1]Multi-label node classification on graph-structured data. TMLR 2023.

Review Point: 1. The method proposed in this paper lacks necessary theoretical analysis.
Review Point: 2. The idea of confidence score is not novel, as it has been widely used in works related to OOD and class imbalance.
Review Point: 3. I noticed that the authors cited the work of Zhao et al.[1], but they did not use the relevant new datasets in the experimental section.
Review Point: 4. The ablation experiments show it is very sensitive to hyperparameters, especially regarding label propagation.
Review Point: 5. No time complexity or runtime comparison. No pseudo-code and no data statistics. [1]Multi-label node classification on graph-structured data. TMLR 2023.
==================================================

Focused review:

1. This paper only compares the running times, F1 scores, and conductance values of the proposed SLQ algorithm with ACL and CRD, more comparisons with other existing methods are needed for better evaluations. 2. The performance of SLQ (quantified by F1 score and conductance values) appears to have limited competitiveness compared with previous CRD method. In Figure 3, SLQ shows worse performance than CRD; while in Figure 4, the improvement of SLQ over ACL appear to be very small. 3. The notations are used inconsistently: e.g., $T$ in Theorem 3.1 refers to the # of iterations; however, on lines 195-196, Assumptions 1 and 2, and Theorem 4.1, $T$ is used to refer to as a cluster. 4. The number of datasets tested in this paper is also very limited. ** UPDATE ** I have read the responses of the authors and all reviews. I would like to thank the authors for responding. I think that my evaluation is proper for this paper, considering the main idea of a simple generalization of an existing optimization formulation, the scope of experimental validation, and the presentation. Also, it is not clear about how useful the theorem is for real datasets.

Review Point: 1. This paper only compares the running times, F1 scores, and conductance values of the proposed SLQ algorithm with ACL and CRD, more comparisons with other existing methods are needed for better evaluations.
Review Point: 2. The performance of SLQ (quantified by F1 score and conductance values) appears to have limited competitiveness compared with previous CRD method. In Figure 3, SLQ shows worse performance than CRD; while in Figure 4, the improvement of SLQ over ACL appear to be very small.
Review Point: 3. The notations are used inconsistently: e.g., $T$ in Theorem 3.1 refers to the # of iterations; however, on lines 195-196, Assumptions 1 and 2, and Theorem 4.1, $T$ is used to refer to as a cluster.
Review Point: 4. The number of datasets tested in this paper is also very limited. ** UPDATE ** I have read the responses of the authors and all reviews. I would like to thank the authors for responding. I think that my evaluation is proper for this paper, considering the main idea of a simple generalization of an existing optimization formulation, the scope of experimental validation, and the presentation. Also, it is not clear about how useful the theorem is for real datasets.
==================================================

Focused review:

: 1.This paper aims to construct a general graph self-supervised framework. However, the problem mentioned in this paper is inherent problem in contrastive learning and it is uncorrelated with graph learning, which makes the motivation unclear. 2.The solution of the paper are both general in contrastive learning. This paper just find other implementations from existing statistical methods. The proposed decorrelation principle uses two strategies to actualize. The first one is Direct Channel Decorrelation, which looks like a combination of redundancy reduction term in Barlow Twins(inter-view) and covariance regularization term in VICREG(intra-view). In addition, the second one-Spectral Regularization-only makes a few changes on ZCA whitening and is similar to the work of ZERO-CLZERO-CL. 3.Experiments are inadequate. The author should give the comparison with other methods which aims to extracting invariant information from two different views and applying specific strategies to prevent collapsed solutions.

Review Point: 2.The solution of the paper are both general in contrastive learning. This paper just find other implementations from existing statistical methods. The proposed decorrelation principle uses two strategies to actualize. The first one is Direct Channel Decorrelation, which looks like a combination of redundancy reduction term in Barlow Twins(inter-view) and covariance regularization term in VICREG(intra-view). In addition, the second one-Spectral Regularization-only makes a few changes on ZCA whitening and is similar to the work of ZERO-CLZERO-CL.
Review Point: 3.Experiments are inadequate. The author should give the comparison with other methods which aims to extracting invariant information from two different views and applying specific strategies to prevent collapsed solutions.
==================================================

Focused review:

* The technical novelty is unclear. The sensor fusion of BEV feature maps are mainly from FusionFormer as mentioned by the author. The modification for prediction and planning seems marginal compared with the existing UniAD. Fully discussions on the differences are needed.
* The proposed methods show superior performance than UniAD. What is the main contribution of this performance? Ablation study on perception, planning, and prediction pipeline with fixed modality input (e.g., only LiDAR, only camera, both modalities) is also needed to justify the performance boost and technical novelty. Right now, it is unclear if the performance gain is from the multi-modality or the pipeline design choice.
* What is the definition of used mADE, and mFDE? Trajectory prediction usually use mADE_k/mFDE_k to evaluate the accuracy of prediction with the consideration of multi-modality of multiple future paths. Please specify the k value if using mADE_k.

Review Point: * The technical novelty is unclear. The sensor fusion of BEV feature maps are mainly from FusionFormer as mentioned by the author. The modification for prediction and planning seems marginal compared with the existing UniAD. Fully discussions on the differences are needed.
Review Point: * The proposed methods show superior performance than UniAD. What is the main contribution of this performance? Ablation study on perception, planning, and prediction pipeline with fixed modality input (e.g., only LiDAR, only camera, both modalities) is also needed to justify the performance boost and technical novelty. Right now, it is unclear if the performance gain is from the multi-modality or the pipeline design choice.
Review Point: * What is the definition of used mADE, and mFDE? Trajectory prediction usually use mADE_k/mFDE_k to evaluate the accuracy of prediction with the consideration of multi-modality of multiple future paths. Please specify the k value if using mADE_k.
==================================================

Focused review:

1. The paper dedicates a significant amount of text to describing concepts, but the proposed method itself is not clearly explained. For example, Section 4 lacks sufficient details, with very few formulas or explanations to clarify how the method works.
2. The authors claim that GEMA integrates behaviors such as local and global search from evolutionary algorithms, but there is no detailed explanation of how this is achieved. Additionally, there are no concrete results presented to validate this claim.
3. The experiments are not comprehensive. For example, in Table 2, the traditional evolutionary algorithms selected are neither the most recent nor the strongest.

Review Point: 1. The paper dedicates a significant amount of text to describing concepts, but the proposed method itself is not clearly explained. For example, Section 4 lacks sufficient details, with very few formulas or explanations to clarify how the method works.
Review Point: 2. The authors claim that GEMA integrates behaviors such as local and global search from evolutionary algorithms, but there is no detailed explanation of how this is achieved. Additionally, there are no concrete results presented to validate this claim.
Review Point: 3. The experiments are not comprehensive. For example, in Table 2, the traditional evolutionary algorithms selected are neither the most recent nor the strongest.
==================================================

Focused review:

- Some parts of the paper need further explanation. For example, in the introduction, the authors mention that some researchers utilize encoders to map graph data into a latent space, but there is a lack of essential discussion about why they are doing that.
- The motivation behind some of the model designs is ambiguous. For instance, in the selection of the encoder and decoder, the conditions that a good encoder and decoder should satisfy are unclear. Instead of directly utilizing the GAE framework, it seems more important to discuss the criteria for selecting a good encoder and decoder and add related experiments to support your claims. I did not see any discussions related to that.

Review Point: - Some parts of the paper need further explanation. For example, in the introduction, the authors mention that some researchers utilize encoders to map graph data into a latent space, but there is a lack of essential discussion about why they are doing that.
Review Point: - The motivation behind some of the model designs is ambiguous. For instance, in the selection of the encoder and decoder, the conditions that a good encoder and decoder should satisfy are unclear. Instead of directly utilizing the GAE framework, it seems more important to discuss the criteria for selecting a good encoder and decoder and add related experiments to support your claims. I did not see any discussions related to that.
==================================================

Focused review:

1.	How the proposed AutoAgents framework expands the scope of collaborative applications and reduces the consumption of resources should be elaborated.
2.	The authors do not explain how to determine the number of agents in the section of the framework for automatic agent generation.
3.	The section about automatic agent generation is too tedious to introduce too much related works
4.	In addition to ChatGPT, Vicuna-13B and GPT4 in Table 2, it has not enough recent models to further show the superiority of the proposed framework-AutoAgents in open-ended question answer task in the experimental part.
5.	In the experimental part, the performance on N=10 is better than N=5 in trivia creative writing task, but there is no explanations.

Review Point: 1. How the proposed AutoAgents framework expands the scope of collaborative applications and reduces the consumption of resources should be elaborated.
Review Point: 2. The authors do not explain how to determine the number of agents in the section of the framework for automatic agent generation.
Review Point: 3. The section about automatic agent generation is too tedious to introduce too much related works 4. In addition to ChatGPT, Vicuna-13B and GPT4 in Table 2, it has not enough recent models to further show the superiority of the proposed framework-AutoAgents in open-ended question answer task in the experimental part.
Review Point: 5. In the experimental part, the performance on N=10 is better than N=5 in trivia creative writing task, but there is no explanations.
==================================================

Focused review:

* The paper only reports the problem but not a solution. The contribution of the paper is questionable, as the unexpected reconstruction of anomalies by an autoencoder was mentioned and studied several times in previous works. According to line 427 of the manuscript, this work is not the first to report the reconstruction of anomalies.
* There are missing references that reported and discussed the anomaly reconstruction phenomenon.
* https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html#Out-of-distribution-images
* Autoencoding under normalization constraints https://arxiv.org/abs/2105.05735 and references therein. Their appendix contains analyses similar to those provided by the manuscript.
* Outlier reconstruction web demo https://swyoon.github.io/outlier-reconstruction/
* The value of the analyses provided by the paper is not clear. Most sections are dedicated to simply showing the existence of reconstructed anomalies, which is somewhat trivial. The analyses do not lead to deeper insight, which can be used to build better anomaly detection algorithms.

Review Point: * The paper only reports the problem but not a solution. The contribution of the paper is questionable, as the unexpected reconstruction of anomalies by an autoencoder was mentioned and studied several times in previous works. According to line 427 of the manuscript, this work is not the first to report the reconstruction of anomalies.
Review Point: * There are missing references that reported and discussed the anomaly reconstruction phenomenon.
Review Point: * https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html#Out-of-distribution-images * Autoencoding under normalization constraints https://arxiv.org/abs/2105.05735 and references therein. Their appendix contains analyses similar to those provided by the manuscript.
Review Point: * Outlier reconstruction web demo https://swyoon.github.io/outlier-reconstruction/ * The value of the analyses provided by the paper is not clear. Most sections are dedicated to simply showing the existence of reconstructed anomalies, which is somewhat trivial. The analyses do not lead to deeper insight, which can be used to build better anomaly detection algorithms.
==================================================

Focused review:

Weakness: I have several concerns. 1. This paper finds that interim semantic maps generated at intermediate layers can provide fine-grained prior information. However, this paper does not provide theoretical analysis and visual results to explain the role of interim semantic maps. 2. This paper does not evaluate the quality of the produced features. Please provide the compared visualization results and analysis with other baseline methods. 3. Please provide the analysis on the fine-tuning time and inference latency. 4. The performance of backbone network is not presented in Table 6,7,8,9. 5. How to ensure the loss weight of the interim semantic maps generated by SPM?

Review Point: 2. This paper does not evaluate the quality of the produced features. Please provide the compared visualization results and analysis with other baseline methods.
Review Point: 3. Please provide the analysis on the fine-tuning time and inference latency.
Review Point: 4. The performance of backbone network is not presented in Table 6,7,8,9.
Review Point: 5. How to ensure the loss weight of the interim semantic maps generated by SPM?
==================================================

Focused review:

4. In several parts, the mathematical description is unclear. For example, the authors do not write the definitions of $X\_N$ and $X\_A$ in the first paragraph of Section 3, $x^{(<i)}$ in (2), $\nabla\_x^{(i)}$ in (4), $h\_A$ and $h\_b$ in (5), and skewed Gaussians of Section 4.1.
5. The justification for the introduction of the hypothesis $\sigma\_n^2<\sigma\_a^2$ is not sufficiently clear. This hypothesis implies that the underlying densities over normal samples are stable globally, not locally. I cannot agree this formulation on the basis of their description alone. Seeing the formulation of the score function, I understood that the authors defined normal samples as samples on $x$ with large $p\_X(x)$ and abnormal samples as samples on $x$ with small $p\_X(x)$. If this is true, I think that $\sigma\_a^2$ tends to be small and that the authors' hypothesis requires the uniformity (stability) of $p\_X(x)$ for $x\in X\_N$ strongly. I don't think that this point, which sounds somewhat odd, is adequately explained (this comment is related to the next comment as well).
6. Figure 2: Here, the authors have attempted to verify the hypothesis, $\sigma\_n^2<\sigma\_a^2$. The authors applied an estimated density instead of an inaccessible underlying density. In a domain $X\_{low}$ with a low density $p\_X(x)$, a density estimate $\hat{p}\_{\theta}(x)$ often have high variance ($Var[\hat{p}\_{\theta}(x)]$ gets large at each $x\in X\_{low}$, where Var is taken over all the draws of sample set). I suspect that this issue may just have greatly affected the experimental results, but have you done enough experiments to rule this suspicion out? Please write how to estimate the density. I consider this experiment to be quite important in motivating the study, considering also the 5th comment.
7. The lead up to the formulation of the proposed method (1) is not clear. The authors should descript the reason why (1) uses only $x\in X\_N$. It is not enough to write “because previous studies have done so”. Make your paper as self-contained as possible. Also, why did (1) cut out only part of the hypothesis $\sigma\_n^2<\sigma\_a^2$ and adopt the regularization term $\lambda\hat{\sigma}\_n^2$? Clarify the reason why the regularization term $\lambda\hat{\sigma}\_n^2/\hat{\sigma}\_a^2$ or $-\lambda\hat{\sigma}\_a^2$ is bad? I worry that the regularization term $\lambda\hat{\sigma}\_n^2$ may have a different effect on the estimation than what the authors expected.
8. I see that the formulation of the proposed method in Section 3 consists of the former part "Regularized density estimation Following ... (low variance) density estimate" and latter part "In recent years, ... (Jaffe et al., 2015) to anomaly detection". Although these are two independent proposals, the discussion throughout the paper is biased toward the former half. The author can use another architecture for a density estimator. If the authors want to claim that the latter part is also an important proposal, they should more clearly state the arguments that support it. For example, experimental results in section 4.2 for OURS without regularization (i.e., $\lambda=0$) should also be reported at least. If otherwise, the readers cannot understand whether the goodness of OURS is based on the former half or the latter half, or both (ablation study in Section 4.3 is insufficient; see the 13rd comment).
9. The authors write "we leverage this property to robustify our estimate" in the part "Feature permutation ensemble". Robustness against what?
10. Did you compute $\sigma\_n^2$ and $\sigma\_a^2$ (not $\hat{\sigma}\_n^2$ and $\hat{\sigma}\_a^2$) for the synthetic data used in Section 4.1? Please tell me values. Do they satisfy $\sigma\_n^2<\sigma\_a^2$?
11. In experiments in Sections 4.1 and 4.2, how to select the regularization parameter $\lambda$ is not descripted.
What are candidates for $\lambda$ (the description "in the range [1, 10]" is unclear)? Although I saw the program code, I could not find any indication that the authors selected $\lambda$ via a proper validation process. If the authors reported results for $\lambda$, which had the best test performance, then the experimental procedure is problematic and Table 1 is inacceptable. Also, if so, increasing candidates for $\lambda$ easily decrease the reported test performance. Furthermore, the range [1, 10] seems to be narrow generally; is that enough exploration?
12. The choice of hyper-parameters for the baseline methods is poor. For example, the authors fixed $k$ as 5 for $k$-NN. This is not a fair comparison.
13. Why did the authors use only 25 datasets in experiments in Section 4.3? Why does Figure 5 write results for 24 datasets? The authors used 52 datasets in Section 4.2. Absence of results for a part of datasets in Sections 4.3 and 4.4 could raise unnecessary doubts and degrade the reliability of the entire experiment. Since this would be undesirable for the authors, it would be better to take measures such as modifying or putting the rest of the results in the Appendix.
14. Most of existing studies use F1-score, not AUC. This may be due to the incompatibility of AUC and imbalance data (which often appear in the context of anomaly detection). Although there exist existing studies using AUC as well, but why do the authors dare to use AUC and not report F1 score?
15. $y$-axis label $\sigma\_a^2/\sigma\_n^2$ of Figure 2 is misleading (it is an estimate); ”extreme” points in the second paragraph of Section 1 is a latex error of ``extreme'' points; right side of Figure 4 (b) is out of text-length; "proposed in (Jaffe et al., 2015)," in p. 4 is error of citation manner (use \citet{}); NTL 2022 in Table 1 is an error of NTL 2021. There may be other minor writing errors. Please check again.

Review Point: 4. In several parts, the mathematical description is unclear. For example, the authors do not write the definitions of $X\_N$ and $X\_A$ in the first paragraph of Section 3, $x^{(<i)}$ in (2), $\nabla\_x^{(i)}$ in (4), $h\_A$ and $h\_b$ in (5), and skewed Gaussians of Section 4.1.
Review Point: 7. The lead up to the formulation of the proposed method (1) is not clear. The authors should descript the reason why (1) uses only $x\in X\_N$. It is not enough to write “because previous studies have done so”. Make your paper as self-contained as possible. Also, why did (1) cut out only part of the hypothesis $\sigma\_n^2<\sigma\_a^2$ and adopt the regularization term $\lambda\hat{\sigma}\_n^2$? Clarify the reason why the regularization term $\lambda\hat{\sigma}\_n^2/\hat{\sigma}\_a^2$ or $-\lambda\hat{\sigma}\_a^2$ is bad? I worry that the regularization term $\lambda\hat{\sigma}\_n^2$ may have a different effect on the estimation than what the authors expected.
Review Point: 9. The authors write "we leverage this property to robustify our estimate" in the part "Feature permutation ensemble". Robustness against what?
Review Point: 10. Did you compute $\sigma\_n^2$ and $\sigma\_a^2$ (not $\hat{\sigma}\_n^2$ and $\hat{\sigma}\_a^2$) for the synthetic data used in Section 4.1? Please tell me values. Do they satisfy $\sigma\_n^2<\sigma\_a^2$?
Review Point: 12. The choice of hyper-parameters for the baseline methods is poor. For example, the authors fixed $k$ as 5 for $k$-NN. This is not a fair comparison.
Review Point: 13. Why did the authors use only 25 datasets in experiments in Section 4.3? Why does Figure 5 write results for 24 datasets? The authors used 52 datasets in Section 4.2. Absence of results for a part of datasets in Sections 4.3 and 4.4 could raise unnecessary doubts and degrade the reliability of the entire experiment. Since this would be undesirable for the authors, it would be better to take measures such as modifying or putting the rest of the results in the Appendix.
Review Point: 14. Most of existing studies use F1-score, not AUC. This may be due to the incompatibility of AUC and imbalance data (which often appear in the context of anomaly detection). Although there exist existing studies using AUC as well, but why do the authors dare to use AUC and not report F1 score?
Review Point: 15. $y$-axis label $\sigma\_a^2/\sigma\_n^2$ of Figure 2 is misleading (it is an estimate); ”extreme” points in the second paragraph of Section 1 is a latex error of ``extreme'' points; right side of Figure 4 (b) is out of text-length; "proposed in (Jaffe et al., 2015)," in p.
Review Point: 4 is error of citation manner (use \citet{}); NTL 2022 in Table 1 is an error of NTL 2021. There may be other minor writing errors. Please check again.
==================================================

Focused review:

1. The analysis critically relies on the fact that the last two layers of the neural network are linear. I can definitely see this condition makes the problem a lot easier to analyze. I am wondering how hard it is to remove such restrictions.
2. It seems the analysis of Theorem 4.4 relies on the neural network in the NTK regime, as the pyramidal topology assumption has appeared in previous works such as (Nguyen & Mondelli, 2020). I don't regard this as a major weakness even if it turned out to be true that the networks are in the NTK regime given the contribution of this work, however, I do appreciate clarification on this.

Review Point: 1. The analysis critically relies on the fact that the last two layers of the neural network are linear. I can definitely see this condition makes the problem a lot easier to analyze. I am wondering how hard it is to remove such restrictions.
Review Point: 2. It seems the analysis of Theorem 4.4 relies on the neural network in the NTK regime, as the pyramidal topology assumption has appeared in previous works such as (Nguyen & Mondelli, 2020). I don't regard this as a major weakness even if it turned out to be true that the networks are in the NTK regime given the contribution of this work, however, I do appreciate clarification on this.
==================================================

Focused review:

1. Use of older models: The paper relies on ALBEF and BLIP, which are relatively older models in the rapidly evolving field of vision and language. The performance in Experiment 1 does not compare to any of the models in the papersincode leaderboard (e.g., https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1). Evaluating SynGround with more recent and state-of-the-art models would significantly strengthen the claims.
2. Limited performance gains: While improvements are reported, the absolute gains from using synthetic data, especially when combined with real data, are relatively modest and may not be statistically significant. Error bars or further statistical analysis should be provided to support the claims of improvement.
3. Clarity and organization: The presentation of experiments could be improved. The motivation and reasoning behind each experiment could be more clearly articulated. Consolidating related experiments (like the BLIP experiments) into fewer tables would enhance readability. The paper would benefit from focusing on the key findings, such as the comparison with web-crawled data, earlier in the presentation.
4. Lack of analysis on scaling limitations: While the paper mentions the potential for infinite data generation, it does not discuss or analyze potential limitations or saturation points in scaling up the use of synthetic data.

Review Point: 1. Use of older models: The paper relies on ALBEF and BLIP, which are relatively older models in the rapidly evolving field of vision and language. The performance in Experiment 1 does not compare to any of the models in the papersincode leaderboard (e.g., https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1). Evaluating SynGround with more recent and state-of-the-art models would significantly strengthen the claims.
Review Point: 2. Limited performance gains: While improvements are reported, the absolute gains from using synthetic data, especially when combined with real data, are relatively modest and may not be statistically significant. Error bars or further statistical analysis should be provided to support the claims of improvement.
Review Point: 3. Clarity and organization: The presentation of experiments could be improved. The motivation and reasoning behind each experiment could be more clearly articulated. Consolidating related experiments (like the BLIP experiments) into fewer tables would enhance readability. The paper would benefit from focusing on the key findings, such as the comparison with web-crawled data, earlier in the presentation.
Review Point: 4. Lack of analysis on scaling limitations: While the paper mentions the potential for infinite data generation, it does not discuss or analyze potential limitations or saturation points in scaling up the use of synthetic data.
==================================================

Focused review:

* My main concern is that the paper fails to put itself to the current position in the literature. This type of projecting MCMC sampling to a more MCMC friendly noise space has been well established in [1], and has been later on adapted to generative modeling regime in e.g. [2, 3], with the deterministic mapping being a VAE or a flow-based model. The contribution of this paper, positioned in these literation, is that it adapted the sampling to a posterior distribution, and leverages a CM with fixed noise as the deterministic mapping. In that case, I think the novelty is limited.
* The paper claims that the accumulation of samples leads to diverse samples. This needs to be further justified by analyzing the convergence behavior of the sampling chains. How do you make sure the samples from adjacent sampling steps are not correlated to each other (this could be guaranteed by other baseline methods that always start the sampling from independently sampled noise).
* The method assume that the sampling from the CM model is with fixed noise, which is lack of justification, and might partially explain why the sampling quality is suboptimal.
* Why are 1-step CM results better than 2-step CM empirically in general?
* Empirical results are not convincing enough. E.g., some samples in the top right row of figure 1 look oversaturated, which might indicate the sampling chain is not stable or mixing.
[1] NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport
[2] VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models
[3] MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC

Review Point: * The paper claims that the accumulation of samples leads to diverse samples. This needs to be further justified by analyzing the convergence behavior of the sampling chains. How do you make sure the samples from adjacent sampling steps are not correlated to each other (this could be guaranteed by other baseline methods that always start the sampling from independently sampled noise).
Review Point: * The method assume that the sampling from the CM model is with fixed noise, which is lack of justification, and might partially explain why the sampling quality is suboptimal.
Review Point: * Why are 1-step CM results better than 2-step CM empirically in general?
Review Point: * Empirical results are not convincing enough. E.g., some samples in the top right row of figure 1 look oversaturated, which might indicate the sampling chain is not stable or mixing. [1] NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport [2] VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models [3] MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC
==================================================

Focused review:

The authors present their method as being the first to implement a sparsity representation method based on the EM algorithm. However, this is not correct, as the authors are missing many related methods that integrate sparsity within an EM algorithm. Some of the missing related methods are:
- Bouveyron, C., & Brunet-Saumard, C. (2014). Discriminative variable selection for clustering with the sparse Fisher-EM algorithm. Computational Statistics, 29, 489-513.
- Ghosh, A. K., & Chakraborty, A. (2017). Use of EM algorithm for data reduction under sparsity assumption. Computational Statistics, 32, 387-407.
- Wang, Z., Gu, Q., Ning, Y., & Liu, H. (2015). High dimensional em algorithm: Statistical optimization and asymptotic normality. Advances in neural information processing systems, 28.
- Latouche, P., Mattei, P. A., Bouveyron, C., & Chiquet, J. (2016). Combining a relaxed EM algorithm with Occam’s razor for Bayesian variable selection in high-dimensional regression. Journal of Multivariate Analysis, 146, 177-190.
- Ročková, V. (2018). Particle EM for variable selection. Journal of the American Statistical Association, 113(524), 1684-1697.
- Ročková, V., & George, E. I. (2014). EMVS: The EM approach to Bayesian variable selection. Journal of the American Statistical Association, 109(506), 828-846.
- Wang, J., Liang, F., & Ji, Y. (2016). An ensemble EM algorithm for Bayesian variable selection. arXiv preprint arXiv:1603.04360.
As a result, the contributions of the submitted work are clearly positioned in such literature. Moreover, the comparative experiments are missing such related work.

Review Point: - Bouveyron, C., & Brunet-Saumard, C. (2014). Discriminative variable selection for clustering with the sparse Fisher-EM algorithm. Computational Statistics, 29, 489-513.
Review Point: - Ghosh, A. K., & Chakraborty, A. (2017). Use of EM algorithm for data reduction under sparsity assumption. Computational Statistics, 32, 387-407.
Review Point: - Wang, Z., Gu, Q., Ning, Y., & Liu, H. (2015). High dimensional em algorithm: Statistical optimization and asymptotic normality. Advances in neural information processing systems, 28.
Review Point: - Latouche, P., Mattei, P. A., Bouveyron, C., & Chiquet, J. (2016). Combining a relaxed EM algorithm with Occam’s razor for Bayesian variable selection in high-dimensional regression. Journal of Multivariate Analysis, 146, 177-190.
Review Point: - Ročková, V. (2018). Particle EM for variable selection. Journal of the American Statistical Association, 113(524), 1684-1697.
Review Point: - Ročková, V., & George, E. I. (2014). EMVS: The EM approach to Bayesian variable selection. Journal of the American Statistical Association, 109(506), 828-846.
Review Point: - Wang, J., Liang, F., & Ji, Y. (2016). An ensemble EM algorithm for Bayesian variable selection. arXiv preprint arXiv:1603.04360. As a result, the contributions of the submitted work are clearly positioned in such literature. Moreover, the comparative experiments are missing such related work.
==================================================

Focused review:

The main weaknesses are the presentation and the significance of the results, mainly for Theorem 3.4 and 3.5.
1. There needs more discussion on the upper bound $\bar{\eta}$ on the step size, and the linear rate $\rho$ in Theorem 3.4, specifically their dependence on 1) the underlying LP problem $(A,b,c)$, and its scale (# of decision variables, #of constraints, etc.); 2) the initialization $u_0$. For example, if either $A$ is ill-conditioned, or the initialization is close to the origin, then I believe $\rho$ should be close to one. Merely showing that GD converges linearly does not make a significant contribution if what authors propose is to implement this GD algorithm for solving real LP problems.
2. Theorem 3.5 only shows that the GD converges to some $x^\infty$ that is close to the desired solution to the LP, but the result is weak in the sense that it doesn't suggest an upper bound on the # of GD iterations for achieving certain accuracy. Specifically, the convergence result one expects is that given some $\epsilon>0$, the GD with some step size $\eta(\epsilon)$ takes $T(\epsilon)$ iterations to achieve either 1) $\|x^T-x^*\|\leq \epsilon$, where $x^*$ is the true optimal solution; 2) or the optimality gap is less than $\epsilon$.
3. Another concern I have is that I don't find, from the discussions and experiments in this paper, any evidence that the proposed algorithm has advantages in solving certain LPs, compared to existing methods.

Review Point: 1. There needs more discussion on the upper bound $\bar{\eta}$ on the step size, and the linear rate $\rho$ in Theorem 3.4, specifically their dependence on
Review Point: 1) the underlying LP problem $(A,b,c)$, and its scale (# of decision variables, #of constraints, etc.);
Review Point: 2) the initialization $u_0$. For example, if either $A$ is ill-conditioned, or the initialization is close to the origin, then I believe $\rho$ should be close to one. Merely showing that GD converges linearly does not make a significant contribution if what authors propose is to implement this GD algorithm for solving real LP problems.
Review Point: 2. Theorem 3.5 only shows that the GD converges to some $x^\infty$ that is close to the desired solution to the LP, but the result is weak in the sense that it doesn't suggest an upper bound on the # of GD iterations for achieving certain accuracy. Specifically, the convergence result one expects is that given some $\epsilon>0$, the GD with some step size $\eta(\epsilon)$ takes $T(\epsilon)$ iterations to achieve either
Review Point: 1) $\|x^T-x^*\|\leq \epsilon$, where $x^*$ is the true optimal solution;
Review Point: 3. Another concern I have is that I don't find, from the discussions and experiments in this paper, any evidence that the proposed algorithm has advantages in solving certain LPs, compared to existing methods.
==================================================

Focused review:

- The datasets used for fine-tuning are somewhat limited in their volume. Exploration with larger datasets, such as the MIMIC-IV clinical notes, might add more depth.
- While the author contends that PubMedBERT is brimming with pertinent data for the tasks at hand even before fine-tuning, the model displays unpredictability when predicting lesser-represented classes post-fine-tuning. Could enlarging the fine-tuning dataset address this challenge?
- The conclusion for Table 2 seems vague. PubMedBERT and BERT perform very similarly and it is hard to conclude that PubMedBERT contains much useful information for the tasks. Also, why the mixed-domian models BioBERT and Clinical BioBERT perform even worse than BERT?

Review Point: - The datasets used for fine-tuning are somewhat limited in their volume. Exploration with larger datasets, such as the MIMIC-IV clinical notes, might add more depth.
Review Point: - While the author contends that PubMedBERT is brimming with pertinent data for the tasks at hand even before fine-tuning, the model displays unpredictability when predicting lesser-represented classes post-fine-tuning. Could enlarging the fine-tuning dataset address this challenge?
Review Point: - The conclusion for Table 2 seems vague. PubMedBERT and BERT perform very similarly and it is hard to conclude that PubMedBERT contains much useful information for the tasks. Also, why the mixed-domian models BioBERT and Clinical BioBERT perform even worse than BERT?
==================================================

Focused review:

1. The writing needs improvement, particularly in clarifying several terms and diagrams. For example, some terms like "find a precise weighting function to balance the state distribution" need better explanation. Clarification is also needed for the diagram in Figure 2.
2. The presentation of experimental results is somewhat confusing. The differences of scenarios in Figure 4 and 5 are not clear, and additional explanations are required for the target coordinates mentioned for Figure 4. Captions of Figures 7 (a) and (b) might need to be swapped, and sections (c) and (d) require clearer explanations.
3. The paper lacks a discussion of limitations, which should be addressed.

Review Point: 1. The writing needs improvement, particularly in clarifying several terms and diagrams. For example, some terms like "find a precise weighting function to balance the state distribution" need better explanation. Clarification is also needed for the diagram in Figure 2.
Review Point: 2. The presentation of experimental results is somewhat confusing. The differences of scenarios in Figure 4 and 5 are not clear, and additional explanations are required for the target coordinates mentioned for Figure 4. Captions of Figures 7 (a) and (b) might need to be swapped, and sections (c) and (d) require clearer explanations.
Review Point: 3. The paper lacks a discussion of limitations, which should be addressed.
==================================================

Focused review:

1. **Presentation** I found the results incredibly difficult to parse. The large tables are hard to exact trends from and I think at times the results that matter are actually split across more than one of these tables or duplicated. For example, Table 3 has some of the data in Table 1 but it's missing other rows one might care about. I was flipping back and forth a bunch and it is very hard to internalize the trends. An informative visualization of this data is missing.
2. **Unclear Experimental Setup** I see in the formal notation that $f_\text{target}$ is the base model, as in one that should know the sensitive/private info and one one which unlearning algorithms haven't yet been run. It isn't clear what that model is -- what size, what training data, how it is trained etc.
3. **Limited Data Scope** The experiments make use of two datasets that I have not seen in many other unlearning papers.

Review Point: 1. **Presentation** I found the results incredibly difficult to parse. The large tables are hard to exact trends from and I think at times the results that matter are actually split across more than one of these tables or duplicated. For example, Table 3 has some of the data in Table 1 but it's missing other rows one might care about. I was flipping back and forth a bunch and it is very hard to internalize the trends. An informative visualization of this data is missing.
Review Point: 2. **Unclear Experimental Setup** I see in the formal notation that $f_\text{target}$ is the base model, as in one that should know the sensitive/private info and one one which unlearning algorithms haven't yet been run. It isn't clear what that model is -- what size, what training data, how it is trained etc.
Review Point: 3. **Limited Data Scope** The experiments make use of two datasets that I have not seen in many other unlearning papers.
==================================================

Focused review:

Although the paper is well written and motivated I find the paper lacking with respect to the standards of the conference and the selected primary area.
1. Lacking novelty : I fail to understand the novelty of the paper. The paper seems to be heavily reliant on the Ensemble Integration method and makes several reference of the method throughout the paper. The only separating elements seems to be the use of LSTM. Though the authors have suggested several configuration in which LEI could be used in. Which by itself makes the novelty of the proposed approach limited. Maybe a more model-based approach starting from a graphical model could help with this. I would also encourage authors to discuss the underlying assumptions behind the longitudinal ensemble integration. This would allow the reader to understand why the usage of LSTM is non trivial and suitable for longitudinal multimodal datasets.
2. The interpretable aspect of the LEI approach still seems to be static and reliant on the EI framework itself. This limits the ability of the LEI framework to find temporal signature that might be more informative for classification. I would further encourage the authors to address this in each of the configuration (time dependent BPs and time distributed BPs). As these configuration treats temporal dependencies differently. This could further enhance and showcase the contribution of the method in different settings.
3. Comparing LEI framework with other approaches could help showcase LEI efficacy which is currently not very clear.

Review Point: 2. The interpretable aspect of the LEI approach still seems to be static and reliant on the EI framework itself. This limits the ability of the LEI framework to find temporal signature that might be more informative for classification. I would further encourage the authors to address this in each of the configuration (time dependent BPs and time distributed BPs). As these configuration treats temporal dependencies differently. This could further enhance and showcase the contribution of the method in different settings.
Review Point: 3. Comparing LEI framework with other approaches could help showcase LEI efficacy which is currently not very clear.
==================================================

Focused review:

- There are a few popular works solving spatial-temporal PDEs with graph neural networks that are not mentioned or compared in the paper, such as meshgraphnet, MP-PDE, etc. There are also a few works in Neural ODEs that are relevant, such as hypersolver, deep Euler method, etc.
- Traditionally, the Runge-Kutta method comes with strict guarantees in accuracy, convergence, stability, etc. But with a GNN as the estimator in a timeslot, such theoretical properties are lost. So I wonder if it is still necessary or beneficial to utilize the Runge-Kutta scheme with GNN when handling a practical PDE problem.
- In the conclusion section, it is claimed that "it is invariant to spatial and temporal discretization". But I don't see how the proposed method can guarantee such invariance capability.

Review Point: - There are a few popular works solving spatial-temporal PDEs with graph neural networks that are not mentioned or compared in the paper, such as meshgraphnet, MP-PDE, etc. There are also a few works in Neural ODEs that are relevant, such as hypersolver, deep Euler method, etc.
Review Point: - Traditionally, the Runge-Kutta method comes with strict guarantees in accuracy, convergence, stability, etc. But with a GNN as the estimator in a timeslot, such theoretical properties are lost. So I wonder if it is still necessary or beneficial to utilize the Runge-Kutta scheme with GNN when handling a practical PDE problem.
Review Point: - In the conclusion section, it is claimed that "it is invariant to spatial and temporal discretization". But I don't see how the proposed method can guarantee such invariance capability.
==================================================

Focused review:

- The novelty of the paper is very limited. The authors mainly take two existing designs and train these models by considering multiple receivers. This is rather trivial, and as such the paper does not introduce any new concept, architecture or tool.
- The federated learning (FL) approach is not well motivated or explained. Why would a vertical FL approach make sense here? As long as the channel model is available, what prevents the encoder from training all the decoders centrally? Is it a complexity issue? The proposed uncoded transmission of gradients, as also observed by the authors, is limiting, and not well motivated.
- Presentation can be improved. Especially in the numerical results part, there are some confusing sentences.
- Comparison is limited to a single relatively weak code from Li et al. It seems that the state of the art for point-to-point channels with feedback (GBAF codes) is not considered in this scenario. Although complexity is argued against these codes, there is no presentation of complexity for the presented schemes.

Review Point: - The novelty of the paper is very limited. The authors mainly take two existing designs and train these models by considering multiple receivers. This is rather trivial, and as such the paper does not introduce any new concept, architecture or tool.
Review Point: - The federated learning (FL) approach is not well motivated or explained. Why would a vertical FL approach make sense here? As long as the channel model is available, what prevents the encoder from training all the decoders centrally? Is it a complexity issue? The proposed uncoded transmission of gradients, as also observed by the authors, is limiting, and not well motivated.
Review Point: - Presentation can be improved. Especially in the numerical results part, there are some confusing sentences.
Review Point: - Comparison is limited to a single relatively weak code from Li et al. It seems that the state of the art for point-to-point channels with feedback (GBAF codes) is not considered in this scenario. Although complexity is argued against these codes, there is no presentation of complexity for the presented schemes.
==================================================

Focused review:

- In section 3 the authors argued that in case the student and the teacher do not share the same embedding dimensions, a learnable projection layer is required which can often harm performance - however, the authors do not provide any explanation or evidence to this sentence (why it harms performance?) nor at least any reference to this determination. Also note that the proposed approach in the paper includes much more projection layers - why in this case the authors don’t think it can harm the performance?
- The authors proposed to use PCA to obtain the informative linear subspaces, which is an off-line process where only the final embedding layer was used. I am wondering whether these linear subspaces could be learned as part of the training of the teacher, to also output these additional embeddings? (e.g. using reconstruction loss)
- There is a significant effort to explain the setup in section 4.1 which I am wondering whether it was necessary, especially as the authors focus on cases where the teacher and student architectures have exactly the same dimensions which as I stated before, not sure why to limit to these cases?
- I would expect the authors to experiment also with regular (large) number of classes as CIFAR-100, TinyImageNet or other datasets to understand what are the limitations of the proposed approach and how it behaves on regular and common cases where there are many categories.
- I found it very hard to understand the t-SNE plots provided in Figure 4. What is the meaning of running different t-SNE for each one of the methods as each individual t-SNE run organizes the points differently? Why the shape of the embeddings look so different in the top row? Further explanation will be helpful.
- An intermediate analysis that shows the meaning of the sub-classes obtained by PCA could help for visualization and understanding. For instance, would we observe meaningful fine-grained classes?
- Main concern is the weak experimental section. Only Table 2 provides detailed classification results. Only one teacher and student architectural choices were used. It is not clear how the method generalizes to other architectures. Also, the results are not convincing enough to my opinion and in some cases are marginal.
- What is the impact of the S hyper-parameter? (The number of sub-classes per class). I would expect some ablation study on this. Minor:
Line 73: form —> from.
Line 202: coarse-graned —> coarse-grained

Review Point: - In section 3 the authors argued that in case the student and the teacher do not share the same embedding dimensions, a learnable projection layer is required which can often harm performance - however, the authors do not provide any explanation or evidence to this sentence (why it harms performance?) nor at least any reference to this determination. Also note that the proposed approach in the paper includes much more projection layers - why in this case the authors don’t think it can harm the performance?
Review Point: - I would expect the authors to experiment also with regular (large) number of classes as CIFAR-100, TinyImageNet or other datasets to understand what are the limitations of the proposed approach and how it behaves on regular and common cases where there are many categories.
Review Point: - I found it very hard to understand the t-SNE plots provided in Figure 4. What is the meaning of running different t-SNE for each one of the methods as each individual t-SNE run organizes the points differently? Why the shape of the embeddings look so different in the top row? Further explanation will be helpful.
Review Point: - An intermediate analysis that shows the meaning of the sub-classes obtained by PCA could help for visualization and understanding. For instance, would we observe meaningful fine-grained classes?
Review Point: - Main concern is the weak experimental section. Only Table 2 provides detailed classification results. Only one teacher and student architectural choices were used. It is not clear how the method generalizes to other architectures. Also, the results are not convincing enough to my opinion and in some cases are marginal.
Review Point: - What is the impact of the S hyper-parameter? (The number of sub-classes per class). I would expect some ablation study on this. Minor: Line 73: form —> from. Line 202: coarse-graned —> coarse-grained
==================================================

Focused review:

1. There is a significant performance variation depending on the choice of the prior model. A method for either reducing performance discrepancies or pre-selecting prior models capable of achieving high performance is necessary.
2. In Table 2, results are reported for experiments using only 20/40/60% of the total data samples, but the intention behind these experiments is unclear. Additionally, on what criteria was the data selected?
3. While Section 4.2 emphasizes training efficiency, based on the caption in Table 1, this appears to be calculated simply by training steps (if otherwise, please provide the specific criteria used to calculate the training budget). Since DELA requires additional computation costs to evolve data labels, this may not be a fair comparison. A more reliable efficiency metric, such as wall clock time for total training duration, would be beneficial.
4. Given the nature of the methodology, where the model undergoes iterative self-learning, it seems likely that overfitting could be an issue. An analysis of overfitting would be appreciated.

Review Point: 1. There is a significant performance variation depending on the choice of the prior model. A method for either reducing performance discrepancies or pre-selecting prior models capable of achieving high performance is necessary.
Review Point: 2. In Table 2, results are reported for experiments using only 20/40/60% of the total data samples, but the intention behind these experiments is unclear. Additionally, on what criteria was the data selected?
Review Point: 3. While Section 4.2 emphasizes training efficiency, based on the caption in Table 1, this appears to be calculated simply by training steps (if otherwise, please provide the specific criteria used to calculate the training budget). Since DELA requires additional computation costs to evolve data labels, this may not be a fair comparison. A more reliable efficiency metric, such as wall clock time for total training duration, would be beneficial.
Review Point: 4. Given the nature of the methodology, where the model undergoes iterative self-learning, it seems likely that overfitting could be an issue. An analysis of overfitting would be appreciated.
==================================================

Focused review:

- This work lacks sufficient innovation compared to existing benchmarks for video temporal understanding, and the claimed novelties are questionable:
- **Emphasizes on fine-grained action understanding**: Most existing benchmarks, such as [1,2,3,4], already consider fine-grained action.
- **Evaluations on both short (<20 seconds) and long (>3 minute) videos**: Video-MME [5] similarly includes videos categorized as short, medium, and long.
- **Extends to video captioning, video grounding, and video generation**: Captioning is already evaluated in benchmarks like TempCompass [1] and Video-ChatGPT [6], while this paper does not actually investigate extensions to video grounding and generation.
- **Evaluations of both video embedding and question-answering models**: Previous work, including Vitatecs [2] and ViLMA [4], also assesses both video embedding and generative VLMs.
- Some results indicate potential quality issues with the data in TemporalBench:
- Human performance on TemporalBench is merely 67.9%.
- GPT-4o, when given only text input, achieves 67.7% accuracy on binary QA, suggesting that a significant portion of negative captions can be identified without reference to the video.
- The evaluation of detailed video captioning in Section 4.4 relies on classical image captioning metrics like CIDEr, BLEU, and ROUGE, which may not be suitable for assessing detailed video captioning.
- Simply concatenating the captions from short video clips to create long video captions can lead to ambiguity; for example, the caption for the first clip may contradict the content of the second clip. This inconsistency can undermine the reliability of the long video QA results.
[1] TempCompass: Do video LLMs really understand videos?
[2] Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models.
[3] MVBench: A Comprehensive Multi-modal Video Understanding Benchmark.
[4] ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models.
[5] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis.
[6] Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models.

Review Point: - This work lacks sufficient innovation compared to existing benchmarks for video temporal understanding, and the claimed novelties are questionable:
Review Point: - **Emphasizes on fine-grained action understanding**: Most existing benchmarks, such as [1,2,3,4], already consider fine-grained action.
Review Point: - **Evaluations on both short (<20 seconds) and long (>3 minute) videos**: Video-MME [5] similarly includes videos categorized as short, medium, and long.
Review Point: - **Extends to video captioning, video grounding, and video generation**: Captioning is already evaluated in benchmarks like TempCompass [1] and Video-ChatGPT [6], while this paper does not actually investigate extensions to video grounding and generation.
Review Point: - **Evaluations of both video embedding and question-answering models**: Previous work, including Vitatecs [2] and ViLMA [4], also assesses both video embedding and generative VLMs.
Review Point: - Some results indicate potential quality issues with the data in TemporalBench:
Review Point: - GPT-4o, when given only text input, achieves 67.7% accuracy on binary QA, suggesting that a significant portion of negative captions can be identified without reference to the video.
Review Point: - The evaluation of detailed video captioning in Section 4.4 relies on classical image captioning metrics like CIDEr, BLEU, and ROUGE, which may not be suitable for assessing detailed video captioning.
==================================================

Focused review:

- Experimental justification is weak to support the importantce and effectiveness of TWD in SSL problem settings.
- The authors investigated the proposed method using only Resnet18 backbone. This raises a concern of robustness of the method. Authors may want to conduct experiments other backbones at least like Resnet50 (as in Lavoie et al. 2022).
- Similarly, I have concerns whether the proposed method works well when it is applied to other SSL methods like BYOL besides SimCLR. This is also important to show the wide applicability of the proposed method.
- Finally, in Table 1, even if the cosine similarity is used, JD regularization is also applicable for simplicial models. It seems unfair since JD is not used in the cases of cosine similarity + simplicial models (softmax, SEM, AF(DCT)). I cannot determine where the improvement comes from (i.e., from TWD or JD). Without JD, TWD seems to be worse than (or comparable to) the cosine similarity in Table 1.
- I could not understand why Wasserstein distance (and its variants) is important in SSL setting. It seems a simple replacement of loss function without any justification. See my question below.

Review Point: - Experimental justification is weak to support the importantce and effectiveness of TWD in SSL problem settings.
Review Point: - The authors investigated the proposed method using only Resnet18 backbone. This raises a concern of robustness of the method. Authors may want to conduct experiments other backbones at least like Resnet50 (as in Lavoie et al. 2022).
Review Point: - Similarly, I have concerns whether the proposed method works well when it is applied to other SSL methods like BYOL besides SimCLR. This is also important to show the wide applicability of the proposed method.
Review Point: - Finally, in Table 1, even if the cosine similarity is used, JD regularization is also applicable for simplicial models. It seems unfair since JD is not used in the cases of cosine similarity + simplicial models (softmax, SEM, AF(DCT)). I cannot determine where the improvement comes from (i.e., from TWD or JD). Without JD, TWD seems to be worse than (or comparable to) the cosine similarity in Table 1.
Review Point: - I could not understand why Wasserstein distance (and its variants) is important in SSL setting. It seems a simple replacement of loss function without any justification. See my question below.
==================================================

Focused review:

1. The majority voting conflict makes me wonder why Mistral is used at all if, in cases of conflict, the decision maker is GPT4 (which is quite a costly model)?
2. Majority voting labels are used as ground-truth. It would be good to add experiments on what would happen if we train on unaggregated labels, as subjectivity is important in such a task.

Review Point: 1. The majority voting conflict makes me wonder why Mistral is used at all if, in cases of conflict, the decision maker is GPT4 (which is quite a costly model)?
Review Point: 2. Majority voting labels are used as ground-truth. It would be good to add experiments on what would happen if we train on unaggregated labels, as subjectivity is important in such a task.
==================================================

Focused review:

1) The paper does not contribute to the existing literature on compositional generalization in neural models and language models (see my questions on this and Furrer’s and Kim’s surveys for references); and does not evaluate the advanced techniques developed, for example Drozdov prompting work. The paper can also benefit from using the other length generalization papers. I am attaching some representatives that could have been included:
- Drozdov et al. “Compositional Semantic Parsing with Large Language Models”
- Liu et al. “Learning algebraic recombination for compositional generalization”.
- Anil et al., “Exploring length generalization in large language models”,
- Newman et al., “The EOS decision and length extrapolation”.
References for some other compositional generalization work that I think is not successful at recursive generalization:
- Russin, “Compositional generalization by factorizing alignment and translation”.
- Conklin, “Meta-learning to compositionally generalize”.
- Csordas, “The devil is in the detail: Simple tricks improve systematic generalization of transformers”.
- Akyurek and Andreas, “LexSym: Compositionality as Lexical Symmetry”.
2) The paper refers to the general category of language models, and the paper states in their contribution *“This demonstrates that language models struggle to understand and generate compositional structure, which implies that the recent achievements in reasoning are not a result of language models' systematical and structural understanding of tasks.”*.
Yet, the paper’s main experiments deal with small and relatively old models on a single fine-tuning setting. Moreover, none of the paper that the phrase “recent achievements in reasoning” refer use Roberta/T5/GPT-2, they instead use Palm and GPT-3. So, I cannot see how the claims you make derived from these results alone.
So, I believe important experiments are needed to validate these claims and strengthen the contribution of the paper.
- How does the increasing size of an LM affect these results? Do we see better generalization with increasing size?
- What about mid-size, newer models trained with updated datasets (LLama, Pythia series).
- What about LLMs with prompting (you have only baseline results in Appendix F4, I suggest moving them to the main body. There are many methods on prompting nowadays gives significantly better results than few-shot prompting, see Drozdov et al., 2023).
- What about LLMs with fine-tuning (GPT had an API for FT).
3) I think Table1, 2 and 8 could be summarized in a single plot which makes the presentation nice. My initial thoughts:
- X axis is `n` where n is maximum training depth
- Y axis include three connected dots per model; first dot is the accuracy at `depth<=n` test examples, second that is accuracy at `depth=n+1` test examples, and the third dot is the accuracy at `depth=n+2`
- Color can specify the different models
- Style can specify the pre-trained vs non-pretrained
You could play with these settings, but the essence is that you can compress many tables nicely into plots that are more informative about the generalization gap w.r.t function of `n`, model and pretraining.

Review Point: 1) The paper does not contribute to the existing literature on compositional generalization in neural models and language models (see my questions on this and Furrer’s and Kim’s surveys for references); and does not evaluate the advanced techniques developed, for example Drozdov prompting work. The paper can also benefit from using the other length generalization papers. I am attaching some representatives that could have been included:
Review Point: - Drozdov et al. “Compositional Semantic Parsing with Large Language Models” - Liu et al. “Learning algebraic recombination for compositional generalization”.
Review Point: - Anil et al., “Exploring length generalization in large language models”, - Newman et al., “The EOS decision and length extrapolation”. References for some other compositional generalization work that I think is not successful at recursive generalization:
Review Point: - Csordas, “The devil is in the detail: Simple tricks improve systematic generalization of transformers”.
Review Point: - How does the increasing size of an LM affect these results? Do we see better generalization with increasing size?
Review Point: - What about mid-size, newer models trained with updated datasets (LLama, Pythia series).
Review Point: - What about LLMs with prompting (you have only baseline results in Appendix F4, I suggest moving them to the main body. There are many methods on prompting nowadays gives significantly better results than few-shot prompting, see Drozdov et al., 2023).
Review Point: - What about LLMs with fine-tuning (GPT had an API for FT).
Review Point: 3) I think Table1, 2 and 8 could be summarized in a single plot which makes the presentation nice. My initial thoughts:
Review Point: - X axis is `n` where n is maximum training depth - Y axis include three connected dots per model; first dot is the accuracy at `depth<=n` test examples, second that is accuracy at `depth=n+1` test examples, and the third dot is the accuracy at `depth=n+2` - Color can specify the different models - Style can specify the pre-trained vs non-pretrained You could play with these settings, but the essence is that you can compress many tables nicely into plots that are more informative about the generalization gap w.r.t function of `n`, model and pretraining.
==================================================

Focused review:

1. Active In-Context Learning has been previously mentioned, referring to the paper "Active learning principles for in-context learning with large language models." by Katerina Margatina et al. (EMNLP 2023). The authors would do well to differentiate their approach from the existing work.
2. In terms of methodology, the authors employ rather common practices, such as spectral clustering and k-nearest neighbors, which do not exhibit significant innovation.
3. Experimentally, the methods compared by the authors are rather naive (such as random selection), and the results are consequently obvious; to be more convincing, the authors should introduce stronger baselines. They might consider incorporating more advanced selection algorithms from the field of active learning.
4. **Typos**:
1. In Equation (4), the notation should be corrected from "$c \in 1,2,…,m$" to "$c = 1,2,…,m$".
2. In Equation (7), the expression "$i=1,…,B$" should be changed to "$x_a \in D_l$", as the variable $i$ does not appear in the formula.
3. The caption for Figure 11(a) is incorrect.

Review Point: 1. Active In-Context Learning has been previously mentioned, referring to the paper "Active learning principles for in-context learning with large language models." by Katerina Margatina et al. (EMNLP 2023). The authors would do well to differentiate their approach from the existing work.
Review Point: 2. In terms of methodology, the authors employ rather common practices, such as spectral clustering and k-nearest neighbors, which do not exhibit significant innovation.
Review Point: 3. Experimentally, the methods compared by the authors are rather naive (such as random selection), and the results are consequently obvious; to be more convincing, the authors should introduce stronger baselines. They might consider incorporating more advanced selection algorithms from the field of active learning.
Review Point: 2. In Equation (7), the expression "$i=1,…,B$" should be changed to "$x_a \in D_l$", as the variable $i$ does not appear in the formula.
==================================================

Focused review:

W1.The writing quality is very poor. **Typos**
- Page 1
- calld -> called
- adverasarial -> adversarial
- Page 2
- (Figure 1) Bias-conflicting -> Bias-aligned
- ...the number of the proportion...-> ...the number or the proportion...
- bset -> best
- Page 3
- calssification -> classification
- v.s. -> vs.
- Page 4
- consturcting -> constructing
- implictions -> implications
- labelign -> labeling
- Page 5
- (Figure 2) Bias-conflicting -> Bias-aligned
- anootations -> annotations
- ...that do note... -> ...that do not...
- Page 6
- $f$open -> $f_{open}$
- ...without relying on provided or pseudo-labels. -> without relying on provided [something] or pseudo-labels.
- Page 7
- laogirtmh -> algorithm
- datasetinclude -> dataset include
- Dogans and Cats -> Dogs and Cats
- classifiaction -> classification
- Page 8
- examels -> examples
- plots -> plot
- three benchmarks -> two benchmarks
- Page 9
- philosohpy -> philosophy
**Inconsistency between results and explanation.**
- Page 4: 'As indicated in Table 3 semi-supervised debiasing is not a straightforward process In and can potentially degrade performance.'
- The authors described that FixMatch degrades performance, but Table 3 shows that FixMatch didn't degrade performance.
**Lack of information**
- On page 2, in the third contribution point, what type of labels are not required?
- What metric is reported in Table 1?
- On page 4, in Observation 1, the training instructions are missing. Did you combine the relevant samples with the original training set for training, or did you only use relevant samples?
- What metric is reported in Table 3?
**Incorrect sentence**
- Page 5 'As previously noted, the open dataset D_{open} does not inherently contain samples that are directly relevant to the target task.'
- I think the sentence can be revised to 'As previously noted, the open dataset D_{open} might inherently contain samples that are not directly relevant to the target task.'
W2. If the open dataset is biased, and the class-wise centroid is biased toward bias-aligned samples, could ORBIS still be helpful for debiasing if all relevant samples are bias-aligned?
W3. Is there any problem arising from the differences between the open dataset and the target dataset? What should we do if the domains of the open dataset and the target dataset are different?
W4. The applied debiasing algorithms (LfF and Disent) are limited in terms of reweighting methods and may be considered outdated. Can ORBIS effective with sampling-based debiasing method (eg. PGD), contrastive learning-based debiasing methods (ex. CNC, CDvG), mixup-based debiasing methods (ex. selecmix), and logit correction-based debiasing method (ex. LC)?
PGD: Mitigating dataset bias by using per-sample gradient, ICLR 2023
- CNC: Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations, ICML 2022
- CDvG: Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation, ICML 2023
- SelecMix: Debiased Learning by Contradicting-pair Sampling, NeurIPS 2022
- LC: avoiding spurious correlations via logit correction, ICLR 2023
W5. In Table 5, I believe that the extremely high performance of ERM is due to the use of a backbone trained on the open dataset. Therefore, I speculate that the target dataset used here might be very similar to the open dataset. Consequently, it doesn't seem like the ideal target dataset for verifying the effectiveness of ORBIS.

Review Point: - bset -> best - Page 3 - calssification -> classification - v.s. -> vs.
Review Point: - Page 4 - consturcting -> constructing - implictions -> implications - labelign -> labeling - Page 5 - (Figure 2) Bias-conflicting -> Bias-aligned - anootations -> annotations - ...that do note... -> ...that do not...
Review Point: - Page 6 - $f$open -> $f_{open}$ - ...without relying on provided or pseudo-labels. -> without relying on provided [something] or pseudo-labels.
Review Point: - On page 4, in Observation 1, the training instructions are missing. Did you combine the relevant samples with the original training set for training, or did you only use relevant samples?
Review Point: - What metric is reported in Table 3? **Incorrect sentence** - Page 5 'As previously noted, the open dataset D_{open} does not inherently contain samples that are directly relevant to the target task.' - I think the sentence can be revised to 'As previously noted, the open dataset D_{open} might inherently contain samples that are not directly relevant to the target task.' W2. If the open dataset is biased, and the class-wise centroid is biased toward bias-aligned samples, could ORBIS still be helpful for debiasing if all relevant samples are bias-aligned?
==================================================

Focused review:

Despite the strengths above, the paper has the following weaknesses:
1. [Presentation] The notation of the paper is a bit confusing. I suggest the authors put a notation table in the appendix to facilitate the reader to better understand every notation in the paper.
2. [Theorem] I did not understand why proof in Appendix G matters. The re-parameterization is straightforward.
3. [Experiments] Regarding the efficiency of the algorithm, using the wall clock time may be a better choice than the number of queries. This is because the proposed algorithm may have a higher complexity per query.
4. [Code] No sample code is provided, there is a reproduction concern.

Review Point: 1. [Presentation] The notation of the paper is a bit confusing. I suggest the authors put a notation table in the appendix to facilitate the reader to better understand every notation in the paper.
Review Point: 2. [Theorem] I did not understand why proof in Appendix G matters. The re-parameterization is straightforward.
Review Point: 3. [Experiments] Regarding the efficiency of the algorithm, using the wall clock time may be a better choice than the number of queries. This is because the proposed algorithm may have a higher complexity per query.
Review Point: 4. [Code] No sample code is provided, there is a reproduction concern.
==================================================

Focused review:

- Although the authors criticize existing methods as being ``generally inefficient for computing graphs with thousands of nodes,'' they do not provide detailed information on the optimization algorithm. It is mentioned that PyTorch was used, but Page 4 does not include specifics beyond this point.
- There is extensive literature on VAR-based causal learning with various regularization methods. While Section 4's coverage is adequate, it does not clearly distinguish the proposed method from existing approaches.

Review Point: - Although the authors criticize existing methods as being ``generally inefficient for computing graphs with thousands of nodes,'' they do not provide detailed information on the optimization algorithm. It is mentioned that PyTorch was used, but Page 4 does not include specifics beyond this point.
Review Point: - There is extensive literature on VAR-based causal learning with various regularization methods. While Section 4's coverage is adequate, it does not clearly distinguish the proposed method from existing approaches.
==================================================

Focused review:

I don't find this paper particularly well-recognized, and I had a hard time finding some relevant experiment details. Can I clarify:
1. Are all the baseline methods (MSP, Energy, DICE) trained with the original real data? And are the synthetic data the same size as the original OOD data?
2. You are using a 70B model to generate the synthetic data but using 13B or 7B data for the OOD detection task. In a way this is distillation? Have you analyzed the impact of the size of the synthetic data generation model? Would a 7B data be able to generate high-quality OOD data?
This is probably minor but I really don't like the way your wrote your related work section (first two paragraphs). Dumping a bunch of citations with minimal descriptions is not particularly useful.

Review Point: 1. Are all the baseline methods (MSP, Energy, DICE) trained with the original real data? And are the synthetic data the same size as the original OOD data?
Review Point: 2. You are using a 70B model to generate the synthetic data but using 13B or 7B data for the OOD detection task. In a way this is distillation? Have you analyzed the impact of the size of the synthetic data generation model? Would a 7B data be able to generate high-quality OOD data? This is probably minor but I really don't like the way your wrote your related work section (first two paragraphs). Dumping a bunch of citations with minimal descriptions is not particularly useful.
==================================================

Focused review:

1. My primary concern regarding this paper is the limited novelty and technical contributions. The proposed method consists of two main components: a) an objectness network for extracting a three-level, object-centric representation and b) a multi-object reasoning module for unsupervised object discovery.
a. The three-level representation, encompassing object existence, center, and boundary information, has been widely explored in the literature. For example, techniques like the Hough transform and Chamfer distance have been extensively used to capture similar representations, where object centers and boundaries are encoded. Using these inherently class-agnostic representations for unsupervised object segmentation is a relatively straightforward extension, limiting the degree of novelty.
b. The multi-object reasoning module, composed of four sequential steps, is designed in a heuristic way. Each step processes the features from individual representation levels, potentially hindering the model's ability to fully exploit the interdependencies between these levels.
2. In general, this paper is clearly written. However, to further enhance its clarity and impact, the following suggestions may be considered:
a. It may be better if Figures 3, 4, and 5 can be integrated into Figure 1. The three-level presentation is repeated several times on pages 1 and 2 with a reference to Figure 1. However, readers may clearly realize what the three-level representation is after seeing the example in Figures 3, 4, and 5 on pages 3 and 4.
b. In Section 2, it would be better to discuss why the proposed method is superior to existing methods, especially those learning object-centric representations with pre-trained features, since the proposed method uses pre-trained features, too.
c. A deeper analysis of the experimental results is necessary. While the paper emphasizes the improved performance of the proposed method, a more in-depth exploration of the underlying reasons for this superiority would strengthen the overall argument.
d. The indexing of the four steps in the reasoning module should be consistent throughout the paper, either using #0 to #3 on page 5 or #1 to #4 on page 6.
3. The sensitivity analysis presented in Table 10 of the supplementary materials is limited in scope. The narrow value ranges of hyperparameter values and the lack of evaluation on multiple datasets hinder a comprehensive assessment of the method's sensitivity to hyperparameter variations across different datasets.

Review Point: 3. The sensitivity analysis presented in Table 10 of the supplementary materials is limited in scope. The narrow value ranges of hyperparameter values and the lack of evaluation on multiple datasets hinder a comprehensive assessment of the method's sensitivity to hyperparameter variations across different datasets.
==================================================

Focused review:

- I partially disagree with the claim that the authors made at the end of the introduction. I think what the authors have shown doesn't mean scaling test-time computing can be preferable to scaling pretraining compute. For both medium and hard questions, using the same compute to scale pertaining works much more in favor of scaling inference compute. The advantage only comes in for easier questions, which I would argue is less important. Plus one can always do inference scaling for larger models. I think this claim may need to be justified more.
- When training a verifier, is it more fair to include the compute to train the verifier as part of the inference-time compute? Similarly for the fine-tuned revision model.
- It is a bit difficult to fathom what "compute optimal" is exactly. How is this obtained or how is it optimized? I understand that strategies are selected based on question difficulties but providing the exact detail would be nice.
- The separation of Verifier and Revision is a bit confusing as both require a PRM. The main distinction I think between sections 5 and 6 is one is using search and another is using revision.

Review Point: - I partially disagree with the claim that the authors made at the end of the introduction. I think what the authors have shown doesn't mean scaling test-time computing can be preferable to scaling pretraining compute. For both medium and hard questions, using the same compute to scale pertaining works much more in favor of scaling inference compute. The advantage only comes in for easier questions, which I would argue is less important. Plus one can always do inference scaling for larger models. I think this claim may need to be justified more.
Review Point: - When training a verifier, is it more fair to include the compute to train the verifier as part of the inference-time compute? Similarly for the fine-tuned revision model.
Review Point: - It is a bit difficult to fathom what "compute optimal" is exactly. How is this obtained or how is it optimized? I understand that strategies are selected based on question difficulties but providing the exact detail would be nice.
Review Point: - The separation of Verifier and Revision is a bit confusing as both require a PRM. The main distinction I think between sections 5 and 6 is one is using search and another is using revision.
==================================================

Focused review:

The paper does not suffer from a clear weakness. Although there are some room for improvement:
1) Please read the paper and correct the typos. For example, there is extra space before the comma in line 75.
2) Please fix the typos in the paper. For example, line 329 refers to Theorem 5.5 but I think it should be Lemma 5.5.
3) Without clear explanation of the results of the theorems and lemmas it is a bit hard to clearly think about the results and appreciate them. Given the space limitation in the paper, I suggest that you add clear discussions about the results obtained in the main text in the appendix. In particular, more discussions about the "interpretation" of the theoretical results can be added in the appendix to help the reader to appreciate the results.

Review Point: 3) Without clear explanation of the results of the theorems and lemmas it is a bit hard to clearly think about the results and appreciate them. Given the space limitation in the paper, I suggest that you add clear discussions about the results obtained in the main text in the appendix. In particular, more discussions about the "interpretation" of the theoretical results can be added in the appendix to help the reader to appreciate the results.
==================================================

Focused review:

The paper could use some more polishing in its presentation:
* The geometric ergodicity of the sequence of updates is a crucial aspect, yet is not defined in the preliminaries. It would also be valuable to add an intuitive explanations for how this fact would be used to derive the risk bounds.
* $D_{{\rm last}, 2}$ used eq. (19) in line 222 is not defined.
* In line 480, the beginning of the sentence should be "Directions ..."
* The authors should also make explicit the fact that the number of samples, $n$, needs to be known a priori to optimize the step size $\gamma$.

Review Point: * The geometric ergodicity of the sequence of updates is a crucial aspect, yet is not defined in the preliminaries. It would also be valuable to add an intuitive explanations for how this fact would be used to derive the risk bounds.
Review Point: * $D_{{\rm last}, 2}$ used eq. (19) in line 222 is not defined.
Review Point: * In line 480, the beginning of the sentence should be "Directions ..." * The authors should also make explicit the fact that the number of samples, $n$, needs to be known a priori to optimize the step size $\gamma$.
==================================================

Focused review:

1. I personally believe that the entire article seems to have been improved and summarized from practical applications. There is relatively little theoretical reasoning in the article, which does not reflect the innovation in the theoretical aspect. And the content of Figure 2 is not concise and clear, it is recommended to supplement the Framework diagram to introduce the model.
2. The biggest problem with the experiments is that although the model proposes a method based on a large model, general indicator analyses such as BLEU-1, BLEU-4, METR, and CIDEr are not shown in the experimental section for Image Captioning tasks. Moreover, no BLIP has online test scores based on the MSCOCO dataset, which are not mentioned in the article. At the same time, there is also a lack of performance comparison with mainstream Image Captioning models in recent years, and the baseline performance of BLIP is not the best in Image Captioning. Without these key experiments and indicators, it is impossible to show the innovation and progressiveness of the methods in the field of Image Captioning.
3. Through Rebuttal, the author provided some supplementary explanations for Weakness 2 (experimental design), but objectively speaking, the readability of this article is somewhat lacking, and some essential details are not handled carefully enough.

Review Point: 1. I personally believe that the entire article seems to have been improved and summarized from practical applications. There is relatively little theoretical reasoning in the article, which does not reflect the innovation in the theoretical aspect. And the content of Figure 2 is not concise and clear, it is recommended to supplement the Framework diagram to introduce the model.
Review Point: 3. Through Rebuttal, the author provided some supplementary explanations for Weakness 2 (experimental design), but objectively speaking, the readability of this article is somewhat lacking, and some essential details are not handled carefully enough.
==================================================

Focused review:

- The performance of the proposed method falls short in comparison to relevant benchmarks. For instance, in the case of the arXiv dataset, the zero-shot accuracy of 73.50 achieved with GPT-3.5, as reported in [1], sets a baseline for comparison. The independent reasoning approach employed for node feature augmentation in this paper essentially replicates the zero-shot task, where the model is provided with the paper's title, abstract, and an instruction to classify the paper. Given this, the expected outcome should be at least matching the accuracy achieved by this straightforward baseline. However, the results on the arXiv dataset do not convincingly demonstrate the effectiveness of the proposed LLM-augmentation+ GCL method, especially if it doesn't outperform vanilla LLM performance. A potential solution could be to report the accuracy of node classification following both independent reasoning and structure-aware reasoning in the node feature augmentation.
- Similar concerns arise with the performance on the PubMed dataset, where the reported zero-shot accuracy is 93.42. References:
[1] Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning, https://arxiv.org/abs/2305.19523

Review Point: - Similar concerns arise with the performance on the PubMed dataset, where the reported zero-shot accuracy is 93.42. References: [1] Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning, https://arxiv.org/abs/2305.19523
==================================================

Focused review:

- The proposed method of modifying the model architecture (replacing the original tokenizer) to elicit the jailbreak does not make much sense; also, the perturbed (attacked) images lack transferability. Given that a text-based attack is already feasible to pose such threats, I tend to buy the proposed method that applies the traditional method of generating adversarial perturbations to a multimodal fusion model. This method, however, is neither novel nor practically applicable to my understanding.
- Using adversarial images to elicit model jailbreak is also not novel; the paper lacks some discussion and comparison with existing works on VLLM [1].
[1] Visual Adversarial Examples Jailbreak Aligned Large Language Models (AAAI 2024)

Review Point: - The proposed method of modifying the model architecture (replacing the original tokenizer) to elicit the jailbreak does not make much sense; also, the perturbed (attacked) images lack transferability. Given that a text-based attack is already feasible to pose such threats, I tend to buy the proposed method that applies the traditional method of generating adversarial perturbations to a multimodal fusion model. This method, however, is neither novel nor practically applicable to my understanding.
Review Point: - Using adversarial images to elicit model jailbreak is also not novel; the paper lacks some discussion and comparison with existing works on VLLM [1]. [1] Visual Adversarial Examples Jailbreak Aligned Large Language Models (AAAI 2024)
==================================================

Focused review:

1. The Introduction is confusing. There is no logical relationship between the three paragraphs of the introduction, and it is unclear what the author is trying to do within the three paragraphs. How do existing methods do it? What problems do they have? What is the difference between the proposed method and the above methods? What is different and innovative about the proposed method?
2. In related work, what is the relationship between the three parts of the introduction? What problems do existing methods have that should be further elaborated?
3. The proposed method is too simple. It feels like a patchwork of existing methods. Compared with existing methods, I don’t know where the core innovation of the proposed method is.
4. The experimental results are unreliable. First, there is a lack of comparison with the most advanced methods. Second, the analysis is insufficient for ablation studies. It is unclear why each of the proposed components can bring performance improvements.

Review Point: 1. The Introduction is confusing. There is no logical relationship between the three paragraphs of the introduction, and it is unclear what the author is trying to do within the three paragraphs. How do existing methods do it? What problems do they have? What is the difference between the proposed method and the above methods? What is different and innovative about the proposed method?
Review Point: 2. In related work, what is the relationship between the three parts of the introduction? What problems do existing methods have that should be further elaborated?
Review Point: 3. The proposed method is too simple. It feels like a patchwork of existing methods. Compared with existing methods, I don’t know where the core innovation of the proposed method is.
Review Point: 4. The experimental results are unreliable. First, there is a lack of comparison with the most advanced methods. Second, the analysis is insufficient for ablation studies. It is unclear why each of the proposed components can bring performance improvements.
==================================================

Focused review:

1. The core idea is simple and limited novel. The utilization of different modalities is widely validated in many general computer vision tasks, i.e., human action recognition, action temporal localization, etc. Directly stacking multiple modalities to enhance network performance makes me question the novelty of the paper. The author should explain their contributions in more detail.
2. The proposed fusion strategy includes MLP, Attention and Convolution. The results in Table 3 reveals the superiority of using MLP. However, the author did not analyze why MLP outperforms the other two fusion methods, but merely compared them in terms of performance. I suggest that a more in-depth discussion would help reviewers better understand your method.
3. The paper claims that "By incorporating video, keypoints and optical flow, our model can effectively capture movement patterns and hand gestures ". However, I cannot find any experiment or visualization results to support this point.
4. The paper claims the distillation technique could maintain the high performance while reducing the computational cost and resource requirements. I believe that a comparison of training time, parameter count, and data preprocessing time between this method and other methods should be provided to support this point. From my viewpoint, keypoint extraction and optical flow computation consume much time.
5. The proposed sign pyramid network is similar to the architecture in the TwoStream-SLR [chen et al., 2022b]. I suggest the author rigorously explains the differences or cites this paper in the proper position.

Review Point: 1. The core idea is simple and limited novel. The utilization of different modalities is widely validated in many general computer vision tasks, i.e., human action recognition, action temporal localization, etc. Directly stacking multiple modalities to enhance network performance makes me question the novelty of the paper. The author should explain their contributions in more detail.
Review Point: 2. The proposed fusion strategy includes MLP, Attention and Convolution. The results in Table 3 reveals the superiority of using MLP. However, the author did not analyze why MLP outperforms the other two fusion methods, but merely compared them in terms of performance. I suggest that a more in-depth discussion would help reviewers better understand your method.
Review Point: 3. The paper claims that "By incorporating video, keypoints and optical flow, our model can effectively capture movement patterns and hand gestures ". However, I cannot find any experiment or visualization results to support this point.
Review Point: 4. The paper claims the distillation technique could maintain the high performance while reducing the computational cost and resource requirements. I believe that a comparison of training time, parameter count, and data preprocessing time between this method and other methods should be provided to support this point. From my viewpoint, keypoint extraction and optical flow computation consume much time.
Review Point: 5. The proposed sign pyramid network is similar to the architecture in the TwoStream-SLR [chen et al., 2022b]. I suggest the author rigorously explains the differences or cites this paper in the proper position.
==================================================

Focused review:

* The settings of UND is more commonly addressed as anomaly detection/one-class classification problems, where there are already significant development in this field with many renowned benchmarks (e.g., MVTEC) and methods (e.g., DeepSVDD, PaDIM, CutPaste, etc.). I think the MNIST dataset is the easiest one and while the authors ignored a significant body of competitive baselines and benchmarks.
* The method only separates background from the subjects, which is a coarse-grained feature in images. For instance, a simple OTSU method preinstall in open-cv can isolate the subject from the background, while training on these processed subject image can enable to model to learn sufficient subject information. In this sense, authors should strengthen the motivation and technical contribution of the proposed method.
* Related to above, the experiments are insufficient to meet the bar. Although the authors present a lot of tables, they are mainly on the same datasets (MNIST-like) and only quote different metrics. It is expected that the method to be applied to mode modern datasets (at least TinyImageNet level with 200 classes) to validate its scalability and generalizability to other datasets. Additional metrics are fine but should be presented in the Appendix.
* While checking the codes, the authors only tested their method on simple CNNs, where the commonly used architectures in the field of novelty detection, such as ResNet and ViT, were not tested. This also bring concerns on the scalability or applicability of the proposed method on more modern vision models.
* Although K’ is defined in problem formulations, the authors did not present how they resolve novel background types from the new samples. It seems that the K’ new backgrounds (unseen in any training samples) will still be fitted in the K GMM components, which severely limits its capability on new background types. Minor:
* Many baselines are of very low performance. This posts questions on whether these baselines are too simple. For instance, DeepSVDD, although a renowned method, is already 6 years old. Authors are suggested to choose more competitive baselines (e.g., the following works of ERM).
* It would be preferred if the standard deviation of each experiment is provided, instead of just reporting the means.

Review Point: * The settings of UND is more commonly addressed as anomaly detection/one-class classification problems, where there are already significant development in this field with many renowned benchmarks (e.g., MVTEC) and methods (e.g., DeepSVDD, PaDIM, CutPaste, etc.). I think the MNIST dataset is the easiest one and while the authors ignored a significant body of competitive baselines and benchmarks.
Review Point: * The method only separates background from the subjects, which is a coarse-grained feature in images. For instance, a simple OTSU method preinstall in open-cv can isolate the subject from the background, while training on these processed subject image can enable to model to learn sufficient subject information. In this sense, authors should strengthen the motivation and technical contribution of the proposed method.
Review Point: * Related to above, the experiments are insufficient to meet the bar. Although the authors present a lot of tables, they are mainly on the same datasets (MNIST-like) and only quote different metrics. It is expected that the method to be applied to mode modern datasets (at least TinyImageNet level with 200 classes) to validate its scalability and generalizability to other datasets. Additional metrics are fine but should be presented in the Appendix.
Review Point: * While checking the codes, the authors only tested their method on simple CNNs, where the commonly used architectures in the field of novelty detection, such as ResNet and ViT, were not tested. This also bring concerns on the scalability or applicability of the proposed method on more modern vision models.
Review Point: * Although K’ is defined in problem formulations, the authors did not present how they resolve novel background types from the new samples. It seems that the K’ new backgrounds (unseen in any training samples) will still be fitted in the K GMM components, which severely limits its capability on new background types. Minor:
Review Point: * Many baselines are of very low performance. This posts questions on whether these baselines are too simple. For instance, DeepSVDD, although a renowned method, is already 6 years old. Authors are suggested to choose more competitive baselines (e.g., the following works of ERM).
Review Point: * It would be preferred if the standard deviation of each experiment is provided, instead of just reporting the means.
==================================================

Focused review:

1. Such strategy requires extra parallel data, which might not exist in many datasets/tasks especially during the pre-training stage. The authors did not consider such cases to propose some cheap ways to acquire such parallel data. Also, utilizing the parallel data for training increase the size of context window, which require larger context window for models and might be expensive.
2. The question answering requires the template mapping to transform the question into a masked statement, which might cause the poor generalization to questions that are not 'Wh-types'/transformable.

Review Point: 1. Such strategy requires extra parallel data, which might not exist in many datasets/tasks especially during the pre-training stage. The authors did not consider such cases to propose some cheap ways to acquire such parallel data. Also, utilizing the parallel data for training increase the size of context window, which require larger context window for models and might be expensive.
Review Point: 2. The question answering requires the template mapping to transform the question into a masked statement, which might cause the poor generalization to questions that are not 'Wh-types'/transformable.
==================================================

Focused review:

1. All experiments were conducted on several locomotion tasks within the reinforcement learning context, which I believe is unacceptable. HPO has too many application scenarios, and I strongly recommend increasing the variety, such as the experiments in PB2 and PB2-Mix.
2. Other HPO methods (such as Bayesian optimization, e.g.,[1-2] ) are also recommended for comparison. Their experiments are also should be considered to run.
[1] Deep power laws for hyperparameter optimization.
[2] In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization.

Review Point: 1. All experiments were conducted on several locomotion tasks within the reinforcement learning context, which I believe is unacceptable. HPO has too many application scenarios, and I strongly recommend increasing the variety, such as the experiments in PB2 and PB2-Mix.
Review Point: 2. Other HPO methods (such as Bayesian optimization, e.g.,[1-2] ) are also recommended for comparison. Their experiments are also should be considered to run. [1] Deep power laws for hyperparameter optimization. [2] In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization.
==================================================

Focused review:

Weakness:
May need some interpretations for some weird cases in the Table and Figures.
For example,
1). In Table 4, it looks more context does not help for Medium tasks?
2). Similarly, in Figure 3, it looks MLP is quite a strong baseline on the Hard tasks?

Review Point: 1). In Table 4, it looks more context does not help for Medium tasks?
Review Point: 2). Similarly, in Figure 3, it looks MLP is quite a strong baseline on the Hard tasks?
==================================================

Focused review:

- One key problem is that the authors motivate their method e.g. by: " Filtering methods like Kalman and extended Kalman filtering Chen et al. (2023) effectively suppress noise and reduce output oscillation by estimating the current state from multi-step historical data. These methods work well with Gaussian noise but struggle with non-Gaussian noise."
However, in the experiment the authors only test on settings with Gaussian noise:
- MuJoco: Table 2,3 and 4 specify a Gaussian noise level
- vehicle trajectory tracking environment: it is unclear what the noise shape is here (perhaps its partial-observable and thus exhibits non-Gaussian noise). In that case the authors should perform an analysis of the shape of stochasticity in this benchmark.
Either way: I would advise either testing again a Filtering method, such as an extended Kalman filter, re-designing the experiments under non-Gaussian settings and/or clearly present how the vehicle trajectory tracking is a RL problem with non-standard noise.

Review Point: - One key problem is that the authors motivate their method e.g. by: " Filtering methods like Kalman and extended Kalman filtering Chen et al. (2023) effectively suppress noise and reduce output oscillation by estimating the current state from multi-step historical data. These methods work well with Gaussian noise but struggle with non-Gaussian noise." However, in the experiment the authors only test on settings with Gaussian noise:
Review Point: - MuJoco: Table 2,3 and 4 specify a Gaussian noise level - vehicle trajectory tracking environment: it is unclear what the noise shape is here (perhaps its partial-observable and thus exhibits non-Gaussian noise). In that case the authors should perform an analysis of the shape of stochasticity in this benchmark. Either way: I would advise either testing again a Filtering method, such as an extended Kalman filter, re-designing the experiments under non-Gaussian settings and/or clearly present how the vehicle trajectory tracking is a RL problem with non-standard noise.
==================================================

Focused review:

1. The paper's writing needs significant improvement. The writing can be made more clear. This clarity can also highlight the motivation behind this work even better as for now it is not super clear to me how this way of probing models in a generative manner does not have issues/challenges (e.g., reliability and faithfulness) that previous discriminative based probing approaches have. These things along with the overall writing of the paper needs to be improved.
2. I am not sure how reliable the trained classifier introduced in section 3.1 is. I think more ablations needs to be done to validate and justify the use of this classifier.
3. How diverse the generated prompts are? I think some ablations on this aspect needs to be done. In general, I feel like the ablation studies need to be strengthen to validate whether the generated prompts are indeed meaningful and diverse enough.
4. VILMO is only tested on ChatGPT while previous studies were more comprehensive in terms of probing more models. I think it would be good to evaluate a larger pool of models to validate effectiveness of VILMO.
5. PPL and SB metrics lack for the VILMO approach compared to the baseline it would be good to see if approaches can be implemented to better control this trade-off.
6. For the human evaluations sample size is too small.

Review Point: 1. The paper's writing needs significant improvement. The writing can be made more clear. This clarity can also highlight the motivation behind this work even better as for now it is not super clear to me how this way of probing models in a generative manner does not have issues/challenges (e.g., reliability and faithfulness) that previous discriminative based probing approaches have. These things along with the overall writing of the paper needs to be improved.
Review Point: 2. I am not sure how reliable the trained classifier introduced in section 3.1 is. I think more ablations needs to be done to validate and justify the use of this classifier.
Review Point: 3. How diverse the generated prompts are? I think some ablations on this aspect needs to be done. In general, I feel like the ablation studies need to be strengthen to validate whether the generated prompts are indeed meaningful and diverse enough.
Review Point: 4. VILMO is only tested on ChatGPT while previous studies were more comprehensive in terms of probing more models. I think it would be good to evaluate a larger pool of models to validate effectiveness of VILMO.
Review Point: 5. PPL and SB metrics lack for the VILMO approach compared to the baseline it would be good to see if approaches can be implemented to better control this trade-off.
Review Point: 6. For the human evaluations sample size is too small.
==================================================

Focused review:

1.The description of DPS is unclear and lacks details:
* On page 7, lines 324-331, the details of Depth-wise Importance Assignment are missing. Specifically, only Equation 7 is given without further explanations, making it difficult for readers to understand how this importance assignment is implemented. Please add more details about it, such as providing a step-by-step explanation of how this assignment is implemented, including any algorithms or pseudocode if applicable.
* When compared with the traditional design matrix, although the proposed Quadtree is more efficient in representing designs, the initial states of Quadtrees are less diverse than design matrices. When using traditional matrices, the initial design matrices can be complex designs, making all candidate designs have equal probabilities to be reached in a limited number of iterations. In comparison, the initial Quadtrees are definitely very simple designs (all states =1 or all states =0). Therefore, for a Quadtree, simple designs have significantly greater chances of being searched than complex designs, especially when maximum iteration $M$ is small. Please provide a discussion or analysis to show that complex designs are adequately explored within $M$ iterations. Alternatively, please provide guidelines on setting $M$ to ensure complex designs are adequately explored.
* On page 4, Algorithm 2, the tree search of the design space is implemented by either resampling a leaf node’s state or splitting a leaf node into four child nodes. But details are not provided in the corresponding paragraph (page 6, lines 317-323). What is the probability of resampling or splitting?
* On page7, it is unclear how Equation 8 can be used to measure the consistency of ordering results. Are both $x_i$ terms in the equation from time point $t$? More explanations should be provided.
* Some experimental setups of DPS are not explained, such as maximum iteration $M$, $K$ in top-$K$ list, and total number of samples $M$ in CSS.
2.The experiments are not solid:
* The experimental setup does not specify the number of runs conducted. Are the results reported in Table 3 obtained from only one run (As no standard deviations are reported in Table 3)? Considering the stochastic processes in DPS and the comparison algorithms, the authors should conduct many runs (e.g. 20 or more) for experiments to ensure the reliability of the results. In addition, the authors should report mean performance and standard deviations in Table 3, and perform appropriate statistical tests to demonstrate that DPS significantly outperforms the comparison algorithms.
* Bayesian optimization (BO) methods have been widely used to solve expensive optimization problems; however, the authors did not compare BO in their experiments and did not explain why it was not considered. Please consider the following options: 1). Include a comparison with a recently proposed BO method in the experiments; 2). Provide a clear explanation for why BO was not considered, discussing any potential limitations or challenges in applying BO to this specific EMS problem.
* Some comparison methods are obsolete. For example, surrogate-GA was proposed decades ago [1]. Although the authors cited a paper published in 2020, the main contribution of this publication was the application of surrogate-GA to special design problems, rather than the development of a novel optimization method. I suggest considering some newly proposed optimization algorithms as comparison methods.
* The comparison experiment in Table 3 is not fair. Some comparison methods, such as DPS and surrogate-GA, have their predictors updated during the optimization, while the predictors in other comparison methods, such as surrogate-assisted RS and invGrad, are trained on offline datasets. Consider running additional fair experiments where all predictors in comparison methods are updated during the optimization.
3. On page 6, Equation 3, there is no explanation about $i$ and $j$ until line 290. Please consider revising the relevant paragraphs to avoid potential confusion.
In addition, some denotations are not consistent throughout the submission. For example, $M$ is the maximum number of iterations on page 4, Algorithm 2, but $M$ is also the number of samples on page 7, Equation 9. $t$ is the time point on page 7, Equation 8, but $t$ is also used to represent frequency in GHz on page 15, line 795 (Appendix A).
4. On page 4, line 204, the optimization problem is formulated with a predictor. However, when the predictor is fitted with limited training data, it is very likely that the optimum found by the predictor is not optimal in simulations. As mentioned in line 210, further validation with high-fidelity simulations is required. Therefore, Equation 2 could be a step in the EMS, but it is inappropriate to formulate the entire EMS problem as Equation 2.
5. Some minor issues:
* In Table 3, the agg obj results of cGAN and cVAE for High-Gain Antenna are incorrect. They should be minimum instead of maximum.
* There are some typos, such as:
1) On page 10, line 492, 'ESS' should be ‘CSS’.
2) On page 15, line 792, 'minimize the maximum' should be 'minimize the minimum’.
* Additionally, there are some grammatical errors, such as 'a expanded' should be 'an expanded.'"
[1] Lim, Dudy, et al. "Generalizing surrogate-assisted evolutionary computation." IEEE Transactions on Evolutionary Computation 14.3 (2009): 329-355.

Review Point: * On page 7, lines 324-331, the details of Depth-wise Importance Assignment are missing. Specifically, only Equation 7 is given without further explanations, making it difficult for readers to understand how this importance assignment is implemented. Please add more details about it, such as providing a step-by-step explanation of how this assignment is implemented, including any algorithms or pseudocode if applicable.
Review Point: * On page 4, Algorithm 2, the tree search of the design space is implemented by either resampling a leaf node’s state or splitting a leaf node into four child nodes. But details are not provided in the corresponding paragraph (page 6, lines 317-323). What is the probability of resampling or splitting?
Review Point: * On page7, it is unclear how Equation 8 can be used to measure the consistency of ordering results. Are both $x_i$ terms in the equation from time point $t$? More explanations should be provided.
Review Point: * Some experimental setups of DPS are not explained, such as maximum iteration $M$, $K$ in top-$K$ list, and total number of samples $M$ in CSS.
Review Point: * The experimental setup does not specify the number of runs conducted. Are the results reported in Table 3 obtained from only one run (As no standard deviations are reported in Table 3)? Considering the stochastic processes in DPS and the comparison algorithms, the authors should conduct many runs (e.g. 20 or more) for experiments to ensure the reliability of the results. In addition, the authors should report mean performance and standard deviations in Table 3, and perform appropriate statistical tests to demonstrate that DPS significantly outperforms the comparison algorithms.
Review Point: * Bayesian optimization (BO) methods have been widely used to solve expensive optimization problems; however, the authors did not compare BO in their experiments and did not explain why it was not considered. Please consider the following options:
Review Point: 1). Include a comparison with a recently proposed BO method in the experiments;
Review Point: 2). Provide a clear explanation for why BO was not considered, discussing any potential limitations or challenges in applying BO to this specific EMS problem.
Review Point: * Some comparison methods are obsolete. For example, surrogate-GA was proposed decades ago [1]. Although the authors cited a paper published in 2020, the main contribution of this publication was the application of surrogate-GA to special design problems, rather than the development of a novel optimization method. I suggest considering some newly proposed optimization algorithms as comparison methods.
Review Point: * The comparison experiment in Table 3 is not fair. Some comparison methods, such as DPS and surrogate-GA, have their predictors updated during the optimization, while the predictors in other comparison methods, such as surrogate-assisted RS and invGrad, are trained on offline datasets. Consider running additional fair experiments where all predictors in comparison methods are updated during the optimization.
Review Point: 3. On page 6, Equation 3, there is no explanation about $i$ and $j$ until line 290. Please consider revising the relevant paragraphs to avoid potential confusion. In addition, some denotations are not consistent throughout the submission. For example, $M$ is the maximum number of iterations on page 4, Algorithm 2, but $M$ is also the number of samples on page 7, Equation 9. $t$ is the time point on page 7, Equation 8, but $t$ is also used to represent frequency in GHz on page 15, line 795 (Appendix A).
Review Point: 4. On page 4, line 204, the optimization problem is formulated with a predictor. However, when the predictor is fitted with limited training data, it is very likely that the optimum found by the predictor is not optimal in simulations. As mentioned in line 210, further validation with high-fidelity simulations is required. Therefore, Equation 2 could be a step in the EMS, but it is inappropriate to formulate the entire EMS problem as Equation 2.
Review Point: 5. Some minor issues:* In Table 3, the agg obj results of cGAN and cVAE for High-Gain Antenna are incorrect. They should be minimum instead of maximum.
Review Point: 1) On page 10, line 492, 'ESS' should be ‘CSS’.
Review Point: 2) On page 15, line 792, 'minimize the maximum' should be 'minimize the minimum’.
Review Point: * Additionally, there are some grammatical errors, such as 'a expanded' should be 'an expanded.'" [1] Lim, Dudy, et al. "Generalizing surrogate-assisted evolutionary computation." IEEE Transactions on Evolutionary Computation 14.3 (2009): 329-355.
==================================================

Focused review:

- The paper primarily utilizes existing models such as CoT and PAL without introducing notable innovations. Its contribution appears to be an application of known methods rather than presenting a transformative approach or fresh insights.
- The paper heavily leans on existing models without introducing significant novel techniques or modifications. This might position the paper as offering incremental improvements rather than groundbreaking contributions.
- The paper's reliance on in-context learning and self-evaluation abilities of LLMs might make the selector sensitive to prompts, potentially limiting its broader applicability.
- While the paper acknowledges LLM biases, it doesn't seem to propose any concrete solutions or mitigation strategies

Review Point: - The paper primarily utilizes existing models such as CoT and PAL without introducing notable innovations. Its contribution appears to be an application of known methods rather than presenting a transformative approach or fresh insights.
Review Point: - The paper heavily leans on existing models without introducing significant novel techniques or modifications. This might position the paper as offering incremental improvements rather than groundbreaking contributions.
Review Point: - The paper's reliance on in-context learning and self-evaluation abilities of LLMs might make the selector sensitive to prompts, potentially limiting its broader applicability.
Review Point: - While the paper acknowledges LLM biases, it doesn't seem to propose any concrete solutions or mitigation strategies
==================================================

Focused review:

1. This work is not well-motivated. It seems estimating the distribution is not necessary since the variance of the learned parameters for each item (teams, fund managers, etc.) already indicates whether the parameters of all items closely center somewhere or spread uniformly. So I don't see the point of a sophisticated second step in the algorithm. This paper does not provide a comparison in the experiments between their approach and the naive approach mentioned above either. 2. Most of this paper seems very technical but the theorems do not qualify as theorems. "sufficiently large constants" and "sufficiently large n" appear in every theorem statement but the authors fail to specify how large is sufficient. These are necessary for theorems.

Review Point: 1. This work is not well-motivated. It seems estimating the distribution is not necessary since the variance of the learned parameters for each item (teams, fund managers, etc.) already indicates whether the parameters of all items closely center somewhere or spread uniformly. So I don't see the point of a sophisticated second step in the algorithm. This paper does not provide a comparison in the experiments between their approach and the naive approach mentioned above either.
Review Point: 2. Most of this paper seems very technical but the theorems do not qualify as theorems. "sufficiently large constants" and "sufficiently large n" appear in every theorem statement but the authors fail to specify how large is sufficient. These are necessary for theorems.
==================================================

Focused review:

- The central claims of the paper are not supported by experimental results: The model performance on both text-to-image taskt is worse with respect to older and smaller models based on pretrained VQ-VAE (e.g MAGVLT [1]), while the improvement on understanding tasks is slight, training for image understanding greatly reduces the performance on image generation.
- The paper does not compare with recent baselines such as VAR models [2] for simple settings like ImageNet256 where VAR achieve 1.92 FID and Jetformer has 6.64 using the same training data and similar model size.
- Optimizing image tokens during training can add instability to the training dynamics, this is especially true in complex large scale multimodal settings.
- The main comparison to validate the usage of this modeling approach with respect to use a VQ-VAE in a controlled setting would be to compare it against a baseline, trained in the exact same setting by substituting the normalizing flow with a VQ-VAE.
- How the approach scales is not clearly analized in the paper by just showing to models scales with relative low difference in parameters count
[1]: MAGVLT: Masked Generative Vision-and-Language Transformer
[2]: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction, Tian et al.

Review Point: - The central claims of the paper are not supported by experimental results: The model performance on both text-to-image taskt is worse with respect to older and smaller models based on pretrained VQ-VAE (e.g MAGVLT [1]), while the improvement on understanding tasks is slight, training for image understanding greatly reduces the performance on image generation.
Review Point: - The paper does not compare with recent baselines such as VAR models [2] for simple settings like ImageNet256 where VAR achieve 1.92 FID and Jetformer has 6.64 using the same training data and similar model size.
Review Point: - Optimizing image tokens during training can add instability to the training dynamics, this is especially true in complex large scale multimodal settings.
Review Point: - The main comparison to validate the usage of this modeling approach with respect to use a VQ-VAE in a controlled setting would be to compare it against a baseline, trained in the exact same setting by substituting the normalizing flow with a VQ-VAE.
Review Point: - How the approach scales is not clearly analized in the paper by just showing to models scales with relative low difference in parameters count [1]: MAGVLT: Masked Generative Vision-and-Language Transformer [2]: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction, Tian et al.
==================================================

Focused review:

1. **The paper writing could be further improved.** For instance, the authors claim the motivation - *'the teacher's output contains barely extra information exceeding GT, thus the “dark knowledge” of the teacher being hardly transferred to the student model through KD'* in Line #76-78 in the Introduction part. However, in Section 3.2 (Motivation) and Figure 2, the authors show the motivation by measuring the PSNR of outputs between the teacher and the student. It seems that the two statements are a little bit contradictory, as the former one indicates that the teacher's outputs can not be good learning materials but the second one leverages the the teacher's outputs as the reference for evaluating whether KD method is good or not. Such circumstances make it hard to understand the central idea of the paper.
2. **The paper lacks further deep analysis of where the performance gains are from.** From the results in Figure 2, it seems that the gains are from improving the fidelity between the teacher and the student. A further question is *Why AugKD can improve the fidelity?* And in Lines #521-529, the authors compare AugKD with data expansion. Thus, a question arises *Does the improvement of fidelity from the expansion of the training set by augmentation?* From another perspective, the question is *how does the augmentation strength affect the fidelity and the final distillation results?* By answering such a series of questions, the paper can help the readers understand the intrinsic mechanism of AugKD.
3. **A minor question about the design of the method.** Although I think the design of the inverse augmentation is clear and plausible, I'm still curious about what would happen if we dropped the inverse augmentation and added the augmentation at the end of the teacher's model in the training stage and still utilized the same architecture as the method in the inference stage.

Review Point: 3. **A minor question about the design of the method.** Although I think the design of the inverse augmentation is clear and plausible, I'm still curious about what would happen if we dropped the inverse augmentation and added the augmentation at the end of the teacher's model in the training stage and still utilized the same architecture as the method in the inference stage.
==================================================

Focused review:

The selection of object categories for evaluation is limited.
- For part-level shape completion, it would be more compelling to include categories with a greater diversity of part shapes rather than focusing primarily on cuboid-like forms. For instance, objects such as globes and lamps in PartNet Mobility exhibit a variety of shapes, including spherical and cylindrical forms, which provide a more comprehensive basis for evaluation. Additionally, the assumption that 'many common articulated objects consist of cuboid-like parts' is not fully substantiated when considering the full range of object categories in PartNet-Mobility.
- In articulation prediction, the formulation assumes that 'the position of corresponding revolute joints will lie closely to, if not overlap with, one of the OBB edges'. However, this assumption seems not to be solid enough either. Take “folding chairs” in PartNet-Mobility for example, the revolute joints of many instances lie not close enough to OBB edges (quadrisection point or even trisection point). Do these assumptions restrict the range of categories suitable for evaluation?

Review Point: - For part-level shape completion, it would be more compelling to include categories with a greater diversity of part shapes rather than focusing primarily on cuboid-like forms. For instance, objects such as globes and lamps in PartNet Mobility exhibit a variety of shapes, including spherical and cylindrical forms, which provide a more comprehensive basis for evaluation. Additionally, the assumption that 'many common articulated objects consist of cuboid-like parts' is not fully substantiated when considering the full range of object categories in PartNet-Mobility.
Review Point: - In articulation prediction, the formulation assumes that 'the position of corresponding revolute joints will lie closely to, if not overlap with, one of the OBB edges'. However, this assumption seems not to be solid enough either. Take “folding chairs” in PartNet-Mobility for example, the revolute joints of many instances lie not close enough to OBB edges (quadrisection point or even trisection point). Do these assumptions restrict the range of categories suitable for evaluation?
==================================================

Focused review:

1. __Single Environment Claim__: Although the authors claim that the proposed method can mitigate spurious correlations using data from a single training environment, Assumptions 2 and 3 appear to imply the need for multiple environments when deriving the theoretical guarantees. Additionally, the empirical studies on Colored MNIST utilize two training environments, which seems inconsistent with this claim. It would be beneficial for the authors to conduct experiments using a single training environment and evaluate the method's performance on both synthetic and semi-synthetic datasets.
2. Assumption 1 appears to be more of an intuitive conjecture, lacking formal theoretical support.
3. __Missing Related Work__: Some relevant related works have been omitted. First, the concept of reweighting to mimic perfect interventions on spurious features for improving distributional robustness has been discussed in [1] and [2]. Additionally, there is a body of work focused on improving group distributional robustness based on the understanding that ERM tends to learn spurious correlations ([3], [4], [5]). The proposed method seems to share similarities with these works. It would be helpful if the authors could discuss the novelty of their approach and how it fills a gap compared to these existing works.
4. __Subsampling and Overfitting Concerns__: The authors use subsampling to remove the dependence between the label and spurious features. However, spurious correlations often occur in highly imbalanced data distributions, and subsampling in such cases could lead to dropping a substantial portion of the data from majority groups. This may increase the risk of overfitting, especially if the remaining dataset is small. It would be great if the authors could address how they mitigate the risk of overfitting in this scenario.
5. __Validation Environment Concerns__: When assessing whether spurious features are learned in the training environment, the authors propose using a validation environment. This appears to contradict the single-training-environment assumption. One of the benefits of the single-environment setting is the reduced requirement for environment labels or predefined environment divisions. However, if a validation environment is required, this benefit is lost. Furthermore, the validity of the test may depend on the level of distributional shift between the training and validation environments. If the shift is minimal, the test might incorrectly conclude that ERM has learned the causal feature. Clarification on these points would be great.
6. __Experimental Setup for WaterBirds Dataset__: Could the authors provide more details regarding the experimental setup for the WaterBirds dataset?
7. __Discussion on Poor Performance in Heterogeneous Training Environments__: The experimental results on Colored MNIST indicate that FMI performs poorly when the training environments are highly heterogeneous. Specifically, when training environments are (0.2, 0.9) or (0.1, 0.9) and the test environment is (0.1) or (0.2), the performance degrades. A detailed discussion on the reasons behind this poor performance and potential ways to address it would be helpful.
8. Minor typo: in line 245, $i, j \in \\{1,2\\}$ should be $i, j \in \\{0,1\\}$?
[1] Makar, Maggie, et al. "Causally motivated shortcut removal using auxiliary labels." International Conference on Artificial Intelligence and Statistics. PMLR, 2022.
[2] Veitch, Victor, et al. "Counterfactual invariance to spurious correlations in text classification." Advances in neural information processing systems 34 (2021): 16196-16208.
[3] Liu, Evan Z., et al. "Just train twice: Improving group robustness without training group information." International Conference on Machine Learning. PMLR, 2021.
[4] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. "Last layer re-training is sufficient for robustness to spurious correlations." arXiv preprint arXiv:2204.02937 (2022).
[5] Yang, Yu, et al. "Identifying spurious biases early in training through the lens of simplicity bias." International Conference on Artificial Intelligence and Statistics. PMLR, 2024.

Review Point: 1. __Single Environment Claim__: Although the authors claim that the proposed method can mitigate spurious correlations using data from a single training environment, Assumptions 2 and 3 appear to imply the need for multiple environments when deriving the theoretical guarantees. Additionally, the empirical studies on Colored MNIST utilize two training environments, which seems inconsistent with this claim. It would be beneficial for the authors to conduct experiments using a single training environment and evaluate the method's performance on both synthetic and semi-synthetic datasets.
Review Point: 2. Assumption 1 appears to be more of an intuitive conjecture, lacking formal theoretical support.
Review Point: 3. __Missing Related Work__: Some relevant related works have been omitted. First, the concept of reweighting to mimic perfect interventions on spurious features for improving distributional robustness has been discussed in [1] and [2]. Additionally, there is a body of work focused on improving group distributional robustness based on the understanding that ERM tends to learn spurious correlations ([3], [4], [5]). The proposed method seems to share similarities with these works. It would be helpful if the authors could discuss the novelty of their approach and how it fills a gap compared to these existing works.
Review Point: 4. __Subsampling and Overfitting Concerns__: The authors use subsampling to remove the dependence between the label and spurious features. However, spurious correlations often occur in highly imbalanced data distributions, and subsampling in such cases could lead to dropping a substantial portion of the data from majority groups. This may increase the risk of overfitting, especially if the remaining dataset is small. It would be great if the authors could address how they mitigate the risk of overfitting in this scenario.
Review Point: 6. __Experimental Setup for WaterBirds Dataset__: Could the authors provide more details regarding the experimental setup for the WaterBirds dataset?
Review Point: 7. __Discussion on Poor Performance in Heterogeneous Training Environments__: The experimental results on Colored MNIST indicate that FMI performs poorly when the training environments are highly heterogeneous. Specifically, when training environments are (0.2, 0.9) or (0.1, 0.9) and the test environment is (0.1) or (0.2), the performance degrades. A detailed discussion on the reasons behind this poor performance and potential ways to address it would be helpful.
==================================================

Focused review:

While the proposed method, ZODIAC, addresses a well-defined clinical problem of generating ECG reports and integrates valuable clinical insights through multi-agent collaboration, concerns remain regarding the rigor of its evaluation.
1. Evaluation Metric: The metrics defined in Table 1 are well-defined for qualitative evaluation, but appear subject to evaluator variability, raising concerns about the objectivity and rigor of its scale in Table 2. The following points are regarding the rigor of this evaluation method which would be expected to be improved in the following submission of the paper.
- Subjectivity of the Metric: The degree of subjectivity within the metrics may not be fully addressed. For example, what are the precise criteria for defining hallucination and bias within the FFH metric? Furthermore, the definition of bias is unclear regarding which 'characteristics' of the patient are considered. Does this refer to patient demographics, clinical features, or another criterion?
- Calibration of the Metric: There are also concerns about whether this metric can serve as a reliable, single quantitative standard for evaluating model outputs. A rigorous calibration process, such as Inter-Rater Reliability (IRR), would help substantiate the metric’s consistency and could address potential variability. This is particularly important because different evaluators may interpret the scale differently—for instance, what a score of 3 represents could vary among physicians.
- Details on Inter-Rater Reliability (IRR): The paper leaves IRR and confidence intervals unexplored, which could suggest inconsistencies in clinical outputs. Addressing IRR would help to ensure that ratings are stable and comparable across evaluators. There are standard deviations reported in Table 2, but it is better to separately report inter-rater confidence intervals as well.
2. Ablation Studies on $\theta_{M2F}$ only or $\theta_{T2F}$ only needed to see the effect of Triple agent vs. Double agents and the effect of leveraging metadata and ECG tracings.
3. There are more recent works on multi-agent collaboration or interaction, which could be included in the related works section of the paper.
[1] Kim, Y., Park, C., Jeong, H., Chan, Y. S., Xu, X., McDuff, D., ... & Park, H. W. (2024). Adaptive Collaboration Strategy for LLMs in Medical Decision Making. arXiv preprint arXiv:2404.15155.
[2] Jin, Q., Wang, Z., Yang, Y., Zhu, Q., Wright, D., Huang, T., … & Lu, Z. (2024). AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning.
[3] Li, J., Wang, S., Zhang, M., Li, W., Lai, Y., Kang, X., ... & Liu, Y. (2024). Agent hospital: A simulacrum of hospital with evolvable medical agents.
[4] Fan, Z., Tang, J., Chen, W., Wang, S., Wei, Z., Xi, J., ... & Zhou, J. (2024). Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis.
[5] Yan, W., Liu, H., Wu, T., Chen, Q., Wang, W., Chai, H., ... & Zhu, L. (2024). ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World.

Review Point: 1. Evaluation Metric: The metrics defined in Table 1 are well-defined for qualitative evaluation, but appear subject to evaluator variability, raising concerns about the objectivity and rigor of its scale in Table 2. The following points are regarding the rigor of this evaluation method which would be expected to be improved in the following submission of the paper.
Review Point: - Subjectivity of the Metric: The degree of subjectivity within the metrics may not be fully addressed. For example, what are the precise criteria for defining hallucination and bias within the FFH metric? Furthermore, the definition of bias is unclear regarding which 'characteristics' of the patient are considered. Does this refer to patient demographics, clinical features, or another criterion?
Review Point: - Calibration of the Metric: There are also concerns about whether this metric can serve as a reliable, single quantitative standard for evaluating model outputs. A rigorous calibration process, such as Inter-Rater Reliability (IRR), would help substantiate the metric’s consistency and could address potential variability. This is particularly important because different evaluators may interpret the scale differently—for instance, what a score of 3 represents could vary among physicians.
Review Point: - Details on Inter-Rater Reliability (IRR): The paper leaves IRR and confidence intervals unexplored, which could suggest inconsistencies in clinical outputs. Addressing IRR would help to ensure that ratings are stable and comparable across evaluators. There are standard deviations reported in Table 2, but it is better to separately report inter-rater confidence intervals as well.
Review Point: 2. Ablation Studies on $\theta_{M2F}$ only or $\theta_{T2F}$ only needed to see the effect of Triple agent vs. Double agents and the effect of leveraging metadata and ECG tracings.
==================================================

Focused review:

1. In the simulation and annotation steps, the simulators I-CNF and EDL rely on systems that are already well-established in prior research, limiting the novelty of the annotation procedure.
2. The equations in the paper could benefit from better formatting, as double references, such as "Eqn. equation," detract from readability.
3. Many important points and comparisons are located in the appendix. These should be moved to the main sections to improve the readability. The writing should be improved.

Review Point: 1. In the simulation and annotation steps, the simulators I-CNF and EDL rely on systems that are already well-established in prior research, limiting the novelty of the annotation procedure.
Review Point: 2. The equations in the paper could benefit from better formatting, as double references, such as "Eqn. equation," detract from readability.
Review Point: 3. Many important points and comparisons are located in the appendix. These should be moved to the main sections to improve the readability. The writing should be improved.
==================================================

Focused review:

While the application of influence functions to assess the contribution of individual data points is compelling, the methodology relies heavily on manually curated validation sets, as evidenced by the ablation experiments. There are two primary concerns:
1) Dependence on Domain Knowledge: The construction of validation sets requires domain-specific knowledge, which could limit generalizability. Although addressing verbosity and sycophancy is valuable, the methodology appears capable of handling other issues if suitable validation sets are available. However, this adaptability is currently contingent on creating each validation set by hand.
2) Risk of Conflicting Improvements: It is unclear how the approach manages potential conflicts between multiple improvement directions. For instance, in the main results (Figure 2), Influence shows higher TPR than GPT-4o at the same FPR, which is promising. However, it raises the question: Does this improvement come at the expense of other metrics? To fully assess the model’s overall robustness, it would be beneficial to provide additional statistics comparing the base model and the Influence-based approach to ensure that the latter does not merely overfit to specific biases, such as length.
Furthermore, as noted in the limitations, the feasibility of improving labelers’ performance in real-world settings may be limited, especially when human labelers’ judgments involve complex decision-making beyond linear reward models. The authors could strengthen their argument by including results from human subjects whose labeling strategies demonstrably improve with guidance. Alternatively, if the current scope remains focused on bias correction within datasets, it would be beneficial to clarify this in the introduction to avoid misleading interpretations about broader applicability.

Review Point: 1) Dependence on Domain Knowledge: The construction of validation sets requires domain-specific knowledge, which could limit generalizability. Although addressing verbosity and sycophancy is valuable, the methodology appears capable of handling other issues if suitable validation sets are available. However, this adaptability is currently contingent on creating each validation set by hand.
==================================================

Focused review:

- The problem addressed is quite limited: scaling up for a specific problem of modular addition tested over uniform distirbution. While the work provided some motivation, it is still unclear what's the impact of the work for future research/applications.
- It is unclear if the technical contributions are significant. The changes proposed are natural and not surprising. Furthermore, although the work tested on a few other modular arithmetic problems, those problems are specific and the evaluation is quite preliminary. It is unclear if the techniques can help for more general learning settings eg other algebraic reasoning tasks.

Review Point: - The problem addressed is quite limited: scaling up for a specific problem of modular addition tested over uniform distirbution. While the work provided some motivation, it is still unclear what's the impact of the work for future research/applications.
Review Point: - It is unclear if the technical contributions are significant. The changes proposed are natural and not surprising. Furthermore, although the work tested on a few other modular arithmetic problems, those problems are specific and the evaluation is quite preliminary. It is unclear if the techniques can help for more general learning settings eg other algebraic reasoning tasks.
==================================================

Focused review:

1) In the experiments with BioT5+ and MolT5, the authors used the BLEU score on prompts and molecules from ChEBI-20. I think BLEU is a misleading score in the text to molecule task. For example, in BLEU, the order of words in a sentence matters, while there are many ways of writing a molecules as SMILES strings. Also, small changes in a molecule SMILES in the right place can have high changes in a property, such as in the case of hydrogen bond donors and acceptors. I recommend the authors discuss this in their manuscript and compare it with scores generated by RDKit (which they used in the other experiments).
2) There are a few minor things that could improve the paper:
- Figure 7 could have different symbols. It was hard to read in a black-and-white version of the paper.
- Worth considering giving the method a name, this will help wider adoption.

Review Point: 2) There are a few minor things that could improve the paper:
Review Point: - Figure 7 could have different symbols. It was hard to read in a black-and-white version of the paper.
Review Point: - Worth considering giving the method a name, this will help wider adoption.
==================================================

Focused review:

If the authors can address the following points to this reviewer's satisfaction, I would be happy to increase my score.
1. The paper mentions the CausalTime pipeline as prior work, noting that it requires a predefined expert graph and lacks interactive features. The contributions of the proposed interface are clearly outlined in Section 4, but it is unclear whether other interfaces or pipelines include these options as well. A table or summary of similarities and differences between the proposed library and prior work would be helpful for distinguishing contributions. For example, bold paragraphs in Section 4 could serve as table column headers with prior work and proposed methods as row headers, using check marks or X’s to denote the capabilities of each interface/pipeline.
2. An outline of how synthetic data is generated is not provided/clear. A section that outlines the synthetic data generation process/notation and ties it to the example synthetic data generated in Figure 2 would be helpful.

Review Point: 2. An outline of how synthetic data is generated is not provided/clear. A section that outlines the synthetic data generation process/notation and ties it to the example synthetic data generated in Figure 2 would be helpful.
==================================================

Focused review:

* Training world models might be very time-consuming.
* Proposed approach lead to the improvement only in approximately 58% of cases. And performance might be decreased dramatically. I've also calculated the differences between "+ COCOA" and "Alone" in Table 1 and it appeared that COCOA decreases performance down by 0.8 points. This indicates that only specific algorithms are expected to benefit on average by COCOA. And for MOPO and MOBILE hyperparameters were heavily tuned for each dataset which might be the cause why they benefited.
* Evaluation is performed only on the Gym MuJoCo datasets which I think is not enough now and evaluation on D4RL AntMaze or Adroit is essential. As an alternative, offline-to-online setup might be tested.

Review Point: * Proposed approach lead to the improvement only in approximately 58% of cases. And performance might be decreased dramatically. I've also calculated the differences between "+ COCOA" and "Alone" in Table 1 and it appeared that COCOA decreases performance down by 0.8 points. This indicates that only specific algorithms are expected to benefit on average by COCOA. And for MOPO and MOBILE hyperparameters were heavily tuned for each dataset which might be the cause why they benefited.
Review Point: * Evaluation is performed only on the Gym MuJoCo datasets which I think is not enough now and evaluation on D4RL AntMaze or Adroit is essential. As an alternative, offline-to-online setup might be tested.
==================================================

Focused review:

## The motivation is good but some method details are quite strange
1. Eq. (7) tries to ensure $(1-\alpha) c + \alpha(c + V_c)=Q_c$. This is not a common Bellman equation for the cost value Q and V function. Instead, this equation is similar to the one for the feasible value function but is still different: $(1-\alpha) h + \alpha \max (h, V_h) =Q_h$. Specifically, for the feasible value function, a maximization term exists, but in Eq. (7), the authors directly replace it as a summation term.
2. The definition of $A_r^\pi$ in Eq. (9) is somehow ad-hoc, solely providing some intuitive explanations.
3. It is strange that directly minimizing the $Q_c$ value in Eq.(11) will not lead to a large bootstrapping error accumulation.
4. If not, this means that the policy under unsafe regions still stay near the behavior policy. So the introduction of TRPO-style optimization in Eq. (12) still tries to ensure a relatively relaxed behavior regularization, which contradicts the motivation of this paper that the behavior regularization for unsafe regions should be dropped.
5. In my view, the potential benefits of BARS are primarily attributed to the definition of $A_r^\pi$. Under its definition, the policy will exhibit a more conservative behavior to avoid unsafe regions and so is safer. For unsafe regions, it is quite hard for me to judge if the policy can obtain a reasonable behavior since no behavior regularization exists anymore and the policy can easily exploit the approximation errors of the Q_c value function. It would be better if the authors could show more rollout trajectories for BARS in Figure 1 when starting at one unsafe region.
6. The authors have identified the safe and unsafe regions through expectile regression, but still need to learn additional Q and Qc value functions through standard bellman update in Eq.(14-15). This can be unstable due to error accumulations and inefficient due to the costly diffusion sampling process.
## Evaluations
7. Table 3 shows that FISOR produces different results for varied cost limits. However, FISOR is one cost limits-agnositc method that studies hard constraint and should not behave differently with different cost limits.

Review Point: 2. The definition of $A_r^\pi$ in Eq. (9) is somehow ad-hoc, solely providing some intuitive explanations.
Review Point: 3. It is strange that directly minimizing the $Q_c$ value in Eq.(11) will not lead to a large bootstrapping error accumulation.
Review Point: 4. If not, this means that the policy under unsafe regions still stay near the behavior policy. So the introduction of TRPO-style optimization in Eq. (12) still tries to ensure a relatively relaxed behavior regularization, which contradicts the motivation of this paper that the behavior regularization for unsafe regions should be dropped.
Review Point: 5. In my view, the potential benefits of BARS are primarily attributed to the definition of $A_r^\pi$. Under its definition, the policy will exhibit a more conservative behavior to avoid unsafe regions and so is safer. For unsafe regions, it is quite hard for me to judge if the policy can obtain a reasonable behavior since no behavior regularization exists anymore and the policy can easily exploit the approximation errors of the Q_c value function. It would be better if the authors could show more rollout trajectories for BARS in Figure 1 when starting at one unsafe region.
Review Point: 6. The authors have identified the safe and unsafe regions through expectile regression, but still need to learn additional Q and Qc value functions through standard bellman update in Eq.(14-15). This can be unstable due to error accumulations and inefficient due to the costly diffusion sampling process. ## Evaluations 7. Table 3 shows that FISOR produces different results for varied cost limits. However, FISOR is one cost limits-agnositc method that studies hard constraint and should not behave differently with different cost limits.
==================================================

Focused review:

- Although the domain of the application (i.e., time-series modeling) is new and the proposed design brings an idea of the latent state evolution (in forecasting), the novelty seems to be limited. The overall architectural design follows the FFN architecture (Tancik, et al, 2020) without any consideration on how to handle multivariate time-series. Also, the idea of the latent modulation and the meta-learning-based training algorithm largely follow the previous approach (Dupont, et al, 2022). Finally, a similar idea of employing temporally-refined latent variables has been explored in (Yin, et al, 2023).
- Regarding auto-decoding process:
- For imputation, it is natural to assume that there is a given set of measurements for a new time-series, and to set up the goal to fill-in unseen data via imputation. For forecasting, however, the assumption of having a separate training period and a look-back window raises some concerns. Having a separate look-back window suggests that the method needs to wait until the new observations are collected to make forecasting. Some of the datasets that are considered in the paper have hourly sampling rate and this time gap might provide enough time to fine-tune other baseline models (with many model parameters, e.g., Transformers). If the ultimate goal is to achieve accurate prediction, with the given time period (an hour), fine-tuning those baselines with a new observation may provide better prediction results.
- Similarly, another concern is fairness on the comparisons. Although it is just 3 gradient steps, auto-decoding is considered as solving an optimization problem to fine-tune the model for the new observations. What happens if the small portion of the other baselines (e.g., the last layer) is fine-tuned during the inference? For example, in forecasting, the model can be fine-tuned after making predictions on the current sliding window and then make predictions on the next sliding window with the updated models.
- Although the method seems to provide accurate predictions both in imputation and forecasting, the method seems to struggle in predicting peaks accurately. In many applications, predicting peaks accurately would have more importance than simply minimizing MSEs (e.g., to properly prepare the electricity supply or properly set up the cost during the peak time period). Based on the eye-ball examination (Figure 5 for example), the model does not seem to provide accurate predictions in peak values.

Review Point: - Although the domain of the application (i.e., time-series modeling) is new and the proposed design brings an idea of the latent state evolution (in forecasting), the novelty seems to be limited. The overall architectural design follows the FFN architecture (Tancik, et al, 2020) without any consideration on how to handle multivariate time-series. Also, the idea of the latent modulation and the meta-learning-based training algorithm largely follow the previous approach (Dupont, et al, 2022). Finally, a similar idea of employing temporally-refined latent variables has been explored in (Yin, et al, 2023).
Review Point: - Similarly, another concern is fairness on the comparisons. Although it is just 3 gradient steps, auto-decoding is considered as solving an optimization problem to fine-tune the model for the new observations. What happens if the small portion of the other baselines (e.g., the last layer) is fine-tuned during the inference? For example, in forecasting, the model can be fine-tuned after making predictions on the current sliding window and then make predictions on the next sliding window with the updated models.
Review Point: - Although the method seems to provide accurate predictions both in imputation and forecasting, the method seems to struggle in predicting peaks accurately. In many applications, predicting peaks accurately would have more importance than simply minimizing MSEs (e.g., to properly prepare the electricity supply or properly set up the cost during the peak time period). Based on the eye-ball examination (Figure 5 for example), the model does not seem to provide accurate predictions in peak values.
==================================================

Focused review:

- What is the difference between the proposed RL fine-tuning (RLFT) and existing methods like PhysDiff? Specifically, what is the relationship between the imitation policy part (Section 3.1) and PhysDiff? I also think it would be beneficial to include a comparison with PhysDiff in the experiments.
- The details of the on-policy RLFT process may be somewhat unclear. For example, it is stated, "we sample 2,048 motions from the training dataset of AIST++." This is a bit ambiguous. Since this is RL fine-tuning, the samplings are generated by the current policy, while as I understand, the diffusion model (EDGE) is already well-trained. Can samplings from the training set provide a balanced input, including negative aspects (penetration/freezing)?
- In the ablation study, I find "EDGE w proj" somewhat confusing; it is described as "similar to PhysDiff" (Line 451). Why do the authors consider PhysDiff as "post-processing"? As I understand it, PhysDiff embeds the simulator into the diffusion models within the final few iterations, which doesn’t seem all equivalent to "post-processing." Could the authors clarify the implementation of "EDGE w proj"? Also, in the demo, the "post-processing" approach results in floating in the air. This seems odd and confusing, as a physical simulator is expected to prevent this.
- Some related works are missed, e.g., Bailando++ (Siyao et al., 2023).

Review Point: - What is the difference between the proposed RL fine-tuning (RLFT) and existing methods like PhysDiff? Specifically, what is the relationship between the imitation policy part (Section 3.1) and PhysDiff? I also think it would be beneficial to include a comparison with PhysDiff in the experiments.
Review Point: - The details of the on-policy RLFT process may be somewhat unclear. For example, it is stated, "we sample 2,048 motions from the training dataset of AIST++." This is a bit ambiguous. Since this is RL fine-tuning, the samplings are generated by the current policy, while as I understand, the diffusion model (EDGE) is already well-trained. Can samplings from the training set provide a balanced input, including negative aspects (penetration/freezing)?
Review Point: - In the ablation study, I find "EDGE w proj" somewhat confusing; it is described as "similar to PhysDiff" (Line 451). Why do the authors consider PhysDiff as "post-processing"? As I understand it, PhysDiff embeds the simulator into the diffusion models within the final few iterations, which doesn’t seem all equivalent to "post-processing." Could the authors clarify the implementation of "EDGE w proj"? Also, in the demo, the "post-processing" approach results in floating in the air. This seems odd and confusing, as a physical simulator is expected to prevent this.
Review Point: - Some related works are missed, e.g., Bailando++ (Siyao et al., 2023).
==================================================

Focused review:

- the proposed method seems to be unstable. The oscillation during the training phase seems much larger than baselines (FedAvg, SCAFFOLD) and its performance to highly dependent on the selection of $\alpha$ which is a hyper-parameter.
- The suggested method needs all participants to be involved in the training throughout the learning process, which is not feasible in Federated Learning due to its distributed nature.
- More extensive experiment is required
- A thorough comparison with state-of-the-art (SOTA) methods is essential to validate the proposed method. Given its applicability to the standard Federated Learning (FL), it is crucial to benchmark its performance against SOTA methods such as FedDC, FedMLB, FedDyn, and MOON. If the method can be used with these established techniques, it's crucial to demonstrate that combining the proposed method with them leads to significant benefits.
- To validate its effectiveness on heterogeneous data, the method needs evaluation across a broader range of scenarios with different levels of data diversity.
- Paper claims that the proposed method reduces communication costs in Federated Learning when training a global model, but it lacks numerical data or theoretical evidence to back up this claim.

Review Point: - the proposed method seems to be unstable. The oscillation during the training phase seems much larger than baselines (FedAvg, SCAFFOLD) and its performance to highly dependent on the selection of $\alpha$ which is a hyper-parameter.
Review Point: - The suggested method needs all participants to be involved in the training throughout the learning process, which is not feasible in Federated Learning due to its distributed nature.
Review Point: - More extensive experiment is required - A thorough comparison with state-of-the-art (SOTA) methods is essential to validate the proposed method. Given its applicability to the standard Federated Learning (FL), it is crucial to benchmark its performance against SOTA methods such as FedDC, FedMLB, FedDyn, and MOON. If the method can be used with these established techniques, it's crucial to demonstrate that combining the proposed method with them leads to significant benefits.
Review Point: - To validate its effectiveness on heterogeneous data, the method needs evaluation across a broader range of scenarios with different levels of data diversity.
Review Point: - Paper claims that the proposed method reduces communication costs in Federated Learning when training a global model, but it lacks numerical data or theoretical evidence to back up this claim.
==================================================

Focused review:

1. The proposed method is merely a combination of existing methods, lacking novelty.
2. The model lacks evaluation of OOD (out-of-distribution) datasets or real-world datasets, making its generalization capability questionable.
3. The experimental settings are unclear in the comparative experiments. LGM is a model that converts 4 views to 3D Gaussians, yet it appears in Table 2 in single-image-to-3D comparison experiments. The authors need to further clarify the evaluation details of other models.

Review Point: 1. The proposed method is merely a combination of existing methods, lacking novelty.
Review Point: 2. The model lacks evaluation of OOD (out-of-distribution) datasets or real-world datasets, making its generalization capability questionable.
Review Point: 3. The experimental settings are unclear in the comparative experiments. LGM is a model that converts 4 views to 3D Gaussians, yet it appears in Table 2 in single-image-to-3D comparison experiments. The authors need to further clarify the evaluation details of other models.
==================================================

Focused review:

There are two major weaknesses of the paper for me-- first, the paper analyzes the improvements in time and space in the RAM/CPU model but deep learning (DL) systems and specifically Transformer models today are run on GPUs (as is the case with the experiments in the paper) and it is not clear that the theoretical improvements in the paper would translate to the GPU model. The second (and related) weakness is that the paper does not do comparison with other architectures that aim to reduce the KV cache but do have implementations that are "GPU aware."
Theoretical Analysis is on the RAM model ----------------------------------------
The theoretical analysis presented in the paper is for the RAM model and presents a random sampling based algorithm to show improvement in the RAM/streaming model. However, modern day system pretty much all run on GPUs and TPUs where random sampling does not tend to work well. This is primarily because in modern hardware all operations happen at the level of blocks-- in other words, reading one entry from a block is the same as reading the entire block. So e.g. if the random sampling does not take this block structure into account (which the algorithms in the paper from what I can tell do _not_ do), then it is not clear if the theoretical gains will manifest in actual systems since it might still be reading all the blocks even if the actual number of entries read by random sampling is much lower. Specifically, it is not clear to me that the proposed implementation would be competitive against an implementation of Attention like FlashAttention (https://github.com/Dao-AILab/flash-attention), which is an _exact_ Attention implementation that is hardware aware. Further, modern GPUs/TPUs allow for block-block matrix-multiplication in $O(1)$ time and an algorithm that is created specifically to exploit such natively supported operations might have a higher number of FLOPs than the proposed algorithm but would still beat the proposed algorithm in wall clock speed.
In summary, the modern hardware systems have native support for specialized operations, which the proposed algorithm does not seem to exploit. This does not make me confident that the proposed algorithm would work better than these hardware aware implementations.
Insufficient comparison with prior related work -----------------------------------------------
The paper's weakness with adequate comparison with prior work can in turn be divided into three parts:
1. Of course it is entirely possible that the proposed algorithms actually run faster hardware aware algorithms (like FlashAttention) but no such comparison is provided in the experimental section of the paper. It is possible that the "Exact" Attention implementation uses a FlashAttention implementation but it is not clear from the paper whether this is the case. It would be good to either get confirmation that the "Exact" Attention uses a FlashAttention implementation (if so, which version) or if not, then it would be good to see comparison performance numbers with FlashAttention.
2. Moving beyond hardware aware exact FlashAttention implementations, there are approximate Attention implementations that have good implementations on modern hardware. Two recent examples-- The Hedgehog & the Porcupine (https://arxiv.org/abs/2402.04347) and Based (https://arxiv.org/abs/2402.18668) utilize the fact that one can approximate the exp function with low degree polynomials and then noting that apply a low degree polynomial to each entry of a low rank matrix gives rise to low rank matrices allows one to use these Taylor series approximations to define a "kernel" and then to use linear Attention instead of softmax Attention to reduce the KV cache size. These recent papers have implementations suited to modern hardware and have accuracy that matches those of Transformers (but are much more efficient).
* I would like to point out that the above idea of using Taylor series + linear attention/low rank matrices have been exploited before. E.g. see the paper of Alman and Song (https://arxiv.org/abs/2302.13214) and the earlier work of Chen et al. (https://arxiv.org/abs/2110.15343). The latter paper also proposed to compress the KV cache under the assumption of keys being clustered (though the clustering model in this paper is cleaner and more general). But in general, for the theoretical results, it would be good to compare the theoretical results in this paper and the results e.g. in the Alman and Song paper (for the case when they get $n^{1+o(1)}$ implementation of Attention.
* In addition to the above body of work on approximating Attention there have been alternative Attention free models that have garnered a lot of Attention. Mamba (https://arxiv.org/abs/2312.00752, also see Mamba 2: https://arxiv.org/abs/2405.21060) is probably the model that has garnered most attention. These models have comparable accuracy to Transformer but are much faster and use much smaller cache sizes.
* Overall the paper should present a comparison with the above body of work (for approximate Attention models this should be done in both Tables 2 and 3, while for Attention free models comparison should be made in Table 3).
3. Finally, one big downside of the results presented in this paper is that the _accuracy_ over the exact Attention implementation takes a big hit. The proposed new algorithms do get an improvement of up to $50$ percent in cache size but also take a similar hit in accuracy. By comparison, the related work mentioned in the above bullet get a much larger improvement in cache size while _essentially not losing in accuracy/perplexity at all_ when compared to exact Transformers. However, these works are not evaluated in Tables 2 and 3 in the paper.

Review Point: 1. Of course it is entirely possible that the proposed algorithms actually run faster hardware aware algorithms (like FlashAttention) but no such comparison is provided in the experimental section of the paper. It is possible that the "Exact" Attention implementation uses a FlashAttention implementation but it is not clear from the paper whether this is the case. It would be good to either get confirmation that the "Exact" Attention uses a FlashAttention implementation (if so, which version) or if not, then it would be good to see comparison performance numbers with FlashAttention.
==================================================

Focused review:

1. I suggest the authors use different notations for single-dimensional and multi-dimensional cases, e.g., x and **x**.
2. The authors claim that each sampling step of previous works predicts single-dimension transitions but not joint transitions of all dimensions. However, based on Proposition 3.1, the proposed DDPM also just predicts single-dimension transitions.
3. As presented in [1], the best FID score on ImageNet 256×256 is 1.97. Why are all the results in Table 2, both for the baseline and the proposed DDPO, significantly worse than 1.97? Is it because classifier-free guidance was not used? If so, why are there no results that include classifier-free guidance? This is my main concern; I will improve my score if a clear response is presented.
[1] Yu et al. An image is worth 32 tokens for reconstruction and generation. NeurIPS 2024.

Review Point: 1. I suggest the authors use different notations for single-dimensional and multi-dimensional cases, e.g., x and **x**.
Review Point: 2. The authors claim that each sampling step of previous works predicts single-dimension transitions but not joint transitions of all dimensions. However, based on Proposition 3.1, the proposed DDPM also just predicts single-dimension transitions.
Review Point: 3. As presented in [1], the best FID score on ImageNet 256×256 is 1.97. Why are all the results in Table 2, both for the baseline and the proposed DDPO, significantly worse than 1.97? Is it because classifier-free guidance was not used? If so, why are there no results that include classifier-free guidance? This is my main concern; I will improve my score if a clear response is presented. [1] Yu et al. An image is worth 32 tokens for reconstruction and generation. NeurIPS 2024.
==================================================

Focused review:

1. Algorithmic Complexity: The incorporation of sophisticated mechanisms like contrastive learning and feedback-based sampling may introduce complexity that complicates the model's implementation and optimization, potentially requiring specialized knowledge or resources to manage effectively.
2. Sensitivity to Feedback Quality: The performance of HERO heavily depends on the relevance and accuracy of the feedback provided. Inconsistent or poor-quality feedback could mislead the learning process, leading to suboptimal or biased model behavior.

Review Point: 1. Algorithmic Complexity: The incorporation of sophisticated mechanisms like contrastive learning and feedback-based sampling may introduce complexity that complicates the model's implementation and optimization, potentially requiring specialized knowledge or resources to manage effectively.
Review Point: 2. Sensitivity to Feedback Quality: The performance of HERO heavily depends on the relevance and accuracy of the feedback provided. Inconsistent or poor-quality feedback could mislead the learning process, leading to suboptimal or biased model behavior.
==================================================

Focused review:

- The novelty of the proposed method is somewhat limited. The main contribution is the neural expression generation via multiple-referring expressions. It seems that this aggregation strategy is simple and lacks insights.
- The authors declare that they proposed different sampling strategies in cross-modal attention for pre-training and fine-tuning to boost the model performance. However, the illustration of this sampling strategy is unclear, and the differences with existing sampling strategies are also unclear. Also, there are no experimental results to support this assertion.
- In Eq.3, the authors used the concat operation but in Table 2(c), the proposed NEG is different MRE Cocat, so the reason is unclear.
- The authors do not show the training convergence in the pre-training strategy, So, it is hard to assert the proposed method achieves faster convergence only by verifying it in the fine-tuning stage.
- I think the comparison is somewhat unfair. The batch size is different. It mainly influences the training convergence and even the performance.

Review Point: - The novelty of the proposed method is somewhat limited. The main contribution is the neural expression generation via multiple-referring expressions. It seems that this aggregation strategy is simple and lacks insights.
Review Point: - The authors declare that they proposed different sampling strategies in cross-modal attention for pre-training and fine-tuning to boost the model performance. However, the illustration of this sampling strategy is unclear, and the differences with existing sampling strategies are also unclear. Also, there are no experimental results to support this assertion.
Review Point: - In Eq.3, the authors used the concat operation but in Table 2(c), the proposed NEG is different MRE Cocat, so the reason is unclear.
Review Point: - The authors do not show the training convergence in the pre-training strategy, So, it is hard to assert the proposed method achieves faster convergence only by verifying it in the fine-tuning stage.
Review Point: - I think the comparison is somewhat unfair. The batch size is different. It mainly influences the training convergence and even the performance.
==================================================

Focused review:

- What is the motivation for choosing the TV-sup norm to compute the difference between the true distribution and the approximated one by LLMs? Is there any theoretical justification behind this selection? If not, at least an ablation study showing the effectiveness of this norm is necessary.
- It is still ambiguous why the authors define the distance between the two mappings by the way around Line 248.
- The result in the Equation (1) is non-trivial. Detailed transformation of it is necessary. At first glance, I questioned the equivalence of the two sides.

Review Point: - What is the motivation for choosing the TV-sup norm to compute the difference between the true distribution and the approximated one by LLMs? Is there any theoretical justification behind this selection? If not, at least an ablation study showing the effectiveness of this norm is necessary.
Review Point: - It is still ambiguous why the authors define the distance between the two mappings by the way around Line 248.
Review Point: - The result in the Equation (1) is non-trivial. Detailed transformation of it is necessary. At first glance, I questioned the equivalence of the two sides.
==================================================

Focused review:

1) The generalizabity of the proposal on other model architectues and datasets is not evaluated explored. Addressing this issue running some extra experiments would strengthen the paper.
2) Table 1 presents the comparison with other compression techniques, however there is no comparison with the works reported in the related work section (e.g., those using quantization, pruning, and knowledge distillation). This could provide a more comprehensive performance context and help the reader understand better the benefits of the proposal.
3) It would be beneficial to mention what is the novelty of the proposal. The way it is written currently, it is seems that the proposal is incremental in terms of novelty as it is a combination of various existing techniques.

Review Point: 1) The generalizabity of the proposal on other model architectues and datasets is not evaluated explored. Addressing this issue running some extra experiments would strengthen the paper.
Review Point: 2) Table 1 presents the comparison with other compression techniques, however there is no comparison with the works reported in the related work section (e.g., those using quantization, pruning, and knowledge distillation). This could provide a more comprehensive performance context and help the reader understand better the benefits of the proposal.
Review Point: 3) It would be beneficial to mention what is the novelty of the proposal. The way it is written currently, it is seems that the proposal is incremental in terms of novelty as it is a combination of various existing techniques.
==================================================

Focused review:

1. The number of samples for Monte Carlo integration is not clear to me. When the authors say 10 samples are used for the affine group, do they mean 10 samples for both rotations and scales combined? How many rotations and scales?
2. Although clear theoretically, experiments don't seem to have tested the settings that were setting this apart from the others, particularly, by MacDonald et al. 2022. Perhaps at least one experiment with n=3 with larger scale changes as part of the test set could be interesting. Even a different version of affNIST with larger scale changes could be interesting.
3. Having numerical values of equivariance/invariance error, and as a function of the number of Monte Carlo samples should also be useful here.
3. I don't fully understand why the proposed group decomposition technique is able to estimate the correlation integral well with only 10 samples while the work by MacDonald et al. need 100.

Review Point: 1. The number of samples for Monte Carlo integration is not clear to me. When the authors say 10 samples are used for the affine group, do they mean 10 samples for both rotations and scales combined? How many rotations and scales?
Review Point: 2. Although clear theoretically, experiments don't seem to have tested the settings that were setting this apart from the others, particularly, by MacDonald et al. 2022. Perhaps at least one experiment with n=3 with larger scale changes as part of the test set could be interesting. Even a different version of affNIST with larger scale changes could be interesting.
Review Point: 3. Having numerical values of equivariance/invariance error, and as a function of the number of Monte Carlo samples should also be useful here.
Review Point: 3. I don't fully understand why the proposed group decomposition technique is able to estimate the correlation integral well with only 10 samples while the work by MacDonald et al. need 100.
==================================================

Focused review:

* Some previous work (https://arxiv.org/pdf/2407.00900) has been done around using open source LLMs as part of the grading framework, although fine tuning a specific model for the answer comparison task is still a notable contribution.
* While the paper presents a significant effort in benchmarking (and the tools presented to the broader research community via this paper will be useful to researchers), the discussion from the numerical results support a lot of things that are already well known (the supremacy of closed-source over open-source models, the performance of math domain models generally being better, few-shot prompting generally resulting in better performance when compared to zero-shot, etc.)

Review Point: * Some previous work (https://arxiv.org/pdf/2407.00900) has been done around using open source LLMs as part of the grading framework, although fine tuning a specific model for the answer comparison task is still a notable contribution.
Review Point: * While the paper presents a significant effort in benchmarking (and the tools presented to the broader research community via this paper will be useful to researchers), the discussion from the numerical results support a lot of things that are already well known (the supremacy of closed-source over open-source models, the performance of math domain models generally being better, few-shot prompting generally resulting in better performance when compared to zero-shot, etc.)
==================================================

Focused review:

The primary issue with the paper is that there are known solutions to this problem that use far simpler approaches than those proposed. The authors are clearly unaware that this is a solved problem. As a consequence, I cannot recommend acceptance without clear comparison with existing solutions, both theoretically and empirically. Theoretically the authors need to justify why such a more computationally complex solution is justified given the existing solutions. Empirically, it is important to know if performance improvements can be obtained wrt state-of-the-art methods.
The paper defines a non-stationary environment " as a nonstationary mixture of various stationary environments".
-- this is a strong assumption
-- in control theory there are well-known solutions to systems under such an assumption. For example, multiple model adaptive control is a known solution.
Murray-Smith, R., & Johansen, T. (Eds.). (2020). Multiple model approaches to nonlinear modelling and control. CRC press.
Basically, given a collection of pre-defined stationary environments, if we generate a controller for each of these environments, then we have a guaranteed controller for any mixture of these stationary environments.
Some references about this:
Zhang, W., & Li, Q. (2020). Stable Weighted Multiple Model Adaptive Control of Continuous-Time Plant. In Virtual Equivalent System Approach for Stability Analysis of Model-based Control Systems (pp. 111-127). Singapore: Springer Singapore.
Provan, G., Quinones-Grueiro, M., & Sohége, Y. (2022). Towards Real-Time Robust Adaptive Control for Non-Stationary Environments. IFAC-PapersOnLine, 55(6), 73-78.
Deng, X., Zhang, Y., & Qi, H. (2022). Towards optimal HVAC control in non-stationary building environments combining active change detection and deep reinforcement learning. Building and environment, 211, 108680.
In all of this prior work, no notion of causality is necessary. Hence it is unclear why the causality-based solution proposed is indeed necessary.
Other Issues
Eq. 3.1 is hard to understand
Why not use standard dynamical systems state-space representation? Or probabilistic representation?
if you invent notation, it should be better than what exists. This is not.
Experiments: "we compare COREP with the following baselines: FNVAE (Feng et al., 2022), VariBAD (Zintgraf et al., 2019), and PPO (Schulman et al., 2017)."
---unless you compare COREP with a mixture-based MMAC solution then I cannot see how to understand how well it does. These are only RL-internal methods.
5 RELATED WORK
The authors completely miss known solutions as referenced earlier.

Review Point: 5 RELATED WORK The authors completely miss known solutions as referenced earlier.
==================================================

Focused review:

1. the motivation. This paper is not well-motivated. Why do we need 2D states to capture periodicity in time series forecasting? Do the authors show the necessity of the 2D modeling for periods?
2. the novelty is limited. The biggest contribution of this paper, the formulation of 1D to 2D transformation seems to be the same as TimesNet [1], which makes the novelty largely limited.
3. the contribution is a little weak. Compared with existing works (such as TimesNet), it only revises several blocks for time series forecasting. The methods don't contribute well to the community.
4. missing of related work. More periodic modeling works should be discussed [2,3]. Also, differences compared with TimesNet should be discussed.
[1] Timesnet: Temporal 2d-variation modeling for general time series analysis. In ICLR.
[2] DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting. In ICLR.
[3] Bridging self-attention and time series decomposition for periodic forecasting. In CIKM.

Review Point: 1. the motivation. This paper is not well-motivated. Why do we need 2D states to capture periodicity in time series forecasting? Do the authors show the necessity of the 2D modeling for periods?
Review Point: 2. the novelty is limited. The biggest contribution of this paper, the formulation of 1D to 2D transformation seems to be the same as TimesNet [1], which makes the novelty largely limited.
Review Point: 3. the contribution is a little weak. Compared with existing works (such as TimesNet), it only revises several blocks for time series forecasting. The methods don't contribute well to the community.
Review Point: 4. missing of related work. More periodic modeling works should be discussed [2,3]. Also, differences compared with TimesNet should be discussed. [1] Timesnet: Temporal 2d-variation modeling for general time series analysis. In ICLR. [2] DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting. In ICLR. [3] Bridging self-attention and time series decomposition for periodic forecasting. In CIKM.
==================================================

Focused review:

- Though adapting new techniques such as slot attention, I found this method to have limited novelty as it addresses common concerns such as patch redundancy in MIL. Many works such as DeepAttnMISL (Yao et al. 2020) achieve similar goals as Slot-MIL in filtering the bag to a smaller set of patches. Overall, relative to the performance improvement demonstrated, the contributions presented by the method may still be too limited and lack extensive validation with diverse downstream tasks.
- One of the outlined contributions of this work (#3) is that Slot-MIL reaches state-of-the-art performance on CAMELYON and TCGA-NSCLC. Slot-MIL outperforms baselines relative to the comparisons developed in this work. However, when compared across studies, the reported best performance for C16 on the test set underperforms other reported results by a large margin. For example, on C16, whereas the accuracy / AUC for Slot-MIL+SubMix is 0.890 / 0.921, the reported best performances for ILRA-MIL (FRC) in Xiang et al. 2023 is 0.922 / 0.965. In other works such as MHIM-MIL (DSMIL) by Tang et al. 2023, the reported best performances is 0.925 / 0.965 (evaluated using cross-validation, not on official C16 test), and Bayes-MIL-APCRF by Cui et al. 2023 have best performances of 0.900 / 0.948. Though not using the same splits for TCGA-NSCLC, the AUCs for this task using 10-fold CV is generally 0.930+ (0.977 in Xiang et al. 2023).
- Benchmarks such as C16 and TCGA-NSCLC lack difficulty and can be easily solved without adapting techniques related to WSI augmentation and slot attention. It would be interesting to explore this method on more diverse tasks that would benefit from data augmentation and "sparsity", such as gene mutation prediction (such as MSI prediction in TCGA-COADREAD), survival analysis, and other challenging tasks such as Gleason score grading in PANDA. The tasks evaluated in this work are limited to diagnostically-simple binary classification problems that do not need sparse MIL or virtual augmentation methods to see clinical translation.
- - Additionally, tasks such as C16, TCGA-NSCLC, TCGA-RCC have been over-explored in computational pathology and should no longer be evaluated as the only tasks evaluated for MIL in the reviewer's opinion. C16 already has many state-of-the art performances and is nearly solved from both fully-supervised and weakly-supervised perspectives. Similarly, TCGA-NSCLC and TCGA-RCC can be generally solved without requiring sophisticated MIL approaches. Overall, it would be more interesting to demonstrate how this method would enable more challenging tasks to be solved in computational pathology.
1. Xiang, J. and Zhang, J., 2022, September. Exploring low-rank property in multiple instance learning for whole slide image classification. In The Eleventh International Conference on Learning Representations.
2. Yufei, C., Liu, Z., Liu, X., Liu, X., Wang, C., Kuo, T.W., Xue, C.J. and Chan, A.B., 2022, September. Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images. In The Eleventh International Conference on Learning Representations.
3. Tang, W., Huang, S., Zhang, X., Zhou, F., Zhang, Y. and Liu, B., 2023. Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4078-4087).
4. Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N. and Huang, J., 2020. Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. Medical Image Analysis, 65, p.101789.

Review Point: - Though adapting new techniques such as slot attention, I found this method to have limited novelty as it addresses common concerns such as patch redundancy in MIL. Many works such as DeepAttnMISL (Yao et al. 2020) achieve similar goals as Slot-MIL in filtering the bag to a smaller set of patches. Overall, relative to the performance improvement demonstrated, the contributions presented by the method may still be too limited and lack extensive validation with diverse downstream tasks.
Review Point: - Benchmarks such as C16 and TCGA-NSCLC lack difficulty and can be easily solved without adapting techniques related to WSI augmentation and slot attention. It would be interesting to explore this method on more diverse tasks that would benefit from data augmentation and "sparsity", such as gene mutation prediction (such as MSI prediction in TCGA-COADREAD), survival analysis, and other challenging tasks such as Gleason score grading in PANDA. The tasks evaluated in this work are limited to diagnostically-simple binary classification problems that do not need sparse MIL or virtual augmentation methods to see clinical translation.
Review Point: - - Additionally, tasks such as C16, TCGA-NSCLC, TCGA-RCC have been over-explored in computational pathology and should no longer be evaluated as the only tasks evaluated for MIL in the reviewer's opinion. C16 already has many state-of-the art performances and is nearly solved from both fully-supervised and weakly-supervised perspectives. Similarly, TCGA-NSCLC and TCGA-RCC can be generally solved without requiring sophisticated MIL approaches. Overall, it would be more interesting to demonstrate how this method would enable more challenging tasks to be solved in computational pathology.
Review Point: 1. Xiang, J. and Zhang, J., 2022, September. Exploring low-rank property in multiple instance learning for whole slide image classification. In The Eleventh International Conference on Learning Representations.
Review Point: 2. Yufei, C., Liu, Z., Liu, X., Liu, X., Wang, C., Kuo, T.W., Xue, C.J. and Chan, A.B., 2022, September. Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images. In The Eleventh International Conference on Learning Representations.
Review Point: 3. Tang, W., Huang, S., Zhang, X., Zhou, F., Zhang, Y. and Liu, B., 2023. Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4078-4087).
Review Point: 4. Yao, J., Zhu, X., Jonnagaddala, J., Hawkins, N. and Huang, J., 2020. Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. Medical Image Analysis, 65, p.101789.
==================================================

Focused review:

* The numerical evaluation is limited in scope. Experiments are conducted on a very small number of datasets, including only one with a ground-truth cluster and a single image from COIL. This limited evaluation does not suffice to draw meaningful conclusions about the algorithm’s general performance. To better assess its effectiveness, experiments on both synthetic and more extensive real-world datasets are required. For example, testing on standard graph clustering benchmarks like the SNAP datasets or generating synthetic graphs with known cluster structures of varying sizes and densities.
* The datasets used in the current experiments are small. Testing on larger graphs is necessary to conclude the algorithm's empirical runtime, especially as the paper claims scalability as a key benefit. For example, testing on graphs with 10^4, 10^5, 10^6, and more nodes would show how runtime scales with graph size.
* The usefulness of the proposed “FastSimple” algorithm becomes more relevant for cases where $k$ (the number of clusters) is large, given that it only uses $\log⁡ k$ vectors rather than computing $k$ eigenvectors. Comparisons with small $k$ values may, therefore, provide misleading insights into the algorithm’s efficiency relative to traditional methods.
For example, on 'Multiple Features' data set, it seems the correct number of clusters is $k=2$. Hence, using 'FastSimple' instead of 'Classical' is totally useless. I believe the difference in running time is simply due to a better implementation by the authors of FastSimple (MacGregor et al.) rather than an intrinsic advantage of FastSimple.
* The section on related works focuses heavily on spectral methods, but other methods have been proposed to solve the min-bisection problem (tracing back to Lin–Kernighan approaches), and those methods need to be discussed as they are related to the authors' proposal. Moreover, the paper [1] already shows that it is possible to partition a well-clustered graph in nearly linear time. Finally, ref [2] shows that spectral analysis can be carried out over extremely large graphs.
Missing references:
[1] Richard Peng, He Sun, and Luca Zanetti. "Partitioning well-clustered graphs: Spectral clustering works!." Conference on learning theory. PMLR, 2015.
[2] Kang, U., Meeder, B., & Faloutsos, C. (2011). Spectral analysis for billion-scale graphs: Discoveries and implementation. In Advances in Knowledge Discovery and Data Mining: 15th Pacific-Asia Conference, PAKDD 2011, Shenzhen, China, May 24-27, 2011, Proceedings, Part II, 13-25. Springer Berlin Heidelberg.

Review Point: * The numerical evaluation is limited in scope. Experiments are conducted on a very small number of datasets, including only one with a ground-truth cluster and a single image from COIL. This limited evaluation does not suffice to draw meaningful conclusions about the algorithm’s general performance. To better assess its effectiveness, experiments on both synthetic and more extensive real-world datasets are required. For example, testing on standard graph clustering benchmarks like the SNAP datasets or generating synthetic graphs with known cluster structures of varying sizes and densities.
Review Point: * The datasets used in the current experiments are small. Testing on larger graphs is necessary to conclude the algorithm's empirical runtime, especially as the paper claims scalability as a key benefit. For example, testing on graphs with 10^4, 10^5, 10^6, and more nodes would show how runtime scales with graph size.
==================================================

Focused review:

1. The paper dedicates significant space to comparisons with traditional methods that compress tokens before using LLMs. However, it overlooks an essential baseline, FastV [1], which also compresses image tokens within the LLM itself and allows for direct comparison of training results. This omission makes the paper less convincing.
2. The paper claims "reducing training/inference time," but does not provide any data demonstrating training time reduction.
3. The proposed strategy appears usable without training; therefore, it would be beneficial to include results showing inference acceleration without additional training.
4. The existence of Table 3 is quite awkward: first, there are numerous gaps in the table, and second, the training data is entirely different, making these models incomparable.
[1]An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models, https://arxiv.org/abs/2403.06764

Review Point: 1. The paper dedicates significant space to comparisons with traditional methods that compress tokens before using LLMs. However, it overlooks an essential baseline, FastV [1], which also compresses image tokens within the LLM itself and allows for direct comparison of training results. This omission makes the paper less convincing.
Review Point: 2. The paper claims "reducing training/inference time," but does not provide any data demonstrating training time reduction.
Review Point: 3. The proposed strategy appears usable without training; therefore, it would be beneficial to include results showing inference acceleration without additional training.
Review Point: 4. The existence of Table 3 is quite awkward: first, there are numerous gaps in the table, and second, the training data is entirely different, making these models incomparable. [1]An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models, https://arxiv.org/abs/2403.06764
==================================================

Focused review:

Overall, the paper is hard for me to follow. In summary the issues are:
- Many definitions given up front (up to beginning of page 4) are fairly non-standard, given the general body of work that shows up at ICLR. At the same time, the presentation features little discussion of definitions after they are given, with most details relegated to appendix.
- Theorem statements in main text contain uncommon terms "probability gap" without definition.
- Definitions that are given contain other undefined terms within definition, e.g.:
- An algorithmic data generating source µ is simply a computable data source by" A "data source" was not defined.
- SI: Inductive inference aims to find a universally valid approximation to µ. What's "universally valid"?
- Propositions (e.g. prop 4, prop 8) are given and followed immediately by a next section with no concluding sentence on what the takeaway should be or what the theorem means in words.
- Many data details (e.g."Variable-order Markov Source", one of the 3 experiments) are not defined in main text and details are relegated to appendix, and, as mentioned, are generally not particularly well known within the ICLR community.
- Many baselines / models not defined in main text: Stack-RNNs, Tape-RNNs, Context Tree Weighting, where the last one is used as the main baseline.
- Important experimental details that are glossed over, e.g. there is a test distribution described as "out-of-distribution" in passing in the analysis of results without a formal experimental setup given for precisely what the shift between in- and out-distribution is.
I will be glad to raise my score if a major rewrite of this paper is undertaken. In particular it must be readable to wider audience without having to refer to the appendix for interpretation of main contributions or for understanding basic setup like datasets and baselines. As mentioned in "Questions" below, it is also necessary to clarify whether the experimental results are something distinct from running basic meta-learning on existing datasets. If not, is the significance in the connection to the theoretical results? If so, what is that connection?

Review Point: - Many definitions given up front (up to beginning of page 4) are fairly non-standard, given the general body of work that shows up at ICLR. At the same time, the presentation features little discussion of definitions after they are given, with most details relegated to appendix.
Review Point: - Theorem statements in main text contain uncommon terms "probability gap" without definition.
Review Point: - Definitions that are given contain other undefined terms within definition, e.g.:
Review Point: - An algorithmic data generating source µ is simply a computable data source by" A "data source" was not defined.
Review Point: - SI: Inductive inference aims to find a universally valid approximation to µ. What's "universally valid"?
Review Point: - Propositions (e.g. prop 4, prop 8) are given and followed immediately by a next section with no concluding sentence on what the takeaway should be or what the theorem means in words.
Review Point: - Many data details (e.g."Variable-order Markov Source", one of the 3 experiments) are not defined in main text and details are relegated to appendix, and, as mentioned, are generally not particularly well known within the ICLR community.
Review Point: - Many baselines / models not defined in main text: Stack-RNNs, Tape-RNNs, Context Tree Weighting, where the last one is used as the main baseline.
==================================================

Focused review:

The theoretical results seem have some limitations: - the assumption alpha>d is quite restrictive. In the high-dimensional setting where m/d= alpha, it is typically assumed that both m and d go to infinity while keeping alpha fixed as a constant. This work does not follow this regime and falls back to the low-dimensional regime m~alpha d>d^2, which limits the appeal of the theoretical result. - again under the assumption alpha>d, the rate of [25] becomes (1/alpha q+ 1/d^2) which means the bias only becomes significant when q is greater than d. Given d is typically large, it is unclear why this becomes a concern in this regime. - the result (theorem 10) in the general case seems to be much worse than the least-squares case, at least by a factor of sqrt{kappa}. There is no discussion about why the results in the general case degenerates compared to the least-squares case, and how it compares with prior result. The authors should also evaluate the performance as a function of the condition number.

Review Point: - the assumption alpha>d is quite restrictive. In the high-dimensional setting where m/d= alpha, it is typically assumed that both m and d go to infinity while keeping alpha fixed as a constant. This work does not follow this regime and falls back to the low-dimensional regime m~alpha d>d^2, which limits the appeal of the theoretical result.
Review Point: - again under the assumption alpha>d, the rate of [25] becomes (1/alpha q+ 1/d^2) which means the bias only becomes significant when q is greater than d. Given d is typically large, it is unclear why this becomes a concern in this regime.
Review Point: - the result (theorem 10) in the general case seems to be much worse than the least-squares case, at least by a factor of sqrt{kappa}. There is no discussion about why the results in the general case degenerates compared to the least-squares case, and how it compares with prior result. The authors should also evaluate the performance as a function of the condition number.
==================================================

Focused review:

1. The technical novelty is limited given the previous work (Stradi et. al., 2024). (Stradi et. all 2024) worked on the same problem with full-information feedback. As far as I can see, there is no substantial difficulty to combine their algorithm with classical estimators for online learning with bandit-feedback.
2. The key problem in (Stradi et. al., 2024) is not resolved (whether it is possible to remove the slater condition number in the violation and regret bound. In other words, whether a $\tilde{O}(\sqrt{T})$ regret & violation bound is achievable without further assumptions for the best-of-both-worlds setting). It is known (Jin et. al., 2020) that for reward-adv MDP without constraints, it is possible to reach an $\tilde(O)(\sqrt{T})$ regret, while for stochastic reward and constraints, there exists algorithm (Agrawal et. al., 2022) to reach $\tilde{O}(\sqrt{T})$ regret\& violation bound. So the question is that: is it possible to get an $\tilde{O}(\sqrt{T})$ regret (violation) bound for CMDP with adv reward (constraints)?
3. As claimed by the authors, one possible benefit by policy optimization is to avoid solving some convex optimization problem. But there is empirical results to demonstrate that the policy-based algorithm is more efficient than the occupancy-based algorithm. References:
[Jin et. al., 2020] Learning Adversarial MDPs with Bandit Feedback and Unknown Transition
[Agrawal et. al., 2022] Regret Guarantees for Model-Based Reinforcement Learning with Long-Term Average Constraints

Review Point: 1. The technical novelty is limited given the previous work (Stradi et. al., 2024). (Stradi et. all 2024) worked on the same problem with full-information feedback. As far as I can see, there is no substantial difficulty to combine their algorithm with classical estimators for online learning with bandit-feedback.
Review Point: 3. As claimed by the authors, one possible benefit by policy optimization is to avoid solving some convex optimization problem. But there is empirical results to demonstrate that the policy-based algorithm is more efficient than the occupancy-based algorithm. References: [Jin et. al., 2020] Learning Adversarial MDPs with Bandit Feedback and Unknown Transition [Agrawal et. al., 2022] Regret Guarantees for Model-Based Reinforcement Learning with Long-Term Average Constraints
==================================================

Focused review:

1.	The performance of MetaDiff is partially dependent on ConvNeXt+LSTM pre-training. For tasks beyond surgical videos, this dependence could limit the model’s adaptability, especially if ConvNeXt features do not generalize well.
2.	Although MetaDiff is well-evaluated on four datasets, these datasets cover similar surgical contexts. The model’s robustness to highly variable surgical environments (e.g., laparoscopic vs. robotic vs. open surgery) remains untested.
3.	High dependency on accurate timestamp annotations. Surgical video data often contains inaccurate timestamp annotations, especially when manually labeled. And underexplored sensitivity to non-surgical contextual information, some frames may contain non-surgical context (e.g., camera adjustments or the organ scene is shown, but no procedure is performed).
4.	Some new but not necessarily SOTA works could also be included for comparison, such as：
[1] SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference. https://arxiv.org/pdf/2409.12467
[2] SR-Mamba: Effective Surgical Phase Recognition with State Space Model. https://arxiv.org/pdf/2407.08333
[3] SPRMamba: Surgical Phase Recognition for Endoscopic Submucosal Dissection with Mamba. https://arxiv.org/abs/2409.12108

Review Point: 1. The performance of MetaDiff is partially dependent on ConvNeXt+LSTM pre-training. For tasks beyond surgical videos, this dependence could limit the model’s adaptability, especially if ConvNeXt features do not generalize well.
Review Point: 2. Although MetaDiff is well-evaluated on four datasets, these datasets cover similar surgical contexts. The model’s robustness to highly variable surgical environments (e.g., laparoscopic vs. robotic vs. open surgery) remains untested.
Review Point: 3. High dependency on accurate timestamp annotations. Surgical video data often contains inaccurate timestamp annotations, especially when manually labeled. And underexplored sensitivity to non-surgical contextual information, some frames may contain non-surgical context (e.g., camera adjustments or the organ scene is shown, but no procedure is performed).
Review Point: 4. Some new but not necessarily SOTA works could also be included for comparison, such as： [1] SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference. https://arxiv.org/pdf/2409.12467 [2] SR-Mamba: Effective Surgical Phase Recognition with State Space Model. https://arxiv.org/pdf/2407.08333 [3] SPRMamba: Surgical Phase Recognition for Endoscopic Submucosal Dissection with Mamba. https://arxiv.org/abs/2409.12108
==================================================

Focused review:

- Many figures in the paper lead to similar conclusions (e.g., Figure 2 vs. Figure 3, Figure 4 vs. Figure 5), which take up lots of space in the paper and make the paper look verbose. It would be better to move some of them to the appendix.
- Figure 4,5 look a little confusing and need more detailed descriptions.
- For most translation directions, the robustness transferred from EN-FR is not significant enough (still lags behind the results on clean corpus by ≈10 BLEU scores). Although the authors use growth rates to make the benefits look more obvious in Tables 1,2,3, the transfer effects are still limited.
- I would like to see more results based on multilingual pre-trained models, e.g., mBART (Liu et al., 2020), mRASP2 (Pan et al., 2021).
1. Liu et al. Multilingual Denoising Pre-training for Neural Machine Translation. TACL 2020.
2. Pan et al. Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. ACL 2021.

Review Point: - Many figures in the paper lead to similar conclusions (e.g., Figure 2 vs. Figure 3, Figure 4 vs. Figure 5), which take up lots of space in the paper and make the paper look verbose. It would be better to move some of them to the appendix.
Review Point: - Figure 4,5 look a little confusing and need more detailed descriptions.
Review Point: - For most translation directions, the robustness transferred from EN-FR is not significant enough (still lags behind the results on clean corpus by ≈10 BLEU scores). Although the authors use growth rates to make the benefits look more obvious in Tables 1,2,3, the transfer effects are still limited.
Review Point: - I would like to see more results based on multilingual pre-trained models, e.g., mBART (Liu et al., 2020), mRASP2 (Pan et al., 2021).
Review Point: 1. Liu et al. Multilingual Denoising Pre-training for Neural Machine Translation. TACL 2020.
Review Point: 2. Pan et al. Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. ACL 2021.
==================================================

Focused review:

+ Though having promising results, there is a lack of motivation or theoretical discussion on why this ranking can be that effective. From my understanding, the standard CE loss will also decrease the incorrect probabilities (though not that actively).
+ It can be better to present some qualitative examples and demonstrate how SelfODD achieves the correct prediction compared to the baseline.

Review Point: + Though having promising results, there is a lack of motivation or theoretical discussion on why this ranking can be that effective. From my understanding, the standard CE loss will also decrease the incorrect probabilities (though not that actively).
Review Point: + It can be better to present some qualitative examples and demonstrate how SelfODD achieves the correct prediction compared to the baseline.
==================================================

Focused review:

Honestly, I’m not familiar with the denoising-based adversarial defense literature, so my concerns mainly focus on the experimental settings and results.
1. In Figure 1. The authors compare their method with others from the literature. While their method achieves competitive accuracy, the model structure (number of parameters) differs from those used in other studies. It is unclear whether the reduction in inference latency is due to a smaller model size, a high-performance deep learning toolkit (Footnote 5), or advancements in the proposed method. What is the adversarial accuracy of the methods from the literature when using the same models as those employed by the authors in this paper?
2. Line 228-229. Could author further clearify what is role of the teacher model, in the teacher-student training paradigm they used in the paper?
3. What is $Z_1$ and $Z_2$ in the pseudocode of algorithm 1?
4. Line 255-264. Why can a trainable MLP be considered a data augmentation method? Setting data augmentation aside, I still cannot agree with the claim that adding a trainable MLP can "ensure" the model captures meaningful representations. It would be helpful if the authors could provide more evidence to support this statement.
5. In Figure 3, the authors compare their results with those of MoCo V3. However, the accuracy of MoCo V3 is significantly lower than what is reported in the original paper when the radius is set to zero, raising concerns about the correctness of the implementation and the fairness of the comparison.
6. Line 466. Minor: It seems it should be Figure 3 rather than Figure 4.
7. NOT A WEAKNESS (CONCERN). I’m just curious—since the paper is focusing on the defense method, could the authors provide further explanation of the motivation behind Section 4.5, "Image Generation"?

Review Point: 1. In Figure 1. The authors compare their method with others from the literature. While their method achieves competitive accuracy, the model structure (number of parameters) differs from those used in other studies. It is unclear whether the reduction in inference latency is due to a smaller model size, a high-performance deep learning toolkit (Footnote 5), or advancements in the proposed method. What is the adversarial accuracy of the methods from the literature when using the same models as those employed by the authors in this paper?
Review Point: 2. Line 228-229. Could author further clearify what is role of the teacher model, in the teacher-student training paradigm they used in the paper?
Review Point: 3. What is $Z_1$ and $Z_2$ in the pseudocode of algorithm 1?
Review Point: 4. Line 255-264. Why can a trainable MLP be considered a data augmentation method? Setting data augmentation aside, I still cannot agree with the claim that adding a trainable MLP can "ensure" the model captures meaningful representations. It would be helpful if the authors could provide more evidence to support this statement.
Review Point: 5. In Figure 3, the authors compare their results with those of MoCo V3. However, the accuracy of MoCo V3 is significantly lower than what is reported in the original paper when the radius is set to zero, raising concerns about the correctness of the implementation and the fairness of the comparison.
Review Point: 6. Line 466. Minor: It seems it should be Figure 3 rather than Figure 4.
Review Point: 7. NOT A WEAKNESS (CONCERN). I’m just curious—since the paper is focusing on the defense method, could the authors provide further explanation of the motivation behind Section 4.5, "Image Generation"?
==================================================

Focused review:

1. The definition of factual errors or hallucinations provided in the paper (L40-L42) appears to be vague and general. It seems that most caption errors can be classified as hallucinations since content words form the main part of the sentences. However, it is unclear what other types of errors exist if they are not factual. Grammar errors come to mind, but it is unlikely that they constitute the primary type of error in generated captions. Therefore, the high number of hallucinations raises some suspicion.
2. It seems that FactVC only involves fine-tuning CLIP from image to video, which, if true, may not be a significant contribution. Additionally, instead of using CLIP for videos, why not utilize VideoCLIP [1], which is specifically designed for zero-shot video-text understanding?
[1] "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"

Review Point: 1. The definition of factual errors or hallucinations provided in the paper (L40-L42) appears to be vague and general. It seems that most caption errors can be classified as hallucinations since content words form the main part of the sentences. However, it is unclear what other types of errors exist if they are not factual. Grammar errors come to mind, but it is unlikely that they constitute the primary type of error in generated captions. Therefore, the high number of hallucinations raises some suspicion.
Review Point: 2. It seems that FactVC only involves fine-tuning CLIP from image to video, which, if true, may not be a significant contribution. Additionally, instead of using CLIP for videos, why not utilize VideoCLIP [1], which is specifically designed for zero-shot video-text understanding? [1] "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"
==================================================

Focused review:

1) The explainer model is essentially an estimation model for interpreting the behavior of the target classification model; however, this paper does not clearly define what the target model is. Furthermore, there is insufficient evidence to show that the method operates effectively across different target models.
2) Some results seem incomplete, as suggested by Figure 1.
3) The ablation study is lacking. While the proposed method focuses on effectively learning the distribution and sampling, there is no analysis of which aspect is more critical to the overall success of the method.
4) The problems identified with existing methods are described conceptually but lack empirical validation.

Review Point: 1) The explainer model is essentially an estimation model for interpreting the behavior of the target classification model; however, this paper does not clearly define what the target model is. Furthermore, there is insufficient evidence to show that the method operates effectively across different target models.
Review Point: 2) Some results seem incomplete, as suggested by Figure 1.
Review Point: 3) The ablation study is lacking. While the proposed method focuses on effectively learning the distribution and sampling, there is no analysis of which aspect is more critical to the overall success of the method.
Review Point: 4) The problems identified with existing methods are described conceptually but lack empirical validation.
==================================================

Focused review:

1. Steganography pursues behavioral security, but the framework causes the size of the stego INR to be larger than the size of the normal cover INR, and an attacker may be able to detect the existence of INR steganography based on this anomalous behavior.
2. Security experiments: although this paper can resist traditional image steganalysis, considering that it is similar to neural network steganography, it should be supplemented with experiments on resisting neural network steganalysis.
3. Comparison experiments: Considering that multimodal data can be converted into binary streams, this paper should be supplemented with comparisons with binary stream steganography (e.g., chatgan, etc.).

Review Point: 1. Steganography pursues behavioral security, but the framework causes the size of the stego INR to be larger than the size of the normal cover INR, and an attacker may be able to detect the existence of INR steganography based on this anomalous behavior.
Review Point: 2. Security experiments: although this paper can resist traditional image steganalysis, considering that it is similar to neural network steganography, it should be supplemented with experiments on resisting neural network steganalysis.
Review Point: 3. Comparison experiments: Considering that multimodal data can be converted into binary streams, this paper should be supplemented with comparisons with binary stream steganography (e.g., chatgan, etc.).
==================================================

Focused review:

1）The progressive sampling proposed in this paper is, in strict terms, still a form of masking technique. It utilizes the self-attention interaction between character embeddings of predicted and unpredicted characters, which is not novel in the context of scene text recognition.
2) Regarding Section 3.2, "Decoupled Non-Autoregressive Decoder," there seems to be an issue with the attention formula in equation (3). It should be $Attention(q,k,v)=softmax(\frac{qk^T}{\sqrt{d_k}})v$. Additionally, the authors mention that $qk^T$ can be expressed as $c_{q}^{T}X + p_{q}^{T}X_p$. However, what purpose does the multiplication of the two position matrices serve, and what is the significance of performing cross-attention between them?
3) Although the authors describe their model as relatively simple, the model structure and training procedures suggest otherwise. It involves a two-stage training process, introduces additional character positional information, and requires prediction outputs at each decoder layer. This approach incorporates a considerable amount of extra information, such as character positional data and a pre-trained CTC decoder for initializing inputs to the non-autoregressive decoder. From this perspective, the proposed method does not seem to be a straightforward model, despite achieving significant performance improvements over other non-autoregressive approaches.
4) While the authors compare their method with numerous text recognition techniques in Table 1, there is a lack of comparison with more recent text recognition technologies. It is unclear when the authors wrote this paper, but the latest method they compare against appears to be a paper published in 2022, making such experimental comparisons less convincing. Moreover, the authors only conducted experiments on some existing datasets, without addressing more challenging datasets like Union14M-Benchmark in this paper.

Review Point: 1）The progressive sampling proposed in this paper is, in strict terms, still a form of masking technique. It utilizes the self-attention interaction between character embeddings of predicted and unpredicted characters, which is not novel in the context of scene text recognition.
Review Point: 2) Regarding Section 3.2, "Decoupled Non-Autoregressive Decoder," there seems to be an issue with the attention formula in equation (3). It should be $Attention(q,k,v)=softmax(\frac{qk^T}{\sqrt{d_k}})v$. Additionally, the authors mention that $qk^T$ can be expressed as $c_{q}^{T}X + p_{q}^{T}X_p$. However, what purpose does the multiplication of the two position matrices serve, and what is the significance of performing cross-attention between them?
Review Point: 3) Although the authors describe their model as relatively simple, the model structure and training procedures suggest otherwise. It involves a two-stage training process, introduces additional character positional information, and requires prediction outputs at each decoder layer. This approach incorporates a considerable amount of extra information, such as character positional data and a pre-trained CTC decoder for initializing inputs to the non-autoregressive decoder. From this perspective, the proposed method does not seem to be a straightforward model, despite achieving significant performance improvements over other non-autoregressive approaches.
Review Point: 4) While the authors compare their method with numerous text recognition techniques in Table 1, there is a lack of comparison with more recent text recognition technologies. It is unclear when the authors wrote this paper, but the latest method they compare against appears to be a paper published in 2022, making such experimental comparisons less convincing. Moreover, the authors only conducted experiments on some existing datasets, without addressing more challenging datasets like Union14M-Benchmark in this paper.
==================================================

Focused review:

- The presentation of the paper is weak. There are large white spaces all over the paper (bullet points, equations, figures). All the images are very small and hard to read. Figures 2, 3, and 4 overlap with the figure description, making the description hard to read. Figures are referred to as Tables in the text. The poor formatting of the paper makes it feel incomplete.
- The paper should provide intuition for its theoretical results. As it reads now, the implication of each theorem is missing. Instead, the authors simply state the theorem followed by a proof. I would suggest placing the proofs in the Appendix so that the main paper can focus on the "why?" for each theorem.
- It is unclear what the proposed algorithm is. I read through the paper multiple times, but it seems Section 3 talks about theoretical results and Section 4 immediately jumps into experimental results without any explanation of what UDA-EDCM is. Furthermore, the only Figure explaining the algorithm, Figure 1, is not referenced anywhere in the text.
- The experimental evidence is not clearly explained. The authors simply list the performance improvements in Section 4.3 and 4.4, with no explanation why the performance is improved.
- The authors should provide more explanation why they do not compare against any general-purpose large language models such as CodeLlama, Llama, GPT, DeepSeekCoder, Claude, or Mistral.

Review Point: - The presentation of the paper is weak. There are large white spaces all over the paper (bullet points, equations, figures). All the images are very small and hard to read. Figures 2, 3, and 4 overlap with the figure description, making the description hard to read. Figures are referred to as Tables in the text. The poor formatting of the paper makes it feel incomplete.
Review Point: - The paper should provide intuition for its theoretical results. As it reads now, the implication of each theorem is missing. Instead, the authors simply state the theorem followed by a proof. I would suggest placing the proofs in the Appendix so that the main paper can focus on the "why?" for each theorem.
Review Point: - It is unclear what the proposed algorithm is. I read through the paper multiple times, but it seems Section 3 talks about theoretical results and Section 4 immediately jumps into experimental results without any explanation of what UDA-EDCM is. Furthermore, the only Figure explaining the algorithm, Figure 1, is not referenced anywhere in the text.
Review Point: - The experimental evidence is not clearly explained. The authors simply list the performance improvements in Section 4.3 and 4.4, with no explanation why the performance is improved.
Review Point: - The authors should provide more explanation why they do not compare against any general-purpose large language models such as CodeLlama, Llama, GPT, DeepSeekCoder, Claude, or Mistral.
==================================================

Focused review:

- It’s not clear to me how the formulation alone is an improvement upon the previous work. I would suggest making it more explicit why this formulation is desirable over other existing work.
- The motivating example does not seem correct.
- The writing needs improvment. For example, $D_{1/2}, V_{1/2}, V_{c}$ are not defined when it is mentioned the first time in the related work. It also impedes my understanding of the proofs of theorems. For details, please see my questions.
- I find these lines 155-157 very difficult to understand until I read the second part of the proof of theorem 3.1: "Such an edge could be explained by a direct edge in the output graph, and also by a directed path that involves only nodes that do not appear in T (since such a path would be an edge in T).”
- In the real-world experiment, the exact total number of variables used in lines 370-371 is difficult to determine. I need to read Figure 4 to understand it.

Review Point: - It’s not clear to me how the formulation alone is an improvement upon the previous work. I would suggest making it more explicit why this formulation is desirable over other existing work.
Review Point: - The writing needs improvment. For example, $D_{1/2}, V_{1/2}, V_{c}$ are not defined when it is mentioned the first time in the related work. It also impedes my understanding of the proofs of theorems. For details, please see my questions.
Review Point: - I find these lines 155-157 very difficult to understand until I read the second part of the proof of theorem 3.1: "Such an edge could be explained by a direct edge in the output graph, and also by a directed path that involves only nodes that do not appear in T (since such a path would be an edge in T).” - In the real-world experiment, the exact total number of variables used in lines 370-371 is difficult to determine. I need to read Figure 4 to understand it.
==================================================

Focused review:

The second contribution is not well presented. The paper significantly lacks experimental analysis to support the effect of α and β. The guideline is also unclear.
As far as I understand, the proposed method requires the ground truth animation to learn the parameters, so the method is usable for compression but not for creative design. Even as a compression method, there is no experiment and discussion about the relationship between compression rate and accuracy degradation.
The authors raised two important points as future work: further analysis of the metric parameters and full control over the parameters. These points are definitely crucial and necessary for acceptance.
According to the above issues, section 2 about related work is not appropriate in its current form.
Since many methods are not appropriately addressed, some of them should still be comparable in the given problem setting.
Detailed comments.
- There is an unnecessary term in the caption of Figure 3.
- "whether whether" at the bottom of page 6.
- The authors introduced "weight" in equation 9, but did not evaluate it.
- The authors simply took a weighted metric $Q_{hyb}$ as 0.5 $Q_{loc}$ + 0.5 $Q_{rot}$, but the former depends on the scaling of the scene and the latter is scale-invariant. The current result should also depend on the scale of the scene, which is inappropriate.
- Although the authors show a quantitative result, it seems inappropriate. The authors' statement means that the quantitative result does not reflect the quality: "Despite the seemingly small quantitative difference between these two, we note that Fig. 5d shows significant perceptually differences."
- The statement lacks support: "we are able to accurately represent a high frame rate animation with very few frames, we achieve a compression rate that requires digital animators to pose fewer keyframes during".

Review Point: - There is an unnecessary term in the caption of Figure 3.
Review Point: - The authors introduced "weight" in equation 9, but did not evaluate it.
Review Point: - The authors simply took a weighted metric $Q_{hyb}$ as 0.5 $Q_{loc}$ + 0.5 $Q_{rot}$, but the former depends on the scaling of the scene and the latter is scale-invariant. The current result should also depend on the scale of the scene, which is inappropriate.
Review Point: - Although the authors show a quantitative result, it seems inappropriate. The authors' statement means that the quantitative result does not reflect the quality: "Despite the seemingly small quantitative difference between these two, we note that Fig. 5d shows significant perceptually differences." - The statement lacks support: "we are able to accurately represent a high frame rate animation with very few frames, we achieve a compression rate that requires digital animators to pose fewer keyframes during".
==================================================

Focused review:

## Weaknesses
1.	**Mixed Experimental Results**: While some datasets show significant improvements, others do not, suggesting the framework's effectiveness may vary depending on dataset characteristics.
2.	**Insufficient Discussion of Decomposition Algorithm**: The decomposition algorithm is introduced, but the number of iterations of decomposition is not discussed.
3.	**Lack of Novelty**: The idea of combining the existing decomposition method with the existing time series forecasting method weakens the innovation of this method.

Review Point: 2. **Insufficient Discussion of Decomposition Algorithm**: The decomposition algorithm is introduced, but the number of iterations of decomposition is not discussed.
Review Point: 3. **Lack of Novelty**: The idea of combining the existing decomposition method with the existing time series forecasting method weakens the innovation of this method.
==================================================

Focused review:

1. While the study shows experimental progress, the improvement in FID (Frechet Inception Distance) is modest.
2. The authors are encouraged to conduct experiments on CIFAR-100 using BigGAN and FFHQ using StyleGAN2 to further verify the effectiveness of the proposed method.
3. To enhance accessibility, the authors should consider simplifying the mathematical notation to cater to a wider audience.

Review Point: 1. While the study shows experimental progress, the improvement in FID (Frechet Inception Distance) is modest.
Review Point: 2. The authors are encouraged to conduct experiments on CIFAR-100 using BigGAN and FFHQ using StyleGAN2 to further verify the effectiveness of the proposed method.
Review Point: 3. To enhance accessibility, the authors should consider simplifying the mathematical notation to cater to a wider audience.
==================================================

Focused review:

- While the method is innovative, its technical depth appears limited, relying significantly on GPT-4 data augmentation. The application of contrastive relevance calibration could be viewed as a nuanced extension of contrastive decoding, lacking substantial novelty.
- The validation for the correctness of generated data is missing. Effective validation would ideally include rigorous human annotation to ensure the quality and factual accuracy of the augmented data.
- The need for an additional module to decompose the query and multiple retrieval processes introduces considerable computational costs, which might limit the method’s applicability in real applications. This aspect, along with its impact on system efficiency, should be discussed.
- The model’s applicability to complex, multi-hop question answering scenarios (e.g., identifying sequential factual relationships) seems limited. When a sub-query is dependent on the other one, it is hard to retrieve relevant documents in a single retrieval step.

Review Point: - While the method is innovative, its technical depth appears limited, relying significantly on GPT-4 data augmentation. The application of contrastive relevance calibration could be viewed as a nuanced extension of contrastive decoding, lacking substantial novelty.
Review Point: - The validation for the correctness of generated data is missing. Effective validation would ideally include rigorous human annotation to ensure the quality and factual accuracy of the augmented data.
Review Point: - The need for an additional module to decompose the query and multiple retrieval processes introduces considerable computational costs, which might limit the method’s applicability in real applications. This aspect, along with its impact on system efficiency, should be discussed.
Review Point: - The model’s applicability to complex, multi-hop question answering scenarios (e.g., identifying sequential factual relationships) seems limited. When a sub-query is dependent on the other one, it is hard to retrieve relevant documents in a single retrieval step.
==================================================

Focused review:

+ How sure are we that the language model won't express their uncertainties via natural language if we prompt them well enough? I'm expecting this capability should be attainable with few-shot prompts.
+ A big confounder is that by tuning the hyperparameters of the uncertainty metrics, we actually find a predictor of the correctness. And this alone (instead of communication) is the real drive behind improved scores. A n ablation is needed for a simple weighted majority vote.
+ As mentioned in L141, Pham et al. and Chen et al. are other improvements made on Du et al. How does the proposed technique compare to them? (just asking, these can be argued to be contemporaneous)

Review Point: + How sure are we that the language model won't express their uncertainties via natural language if we prompt them well enough? I'm expecting this capability should be attainable with few-shot prompts.
Review Point: + A big confounder is that by tuning the hyperparameters of the uncertainty metrics, we actually find a predictor of the correctness. And this alone (instead of communication) is the real drive behind improved scores. A n ablation is needed for a simple weighted majority vote.
Review Point: + As mentioned in L141, Pham et al. and Chen et al. are other improvements made on Du et al. How does the proposed technique compare to them? (just asking, these can be argued to be contemporaneous)
==================================================

Focused review:

- Missing references [1,2,3].
- The idea of the proposed method sounds very normal. It is very common and has been well studied to use gradient and attention to localize the object in Transformer-based architecture. Talking about text-image alignment, [4] also shows a more precise segmentation results than the proposed method.
- Figure 4 shows typo in the x-axis. The x-axis means the "% of accumulated layer", so it should be 10, 20, ..., 100 rather than 0.1, 0.2, ..., 1.0. References:
[1] IA-RED^2: Interpretability-Aware Redundancy Reduction for Vision Transformers.
[2] Emerging properties in self-supervised vision transformers.
[3] Exploring Visual Explanations for Contrastive Language-Image Pre-training.
[4] Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models

Review Point: - Missing references [1,2,3].- The idea of the proposed method sounds very normal. It is very common and has been well studied to use gradient and attention to localize the object in Transformer-based architecture. Talking about text-image alignment, [4] also shows a more precise segmentation results than the proposed method.
==================================================

Focused review:

1. This work is too hasty and lacks sufficient experiments to support the conclusion. I would raise my score if the authors could provide a fully prepared version during the rebuttal stage.
2. There is a lack of discussion and comparison with related work, such as non-autoregressive [1] or semi-autoregressive [2] sequence generation approaches.
3. The authors report only the theoretical decoding speedup, without evaluating its performance in a real-world environment.
4. Reporting test losses on WMT benchmarks is not a standard practice. Please implement greedy or beam search and provide the corresponding BLEU scores for a more meaningful evaluation.
[1] Gu, Jiatao, et al. "Non-autoregressive neural machine translation." arXiv preprint arXiv:1711.02281 (2017).
[2] Wang, Chunqi, Ji Zhang, and Haiqing Chen. "Semi-autoregressive neural machine translation." arXiv preprint arXiv:1808.08583 (2018).

Review Point: 2. There is a lack of discussion and comparison with related work, such as non-autoregressive [1] or semi-autoregressive [2] sequence generation approaches.
Review Point: 3. The authors report only the theoretical decoding speedup, without evaluating its performance in a real-world environment.
Review Point: 4. Reporting test losses on WMT benchmarks is not a standard practice. Please implement greedy or beam search and provide the corresponding BLEU scores for a more meaningful evaluation. [1] Gu, Jiatao, et al. "Non-autoregressive neural machine translation." arXiv preprint arXiv:1711.02281 (2017). [2] Wang, Chunqi, Ji Zhang, and Haiqing Chen. "Semi-autoregressive neural machine translation." arXiv preprint arXiv:1808.08583 (2018).
==================================================

Focused review:

- By comparing against Alpaca and CodeAlpaca, the authors demonstrate that their data generation technique is superior to other prompt-based automatic data generation techniques for this task. However, it is not clear whether such techniques are better than just directly using human-written examples from GitHub. In Lines 163-175, the authors claim that GitHub commits are too noisy to use directly; however, this is not empirically validated. GitHub serves as an extremely large data source, and given that one of the findings of this paper is that the scale of the data is a profound factor of code-editing ability, it is important to understand whether the scale of the data reduces the impact of noise. Moreover, commit messages are not the only source of NL instructions from GitHub. Another source is pull request comments (and the corresponding code edits). In fact, there is already a large-scale benchmark for this: CodeReviewer (see missing references). As a point of reference, it would be important to understand what the effect is of fine-tuning open-source LLMs on CodeReviewer, and how this compares to fine-tuning on CodeInstruct.
- The evaluation is limited and weak. First, the test set entails only 134 examples which are manually curated by the authors of the paper, and it appears that they are all in Python. Next, the main form of evaluation is prompting GPT-4 to judge the correctness of model predictions. While this has been explored for other tasks, this has not been established as a valid evaluation strategy for code editing. In Appendix E, the authors provide a justification for using GPT-4 for evaluation by comparing with human evaluation. However, I do not find this convincing for a few reasons. 1) The human evaluation is done by authors of this paper and not by external evaluators, and no information is given about annotator agreement, 2) The human evaluation entailed three classes (correct, partial, wrong), while GPT-4 evaluates based on 2 classes ("Yes" or "No"). For comparison, they group "partial" with "correct" while it should actually be considered wrong. 3) Finally, the consistency ratio is 68.4%, which is lower than what I would expect for strong evaluation.

Review Point: - The evaluation is limited and weak. First, the test set entails only 134 examples which are manually curated by the authors of the paper, and it appears that they are all in Python. Next, the main form of evaluation is prompting GPT-4 to judge the correctness of model predictions. While this has been explored for other tasks, this has not been established as a valid evaluation strategy for code editing. In Appendix E, the authors provide a justification for using GPT-4 for evaluation by comparing with human evaluation. However, I do not find this convincing for a few reasons.
Review Point: 1) The human evaluation is done by authors of this paper and not by external evaluators, and no information is given about annotator agreement,
Review Point: 2) The human evaluation entailed three classes (correct, partial, wrong), while GPT-4 evaluates based on 2 classes ("Yes" or "No"). For comparison, they group "partial" with "correct" while it should actually be considered wrong.
Review Point: 3) Finally, the consistency ratio is 68.4%, which is lower than what I would expect for strong evaluation.
==================================================

Focused review:

1.	The literature review on band selection is limited in the section of related work. A number of new relevant methods are not mentioned.
2.	The problem formulation and motivation of this paper are not precise. Some statements lack evidence and are not convincing. For instance: in line 46, “.These approaches are problematic because the assigned importance is not always precise and overlooks the interplay between bands”. There are many band selection methods that leverage the band correlations. Moreover, in line 52: “Existing methods of imposing sparsity, such as L1 and L2 losses, do not consistently yield table sparsity effects;” This sentence is vague and is difficult to understand. The claimed second challenge in line 52 is not convincing.
3.	The comparison is unfair. The proposed method is supervised, which introduced the post-task related loss. However, the compared methods are task-independent. For this reason, the results are not convincing.

Review Point: 1. The literature review on band selection is limited in the section of related work. A number of new relevant methods are not mentioned.
Review Point: 2. The problem formulation and motivation of this paper are not precise. Some statements lack evidence and are not convincing. For instance: in line 46, “.These approaches are problematic because the assigned importance is not always precise and overlooks the interplay between bands”. There are many band selection methods that leverage the band correlations. Moreover, in line 52: “Existing methods of imposing sparsity, such as L1 and L2 losses, do not consistently yield table sparsity effects;” This sentence is vague and is difficult to understand. The claimed second challenge in line 52 is not convincing.
Review Point: 3. The comparison is unfair. The proposed method is supervised, which introduced the post-task related loss. However, the compared methods are task-independent. For this reason, the results are not convincing.
==================================================

Focused review:

1. This paper proposes that multiple transformation techniques can be applied to generate the candidate inputs for jailbreak attacks. I am not clear on how many candidate inputs are necessary to conduct the attacks. Is there a strategy to guide the optimized input generation?
2. The evaluation seems not practical to me. From my understanding, it relies on external models to judge harmfulness, raising questions about the accuracy and reliability of these models as evaluators. I am concerned that the significant results reported may stem from biases or limitations within these external models rather than the actual effectiveness of the proposed attack. For instance, responses that are unrelated to the original harmful intent—such as teaching children survival information in case of a bomb threat—could potentially be flagged as harmful simply due to the presence of certain keywords. This could lead to false positives, where benign content is mistakenly categorized as harmful.

Review Point: 1. This paper proposes that multiple transformation techniques can be applied to generate the candidate inputs for jailbreak attacks. I am not clear on how many candidate inputs are necessary to conduct the attacks. Is there a strategy to guide the optimized input generation?
==================================================

Focused review:

- Although the coverage of tasks in the experiments seems sufficient, the variation of the tested model variants is limited. Specifically, only a different single model (e.g., BERT-base) is examined in different experiments.
- How many models with different seeds are used in the experiments? If each experiment relies on a single run, I suspect the generality of the improvement by TLM and the significance of the performance differences.
- Ablation study and some analyses (Section 4.5) are also conducted only in specific tasks (e.g., STS-B) or input. I understand that conducting all the analyses in all the tasks requires a high computation cost, but I would like to see a motivation for why the specific task/model was used in each analysis, and at least the information in this paper alone seems to cherry-pick the good results. Furthermore, I am not sure what generalizable findings can be obtained by seeing the attention pattern in a single instance (Figure 4).
To sum up, the focus of the experiments seemed to be blurred (in terms of model and task types), with sporadic evidence of effectiveness for certain combinations of tasks and models. In other words, I'd like to know the motivation why each setting was chosen. I am willing to accept reduced task coverage, so I would like to see more solid findings with respect to, especially, model size/variants/seeds and the characteristics/limitations of the proposed method (the robustness of the results in Tables 6--8 are).

Review Point: - Although the coverage of tasks in the experiments seems sufficient, the variation of the tested model variants is limited. Specifically, only a different single model (e.g., BERT-base) is examined in different experiments.
Review Point: - How many models with different seeds are used in the experiments? If each experiment relies on a single run, I suspect the generality of the improvement by TLM and the significance of the performance differences.
==================================================

Focused review:

- The paper lacks a clear motivation for the problem of semantic shift detection and the importance of modeling diversity-agnostic non-semantic patterns.
- The paper could provide more details about the experimental setup, such as the hyperparameters used.

Review Point: - The paper lacks a clear motivation for the problem of semantic shift detection and the importance of modeling diversity-agnostic non-semantic patterns.
Review Point: - The paper could provide more details about the experimental setup, such as the hyperparameters used.
==================================================

Focused review:

* Utilizing the aggregate of past statistics to adjust the model parameters poses a challenge. As the model parameters shift with each time step, past statistics, derived from earlier model parameters, might diverge significantly from what would be obtained if the model gradients were calculated using all the previously stored data. I'm curious if the authors could elaborate on the conditions under which using aggregated past gradients to update the current model would be successful or not.
* The problem is framed in an online setting where the actual predictive labels are seen after the model makes its predictions. More realistically, if the model consistently underperforms, it could result in a shift in data distribution. Specifically, the minority group might cease supplying data for model updates. Could the author discuss how the proposed algorithm would operate under these circumstances?

Review Point: * Utilizing the aggregate of past statistics to adjust the model parameters poses a challenge. As the model parameters shift with each time step, past statistics, derived from earlier model parameters, might diverge significantly from what would be obtained if the model gradients were calculated using all the previously stored data. I'm curious if the authors could elaborate on the conditions under which using aggregated past gradients to update the current model would be successful or not.
Review Point: * The problem is framed in an online setting where the actual predictive labels are seen after the model makes its predictions. More realistically, if the model consistently underperforms, it could result in a shift in data distribution. Specifically, the minority group might cease supplying data for model updates. Could the author discuss how the proposed algorithm would operate under these circumstances?
==================================================

Focused review:

1. The use of Gaussian distributions in representation learning is not entirely new. The paper could benefit from a clearer distinction between GCS and other probabilistic models used in similar contexts.
2. The paper primarily focuses on a specific set of tasks and datasets. To fully establish the significance of GCS, the authors should explore its applicability to a wider range of tasks and domains.

Review Point: 1. The use of Gaussian distributions in representation learning is not entirely new. The paper could benefit from a clearer distinction between GCS and other probabilistic models used in similar contexts.
Review Point: 2. The paper primarily focuses on a specific set of tasks and datasets. To fully establish the significance of GCS, the authors should explore its applicability to a wider range of tasks and domains.
==================================================

Focused review:

1. Peer-to-peer distributed training and gradient-tracking methodology are not new. And it's difficult to spot the novelty of the proposed approach.
2. The results in Table 2 to Table 4 are far from convincing. Besides Local-FT, hierarchical distributed training should be another baseline to be included.
3. The experiments cannot support the claim, "the proposed method performs effectively in heterogeneous data settings and eliminates the bias caused by the non-uniform data distributions present across different computational nodes" since the training and test data for each node are from the same dataset.

Review Point: 1. Peer-to-peer distributed training and gradient-tracking methodology are not new. And it's difficult to spot the novelty of the proposed approach.
Review Point: 2. The results in Table 2 to Table 4 are far from convincing. Besides Local-FT, hierarchical distributed training should be another baseline to be included.
Review Point: 3. The experiments cannot support the claim, "the proposed method performs effectively in heterogeneous data settings and eliminates the bias caused by the non-uniform data distributions present across different computational nodes" since the training and test data for each node are from the same dataset.
==================================================

Focused review:

1. The evaluation is mainly based on the derived TEXTBINDEVAL dataset, and also seems do damage to the results of the benchmark datasets. Besides, given the automatic text generation metrics such as BLEU, Rouge, it is hard to know on where the derived dataset and training helps.
2. For the data collection procedure, clustering the image together and chat about the visually similar images may not be the real-world demands for multimodal instruction-following.
3. Compared with other work about multimodal instruction-following, such as Otter[1], what is the advantage and difference of this method?
[1] Otter: A Multi-Modal Model with In-Context Instruction Tuning

Review Point: 1. The evaluation is mainly based on the derived TEXTBINDEVAL dataset, and also seems do damage to the results of the benchmark datasets. Besides, given the automatic text generation metrics such as BLEU, Rouge, it is hard to know on where the derived dataset and training helps.
Review Point: 2. For the data collection procedure, clustering the image together and chat about the visually similar images may not be the real-world demands for multimodal instruction-following.
Review Point: 3. Compared with other work about multimodal instruction-following, such as Otter[1], what is the advantage and difference of this method? [1] Otter: A Multi-Modal Model with In-Context Instruction Tuning
==================================================

Focused review:

- In my opinion, the main weakness of the paper is the reduced size of the dataset. Authors propose a dataset with around 10k examples, from which 10% of it is used for testing. This means, the test performance which will be driving the field and the performance in the task will be computed over 1000 samples, which in my opinion is very limited. From the paper description, I understand the annotation is very costly, but I wonder if the fact that the labels are clean compensates the small amount of overall data. I believe authors should analyse in depth the effect of dataset size in the models. - Did the authors check whether there is an added bias when using images from Getty? As the images do not come from the original memes, it could be that the distribution is different between the newly defined memes and the original ones. - It would be interesting to look at the failure modes of the models. Are the models consistent on their failures? Are there particularly hard categories in the task? - How are the authors dealing with non-standard text (acronyms, non existent words, etc)? As memes are collected in the wild, I would assume some of the text is non standard. Are authors correcting the text if that happens?

Review Point: - Did the authors check whether there is an added bias when using images from Getty? As the images do not come from the original memes, it could be that the distribution is different between the newly defined memes and the original ones.
Review Point: - It would be interesting to look at the failure modes of the models. Are the models consistent on their failures? Are there particularly hard categories in the task?
Review Point: - How are the authors dealing with non-standard text (acronyms, non existent words, etc)? As memes are collected in the wild, I would assume some of the text is non standard. Are authors correcting the text if that happens?
==================================================

Focused review:

- The motivation of this work is rather unclear to me. Is this work about advocating the use of pre-trained large language models as a potential method for compression? If so, how can they be used as such in practice considering their limitations? Or is it about using the compression framework to better understand large language models? If so, why is it interesting to study pre-trained large language models "through the lens of compression"?
- The authors mention that they “advocate for using (lossless) compression to study foundation models”. Why and what benefits does this framework have? It is not clear to me how the results in this paper should help my understanding of large language models beyond their use as compressors? What are the further implications of the results?
- No experiments with pre-trained models other than Chinchilla-70B. Having more models could provide more evidence on the compression capabilities of pre-trained large language models and to see how compression capabilities correlate with prediction performance
- Not using publicly available pre-trained large language models for reproducibility
- The results for the generative modeling performance of compressors and Chinchilla-70B look rather poor. For example, the generated image in Figure 3 looks unconvincing since only lines are generated and not actual image content, and a quantitative analysis is also missing. Why is this section important, and why would it fit into the rest of the paper?

Review Point: - The motivation of this work is rather unclear to me. Is this work about advocating the use of pre-trained large language models as a potential method for compression? If so, how can they be used as such in practice considering their limitations? Or is it about using the compression framework to better understand large language models? If so, why is it interesting to study pre-trained large language models "through the lens of compression"?
Review Point: - The authors mention that they “advocate for using (lossless) compression to study foundation models”. Why and what benefits does this framework have? It is not clear to me how the results in this paper should help my understanding of large language models beyond their use as compressors? What are the further implications of the results?
==================================================

Focused review:

- The number of samples is a cause for concern. (J=5, K=10)
-- J=5 : does this imply that the model uses 5 samples to infer the metadata and metrics that characterize the entire distribution? Given the size of the data and the number of topics that may be present (100K/300K), it merits more discussion on whether J samples can represent the data sufficiently.
-- K=10 : Having obtained metadata and metrics, as well as a prompt that grades samples according to the rubric defined, lines 192-194 suggest that only K samples out of the dataset are then assigned to clusters by the LLM. Can a cluster score over 10 samples definitely yield an idea of the diversity of the dataset? This could benefit from more supporting evidence.
In light of this, conducting sensitivity analyses, and extending the ablation studies in B.2 to higher values of K and a range of values for J would mitigate this concern. Furthermore, including other clustering metrics in the ablation study beyond the novel LLM cluster metric would help corroborate these values more strongly.
- The self-verification module could benefit from clarification along the following lines:
-- Was there any human evaluation performed in order to verify the outcomes of the self-verification module, and in order to find the optimal verification prompt?
-- Using the same model to verify its own reasoning chain and clustering judgements may lead to a positive bias; the verification module would benefit from utilizing an LLM that is not part of the earlier steps of cluster score generation pipeline.
- There is a lack of clarity on the difference between the "metadata" and "metric" axes. How are these factors differentiated? Based on the sample outputs in Appendix D.2, there is some overlap between the nature of the "metadata" and "metric" keys, in terms of quantifiability and subjectivity. This lack of distinction is made stronger by the prompt templates in Appendix D.1, where the templates for metrics and metadata generation are nearly identical. It would be helpful to delineate [1] how these terms are different in theory, [2] in practice, how the outcomes of their respective prompts differ, and [3] whether this difference is essential to maintain, as opposed to a single prompt that generates factors that could be either "metrics" or "metadata".
- Additionally, some of the "metadata" and "metrics" are ill-defined, with only qualitative judgements associated with them, and that too only for the values on the extreme ends of the 1-5 scoring scale (See Appendix D.2) . This raises a concern: LLMs often show a lack of consistency in their outputs, even for the same input. This is an even greater concern when LLMs are used to confer subjective evaluations on the input. The diversity metric proposed in this work may be non-reproducible, especially over different samples of the same dataset. The inconsistency could be be mitigated by including a more detailed, unambiguous rubric. The results would also be strengthened by running multiple trials with various seeds for random sampling the data.
- Figures 5 and 6 show that the inter-cluster variance achieved by using LLM Cluster score is higher than that achieved by other methods, thus demonstrating its efficacy in segmenting a dataset with 100K-300K topics. However, owing to the non-uniform scaling of the X-axis and the non-comparable nature of the range of values ([24,24.2] for Perplexity, [192,100] for K-Means, [3.5,5] for LLM Cluster score), it is not possible to definitively determine that a higher cluster score is more correlated with diversity than a higher measure of any other diversity metric. Contextualizing the comparative performance of these scores and their usefulness in 3.2 onwards, along with presenting standardized results in the visualizations would help better understand the benefits of LLM cluster score over other metrics.

Review Point: - The self-verification module could benefit from clarification along the following lines: -- Was there any human evaluation performed in order to verify the outcomes of the self-verification module, and in order to find the optimal verification prompt? -- Using the same model to verify its own reasoning chain and clustering judgements may lead to a positive bias; the verification module would benefit from utilizing an LLM that is not part of the earlier steps of cluster score generation pipeline.
==================================================

Focused review:

1. I believe the title and abstract are misleading about the scope of the paper: while POGEMA appeals to a broad audience of MARL-based navigation in title and abstract, in fact it is about two variants of MAPF on discrete grids with simplified settings. For instance, in terms of MAPF, continuous variants like [1] does not appear to be considered. Moreover, there does not seem to be any mention about any-angle versions that would make the problem more realistic and interesting like [2].
2. One main concern about this paper is that while authors introduce a new MARL environment, they mostly perform inference with it and several results are not reproduced by retraining models. For instance, the DCC and SCRIMP codes do not contain any training script or guidance on how to reproduce results. I believe comparing training performance or speed with the proposed environment against the original implementation would strengthen the paper.
3. Authors claim scalability of >1000 agents. I cannot find any result to substantiate this claim.
4. The LaCAM version the authors mention refers to the first publication. However, new variants were released open source, i.e. LaCAM3 [3]. Moreover, versions as [4] can solve scales up to 10k agents in seconds.
5. Authors acknowledge the limitation of lack of JAX support and GPU-based parallelization tools. In this aspect, I believe the proposed (L)MAPF environments could be created by quite simple modifications of existing environments in Jumanji as the RobotWarehouse ([https://instadeepai.github.io/jumanji/environments/robot_warehouse/](https://instadeepai.github.io/jumanji/environments/robot_warehouse/)).
6. While this is a relatively minor point, I believe there are quite a few problems in terms of writing for the related work. Firstly: I don’t see why the paper should mention all multi-agent benchmarks, when the considered problems are only two very specialized ones. In this sense, Table 1 should contain information about “topic” or “area”, which is missing. GPU parallelization is not considered in the table. The main issue here is that it appears that POGEMA solves all the issues of previous MARL benchmarks such as Nocturne, while they solve arguably much simpler problems in restricted settings. Finally: I don’t see why there is a need to explain every single component as a separate paragraph for a total of almost 3 pages, such as “Python-based” and “PyPI listed" .
7. Finally, as a benchmark, it would be nice to include insights, i.e., what are possible future directions of research. Looking at the graph, one might conclude that there is no point in conducting research in MARL+MAPF, given that heuristic approaches perform well in all metrics, except scalability, only for the LMAPF case.
[1] Andreychuk, Anton, et al. "Multi-agent pathfinding with continuous time." Artificial Intelligence 305 (2022): 103662.
[1] Yakovlev, Konstantin, Anton Andreychuk, and Roni Stern. "Optimal and Bounded Suboptimal Any-Angle Multi-agent Pathfinding." Proceedings of the International Symposium on Combinatorial Search. Vol. 17. 2024.
[3] Okumura, Keisuke. "Engineering LaCAM\*: Towards Real-time, Large-scale, and Near-optimal Multi-agent Pathfinding." Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems. 2024.
[4] Okumura, Keisuke. "Improving lacam for scalable eventually optimal multi-agent pathfinding." arXiv preprint arXiv:2305.03632 (2023).

Review Point: 1. I believe the title and abstract are misleading about the scope of the paper: while POGEMA appeals to a broad audience of MARL-based navigation in title and abstract, in fact it is about two variants of MAPF on discrete grids with simplified settings. For instance, in terms of MAPF, continuous variants like [1] does not appear to be considered. Moreover, there does not seem to be any mention about any-angle versions that would make the problem more realistic and interesting like [2].
Review Point: 2. One main concern about this paper is that while authors introduce a new MARL environment, they mostly perform inference with it and several results are not reproduced by retraining models. For instance, the DCC and SCRIMP codes do not contain any training script or guidance on how to reproduce results. I believe comparing training performance or speed with the proposed environment against the original implementation would strengthen the paper.
Review Point: 3. Authors claim scalability of >1000 agents. I cannot find any result to substantiate this claim.
Review Point: 4. The LaCAM version the authors mention refers to the first publication. However, new variants were released open source, i.e. LaCAM3 [3]. Moreover, versions as [4] can solve scales up to 10k agents in seconds.
Review Point: 5. Authors acknowledge the limitation of lack of JAX support and GPU-based parallelization tools. In this aspect, I believe the proposed (L)MAPF environments could be created by quite simple modifications of existing environments in Jumanji as the RobotWarehouse ([https://instadeepai.github.io/jumanji/environments/robot_warehouse/](https://instadeepai.github.io/jumanji/environments/robot_warehouse/)).
Review Point: 7. Finally, as a benchmark, it would be nice to include insights, i.e., what are possible future directions of research. Looking at the graph, one might conclude that there is no point in conducting research in MARL+MAPF, given that heuristic approaches perform well in all metrics, except scalability, only for the LMAPF case. [1] Andreychuk, Anton, et al. "Multi-agent pathfinding with continuous time." Artificial Intelligence 305 (2022): 103662. [1] Yakovlev, Konstantin, Anton Andreychuk, and Roni Stern. "Optimal and Bounded Suboptimal Any-Angle Multi-agent Pathfinding." Proceedings of the International Symposium on Combinatorial Search. Vol.
Review Point: 17. 2024. [3] Okumura, Keisuke. "Engineering LaCAM\*: Towards Real-time, Large-scale, and Near-optimal Multi-agent Pathfinding." Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems. 2024. [4] Okumura, Keisuke. "Improving lacam for scalable eventually optimal multi-agent pathfinding." arXiv preprint arXiv:2305.03632 (2023).
==================================================

Focused review:

1. The analysis of why ReLU outperforms x^2 is too superficial to be a contribution. This point is not novel at all. The impact of sparsity induction and ReLU's desirable attributes have been thoroughly examined in prior works (Serra, T., Tjandraatmadja, C., & Ramalingam, S. (2018). Bounding and Counting Linear Regions of Deep Neural Networks. Proceedings of the 35th International Conference on Machine Learning.). Moreover, the comparison of relu(x), x^2, relu(x)^2 in Table 4 is not complete, making it unclear whether relu(x)^2 is better or worse that relu(x), however, which is stated in Section 3.1.
2. This paper lacks comparison with the latest methods. Authors should compare with the following papers:
a. Jha, Nandan Kumar, and Brandon Reagen. "DeepReShape: Redesigning Neural Networks for Efficient Private Inference." arXiv preprint arXiv:2304.10593 (2023).
b. Souvik Kundu, et al. Learning to linearize deep neural networks for secure and efficient private inference. International Conference on Learning Representation, 2023.
c. Kundu, Souvik, et al. "Making Models Shallow Again: Jointly Learning to Reduce Non-Linearity and Depth for Latency-Efficient Private Inference." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
d. Zeng, Wenxuan, et al. "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
e. Zhang, Yuke, et al. "SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
f. Dhyani, Naren, et al. "PriViT: Vision Transformers for Fast Private Inference." arXiv preprint arXiv:2310.04604 (2023).

Review Point: 1. The analysis of why ReLU outperforms x^2 is too superficial to be a contribution. This point is not novel at all. The impact of sparsity induction and ReLU's desirable attributes have been thoroughly examined in prior works (Serra, T., Tjandraatmadja, C., & Ramalingam, S. (2018). Bounding and Counting Linear Regions of Deep Neural Networks. Proceedings of the 35th International Conference on Machine Learning.). Moreover, the comparison of relu(x), x^2, relu(x)^2 in Table 4 is not complete, making it unclear whether relu(x)^2 is better or worse that relu(x), however, which is stated in Section 3.1.
==================================================

Focused review:

1. As stated in the 3rd paragraph, the main motivation is "The existing literature offers limited theoretical exploration of conditional class probabilities-based algorithms for classification in complex scenarios, particularly concerning the cross-entropy loss.” It can be better to show the connection of the findings in this paper (i.e., kernel logistic regression for complex classification scenarios) and existing findings for standard classification scenario.
2. It can be better to show the connection of the three contributions in introduction. In the current version, it is even hard to find if there is some connection them with the title of this paper.
3. The main novelty is to restrict applications into three kinds of scenarios, i.e., long-tailed learning, domain adaptation, and transfer learning, which are called complex classification scenarios. I know all of them, but I don't know why they are put together in this paper (especially for long-tailed learning). Is there any logic to that? Why does the author focus on these three scenarios? The current version feels pieced together.

Review Point: 1. As stated in the 3rd paragraph, the main motivation is "The existing literature offers limited theoretical exploration of conditional class probabilities-based algorithms for classification in complex scenarios, particularly concerning the cross-entropy loss.” It can be better to show the connection of the findings in this paper (i.e., kernel logistic regression for complex classification scenarios) and existing findings for standard classification scenario.
Review Point: 2. It can be better to show the connection of the three contributions in introduction. In the current version, it is even hard to find if there is some connection them with the title of this paper.
Review Point: 3. The main novelty is to restrict applications into three kinds of scenarios, i.e., long-tailed learning, domain adaptation, and transfer learning, which are called complex classification scenarios. I know all of them, but I don't know why they are put together in this paper (especially for long-tailed learning). Is there any logic to that? Why does the author focus on these three scenarios? The current version feels pieced together.
==================================================

Focused review:

- Since BABEL has a limited vocabulary, DART's vocabulary is limited.
- Since motion generation in DART is autoregressive and real-time, I am wondering what the model would do if the text does not change but the model is continuously rolled out. Would the motion be repeated, or would the model be stuck in a weird space? Provide a few single-text prompt experiments for an extended period (e.g., several minutes) and analyze the resulting motion would be helpful.

Review Point: - Since BABEL has a limited vocabulary, DART's vocabulary is limited.
Review Point: - Since motion generation in DART is autoregressive and real-time, I am wondering what the model would do if the text does not change but the model is continuously rolled out. Would the motion be repeated, or would the model be stuck in a weird space? Provide a few single-text prompt experiments for an extended period (e.g., several minutes) and analyze the resulting motion would be helpful.
==================================================

Focused review:

1. It would have been interesting to discuss the connection of task complexity results under linear classification to Wu et al. [1]. In addition, briefly mentioning the importance of investigating linear classification instead of regression in the paper would be beneficial.
2. Can you explain why it would be interesting to consider a different signal-to-noise ratio R\tilde at test time? Analyzing the difference between M and N seems to be more interesting.
3. The reviewer personally would like to see consistent empirical observation with simple experiments (such as a simplified version of Raventós et al [2]) but understand this might be out of the scope of this work. Maybe the author can consider mentioning it as a limitation.
[1] How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression? Wu et al. ICLR 2024
[2] Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. Raventós et al. NeurIPS 2023

Review Point: 1. It would have been interesting to discuss the connection of task complexity results under linear classification to Wu et al. [1]. In addition, briefly mentioning the importance of investigating linear classification instead of regression in the paper would be beneficial.
Review Point: 2. Can you explain why it would be interesting to consider a different signal-to-noise ratio R\tilde at test time? Analyzing the difference between M and N seems to be more interesting.
Review Point: 3. The reviewer personally would like to see consistent empirical observation with simple experiments (such as a simplified version of Raventós et al [2]) but understand this might be out of the scope of this work. Maybe the author can consider mentioning it as a limitation. [1] How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression? Wu et al. ICLR 2024 [2] Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. Raventós et al. NeurIPS 2023
==================================================

Focused review:

- While the rotation matrix method is effective in mitigating outliers in LLM quantization, it is better if more rigorous explanations/proofs are provided for the rationale behind, besides empirical results.
- Introducing rotation matrices into the inference pipeline may lead to overhead in computing. Could the authors provide analysis the overhead vs benefit from quantization?

Review Point: - While the rotation matrix method is effective in mitigating outliers in LLM quantization, it is better if more rigorous explanations/proofs are provided for the rationale behind, besides empirical results.
Review Point: - Introducing rotation matrices into the inference pipeline may lead to overhead in computing. Could the authors provide analysis the overhead vs benefit from quantization?
==================================================

Focused review:

- Main condition for the paper is training free guidance, all the benchmarks shown where the results stand out, use models which are trained for the purpose guidance.
- Presentation suffers somewhat from being overly compact.

Review Point: - Main condition for the paper is training free guidance, all the benchmarks shown where the results stand out, use models which are trained for the purpose guidance.
==================================================

Focused review:

1.As far as storytelling is concerned, the motivation for using SAE for model interpretability is not clear to me in the abstract and introduction. I can find some clues in Section 3, but in general it is not obvious, especially to readers unfamiliar with the topic.
2.Overall, the readability of the paper needs further improvement.
3.In terms of methodology, SAE has been well studied in interpreting LLMs (see https://arxiv.org/pdf/2406.04093; https://openreview.net/pdf?id=XkMrWOJhNd). I understand that the focus of this work is on VLMs, and while I like the topic discussed in this paper, I have to say that it does not seem very novel to me. Also, I note that gated SAE from Rajamanoharan et al., 2024.
4.The following paper utilizes the idea of sparse learning (not limited to SAE) to interpret VLMs: https://arxiv.org/abs/2402.10376. The authors do not mention this work.
5.Regarding experiments, in my opinion, the current results are not very convincing, mainly because of the choice of competing methods. I understand that VLM-based methods should be main competitors. But the authors should also compare their model with other non-VLM approaches, because there are already many. I believe that the study of model interpretability should be based on the fact that the model can work well. In addition, there are more VLM-based, open-source methods for radiology report generation, e.g., RaDialog and R2GenGPT, which the authors have not considered. Note that many VLM and non-VLM approaches are open-source, and the authors can reimplement them and compute all the same metrics for them.

Review Point: 1.As far as storytelling is concerned, the motivation for using SAE for model interpretability is not clear to me in the abstract and introduction. I can find some clues in Section 3, but in general it is not obvious, especially to readers unfamiliar with the topic.
Review Point: 3.In terms of methodology, SAE has been well studied in interpreting LLMs (see https://arxiv.org/pdf/2406.04093; https://openreview.net/pdf?id=XkMrWOJhNd). I understand that the focus of this work is on VLMs, and while I like the topic discussed in this paper, I have to say that it does not seem very novel to me. Also, I note that gated SAE from Rajamanoharan et al., 2024.
Review Point: 4.The following paper utilizes the idea of sparse learning (not limited to SAE) to interpret VLMs: https://arxiv.org/abs/2402.10376. The authors do not mention this work.
==================================================

Focused review:

- The biggest weakness of the current version of the paper is that the notion of "knowledge orthogonality" is not well articulated enough. There are two aspects. One is about what the notion means. The authors write it "means that the rules within the
benchmark are independent of the domain-specific knowledge the model is exposed to during pre-training" (Lines 76-77). What do you mean by "independent" here? Does it just mean "likely absent"? Could you clarify this central notion more in-depth? The other aspect is whether the five categories satisfy knowledge orthogonality. For example, one could argue that the Puzzle example in Figure 2 is basically the minesweeper game, which is a famous and popular game LLMs know a lot about. I am wondering how you would argue that is "knowledge orthogonal". Moreover, in the Counterfactual category, most of the cover stories are culturally well-known (e.g., Pokemon and Avengers). In what sense are they "independent" of pre-training data?
- Relatedly, in Section 3.2 the authors mention that the raw rules/problems taken from other sources are "adjusted/refined/modified/etc." for this benchmark. How was that process done? What did the authors do to ensure that the end results "meet the specific challenges of KOR-Bench"? It would be good to see more explanations, since this is important given the nature of the proposed benchmark.
- For the evaluation, in terms of prompting strategies this work only studies zero-shot and few-shot settings. We know that prompting matters for LLM reasoning, and there are many methods that can improve reasoning performance (e.g., most notably chain-of-thought/CoT). It would have been a stronger paper if the authors had evaluated the problems with at least one more sophisticated method. Or, if CoT sometimes/often happens automatically and the authors intend that to the case, it should be discussed explicitly in the paper.
- The size of the benchmark is not particularly large, as the authors note. It is still good, and the authors indicate the plan to expand it.
- I think the writing of Section 6 can be improved, especially 6.2 and 6.3. "Self-correction" is introduced rather abruptly with minimal setup/context, and I am not sure that I fully understand what "complex task processing" is about. I recommend that the authors elaborate on those analyses.

Review Point: - Relatedly, in Section 3.2 the authors mention that the raw rules/problems taken from other sources are "adjusted/refined/modified/etc." for this benchmark. How was that process done? What did the authors do to ensure that the end results "meet the specific challenges of KOR-Bench"? It would be good to see more explanations, since this is important given the nature of the proposed benchmark.
Review Point: - For the evaluation, in terms of prompting strategies this work only studies zero-shot and few-shot settings. We know that prompting matters for LLM reasoning, and there are many methods that can improve reasoning performance (e.g., most notably chain-of-thought/CoT). It would have been a stronger paper if the authors had evaluated the problems with at least one more sophisticated method. Or, if CoT sometimes/often happens automatically and the authors intend that to the case, it should be discussed explicitly in the paper.
Review Point: - The size of the benchmark is not particularly large, as the authors note. It is still good, and the authors indicate the plan to expand it.
Review Point: - I think the writing of Section 6 can be improved, especially 6.2 and 6.3. "Self-correction" is introduced rather abruptly with minimal setup/context, and I am not sure that I fully understand what "complex task processing" is about. I recommend that the authors elaborate on those analyses.
==================================================

Focused review:

1. The main contribution of this paper is introducing a variant of CFG into SiD, i.e., long and short CFGs, which aim to balance semantic alignment and the quality of generated images. However, there seems to be a contradiction, as no single $\tau$ value optimally balances both CLIP and FID scores (see results in Tables 1 and 2). Are there intrinsic reasons for this phenomenon?
2. A more intuitive ablation study would better showcase the effectiveness of the proposed long and short CFG strategies. Specifically, in Tables 1 and 2, rather than using SiD-LSG alone, could you provide the results (both CLIP and FID) for SiD with the same $\tau$ value across different configurations: SiD (short CFG only), SiD (long CFG only), SiD (CFG only), and SiD (original)?

Review Point: 1. The main contribution of this paper is introducing a variant of CFG into SiD, i.e., long and short CFGs, which aim to balance semantic alignment and the quality of generated images. However, there seems to be a contradiction, as no single $\tau$ value optimally balances both CLIP and FID scores (see results in Tables 1 and 2). Are there intrinsic reasons for this phenomenon?
Review Point: 2. A more intuitive ablation study would better showcase the effectiveness of the proposed long and short CFG strategies. Specifically, in Tables 1 and 2, rather than using SiD-LSG alone, could you provide the results (both CLIP and FID) for SiD with the same $\tau$ value across different configurations: SiD (short CFG only), SiD (long CFG only), SiD (CFG only), and SiD (original)?
==================================================

Focused review:

The authors should have discussed in more detail the pros and cons for the different HPO strategies. In the following some ideas what could be discussed or my opinion on each of the strategies:
- first-task HPO: many CL methods, e.g., DER++, have hyperparameters that only matter if there is more than one task. how can you tune them in this case?
- current-task HPO: this is useless since it does not allow to measure for important metrics such as forgetting.
- seen-tasks HPO: this increases the required memory size since this is data we cannot train on. some scenarios don't allow for any memory. is it better to use the additional data for HPO or for training?
Besides the clear disadvantages of the methods discussed above, we don't observe this empirically.
The authors imply that we care about "end-of-training" performance. They completely overlook that we actually care about anytime performance and that the concept of "end" doesn't exist in CL. It would be great if the authors would show how performance changes over time. Besides this fact, it's a common plot in many CL papers as well as a nice ablation study.
What about other metrics such as forgetting? As I've mentioned above, some HPO strategies would completely ignore it and we'd assume to see this in this metric.
The authors argue that all tested methods perform equally and therefore "first-task HPO" should be preferred since it is computationally cheap. In my opinion such a recommendation should not be made. The authors should make clear when exactly this choice is justified.

Review Point: - first-task HPO: many CL methods, e.g., DER++, have hyperparameters that only matter if there is more than one task. how can you tune them in this case?
Review Point: - current-task HPO: this is useless since it does not allow to measure for important metrics such as forgetting.
==================================================

Focused review:

1. Some details are not very clear:
- In the second paragraph of the introduction, the author uses the word "Thus," but the reasoning relationship is not evident.
- Figure 2 could be improved for better aesthetics.
- In the method section, the term "gap labels" should be clarified from the beginning, as it might cause confusion for readers.
- In section 6.1, it is not clear on which test set the comparison of reranking results and the output of Riken&Tohoku is based.
2. It will be more convincing to conduct a human evaluation experiment on the system combination results.
3. Reporting the time taken for system combination will be beneficial. Is the proposed method faster than previous work?

Review Point: - In the second paragraph of the introduction, the author uses the word "Thus," but the reasoning relationship is not evident.
Review Point: - In the method section, the term "gap labels" should be clarified from the beginning, as it might cause confusion for readers.
Review Point: - In section 6.1, it is not clear on which test set the comparison of reranking results and the output of Riken&Tohoku is based.
Review Point: 2. It will be more convincing to conduct a human evaluation experiment on the system combination results.
Review Point: 3. Reporting the time taken for system combination will be beneficial. Is the proposed method faster than previous work?
==================================================

Focused review:

1. The method doesn't compare to closely related flow-based optimal transport methods, such as Rectified Flow (Flow straight and fast: Learning to generate and transfer data with rectified flow, ICLR-2023) and Flow Matching (Flow Matching for Generative Modeling, ICLR-2023). I suggest the authors compare with these methods as well.
2. The paper lacks a visual comparison for image-to-image translation problems between different methods and a discussion of why competing methods perform worse. It is not clear why the proposed method achieves better numerical results when it has similar visual results to competitors.
3. It is not clear how well the method computes optimal transport in high dimensions. I suggest that the authors evaluate their method on the Wasserstein-2 benchmark (Do neural optimal transport solvers work? A continuous Wasserstein-2 benchmark, NeurIPS-2021).

Review Point: 1. The method doesn't compare to closely related flow-based optimal transport methods, such as Rectified Flow (Flow straight and fast: Learning to generate and transfer data with rectified flow, ICLR-2023) and Flow Matching (Flow Matching for Generative Modeling, ICLR-2023). I suggest the authors compare with these methods as well.
Review Point: 2. The paper lacks a visual comparison for image-to-image translation problems between different methods and a discussion of why competing methods perform worse. It is not clear why the proposed method achieves better numerical results when it has similar visual results to competitors.
Review Point: 3. It is not clear how well the method computes optimal transport in high dimensions. I suggest that the authors evaluate their method on the Wasserstein-2 benchmark (Do neural optimal transport solvers work? A continuous Wasserstein-2 benchmark, NeurIPS-2021).
==================================================

Focused review:

- Limited and unconvincing experimental evaluation:
- The experimental evaluation relies exclusively on simple, toy environments that fail to capture the complexity and challenges of real-world applications, especially the line and Grid tasks with small scale. For example, the authors can consider the more complex and challenging molecule generation task and biological sequence design tasks from (Bengio et al., 2021) and (Jain et al., 2022) with high-dimensional and complex spaces.
- The authors have not demonstrated the method's effectiveness in scenarios where exploration is genuinely challenging or where reward signals are naturally rare
- The results exhibit high variance (column 2, row 2 in column 3, rows 1-2 in column 4, rows 1-2 in column 5 in Fig. 4) and L1 loss values (all figures in Fig. 4), contradicting the paper's claims about robustness
- Insufficient empirical evidence for claims:
- The paper claims that MetaGFN "is the most robust" (line 375) variant, yet this assertion is questionable given the high L1 loss values and large variances shown in the results (Fig. 4)
- Scalability concerns:
- No evaluation in high-dimensional spaces, which is crucial for real-world applications
- Missing discussion of potential limitations in high-dimensional settings
- Novelty concerns:
- The paper primarily combines two existing methods (metadynamics and GFlowNets), but the experimental results do not convincingly demonstrate advantages over simpler alternatives, and the practical utility of the method remains uncertain given the limited evaluation. Therefore, the contribution seems incremental without strong empirical support

Review Point: - The experimental evaluation relies exclusively on simple, toy environments that fail to capture the complexity and challenges of real-world applications, especially the line and Grid tasks with small scale. For example, the authors can consider the more complex and challenging molecule generation task and biological sequence design tasks from (Bengio et al., 2021) and (Jain et al., 2022) with high-dimensional and complex spaces.
Review Point: - The authors have not demonstrated the method's effectiveness in scenarios where exploration is genuinely challenging or where reward signals are naturally rare - The results exhibit high variance (column 2, row 2 in column 3, rows 1-2 in column 4, rows 1-2 in column 5 in Fig. 4) and L1 loss values (all figures in Fig. 4), contradicting the paper's claims about robustness - Insufficient empirical evidence for claims:
Review Point: - The paper claims that MetaGFN "is the most robust" (line 375) variant, yet this assertion is questionable given the high L1 loss values and large variances shown in the results (Fig. 4) - Scalability concerns:
Review Point: - No evaluation in high-dimensional spaces, which is crucial for real-world applications - Missing discussion of potential limitations in high-dimensional settings - Novelty concerns:
Review Point: - The paper primarily combines two existing methods (metadynamics and GFlowNets), but the experimental results do not convincingly demonstrate advantages over simpler alternatives, and the practical utility of the method remains uncertain given the limited evaluation. Therefore, the contribution seems incremental without strong empirical support
==================================================

Focused review:

* My main concern about the paper is its lack of clarity in writing. The paper seems to focus a little too much on technical details and verbose explanation, making it hard to follow the main points sometimes. For example, lines 371 to 376 briefly mention joint tuning and sequential tuning, which are minor technical details of fine-tuning choices and do not largely contribute to the main point being discussed in the paragraph which is about input order sensitivity in cross-subject transfer. It might be better to show one type of tuning and leave the other in the Appendix to maintain the flow of writing, which would also help make the plots in Figure 4 cleaner with key takeaways only. Another example is Figure 3C to 3E, where it might be better to only show the 1.5hr, 200hr and 2khr models to avoid cluttering in the plots while still convey the main points.
* Cross-subject transfer which is the main use case of neuroscience pretrained models does not seem to be promising with the proposed model. The usefulness of such foundation model to the community therefore might be limited.

Review Point: * Cross-subject transfer which is the main use case of neuroscience pretrained models does not seem to be promising with the proposed model. The usefulness of such foundation model to the community therefore might be limited.
==================================================

Focused review:

: the evaluation section has 2 experiments, but only 2 very insightful detailed examples. The paper can use a few more examples to illustrate more differences of the output sequences. This would allow the reader to internalize how the non-monotonicity in a deeper way.
Questions: In details, how does the decoding algorithm actually avoid repetitions? In other way, how does other models actually degrade validation perplexity using their decoding algorithm?
Typos, Grammar, etc.: Page 7, section 4.2, par. 2: the callout to table 5 should go to table 3, instead. Page 7, section 5, last par.: figure 6 callout is not directing properly

Review Point: 2: the callout to table 5 should go to table 3, instead. Page 7, section 5, last par.: figure 6 callout is not directing properly
==================================================

Focused review:

1. One of the main concern is the motivation of the method, which I think is not well explained. I do not understand why the diffusion processes can synthesize OOD samples without enough and well-defined prior knowledge of the out-of-distribution.
2. The methodology to adjust shifts via hyperparameters, such as λ. However, it lacks a thorough discussion on selecting these parameters across different datasets, impacting reproducibility and usability.
3. The discussions on graph OOD are very limited, especially the Invariant Graph Learning part. The authors might consider referring to the survey [1] or the other surveys for introducing more related works and comparing the differences.
[1] Li, et al. Out-Of-Distribution Generalization on Graphs: A Survey. https://arxiv.org/pdf/2202.07987
I am happy to increase my score if the concerns can be fully addressed.

Review Point: 1. One of the main concern is the motivation of the method, which I think is not well explained. I do not understand why the diffusion processes can synthesize OOD samples without enough and well-defined prior knowledge of the out-of-distribution.
Review Point: 2. The methodology to adjust shifts via hyperparameters, such as λ. However, it lacks a thorough discussion on selecting these parameters across different datasets, impacting reproducibility and usability.
Review Point: 3. The discussions on graph OOD are very limited, especially the Invariant Graph Learning part. The authors might consider referring to the survey [1] or the other surveys for introducing more related works and comparing the differences. [1] Li, et al. Out-Of-Distribution Generalization on Graphs: A Survey. https://arxiv.org/pdf/2202.07987 I am happy to increase my score if the concerns can be fully addressed.
==================================================

Focused review:

* Although I understand building helpful-only LLMs is meaningful, the method proposed in this paper does not seem to be a reasonable solution, since it inevitably makes the LLM less safe. As shown in Table 4, the rejection rate decreases on toxic queries, which is undesirable. If we really need a helpful LLM and do not need to consider the security issue, instead of tuning the LLM after safety alignment, a reasonable alternative may be simply excluding safety-related data from the alignment stage. Without aligning LLM with safety-related data, the LLM should be naturally "helpful". Therefore the proposed method has limited technical contribution.
* The helpful-only LLM is only evaluated on Arena-Hard (for LLMs' general capability), which cannot form a comprehensive evaluation.

Review Point: * The helpful-only LLM is only evaluated on Arena-Hard (for LLMs' general capability), which cannot form a comprehensive evaluation.
==================================================

Focused review:

1.	It is not clear why focusing on using TV instead of other metrics like Wasserstein distance for evaluating the fidelity of the generated data. For two Gaussians, Wasserstein distance has closed form solution. The paper only listed $f$-divergence in the related work.
2.	The experimental results are very limited. Is it possible to add some natural image data results, such as CIFAR 10?
3.	It is not clear how to effectively select the appropriate hypothesis class and classifier in practice to achieve accurate TV distance estimation.
4.	On page 3 below eq. (3), the authors mentioned that “Intuitively, if none of classifies yields a large lower bound, then the synthetic data can be considered similar to the real data, indicating that their total variation distance is small.” I feel that it is not true, since each classifier only offers a lower bound. Even if all lower bounds are small, it doesn’t guarantee that the TV is small.

Review Point: 1. It is not clear why focusing on using TV instead of other metrics like Wasserstein distance for evaluating the fidelity of the generated data. For two Gaussians, Wasserstein distance has closed form solution. The paper only listed $f$-divergence in the related work.
Review Point: 2. The experimental results are very limited. Is it possible to add some natural image data results, such as CIFAR 10?
Review Point: 3. It is not clear how to effectively select the appropriate hypothesis class and classifier in practice to achieve accurate TV distance estimation.
Review Point: 4. On page 3 below eq. (3), the authors mentioned that “Intuitively, if none of classifies yields a large lower bound, then the synthetic data can be considered similar to the real data, indicating that their total variation distance is small.” I feel that it is not true, since each classifier only offers a lower bound. Even if all lower bounds are small, it doesn’t guarantee that the TV is small.
==================================================

Focused review:

- The main limitation of this work is found in the assumption made in Proposition 1, where the authors presume the existence of the inverse of $P^{\pi}(s,s')$. This assumption effectively implies that any policy $\pi$ is reversible within the environment, meaning the agent following such a policy can always undo its actions. This is a very strong assumption; for example, in an environment where an agent can break a vase, this assumption suggests the agent can also reverse this action and restore the vase to its original state. This assumption is not highlighted enough and significantly narrows the scope of this work in comparison to broader decision-making settings. I recommend the authors explicitly state this assumption and discuss its implications.
- The practical utility of the proposed metrics is unclear. The 3D plots (Fig 3 and 4) are challenging to interpret. I suggest the authors present the same findings using separate 2D plots—one for the distance to optimal and another for the stepwise-distance.

Review Point: - The practical utility of the proposed metrics is unclear. The 3D plots (Fig 3 and 4) are challenging to interpret. I suggest the authors present the same findings using separate 2D plots—one for the distance to optimal and another for the stepwise-distance.
==================================================

Focused review:

Since the paper primarily analyzes the relationship between the diffusion latents, generated samples/ data points, and the reverse DDIM latent without any methodological contributions, I would expect the experiments section to be more detailed and clear. More specifically, the following observations stand out:
1. **Missing experimental details**: For instance, the image resolution at which the models were trained is missing for all datasets. Similarly, details on the network architecture used for the diffusion denoiser and training hyperparameters are missing for both pixel space diffusion models and LDMs. Moreover, it is unclear from the text how the angles between different vectors were computed in Figure 2. While these are only a few examples, I would request the authors include all experimental details in the Appendix.
2. **Limited experiments and overclaiming**: Firstly, if I understand correctly, Sections 4.2 and 4.3 reaffirm already existing conclusions about the DDIM Inversion procedure (as the authors note in lines 172-173) using a different experimental methodology. In this context, can the authors point out additional insights that can be drawn from these experiments? Secondly, the authors note the following in Line 175: We study the implications of this fact and show its far-reaching consequences. However, it is not clear from the main text what these implications are, as these are never discussed and, therefore, seem like overclaiming. Can the authors discuss this in detail? I would have liked to see the impact this can have on the DDIM inversion-based editing or reconstruction capabilities, which would justify this claim. Lastly, I don't see any results on a large-scale experiment (say ImageNet-256), and it is unclear how severe this problem is at scale. Can the authors comment on this and include relevant experiments?
3. While the experiments in Section 4.4 are interesting, what is their significance in the context of the broader picture of the paper? If I understand correctly, the focus of the main text is to highlight issues with DDIM inversion by analyzing the relationship between the inverted latents, the original latents, and the generated samples, and therefore it is not clear how Section 4.4 fits here since there seems to be no reference to DDIM inversion here. Secondly, in Section 4.4, what is the minimum L2 distance criterion for the assignment of images to noise mentioned in line 321?
**Minor Comments**
1. The introduction can be improved. For instance, the authors note the following: ```
Nevertheless, one of the significant drawbacks that distinguishes diffusion-based approaches from other generative models like Variational Autoencoders (Kingma & Welling, 2014), Flows (Kingma & Dhariwal, 2018), or Generative Adversarial Networks (Goodfellow et al., 2014) is the lack of implicit latent space that encodes training data into low-dimensional, interpretable representations. ```
While GANs and VAEs indeed are designed to assign low-dimensional latent codes to the data, Flows/Continuous flows also do not possess a low-dimensional latent space and are similar to diffusion models in that aspect. In fact, the ODE sampling in diffusion models is equivalent to simulating a continuous normalizing flow with a vector field defined in terms of the score function. Therefore, this claim is misleading, and it would be great if the authors could revise this in the main text.
2. **Missing citations**: Reference to related work is missing in some places. For instance, in line 37, `combining diffusion models with additional external models`, references to several related works are missing [1,2]
[1] DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents, Pandey et al.
[2] Score-based Generative Modeling in Latent Space, Vahdat et al.
3. Figure 1c: There is a single latent $\hat{x}_T$ for a panel of 4 images, and it is thus confusing. Could the authors clarify which image in this panel the generated latent corresponds to?
4. Table 1: What does each row correspond to? Does it denote the correlation of the pixels in the latent (pure Gaussian noise or reversed DDIM) vector vs data samples?

Review Point: 1. **Missing experimental details**: For instance, the image resolution at which the models were trained is missing for all datasets. Similarly, details on the network architecture used for the diffusion denoiser and training hyperparameters are missing for both pixel space diffusion models and LDMs. Moreover, it is unclear from the text how the angles between different vectors were computed in Figure 2. While these are only a few examples, I would request the authors include all experimental details in the Appendix.
Review Point: 2. **Missing citations**: Reference to related work is missing in some places. For instance, in line 37, `combining diffusion models with additional external models`, references to several related works are missing [1,2] [1] DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents, Pandey et al. [2] Score-based Generative Modeling in Latent Space, Vahdat et al.
Review Point: 3. Figure 1c: There is a single latent $\hat{x}_T$ for a panel of 4 images, and it is thus confusing. Could the authors clarify which image in this panel the generated latent corresponds to?
Review Point: 4. Table 1: What does each row correspond to? Does it denote the correlation of the pixels in the latent (pure Gaussian noise or reversed DDIM) vector vs data samples?
==================================================

Focused review:

1.	The insight of this paper seems to be on shaky ground. Figures 1 and 4-12 do not clearly show the statistical difference between anomalous graphs and normal graphs with respect to Rayleigh Quotient. Detailed text description about the ''significant disparity" between two classes is necessary but not found on the current version. (This is the main reason why I currently tend to reject this paper.)
2.	I recommend briefly introducing Rayleigh Quotient in the introduction or preliminaries section.
3.	In page 4, authors wrote "If the graph Laplacian $\mathbf L$ and graph signal $\mathbf x$ of two graphs are close, then their Rayleigh Quotients will be close to each other and these two graphs will highly likely belong to the same class." I tend to think this statement is correct, but it seems not applicable for anomalies. Anything that is different from normal can be regarded as an anomaly, but we actually cannot get training data that can represent the full picture of anomalies. In addition, two anomalous graphs can also be very different. This paper uses two-class datasets for experiments, but what will be the result if we regard the third class that has never appeared in the training set as anomalies to test the proposed model?
4.	In page 5, authors wrote "However, as analyzed in Section 3.1, to capture the spectral properties of anomalous graphs, it is necessary to consider the spectral energy with respect to each eigenvalue." But, the reason for using graph wavelet convolution is still unclear.

Review Point: 1. The insight of this paper seems to be on shaky ground. Figures 1 and 4-12 do not clearly show the statistical difference between anomalous graphs and normal graphs with respect to Rayleigh Quotient. Detailed text description about the ''significant disparity" between two classes is necessary but not found on the current version. (This is the main reason why I currently tend to reject this paper.) 2. I recommend briefly introducing Rayleigh Quotient in the introduction or preliminaries section.
Review Point: 4. In page 5, authors wrote "However, as analyzed in Section 3.1, to capture the spectral properties of anomalous graphs, it is necessary to consider the spectral energy with respect to each eigenvalue." But, the reason for using graph wavelet convolution is still unclear.
==================================================

Focused review:

- The algorithm description and training procedure was not clear from the main body of the paper. - The problem statement could be more clear (see my notes later regarding section 2.1). - The presentation of the empirical results could be improved.

Review Point: - The algorithm description and training procedure was not clear from the main body of the paper.
Review Point: - The problem statement could be more clear (see my notes later regarding section 2.1).
Review Point: - The presentation of the empirical results could be improved.
==================================================

Focused review:

1. The novelty of this paper is limited.
2. The results of tiny/small/base-sized models are given. Will the model design still work on large-sized models?
3. As the authors claim that ConvNets can offer a hardware-friendly solution compared to ViTs, could you show some advantages (like inference speed or memory usage on specific devices) owned by ConvNeSt compared to ViTs. What is more, the comparison with ConvNeXt should also be given.

Review Point: 2. The results of tiny/small/base-sized models are given. Will the model design still work on large-sized models?
Review Point: 3. As the authors claim that ConvNets can offer a hardware-friendly solution compared to ViTs, could you show some advantages (like inference speed or memory usage on specific devices) owned by ConvNeSt compared to ViTs. What is more, the comparison with ConvNeXt should also be given.
==================================================

Focused review:

1. The related work should include the introduction of the concept removal methods, such as the Safe Latent Diffusion mentioned in the paper.
2. In the concept extraction stage, the selection/ generation of the prompt pairs, which are semantically similar but different from the target concept, is not clearly specified. Producing high-quality prompt pairs requires extensive specialized knowledge. This can affect the effectiveness of the algorithm and increase the difficulty of reproduction.
3. The generation of p ̃_cont is simply by a linear combination of the embedding of P and extracted empirical representation c ̂, which needs further justification. The definition of ‘target prompt P’ is not specified.
4. The ablation study is not properly implemented. For example, it might be better to demonstrate the performance of the algorithm with and without discrete optimization.
5. The algorithm strongly emphasizes that the text encoder is the CLIP model. It might be better to test on other text encoders.

Review Point: 1. The related work should include the introduction of the concept removal methods, such as the Safe Latent Diffusion mentioned in the paper.
Review Point: 2. In the concept extraction stage, the selection/ generation of the prompt pairs, which are semantically similar but different from the target concept, is not clearly specified. Producing high-quality prompt pairs requires extensive specialized knowledge. This can affect the effectiveness of the algorithm and increase the difficulty of reproduction.
Review Point: 3. The generation of p ̃_cont is simply by a linear combination of the embedding of P and extracted empirical representation c ̂, which needs further justification. The definition of ‘target prompt P’ is not specified.
Review Point: 4. The ablation study is not properly implemented. For example, it might be better to demonstrate the performance of the algorithm with and without discrete optimization.
Review Point: 5. The algorithm strongly emphasizes that the text encoder is the CLIP model. It might be better to test on other text encoders.
==================================================

Focused review:

- One main weakness is the fact that a big part of the paper is devoted to reproducing experiments from [1]. In these experiments, the only significant difference is the fact that they report both accuracy and cross entropy loss.
- I fully agree with the author’s comment on section 3 regarding tSNE maps: “While this visualization may not accurately represent the intricate inherent structure of high-dimensional features, it aims to gain insights into the clustering and distribution of data points, thereby enhancing our understanding of the model’s internal representations.” While tSNE maps might give intuition about a phenomenon, they can also fabricate patterns and might not be sufficient evidence for a proposition.
- The writing is sometimes confusing. For instance, in section 3.3: “we calculate the prediction accuracy Kp of noisy labelled data of the original clean labels for each noisy labeled data point matching the label prediction of its k-nearest neighbors in the learned feature space”.
Thus, I do not see substantial empirical nor theoretical contributions in this work.
[1] DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT Nakkiran et al.

Review Point: - One main weakness is the fact that a big part of the paper is devoted to reproducing experiments from [1]. In these experiments, the only significant difference is the fact that they report both accuracy and cross entropy loss.
Review Point: - I fully agree with the author’s comment on section 3 regarding tSNE maps: “While this visualization may not accurately represent the intricate inherent structure of high-dimensional features, it aims to gain insights into the clustering and distribution of data points, thereby enhancing our understanding of the model’s internal representations.” While tSNE maps might give intuition about a phenomenon, they can also fabricate patterns and might not be sufficient evidence for a proposition.
Review Point: - The writing is sometimes confusing. For instance, in section 3.3: “we calculate the prediction accuracy Kp of noisy labelled data of the original clean labels for each noisy labeled data point matching the label prediction of its k-nearest neighbors in the learned feature space”. Thus, I do not see substantial empirical nor theoretical contributions in this work. [1] DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT Nakkiran et al.
==================================================

Focused review:

The main weakness of the paper is the lack of analysis on what makes the method works. This method is relatively complex, with three different components, and there are several natural ablations that have not been done. While it is understandable that there are API costs to contend with, even using open source models like Llama with synthetically assigned costs for ablations could have been done. However, I still believe that the paper should be accepted.
Given that all related papers that are not speculative decoding are too recent for the authors to take into account (around/after ICLR deadline), it is fine not to have external baselines to compare to (speculative decoding provides better latency, but does not have the same cost improvements, afaik). However, some ablations on the method would be useful:
- How much does the search space pruning help in practice, versus just ordering the APIs in increasing order of cost? (vs just anecdotes shown in the experimental section)
- How much changing the threshold T effected results? Is the method very sensitive to T? This would be useful to practitioners.
Some of the design space is indeed constrained by only having access to the output (see comments on scorer in Questions section). I believe writing this in a discussion somewhere in the draft would be helpful in answering questions a reader would have on a first reading.

Review Point: - How much does the search space pruning help in practice, versus just ordering the APIs in increasing order of cost? (vs just anecdotes shown in the experimental section) - How much changing the threshold T effected results? Is the method very sensitive to T? This would be useful to practitioners. Some of the design space is indeed constrained by only having access to the output (see comments on scorer in Questions section). I believe writing this in a discussion somewhere in the draft would be helpful in answering questions a reader would have on a first reading.
==================================================

Focused review:

1. **Limited Generalization**: SLOWFAST-VGEN’s performance may drop in scenarios outside its training domains, such as novel environments or complex, unforeseen actions, limiting its adaptability in dynamic real-world applications.
2. **Memory Inconsistency Over Long Sequences**: TEMP-LORA effectively stores episodic memory within single episodes but struggles with consistency across multi-episode or long-term tasks, potentially leading to fragmented recall in extended sequences. Since the authors mention that the slowing learning for world modeling can capture general dynamics, some long-horizon tasks need to be evaluated, such as moving to the right and then to the left.
3. **High Computational Cost**: The reliance on a diffusion model for video generation demands significant computational resources. Although using LoRA, the two-stage training. What about latent diffusion models for the slow learning, while video diffusion models for the fast learning?

Review Point: 1. **Limited Generalization**: SLOWFAST-VGEN’s performance may drop in scenarios outside its training domains, such as novel environments or complex, unforeseen actions, limiting its adaptability in dynamic real-world applications.
Review Point: 2. **Memory Inconsistency Over Long Sequences**: TEMP-LORA effectively stores episodic memory within single episodes but struggles with consistency across multi-episode or long-term tasks, potentially leading to fragmented recall in extended sequences. Since the authors mention that the slowing learning for world modeling can capture general dynamics, some long-horizon tasks need to be evaluated, such as moving to the right and then to the left.
Review Point: 3. **High Computational Cost**: The reliance on a diffusion model for video generation demands significant computational resources. Although using LoRA, the two-stage training. What about latent diffusion models for the slow learning, while video diffusion models for the fast learning?
==================================================

Focused review:

Overall the method seems very ad-hoc: While the use of a pre-trained model as an encoder that generates a fused feature map for the classifier is a nice idea, the other two contributions of the paper seem very ad-hoc. The structured-distillation loss seems like a cumbersome way of enforcing knowledge-distillation, and it is not clear if the so-called point-wise knowledge-distillation on more points would achieve similar or better results than structured distillation. The overall training objective (Eq. 4) is the combination of everything, experience-replay, point-wise knowledge distillation, and structured distillation and it is not clear if everything is needed in that objective. Second, the SP-normalization seems even more ad-hoc where half of the features are passed through BatchNorm layers, and the other half are sent to a combination of GroupNorm and InstanceNorm, without any criterion. If BatchNorm is bad for continual learning then there would be excessive forgetting through the half that was sent to the batchnorm. As for Group or InstanceNorm, if the features drift from one task to the next, how well these normalization techniques would be able to cope with forgetting. Overall, it seems that in designing SP-normalization the whole kitchen sync is thrown at the features.
Writing is not clear: The writing of the paper is not clear and makes the paper a difficult read. The tuple notation for denoting data samples is incredibly cumbersome and non-intuitive. Sec 3.2 is not clear at all, terms are used without any definitions. For example, the functional form of the so-called structure-wise potential ψ
is not provided until the end of the section making it difficult to understand what is happening in Eq. 2. Similarly, throughout Sec 3, intuitions are lacking and it seems that everything is combined in an ad-hoc way. What the authors refer to as knowledge-distillation based on ER, is simple experience replay, there is no knowledge-distillation there. Experiments:
Do the baselines, GEM, ER, MER etc also use the pre-trained model? This is not clear from the experiments.
Do all the baselines use the same codebase?
Could the authors compare the compute and memory cost of their method vs the baselines?

Review Point: 2. Similarly, throughout Sec 3, intuitions are lacking and it seems that everything is combined in an ad-hoc way. What the authors refer to as knowledge-distillation based on ER, is simple experience replay, there is no knowledge-distillation there. Experiments: Do the baselines, GEM, ER, MER etc also use the pre-trained model? This is not clear from the experiments. Do all the baselines use the same codebase? Could the authors compare the compute and memory cost of their method vs the baselines?
==================================================

Focused review:

1. The novelty of this paper is limited. As mentioned in the summary, two existing momentum-based techniques are additively combined with the modification to adjust previous iterate time step parameter. In my opinion, to call the proposed formulation generalized heavy-ball momentum is somewhat a far-stretch.
2. Some theoretical analysis not provided. Although averaging has been applied, it is not clear why having this gap or "window" is desirable in general and particularly in the federated learning setting. Motivation/intuition needs to be given.
3. Not clear if the results are reproducible since code is not given.
4. Missing some related work:
* Xin, R. and Khan, U. Distributed heavy-ball: a generalization and acceleration of first-order methods with grading tracking. 2018.
* Das, R. et al. Faster non-convex federated learning via global and local momentum. 2022.
* Kim, G. et al. Communication-Efficient Federated Learning with Acceleration of Global Momentum. 2022.
Additional: labels missing on graphs.

Review Point: 1. The novelty of this paper is limited. As mentioned in the summary, two existing momentum-based techniques are additively combined with the modification to adjust previous iterate time step parameter. In my opinion, to call the proposed formulation generalized heavy-ball momentum is somewhat a far-stretch.
Review Point: 2. Some theoretical analysis not provided. Although averaging has been applied, it is not clear why having this gap or "window" is desirable in general and particularly in the federated learning setting. Motivation/intuition needs to be given.
Review Point: 3. Not clear if the results are reproducible since code is not given.
Review Point: * Xin, R. and Khan, U. Distributed heavy-ball: a generalization and acceleration of first-order methods with grading tracking. 2018.
Review Point: * Das, R. et al. Faster non-convex federated learning via global and local momentum. 2022.
Review Point: * Kim, G. et al. Communication-Efficient Federated Learning with Acceleration of Global Momentum. 2022. Additional: labels missing on graphs.
==================================================

Focused review:

1. The paper lacks focus. While it starts by discussing the goal as understanding the effect of different position encodings, everything before the evaluation section does not discuss this point sufficiently. On the contrary, the paper presents the connection between MPNNs and GTs as a contribution, despite being well known in the literature (see for instance Velickovic 2023) and not central to the main focus of the paper.
2. Because of the lack of focus, the contributions of the paper related to the main goal are limited. The paper only tests few positional encoding techniques and few graph transformers. More importantly, *there is no discussion on the conclusions of the benchmarking study*: which PE is the best under which assumptions? The authors only discuss which PE is the best based on the dataset. I think it is necessary to have a section discussing when it is best to use a certain PE, based on the characteristics of the dataset (e.g., large graphs, local/global information needed for the prediction), of the task, or of the architecture used. Otherwise the paper does not help practitioners in their choice of PE, it simply lists which one is best for that particular dataset tested, and any variation in the dataset could lead to completely different conclusions.
Velickovic 2023. Everything is Connected: Graph Neural Networks

Review Point: 1. The paper lacks focus. While it starts by discussing the goal as understanding the effect of different position encodings, everything before the evaluation section does not discuss this point sufficiently. On the contrary, the paper presents the connection between MPNNs and GTs as a contribution, despite being well known in the literature (see for instance Velickovic 2023) and not central to the main focus of the paper.
==================================================

Focused review:

1.	Limited real-world impact on runtime: Although the method skips significant computations, there is no reported direct improvement in runtime, which reduces its practical appeal (as the authors rightly discuss in the conclusion). Future work should focus on addressing hardware and framework optimizations to convert computational savings into time efficiency.
2.	Data and model-specific application: The approach has been validated primarily on the FastSurfer model, which might limit generalizability. Moreover, the model has been validation only for a single dataset. NaN Pooling and Convolution may not directly transfer to models or tasks where background regions are less prevalent.
3.	Accuracy deviation in certain regions: In regions like the cerebellum, the NaN-modified FastSurfer model showed increased variability where segmentation accuracy slightly declined.
4.	Potential overhead from NaN management: The reliance on CPU-based PyTorch adaptations for NaN management is a limitation, as these are not scalable to GPU-optimized frameworks, potentially hampering applicability to larger datasets or real-time processing needs.
5.	Lack of implementation for 3D convolutions: A large fraction of medical imaging modalities produces 3D images (MRI, CT, SPECT, PET). Most recent works in 3D medical image segmentation have focussed on 3D CNNs since they allow capturing information across all three spatial dimensions, preserving the anatomical context between adjacent slices. This is also evident from many of the recent medical image segmentation challenges (organized by MICCAI), where the winning solutions utilized some version of 3D architectures such as nnUNet [Isensee, et al, Nature Methods 2020], SegResNet [Myronenko, et al, arXiv:2209:10809 (2022)], or SwinUNETR [Hatamizadeh, et al, arXiv:2201.01266v1 (2022)]. This work implements their method only for 2D CNNs which limits their broader applicability for 3D medical image segmentation.
6.	Lack of comparison to other baselines: No comparison were made to other similar methods for medical image segmentation that implement “sparsification” of data for reducing computational costs. Some of these include sparse CNN [Li, et al, 10.36227/techrxiv.19137518.v2], and dictionary learning and sparse coding [Tong, et al, NeuroImage, Vol 76 (2013)].

Review Point: 1. Limited real-world impact on runtime: Although the method skips significant computations, there is no reported direct improvement in runtime, which reduces its practical appeal (as the authors rightly discuss in the conclusion). Future work should focus on addressing hardware and framework optimizations to convert computational savings into time efficiency.
Review Point: 2. Data and model-specific application: The approach has been validated primarily on the FastSurfer model, which might limit generalizability. Moreover, the model has been validation only for a single dataset. NaN Pooling and Convolution may not directly transfer to models or tasks where background regions are less prevalent.
Review Point: 3. Accuracy deviation in certain regions: In regions like the cerebellum, the NaN-modified FastSurfer model showed increased variability where segmentation accuracy slightly declined.
Review Point: 4. Potential overhead from NaN management: The reliance on CPU-based PyTorch adaptations for NaN management is a limitation, as these are not scalable to GPU-optimized frameworks, potentially hampering applicability to larger datasets or real-time processing needs.
Review Point: 6. Lack of comparison to other baselines: No comparison were made to other similar methods for medical image segmentation that implement “sparsification” of data for reducing computational costs. Some of these include sparse CNN [Li, et al, 10.36227/techrxiv.19137518.v2], and dictionary learning and sparse coding [Tong, et al, NeuroImage, Vol 76 (2013)].
==================================================

Focused review:

1.There may be potential biases in the selection of moral foundations and their interpretations.
2.The scope of the ethical values considered may not be comprehensive or universally applicable.
3.It’s unclear how the VILMO method scales or its effectiveness across different LLMs and settings.

Review Point: 1.There may be potential biases in the selection of moral foundations and their interpretations.
Review Point: 2.The scope of the ethical values considered may not be comprehensive or universally applicable.
Review Point: 3.It’s unclear how the VILMO method scales or its effectiveness across different LLMs and settings.
==================================================

Focused review:

1. Insufficient Experimental Results on C&P Conflict Causes and Impact: The paper does not provide extensive experimental results on the root causes of C&P conflicts or a detailed analysis of their impact on downstream tasks. While it demonstrates the existence of these conflicts and the effectiveness of the proposed method in reducing them, a deeper investigation into why these conflicts occur and how they specifically affect performance in various tasks could strengthen the paper's contributions.
2. The applicability of the Multimodal Knowledge Consistency Fine-tuning method is discussed in the context of document understanding but may require further discussion for broader multimodal tasks. The paper could benefit from exploring how well these findings generalize to other domains beyond document understanding, such as scene understanding or visual reasoning.
3. The paper does not explore whether C&P conflicts can be mitigated through simple in-context learning strategies, such as performing VQA followed by OCR based on the previous context, or vice versa. This approach might provide insights into whether the conflicts are resolvable with less complex interventions.
4. The paper treats C&P conflict primarily as a consistency issue. However, it does not provide statistics on cases where the model's output consistency is low due to a failure to follow instructions correctly. Differentiating between consistency issues and instruction-following issues could offer a more nuanced understanding of the conflicts.
5. Ablation Study Metrics: The ablation study in Table 5 focuses on consistency metrics but does not include corresponding accuracy or F1 scores. Providing these metrics would offer a more comprehensive view of the method's performance, especially in terms of the trade-offs between consistency and accuracy.

Review Point: 1. Insufficient Experimental Results on C&P Conflict Causes and Impact: The paper does not provide extensive experimental results on the root causes of C&P conflicts or a detailed analysis of their impact on downstream tasks. While it demonstrates the existence of these conflicts and the effectiveness of the proposed method in reducing them, a deeper investigation into why these conflicts occur and how they specifically affect performance in various tasks could strengthen the paper's contributions.
Review Point: 2. The applicability of the Multimodal Knowledge Consistency Fine-tuning method is discussed in the context of document understanding but may require further discussion for broader multimodal tasks. The paper could benefit from exploring how well these findings generalize to other domains beyond document understanding, such as scene understanding or visual reasoning.
Review Point: 3. The paper does not explore whether C&P conflicts can be mitigated through simple in-context learning strategies, such as performing VQA followed by OCR based on the previous context, or vice versa. This approach might provide insights into whether the conflicts are resolvable with less complex interventions.
Review Point: 4. The paper treats C&P conflict primarily as a consistency issue. However, it does not provide statistics on cases where the model's output consistency is low due to a failure to follow instructions correctly. Differentiating between consistency issues and instruction-following issues could offer a more nuanced understanding of the conflicts.
Review Point: 5. Ablation Study Metrics: The ablation study in Table 5 focuses on consistency metrics but does not include corresponding accuracy or F1 scores. Providing these metrics would offer a more comprehensive view of the method's performance, especially in terms of the trade-offs between consistency and accuracy.
==================================================

Focused review:

- No large scale datasets like imagenet
- The benchmarks are limited to zero shot classification which is an easy task compared to zero-shot semantic segmentation and instance segmentation where this method could struggle.
- I don't see this work as actual zero shot because the pretrained model has so much information about the classes present in the chosen datasets. This work would be more impactful if the experiments were conducted on rare classes to test whether this method generalizes well. ChatGPT has been trained on the internet, so this work is far from zero shot learning unless we include classes that are least likely to be seen by ChatGPT.

Review Point: - No large scale datasets like imagenet - The benchmarks are limited to zero shot classification which is an easy task compared to zero-shot semantic segmentation and instance segmentation where this method could struggle.
Review Point: - I don't see this work as actual zero shot because the pretrained model has so much information about the classes present in the chosen datasets. This work would be more impactful if the experiments were conducted on rare classes to test whether this method generalizes well. ChatGPT has been trained on the internet, so this work is far from zero shot learning unless we include classes that are least likely to be seen by ChatGPT.
==================================================

Focused review:

1. Regarding the technical novelty of this work, a related work was published on ICCAD 2024, named RTLRewriter [1]. Many techniques are similar to each other. RTLRewriter is a multimodal one that considers manual, code, and notes and also incorporates graph partitioning techniques, retrieval-augmented inference, etc.
2. During zero-shot inference, it is not reliable to simply average the performance metrics of retrieved similar circuits to make predictions. This issue seems to be reflected in the experimental results in Table 3, where the performance decreases with the increased available retrievals. Meanwhile, sufficient circuit data is required when building a VectorStore with quality metrics, which contradicts the concept of zero-shot inference.
3. The CircuitFusion implements summary-centric fusion to achieve alignment with information from other modalities. However, this approach may result in an underrepresentation of structural information or code-level details, which could be particularly important in timing, power and area performance. Additional experiments and explanations are needed to demonstrate the necessity of the summary mode.
4. The downstream applications should be further elaborated. Specifically, how to adjust the RTL design based on the obtained performance predictions warrants more detailed discussion. Currently, this work shows less relevance to the title's concept of 'agile chip design', as it does not adequately illustrate how CircuitFusion can be effectively applied to the optimization of RTL code. Prior works: [1] uses the encoded different modal RTL for new code reconstruction. [2] can notate detailed timing information on HDL and set fine-grained optimization options in the synthesis script for optimization applications.
5. The uploaded source code for CircuitFusion lacks readability and is hard to run. For example, many of the files mentioned in the README do not exist (data_bench, pretrain_model, pos and others).
[1] RTLRewriter: Methodologies for Large Models aided RTL Code Optimization [ICCAD 2024](https://arxiv.org/abs/2409.11414)
[2] Annotating Slack Directly on Your Verilog: Fine-Grained RTL Timing Evaluation for Early Optimization (https://arxiv.org/abs/2403.18453)

Review Point: 1. Regarding the technical novelty of this work, a related work was published on ICCAD 2024, named RTLRewriter [1]. Many techniques are similar to each other. RTLRewriter is a multimodal one that considers manual, code, and notes and also incorporates graph partitioning techniques, retrieval-augmented inference, etc.
Review Point: 2. During zero-shot inference, it is not reliable to simply average the performance metrics of retrieved similar circuits to make predictions. This issue seems to be reflected in the experimental results in Table 3, where the performance decreases with the increased available retrievals. Meanwhile, sufficient circuit data is required when building a VectorStore with quality metrics, which contradicts the concept of zero-shot inference.
Review Point: 3. The CircuitFusion implements summary-centric fusion to achieve alignment with information from other modalities. However, this approach may result in an underrepresentation of structural information or code-level details, which could be particularly important in timing, power and area performance. Additional experiments and explanations are needed to demonstrate the necessity of the summary mode.
Review Point: 4. The downstream applications should be further elaborated. Specifically, how to adjust the RTL design based on the obtained performance predictions warrants more detailed discussion. Currently, this work shows less relevance to the title's concept of 'agile chip design', as it does not adequately illustrate how CircuitFusion can be effectively applied to the optimization of RTL code. Prior works: [1] uses the encoded different modal RTL for new code reconstruction. [2] can notate detailed timing information on HDL and set fine-grained optimization options in the synthesis script for optimization applications.
Review Point: 5. The uploaded source code for CircuitFusion lacks readability and is hard to run. For example, many of the files mentioned in the README do not exist (data_bench, pretrain_model, pos and others). [1] RTLRewriter: Methodologies for Large Models aided RTL Code Optimization [ICCAD 2024](https://arxiv.org/abs/2409.11414) [2] Annotating Slack Directly on Your Verilog: Fine-Grained RTL Timing Evaluation for Early Optimization (https://arxiv.org/abs/2403.18453)
==================================================

Focused review:

- Personally, the method is not very innovative.
- I believe the baselines for comparison are not strong enough, particularly when it comes to methods for drug-drug interaction (DDI).
- The paper also does not utilize datasets that are commonly employed in DDI tasks.

Review Point: - I believe the baselines for comparison are not strong enough, particularly when it comes to methods for drug-drug interaction (DDI).
Review Point: - The paper also does not utilize datasets that are commonly employed in DDI tasks.
==================================================

Focused review:

While I find the proposed approach interesting and promising, I find the claims too strong (I would expect a more refined discussion on what type of problems, and what additional assumptions are needed for DSS to perform well) and the experimental validation lacking, and some of the simple baselines likely misrepresented. In general, replacing known robust solutions for some critical domains (e.g. power-flow, aerodynamics simulations for airplanes, e.t.c) by fast approximate solvers is exciting, but also dangerous -- and to what extent you can guarantee that DSS can do well for the more difficult cases? Is there a possibility of catastrophic failures? Are there applications you envision where this risk is reduced? 1) While the authors establish "universality" -- similar to the corresponding results for plain feed-forward NNs, there's no way to ensure that the GNN will actually be able to find the best function in that space. So we have to rely on extensive experimental validation to test it. In particular I imagine that DSS works much better for some types of problems (e.g. easy smoothing-type of problems), and much worse for "frustrated" or ill-conditioned systems, having a very difficult time to match direct solutions. I would like to see some discussion around limitations of the approach, and perhaps additional assumptions that you're implicitly imposing. 2) The case of Poisson equation leading to a simple linear system -- is a nice "sanity check", but given that the paper is addressing a general class of nonlinear systems, PDEs, and optimization problems, I would expect a broader array of test cases. a) The baseline numbers for plain linear-system solvers seem dramatically slow: in my simple tests on random asymmetric dense linear systems using python numpy.linalg.solve for 500x500 matrix, it requires a running time of around 3-4ms , whereas the paper claims LU takes about 2 seconds. (An order of 1000x off!) Are the units correct? In 2 seconds one can solve linear systems of size 5000x5000. b) For iterative solvers, the number of iterations to reach a given level of accuracy depends on the condition number of the system. The GNN message-passing solution is more akin to an iterative solver so probably behaves similarly. So I'd be curious if the fixed number-of-steps approach makes sense, and works for more challenging problems. In particular -- interesting practical distributions of problems will have both high and low condition numbers, so adaptive number of steps (to reach a given tolerance) would make more sense. For example this would hold true for the linear Gaussian belief propagation (a naive version of a GNN). c) Also the Poisson equation has a very small number of unique parameters, and it would be interesting to see if a GNN is able to learn how to solve non-homogeneous versions of linear systems, with many more unique parameters. 3) The paper does have a nonlinear system arising from the Power-flow problem. However, based on particular inputs, power-flow can exhibit a very easy linear-like behavior, but also a very complex nonlinear behavior when the system is under a heavy load / stress. Taking random samples of loads it's likely you're mostly in the easy linear part, which is easy to solve, and hence you get very high correlations. I would like to see how well the DSS approach behaves as you explicitly try to come to the boundary of feasibility -- e.g. by increasing the loads. I would be also interested to look not at correlations, but at the worst-error over the data. A collection of realistic test-cases for power-flow is available in "The Power Grid Library for Benchmarking AC Optimal Power Flow Algorithms", by Carleton Coffrin et.al. 4) Are there any guarantees that the GNN solution will satisfy the standard OPF feasibility constraints? especially in the more difficult cases?

Review Point: 4) Are there any guarantees that the GNN solution will satisfy the standard OPF feasibility constraints? especially in the more difficult cases?
==================================================

Focused review:

* **Non-standard and vague experimental setting**s: This paper claims to use 6 self-generated test cases for each problem to solve. However, I couldn't find any related descriptions or details about how they generate them. The self-generated test cases are neither the contribution of this paper nor part of some public datasets/benchmarks. It is thus hard to reproduce and compare results from this paper with other methods in this domain. It is also unclear how many LLM requests/tokens they use in these experiments.
* **Missing baselines; only simplest baselines**: There are many more papers discussing the search strategy with LLM-based code refinements/generation (exploration v.s. exploitation), such as Fun-Search [1] based on evolution search and REx[2] based on bandits algorithms. There are many more popular hard-coded tree expansion policies in the field such as [3,4].
* **Missing details of the method**: There are many confusing or missing descriptions of the method such as (1) the definition of rewards/values for each action/code; (2) how to initialize Q in UCB/MCTS; (3) how to generate test cases; (4) what is the overall pipeline? how many LLM requests for each "submission"?
* **Complicated method; Unclear contributions of each component**: This paper includes many components in its method including (1) self-generated test cases, (2) itemized COT prompting, (3) UCB-based tree-expansion policy, (4) reusing COTs, (5) "foresting", and so on. It is unclear how each component contributes to the performance. I would suggest the authors perform ablation studies in a more common/standard setting, removing influences of components that are not part of this paper's contributions.
[1] Romera-Paredes, Bernardino, et al. "Mathematical discoveries from program search with large language models." Nature 625.7995 (2024): 468-475.
[2] Tang, Hao, et al. "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff." arXiv preprint arXiv:2405.17503 (2024).
[3] Olausson, Theo X., et al. "Is Self-Repair a Silver Bullet for Code Generation?." The Twelfth International Conference on Learning Representations. 2023.
[4] Wang, Ruocheng, et al. "Hypothesis search: Inductive reasoning with language models." arXiv preprint arXiv:2309.05660 (2023).

Review Point: * **Non-standard and vague experimental setting**s: This paper claims to use 6 self-generated test cases for each problem to solve. However, I couldn't find any related descriptions or details about how they generate them. The self-generated test cases are neither the contribution of this paper nor part of some public datasets/benchmarks. It is thus hard to reproduce and compare results from this paper with other methods in this domain. It is also unclear how many LLM requests/tokens they use in these experiments.
Review Point: * **Missing baselines; only simplest baselines**: There are many more papers discussing the search strategy with LLM-based code refinements/generation (exploration v.s. exploitation), such as Fun-Search [1] based on evolution search and REx[2] based on bandits algorithms. There are many more popular hard-coded tree expansion policies in the field such as [3,4].
Review Point: * **Missing details of the method**: There are many confusing or missing descriptions of the method such as (1) the definition of rewards/values for each action/code; (2) how to initialize Q in UCB/MCTS; (3) how to generate test cases; (4) what is the overall pipeline? how many LLM requests for each "submission"?
==================================================

Focused review:

1. **Novelty of the method**: Using compression to defend or detect adversarial examples is not a new insight in the security field. Some well-known methods relevant to the topic, e.g., [feature distillation](https://arxiv.org/abs/1803.05787), are not compared or even mentioned in this paper.
2. **Poorly structured writing**: I advise adding a background section to clarify the notations instead of placing them in the introduction and across the paper. For the experimental part, the metrics are not well described. It confuses readers to follow the reported results.
3. **Weak experiments**: A too-limited pool of models is investigated in this paper. A ResNet alone is not representative of the method's stability and reliability. What's more, too many dimensions concerning the detection method are not well explored. What about the impact of the empirical threshold? What about the resource cost of the technique? The method is claimed to be a real-time detection, so where is the evidence? I also wonder about the false positive rate of the detection method.

Review Point: 1. **Novelty of the method**: Using compression to defend or detect adversarial examples is not a new insight in the security field. Some well-known methods relevant to the topic, e.g., [feature distillation](https://arxiv.org/abs/1803.05787), are not compared or even mentioned in this paper.
Review Point: 2. **Poorly structured writing**: I advise adding a background section to clarify the notations instead of placing them in the introduction and across the paper. For the experimental part, the metrics are not well described. It confuses readers to follow the reported results.
Review Point: 3. **Weak experiments**: A too-limited pool of models is investigated in this paper. A ResNet alone is not representative of the method's stability and reliability. What's more, too many dimensions concerning the detection method are not well explored. What about the impact of the empirical threshold? What about the resource cost of the technique? The method is claimed to be a real-time detection, so where is the evidence? I also wonder about the false positive rate of the detection method.
==================================================

Focused review:

1. This paper shares similar ideas with Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding (Bhardwaj et al., 2022), in terms of both model architecture and employing k-means algorithm for quantized-prompt learning. How these two works differ is unclear, which limits the novelty of this paper.
2. The author mentioned that the learned prompts could encode syntax-related information, but this claim is not supported well in experiments.
3. The author argues that a limited number of quantized prompts are able to encode the information related to transforming rules for paraphrase generation, which should not be dedicated to a specific dataset. Thus, we have a concern that how well the quantized prompts learned in one dataset or one domain performs in the unseen domains. However, this essential property was not examined.

Review Point: 1. This paper shares similar ideas with Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding (Bhardwaj et al., 2022), in terms of both model architecture and employing k-means algorithm for quantized-prompt learning. How these two works differ is unclear, which limits the novelty of this paper.
Review Point: 2. The author mentioned that the learned prompts could encode syntax-related information, but this claim is not supported well in experiments.
Review Point: 3. The author argues that a limited number of quantized prompts are able to encode the information related to transforming rules for paraphrase generation, which should not be dedicated to a specific dataset. Thus, we have a concern that how well the quantized prompts learned in one dataset or one domain performs in the unseen domains. However, this essential property was not examined.
==================================================

Focused review:

Though I really like the paper at a high-level, there are quite a few significant improvements that can make the paper more mature:
- The environment assumes a single recipe in the game and thus missing an important aspect of the game. There are variants of Overcooked that use multiple recipes that make the game more interesting and require more complex coordination [1,2,3]. I believe including more possible recipes improves the depth of the environment significantly, arguably adding another important layer of generalization.
- line 288: the observation space is not explained in details, it is especially hard for readers without prior Overcooked knowledge.
- It is not clear if the UED agents were just undertrained or the UED algorithms themselves struggle to generate complex layout. I wish the authors include training curves showing the performance on seen and unseen layouts of UED agents.
- The result of per layout generalization performance is missing from the main paper. I believe Table 13 shows this result but is not included in the main paper. I compare Table 13 and Table 14 (which is the raw results of Fig 5) and see that the self-play performance is actually lower than *test cross-play* performance, hinting that the agents are being *carried* by the FCP test partners. I believe that this observation is important and should be included and discussed in the main text.
- FCP does not explicitly generate diverse agents, which might not be a robust method for generating test partners.
Minor comments:
- Random and Stay baselines (Fig 5) are never introduced
- Fig 4 sentence 2: I find the sentence to not be consistent with the figure. Some of the generated layouts are definitely solvable.
- line 54: empty space on the right
- last line of page 2: the citation should not have parentheses as it is a part of a sentence
[1] Wu, Sarah A., et al. "Too many cooks: Bayesian inference for coordinating multi‐agent collaboration." Topics in Cognitive Science 13.2 (2021): 414-432.
[2] Yu, Chao, et al. "Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased." The Eleventh International Conference on Learning Representations.
[3] Charakorn, Rujikorn, Poramate Manoonpong, and Nat Dilokthanakul. "Generating diverse cooperative agents by learning incompatible policies." The Eleventh International Conference on Learning Representations. 2023.

Review Point: - The environment assumes a single recipe in the game and thus missing an important aspect of the game. There are variants of Overcooked that use multiple recipes that make the game more interesting and require more complex coordination [1,2,3]. I believe including more possible recipes improves the depth of the environment significantly, arguably adding another important layer of generalization.
Review Point: - line 288: the observation space is not explained in details, it is especially hard for readers without prior Overcooked knowledge.
Review Point: - It is not clear if the UED agents were just undertrained or the UED algorithms themselves struggle to generate complex layout. I wish the authors include training curves showing the performance on seen and unseen layouts of UED agents.
Review Point: - The result of per layout generalization performance is missing from the main paper. I believe Table 13 shows this result but is not included in the main paper. I compare Table 13 and Table 14 (which is the raw results of Fig 5) and see that the self-play performance is actually lower than *test cross-play* performance, hinting that the agents are being *carried* by the FCP test partners. I believe that this observation is important and should be included and discussed in the main text.
Review Point: - FCP does not explicitly generate diverse agents, which might not be a robust method for generating test partners. Minor comments:
Review Point: - Random and Stay baselines (Fig 5) are never introduced - Fig 4 sentence 2: I find the sentence to not be consistent with the figure. Some of the generated layouts are definitely solvable.
Review Point: - line 54: empty space on the right - last line of page 2: the citation should not have parentheses as it is a part of a sentence [1] Wu, Sarah A., et al. "Too many cooks: Bayesian inference for coordinating multi‐agent collaboration." Topics in Cognitive Science 13.2 (2021): 414-432. [2] Yu, Chao, et al. "Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased." The Eleventh International Conference on Learning Representations. [3] Charakorn, Rujikorn, Poramate Manoonpong, and Nat Dilokthanakul. "Generating diverse cooperative agents by learning incompatible policies." The Eleventh International Conference on Learning Representations. 2023.
==================================================

Focused review:

While I see the contribution favourably, there are major weaknesses
precluding the presentation of the paper in its current form. The
primary one is a **missing analysis of fundamental properties**. While
I appreciate the broad range of different applications, this invariably
means that some depth is lost and analyses are relatively superficial.
Introducing a new measure and then showing its utility requires a more
in-depth view of data, though. For instance, when introducing the
latent representations in Table 1, readers are only shown the actual
scores, but the primary assumption is that the scores actually capture
the relevant properties of the data. Phrased somewhat hyperbolically:
I believe that NSA can be calculated as described, but I do not
understand whether it measures something 'interesting.' The fact that it
correlates with performance metrics is relevant, but immediately
suggests a more detailed comparison scenario, for instance in the form
of an early stopping criterion or regularisation term. Otherwise, most
of the analyses strike me as too speculative. I will provide additional
comments below.
- I'd suggest to shorten the RTD explanation or substantially extend it.
Currently, it makes use of jargon like $\min G(R, Q)$, 'barcode,' and
Vietoris--Rips filtrations that are not sufficiently explained (I am
familiar with the work and I believe that a deep dive into topological
methods is not required).
- To simplify the notation, I'd either use the actual Euclidean norm in
the definition of NSA or write $x$ and $0$ as vectors.
- The autoencoders experiment is somewhat out of place since the
introduction sets up a paper on GNNs. Given the broad scope of NSA,
I think it might be best to stay with the autoencoder comparison,
using data with a known ground truth. This could take the form of
building confidence by starting with simple toy examples like a 'Swiss
Roll' or other data sets and showing that NSA matches the intuition.
Overall, my **main concern** is that the measure is just too coarse, in
particular given large data sets. It essentially amounts to comparing
averaged distance representations, and more in-depth experiments and/or
theoretical analyses would be required here.

Review Point: - I'd suggest to shorten the RTD explanation or substantially extend it. Currently, it makes use of jargon like $\min G(R, Q)$, 'barcode,' and Vietoris--Rips filtrations that are not sufficiently explained (I am familiar with the work and I believe that a deep dive into topological methods is not required).
Review Point: - To simplify the notation, I'd either use the actual Euclidean norm in the definition of NSA or write $x$ and $0$ as vectors.
==================================================

Focused review:

About the contributions:
- First, the authors claim that this work introduces the concept of subject repositioning, but obviously, it is one of the important features released by the Google Photos in May 2023, named Magic Editor in Google Photos (https://blog.google/products/photos/google-photos-magic-editor-pixel-io-2023/).
- Second, the proposed framework, namely SEELE, combines many existing techniques, and can be considered as an engineering work (like Magic Editor in Google Photos). Moreover, the authors claims that the SEELE addresses multiple generative sub-tasks in subject repositioning using a single diffusion model, on one hand, SEELE actually contains many components not only the diffusion model for generative sub-tasks, on the other hand, it is confusing what is the exact meaning of using a single diffusion model since it seems that different generative sub-tasks are tackled with different datasets at least.
- Third, about the task inversion, from Figure 3, it seems that the differences are small since it's not difficult to change the textual inversion to task inversion by using different training datasets.
- At last, ReS dataset is helpful for the evaluation of subject repositioning task, however, the comparison of this work with Magic Editor of Google Photos is not well addressed in this paper.
About the reproducibility:
- This work contains many different components with different training datasets as well as different training strategies, thus it is not easy to understand and reproduce the details.

Review Point: - Second, the proposed framework, namely SEELE, combines many existing techniques, and can be considered as an engineering work (like Magic Editor in Google Photos). Moreover, the authors claims that the SEELE addresses multiple generative sub-tasks in subject repositioning using a single diffusion model, on one hand, SEELE actually contains many components not only the diffusion model for generative sub-tasks, on the other hand, it is confusing what is the exact meaning of using a single diffusion model since it seems that different generative sub-tasks are tackled with different datasets at least.
Review Point: - Third, about the task inversion, from Figure 3, it seems that the differences are small since it's not difficult to change the textual inversion to task inversion by using different training datasets.
Review Point: - At last, ReS dataset is helpful for the evaluation of subject repositioning task, however, the comparison of this work with Magic Editor of Google Photos is not well addressed in this paper. About the reproducibility:
Review Point: - This work contains many different components with different training datasets as well as different training strategies, thus it is not easy to understand and reproduce the details.
==================================================

Focused review:

1. The rationality of ScaleBench is questionable. First, while the perspective effect often occurs in crowd localization, there exist images that are captured from different angles (e.g., top view). In such scenarios, image distribution regularization may fail to partition the images correctly. Second, in the real world, scale shift is often coupled with other factors, such as occlusion, weather, and appearance. For example, when the object suffers from significant appearance variations, the counting model may fail to localize objects even if training and testing data yield the same scale distribution. Third, dividing images into patches will inevitably result in incomplete objects, which could affect the localization results. Therefore, evaluations on ScaleBench may not rigorously reflect the influence of scale shift.
2. The proposed SemanticHook does not exhibit superiority over existing methods. As shown in Table 1, the simplest baseline ERM already achieves good results. The proposed method is not necessarily better than ERM.
3. Following the previous comment, the rationale of SemanticHook is not entirely convincing. Eq. 6 suggests that p(s, c, …) can lead to a spurious association between the output y and scale c. This term is a joint distribution of semantic s and scale c. However, the authors merely try to enhance the semantic association between semantic s and output y. Experimental results demonstrate that such a technique does not address scale shift effectively. Additionally, perturbing image is not a new idea, which is widely used in adversarial attack.
4. It appears that the influence of image interpolation is not rigorously quantified in Table 4. First, the implementation of Random Augmentation shall be modified according to different domains, i.e., the range of random scaling should be customized based on domain Tiny, Small, and Normal. Second, it is necessary to train the model using different source domains to identify the effect of image interpolation. The results on domain Big are insufficient to conclude that the benefits of image interpolation are modest.
5. Regarding training details. In practice, random scaling is commonly used to alleviate scale variations. As the authors use this technique to train the model, the reported results may not correctly reveal the effect of scale shift, because random scaling already simulates different scales.
6. The paper lacks evaluations on previous methods featuring multi-scale architecture, e.g., STEER. Evaluations on these methods are helpful in revealing whether previous methods can handle scale variations.

Review Point: 2. The proposed SemanticHook does not exhibit superiority over existing methods. As shown in Table 1, the simplest baseline ERM already achieves good results. The proposed method is not necessarily better than ERM.
Review Point: 3. Following the previous comment, the rationale of SemanticHook is not entirely convincing. Eq. 6 suggests that p(s, c, …) can lead to a spurious association between the output y and scale c. This term is a joint distribution of semantic s and scale c. However, the authors merely try to enhance the semantic association between semantic s and output y. Experimental results demonstrate that such a technique does not address scale shift effectively. Additionally, perturbing image is not a new idea, which is widely used in adversarial attack.
Review Point: 4. It appears that the influence of image interpolation is not rigorously quantified in Table 4. First, the implementation of Random Augmentation shall be modified according to different domains, i.e., the range of random scaling should be customized based on domain Tiny, Small, and Normal. Second, it is necessary to train the model using different source domains to identify the effect of image interpolation. The results on domain Big are insufficient to conclude that the benefits of image interpolation are modest.
Review Point: 5. Regarding training details. In practice, random scaling is commonly used to alleviate scale variations. As the authors use this technique to train the model, the reported results may not correctly reveal the effect of scale shift, because random scaling already simulates different scales.
Review Point: 6. The paper lacks evaluations on previous methods featuring multi-scale architecture, e.g., STEER. Evaluations on these methods are helpful in revealing whether previous methods can handle scale variations.
==================================================

Focused review:

- Some parts are a bit difficult to read and have unusual terminology (e.g., "local traps"?) and the paper contains some minor grammar and cosmetic issues which make the quite complex approach harder to understand.
- The method is actually three methods, and the user is to pick the one most suitable for their problem. This seems a bit complicated, why not go for the most general?
Example presentation issues (I recommend you do another polish pass):
- General: excessive underlining
- 470: typo: physical simulator
- 474: typo: can work in general safety guarantees

Review Point: - Some parts are a bit difficult to read and have unusual terminology (e.g., "local traps"?) and the paper contains some minor grammar and cosmetic issues which make the quite complex approach harder to understand.
Review Point: - The method is actually three methods, and the user is to pick the one most suitable for their problem. This seems a bit complicated, why not go for the most general? Example presentation issues (I recommend you do another polish pass):
==================================================

Focused review:

1. While Figure 1 provides an intuitive comparison with related work, the related work section can be improved to amplify the paper's position. In particular, using adversarial learning to identify regions where the model makes the most mistakes is not novel and has been studied before by `Lahoti, Preethi, et al.` [1]. I recommend the authors discuss this work and consider it as a baseline for comparison.
2. The proof of theorem 3.2 seems to rely on the assumption that samples from high error regions mostly influence the gradient updates. This seems to depend on the success rate of the fairness attack, and the authors did not demonstrate that the attack can fully construct `R_unfair`. I suggest to clarify the assumptions used in theorem 3.2.
3. It is suggested the author could perform additional experiments to show the success rate of the fairness attack on real datasets, i.e., the unfair region is actually the region targeted by the fairness attack. This will support the assumption that the attack can effectively identify `R_unfair` and oversample data points from that region. For example, predicting the accuracy of a sensitive attribute classifier in these regions and comparing it with the ground truth `R_unfair`. These experiments showing that the fairness attack effectively augments data points in the ground truth `R_unfair` would support the inherent assumption made in theorem 3.2.
4. Some parts of the paper can be improved for clarity; for example, in Figure 4-6, some baseline methods have fewer tradeoff points than others; the number of parameters controlling the tradeoff between $\Delta_{EOD}$ and Acc. should be provided. In Fig. 2(a), providing the decision boundary for different perturbation levels $\delta$ might provide better insight into the magnitude of the fairness attack over the corrected boundary.
5. Throughout the paper (e.g., Line 137), it’s stated that the method improves group fairness _without compromising accuracy_; as the results show the tradeoff between fairness and accuracy, the statement should be updated to reflect this fact. For example, the authors formulate the statement as "_improves group fairness while maintaining accuracy_ (as much as possible)". This would highlight the fact that there is a potential tradeoff.
[1] Lahoti, Preethi, et al. "Fairness without demographics through adversarially reweighted learning." Advances in neural information processing systems 33 (2020): 728-740.

Review Point: 1. While Figure 1 provides an intuitive comparison with related work, the related work section can be improved to amplify the paper's position. In particular, using adversarial learning to identify regions where the model makes the most mistakes is not novel and has been studied before by `Lahoti, Preethi, et al.` [1]. I recommend the authors discuss this work and consider it as a baseline for comparison.
Review Point: 2. The proof of theorem 3.2 seems to rely on the assumption that samples from high error regions mostly influence the gradient updates. This seems to depend on the success rate of the fairness attack, and the authors did not demonstrate that the attack can fully construct `R_unfair`. I suggest to clarify the assumptions used in theorem 3.2.
Review Point: 3. It is suggested the author could perform additional experiments to show the success rate of the fairness attack on real datasets, i.e., the unfair region is actually the region targeted by the fairness attack. This will support the assumption that the attack can effectively identify `R_unfair` and oversample data points from that region. For example, predicting the accuracy of a sensitive attribute classifier in these regions and comparing it with the ground truth `R_unfair`. These experiments showing that the fairness attack effectively augments data points in the ground truth `R_unfair` would support the inherent assumption made in theorem 3.2.
Review Point: 4. Some parts of the paper can be improved for clarity; for example, in Figure 4-6, some baseline methods have fewer tradeoff points than others; the number of parameters controlling the tradeoff between $\Delta_{EOD}$ and Acc. should be provided. In Fig. 2(a), providing the decision boundary for different perturbation levels $\delta$ might provide better insight into the magnitude of the fairness attack over the corrected boundary.
Review Point: 5. Throughout the paper (e.g., Line 137), it’s stated that the method improves group fairness _without compromising accuracy_; as the results show the tradeoff between fairness and accuracy, the statement should be updated to reflect this fact. For example, the authors formulate the statement as "_improves group fairness while maintaining accuracy_ (as much as possible)". This would highlight the fact that there is a potential tradeoff. [1] Lahoti, Preethi, et al. "Fairness without demographics through adversarially reweighted learning." Advances in neural information processing systems 33 (2020): 728-740.
==================================================

Focused review:

The experimental results do not effectively validate the method's effectiveness.
1.	Why were tests conducted only on 1080p videos? To validate the method's generality, the authors should test it on videos of different resolutions. Typically, in video compression tasks, testing is done on classes like B, C, D, and E, but the authors have provided results for only class B. To ensure the method's effectiveness across various resolutions, the authors should supplement the results on these different video classes. Additionally, what issues arise from center-cropping input images to multiples of 128? Why not consider the original image dimensions?
2.	The proposed method is trained on V100 FP32 and tested on both V100 FP32 and P40 FP16. I agree that different precision platforms can impact results. However, when testing with the lower precision platform, it's customary to convert FP32 to FP16 for comparison. Have the authors provided such comparisons, particularly with traditional methods like H.265 and H.264?
3.	How was the codebook generated? It is mentioned that Zhu et al.'s method is used. Could they provide specific details on the training process, or did they directly employ pre-trained codebook? If that, how can we guarantee that this codebook can adapt to the testing dataset?
4.	Why wasn't bitrate considered during training? Would joint optimization potentially yield better results?
5.	Could the authors provide an explanation for the design of codebook sizes as mentioned here? “While for predicted frames, we use three different groups of codebook sizes to achieve different video compression ratios as {8192, 2048, 512}, {64, 2048, 512} and {8, 2048, 512}.”
6.	I believe that using codebooks for compression is not only cross-platform but also an efficient approach. Why haven't the authors compared their method with more recent approaches like VVC and DCVC-DC?
7.	The authors used GOP=12 for H.264 and H.265 configurations, but the model in the paper used GOP=32. Setting H.264/H.265 configurations to GOP=32 would ensure a fairer comparison.
8.	H.264 and H.265 compression configurations seem a little different from the common test configuration. Could you provide specific instructions for H.264 and H.265 compression configurations?
9.	In Sec 2.1, “Consequently, To avoid….” here 'T' should be lowercase.

Review Point: 1. Why were tests conducted only on 1080p videos? To validate the method's generality, the authors should test it on videos of different resolutions. Typically, in video compression tasks, testing is done on classes like B, C, D, and E, but the authors have provided results for only class B. To ensure the method's effectiveness across various resolutions, the authors should supplement the results on these different video classes. Additionally, what issues arise from center-cropping input images to multiples of 128? Why not consider the original image dimensions?
Review Point: 2. The proposed method is trained on V100 FP32 and tested on both V100 FP32 and P40 FP16. I agree that different precision platforms can impact results. However, when testing with the lower precision platform, it's customary to convert FP32 to FP16 for comparison. Have the authors provided such comparisons, particularly with traditional methods like H.265 and H.264?
Review Point: 3. How was the codebook generated? It is mentioned that Zhu et al.'s method is used. Could they provide specific details on the training process, or did they directly employ pre-trained codebook? If that, how can we guarantee that this codebook can adapt to the testing dataset?
Review Point: 4. Why wasn't bitrate considered during training? Would joint optimization potentially yield better results?
Review Point: 5. Could the authors provide an explanation for the design of codebook sizes as mentioned here? “While for predicted frames, we use three different groups of codebook sizes to achieve different video compression ratios as {8192, 2048, 512}, {64, 2048, 512} and {8, 2048, 512}.” 6. I believe that using codebooks for compression is not only cross-platform but also an efficient approach. Why haven't the authors compared their method with more recent approaches like VVC and DCVC-DC?
Review Point: 7. The authors used GOP=12 for H.264 and H.265 configurations, but the model in the paper used GOP=32. Setting H.264/H.265 configurations to GOP=32 would ensure a fairer comparison.
Review Point: 8. H.264 and H.265 compression configurations seem a little different from the common test configuration. Could you provide specific instructions for H.264 and H.265 compression configurations?
Review Point: 9. In Sec 2.1, “Consequently, To avoid….” here 'T' should be lowercase.
==================================================

Focused review:

- There is a lack of justification of why Adacos has lower uncertainty than Arcrface in fairness metric. To fairly compare the performance of Adacos and Arcface, training those methods on Fair Face Recognition dataset [A] might help to justify the performance better. Note that there are multiple FR methods [B,C] that focus on solving fairness problem in face recognition. Comparison with those methods might be useful to justify the proposed metric.
- Instead of the background and preliminaries, it is more important to include more comparison of fairness metrics with various FR methods in the supplementary material in the main manuscript. As the paper focuses on the application of fair recognition technology, it is important to add more justification on the corresponding problem.
Additional references
- [A] FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition, ECCVW 2020
- [B] Consistent Instance False Positive Improves Fairness in Face Recognition, CVPR 2021
- [C] RamFace: Race Adaptive Margin Based Face Recognition for Racial Bias Mitigation, IJCB 2021

Review Point: - There is a lack of justification of why Adacos has lower uncertainty than Arcrface in fairness metric. To fairly compare the performance of Adacos and Arcface, training those methods on Fair Face Recognition dataset [A] might help to justify the performance better. Note that there are multiple FR methods [B,C] that focus on solving fairness problem in face recognition. Comparison with those methods might be useful to justify the proposed metric.
Review Point: - Instead of the background and preliminaries, it is more important to include more comparison of fairness metrics with various FR methods in the supplementary material in the main manuscript. As the paper focuses on the application of fair recognition technology, it is important to add more justification on the corresponding problem. Additional references - [A] FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition, ECCVW 2020 - [B] Consistent Instance False Positive Improves Fairness in Face Recognition, CVPR 2021 - [C] RamFace: Race Adaptive Margin Based Face Recognition for Racial Bias Mitigation, IJCB 2021
==================================================

Focused review:

1. More comparisons with recent works should be provided in Tables 1 and 2. Additionally, there is a minor mistake: the detector names “GroupFree” and “Group-Free” in the first two rows of Tables 1 and 2 do not match.
2. The article gives a subtractive ablation experiment. I would like to see an additive ablation experiment, such as how the effect of verb alone works.
3. The article does not give the performance of the proposed IntentNet in traditional 3D grounding.

Review Point: 1. More comparisons with recent works should be provided in Tables 1 and 2. Additionally, there is a minor mistake: the detector names “GroupFree” and “Group-Free” in the first two rows of Tables 1 and 2 do not match.
Review Point: 2. The article gives a subtractive ablation experiment. I would like to see an additive ablation experiment, such as how the effect of verb alone works.
Review Point: 3. The article does not give the performance of the proposed IntentNet in traditional 3D grounding.
==================================================

Focused review:

This paper shows substantial weakness that make irrelevant for a publication. I split my remarks into major (the ones that really block the publication) and minor (important but non-blocking).
## Major remarks
1. **Novelty**: the main contribution of this article can be sum up as using GW distance with a compound metric on cross-domain data. It is not a new result about GW distance or a new variant of the optimal transport framework. The compound metric is an old idea that can be found in many other papers dealing with graph (see [1] for example). Furthermore, none of the proposition or theorem are new, they are directly taken from [2]. Secondly, the Fused-Gromov-Wassertein allows cross-modal domain data on both nodes and edges, it is just a matter of metrics.
2. **Writing**: while reading the paper, it seems incomplete as if sentences were missing. For example between the first paragraph of section 1.1 and the second paragraph, there is no transition. The text jumps from graph neural networks to optimal transport without any details on how these two frameworks can be related. Furthermore, the presentation of the GW distance is incomplete with notations that appear from nowhere. Another example, the first sentence of page 3 is with no link with the second sentence...
3. **Experiments**: the experiments are incomplete, it need more comparison against state-of-art methods. Since we are dealing with graph, I would expect comparison against FGW [3], KerGM [4] or GWL [5]. Since we have a distance classification results are also possible.
## Minor remarks
1. It seems there is a confusion between graph isomorphism and graph matching problems. While the complexity class of the first is still research question, the second is known to be NP-Complete. Since the GW distance is trying to solve the graph matching problem, it would be more appropriate to have a discussion about it.
2. In page 3, the authors formulation the GW distance as a Koopmans-Beckmann QAP. However, it the first time in the paper that the QAP appears. I think the QAP Koopmans-Beckmann should be presented in the related works section.
3. I agree that the GW distance is not convex, but the problem presented in (3) is convex... Seems there is a confusion between the set all permutation matrices (non-convex set) and the Birkhoff polytope (convex).
4. In page 4. I don't understand the notion of graph order. Does it means the dimension of features is the same?
## Reference
[1] Mahé, P., Ueda, N., Akutsu, T., Perret, J. L., & Vert, J. P. (2004, July). Extensions of marginalized graph kernels. In Proceedings of the twenty-first international conference on Machine learning (p. 70).
[2] Peyré, G., Cuturi, M., & Solomon, J. (2016, June). Gromov-wasserstein averaging of kernel and distance matrices. In International conference on machine learning (pp. 2664-2672). PMLR.
[3] Vayer, T., Chapel, L., Flamary, R., Tavenard, R., & Courty, N. (2020). Fused Gromov-Wasserstein distance for structured objects. Algorithms, 13(9), 212.
[4] Zhang, Z., Xiang, Y., Wu, L., Xue, B., & Nehorai, A. (2019). Kergm: Kernelized graph matching. Advances in Neural Information Processing Systems, 32.
[5] Xu, H., Luo, D., Zha, H., & Duke, L. C. (2019, May). Gromov-wasserstein learning for graph matching and node embedding. In International conference on machine learning (pp. 6932-6941). PMLR.

Review Point: 2. **Writing**: while reading the paper, it seems incomplete as if sentences were missing. For example between the first paragraph of section 1.1 and the second paragraph, there is no transition. The text jumps from graph neural networks to optimal transport without any details on how these two frameworks can be related. Furthermore, the presentation of the GW distance is incomplete with notations that appear from nowhere. Another example, the first sentence of page 3 is with no link with the second sentence...
Review Point: 3. **Experiments**: the experiments are incomplete, it need more comparison against state-of-art methods. Since we are dealing with graph, I would expect comparison against FGW [3], KerGM [4] or GWL [5]. Since we have a distance classification results are also possible. ## Minor remarks 1. It seems there is a confusion between graph isomorphism and graph matching problems. While the complexity class of the first is still research question, the second is known to be NP-Complete. Since the GW distance is trying to solve the graph matching problem, it would be more appropriate to have a discussion about it.
Review Point: 2. In page 3, the authors formulation the GW distance as a Koopmans-Beckmann QAP. However, it the first time in the paper that the QAP appears. I think the QAP Koopmans-Beckmann should be presented in the related works section.
Review Point: 3. I agree that the GW distance is not convex, but the problem presented in (3) is convex... Seems there is a confusion between the set all permutation matrices (non-convex set) and the Birkhoff polytope (convex).
==================================================

Focused review:

- I am unclear about the motivation behind this paper, particularly regarding the decision to utilize (distilled) AlphaFold instead of directly using AFDB. For instance, the paper states, "Despite this success, large-scale training is computationally expensive. A more efficient method could be to use a pre-trained forward folding model to guide the training of the inverse folding model." However, I fail to see the efficiency benefits of this approach, as utilizing the AF model (or distilled AF models) would entail additional on-the-fly inference costs compared to employing the AFDB.
- It is not clear whether the proposed method can outperform the model trained with AFDB or not.
- The overall performance improvement appears not much, and it is not clear where the gain comes from.

Review Point: - I am unclear about the motivation behind this paper, particularly regarding the decision to utilize (distilled) AlphaFold instead of directly using AFDB. For instance, the paper states, "Despite this success, large-scale training is computationally expensive. A more efficient method could be to use a pre-trained forward folding model to guide the training of the inverse folding model." However, I fail to see the efficiency benefits of this approach, as utilizing the AF model (or distilled AF models) would entail additional on-the-fly inference costs compared to employing the AFDB.
Review Point: - It is not clear whether the proposed method can outperform the model trained with AFDB or not.
Review Point: - The overall performance improvement appears not much, and it is not clear where the gain comes from.
==================================================

Focused review:

- The idea of multi-step SAM is not new and has been explored before. Although the interpolation step makes this work different from the previous work, I think that is a marginal contribution.
- The numerical experiments are rather limited. As far as I could tell, they only consider CIFAR and a down-sized version of ImageNet, only using ResNets. I think the complete ImageNet should be in the numerical studies, and some transformer-based models need to be added (such as ViTs or BERT).

Review Point: - The idea of multi-step SAM is not new and has been explored before. Although the interpolation step makes this work different from the previous work, I think that is a marginal contribution.
Review Point: - The numerical experiments are rather limited. As far as I could tell, they only consider CIFAR and a down-sized version of ImageNet, only using ResNets. I think the complete ImageNet should be in the numerical studies, and some transformer-based models need to be added (such as ViTs or BERT).
==================================================

Focused review:

- Better definition of question awareness/determinacy: Why does average kurtosis reflect determinacy? Is determinacy equivalent to “certainty” of the model? How do these two concepts link together? I am not convinced by the claim in section 2.4 that LLMs have fundamental question awareness on some scenarios, maybe these tasks are indirectly or directly presented in their training data, and thus not necessarily mean that they know that they need to choose more deterministically.
- QAT has a phase of continual fine-tuning prior to the temperature tuning phase, so I would love to see a comparison with baselines like simply tuning the temperature by hand. How does the cost differ? Which one performs better? Without a comprehensive comparison, and human evaluations for open-ended writing tasks, I don’t know whether QAT truly improves the generation.

Review Point: - Better definition of question awareness/determinacy: Why does average kurtosis reflect determinacy? Is determinacy equivalent to “certainty” of the model? How do these two concepts link together? I am not convinced by the claim in section 2.4 that LLMs have fundamental question awareness on some scenarios, maybe these tasks are indirectly or directly presented in their training data, and thus not necessarily mean that they know that they need to choose more deterministically.
Review Point: - QAT has a phase of continual fine-tuning prior to the temperature tuning phase, so I would love to see a comparison with baselines like simply tuning the temperature by hand. How does the cost differ? Which one performs better? Without a comprehensive comparison, and human evaluations for open-ended writing tasks, I don’t know whether QAT truly improves the generation.
==================================================

Focused review:

: 1. The analysis only considers the case of batch size = 1 and it is not clear how the batch size affects the effective initialization scale. 2. The effect of the step size in Theorem 1 is not clearly discussed. On the one hand, a large step size will accelerate the convergence and thus the integral of the loss will be small, but on the other hand, the step size appears explicitly in the exponent. So, is increasing step size result in smaller effective α
? If yes, is it correct also for GD or only for SGD ? 3. It would be good to complement the paper will experiments on real data with real networks. Clearly, the interaction of initialization scale, batch size and step size is an important practical question.
More comments:
line 130: I don’t think the model is equivalent to the model β = u ⊙ v
, as discussed in section 4 of [1].
In Theorem 1 the tradeoff in p
is not clear. It seems that p
can be arbitrarily small.
In Theorem 1, can you explain why a bound on the step size is needed ? Is it for stability reasons ?
Several relevant papers are missing:
On the implicit bias of SGD: https://arxiv.org/abs/2101.12176 https://arxiv.org/abs/2003.07802
On diagonal linear networks and mirror descent: https://arxiv.org/abs/2007.06738 https://arxiv.org/abs/2004.01025
In section 3 the kernel regime is discussed. Is it possible to have a similar discussion for the rich regime ? Is there a difference between SGD and GD in the rich regime ?
In line 220 the integral goes to 0, I couldn’t find the proof in the appendix. Also, the upper bound of the integral in proposition 3 goes to infinity when α → ∞
. Can you explain this ?
Can you explain why the equality in line 303 is correct ? α
is sometimes used as a scalar and sometimes as a vector, and it will be good to distinguish between them, e.g. make vector a bold symbol. For example, in proposition 3 it is not clear if α
is a scalar or a vector.
In section 5.1 please specify what is the step size.
In section 5.3 will be useful to compare the result to [12]. typos:
line 160: I think the second diag is not needed.
line 316: why -> while
line 348: understand -> understanding
line 444: α t
is incorrectly defined

Review Point: 2. The effect of the step size in Theorem 1 is not clearly discussed. On the one hand, a large step size will accelerate the convergence and thus the integral of the loss will be small, but on the other hand, the step size appears explicitly in the exponent. So, is increasing step size result in smaller effective α ? If yes, is it correct also for GD or only for SGD ?
==================================================

Focused review:

The reviewer has the following major concerns, and therefore leans on a rejection.
1. Conclusion not very surprising: regarding the finding-(1) in the above summary, when distilling a grokked model trained on a task p1 to a student model training on another task p2, it is shown that the student model is easier to generalize (acceleration of grokking) and the required data for p2 is below critical data size. However, the authors use tasks whose differences are only up to the modulus P (if the reviewer understands correctly). It is intuitive that such a KD process can inject some bias (or act as some ‘pre-training’) that facilitates the generalization on a similar task p2. Did the authors try to transfer to other tasks such as changing the binary operators?
2. Part of the conclusion is not new: regarding the finding-(2) in the above summary, existing works already showed with counterexamples that a decreasing weight norm may not be causally related to grokking, see examples in [1][2].
3. The experimental setup lacks justification:
- the reviewer is confused about the experimental setup, specifically why the training is performed in 30000 epochs and the data fraction is 30%/20%/10%? Are these used in some prior works?
- In section 5, the authors show that training a ‘larger’ model on a joint distribution of two tasks does not lead to grokking, but training two models on each task individually and distilling the two models into a ‘larger’ model allows for grokking. Why should one care about this setup? What is its implication?
4. The writing can be improved.
- For example, figure 1 is not mentioned in the text if the reviewer is not mistaken. In addition, many parts in the paper miss critical citations for a smooth reading (e.g. row 48, 50-51, 201-207)
- More intuition or explanation on why a set of experiments is conducted and why the use of KD objective makes a difference, would be greatly helpful.
5. Practical implication: the paper is motivated to consider a low-data regime (subject to security protocols and privacy regulations) for the purpose of facilitating generalization under limited data conditions, however, the reviewer had a hard time connecting the toy setting with the real-world applications. Specifically, the authors seem not to verify that a grokked model can reduce the delay generalization on the student model for a broad range of tasks.
[1] Grokking as the transition from lazy to rich training dynamics. Kumar et al. ICLR 2024
[2] Progress Measures for Grokking on Real-world Tasks. Golechha. HiLD 2024: 2nd Workshop on High-dimensional Learning Dynamics

Review Point: 2. Part of the conclusion is not new: regarding the finding-(2) in the above summary, existing works already showed with counterexamples that a decreasing weight norm may not be causally related to grokking, see examples in [1][2].
Review Point: - the reviewer is confused about the experimental setup, specifically why the training is performed in 30000 epochs and the data fraction is 30%/20%/10%? Are these used in some prior works?
Review Point: - In section 5, the authors show that training a ‘larger’ model on a joint distribution of two tasks does not lead to grokking, but training two models on each task individually and distilling the two models into a ‘larger’ model allows for grokking. Why should one care about this setup? What is its implication?
Review Point: - For example, figure 1 is not mentioned in the text if the reviewer is not mistaken. In addition, many parts in the paper miss critical citations for a smooth reading (e.g. row 48, 50-51, 201-207) - More intuition or explanation on why a set of experiments is conducted and why the use of KD objective makes a difference, would be greatly helpful.
==================================================

Focused review:

1. There is a trade-off between optimization error and statistical error in the developed theory. Therefore, it is unclear whether Nesterov's acceleration offers a true advantage over the naive method.
2. Empirically and theoretically, the advantages and weaknesses compared to direct reward optimization algorithms (e.g., PPO) are unclear.
3. Experiment results are somewhat weak and does not validate the theory.

Review Point: 1. There is a trade-off between optimization error and statistical error in the developed theory. Therefore, it is unclear whether Nesterov's acceleration offers a true advantage over the naive method.
Review Point: 2. Empirically and theoretically, the advantages and weaknesses compared to direct reward optimization algorithms (e.g., PPO) are unclear.
Review Point: 3. Experiment results are somewhat weak and does not validate the theory.
==================================================

Focused review:

1) The generalizability across datasets. The proposed method is generalizable and can handle unseen objects. I'm curious about the generalizability across datasets. For example, the proposed method is trained on the CO3D dataset, but how about the generalizability of the LINEMOD dataset? I think the generalizability across datasets is important for the proposed method, especially for real-world applications.
2) Running time comparisons. The paper needs to sample M=50000 poses for verification at test time. I'm concerned about the efficiency of estimating a single pose, even if the verifications are parallelized. I think the running time comparisons with other methods are necessary.

Review Point: 1) The generalizability across datasets. The proposed method is generalizable and can handle unseen objects. I'm curious about the generalizability across datasets. For example, the proposed method is trained on the CO3D dataset, but how about the generalizability of the LINEMOD dataset? I think the generalizability across datasets is important for the proposed method, especially for real-world applications.
Review Point: 2) Running time comparisons. The paper needs to sample M=50000 poses for verification at test time. I'm concerned about the efficiency of estimating a single pose, even if the verifications are parallelized. I think the running time comparisons with other methods are necessary.
==================================================

Focused review:

- **Incremental Contribution**: While this paper makes a commendable attempt to address the limitations of prior code generation benchmarks by offering a more fine-grained evaluation, its novelty and technical contributions are relatively incremental. It aligns with a line of research that introduces increasingly challenging benchmarks to highlight the limitations of LLMs without substantially advancing our understanding of how to overcome these challenges. Additionally, the insights drawn from the results are somewhat **limited**, especially considering that other more complex code generation benchmarks, such as LiveCodeBench, already exist. To make it more solid work, the authors should provide more insights from the results and how they guige targeted improvements to model architectures or training. For example, they can analyze patterns in the errors to gain insights into fundamental limitations of current LLM approaches.
- **Unsystematic Problem Curation**: The problem categories and challenges are curated manually, which may affect consistency, coverage, and diversity. The lack of an automated, systematic process for problem generation could limit the scalability and reproducibility of this benchmark, making it harder to ensure high quality and comprehensive coverage across problem types.

Review Point: - **Unsystematic Problem Curation**: The problem categories and challenges are curated manually, which may affect consistency, coverage, and diversity. The lack of an automated, systematic process for problem generation could limit the scalability and reproducibility of this benchmark, making it harder to ensure high quality and comprehensive coverage across problem types.
==================================================

Focused review:

a. Empirical results aren't strong enough to back the three main claims; Need further analysis. i. For example, changing learning rate didn't seem to help with the learning task on FashionMNIST dataset. ii. Increasing the discretization of data distribution doesn't seem to improve the learning for (FashionMNIST, Contrast), (FashionMNIST, Blur), (MNIST, Blur), (MNIST, Contrast), and (MNIST, Shift). iii. The results of resetting network biases are not consistent across different datasets; need further analysis to make a more clear conclusion. iv. Some test loss differences seem too small and made me wonder about their statistical significance.
Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. Reject. Because of the aforementioned weaknesses, it is unclear what conclusion to make and apply to other research topics and/or applications.
Provide supporting arguments for your recommendation. See 2.(2).a.
Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. (1) Could you provide further analyses to refute i-iv in 2.(2).a?
Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. (1) It will be helpful to qualitatively understand what the fine-tuned network learned if you visualize convolution features of both pretrained and fine-tuned models.

Review Point: 2.(2).a. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. (1) Could you provide further analyses to refute i-iv in 2.(2).a? Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. (1) It will be helpful to qualitatively understand what the fine-tuned network learned if you visualize convolution features of both pretrained and fine-tuned models.
==================================================

Focused review:

- The paper lacks technical novelty as it relies on ASYRP (https://openreview.net/forum?id=pd1P2eUBVfq). This foundation alone, in my opinion, does not meet the standards for publication in ICLR. The methodology section closely mirrors that of the ASYRP paper, encompassing the training of the h-generator module and the sampling process. The asserted 'Zero Shot Inversion' process appears to be synonymous with the asymmetric reversal process outlined in the ASYRP paper, albeit with the addition of an attribute encoder.
- The experimentation is confined to DDPM, with results excluding recent foundational models such as Stable Diffusion.
- The number of attributes (man, old, smiling, young) utilized in the in-domain evaluation is insufficient. Additionally, only 2 domains (human faces and LSUN dataset) are used.
- I failed to identify any practical advantages of this approach over previous methods concerning time, diversity, and generalizability. Given the inadequacy of the experiments, I remain unconvinced.
- Table 1 illustrates no superiority of the ZIP approach over previous methods in terms of results.

Review Point: - The paper lacks technical novelty as it relies on ASYRP (https://openreview.net/forum?id=pd1P2eUBVfq). This foundation alone, in my opinion, does not meet the standards for publication in ICLR. The methodology section closely mirrors that of the ASYRP paper, encompassing the training of the h-generator module and the sampling process. The asserted 'Zero Shot Inversion' process appears to be synonymous with the asymmetric reversal process outlined in the ASYRP paper, albeit with the addition of an attribute encoder.
Review Point: - The experimentation is confined to DDPM, with results excluding recent foundational models such as Stable Diffusion.
Review Point: - The number of attributes (man, old, smiling, young) utilized in the in-domain evaluation is insufficient. Additionally, only 2 domains (human faces and LSUN dataset) are used.
Review Point: - I failed to identify any practical advantages of this approach over previous methods concerning time, diversity, and generalizability. Given the inadequacy of the experiments, I remain unconvinced.
Review Point: - Table 1 illustrates no superiority of the ZIP approach over previous methods in terms of results.
==================================================

Focused review:

1. The paper lacks justification for the adiabatic assumption. The paper creates specific settings for the experiments and makes some discussions, but are there many real-world scenarios in which such adiabatic assumptions can be applied? Also, there seems to require a more detailed formulation of such an adiabatic assumption. Say if the new information is too little, then it gets back to the ordinary training and there is no reason to do continual learning. Some mathematical formulation of the adiabatic assumption should be defined.
2. In addition to the mathematical definition of the adiabatic assumption, it is also not clear why the selected settings in the experiments satisfy such an assumption. Why the tasks are chosen in such a way? From a more practical perspective, if we are dealing with real-world tasks, how do we check if the incoming task satisfy the adiabatic assumption and we can use the proposed method?
3. The paper only tests for certain restricted settings and uses a relatively simple model (GMM). Though the paper mentions that there could be more advanced version of the GMM model that could solve the capacity problem, but there does not seem to be much evidence to support such a claim.
4. It is claimed that the proposed method does not have the problem of scaling up as the number of tasks increases. However, for GMM, there is the number of clusters and I wonder should such a number be set according to the total number of tasks? If there are infinite number of classes coming in a stream, should the number of clusters also increase? Even though the change of size could be small, if we want to use a much more capable model as mentioned in the paper, will the model size go up as the tasks increase? Minor:
Page 1: "On the one hand, there are ”true” replay" (the quote symbols)

Review Point: 1. The paper lacks justification for the adiabatic assumption. The paper creates specific settings for the experiments and makes some discussions, but are there many real-world scenarios in which such adiabatic assumptions can be applied? Also, there seems to require a more detailed formulation of such an adiabatic assumption. Say if the new information is too little, then it gets back to the ordinary training and there is no reason to do continual learning. Some mathematical formulation of the adiabatic assumption should be defined.
Review Point: 2. In addition to the mathematical definition of the adiabatic assumption, it is also not clear why the selected settings in the experiments satisfy such an assumption. Why the tasks are chosen in such a way? From a more practical perspective, if we are dealing with real-world tasks, how do we check if the incoming task satisfy the adiabatic assumption and we can use the proposed method?
Review Point: 3. The paper only tests for certain restricted settings and uses a relatively simple model (GMM). Though the paper mentions that there could be more advanced version of the GMM model that could solve the capacity problem, but there does not seem to be much evidence to support such a claim.
==================================================

Focused review:

1. The study's exclusive use of synthetic data (UUID key-value pairs) may not accurately reflect performance on natural language tasks or domain-specific applications.
2. The paper may be limited in techinical contribution.

Review Point: 1. The study's exclusive use of synthetic data (UUID key-value pairs) may not accurately reflect performance on natural language tasks or domain-specific applications.
==================================================

Focused review:

Weakness： 1. This paper is built on the SPAIR framework and focuses on point cloud data, which is somehow incremental. 2. There is no ablation study to validate the effectiveness of the proposed components and the loss. 3. It is hard to follow Sec. 3.2. The author may improve it and give more illustrations and examples. 4. It is unclear how the method can work and decompose a scene into different objects. I did not see how Chamfer Mixture loss can achieve this goal. More explanation should go here.

Review Point: 1. This paper is built on the SPAIR framework and focuses on point cloud data, which is somehow incremental.
Review Point: 2. There is no ablation study to validate the effectiveness of the proposed components and the loss.
Review Point: 3. It is hard to follow Sec. 3.2. The author may improve it and give more illustrations and examples.
Review Point: 4. It is unclear how the method can work and decompose a scene into different objects. I did not see how Chamfer Mixture loss can achieve this goal. More explanation should go here.
==================================================

Focused review:

1. The main concern is the technical contribution. CLIP uses image-text similarity to perform zero-shot classification, so evaluating the similarities roughly equals evaluating classification accuracy.
2. Pseudo-CMM is not clearly described. How to set the pseudo label for a sample?
3. The CPE algorithm is conducted on the test set. Although the paper claims the annotations are not used, it still seems inappropriate.

Review Point: 1. The main concern is the technical contribution. CLIP uses image-text similarity to perform zero-shot classification, so evaluating the similarities roughly equals evaluating classification accuracy.
Review Point: 2. Pseudo-CMM is not clearly described. How to set the pseudo label for a sample?
Review Point: 3. The CPE algorithm is conducted on the test set. Although the paper claims the annotations are not used, it still seems inappropriate.
==================================================

Focused review:

- Could benefit from more analysis of failure cases.
- The label refinement stage using human occlusion cues may be problematic when interactions are ambiguous or when multiple affordances exist.
- The mapping from affordance to part names is ad-hoc and manually crafted, which limits the scalability to new affordance types and more complex objects.

Review Point: - The label refinement stage using human occlusion cues may be problematic when interactions are ambiguous or when multiple affordances exist.
Review Point: - The mapping from affordance to part names is ad-hoc and manually crafted, which limits the scalability to new affordance types and more complex objects.
==================================================

Focused review:

- One of the biggest weaknesses is the clear discussion/motivation on why other methods are problematic. This is important because the use of continuous latent space in this kind of problem has been rigorously discussed in the ML literature and without clearly differentiating with prior works, the contribution is weakened. - The experiments for ablation is missing. For example, the effect of bias of importance sampling (or benefit of the beam search) is not demonstrated empirically.

Review Point: - One of the biggest weaknesses is the clear discussion/motivation on why other methods are problematic. This is important because the use of continuous latent space in this kind of problem has been rigorously discussed in the ML literature and without clearly differentiating with prior works, the contribution is weakened.
Review Point: - The experiments for ablation is missing. For example, the effect of bias of importance sampling (or benefit of the beam search) is not demonstrated empirically.
==================================================

Focused review:

- Frequency of Reproduced Sequences: The paper could benefit from clarifying how often the reproduced sequences appear within their training data proxy, AUXDATASET. Understanding whether these snippets are rare or commonly encountered would help contextualize the reproduction risks.
- Justification of 50-Character Threshold: The choice of a 50-character threshold to define reproduced sequences is not fully justified. In particular, this is quite different from past work. While some examples in the Appendix suggest that 50 characters is a meaningful number, I believe most examples highlight that such sequence lengths can be so common in the natural language distribution that their reproduction does not matter. Further explanation would help readers assess whether this threshold adequately captures the difference between common phrases and more problematic reproductions.
- Data in Figure 2(b): Figure 2(b) appears to have only partial bar plots for some models (Llama and GPT), making the comparison across models less robust. Or am I missing something here?
Overall, I am constantly battling between thinking that 50 characters is too less, and then seeing the argument that these reproduction rates are much higher than humans. This makes me wonder if humans are the right baseline here. Would a human with a passage (RAG style reading comprehension) be a better baseline? There is a qualitative dichotomy here: the 50 characters do not feel meaningful when visualized, yet stay higher than what a human would reproduce.

Review Point: - Frequency of Reproduced Sequences: The paper could benefit from clarifying how often the reproduced sequences appear within their training data proxy, AUXDATASET. Understanding whether these snippets are rare or commonly encountered would help contextualize the reproduction risks.
Review Point: - Justification of 50-Character Threshold: The choice of a 50-character threshold to define reproduced sequences is not fully justified. In particular, this is quite different from past work. While some examples in the Appendix suggest that 50 characters is a meaningful number, I believe most examples highlight that such sequence lengths can be so common in the natural language distribution that their reproduction does not matter. Further explanation would help readers assess whether this threshold adequately captures the difference between common phrases and more problematic reproductions.
==================================================

Focused review:

* The first major weakness of this paper is that it seems to ignore relevant literature in this area. Interpolation has been used and demonstrated for a long time [1, 2], the latent traversal has also been discovered before [3]. Despite this paper strikes the point that it focuses on diffusion models, but the authors should still consider proper comparisons or acknowledgments.
* Given this problem has been studied before, especially interpolation, the technical contribution of this paper is very limited. Another major contribution claimed in the paper (learning a semantics-guided autoencoding diffusion model) has also been proposed in [4]. The formulation looks exactly the same.
* It is not surprising that linear interpolation could lead to improved properties as studied widely in images and some in molecules (e.g. [3]), but the experimental details need to be included for a fair comparison and evaluation.
* The template-based manipulation seems an interesting setup, but a more realistic one should be specifying a specific molecule and then manipulate that molecule, given the stochastic forward process of diffusion model, maybe it is not possible. The name of "template-based" manipulation seems referring to manipulate any given molecule rather than a randomly sampled molecule.
* [Minor] The explanation of stability and validity seems to be different from the literature, see e.g. [5].
[1] Gómez-Bombarelli, R., Wei, J.N., Duvenaud, D., Hernández-Lobato, J.M., Sánchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T.D., Adams, R.P. and Aspuru-Guzik, A., 2018. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2), pp.268-276.
[2] Zang, C. and Wang, F., 2020, August. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 617-626).
[3] Du, Y., Liu, X., Shah, N.M., Liu, S., Zhang, J. and Zhou, B., 2022. ChemSpacE: Interpretable and Interactive Chemical Space Exploration. Transactions on Machine Learning Research.
[4] Wang, Y., Schiff, Y., Gokaslan, A., Pan, W., Wang, F., De Sa, C. and Kuleshov, V., 2023. InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models. ICML 2023.
[5] Hoogeboom, E., Satorras, V.G., Vignac, C. and Welling, M., 2022, June. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning (pp. 8867-8887). PMLR.

Review Point: * The first major weakness of this paper is that it seems to ignore relevant literature in this area. Interpolation has been used and demonstrated for a long time [1, 2], the latent traversal has also been discovered before [3]. Despite this paper strikes the point that it focuses on diffusion models, but the authors should still consider proper comparisons or acknowledgments.
Review Point: * Given this problem has been studied before, especially interpolation, the technical contribution of this paper is very limited. Another major contribution claimed in the paper (learning a semantics-guided autoencoding diffusion model) has also been proposed in [4]. The formulation looks exactly the same.
Review Point: * It is not surprising that linear interpolation could lead to improved properties as studied widely in images and some in molecules (e.g. [3]), but the experimental details need to be included for a fair comparison and evaluation.
Review Point: * The template-based manipulation seems an interesting setup, but a more realistic one should be specifying a specific molecule and then manipulate that molecule, given the stochastic forward process of diffusion model, maybe it is not possible. The name of "template-based" manipulation seems referring to manipulate any given molecule rather than a randomly sampled molecule.
==================================================

Focused review:

The paper emphasizes the use of a parametric module to decompose time series into an informative part and a task-irrelevant part and perform parameter transformation on the informative part to obtain an enhanced view. However, it is not sufficiently clear.
1. What role does g play specifically? Especially in the analysis of the ablation experiments, I only observed differences in the results;
2. It's ambiguous why h is able to focus on the informative part of the sequence. The author should provide several case studies to prove that h can indeed play a role in separating the information part and noise part of the time series.
3. The two networks h and g use the same structure. How to ensure that they indeed perform the two different functions claimed by the author?
4. How is Δv set in the experiments?
5. The paper does not verify whether the positive view generated by the proposed method contains more information than the original sequence. Although the paper showcases the augmented instances through visualization in Figure 5, as the authors mentioned in the abstract, "it is impractical to visually inspect the temporal structures in time series." Providing explanations for the generated positive samples based on Property 3 or other relevant data would be beneficial.
6. From the visualization in Figure 5, it appears that the output mask of g is a binary mask, which does not match the description in the article. The article interprets g as a non-zero transformation mask. If the output of g is also a binary mask, does it mean that the optimal transformation method for sample enhancement that the model learns is simply to mask some observations of the original sequence?

Review Point: 1. What role does g play specifically? Especially in the analysis of the ablation experiments, I only observed differences in the results;
Review Point: 2. It's ambiguous why h is able to focus on the informative part of the sequence. The author should provide several case studies to prove that h can indeed play a role in separating the information part and noise part of the time series.
Review Point: 3. The two networks h and g use the same structure. How to ensure that they indeed perform the two different functions claimed by the author?
Review Point: 5. The paper does not verify whether the positive view generated by the proposed method contains more information than the original sequence. Although the paper showcases the augmented instances through visualization in Figure 5, as the authors mentioned in the abstract, "it is impractical to visually inspect the temporal structures in time series." Providing explanations for the generated positive samples based on Property 3 or other relevant data would be beneficial.
Review Point: 6. From the visualization in Figure 5, it appears that the output mask of g is a binary mask, which does not match the description in the article. The article interprets g as a non-zero transformation mask. If the output of g is also a binary mask, does it mean that the optimal transformation method for sample enhancement that the model learns is simply to mask some observations of the original sequence?
==================================================

Focused review:

1. It can be seen from Table 3 that the biggest innovation in this paper, WD, has little improvement on performance.
2. The full text lacks innovation. For example, TPQ and BKD in this paper are not innovative.
3. Setting the scale factor for the out-channel of weights is not a new method, which has been widely used in the channel-wise quantization.
4. The paper lacks the effect of image generation at higher resolution (i.e., 1024x1024).

Review Point: 1. It can be seen from Table 3 that the biggest innovation in this paper, WD, has little improvement on performance.
Review Point: 2. The full text lacks innovation. For example, TPQ and BKD in this paper are not innovative.
Review Point: 3. Setting the scale factor for the out-channel of weights is not a new method, which has been widely used in the channel-wise quantization.
Review Point: 4. The paper lacks the effect of image generation at higher resolution (i.e., 1024x1024).
==================================================

Focused review:

1. In most applications we have multi-valued multiple protected attributes. It seems that it is non-trivial to extend the loss function to cater to such datasets. How does the method scale with multi valued multiple protected attribute setup?
2. The choice of \alpha hyperparameter is arbitrary. Why 4 works and not 8? How does one choose the value of this hyperparameter in real world scenario?
3. The paper completely ignores the discussion on hyperparameter settings for the competitors. To ensure fairness to competitors, the detailed hyperparameter settings should be included for each compared method to better illustrate the benefits of the proposed method.

Review Point: 1. In most applications we have multi-valued multiple protected attributes. It seems that it is non-trivial to extend the loss function to cater to such datasets. How does the method scale with multi valued multiple protected attribute setup?
Review Point: 2. The choice of \alpha hyperparameter is arbitrary. Why 4 works and not 8? How does one choose the value of this hyperparameter in real world scenario?
Review Point: 3. The paper completely ignores the discussion on hyperparameter settings for the competitors. To ensure fairness to competitors, the detailed hyperparameter settings should be included for each compared method to better illustrate the benefits of the proposed method.
==================================================

Focused review:

1) Parts of paper appear to be LLM-written (for instance: L281-L304). Itself it is not a bad thing, yet those parts of paper lack substance and sometimes repeat the same information.
2) This paper would benefit from more experiments on decoder models (in addition to existing experiments on encoder models). This would make the paper even more relevant. Indeed, in the introduction, authors cite OpenAI's techincal report as evidence that LLMs have shown substantial performance across domains. It would be naturally to include said LLMs (maybe not OpenAI's LLMs, but generally) into the scope of this paper.
3) A reasonably-sized limitations section would also benefit the paper.

Review Point: 1) Parts of paper appear to be LLM-written (for instance: L281-L304). Itself it is not a bad thing, yet those parts of paper lack substance and sometimes repeat the same information.
Review Point: 2) This paper would benefit from more experiments on decoder models (in addition to existing experiments on encoder models). This would make the paper even more relevant. Indeed, in the introduction, authors cite OpenAI's techincal report as evidence that LLMs have shown substantial performance across domains. It would be naturally to include said LLMs (maybe not OpenAI's LLMs, but generally) into the scope of this paper.
Review Point: 3) A reasonably-sized limitations section would also benefit the paper.
==================================================

Focused review:

1. While it is interesting to explore different ways of constructing prediction sets beyond a simple thresholding criterion, CP offers a straightforward justification rooted in statistical hypothesis testing for rejecting a label from the set. It is unclear what statistical reasoning the proposed approach uses to reject a hypothesis (i.e., a label being in the prediction set) that traditional hypothesis testing could not reject.
2. In general, CP is a popular framework due to its ability to reason and provide finite-sample guarantees. Asymptotic results, while potentially useful when fundamental assumptions like exchangeability are violated, may not be particularly interesting.
3. I don't necessarily agree that having an empty set in CP is better than having a set with multiple labels but no coverage. If CP serves as an uncertainty representation tool, we naturally expect to see relative comparisons between the set sizes for different instances. Thus, a set with three labels is more uncertain than one with two labels, even if none of them cover the ground truth.

Review Point: 1. While it is interesting to explore different ways of constructing prediction sets beyond a simple thresholding criterion, CP offers a straightforward justification rooted in statistical hypothesis testing for rejecting a label from the set. It is unclear what statistical reasoning the proposed approach uses to reject a hypothesis (i.e., a label being in the prediction set) that traditional hypothesis testing could not reject.
Review Point: 2. In general, CP is a popular framework due to its ability to reason and provide finite-sample guarantees. Asymptotic results, while potentially useful when fundamental assumptions like exchangeability are violated, may not be particularly interesting.
Review Point: 3. I don't necessarily agree that having an empty set in CP is better than having a set with multiple labels but no coverage. If CP serves as an uncertainty representation tool, we naturally expect to see relative comparisons between the set sizes for different instances. Thus, a set with three labels is more uncertain than one with two labels, even if none of them cover the ground truth.
==================================================

Focused review:

: The greatest weaknesses, with respect to ACL are that 1) readability scores are of limited interest within the field of computational linguistics. While they are somewhat useful in educational and public communication fields, their impact on the progress of computational linguistics is limited. A minor weakness is in the writing: the paper has numerous minor grammatical errors.
Although the discussion compares the performance of the PDS1 and PDW1 features from the previous work, it is unclear how poorly the previous readability measures perform, relevant to the one developed here, for practical purposes.
- General Discussion: This paper would be a stronger candidate for inclusion if the corpus (and importantly, labels developed) were released. It could be used more widely than the development of scalar readability metrics, and would enable (e.g.) investigation of application of more powerful feature-selection methods.

Review Point: 1) readability scores are of limited interest within the field of computational linguistics. While they are somewhat useful in educational and public communication fields, their impact on the progress of computational linguistics is limited. A minor weakness is in the writing: the paper has numerous minor grammatical errors. Although the discussion compares the performance of the PDS1 and PDW1 features from the previous work, it is unclear how poorly the previous readability measures perform, relevant to the one developed here, for practical purposes.
Review Point: -General Discussion: This paper would be a stronger candidate for inclusion if the corpus (and importantly, labels developed) were released. It could be used more widely than the development of scalar readability metrics, and would enable (e.g.) investigation of application of more powerful feature-selection methods.
==================================================

Focused review:

Overall this is an excellent submission. Congratulations! The following is an attempt to find some weaknesses as this section requires me to: - Not all RL environments can be “reset” by reaching a state from which the task execution is challenging. For example, pouring water into a container can be very challenging if the agent spills the water on the floor first. What are your thoughts on the limitations of the approach and in general, safe learning while also trying to find challenging states to reset to? I suggest some discussion of this in the paper. - The ablation conditions could be explained more. I am still a bit unsure what the “Multiple-Resetters” ablation is exactly. - Did you attempt any other hierarchical tasks, within the ant environment or on the other Mujoco ones?

Review Point: - Not all RL environments can be “reset” by reaching a state from which the task execution is challenging. For example, pouring water into a container can be very challenging if the agent spills the water on the floor first. What are your thoughts on the limitations of the approach and in general, safe learning while also trying to find challenging states to reset to? I suggest some discussion of this in the paper.
Review Point: - The ablation conditions could be explained more. I am still a bit unsure what the “Multiple-Resetters” ablation is exactly.
Review Point: - Did you attempt any other hierarchical tasks, within the ant environment or on the other Mujoco ones?
==================================================

Focused review:

- Although the paper report interesting findings, I cannot clarify the main contribution.
- As far as I understand, the contribution seems to be more into their proposed method to restore the text.
- If so, more analysis and ablations are required for the evaluation
- For small length of tokens, the performance is much better compared to the baseline, but I cannot find the explanation.
- Discussion on Table 4 and Table 5 is missing
- Furthermore, there is no clear reason for not comparing with other embedding inversion methods and the methods mentioned in related work section; because of this, I cannot convince how much Vec2Text is effective.
- Without reasonable baseline, people could consider the performance as a result of the further training process.
- On the other hand, the Gaussian noise method weakens the contribution of vec2text because the method is not robust on the noise, which means not generalized well.
In conclusion, I personally feel the findings interesting but the presentation of current version is weak.

Review Point: - Although the paper report interesting findings, I cannot clarify the main contribution.
Review Point: - As far as I understand, the contribution seems to be more into their proposed method to restore the text.
Review Point: - If so, more analysis and ablations are required for the evaluation - For small length of tokens, the performance is much better compared to the baseline, but I cannot find the explanation.
Review Point: - Discussion on Table 4 and Table 5 is missing - Furthermore, there is no clear reason for not comparing with other embedding inversion methods and the methods mentioned in related work section; because of this, I cannot convince how much Vec2Text is effective.
Review Point: - Without reasonable baseline, people could consider the performance as a result of the further training process.
Review Point: - On the other hand, the Gaussian noise method weakens the contribution of vec2text because the method is not robust on the noise, which means not generalized well. In conclusion, I personally feel the findings interesting but the presentation of current version is weak.
==================================================

Focused review:

1. The applicability of the proposed method is limited, as its effectiveness has only been verified on tasks such as multiple-choice questions. How about its performance on other video understanding-related tasks (open-ended VideoQA or text generation), such as action recognition, text-video localization, temporal reasoning tasks, and prediction-related tasks?
2. Although the authors claim to use coarse-to-fine hierarchical feature extraction, essentially, it still involves aggregation at the video frame level. This will prevent the model from effectively extracting fine-grained information within video frames, thereby limiting its performance on finer-grained video understanding tasks.
3. This algorithm requires multiple uses of LLM or VLM. Given the limitations of LLMs or VLMs, such as severe hallucination issues, how do the authors ensure the accuracy of the results obtained each time? For example, in Relevance Scoring, on one hand, Cap(.) is used to obtain captions for keyframes. How can we ensure that critical information is not lost? Additionally, using an LLM to judge relevance to the query, how can we ensure the accuracy of this relevance judgment? Furthermore, is it appropriate to filter and aggregate all video content based solely on the query? For instance, can a simple question like "Please describe the video content" be answered accurately?
4. This method involves a large number of hyperparameters.

Review Point: 1. The applicability of the proposed method is limited, as its effectiveness has only been verified on tasks such as multiple-choice questions. How about its performance on other video understanding-related tasks (open-ended VideoQA or text generation), such as action recognition, text-video localization, temporal reasoning tasks, and prediction-related tasks?
Review Point: 2. Although the authors claim to use coarse-to-fine hierarchical feature extraction, essentially, it still involves aggregation at the video frame level. This will prevent the model from effectively extracting fine-grained information within video frames, thereby limiting its performance on finer-grained video understanding tasks.
==================================================

Focused review:

1)	My main concern is that all the experiments in this paper are conducted on color-related questions, so the conclusions may not be applicable to all VQA examples. If the authors believe that the experimental results on color-related questions can represent other VQA examples, I hope they can provide experimental evidence to support this.
2)	Did the authors verify the hypothesis mentioned in line 225 of section 3.2? If so, I hope they can provide this evidence in the paper.
3)	I am curious about the specific form of the probability function p in Equation 6 and 7.
4)	What do the green, blue, and pink colors in Figure 1 represent? I hope the authors can add a necessary legend or explanation to the figure.
5)	There are several spelling errors in the text, such as "Figure 3" in line 368, which I suspect is a typo and should actually refer to "Figure 5."

Review Point: 1) My main concern is that all the experiments in this paper are conducted on color-related questions, so the conclusions may not be applicable to all VQA examples. If the authors believe that the experimental results on color-related questions can represent other VQA examples, I hope they can provide experimental evidence to support this.
Review Point: 2) Did the authors verify the hypothesis mentioned in line 225 of section 3.2? If so, I hope they can provide this evidence in the paper.
Review Point: 3) I am curious about the specific form of the probability function p in Equation 6 and 7.
Review Point: 4) What do the green, blue, and pink colors in Figure 1 represent? I hope the authors can add a necessary legend or explanation to the figure.
==================================================

Focused review:

While the paper makes significant contributions, there are areas that could be improved:
- Lack of Intuitive Explanation: It is challenging to develop an intuition for why ResNets behave differently from MLPs. Providing more intuitive explanations or illustrative examples before introducing the mathematical formalism would help readers grasp the core concepts and follow the subsequent analysis more effectively.
- Limited Architectural Comparison: The focus on ResNets without discussing other architectures like AlexNet leaves some gaps. Clarifying whether the observed behaviors are due to specific features like skip connections or are common across different architectures would strengthen the generality of the findings.

Review Point: - Lack of Intuitive Explanation: It is challenging to develop an intuition for why ResNets behave differently from MLPs. Providing more intuitive explanations or illustrative examples before introducing the mathematical formalism would help readers grasp the core concepts and follow the subsequent analysis more effectively.
Review Point: - Limited Architectural Comparison: The focus on ResNets without discussing other architectures like AlexNet leaves some gaps. Clarifying whether the observed behaviors are due to specific features like skip connections or are common across different architectures would strengthen the generality of the findings.
==================================================

Focused review:

1. The method is not novel. It totally follows the method in ITI [1] without considering the properties of safety itself, which influence the accuracy of the analysis. (1) As noted in the abstract (line 14), only 10 sentences can compromise the models' safety mechanisms. However, as illustrated in line 193, the safety components in LLMs are composed of three-fourths for both LLama3-8b and LLama3-70b. If the safety mechanism is easily breached, it may be due to a relatively small number of parameters. (2) The head is not a fine-grained aspect of analysis, as there are papers exploring neuron-level safety mechanisms [2]. Additionally, the feed-forward layer is important for safety, as models may refuse to extract harmful knowledge and only extract safety knowledge such as "Apologize I can not ....." [3]. However, the paper ignores this and focuses solely on heads in self-attention.
2. The analysis is not comprehensive. Figure 2 examines the "safety direction" at layer 31, the final layer of LLama3-8B. This analysis is biased, as it only evaluates the output of different models. It's expected that embeddings will have some relation or differences because the models' outputs vary. What would be more valuable is an analysis of the intermediate layers, exploring how the mechanism identifies and mitigates harmful information. However, the paper lacks further analysis, which would have been interesting.
3. The experiments in Table 1 lack baselines, and the baselines in Figure 5 are all trivial. It’s important to compare lightweight defense methods, such as [4], to demonstrate the effectiveness of your approach.
[1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model, NeurIPS 2023
[2] Finding Safety Neurons in Large Language Models, Arxiv 2024
[3] Transformer Feed-Forward Layers Are Key-Value Memories, ACL 2021
[4] Improving Alignment and Robustness with Circuit Breakers, NeurIPS 2024

Review Point: 2. The analysis is not comprehensive. Figure 2 examines the "safety direction" at layer 31, the final layer of LLama3-8B. This analysis is biased, as it only evaluates the output of different models. It's expected that embeddings will have some relation or differences because the models' outputs vary. What would be more valuable is an analysis of the intermediate layers, exploring how the mechanism identifies and mitigates harmful information. However, the paper lacks further analysis, which would have been interesting.
Review Point: 3. The experiments in Table 1 lack baselines, and the baselines in Figure 5 are all trivial. It’s important to compare lightweight defense methods, such as [4], to demonstrate the effectiveness of your approach. [1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model, NeurIPS 2023 [2] Finding Safety Neurons in Large Language Models, Arxiv 2024 [3] Transformer Feed-Forward Layers Are Key-Value Memories, ACL 2021 [4] Improving Alignment and Robustness with Circuit Breakers, NeurIPS 2024
==================================================

Focused review:

1. The architecture of PASSAT, particularly the internal workings of the spherical GNN and the interaction between the “interaction branch” and “velocity branch,” is not sufficiently detailed. A more detailed architectural diagram and step-by-step explanation would make the model’s structure and operation easier to understand and potentially reproduce.
2. The combination of a spherical GNN and Navier-Stokes equations implies high computational demands. The paper does not provide benchmarks for computational efficiency, which is crucial for real-world applications.
3. The benchmarks should include the SOTA Numerical Weather Prediction (NWP) models, which are the industry standard for accuracy and robustness in weather forecasting.
4. While Navier-Stokes equations are employed to capture fluid dynamics on a global scale, the work does not clarify how it manages varying boundary conditions across different geographical or atmospheric regions.
5. **(Major concern)** Although the paper employs the Navier-Stokes (NS) equations for physical realism, it does not provide a theoretical justification for their selection over other possible fluid dynamics models. Moreover, it is unclear how PASSAT addresses the variability and abrupt changes often observed in real-world data, which may not always align with the idealized assumptions of the NS framework. For instance, sudden atmospheric shifts and noise present in observational data could challenge the applicability and robustness of the NS-based approach in capturing highly dynamic or turbulent weather events.

Review Point: 1. The architecture of PASSAT, particularly the internal workings of the spherical GNN and the interaction between the “interaction branch” and “velocity branch,” is not sufficiently detailed. A more detailed architectural diagram and step-by-step explanation would make the model’s structure and operation easier to understand and potentially reproduce.
Review Point: 2. The combination of a spherical GNN and Navier-Stokes equations implies high computational demands. The paper does not provide benchmarks for computational efficiency, which is crucial for real-world applications.
Review Point: 3. The benchmarks should include the SOTA Numerical Weather Prediction (NWP) models, which are the industry standard for accuracy and robustness in weather forecasting.
Review Point: 4. While Navier-Stokes equations are employed to capture fluid dynamics on a global scale, the work does not clarify how it manages varying boundary conditions across different geographical or atmospheric regions.
Review Point: 5. **(Major concern)** Although the paper employs the Navier-Stokes (NS) equations for physical realism, it does not provide a theoretical justification for their selection over other possible fluid dynamics models. Moreover, it is unclear how PASSAT addresses the variability and abrupt changes often observed in real-world data, which may not always align with the idealized assumptions of the NS framework. For instance, sudden atmospheric shifts and noise present in observational data could challenge the applicability and robustness of the NS-based approach in capturing highly dynamic or turbulent weather events.
==================================================

Focused review:

* While Section 3.2 provides empirical evidence, the theoretical understanding of why CoTFormer works better could be deeper. Through analysis of attention patterns, we observe that tokens in later repeats tend to focus heavily on earlier representations that capture key contextual information, suggesting the model learns to leverage complementary features detected at different processing stages. This selective attention to informative past representations may help explain why CoTFormer outperforms the baseline Block Universal Transformer, where such cross-repeat attention patterns are not possible.
* Could better connect to recent theoretical work on transformer expressivity discussed in Section 2.
* The sequence lengths that are used for training (256) are quite short relative to the lengths that are used for training modern language models and are shorter relative to common LLM evals and typical chatbot conversations.
* Performance gap between adaptive and fixed-depth CoTFormers under same compute budget (Section 5)
* Training efficiency of deeper layers could be improved (e.g. increasing the gradient information during adaptive training), as shown by the analysis of router weights distribution (Figure 5)

Review Point: * While Section 3.2 provides empirical evidence, the theoretical understanding of why CoTFormer works better could be deeper. Through analysis of attention patterns, we observe that tokens in later repeats tend to focus heavily on earlier representations that capture key contextual information, suggesting the model learns to leverage complementary features detected at different processing stages. This selective attention to informative past representations may help explain why CoTFormer outperforms the baseline Block Universal Transformer, where such cross-repeat attention patterns are not possible.
Review Point: * Could better connect to recent theoretical work on transformer expressivity discussed in Section 2.
Review Point: * The sequence lengths that are used for training (256) are quite short relative to the lengths that are used for training modern language models and are shorter relative to common LLM evals and typical chatbot conversations.
Review Point: * Performance gap between adaptive and fixed-depth CoTFormers under same compute budget (Section 5) * Training efficiency of deeper layers could be improved (e.g. increasing the gradient information during adaptive training), as shown by the analysis of router weights distribution (Figure 5)
==================================================

Focused review:

* I believe some descriptions in the paper need to be refined for greater precision. For instance, in Section 3, the statement, “when the number of local update steps is set to $K=1$ , the entire federated learning process reduces to the standard Lion algorithm,” is somewhat misleading. Even with $K=1$ , there is still a slight mismatch between the standard Lion algorithm and the proposed FedSMU.
* Remark 4.5 is somewhat confusing. The authors state, "we use a 1-bit quantization compression method. If a higher-bit compression (e.g., $\alpha$-bit) is used, the additional coefficient α will further slow down the overall convergence rate." This conclusion seems non-intuitive and not straightforward, as there is no clear indication in the theorem of how the convergence rate is influenced by $\alpha$. Typically, using higher-bit compression results in less compression loss, which would suggest improved performance rather than a slower convergence rate.
* It is unclear to me why $\tau_{\max}$ is used as an indicator for studying partial participation settings. Could you provide the practical value of $\tau_{\max}$ used in your experiments? Is there a specific reason for choosing this $\tau_{\max}$ indicator instead of the more common “sampling ratio $n/m$” (sampling $n$ out of $m$ clients), aside from practical considerations? It seems that in the experimental section, the participation ratio is also used.
* Given that Lion is an adaptive optimization strategy, it is important to discuss and compare it with other adaptive optimization methods, particularly those focused on communication efficiency, such as [1] or communication-efficient versions of AdamW in FL.
[1] Wang, Y., Lin, L., and Chen, J. Communication-efficient adaptive federated learning.

Review Point: * I believe some descriptions in the paper need to be refined for greater precision. For instance, in Section 3, the statement, “when the number of local update steps is set to $K=1$ , the entire federated learning process reduces to the standard Lion algorithm,” is somewhat misleading. Even with $K=1$ , there is still a slight mismatch between the standard Lion algorithm and the proposed FedSMU.
Review Point: * Remark 4.5 is somewhat confusing. The authors state, "we use a 1-bit quantization compression method. If a higher-bit compression (e.g., $\alpha$-bit) is used, the additional coefficient α will further slow down the overall convergence rate." This conclusion seems non-intuitive and not straightforward, as there is no clear indication in the theorem of how the convergence rate is influenced by $\alpha$. Typically, using higher-bit compression results in less compression loss, which would suggest improved performance rather than a slower convergence rate.
Review Point: * It is unclear to me why $\tau_{\max}$ is used as an indicator for studying partial participation settings. Could you provide the practical value of $\tau_{\max}$ used in your experiments? Is there a specific reason for choosing this $\tau_{\max}$ indicator instead of the more common “sampling ratio $n/m$” (sampling $n$ out of $m$ clients), aside from practical considerations? It seems that in the experimental section, the participation ratio is also used.
Review Point: * Given that Lion is an adaptive optimization strategy, it is important to discuss and compare it with other adaptive optimization methods, particularly those focused on communication efficiency, such as [1] or communication-efficient versions of AdamW in FL. [1] Wang, Y., Lin, L., and Chen, J. Communication-efficient adaptive federated learning.
==================================================

Focused review:

1. There are many existing works studying the generalization of centralized and decentralized federated learning algorithms. However, this paper missed a lot of them. Then, it is not clear what new contributions this paper has. e.g.,is the bound of this paper comparable with existing ones?
[1] https://openreview.net/pdf?id=-EHqoysUYLx
[2] https://arxiv.org/abs/2306.02939
2. This paper does not show how the heterogeneity affects the generalization error.
3. When the communication graph is fully connected, DFedAvg becomes FedAvg. But the generalization error of this paper does not have this relationship.

Review Point: 1. There are many existing works studying the generalization of centralized and decentralized federated learning algorithms. However, this paper missed a lot of them. Then, it is not clear what new contributions this paper has. e.g.,is the bound of this paper comparable with existing ones? [1] https://openreview.net/pdf?id=-EHqoysUYLx [2] https://arxiv.org/abs/2306.02939 2. This paper does not show how the heterogeneity affects the generalization error.
Review Point: 3. When the communication graph is fully connected, DFedAvg becomes FedAvg. But the generalization error of this paper does not have this relationship.
==================================================

Focused review:

- The comparison experiment setup is unclear. Were the same data conditions used the experiment section? (see Q1)
- The core of this paper is to point out the shortages of existing MI attacks on foundation models. However, in the introduction, the discussion does not revolve around this point but rather focuses on how simple attacks can also achieve good results. It is recommended to revise the structure of the introduction to highlight the main contributions of the paper.
- The experimental section is divided into sections based on the datasets, which makes it difficult to correspond with the previously mentioned common reasons for the intrinsic differences. This hinders the reader's understanding of the experiments and the paper's arguments.

Review Point: - The comparison experiment setup is unclear. Were the same data conditions used the experiment section? (see Q1) - The core of this paper is to point out the shortages of existing MI attacks on foundation models. However, in the introduction, the discussion does not revolve around this point but rather focuses on how simple attacks can also achieve good results. It is recommended to revise the structure of the introduction to highlight the main contributions of the paper.
Review Point: - The experimental section is divided into sections based on the datasets, which makes it difficult to correspond with the previously mentioned common reasons for the intrinsic differences. This hinders the reader's understanding of the experiments and the paper's arguments.
==================================================

Focused review:

- Relying on rule-based expert policies for demonstration collection can limit the complexity of the learned behaviors. Exploring methods for learning these demonstrations or incorporating human demonstrations could lead to more robust and generalizable policies.
- The source code and dataset have not been released.
- Environmental specifications, including frame rate and API details, are not provided.
- The paper primarily compares against affordance-based methods. A comparison to other offline imitation learning methods (e.g., [1]) and online learning-from-demo methods (e.g., [2][3][4]) would provide a more complete picture of the method's performance.
[1] Zhao, Tony Z., et al. "Learning fine-grained bimanual manipulation with low-cost hardware." arXiv preprint arXiv:2304.13705 (2023).
[2] Ball, Philip J., et al. "Efficient online reinforcement learning with offline data." International Conference on Machine Learning. PMLR, 2023.
[3] Rajeswaran, Aravind, et al. "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations." arXiv preprint arXiv:1709.10087 (2017).
[4] Ho, Jonathan, and Stefano Ermon. "Generative adversarial imitation learning." Advances in neural information processing systems 29 (2016).

Review Point: - Relying on rule-based expert policies for demonstration collection can limit the complexity of the learned behaviors. Exploring methods for learning these demonstrations or incorporating human demonstrations could lead to more robust and generalizable policies.
Review Point: - The source code and dataset have not been released.
Review Point: - Environmental specifications, including frame rate and API details, are not provided.
==================================================

Focused review:

1. There are generally three metrics for evaluating point process models: log-likelihood, accuracy (acc), and root mean square error (RMSE) [1]. Among these, log-likelihood measures the model’s goodness-of-fit, while accuracy and RMSE measure the model’s event prediction performance. This paper only uses log-likelihood. Furthermore, in terms of log-likelihood, the proposed method does not demonstrate a significant advantage over other baseline models.
2. Equation 8 seems to imply an assumption that the influence between events is always a positive excitation (because the softplus function is applied to all components, including W , K(t), and μ_k). What if the influence of events is "inhibition" rather than "excitation"?
3. The neural kernel function seems capable of modeling only the influence of one event on another, but in some scenarios, multiple events occurring together may be required to trigger a subsequent event, as in the case of synergy [2]. Reference:
[1] EASYTPP: TOWARDS OPEN BENCHMARKING TEMPORAL POINT PROCESSES (ICLR'24)
[2] CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods (ICML'20)

Review Point: 1. There are generally three metrics for evaluating point process models: log-likelihood, accuracy (acc), and root mean square error (RMSE) [1]. Among these, log-likelihood measures the model’s goodness-of-fit, while accuracy and RMSE measure the model’s event prediction performance. This paper only uses log-likelihood. Furthermore, in terms of log-likelihood, the proposed method does not demonstrate a significant advantage over other baseline models.
Review Point: 2. Equation 8 seems to imply an assumption that the influence between events is always a positive excitation (because the softplus function is applied to all components, including W , K(t), and μ_k). What if the influence of events is "inhibition" rather than "excitation"?
Review Point: 3. The neural kernel function seems capable of modeling only the influence of one event on another, but in some scenarios, multiple events occurring together may be required to trigger a subsequent event, as in the case of synergy [2]. Reference: [1] EASYTPP: TOWARDS OPEN BENCHMARKING TEMPORAL POINT PROCESSES (ICLR'24) [2] CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods (ICML'20)
==================================================

Focused review:

- The idea is not completely novel. Adversarial attacks in combination with class activation mappings have for example been discussed in [1]. However, the authors use it for robustifying their models which is in my opinion sufficiently different. Nonetheless, authors should include a citation of that work.
- The empirical evaluation can be extended by using different adversarial attacks, e.g., Carlini-Wagner attack or AutoAttack.
- The literature review seems somewhat short. I suggest authors spend more time looking for relevant related works.
- Performence of the PART method is somewhat underwhelming. The improvement is only incremental (usually only in the range of ~1%).
- Section 3.1 “AE generation process.” is tough to read. Authors should work on the presentation of that section. Maybe a small table on the side would help to introduce the notation.
- Figure 4: Authors should mention what is indicated by the shaded areas.
Overall, the ideas in this paper are not ground-breaking, but the solid theoretical and empirical analysis justify its publication in ICLR, which is why I recommend to accept this submission.
Minor details:
- Missing whitespace “Table 2: Robustness(%) of…”
- Eq. 12-15 “subject to” should not be typeset in math mode
- Lemma 1: “(i).” unusual period References
[1] Dong, Xiaoyi, et al. "Robust superpixel-guided attentional adversarial attack." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.

Review Point: - The idea is not completely novel. Adversarial attacks in combination with class activation mappings have for example been discussed in [1]. However, the authors use it for robustifying their models which is in my opinion sufficiently different. Nonetheless, authors should include a citation of that work.
Review Point: - The empirical evaluation can be extended by using different adversarial attacks, e.g., Carlini-Wagner attack or AutoAttack.
Review Point: - The literature review seems somewhat short. I suggest authors spend more time looking for relevant related works.
Review Point: - Performence of the PART method is somewhat underwhelming. The improvement is only incremental (usually only in the range of ~1%).
Review Point: - Section 3.1 “AE generation process.” is tough to read. Authors should work on the presentation of that section. Maybe a small table on the side would help to introduce the notation.
Review Point: - Figure 4: Authors should mention what is indicated by the shaded areas. Overall, the ideas in this paper are not ground-breaking, but the solid theoretical and empirical analysis justify its publication in ICLR, which is why I recommend to accept this submission. Minor details:
Review Point: - Missing whitespace “Table 2: Robustness(%) of…” - Eq. 12-15 “subject to” should not be typeset in math mode - Lemma 1: “(i).” unusual period References [1] Dong, Xiaoyi, et al. "Robust superpixel-guided attentional adversarial attack." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
==================================================

Focused review:

* Lack of novelty: The paper presents a method closely related to V2X-ViT with two main modifications: a parallel architecture for multi-agent and spatial attention instead of a sequential setup, and a replacement of window attention with a CNN-based S-Conv and Dilated Neighborhood Attention Transformer for S-Att. However, the novelty and justification for these modifications appear limited, and key design choices lack sufficient exploration.
* Motivation is insufficient: The motivation for using a parallel rather than sequential architecture for better robustness against noise for perception module is insufficient, with no clear rationale for improved noise handling or scalability.
* The tested heading angle range is narrow, making it unrealistic for real-world applications. Please consider increasing the experimented value range for more realistic setups.

Review Point: * Lack of novelty: The paper presents a method closely related to V2X-ViT with two main modifications: a parallel architecture for multi-agent and spatial attention instead of a sequential setup, and a replacement of window attention with a CNN-based S-Conv and Dilated Neighborhood Attention Transformer for S-Att. However, the novelty and justification for these modifications appear limited, and key design choices lack sufficient exploration.
Review Point: * Motivation is insufficient: The motivation for using a parallel rather than sequential architecture for better robustness against noise for perception module is insufficient, with no clear rationale for improved noise handling or scalability.
Review Point: * The tested heading angle range is narrow, making it unrealistic for real-world applications. Please consider increasing the experimented value range for more realistic setups.
==================================================

Focused review:

1. In table 2, while authors demonstrate the improvements over ImageBind on T2V and V2T tasks, these two models are trained with different backbones, model initializations, finetuning techniques, and training data. This leads to an unfair comparison, especially considering the proposed model is leveraging more video data.
2. Based on my understanding, LanguageBind is initialized from OpenCLIP and continues to train on the VIDAL-10M dataset. Compared to OpenCLIP, it is difficult to tell whether the performance improvement comes from the proposed dataset or the new pretraining paradigm.
3. In table 4, do the authors have any intuition why raw caption works best for the Infrared modality?

Review Point: 1. In table 2, while authors demonstrate the improvements over ImageBind on T2V and V2T tasks, these two models are trained with different backbones, model initializations, finetuning techniques, and training data. This leads to an unfair comparison, especially considering the proposed model is leveraging more video data.
Review Point: 2. Based on my understanding, LanguageBind is initialized from OpenCLIP and continues to train on the VIDAL-10M dataset. Compared to OpenCLIP, it is difficult to tell whether the performance improvement comes from the proposed dataset or the new pretraining paradigm.
Review Point: 3. In table 4, do the authors have any intuition why raw caption works best for the Infrared modality?
==================================================

Focused review:

This paper exhibits several Weaknesses:
1.There is a lack of generalization verification across various types of video diffusion models, such as VideoCrafter, EmuVideo (based on the U-Net structure), and CogVideoX-I2V (based on Diffusion Transformer).
2.The issue of inconsistent features across frames, as discussed in the introduction, may not be universally applicable. This problem might not be evident in some of the latest 3D full attention-based video diffusion models (e.g., OpenSoraPlan 1.2, EasyAnimate V4, and CogVideoX) or when using VideoVAE.
3.There are errors in the formula expressions:
1)The symbols in Formula 1 are confusing. Does Fn represent an operation or a feature map? How does Bb,n (denoting [h, w, x, y]) perform matrix operations with feature maps (shape of [h, w, d])? What does “[Bb,n]” signify?
2)There are two instances of zt* in formula 2.
4.In line 194, the author emphasizes "Yet, all of these methods focus on text-based generation." However, MOFT[1] is also training-free and can achieve image-to-video motion control. In addition, MOFT incorporates the concept of optimizing denoising latents. Can the authors elaborate on the similarities and differences compared to MOFT in terms of functionality and methodology?
[1] Video Diffusion Models are Training-free Motion Interpreter and Controller.

Review Point: 1.There is a lack of generalization verification across various types of video diffusion models, such as VideoCrafter, EmuVideo (based on the U-Net structure), and CogVideoX-I2V (based on Diffusion Transformer).
Review Point: 2.The issue of inconsistent features across frames, as discussed in the introduction, may not be universally applicable. This problem might not be evident in some of the latest 3D full attention-based video diffusion models (e.g., OpenSoraPlan 1.2, EasyAnimate V4, and CogVideoX) or when using VideoVAE.
Review Point: 1)The symbols in Formula 1 are confusing. Does Fn represent an operation or a feature map? How does Bb,n (denoting [h, w, x, y]) perform matrix operations with feature maps (shape of [h, w, d])? What does “[Bb,n]” signify?
Review Point: 4.In line 194, the author emphasizes "Yet, all of these methods focus on text-based generation." However, MOFT[1] is also training-free and can achieve image-to-video motion control. In addition, MOFT incorporates the concept of optimizing denoising latents. Can the authors elaborate on the similarities and differences compared to MOFT in terms of functionality and methodology? [1] Video Diffusion Models are Training-free Motion Interpreter and Controller.
==================================================

Focused review:

- the Wasserstein distortion as introduced in section 2 is not really new and is in fact very close to the work of Freeman et al (2012) the main innovation being that the parameter sigma can freely be fixed at any position in the image instead of being constrained by the eccentricity of the visual receptive fields.
- the maths of section 2 are overly complicated for a naive reader : in the end the authors use discrete optimal transport between empirical distributions and assume Gaussiannity which in summary corresponds to adjust the mean and standard dev of the local features to a new image (initialized from a white noise image) to the mean and standard dev of the local features of an exemplar image.
- the pooling distribution $q_\sigma$ corresponds to a local weighting of the statistics with width $\sigma$ (as in Freeman 2012).
- when section 2 is well understood the theoretical results become trivial : (i) in the large $\sigma$ limit this is standard texture synthesis framework (Portilla-Simoncelli, Gatys). The specific setting of equation (12) has been empirically evaluated for texture synthesis against Gatys and Portilla-Simoncelli by Vacher et al (Neurips 2020). (ii) in the small $\sigma$ limit this is the exact reconstruction of an image from its feature
- the numerical experiments illustrate the nature of sigma but I do not see in which it contributes to vision study (even in terms of methods)

Review Point: - the Wasserstein distortion as introduced in section 2 is not really new and is in fact very close to the work of Freeman et al (2012) the main innovation being that the parameter sigma can freely be fixed at any position in the image instead of being constrained by the eccentricity of the visual receptive fields.
Review Point: - the maths of section 2 are overly complicated for a naive reader : in the end the authors use discrete optimal transport between empirical distributions and assume Gaussiannity which in summary corresponds to adjust the mean and standard dev of the local features to a new image (initialized from a white noise image) to the mean and standard dev of the local features of an exemplar image.
Review Point: - the pooling distribution $q_\sigma$ corresponds to a local weighting of the statistics with width $\sigma$ (as in Freeman 2012).
Review Point: - when section 2 is well understood the theoretical results become trivial : (i) in the large $\sigma$ limit this is standard texture synthesis framework (Portilla-Simoncelli, Gatys). The specific setting of equation (12) has been empirically evaluated for texture synthesis against Gatys and Portilla-Simoncelli by Vacher et al (Neurips 2020). (ii) in the small $\sigma$ limit this is the exact reconstruction of an image from its feature - the numerical experiments illustrate the nature of sigma but I do not see in which it contributes to vision study (even in terms of methods)
==================================================

Focused review:

1. I am very concerned about the insufficient related work section. It is very short and does not cover many important aspects. For example, there are many other existing scientific literature database, arXiv, pmc, pubmed, semantic scholar, with much larger scale (over millions of articles), but they are not cited. There have been many advancements in the field of neural information retrieval with state-of-the-art embedding models such as OpenAI-v3, Grit-LM, Rank-Llama, but the authors choose to use a fairly out-of-date embedding model (sentence-bert) with a much shorter context window. There are also many works on using LLM directly to retrieve, search and select scientific literature such as RankGPT, this method is compared with other GPT-based retrieval strategies in BIRCO, a benchmark that includes a scientific literature benchmark, DORIS-MAE. None of these works are mentioned. At least some of them should be used as baseline models to compare with the paper's proposed retrieval strategy.
2. The second concern is the scale and comprehensiveness of this work. This work only focuses on a very narrow field in computer science, namely AI/ML, and its experiment setting is on an even narrower field, NLP. I would be fine with these settings except for the fact that the authors named their paper "scientific paper idea proposer". The word scientific implies a much larger domain, including biomedicine, physics, math, computer science and other quantitative sciences. Because the scale of this work is severely limited, I am doubtful for its contribution to the research community. FYI, these limitations about scales and lack of comprehensiveness should be openly acknowledged in the limitation section (which is also insufficient and very short).
3. The third concern is the lack of originality of this work. It is true that the problem this paper is trying to tackle is very interesting and challenging (i.e. scientific idea creation). However, this paper does not use sufficiently innovative approaches to solve it. The entire retrieval strategy is an ensemble of previously utilized techniques, such as entity matching, semantic embedding search, co-citation (this is implemented in many existing databases such as PubMed), and clustering. The idea proposal pipeline is only an implementation but with limited originality. In my opinion the most original aspect of this paper is its introduction of the annotated database where each paper has its ideas extracted and processed, though the paper does not talk about quantitative quality control of the database creation process.
4. I have some questions/doubts about the evaluation section in the paper, I will reserve them to the question section.

Review Point: 4. I have some questions/doubts about the evaluation section in the paper, I will reserve them to the question section.
==================================================

Focused review:

1.	Strong Assumptions:The main result relies on restrictive assumptions about LDS structures and eigenvalue constraints (e.g., eigenvalues must fall within “good” ranges), which may limit the algorithm’s applicability in situations that do not strictly follow LDS dynamics or contain eigenvalues outside these ranges.
2.	Limited Empirical Validation: The empirical scope is narrow, with experiments focused primarily on synthetic LDS data and a single deep learning task. This raises concerns about the method’s practical utility and performance in diverse real-world settings.
3.	Conceptual Novelty and Integration of Existing Ideas: While the application of Asymmetric-Regret bounds to shorter contexts is novel, the overall contribution seems primarily integrative, synthesizing elements from Hazan’s work on spectral filtering, convex optimization, and regret minimization. The empirical and conceptual innovations feel incremental rather than groundbreaking, especially given the limited empirical exploration.

Review Point: 1. Strong Assumptions:The main result relies on restrictive assumptions about LDS structures and eigenvalue constraints (e.g., eigenvalues must fall within “good” ranges), which may limit the algorithm’s applicability in situations that do not strictly follow LDS dynamics or contain eigenvalues outside these ranges.
Review Point: 2. Limited Empirical Validation: The empirical scope is narrow, with experiments focused primarily on synthetic LDS data and a single deep learning task. This raises concerns about the method’s practical utility and performance in diverse real-world settings.
Review Point: 3. Conceptual Novelty and Integration of Existing Ideas: While the application of Asymmetric-Regret bounds to shorter contexts is novel, the overall contribution seems primarily integrative, synthesizing elements from Hazan’s work on spectral filtering, convex optimization, and regret minimization. The empirical and conceptual innovations feel incremental rather than groundbreaking, especially given the limited empirical exploration.
==================================================

Focused review:

- A key limitation is the lack of evaluation on larger-scale datasets comparable to those used by models like CLIP (e.g., 400 million data pairs). The paper acknowledges this but does not include any further experiments or discussions on large-scale datasets to substantiate the scalability of Align-VL. There are publicly available large-scale text-image datasets, such as the LAION dataset, which could have been used to provide additional insights. Including a discussion on potential challenges or a small-scale simulated analysis could strengthen this point.
- The model evaluation is somewhat limited in scope. First, the authors only used the Flickr dataset for evaluation, which is insufficient to assess the model's general performance across a broader range of data. It would be better to include additional datasets, such as MSCOCO, to provide a more comprehensive evaluation. Second, the evaluation was restricted to the image and text retrieval task. Evaluating the model's performance on other tasks, such as zero-shot classification, would provide a more complete understanding of its capabilities.

Review Point: - A key limitation is the lack of evaluation on larger-scale datasets comparable to those used by models like CLIP (e.g., 400 million data pairs). The paper acknowledges this but does not include any further experiments or discussions on large-scale datasets to substantiate the scalability of Align-VL. There are publicly available large-scale text-image datasets, such as the LAION dataset, which could have been used to provide additional insights. Including a discussion on potential challenges or a small-scale simulated analysis could strengthen this point.
Review Point: - The model evaluation is somewhat limited in scope. First, the authors only used the Flickr dataset for evaluation, which is insufficient to assess the model's general performance across a broader range of data. It would be better to include additional datasets, such as MSCOCO, to provide a more comprehensive evaluation. Second, the evaluation was restricted to the image and text retrieval task. Evaluating the model's performance on other tasks, such as zero-shot classification, would provide a more complete understanding of its capabilities.
==================================================

Focused review:

:
1. My main concern is that the contribution may be not enough for this conference. Although the application is novel, the idea of using temporal information to build graphs (and weighted adjacency matrix) is not new in pattern recognition and machine learning areas. In addition, the proposed method seems straightforward.
2. Lack of ablation experiment: While the paper presents the theoretical benefits of the TRW-GCN framework, there is a need for empirical evaluation to demonstrate its tangible benefits. Comparing the performance of GCN with and without TRW on a temporal dataset would provide more evidence of the model's effectiveness.
3. Lack of intuitive visualization of experimental results: The paper presents experimental results to demonstrate the superiority of the TRW-GCN framework, but the visualizations and figures provided are not clear and do not effectively convey the findings. The authors should consider improving the clarity and quality of the figures to make the experimental results more intuitive and easier to interpret.

Review Point: 2. Lack of ablation experiment: While the paper presents the theoretical benefits of the TRW-GCN framework, there is a need for empirical evaluation to demonstrate its tangible benefits. Comparing the performance of GCN with and without TRW on a temporal dataset would provide more evidence of the model's effectiveness.
Review Point: 3. Lack of intuitive visualization of experimental results: The paper presents experimental results to demonstrate the superiority of the TRW-GCN framework, but the visualizations and figures provided are not clear and do not effectively convey the findings. The authors should consider improving the clarity and quality of the figures to make the experimental results more intuitive and easier to interpret.
==================================================

Focused review:

1. Lack of Novelty: This paper applies existing diffusion models to semantic segmentation tasks in label-scarce scenarios, which has already been explored in related works [1-4]. More importantly, compared to these works, the approach proposed here does not exhibit significant technical innovation, particularly in the Layer-Timestep Adaptive Adapter (LT-Adapter) module. This module lacks a clear uniqueness and advantage over existing methods, especially in the context of fine-tuning diffusion models with pseudo-labels, e.g., [3].
2. Unclear contribution: The paper does not fully clarify the specific technical advances that make this work better than [3] and [4]. The improvement of this work may simply come from the additional sampling of the diffusion model on pseudo-labeled images rather than from any innovative technical improvements, and it seems that the [3][4] techniques can also be used to sample on pseudo-labeled data. From this perspective, the comparison in Table 2 does not seem to be entirely fair, because neither [1] nor [2] use pseudo-labeled data for sampling, and even [2] does not fine-tune the diffusion model at all. In addition, there is no clear relationship between LT-Adapter and fine-tuning with pseudo-labels, which makes the contribution of this work seem ambiguous and less influential.
3. Unclear technical details. Moreover, it is difficult to see the improvement in the principle of LT-Adapter's fine-tuning method compared to [3]'s method, and what definite benefits it brings. Pseudo-labels are full of semantic noise. How to ensure semantic inconsistency when using pseudo-labels for sampling?
Overall, this paper seems to be an application of some technologies, with weak innovation, lack of substantial technical advantages and major contributions and fails to convincingly demonstrate the practical value and necessity of this method compared with existing methods.
[1] DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models
[2] Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation
[3] Freestyle layout-to-image synthesis.
[4] Freemask

Review Point: 1. Lack of Novelty: This paper applies existing diffusion models to semantic segmentation tasks in label-scarce scenarios, which has already been explored in related works [1-4]. More importantly, compared to these works, the approach proposed here does not exhibit significant technical innovation, particularly in the Layer-Timestep Adaptive Adapter (LT-Adapter) module. This module lacks a clear uniqueness and advantage over existing methods, especially in the context of fine-tuning diffusion models with pseudo-labels, e.g., [3].
==================================================

Focused review:

The examples provided are simple, which is helpful for understanding the relevant concepts; however it would be nice to demonstrate an application with a (considerably) larger set of variables or dependencies.
It is (somewhat) unclear how often the proposed solution approach will be expected to produce `good’ results in minimizing in minimizing equation (3).
Minor point: the term “representation” appears several times in the manuscript but is given a formal definition on pg. 6. The use of the term may be standard in the particular literature, but it would be constructive to at least reference the definition at first occurrence, or to use a different term whenever that definition is not applicable and “representation” is used more loosely.

Review Point: 6. The use of the term may be standard in the particular literature, but it would be constructive to at least reference the definition at first occurrence, or to use a different term whenever that definition is not applicable and “representation” is used more loosely.
==================================================

Focused review:

- The requirement for humans to decompose problems is a significant precondition. Additionally, humans are tasked with evaluating the LLM-generated plans and executing the tasks, which could be problematic in complex domains.
- The findings regarding the use of multiple sessions are somewhat expected, given that the problem decomposition is already done, significantly reducing the difficulty of the problem for the LLM.
- The details regarding the additional benchmark results are vague, particularly whether they refer to the coordination aspect or the use of multiple sessions.
- Depending on human evaluations to assess plans may be unreliable in complex domains, even when the evaluator is an expert.

Review Point: - The requirement for humans to decompose problems is a significant precondition. Additionally, humans are tasked with evaluating the LLM-generated plans and executing the tasks, which could be problematic in complex domains.
Review Point: - The findings regarding the use of multiple sessions are somewhat expected, given that the problem decomposition is already done, significantly reducing the difficulty of the problem for the LLM.
Review Point: - The details regarding the additional benchmark results are vague, particularly whether they refer to the coordination aspect or the use of multiple sessions.
Review Point: - Depending on human evaluations to assess plans may be unreliable in complex domains, even when the evaluator is an expert.
==================================================

Focused review:

1. The paper provides configuration details for the long-context methods, but it lacks a detailed analysis of how hyperparameters were tuned or their impact on performance.
2. A more thorough analysis of hyperparameter sensitivity could provide insights into the robustness of the proposed methods.
3. Table 4 is a little confusing. Why some highest scores are not bold? And what is the meaning of the underscore?

Review Point: 1. The paper provides configuration details for the long-context methods, but it lacks a detailed analysis of how hyperparameters were tuned or their impact on performance.
Review Point: 2. A more thorough analysis of hyperparameter sensitivity could provide insights into the robustness of the proposed methods.
Review Point: 3. Table 4 is a little confusing. Why some highest scores are not bold? And what is the meaning of the underscore?
==================================================

Focused review:

Some comments
1. In the large-scale evaluation reported, I majorly see MSE loss being used. How does the ODE analysis deviate or negatively impacted by commonly used loss functions for classification tasks? While this might be part of future work I think addressing such limitations in the main body would improve the paper.
2. The analysis relies heavily on the largest eigenvalue estimation for ODE analysis. I think it might be helpful to account for how precise/reliable such estimations are when empirically implemented.
3. I might have missed this but I do not see any ablations on the behavior of $\sigma^2$, that models $\mathbb E[x_t^2]$ (equation 2). Is it because it has a fixed value due to the 3 desiderata described in line 249? In either case, I think it might be good to show its evolution across training to support its expected behavior.
Minor weakness
1. Figure 1: x-axis labels missing. While I can infer what is being plotted I think the figures should be precise on labels for faster understanding.
2. Figure 3, the legend is difficult to read might be possible to increase the font a bit.
3. Figure 4: The gray curves are mostly hard to see. I think it might be helpful to use a color palette that makes the curves more discernable.

Review Point: 2. The analysis relies heavily on the largest eigenvalue estimation for ODE analysis. I think it might be helpful to account for how precise/reliable such estimations are when empirically implemented.
Review Point: 3. I might have missed this but I do not see any ablations on the behavior of $\sigma^2$, that models $\mathbb E[x_t^2]$ (equation 2). Is it because it has a fixed value due to the 3 desiderata described in line 249? In either case, I think it might be good to show its evolution across training to support its expected behavior. Minor weakness 1. Figure 1: x-axis labels missing. While I can infer what is being plotted I think the figures should be precise on labels for faster understanding.
Review Point: 2. Figure 3, the legend is difficult to read might be possible to increase the font a bit.
Review Point: 3. Figure 4: The gray curves are mostly hard to see. I think it might be helpful to use a color palette that makes the curves more discernable.
==================================================

Focused review:

- The writing of the manuscript can be improved. For example, it is suggested to provide the full name of an abbreviated term as it's first used (HD, HCI)
- This manuscript, while addressing an intriguing problem, reads more like a report than a research paper. It examines the capability of MLLMs to interpret images containing structured data. I believe this manuscript might be suitable for another conference that focuses more on the application.
- The comparisons should contain some stronger heuristic methods to set the upper bound of the non-learning-based method.
- As the MLLM relies on the image of the graph, different visualization methods should be examined. Meanwhile, for larger networks and fine-grained tasks (like recommendations on social networks), visualization might be an inefficient or even impossible way to represent the graph.

Review Point: - The writing of the manuscript can be improved. For example, it is suggested to provide the full name of an abbreviated term as it's first used (HD, HCI) - This manuscript, while addressing an intriguing problem, reads more like a report than a research paper. It examines the capability of MLLMs to interpret images containing structured data. I believe this manuscript might be suitable for another conference that focuses more on the application.
Review Point: - The comparisons should contain some stronger heuristic methods to set the upper bound of the non-learning-based method.
Review Point: - As the MLLM relies on the image of the graph, different visualization methods should be examined. Meanwhile, for larger networks and fine-grained tasks (like recommendations on social networks), visualization might be an inefficient or even impossible way to represent the graph.
==================================================

Focused review:

My main concerns are
- This work only evaluates/tackles VLLM instead of MLLM as claimed multiple times in title and throughout paper, though I could maybe see the way to extend to other modalities.
- Having the implicit misleading information generated by GPT-4o seems like a "fighting fire with fire" approach -- I think it is better to have at least a subset of implicit ones written by human annotators so that we can see whether there is any difference between the human-generated ones and GPT-4o generated ones.
- During finetuning, a random set of explicit and implicit misled samples are used for finetuning, yet I am afraid the explicit misleading info has a too obvious and unique pattern due to how it's designed, hence too easy to pick them up, making the improvement after finetuning not too surprising.
- Instead of finetuning, I would recommend the authors to simply systematically prompt the MLLMs, such as "The questions might contain misleading information, you should try to answer the question correctly despite of those misleading information ..."; another version could even give it two examples (one explicit and one implicit). I would guess/assume, simply doing this extra prompting will make the results much better.
- The questions only include multi-choice and T/F styles, which certainly makes the metrics calculation easier (reflected in equation 1 and 2), yet probably losing the delicacy in the type of Q/A addressed?

Review Point: - Having the implicit misleading information generated by GPT-4o seems like a "fighting fire with fire" approach -- I think it is better to have at least a subset of implicit ones written by human annotators so that we can see whether there is any difference between the human-generated ones and GPT-4o generated ones.
Review Point: - During finetuning, a random set of explicit and implicit misled samples are used for finetuning, yet I am afraid the explicit misleading info has a too obvious and unique pattern due to how it's designed, hence too easy to pick them up, making the improvement after finetuning not too surprising.
Review Point: - Instead of finetuning, I would recommend the authors to simply systematically prompt the MLLMs, such as "The questions might contain misleading information, you should try to answer the question correctly despite of those misleading information ..."; another version could even give it two examples (one explicit and one implicit). I would guess/assume, simply doing this extra prompting will make the results much better.
Review Point: - The questions only include multi-choice and T/F styles, which certainly makes the metrics calculation easier (reflected in equation 1 and 2), yet probably losing the delicacy in the type of Q/A addressed?
==================================================

Focused review:

1. The proposed is rather small-scaled. There are 24 videos captured in total, with 80 to 140 frames for each video. Considering a FPS of 60, it's just 1-2 seconds. It would be better called an image dataset instead of a video dataset.
2. I assume the dataset would be open-sourced? Although the paper does not explicitly state that.
3. As the authors stated, the prototype is cumbersome with low stability and also private. This weakens the reproducibility of the work.
4. The background of the dataset and issues need further clarifications. It is not clear to me at the moment.

Review Point: 1. The proposed is rather small-scaled. There are 24 videos captured in total, with 80 to 140 frames for each video. Considering a FPS of 60, it's just 1-2 seconds. It would be better called an image dataset instead of a video dataset.
Review Point: 2. I assume the dataset would be open-sourced? Although the paper does not explicitly state that.
Review Point: 3. As the authors stated, the prototype is cumbersome with low stability and also private. This weakens the reproducibility of the work.
Review Point: 4. The background of the dataset and issues need further clarifications. It is not clear to me at the moment.
==================================================

Focused review:

1.The task-related token included in the matrices is not explained enough, how is it utilized and how is it embedded to give the information of the types of the tasks. It lacks some details about it in the description.
2.In evaluation part, there’s a lack of adequate analysis of the relationship between the poor performance in some tasks and model size.
3.In Part 3, there is a lack of detailed visualizations to show the internal framework of the model, as well as the details of the training and inference process.

Review Point: 1.The task-related token included in the matrices is not explained enough, how is it utilized and how is it embedded to give the information of the types of the tasks. It lacks some details about it in the description.
Review Point: 2.In evaluation part, there’s a lack of adequate analysis of the relationship between the poor performance in some tasks and model size.
Review Point: 3.In Part 3, there is a lack of detailed visualizations to show the internal framework of the model, as well as the details of the training and inference process.
==================================================

Focused review:

1. The proposed Motion-Agent framework seems to focus primarily on conversational motion generation, a capability that, according to the authors, could also be achieved using additional datasets for task-specific instruction tuning. While the authors assert that Motion-Agent is efficient, the experiments presented offer insufficient evidence to substantiate this claim.
2. The authors are encouraged to expand the discussion on the potential advantages of the proposed Motion-Agent framework. For instance, how does the model address out-of-domain motion concepts in comparison with current methods? A more thorough analysis of the model's generalization capabilities would contribute to a deeper understanding of its overall effectiveness.
3. The authors claim that Motion-Agent can theoretically achieve infinite motion generation. However, plots or tables illustrating changes with increasing conversation turns and motion lengths, should be provided to substantiate this claim.
4. The paper would benefit from ablation studies on the motion tokenizer, as well as comparisons with state-of-the-art RVQ-VAE models, such as MoMask which also employs RVQ-VAE to convert motion into a discrete representation.
5. The paper lacks comparisons on additional MotionGPT benchmarks, such as motion composition tasks. Tuning MotionLLM with these task is something that can be easily done.

Review Point: 1. The proposed Motion-Agent framework seems to focus primarily on conversational motion generation, a capability that, according to the authors, could also be achieved using additional datasets for task-specific instruction tuning. While the authors assert that Motion-Agent is efficient, the experiments presented offer insufficient evidence to substantiate this claim.
Review Point: 2. The authors are encouraged to expand the discussion on the potential advantages of the proposed Motion-Agent framework. For instance, how does the model address out-of-domain motion concepts in comparison with current methods? A more thorough analysis of the model's generalization capabilities would contribute to a deeper understanding of its overall effectiveness.
Review Point: 3. The authors claim that Motion-Agent can theoretically achieve infinite motion generation. However, plots or tables illustrating changes with increasing conversation turns and motion lengths, should be provided to substantiate this claim.
Review Point: 4. The paper would benefit from ablation studies on the motion tokenizer, as well as comparisons with state-of-the-art RVQ-VAE models, such as MoMask which also employs RVQ-VAE to convert motion into a discrete representation.
Review Point: 5. The paper lacks comparisons on additional MotionGPT benchmarks, such as motion composition tasks. Tuning MotionLLM with these task is something that can be easily done.
==================================================

Focused review:

The main issue I have with the paper is that the limitations/assumptions aren't sufficiently discussed. In particular:
- Deep reinforcement learning applications often involve learning from high dimensional input (e.g., pixels) or dealing with continuous controls (e.g., MuJoCo, real-word robotics). However, it's hard to see how the proposed approach would extend beyond discrete, gridworld-style tasks. At the top of page 6 it's claimed: "This choice of function set is consistent with the numerical variable representation commonly employed in DRL tasks", but seemingly the true assumption here is that the state can be encoded as a vector of *integers*. If true, this should be acknowledged, and either way I'd like to see a discussion of how the approach could be extended to more complex domains.
- The assumptions around "critical actions" aren't clear. It seems that the approach requires a human expert to deem which state variables are important (e.g., those corresponding to the inventory in Minecraft) and which are non-critical (e.g., the player's position). Again, this ought to be discussed.
A secondary concern is that there doesn't appear to be any source code included with the submission (correct me if I'm wrong) and the reinforcement learning setup isn't clear. For example:
- How is the 8x8 gridworld represented to the agent? Do you use different channels to represent different types of object, or just a single channel with different numerical values?
- Why is the agent paid a decaying reward based on the current step number (Appendix B.5)? Is this meant to be paid only at episode termination, or at every step? (The former would make more sense to me, but it's not expressed like this.)
- The gridworld and the PDDL state are encoded via a convolutional neural net and a fully-connected network, respectively. Are these encoders trained or fixed? What are kernel sizes of the CNN?
- I found the generalizability experiments quite confusing. Is the "ours" agent retrained in the variant domains after re-inducing the rules? (I can't see how it would generalize otherwise.) Are the original demonstrations for GAIL and BC-PPO discarded? Is BC-PPO retrained in the variant domain?

Review Point: - The assumptions around "critical actions" aren't clear. It seems that the approach requires a human expert to deem which state variables are important (e.g., those corresponding to the inventory in Minecraft) and which are non-critical (e.g., the player's position). Again, this ought to be discussed. A secondary concern is that there doesn't appear to be any source code included with the submission (correct me if I'm wrong) and the reinforcement learning setup isn't clear. For example:
Review Point: - How is the 8x8 gridworld represented to the agent? Do you use different channels to represent different types of object, or just a single channel with different numerical values?
Review Point: - Why is the agent paid a decaying reward based on the current step number (Appendix B.5)? Is this meant to be paid only at episode termination, or at every step? (The former would make more sense to me, but it's not expressed like this.) - The gridworld and the PDDL state are encoded via a convolutional neural net and a fully-connected network, respectively. Are these encoders trained or fixed? What are kernel sizes of the CNN?
Review Point: - I found the generalizability experiments quite confusing. Is the "ours" agent retrained in the variant domains after re-inducing the rules? (I can't see how it would generalize otherwise.) Are the original demonstrations for GAIL and BC-PPO discarded? Is BC-PPO retrained in the variant domain?
==================================================

Focused review:

weakness (quality, clarity, originality, and significance): It is the first paper analyzing difficulties one faces in optimizing multi-layer homogeneous functions. If the gradient changes smoothly, then it is possible to prove global converges as discussed in related work of the paper. The main difficulty here lies in the fact that the gradient is not changing smoothly. To my knowledge, existing global convergence results that I am aware of does not work for multi-layer homogeneous functions. Therefore, I consider this paper as an important contribution that provides convergence analysis for a class of non-smooth non-convex unconstrained optimization problem that is optimizing multi-layer homogeneous functions. I think this paper also elucidates the capabilities of batch normalization commonly used in deep learning. It is thought that batch normalization helps in regularizing and improving gradient flow in addition to reducing number of local minima. This paper shows that without batch normalization, GD already has regularization and good gradient flow. Therefore, the main power of batch normalization could be the power of avoiding local minima. In spite of the page limit, the details and necessary information were clearly delivered in this paper. The overall concept of the paper is clear and it is easy to read paper. One of the problems about this paper is the lack of enough experiments supporting the proofs of the paper. The simulation is limited to a matrix factorization experiment of Fig. 1. Similar investigation for deep neural network with homogenous activation function should be considered.

Review Point: 1. Similar investigation for deep neural network with homogenous activation function should be considered.
==================================================

Focused review:

1. The method may have certain limitations. Specifically, using WikiData and SPARQL queries to construct the supplementary dataset may not be applicable to non-factual or non-encyclopedic data, limiting the generalizability of the approach.
2. The organization of the paper could be improved. The experimental section is relatively short, with some important experimental tables relegated to the appendix. Additionally, the paper lacks a comprehensive ablation study.
3. The explanation and analysis of the experimental results are somewhat lacking. For instance, in Table 3, it is unclear what the values represent (e.g., accuracy) and why the performance of IFMET improves as the edit batch size increases, while the performance of other baselines decreases. More detailed explanations and discussions of these observations would strengthen the paper.

Review Point: 1. The method may have certain limitations. Specifically, using WikiData and SPARQL queries to construct the supplementary dataset may not be applicable to non-factual or non-encyclopedic data, limiting the generalizability of the approach.
Review Point: 2. The organization of the paper could be improved. The experimental section is relatively short, with some important experimental tables relegated to the appendix. Additionally, the paper lacks a comprehensive ablation study.
Review Point: 3. The explanation and analysis of the experimental results are somewhat lacking. For instance, in Table 3, it is unclear what the values represent (e.g., accuracy) and why the performance of IFMET improves as the edit batch size increases, while the performance of other baselines decreases. More detailed explanations and discussions of these observations would strengthen the paper.
==================================================

Focused review:

My main concern about this paper lies in the experiments. In general, I am not very convinced by their result that this method significantly improves upon the existing literature.
1. The authors mainly conduct qualitative comparison with PEZ and textual inversion, and not with CLIP-Interrogator, which is very misleading given that CLIP-Interrogator is the best performing baseline based on Table 1 and it can also generate prompts that have similar human interpretability in comparison to the proposed method when used with Llava or BLIP.
2. The authors mention PH2P in their literature review but did not use it as a baseline. From the PH2P paper it seems that they can also obtain similar human interpretability.
3. PEZ also has a variation that incorporates language fluency objectives (Section 5 in PEZ paper). Since this is the main contribution of this paper, the authors should consider comparing it with this variation too.
4. Authors should also consider (at least conceptually) compare with PRISM (https://arxiv.org/pdf/2403.19103), which is another prompt inversion method that uses VLM in their process and can achieve pretty good human interpretability.
5. The main contribution of this paper is to generate human readable prompts. However, the authors fail to provide a principle and quantitative way to measure this contribution. Metrics like perplexity can be easily implemented here.
6. The authors only compare performance on one text-to-image model and fail to demonstrate the generalizability of the inverted prompts on different text-to-image models.
7. The qualitative comparisons provided in this paper are very limited.
8. No limitation section or ethic statement. Given the potential malicious usage of this method (e.g. to generate inappropriate contents), I would encourage the authors to include these sections.

Review Point: 1. The authors mainly conduct qualitative comparison with PEZ and textual inversion, and not with CLIP-Interrogator, which is very misleading given that CLIP-Interrogator is the best performing baseline based on Table 1 and it can also generate prompts that have similar human interpretability in comparison to the proposed method when used with Llava or BLIP.
Review Point: 2. The authors mention PH2P in their literature review but did not use it as a baseline. From the PH2P paper it seems that they can also obtain similar human interpretability.
Review Point: 3. PEZ also has a variation that incorporates language fluency objectives (Section 5 in PEZ paper). Since this is the main contribution of this paper, the authors should consider comparing it with this variation too.
Review Point: 4. Authors should also consider (at least conceptually) compare with PRISM (https://arxiv.org/pdf/2403.19103), which is another prompt inversion method that uses VLM in their process and can achieve pretty good human interpretability.
Review Point: 5. The main contribution of this paper is to generate human readable prompts. However, the authors fail to provide a principle and quantitative way to measure this contribution. Metrics like perplexity can be easily implemented here.
Review Point: 6. The authors only compare performance on one text-to-image model and fail to demonstrate the generalizability of the inverted prompts on different text-to-image models.
Review Point: 7. The qualitative comparisons provided in this paper are very limited.
Review Point: 8. No limitation section or ethic statement. Given the potential malicious usage of this method (e.g. to generate inappropriate contents), I would encourage the authors to include these sections.
==================================================

Focused review:

- The mathematical formulas in the paper should be corrected. Some parentheses are missing, and there are some symbols that were not introduced in the text, e.g. $p_{0t}$. I understand that the equation in Section 4.1 should correspond to the equation introduced in the cited publication (Song et al., 2021), but the new undefined symbols and missing parentheses make this equation hardly readable.
- In Table 1, the results of DiffDock with modification explained in Section 4.4 and without dataset extension should also be presented.
- In this benchmark, the only evaluation metric (besides computation time) is the percentage of the poses with RMSD below 2 or 5 A. Given the recent criticism of ML-based docking models, it would be advised to include conformation quality metrics like those proposed in PoseBusters [1].
- It would be interesting to quantify the similarity between the binding pockets used in training, fine-tuning, and testing. I am curious if using complexes from the same dataset for fine-tuning and testing could create biases in the data due to the way the data is preprocessed and filtered. Furthermore, you should also try fine-tuning using only PDBBind structures, and see if the results for the DockGen benchmark improve in this setup.
[1] Buttenschoen, M., Morris, G. M., & Deane, C. M. (2023). PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences. arXiv preprint arXiv:2308.05777.
Before this paper can be published, it is essential to clarify the mathematical formulation of the method. Furthermore, additional results that disentangle the three new factors (architectural changes, dataset expansion, and confidence bootstrapping) would not only increase the credibility of the findings but also enhance the overall quality of the paper. I am willing to increase my score if my comments are properly addressed by the Authors. -----------
Edit: I changed my score (5 -> 6) after reading the other reviews and Authors' responses. The paper was significantly improved. Most of my concerns were resolved, and it seems the remaining concerns can be resolved in the final version of the manuscript if the running experiments are finished.

Review Point: - The mathematical formulas in the paper should be corrected. Some parentheses are missing, and there are some symbols that were not introduced in the text, e.g. $p_{0t}$. I understand that the equation in Section 4.1 should correspond to the equation introduced in the cited publication (Song et al., 2021), but the new undefined symbols and missing parentheses make this equation hardly readable.
Review Point: - In Table 1, the results of DiffDock with modification explained in Section 4.4 and without dataset extension should also be presented.
Review Point: - In this benchmark, the only evaluation metric (besides computation time) is the percentage of the poses with RMSD below 2 or 5 A. Given the recent criticism of ML-based docking models, it would be advised to include conformation quality metrics like those proposed in PoseBusters [1].
==================================================

Focused review:

1. Natural images especially non-texture type of images may not have intrinsic low-rank structures would limit the application of the proposed method on image denoising. 2. Since only the L1-regularization is imposed on the noise component, the proposed method can handle the salt-and-pepper noise well but the denoising performance is unknown for other types of noise. 3. The paper still needs more practical guidance on the learning rate selection case-by-case and a brief discussion of the impact of sampling rate on the performance.

Review Point: 1. Natural images especially non-texture type of images may not have intrinsic low-rank structures would limit the application of the proposed method on image denoising.
Review Point: 2. Since only the L1-regularization is imposed on the noise component, the proposed method can handle the salt-and-pepper noise well but the denoising performance is unknown for other types of noise.
Review Point: 3. The paper still needs more practical guidance on the learning rate selection case-by-case and a brief discussion of the impact of sampling rate on the performance.
==================================================

Focused review:

1. This method is cumbersome as it requires inputting both clean and adversarial images to compute the difference.
2. In Section 3.1, numerous mathematical tools are introduced but ultimately seem unnecessary, as the final objective function minimizes the initial starting point. This makes much of the content in Section 3.1 redundant.
3. Experiments are conducted only on the CIFAR dataset with CNN structures. The experimental design should be expanded to include both small- and large-scale datasets, as well as ViT models.

Review Point: 1. This method is cumbersome as it requires inputting both clean and adversarial images to compute the difference.
Review Point: 2. In Section 3.1, numerous mathematical tools are introduced but ultimately seem unnecessary, as the final objective function minimizes the initial starting point. This makes much of the content in Section 3.1 redundant.
Review Point: 3. Experiments are conducted only on the CIFAR dataset with CNN structures. The experimental design should be expanded to include both small- and large-scale datasets, as well as ViT models.
==================================================

Focused review:

Weakness] This paper is not technically novel and the ideas are simple. It is an incremental step from LiT and from fine tuning approaches that use adapters before. In addition, as stated in the papers, it has a limited set up, which include 1) only testing with COCO dataset, and later 1.5M image-text pairs. It might not work for other datasets, 2) only focusing on zero-shot classification and retrieval tasks. Their conclusions might not hold for other downstream problems, 3) only testing on transformer architecture, which in my opinion, is not a main limitation, however.

Review Point: 1) only testing with COCO dataset, and later 1.5M image-text pairs. It might not work for other datasets,
Review Point: 2) only focusing on zero-shot classification and retrieval tasks. Their conclusions might not hold for other downstream problems,
Review Point: 3) only testing on transformer architecture, which in my opinion, is not a main limitation, however.
==================================================

Focused review:

Weakness: - This paper shares a similar idea with EdgeStereo, this method should be included for comparison and discussion. - In my opinion, using edges as a regularization may inevitably introduce the risk of producing fake details on the disparity maps. How to overcome this problem? The SceneFlow dataset is relatively simple with only synthetic objects. I wonder whether Canny edges can also produce comparable accuracy as compared to GT edges on the KITTI datasets. Besides, it would be better to include learning based edge detection methods for comparison and discussion. - It seems the hyper-parameter gamma in Eq. (5) is critical to the performance of the proposed method. Ablation experiments should be conducted to validate the choice of this parameter. - As including additional information (e.g., edges) as regularizations can often improve the generalization performance, I wonder whether the proposed method can introduce consistent improvements on other datasets like ETH3D and Middlebury.

Review Point: - This paper shares a similar idea with EdgeStereo, this method should be included for comparison and discussion.
Review Point: - In my opinion, using edges as a regularization may inevitably introduce the risk of producing fake details on the disparity maps. How to overcome this problem? The SceneFlow dataset is relatively simple with only synthetic objects. I wonder whether Canny edges can also produce comparable accuracy as compared to GT edges on the KITTI datasets. Besides, it would be better to include learning based edge detection methods for comparison and discussion.
Review Point: - It seems the hyper-parameter gamma in Eq. (5) is critical to the performance of the proposed method. Ablation experiments should be conducted to validate the choice of this parameter.
Review Point: - As including additional information (e.g., edges) as regularizations can often improve the generalization performance, I wonder whether the proposed method can introduce consistent improvements on other datasets like ETH3D and Middlebury.
==================================================

Focused review:

* Some of the notation is conflicting/confusing. Please see the questions.
* The prior work of Bleistein and Guilloux, and Marion is referenced. However, the bounds derived in these paper and in this paper are not compared.
* The main theorems 5.9 and 6.1 have only an outline of the proof, and there is not a full proof in the appendix.
* The numerically illustrations are missing details. Please see the questions.

Review Point: * Some of the notation is conflicting/confusing. Please see the questions.
Review Point: * The prior work of Bleistein and Guilloux, and Marion is referenced. However, the bounds derived in these paper and in this paper are not compared.
Review Point: * The main theorems 5.9 and 6.1 have only an outline of the proof, and there is not a full proof in the appendix.
Review Point: * The numerically illustrations are missing details. Please see the questions.
==================================================

Focused review:

: - Clarity of the paper could be improved, especially section 3 and Figure 2. While the approach is understandable, I was a bit confused at first by the notation. Adding the dimensionality of the different variables could help as they sometime refer to feature map, set of vectors or flat average-pooled vector. I would also suggest using different function name for the CNN encoder, transformer encoder, decoder… - The approach is only compared to DETR variants. It’s unclear if the approach brings some benefit compared to other object detection work such as VITDet (from Exploring Plain Vision Transformer Backbones for Object Detection, Li et al. 2022). - The paper only focuses on object-detection tasks. While I understand this is the focus of the current work, significance of Siamese DETR could be improved by demonstrating its applicability to other tasks such as segmentation or classificaiton.

Review Point: - The paper only focuses on object-detection tasks. While I understand this is the focus of the current work, significance of Siamese DETR could be improved by demonstrating its applicability to other tasks such as segmentation or classificaiton.
==================================================

Focused review:

:
Lack of clarity. The paper lacks important information to reproduce the results:
Overall, the paper lacks a clear high-level explanation of the proposed method. In particular, I think Fig. 2 is very hard to parse and fails to communicate the intuition or high-level idea of the proposed method. The section b) of Fig. 2 is quite convoluted and the text lacks details about how to parse the image. It is unclear to me how the features shown in Fig. 2 are extracted; what the positional embedding used is; and a justification of the architecture used for the “Surface Extractor” in the Figure. From the “Network Architecture” paragraph it seems like the proposed approach is just putting existing components together; this in my opinion decreases the novelty of the approach.
What is the loss used in Eq. 7? I could not find any no discussion about it.
The estimation refinement process presented in Sec. 3.2 lacks details:
The update step shown in Eq. 9 does not preserve the properties of a rotation matrix. It is unclear from Eq. 8 how the optimization problem ensures that the rotation estimates still belong to the SO(3) group. This is a crucial aspect since the paper states that it aims at refining a pose, and the formulation does not seem to be that solid.
The method lacks robustness. This is because the formulation assumes that the pre-trained SDF function is perfect. However, this may not be the case and the estimates can be severely affected.
The proposed method requires a pre-trained SDF function for every object. I think this is not scalable as it requires training several networks, increasing time and computational resources.
Insufficient experiments:
The paper mainly claims that the proposed method is faster than ICP in the abstract and Table 1. Unfortunately, I don’t see a more complete experiment backing this up besides Table 1. In principle, comparing both methods is not a fair comparison because the proposed method uses GPUs and requires an SDF network for each object. Thus, the training time of each SDF network for each object is discounted in Table 1.
The ablation study is quite limited. It is unclear an optimal number of iterations (or the value of n in Algo. 1). Second, there are other parameters that can affect the performance (e.g., L and L_max) of the proposed approach.

Review Point: 1). Second, there are other parameters that can affect the performance (e.g., L and L_max) of the proposed approach.
==================================================

Focused review:

:
There can be more analysis of when the proposed method fails and other potential shortcomings of their approach.
There should be more analysis on what types of multimodal inputs and modalities the model estimates uncertainty for, including visualizations and qualitative examples.
The paper can benefit from some additional analysis and discussion of the additional training time and space complexity resulting from their model.
There should be some reference to recent work in learning robust multimodal representations [1,2].
The model currently handles multimodal interactions using a very simple late-fusion style mixture of distributions - how can the method be extended to more fine-grained multimodal representation learning methods such as those in Table 5 (e.g. RMFN, MFM etc.)
There is not enough evaluation to show that the model has accurately estimated uncertainty apart from some visualizations. How do you evaluate whether your model has estimated uncertainty correctly? Are there any metrics for judging this? I think this might be a good reference: https://github.com/uncertainty-toolbox/uncertainty-toolbox
[1] Liang et al., Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization. ACL 2019
[2] Lee et al., Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors. 2020
Not a weakness, but a general suggestion: The authors can take a look at https://arxiv.org/abs/2107.07502 for a wide suite of multimodal methods and datasets that they can test their method on.
I think the authors can say a bit more about the societal impact of multimodal models, including their applications in robotics, healthcare, education etc.

Review Point: 2020 Not a weakness, but a general suggestion: The authors can take a look at https://arxiv.org/abs/2107.07502 for a wide suite of multimodal methods and datasets that they can test their method on. I think the authors can say a bit more about the societal impact of multimodal models, including their applications in robotics, healthcare, education etc.
==================================================

Focused review:

While I agree with the motivation behind the paper, I am not sure about the soundness of the methodology followed to generate the reference answers:
1. Using the response itself to generate an "adapted reference", the evaluation might indirectly validate the response’s content and structure. This may lead to artificially inflated evaluations, as the evaluator is essentially comparing the response against a modified version of itself, which serves as the reference.
2. If the response contains subtle errors, the adapted reference “might” effectively validate or normalize these errors. These is no study around whether the reviser indeed accounts for or corrects for these errors.
3. While this approach may work well for evaluations of standard NLG tasks as well as some open-ended tasks that care about the language generation capabilities, but for evaluations that care about the factual accuracy of the responses (something where LLMs are overall known to hallucinate), this simple revision may not be robust.

Review Point: 1. Using the response itself to generate an "adapted reference", the evaluation might indirectly validate the response’s content and structure. This may lead to artificially inflated evaluations, as the evaluator is essentially comparing the response against a modified version of itself, which serves as the reference.
Review Point: 2. If the response contains subtle errors, the adapted reference “might” effectively validate or normalize these errors. These is no study around whether the reviser indeed accounts for or corrects for these errors.
Review Point: 3. While this approach may work well for evaluations of standard NLG tasks as well as some open-ended tasks that care about the language generation capabilities, but for evaluations that care about the factual accuracy of the responses (something where LLMs are overall known to hallucinate), this simple revision may not be robust.
==================================================

Focused review:

The approach does appear to have significant limitations.
1. The approach assumes access to an approximate model of the transition dynamics. The authors argue this can be relatively simple to learn because it only requires a single step, but this can be difficult in high-dimensional and stochastic settings.
2. The approach often assumes a pre-trained low-level controller.

Review Point: 1. The approach assumes access to an approximate model of the transition dynamics. The authors argue this can be relatively simple to learn because it only requires a single step, but this can be difficult in high-dimensional and stochastic settings.
==================================================

Focused review:

There are several key weaknesses that I find concerning in this paper :
1. The proposed objective adds to the line of work around representation learning for RL; in addition to several past works that recently also studied bisimulation based objectives for control and offline RL. While the paper refers to past works, little comparisons are made to those prior works, either theoretically or experimentally - and it seems the authors start with policy evaluartion - discusses metrics related to stability for it - and claims to show expeirmentally why policy evaluation can be done well with the KROPE objective. However, several prior works have done this already, whether implicitly or explicitly showing the policy evaluation measure; and have demonstrated significantly empirically why bisimulation can be a good measure fior learning representations for offline RL.
2. Following from the above comment - I do not understand the exact benefits the kernel based approach buys for this; whether we do bisimulartion based representation or not (such as looking into inverse or forward dynamics models for learning representations) - the benefits of kernels seem to be not explained too well here. Are there new techniques in deep kernel learning literature that can be exploited here? What happens when we go to large state action spaces, with complex tasks - how well would the kernel based approach scale here?
3. Experiments are done on simple tasks where kernel based representations can assumably do similar to other objectives. However, as you scale to larger state-action spaces, I do not think the paper demonstrates well how well their algorithm would scale in this context. I would experiment more thorough comparisons with a large body of prior works for comparison, either empirically or theoretically.
4. Recent works (e.g Lamb et al.,) have significantly studied inverse dynamics models and claimed how these objectives can learn the true underlying latent dynamics in an unknown environment. In addition, Zang et al, studied and compared some of these inverse dynamics objectives with the bisimulation based objectives in offline RL - and demonstrated effectiveness of each approach on several empirical benchmarks. How well does the proposed KROPE representation do, in light of all these past works? In all these works, there is also the implicit or explicit demonstration of good enough policy evaluations to then do control. I do not think the discussion of policy evalaution and stability here is enough to show that KROPE can learn good policies for control.

Review Point: 2. Following from the above comment - I do not understand the exact benefits the kernel based approach buys for this; whether we do bisimulartion based representation or not (such as looking into inverse or forward dynamics models for learning representations) - the benefits of kernels seem to be not explained too well here. Are there new techniques in deep kernel learning literature that can be exploited here? What happens when we go to large state action spaces, with complex tasks - how well would the kernel based approach scale here?
Review Point: 3. Experiments are done on simple tasks where kernel based representations can assumably do similar to other objectives. However, as you scale to larger state-action spaces, I do not think the paper demonstrates well how well their algorithm would scale in this context. I would experiment more thorough comparisons with a large body of prior works for comparison, either empirically or theoretically.
==================================================

Focused review:

1. The dataset is pure synthetic and constructed by a limited number of textual templates. I have concerns about the FoR classification task given the template "<locatum> <spatial relation> <relatum> <perspective>". It seems hard to disentangle this task with linguistic and common-sense reasoning of LLMs. For example, LLMs are able to determine whether the perspective is intrinsic or relative by analyzing perspective template, and analyze topology template to determine whether the locatum is external or internal. Both of them don't necessitate understanding the underlying spatial configuration under a specific perspective. On the contrary, the text-to-image task indeed requires the model to interprete the spatial configuration and transform the perspective to camera's.
2. Again, since the dataset is synthetic and constructed by textual templates, the inductive bias might be leveraged by SG-prompting.
3. Lack of full prompts of different settings, such as few-shot, CoT, SG-prompting, text-to-layout and SG to layout.

Review Point: 2. Again, since the dataset is synthetic and constructed by textual templates, the inductive bias might be leveraged by SG-prompting.
Review Point: 3. Lack of full prompts of different settings, such as few-shot, CoT, SG-prompting, text-to-layout and SG to layout.
==================================================

Focused review:

1. The definition and quantification of "parameter imbalance" lack rigorous theoretical justification. The causal relationship between pre-training data imbalance and parameter imbalance is not thoroughly explained. The selection of semantic factors as confounders needs stronger theoretical support.
2. The claim that parameter imbalance has greater impact than data imbalance needs more empirical evidence.
3. Incomplete ablation studies for key components and lack of analysis on computational overhead and efficiency.

Review Point: 1. The definition and quantification of "parameter imbalance" lack rigorous theoretical justification. The causal relationship between pre-training data imbalance and parameter imbalance is not thoroughly explained. The selection of semantic factors as confounders needs stronger theoretical support.
Review Point: 2. The claim that parameter imbalance has greater impact than data imbalance needs more empirical evidence.
Review Point: 3. Incomplete ablation studies for key components and lack of analysis on computational overhead and efficiency.
==================================================

Focused review:

: - nothing ground-breaking, application of existing technologies - code not available - results are as could be expected - General Discussion: - why didn't you use established audio features such as MFCCs?
- Minor Details: - L155 and other places: a LSTM -> an LSTM - L160, L216 and other Places: why are there hyphens (-) after the text?
- L205: explanation of convolution is not clear - Table1 should appear earlier, on page 2 already cited - L263: is 3D-CNN a standard approach in video processing? alternatives?
- L375, 378: the ^ should probably positioned above the y - L380: "to check overfitting" -> did you mean "to avoid"?
- L403, 408..: put names in " " or write them italic, to make it easier to recognize them - L420: a SVM -> an SVM - L448: Output ... are -> wrong numerus, either "Outputs", or use "is" - L489: superflous whitespace after "layer" - L516, 519: "concatenation" should not be in a new line - L567: why don't you know the exact number of persons?
- L626: remove comma after Since - L651: doesnt -> does not - L777: insert "hand, the" after other - References: need some cleanup: L823 superflous whitespace, L831 Munich, L860 what is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines, L956 indent Linguistics properly

Review Point: - Minor Details:- L155 and other places: a LSTM -> an LSTM - L160, L216 and other Places: why are there hyphens (-) after the text?
Review Point: - L205: explanation of convolution is not clear - Table1 should appear earlier, on page 2 already cited - L263: is 3D-CNN a standard approach in video processing? alternatives?
Review Point: - L375, 378: the ^ should probably positioned above the y - L380: "to check overfitting" -> did you mean "to avoid"?
Review Point: - L403, 408..: put names in " " or write them italic, to make it easier to recognize them - L420: a SVM -> an SVM - L448: Output ... are -> wrong numerus, either "Outputs", or use "is" - L489: superflous whitespace after "layer" - L516, 519: "concatenation" should not be in a new line - L567: why don't you know the exact number of persons?
Review Point: - L626: remove comma after Since - L651: doesnt -> does not - L777: insert "hand, the" after other - References: need some cleanup: L823 superflous whitespace, L831 Munich, L860 what is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines, L956 indent Linguistics properly
==================================================

Focused review:

* The model’s hypothesis class (e.g. mode switching between long-distance free space motion, and short horizon fine-grained manipulation) imposes a major constraint on the kinds of problems it can represent effectively. Of course, in the extreme case, either one or the other mode can always be predicted, but in these settings no benefit / insight is offered, and hard attention switching across the inputs limits flexibility.
* The assumptions about dataset preparation are a major weakness/limitation here, for two reasons:
1. Adding a different label for mode is a major annotation requirement, and can be quite noisy/arbitrary.
2. The notion of labeled “salient points” may make some sense because humans do attend to salient points reasonably consistently, but generally speaking they are not a consistent/principled thing to expect a human to label - who is to say which specific points should be annotated? Seems quite noisy, and a major source of error.
* Comparisons were somewhat weak.The authors assert that long-horizon goal prediction is not precise enough for this kind of imitation learning, but evidence from e.g. 3DDA, RPDiff, NDF, TAX-Pose, etc. demonstrate that the quasistatic problems considered in the real-world tasks are probably solvable with goal prediction.
* Benchmark tasks are also somewhat weak. While the real-world experiments are good, the authors should compare against other methods on simulated benchmarks (including annotating using their system or a synthetic annotation scheme) such as RLBench where some medium-horizon tasks have been examined with a litany of different approaches.
* There’s not much analysis of sample complexity.
* Several related works are missing / not discussed:
* Spatial grounded policies: Spatial Action Maps, Transporter Nets, HACMan, 3D Diffuser Actor
* Relative placement / reasoning: NDF, TAX-Pose, etc.
* Keyframing: Many of Stephen James’ works on top of RLBench look at long-horizon gripper positional reasoning and keyframing.
* The “soft salience” mechanism has been used before (e.g. TossingBot, many others), so while it’s pretty standard it would be good to compare / reference
* The variation for initial conditions in the real-world experiments appear somewhat limited, and having the high-level policy run on a global world frame is a bit unfair as a selling point because it doesn’t have to generalize as much by construction (probably could modify the other methods to have similar properties).

Review Point: * The model’s hypothesis class (e.g. mode switching between long-distance free space motion, and short horizon fine-grained manipulation) imposes a major constraint on the kinds of problems it can represent effectively. Of course, in the extreme case, either one or the other mode can always be predicted, but in these settings no benefit / insight is offered, and hard attention switching across the inputs limits flexibility.
Review Point: * The assumptions about dataset preparation are a major weakness/limitation here, for two reasons:
Review Point: 1. Adding a different label for mode is a major annotation requirement, and can be quite noisy/arbitrary.
Review Point: 2. The notion of labeled “salient points” may make some sense because humans do attend to salient points reasonably consistently, but generally speaking they are not a consistent/principled thing to expect a human to label - who is to say which specific points should be annotated? Seems quite noisy, and a major source of error.
Review Point: * Comparisons were somewhat weak.The authors assert that long-horizon goal prediction is not precise enough for this kind of imitation learning, but evidence from e.g. 3DDA, RPDiff, NDF, TAX-Pose, etc. demonstrate that the quasistatic problems considered in the real-world tasks are probably solvable with goal prediction.
Review Point: * Benchmark tasks are also somewhat weak. While the real-world experiments are good, the authors should compare against other methods on simulated benchmarks (including annotating using their system or a synthetic annotation scheme) such as RLBench where some medium-horizon tasks have been examined with a litany of different approaches.
Review Point: * Spatial grounded policies: Spatial Action Maps, Transporter Nets, HACMan, 3D Diffuser Actor * Relative placement / reasoning: NDF, TAX-Pose, etc.
Review Point: * Keyframing: Many of Stephen James’ works on top of RLBench look at long-horizon gripper positional reasoning and keyframing.
Review Point: * The “soft salience” mechanism has been used before (e.g. TossingBot, many others), so while it’s pretty standard it would be good to compare / reference * The variation for initial conditions in the real-world experiments appear somewhat limited, and having the high-level policy run on a global world frame is a bit unfair as a selling point because it doesn’t have to generalize as much by construction (probably could modify the other methods to have similar properties).
==================================================

Focused review:

1. While decoupling layout and glyph generation increases flexibility, it may also add to the model's complexity, potentially affecting training and inference efficiency.
2. Are there any application scenarios for this task? The author could analyze its practicality.
3. The paper mentions difficulties in imitating styles with extensive cursive connections between characters due to the independent generation of each character, indicating potential limitations in handling certain calligraphic styles.

Review Point: 1. While decoupling layout and glyph generation increases flexibility, it may also add to the model's complexity, potentially affecting training and inference efficiency.
Review Point: 2. Are there any application scenarios for this task? The author could analyze its practicality.
Review Point: 3. The paper mentions difficulties in imitating styles with extensive cursive connections between characters due to the independent generation of each character, indicating potential limitations in handling certain calligraphic styles.
==================================================

Focused review:

1. The authors have empirically observed the occurrence of agreement-on-the-line (AGL) and accuracy-on-the-line (ACL) following test-time adaptation (TTA). To enhance the manuscript, it would be beneficial for the authors to provide a more extensive explanation and discussion regarding this observed phenomenon.
2. The proposed temperature scaling method may not be applicable in latency-sensitive real-world applications when considering efficiency. As described in section 3.2, the optimal temperature is calculated after the network makes predictions on the full test set. Therefore, the proposed method require a significant delay for model adaptation. More discussion on efficiency is required.
3. Although the authors conduct experiments on different TTA methods, the problem setting of this paper is different from that of TTA. As shown in Algorithm 1, the proposed performance estimation method requires in-domain data and labels, which, however, are inaccessible under the settings of TTA.
4. Figures 1-3 are difficult to understand. For example, it is unclear what the values of the horizontal and the vertical axes in Figure 1 represent. More explanations should be provided.
5. The authors only analyze the agreement-on-the-line phenomenon on CNN-based models. However, powerful transformer-based models should also be involved in the experiments, such as ViT[1], Swin Transformer[2], PoolFormer[3], and so on.
6. In the Introduction, the authors claim that "Baek et al. (2022) show that the ID and OOD agreement between classifiers shows a strong linear correlation". However, Baek et al. (2022) only study the correlation of disagreement and error between classifiers. It would be preferable to provide a more precise description.
7. In Table 2, the authors only provide insufficient results on several domains of the test datasets to demonstrate the effectiveness of the proposed performance estimation. It would be better if the authors could provide more experimental results under these datasets, such as all 15 corrupted datasets in ImageNet-C.
8. In Figure 3, the authors demonstrate that the phenomenon of AGL and ACL also occurs when varying TTA hyperparameters, such as learning rate and batch size. However, from Table 1 to Table 3, the authors only study the effect of the proposed methods using different architectures or different training checkpoints. More ablation study using different hyperparameter setup is suggested.
9. As shown in Figure 4, varying learning rate in TTT exhibits a negative correlation between ID and OOD accuracies. To establish a comprehensive study, more experiments should be conducted to verify if this phenomenon occurs in other test-time training methods, such as TTT++[4] and TTT-MAE[5].
10. On page 7, the sentence “let X the random variable” should be changed to “let X be the random variable”.
[1] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021.
[2] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV 2021.
[3] PoolFormer: MetaFormer Is Actually What You Need for Vision, CVPR 2022.
[4] TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive? NeurIPS 2021.
[5] Test-Time Training with Masked Autoencoders, NeurIPS 2022.

Review Point: 1. The authors have empirically observed the occurrence of agreement-on-the-line (AGL) and accuracy-on-the-line (ACL) following test-time adaptation (TTA). To enhance the manuscript, it would be beneficial for the authors to provide a more extensive explanation and discussion regarding this observed phenomenon.
Review Point: 2. The proposed temperature scaling method may not be applicable in latency-sensitive real-world applications when considering efficiency. As described in section 3.2, the optimal temperature is calculated after the network makes predictions on the full test set. Therefore, the proposed method require a significant delay for model adaptation. More discussion on efficiency is required.
Review Point: 3. Although the authors conduct experiments on different TTA methods, the problem setting of this paper is different from that of TTA. As shown in Algorithm 1, the proposed performance estimation method requires in-domain data and labels, which, however, are inaccessible under the settings of TTA.
Review Point: 4. Figures 1-3 are difficult to understand. For example, it is unclear what the values of the horizontal and the vertical axes in Figure 1 represent. More explanations should be provided.
Review Point: 5. The authors only analyze the agreement-on-the-line phenomenon on CNN-based models. However, powerful transformer-based models should also be involved in the experiments, such as ViT[1], Swin Transformer[2], PoolFormer[3], and so on.
Review Point: 6. In the Introduction, the authors claim that "Baek et al. (2022) show that the ID and OOD agreement between classifiers shows a strong linear correlation". However, Baek et al. (2022) only study the correlation of disagreement and error between classifiers. It would be preferable to provide a more precise description.
Review Point: 7. In Table 2, the authors only provide insufficient results on several domains of the test datasets to demonstrate the effectiveness of the proposed performance estimation. It would be better if the authors could provide more experimental results under these datasets, such as all 15 corrupted datasets in ImageNet-C.
Review Point: 8. In Figure 3, the authors demonstrate that the phenomenon of AGL and ACL also occurs when varying TTA hyperparameters, such as learning rate and batch size. However, from Table 1 to Table 3, the authors only study the effect of the proposed methods using different architectures or different training checkpoints. More ablation study using different hyperparameter setup is suggested.
Review Point: 9. As shown in Figure 4, varying learning rate in TTT exhibits a negative correlation between ID and OOD accuracies. To establish a comprehensive study, more experiments should be conducted to verify if this phenomenon occurs in other test-time training methods, such as TTT++[4] and TTT-MAE[5].
Review Point: 10. On page 7, the sentence “let X the random variable” should be changed to “let X be the random variable”. [1] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021. [2] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV 2021. [3] PoolFormer: MetaFormer Is Actually What You Need for Vision, CVPR 2022. [4] TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive? NeurIPS 2021. [5] Test-Time Training with Masked Autoencoders, NeurIPS 2022.
==================================================

Focused review:

1. Lack of ablation studies. For example, additional experiments could be included to examine the impact of various pruning strategies on the model and to assess the effects of different hyperparameter settings, such as K1, K2 and the ratio.
2. The authors might consider including additional benchmarks, such as MME and AI2D, and presenting fine-grained performance scores. Additionally, it would be helpful to include metrics such as GPU memory and total time in the comparisons to provide a more comprehensive evaluation.

Review Point: 1. Lack of ablation studies. For example, additional experiments could be included to examine the impact of various pruning strategies on the model and to assess the effects of different hyperparameter settings, such as K1, K2 and the ratio.
Review Point: 2. The authors might consider including additional benchmarks, such as MME and AI2D, and presenting fine-grained performance scores. Additionally, it would be helpful to include metrics such as GPU memory and total time in the comparisons to provide a more comprehensive evaluation.
==================================================

Focused review:

- **Dependence on the performance at $w^\*$**:
The first part of the bound ($\mathbb{E}[x-\mathcal{R}(w^\*)]$) still depends on the performance of the leakage algorithm $\mathcal{R}$. This means that, in practice, the computational complexity of obtaining the proposed bound is the same as the complexity of the traditional evaluation of leakage attacks (which is very high, especially for attacks based on optimization). It also means that the issues related to estimating the performance of $\mathcal{R}$ in the presence of randomness ( e.g. from the initialization or the choice of the client data batch) remain as hard to solve for the proposed bound as for the original leakage problem. Further, the problem of choosing the optimal hyperparameters for leakage attacks in the presence of such randomness, which would have been one of the best applications of the proposed bound, also remains as hard as before. Finally, the dependency also introduces possible additional challenges compared to evaluating the attack directly on $w_t$ - in particular, estimating $w^\*$. While for convex models, such as the ones explored in the paper, estimating $w^\*$ is not hard, for models with multiple local minima and equivalent solutions ( like generic neural networks ), it is actually challenging to estimate $w^\*$, and it represents an additional source of randomness.
- **The bound's dependence on $t$:**
In Section 5 of the paper, the authors demonstrate, both practically and theoretically, that their bound predicts that as $t$ increases, the vulnerability of the attacked model increases. This is in stark contradiction with the empirical observations about gradient leakage attacks where exactly the opposite is true (e.g. See [1,2]). This mismatch is caused by the second part of the bound, which is supposed to precisely capture the dependence of the attack's success through time but, in reality, is based solely on the convergence of FedAvg and disregards any knowledge of $\mathcal{R}$ but its Lipschitzness. As such, the bound does not capture the evolution of $\mathcal{R}$ with time, only the evolution of the weights.
- **Unclear or missing implementation details:**
1. When the first term of the bound $\mathbb{E}[x-\mathcal{R}(w^\*)]$ is estimated, what is the average taken over? Multiple batches of the client data? Multiple initializations of the algorithms? Across different attack hyperparameters? All?
2. How is $\Gamma$ approximated in practice? How is heterogeneity controlled for in general in the experiments provided? Can you demonstrate the results of experiments on different levels of heterogeneity?
3. The authors in Figure 2 and Algorithm 1 talk about their unrolled network to have parameters $\theta^i$. Where are those parameters coming from, and what do they represent for the used leakage attacks? Even in Line 4 of Algorithm 1, where they are defined, they don't seem to be used. Also, can you elaborate on why you tune them with layer-wise methods instead of SGD? The authors just say "better generalizability" with no context.
- **Problems with the presented evaluation:**
1. All empirical results are presented in the plots on a scale, which makes it very hard to interpret them. In particular, I suggest that the authors use two different scales for the bounds and the empirical results. They can still present both results in the same plot for comparison reasons, but they can show the empirical scale on the left part of the figure and the bound scale on the right. This will enable better comparison between the trends in the two modes of evaluation, as now the empirical models always look completely flat.
2. The paper's main claim is that the proposed bound is a useful tool for evaluating the practical performance of gradient leakage attacks. Yet, the authors do not provide a correlation metric between their bound and the empirical evaluation results. Can the authors precisely measure how correlated their bound is to the actual gradient leakage results?
3. While the paper focuses on analyzing how gradient leakage performance changes with $E$, $N$, and $t$, I want to see their bounds used for comparing the same gradient leakage attack on different models and architectures, as well as, across different hyperparameters such as initialization strategies, regularizer strengths, etc. See [3].
4. The authors claim that one needs to interpret their bound as **average** and not **best-case** reconstruction performance ( Section 6 in the paper ), yet they only provide a single experiment ( Figure 9c ) where they compare against average reconstruction performance. All experiments in the paper should show average behavior for the author's claims to be substantiated.
- **Poor experimental results:**
The current experimental results are weak. What I mean by this is that in many experiments, the bounds do not well reflect what happens to practical performance. For example, in Figures 3 and 4, the empirical evaluation puts invGrad and GGL very close to each other in terms of performance, with invGrad sometimes even better. At the same time, the bounds consistently put the performance of invGrad to be similar to DLG and iDLG. Similarly, in Figures 5a and 5c, the bound predicts better reconstructions from DLG compared to invGrad and iDLG, while the practical performance of DLG (expectedly) is quite a lot worse than invGrad and iDLG. If the authors want to claim this is due to average vs best-case performance, they should provide more empirical evidence than Figure 9c for these discrepancies as they are noticeable in **almost** all figures in the paper.
- **Applicability of the proposed bounds:**
For the proposed bounds to be computable, one needs to execute the leakage attacks on federated models and losses that jointly satisfy both $\mu$-convexity and $L$-smoothness at the same time. Unfortunately, this restricts the usability of the bound to the federated learning models and losses that are jointly "close to" representing a quadratic function, even though the original attacks are applicable and tested on much more complex models. Further, those assumptions cannot be trivially disregarded, as the bound does not only make these assumptions but requires estimates of $\mu$ and $L$ to be computed. This forces the authors to restrict their federated models in their experiments to only Logistic Regression and 2-layer convolutional neural network without activations.
Similarly, the bound also depends on upper bounds on the variance and size of gradients, which the authors are forced to unsoundly approximate even for the simple networks used.
Finally, the precision of the second term of the bound heavily depends on the ability to accurately estimate the Lipshitz constant of $\mathcal{R}$. This naturally means that more complex methods $\mathcal{R}$, such as very deep neural networks, are penalized more heavily in their second term when they should not necessarily have to be. For example, methods like [4] and [5] have been shown to be very effective at recovering user data, even if they would likely have large Lipshitz constant estimates.
- **Not important:**
1. The authors should cite [1] and [3] as prior frameworks that attempt to analyze leakage attacks.
2. Some attacks on language models attacks like [6] and [7], might be hard to represent in this framework due to being more look-up-based than optimization-based. Similarly, some malicious server attacks might be hard to represent in this framework due to their dependency on the particular malicious weights sent to the client, which are far away from $w^\*$ like in [8]. Approaches like [9] also cannot be handled. To this end, the paper can benefit from a discussion of the limitations of the bound in terms of what types of leakage attacks it supports.
3. In Sec. 3.2, the definition of $\mathcal{R}(w_t)$ has expectation over $(x,y)$ which makes no sense in this context.
4. In Sec 3.1, the authors claim that full device participation is "unrealistic in practice". Due to cross-silo applications of FL, they might want to tune this claim down.
5. In Algorithm 2, Line 1, second statement: $\phi_h$ should be $\phi_H$ instead
5. In Appendix C.1.1, the notation for the regularizer parameter of the Logistic Regression $\gamma$ clashes with $\gamma$ used in the various bounds in the paper.
6. In Appendix C.1.1, the paragraph on computing $L$ provides two different bounds on $L$ - one with and one without $2\gamma$

Review Point: - **Unclear or missing implementation details:** 1. When the first term of the bound $\mathbb{E}[x-\mathcal{R}(w^\*)]$ is estimated, what is the average taken over? Multiple batches of the client data? Multiple initializations of the algorithms? Across different attack hyperparameters? All?
Review Point: 2. How is $\Gamma$ approximated in practice? How is heterogeneity controlled for in general in the experiments provided? Can you demonstrate the results of experiments on different levels of heterogeneity?
Review Point: 3. The authors in Figure 2 and Algorithm 1 talk about their unrolled network to have parameters $\theta^i$. Where are those parameters coming from, and what do they represent for the used leakage attacks? Even in Line 4 of Algorithm 1, where they are defined, they don't seem to be used. Also, can you elaborate on why you tune them with layer-wise methods instead of SGD? The authors just say "better generalizability" with no context.
Review Point: 2. The paper's main claim is that the proposed bound is a useful tool for evaluating the practical performance of gradient leakage attacks. Yet, the authors do not provide a correlation metric between their bound and the empirical evaluation results. Can the authors precisely measure how correlated their bound is to the actual gradient leakage results?
Review Point: 3. While the paper focuses on analyzing how gradient leakage performance changes with $E$, $N$, and $t$, I want to see their bounds used for comparing the same gradient leakage attack on different models and architectures, as well as, across different hyperparameters such as initialization strategies, regularizer strengths, etc. See [3].
Review Point: 4. The authors claim that one needs to interpret their bound as **average** and not **best-case** reconstruction performance ( Section 6 in the paper ), yet they only provide a single experiment ( Figure 9c ) where they compare against average reconstruction performance. All experiments in the paper should show average behavior for the author's claims to be substantiated.
Review Point: - **Not important:** 1. The authors should cite [1] and [3] as prior frameworks that attempt to analyze leakage attacks.
Review Point: 2. Some attacks on language models attacks like [6] and [7], might be hard to represent in this framework due to being more look-up-based than optimization-based. Similarly, some malicious server attacks might be hard to represent in this framework due to their dependency on the particular malicious weights sent to the client, which are far away from $w^\*$ like in [8]. Approaches like [9] also cannot be handled. To this end, the paper can benefit from a discussion of the limitations of the bound in terms of what types of leakage attacks it supports.
Review Point: 3. In Sec. 3.2, the definition of $\mathcal{R}(w_t)$ has expectation over $(x,y)$ which makes no sense in this context.
Review Point: 4. In Sec 3.1, the authors claim that full device participation is "unrealistic in practice". Due to cross-silo applications of FL, they might want to tune this claim down.
Review Point: 5. In Algorithm 2, Line 1, second statement: $\phi_h$ should be $\phi_H$ instead 5. In Appendix C.1.1, the notation for the regularizer parameter of the Logistic Regression $\gamma$ clashes with $\gamma$ used in the various bounds in the paper.
Review Point: 6. In Appendix C.1.1, the paragraph on computing $L$ provides two different bounds on $L$ - one with and one without $2\gamma$
==================================================

Focused review:

1. Some related literature is missing, e.g., [1]. Specifically, the SBM model with $\rho = 1/2$ and Gaussian Features aligns with Section 4 in [1] and should be cited and compared. Additionally, for the SBM with GF where $\rho = 1/2$ and $a_1 = a_2 = a$, the expression for $I^*$ takes a simple form in Equation (4.4) of [1]. Providing explicit forms of $I^*$ for other models would enhance interpretability of the results.
2. For SBM with BEC/BSC, the algorithms assume prior knowledge of parameters $(\rho,\varepsilon, \alpha)$. The paper should mention this clearly and discuss why it's reasonable to assume this prior knowledge, or suggest how these parameters could be estimated. From a theory standpoint, achieving optimality might require paying a cost for adapting to these parameters, especially in regimes $\varepsilon = n^{-\beta}$ and $\alpha = n^{-\beta}$ for $\beta>0$.
3. The two-community assumption limits the algorithm’s practical use. While I understand that the optimality of the spectral algorithm depends on this assumption, the statement in Appendix A, "The entire framework of genie-aided estimation naturally generalizes to the multi-community case," is confusing. Can the authors elaborate on how the spectral algorithm could be modified, for example, to handle $K=3$ communities?
4. There are no experiments. It would be helpful to include numerical experiments demonstrating the algorithm’s performance on the specific models considered.
[1] Abbe, E., Fan, J., & Wang, K. (2022). An $\ell_p$ theory of PCA and spectral clustering. The Annals of Statistics, 50(4), 2359-2385.

Review Point: 1. Some related literature is missing, e.g., [1]. Specifically, the SBM model with $\rho = 1/2$ and Gaussian Features aligns with Section 4 in [1] and should be cited and compared. Additionally, for the SBM with GF where $\rho = 1/2$ and $a_1 = a_2 = a$, the expression for $I^*$ takes a simple form in Equation (4.4) of [1]. Providing explicit forms of $I^*$ for other models would enhance interpretability of the results.
Review Point: 2. For SBM with BEC/BSC, the algorithms assume prior knowledge of parameters $(\rho,\varepsilon, \alpha)$. The paper should mention this clearly and discuss why it's reasonable to assume this prior knowledge, or suggest how these parameters could be estimated. From a theory standpoint, achieving optimality might require paying a cost for adapting to these parameters, especially in regimes $\varepsilon = n^{-\beta}$ and $\alpha = n^{-\beta}$ for $\beta>0$.
Review Point: 3. The two-community assumption limits the algorithm’s practical use. While I understand that the optimality of the spectral algorithm depends on this assumption, the statement in Appendix A, "The entire framework of genie-aided estimation naturally generalizes to the multi-community case," is confusing. Can the authors elaborate on how the spectral algorithm could be modified, for example, to handle $K=3$ communities?
Review Point: 4. There are no experiments. It would be helpful to include numerical experiments demonstrating the algorithm’s performance on the specific models considered. [1] Abbe, E., Fan, J., & Wang, K. (2022). An $\ell_p$ theory of PCA and spectral clustering. The Annals of Statistics, 50(4), 2359-2385.
==================================================

Focused review:

1. More details needed about RLHF. At present, it's unclear why CoH outperforms RLHF. Is the trained RM ineffective at modeling preference (add RM performance to Fig3)? Or is the learning algorithm unable to leverage the RM effectively (could it be the choice of prompts used for RLHF?), in which case an alternate RM-based baseline could be considered (e.g. rejection sampling, reinforced self-training)?
2. Why do you only prompt with 'Good:' at inference time? It seems that an advantage of the different natural language feedback templates is that the CoH-trained LM will (hopefully) learn to *consider an initially generated bad response, and leverage it to produce a good response*. Perhaps leveraging this capability, and giving the LM an attempt to produce a better response, may yield further benefits. Some discussion or analysis about the potential of different natural language feedback templates at inference time would be useful.
3. More discussion is warranted about the performance relative to C-SFT. If my understanding is correct that C-SFT is essentially training with ("good: {good response}", "bad: {bad response}"), then I wonder if CoH with only ("good: {good response} bad: {bad response}", "bad: {bad response} good: {good response}" is expected to perform better (under the current inference setup)? If yes, why is this the case? If not, where is the advantage of CoH relative to C-SFT coming from?

Review Point: 1. More details needed about RLHF. At present, it's unclear why CoH outperforms RLHF. Is the trained RM ineffective at modeling preference (add RM performance to Fig3)? Or is the learning algorithm unable to leverage the RM effectively (could it be the choice of prompts used for RLHF?), in which case an alternate RM-based baseline could be considered (e.g. rejection sampling, reinforced self-training)?
Review Point: 2. Why do you only prompt with 'Good:' at inference time? It seems that an advantage of the different natural language feedback templates is that the CoH-trained LM will (hopefully) learn to *consider an initially generated bad response, and leverage it to produce a good response*. Perhaps leveraging this capability, and giving the LM an attempt to produce a better response, may yield further benefits. Some discussion or analysis about the potential of different natural language feedback templates at inference time would be useful.
Review Point: 3. More discussion is warranted about the performance relative to C-SFT. If my understanding is correct that C-SFT is essentially training with ("good: {good response}", "bad: {bad response}"), then I wonder if CoH with only ("good: {good response} bad: {bad response}", "bad: {bad response} good: {good response}" is expected to perform better (under the current inference setup)? If yes, why is this the case? If not, where is the advantage of CoH relative to C-SFT coming from?
==================================================

Focused review:

* The paper does not provide a thorough comparison of RLLTE with other existing RL frameworks.
* The proposed LLM-empowered copilot is in its early stages and may not be as effective as expected.
* The paper does not discuss potential limitations or challenges in implementing the proposed framework.

Review Point: * The paper does not provide a thorough comparison of RLLTE with other existing RL frameworks.
Review Point: * The proposed LLM-empowered copilot is in its early stages and may not be as effective as expected.
Review Point: * The paper does not discuss potential limitations or challenges in implementing the proposed framework.
==================================================

Focused review:

1. I found that the paper was hard to read, and at times imprecise. One suggestion I have is to write the paper in a way where it is easy to identify what concrete properties or statements are being proved.
2. For example, in Line 251, I was originally confused about why can we apply Eq (7), since that result only makes sense for the linearly separable case (especially since it’s talking about hard SVM). It might help if you formally stated the result of Ji and Telgarsky was stated and then also your variant where you include the bias, commenting explicitly on the difference between what “linearly separable” means in these two settings.
3. It seems that the problem setup is (WLOG) assuming that all the labels are -1, so that to classify correctly we should output a negative scalar. Can you explicitly state this (for example, by just defining what the labels are)

Review Point: 1. I found that the paper was hard to read, and at times imprecise. One suggestion I have is to write the paper in a way where it is easy to identify what concrete properties or statements are being proved.
Review Point: 2. For example, in Line 251, I was originally confused about why can we apply Eq (7), since that result only makes sense for the linearly separable case (especially since it’s talking about hard SVM). It might help if you formally stated the result of Ji and Telgarsky was stated and then also your variant where you include the bias, commenting explicitly on the difference between what “linearly separable” means in these two settings.
Review Point: 3. It seems that the problem setup is (WLOG) assuming that all the labels are -1, so that to classify correctly we should output a negative scalar. Can you explicitly state this (for example, by just defining what the labels are)
==================================================

Focused review:

1. Availability. This method employs a Shortest Path Estimator (SPE) to determine the shortest paths from all nodes to the goal and conducts experiments in a maze environment (15 x 15, 20 x 20, 25 x 25). However, this environment may be too simplistic to provide substantial empirical evidence. I wonder whether this method can be scaled to tasks with larger state spaces.
2. Experiments. Additional training details should be provided, such as the number of training steps, hyperparameters used, and whether the maze environment was randomly generated during the training process.
3. Clarity. The implementation of high-level algorithms by human design is best placed in the ‘Method’ section. And their specific implementation needs to be further elucidated.

Review Point: 1. Availability. This method employs a Shortest Path Estimator (SPE) to determine the shortest paths from all nodes to the goal and conducts experiments in a maze environment (15 x 15, 20 x 20, 25 x 25). However, this environment may be too simplistic to provide substantial empirical evidence. I wonder whether this method can be scaled to tasks with larger state spaces.
Review Point: 2. Experiments. Additional training details should be provided, such as the number of training steps, hyperparameters used, and whether the maze environment was randomly generated during the training process.
Review Point: 3. Clarity. The implementation of high-level algorithms by human design is best placed in the ‘Method’ section. And their specific implementation needs to be further elucidated.
==================================================

Focused review:

- The basic problem in this paper is indeed equivalent to the generalized target/label shift, where label distribution and conditional distribution change simultaneously. However, many important and closely related references are not introduced and discussed.
- Consider the existing results for generalized target/label shift, the generalization error analysis provided in Thm. 1 seems to be less compact and not informative.
- Important theoretical results for the main merits, i.e., conditional variant of SSD, are missing, which makes the proposed method less technically sound.
- The organization and clarity should be improved. Some justification and intuition for the math definition or theoretical results are insufficient.
- The experiment comparison is insufficient, where some related works are omitted.

Review Point: - The basic problem in this paper is indeed equivalent to the generalized target/label shift, where label distribution and conditional distribution change simultaneously. However, many important and closely related references are not introduced and discussed.
Review Point: - Consider the existing results for generalized target/label shift, the generalization error analysis provided in Thm.
Review Point: - Important theoretical results for the main merits, i.e., conditional variant of SSD, are missing, which makes the proposed method less technically sound.
Review Point: - The organization and clarity should be improved. Some justification and intuition for the math definition or theoretical results are insufficient.
Review Point: - The experiment comparison is insufficient, where some related works are omitted.
==================================================

Focused review:

#### 1. More related works should be discussed:
The authors should discuss additional related works or acknowledge that these related works have inspired them, including but not limited to:
+ At the level of model design, the proposed method seems to be a revised version of [1], which employs ROI-wise embedding rather than voxel-wise embedding.
+ At the level of research ideas, some works also train encoding models and then use them for neuroscience explorations, such as [2][3].
+ at the level of fMRI representation learning, [4][5] already show the use of multi-subject data can enhance each subject's representation, and [4][6][7] already show the use of other subject's fMRI can achieve few-shot transfer learning.
#### 2. Limited evaluation metrics:
In this paper, only voxel-wise Pearson coefficients and retrieval results are used as evaluation metrics, and the inclusion of more metrics such as $R^2$, MSE, etc. can further indicate the fMRI encoding accuracy.
#### 3. On fMRI replicable
The method proposed by the authors fails to address the issue of fMRI replicability, which is a common problem with regression-based fMRI encoding models. The authors already discuss this in their limitation and assume that the fMRI captured by subjects viewing the same image multiple times is the same. However, this assumption may greatly limit the training of fMRI encoding models.
[1] Hossein Adeli et al. Predicting brain activity using Transformers. bioRxiv, 2023: 2023.08. 02.551743.
[2] Andrew Luo et al. Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models. NeurIPS 2023.
[3] Andrew Luo et al. BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity. ICLR 2024.
[4] Shizun Wang et al. A cross-subject brain decoding framework. CVPR 2024.
[5] Guangyin Bao et al. Wills Aligner: A Robust Multi-Subject Brain Representation Learner. arXiv:2404.13282.
[6] Paul S. Scotti et al. MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data. ICML 2024.
[7] Zixuan Gong et al. MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and Semantic Correction. arXiv:2404.12630.

Review Point: + At the level of model design, the proposed method seems to be a revised version of [1], which employs ROI-wise embedding rather than voxel-wise embedding.
Review Point: + At the level of research ideas, some works also train encoding models and then use them for neuroscience explorations, such as [2][3].
==================================================

Focused review:

* The method design is a bit ad-hoc and does not seem to be generalizable enough for evaluating open-ended generation, considering how flexible/free-form the generation results can be. Specifically, the method requires splitting multiple answers being compared into the same amount of segments, and expecting that the answers are somewhat overlapping semantically. However, this may not work well when the generation results are largely different from each other, and this could be very common in open-ended generation. For example, in creative writing tasks, the answers being compared may not have any semantic overlap, and may even differ a lot in their lengths. It seems that the proposed method does not take these cases into account. Also, I'm not sure if splitting a response into multiple segments is really a good idea, since generation evaluation usually has to consider the answers' coherency (potentially spanning over long-range contexts that should be evaluated as a whole).
* The experiment evaluation only focuses on the consistency rates and does not provide evidence of its impact on the accuracy (e.g., whether the calibrated prediction corresponds better to human judgments). Such evaluations are obviously necessary because any naively deterministic method (for example, a system that always prefers the longer answer) will have a consistent rate of 100% but will not be useful in practice.
* I expect to see stronger baselines included (there are several methods aiming at enhancing LLM-as-evaluators, though they may not be completely addressing position bias). I'd encourage the authors to also discuss the new studies that came out after the ICLR submission deadline such as Zeng et al. (this is not a weakness but a suggestion) Reference:
Zeng et al. “Evaluating Large Language Models at Evaluating Instruction Following.” ArXiv abs/2310.07641

Review Point: * The experiment evaluation only focuses on the consistency rates and does not provide evidence of its impact on the accuracy (e.g., whether the calibrated prediction corresponds better to human judgments). Such evaluations are obviously necessary because any naively deterministic method (for example, a system that always prefers the longer answer) will have a consistent rate of 100% but will not be useful in practice.
Review Point: * I expect to see stronger baselines included (there are several methods aiming at enhancing LLM-as-evaluators, though they may not be completely addressing position bias). I'd encourage the authors to also discuss the new studies that came out after the ICLR submission deadline such as Zeng et al. (this is not a weakness but a suggestion) Reference: Zeng et al. “Evaluating Large Language Models at Evaluating Instruction Following.” ArXiv abs/2310.07641
==================================================

Focused review:

- Reporting p-values is not exactly ideal, especially when metrics are available.
- No general summary to assist with implementation.
- No examples conducted on datasets where heavy tails are known to be especially relevant (e.g. economic datasets).
- No discussion of, or comparisons to, similar developments in the normalizing flow literature (e.g. [1] and [2]).
[1] Jaini, P., Kobyzev, I., Yu, Y., & Brubaker, M. (2020). Tails of Lipschitz triangular flows. In International Conference on Machine Learning (pp. 4673-4681). PMLR.
[2] Liang, F., Mahoney, M., & Hodgkinson, L. (2022). Fat–Tailed Variational Inference with Anisotropic Tail Adaptive Flows. In International Conference on Machine Learning (pp. 13257-13270). PMLR.

Review Point: - Reporting p-values is not exactly ideal, especially when metrics are available.
Review Point: - No examples conducted on datasets where heavy tails are known to be especially relevant (e.g. economic datasets).
Review Point: - No discussion of, or comparisons to, similar developments in the normalizing flow literature (e.g. [1] and [2]). [1] Jaini, P., Kobyzev, I., Yu, Y., & Brubaker, M. (2020). Tails of Lipschitz triangular flows. In International Conference on Machine Learning (pp. 4673-4681). PMLR. [2] Liang, F., Mahoney, M., & Hodgkinson, L. (2022). Fat–Tailed Variational Inference with Anisotropic Tail Adaptive Flows. In International Conference on Machine Learning (pp. 13257-13270). PMLR.
==================================================

Focused review:

:
1) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).
2) I don't think the baseline comparisons made in the experiments are appropriate. The proposal is a method to choose the appropriate number of bins in the estimate, and should be compared to other methods to do so instead of to an arbitrary choice of number of bins as is done in section 5.2. Without this comparison, I have no way to judge if this is a good autotuning method or not. Reasonable comparisons could be, e.g., choosing b
by cross validation, or, in equal mass binning, choosing b
so that each bin has a reasonable number of samples for the error y ― k
to not be too large.
3) While the focus of the paper is on bias, it should be noted that by searching over many different bin sizes, the variance of ECE_sweep may be inflated. If this is to such an extent that the gains in bias relative to other autotuning methods are washed out, then this estimator would not be good. To judge this requires at least that the variances for ECE_sweep are reported, but these are never mentioned in the main text.
4) Choice of law in simulation in section 3, which are used to illustrate the dependence of bias on the number of bins, not aligned with the laws/curves in figure 3. Taking the latter as representative of the sort of laws and calibration curves that arise in practice, there are two issues:
4a) The pdfs of f
tend to be a lot more peaked near the end than the one explored in section 3 - this is borne out by the values of α , β
in the fits in Table 1. Beta(1.1,1) is remarkably flat compared to the curves in Fig 3.
4b) There seem to be a few different qualitative properties of the calibration curves - monotone but with a large intercept at 0; those with an inflection point in the middle; and those with the bulk lying below the y = x
line. In particular, all of them tend to have at least some region above the y = x
line. The choice of curve c 2
in section 3 doesn't completely align with any of these cases, but even if we make the case that it aligns with the third type, this leaves two qualitative behaviours unexplored.
In fact, the choice of laws is such that the error of the hard classifier that thresholds f at 1 / 2 is 26 %
. I don't think we're usually interested in the calibration of a predictor as poor as this in practice.
All of this makes me question the relevance of this simulation. Is the dependence of the bias on the number of bins as strong for the estimated laws as it is for these? Seeing the the equivalents of figs 7 and 8 for the laws from section 5 would go a long way in sorting this out.
5) Experiments: As I previously mentioned, I don't think the correct baselines are compared to. Instead of posing the method against other autotuning schemes, just one choice of the number of bins is taken. This already makes it near impossible to judge the efficacy of this method.
Despite this, even the data presented does not make a clear case for ECE_sweep. In Fig. 4 we see that the bias of EW_sweep is even worse than EW. This already means that the sweep estimate doesn't fix the issues of ECE_bin in all contexts. It is the case that EM_sweep has better bias than EM, but again, for samples large enough for the variances to be in control, it seems like these numbers are both converging to the same, so I don't see any distinct advantage when it comes to estimation. (of course, this is moot because this isn't the right comparison anyway)
Also, Fig. 5 is flawed because it compares EW and EM_sweep. It should either compare EM and EM_sweep, or EW and EW_sweep, I don't see why EW and EM_sweep are directly comparable.
Minor issues:
a) Algorithm (1) and the formula for ECE_sweep in section 4 don't compute the same thing. In algorithm (1), you find the largest b
such that the resulting y ― k
is a monotone sequence, and return the ECE_bin for this number of bins. In the formula, you maximise the ECE_bin for all b that yield a monotone y ― k
. From the preceding text, I assumed that the quantity in Algorithm (1) is intended.
b) Why is the L p
norm definition of the ECEs introduced at all? In the paper only p = 2
is used throughout. I feel like the p
just complicates things without adding much - even if you only present the L 2
definition, the fact that a generic p
can be used instead should be obvious to the audience.
c) Design considerations for ECE_sweep - it is worth noting that accuracy is not all that we want in an estimate of calibration error. For instance, one might reasonably want to add this as a regulariser when training a model in order to obtain better calibrated solutions. One issue with ECE_sweep is that it introduces a problem in that how the number of bins in the ECE_sweep estimate changes with a small change in model parameters seems very difficult to handle, which makes this a nondifferentiable loss. Broader issues of this form, and a discussion of how they may be mitigated, could lead to a more well rounded paper. Comments:
a) Exact monotonicity in the ECE_sweep proposal - I find the argument stemming from the monotonicity of the true calibration curve, and the idea to use this to nail down a maximum binning size interesting. However, why should we demand exact monotonicity in the bin heights? Each y ― k
will have noise at the scale of roughly b / n ,
(for equal mass binning with b
bins), and in my opinion, violation of monotonicity at this scale should not be penalised. Also, what if a few y ― k
s decrease but most are increasing (i.e., the sequence has a few falling regions, but the bulk is increasing)? Perhaps instead of dealing with this crudely, the error of a shape constrained estimator may serve as a better proxy.
b) Isn't the procedure for parametrically fitting the pdf of f
, and E [ Y f ( X ) ] ,
and then integrating the bias a completely different estimator for TCE of a model? In fact, if the laws are a good fit, as is claimed in section 5.1, then this plug in estimator might do well simply because the integration is exact. In fact, since the fit is parametric, this can further be automatically differentiated (if, say, f
were a DNN), and thus used to train.
c) It would be interesting to see what number of bins are ultimately adopted in the ECE_sweep computations that are performed.
Overall opinion: The lack of comparison to appropriate baselines makes it near impossible for me to judge the validity of the proposed estimator. I feel like this is a deep methodological flaw when it comes to evaluating the main proposal of the paper. This is a real pity because I quite like some of the ideas in the paper.
Due to the inability to evaluate the main contribution of the paper, i am rating it a strong reject. I'd be completely open to re-rating it if appropriate comparisons are performed, and the case for the method is properly made.

Review Point: 3) While the focus of the paper is on bias, it should be noted that by searching over many different bin sizes, the variance of ECE_sweep may be inflated. If this is to such an extent that the gains in bias relative to other autotuning methods are washed out, then this estimator would not be good. To judge this requires at least that the variances for ECE_sweep are reported, but these are never mentioned in the main text.
==================================================

Focused review:

1. **[Potential Efficiency Issue]** A major concern relates to the original SOLE’s efficiency, particularly in terms of speed and memory consumption. Aggregating raw 2D images into a 3D point cloud is likely to be slow. Even if this process is considered a preprocessing step, the loading and processing of per-point CLIP features could also be extremely resource-intensive. This may pose a limitation for real-world applications.
2. **[Evaluation of Efficiency]** Building on the previous point, the manuscript lacks a detailed breakdown of model speed and memory consumption. This is a crucial metric for real-world applications and the potential scalability of this method. Identifying which components contribute to inefficiency would be valuable for future research. Although some numbers are provided in the appendix, this analysis is important, even if the results are not entirely favorable.
3. **[Reliance on Over-segmentation GT]** Inherited from Mask3D, the proposed method also relies on graph-based over-segmentation results, which are used for ground truth labeling in ScanNet and ScanNet200. This reliance may introduce certain issues (though originally introduced by Mask3D).

Review Point: 1. **[Potential Efficiency Issue]** A major concern relates to the original SOLE’s efficiency, particularly in terms of speed and memory consumption. Aggregating raw 2D images into a 3D point cloud is likely to be slow. Even if this process is considered a preprocessing step, the loading and processing of per-point CLIP features could also be extremely resource-intensive. This may pose a limitation for real-world applications.
Review Point: 2. **[Evaluation of Efficiency]** Building on the previous point, the manuscript lacks a detailed breakdown of model speed and memory consumption. This is a crucial metric for real-world applications and the potential scalability of this method. Identifying which components contribute to inefficiency would be valuable for future research. Although some numbers are provided in the appendix, this analysis is important, even if the results are not entirely favorable.
Review Point: 3. **[Reliance on Over-segmentation GT]** Inherited from Mask3D, the proposed method also relies on graph-based over-segmentation results, which are used for ground truth labeling in ScanNet and ScanNet200. This reliance may introduce certain issues (though originally introduced by Mask3D).
==================================================

Focused review:

### Contribution:
- Line 256: It's appreciated and essential that the paragraph "Empirical validation of maximum-likelihood sharpening" verifies that sharpening provides downstream task improvement; however, in my view, this is a fairly known fact which could be presented in a more concise way, or acknowledged as a general fact.
- Definition 3.2: the generation-verification operation seems a bit restricted, but in a self-improvement context, this seems enough to me.
- The results seem limited to the specific algorithms chosen IN RLHF-sharpening mainly REBEL and XPO.
### Presentation:
- It can be misunderstood from the paper that self-improvement methods should exclusively be seen as sharpening methods, from the strong questions raised in the abstract and the introduction. I suggest rephrasing those as motivation to see self-improvement as sharpening.
- Line 194 typo: the expectation's long definition should be over $\pi$ instead of $\pi_{base}$.
- Proposition 3.1 introduces an important result, which is then not discussed. The transition to the next section is too abrupt.

Review Point: - Definition 3.2: the generation-verification operation seems a bit restricted, but in a self-improvement context, this seems enough to me.
Review Point: - The results seem limited to the specific algorithms chosen IN RLHF-sharpening mainly REBEL and XPO. ### Presentation:
Review Point: - It can be misunderstood from the paper that self-improvement methods should exclusively be seen as sharpening methods, from the strong questions raised in the abstract and the introduction. I suggest rephrasing those as motivation to see self-improvement as sharpening.
Review Point: - Proposition 3.1 introduces an important result, which is then not discussed. The transition to the next section is too abrupt.
==================================================

Focused review:

1. In the visualization results shown in Figure 3, the response labeled as “recover” in the first sample of the third row appears to be an error, as does the response in the last sample of the same row. These results indicate that while the current method enhances diversity, it still includes some erroneous responses. How do you ensure the quality of the generated responses?
2. It is intriguing that the ViT backbone of CLIP is considered as a unified vision encoder in MLLM. Could this architecture produce different patterns and further improve performance?
3. The definition of CFG is missing in Table 1.
4. While the paper provides extensive interpretation of the experimental results, it lacks an in-depth analysis of the reasons behind the observed patterns in the results.
5. The writing is somewhat verbose. For instance, in Subsection 3.6, the second sentence is redundant as it repeats the information in the first sentence.
6. Some equations could be improved; for example, Equations 9 and 10 differ by only one symbol.
7. There are a few typos, such as a missing period on line 82 and an incorrect number on line 360.

Review Point: 1. In the visualization results shown in Figure 3, the response labeled as “recover” in the first sample of the third row appears to be an error, as does the response in the last sample of the same row. These results indicate that while the current method enhances diversity, it still includes some erroneous responses. How do you ensure the quality of the generated responses?
Review Point: 2. It is intriguing that the ViT backbone of CLIP is considered as a unified vision encoder in MLLM. Could this architecture produce different patterns and further improve performance?
Review Point: 3. The definition of CFG is missing in Table 1.
Review Point: 4. While the paper provides extensive interpretation of the experimental results, it lacks an in-depth analysis of the reasons behind the observed patterns in the results.
Review Point: 5. The writing is somewhat verbose. For instance, in Subsection 3.6, the second sentence is redundant as it repeats the information in the first sentence.
Review Point: 6. Some equations could be improved; for example, Equations 9 and 10 differ by only one symbol.
==================================================

Focused review:

- The experiment results only show the performance judged by LLMs. However, relying on LLMs to provide a score (out of 100 as shown in the prompts) may not always be reliable, specifically with such kind of long outputs. For example, is it guaranteed a 80-scored passage is always better than a 75-scored passage.
- Some short examples showing how the actual final outputs look like by different methods could be helpful.
- The experiment setting of this paper and the baseline is not strictly fair, in the sense that AutoSurvey retrieves papers from a database while this paper use the 2-hop neighbor network of the human generated survey. The earlier one is a more realistic setting to me, because writing a literature review needs to find relevant papers.
- The paper needs some proofreads as multiple typos exist. For example, at the end of page 8, 'opf' -> 'of'

Review Point: - The experiment results only show the performance judged by LLMs. However, relying on LLMs to provide a score (out of 100 as shown in the prompts) may not always be reliable, specifically with such kind of long outputs. For example, is it guaranteed a 80-scored passage is always better than a 75-scored passage.
Review Point: - Some short examples showing how the actual final outputs look like by different methods could be helpful.
Review Point: - The experiment setting of this paper and the baseline is not strictly fair, in the sense that AutoSurvey retrieves papers from a database while this paper use the 2-hop neighbor network of the human generated survey. The earlier one is a more realistic setting to me, because writing a literature review needs to find relevant papers.
==================================================

Focused review:

1. Although Figure 2 shows some motivating failure examples for text concepts, some of them seem to be solved by prompting engineering, e.g., "what are useful visual features for distinguishing a blue Grosebeak in a photo", "eliminating the answers with the concept cannot observe from an image".
2. The performance improvement is relatively minor as compared to Labo. Although the interpretability is the key idea, while the user study with 27 users shows significant improvement, the results may be very subjective and sensitive to the selected samples. It is suggested to propose some new metrics that can automatically measure the interpretability.
3. Important references are missing, e.g., [A] and its subsequent works. It is suggested to compare with it empirically and theoretically.
4. The references are out-of-date, i.e., only two papers published in 2023 are cited.
[A] Visual Classification via Description from Large Language Models. ICLR, 2023.

Review Point: 1. Although Figure 2 shows some motivating failure examples for text concepts, some of them seem to be solved by prompting engineering, e.g., "what are useful visual features for distinguishing a blue Grosebeak in a photo", "eliminating the answers with the concept cannot observe from an image".
Review Point: 2. The performance improvement is relatively minor as compared to Labo. Although the interpretability is the key idea, while the user study with 27 users shows significant improvement, the results may be very subjective and sensitive to the selected samples. It is suggested to propose some new metrics that can automatically measure the interpretability.
Review Point: 3. Important references are missing, e.g., [A] and its subsequent works. It is suggested to compare with it empirically and theoretically.
Review Point: 4. The references are out-of-date, i.e., only two papers published in 2023 are cited. [A] Visual Classification via Description from Large Language Models. ICLR, 2023.
==================================================

Focused review:

- Coverage of the contemporary literature is insufficient. PubMedBERT, BioLinkBERT, and SapBERT are all models released in 2021-2022, which is more than 2 years ago. Much work has been produced on the topic since then, which is not engaged in this paper. A quick search yielded `BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights`, `CODER: Knowledge-infused cross-lingual medical term embedding for term normalization` and `MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval`. Not engaging these recent works means that stronger baselines have been omitted.
- The gains over chosen baselines are insufficient. Concerning QA (Table 1), the proposed strategy does not perform as well as the DRAGON models, and the improvements produced by GRABLI training, as measured in the paper, are not significant (one would need at least 2x the standard deviation for this, while the measured improvement rarely exceeds even 1x the standard deviation). Concerning NEL (Table 2), pretrained models undergoing GRABLI training do not score better than SapBERT, and SapBERT performs worse after GRABLI training. Given that the proposed methodology is significantly more complicated than SapBERT training, obvious gains in the evaluated benchmarks would be desirable. Concerning RE (Table 6), no baseline was provided and the GRABLI training improvements are again not likely to be significant. Finally, SapBERT is not SOTA anymore and stronger baselines should have been considered.
- The complexity added by the GNN does not seem necessary either, since LLM embedding of a linearized graph performs about equally well, as per Table 3. Furthermore, it's unclear whether two different LMs are required or whether one LM could be used for text and linearized graph embedding, as in other related works.
- The quality of the semantic entity representations themselves is not evaluated, for instance through Link Prediction tasks. The article is only focusing on the training methodology aspect but does not provide sufficient evidence that the training methodology performs better or is more efficient than other training methodologies.

Review Point: - Coverage of the contemporary literature is insufficient. PubMedBERT, BioLinkBERT, and SapBERT are all models released in 2021-2022, which is more than 2 years ago. Much work has been produced on the topic since then, which is not engaged in this paper. A quick search yielded `BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights`, `CODER: Knowledge-infused cross-lingual medical term embedding for term normalization` and `MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval`. Not engaging these recent works means that stronger baselines have been omitted.
Review Point: - The complexity added by the GNN does not seem necessary either, since LLM embedding of a linearized graph performs about equally well, as per Table 3. Furthermore, it's unclear whether two different LMs are required or whether one LM could be used for text and linearized graph embedding, as in other related works.
Review Point: - The quality of the semantic entity representations themselves is not evaluated, for instance through Link Prediction tasks. The article is only focusing on the training methodology aspect but does not provide sufficient evidence that the training methodology performs better or is more efficient than other training methodologies.
==================================================

Focused review:

There are no major weaknesses in the paper that the reviewer could find. Please see the questions section below for some clarifying questions.
Some minor comments about improving the quality of presentation are as follows:
1) Consider paraphrasing some sentences to make them clearer:
* On page 2, "Considering the evident a misalignment where the LLM trains on natural language tokens but infers using distillation vectors..."
* In Appendix D, "This will not only lose the information from the discarded tokens and cannot distill demonstration with large $K$ (e.g. $K > 1000$ (Hao et al., 2022))."
2) On page 2, "...we embarked on a in-depth..." --> "...we embarked on **an** in-depth..."
3) In Table 2, "0-shot" --> "zero-shot"
4) In Appendix D, "...are shading insights for future work." --> "...shedding insights for future work."?

Review Point: * On page 2, "Considering the evident a misalignment where the LLM trains on natural language tokens but infers using distillation vectors..." * In Appendix D, "This will not only lose the information from the discarded tokens and cannot distill demonstration with large $K$ (e.g. $K > 1000$ (Hao et al., 2022))."
==================================================

Focused review:

My major concerns are about the experiments:
- The proposed method cannot achieve SOTA performance on Camelyon 16 and 17 benchmarks. Why not listing the challenge winners in table 1? For example, in Camelyon 16 challenge, the winner (Harvard & MIT) already achieves 99.4% AUC. Also, the SOTA of Camelyon 17 is from DeepBio Inc. For more recent results on the two benchmarks, you can check table 1 in PFA-Scannet [MICCAI 2019]. It is suggested to include a comparison to these top-performing methods and explain how your approach compares in terms of performance and computational efficiency.
- Why not using the challenge metrics for evaluation? For Camelyon16, FROC is a more challenging metric compared with AUC reported in this paper. Also, kappa score should be compared for Camelyon17 benchmark. Please explain why you chose AUC over FROC for Camelyon16 and to provide results using both metrics if possible. Similarly, for Camelyon17, results using the kappa score are requested, which would allow for a more direct comparison to other methods evaluated on this benchmark.
- The tumor localization performance is not satisfying. Even though the proposed method is not prioritized for this task, the gap of CAMIL, around 3% dice, is too large. Please discuss potential reasons for this performance gap in tumor localization, and suggest ways that might improve this aspect of proposed method, even if it's not the primary focus.

Review Point: - The proposed method cannot achieve SOTA performance on Camelyon 16 and 17 benchmarks. Why not listing the challenge winners in table 1? For example, in Camelyon 16 challenge, the winner (Harvard & MIT) already achieves 99.4% AUC. Also, the SOTA of Camelyon 17 is from DeepBio Inc. For more recent results on the two benchmarks, you can check table 1 in PFA-Scannet [MICCAI 2019]. It is suggested to include a comparison to these top-performing methods and explain how your approach compares in terms of performance and computational efficiency.
Review Point: - Why not using the challenge metrics for evaluation? For Camelyon16, FROC is a more challenging metric compared with AUC reported in this paper. Also, kappa score should be compared for Camelyon17 benchmark. Please explain why you chose AUC over FROC for Camelyon16 and to provide results using both metrics if possible. Similarly, for Camelyon17, results using the kappa score are requested, which would allow for a more direct comparison to other methods evaluated on this benchmark.
Review Point: - The tumor localization performance is not satisfying. Even though the proposed method is not prioritized for this task, the gap of CAMIL, around 3% dice, is too large. Please discuss potential reasons for this performance gap in tumor localization, and suggest ways that might improve this aspect of proposed method, even if it's not the primary focus.
==================================================

Focused review:

- The writing style leaves a lot to be desired as the text is quite hard to read. The narrative is non-linear, many important things for understanding are put in the Appendix, even the notations. Section 3.2 MAIN ALGORITHM SKETCH, where the essence of the underlying algorithm is revealed, is overflowing with links to Appendix.
- The paper is widely cited following paper (Seddik et al, 2021), so it is difficult to understand what the current paper is about without reading the cited paper.
- The most important weakness, in my opinion, is too few experiments that have been conducted only on synthetic data. In general, experimental section 4 is very short, subsections 4.2 and 4.3 consist of only figures. Thus, it is not clear at all the relevance of this paper, where practically the presented results can be applied.
- Neither is there any comparison with other existing methods.
- Only rank-2 order-3 model is considered, which significantly reduces the breadth of application of the method and, as a consequence, its practical value.

Review Point: - The writing style leaves a lot to be desired as the text is quite hard to read. The narrative is non-linear, many important things for understanding are put in the Appendix, even the notations. Section 3.2 MAIN ALGORITHM SKETCH, where the essence of the underlying algorithm is revealed, is overflowing with links to Appendix.
Review Point: - The paper is widely cited following paper (Seddik et al, 2021), so it is difficult to understand what the current paper is about without reading the cited paper.
Review Point: - The most important weakness, in my opinion, is too few experiments that have been conducted only on synthetic data. In general, experimental section 4 is very short, subsections 4.2 and 4.3 consist of only figures. Thus, it is not clear at all the relevance of this paper, where practically the presented results can be applied.
Review Point: - Neither is there any comparison with other existing methods.
Review Point: - Only rank-2 order-3 model is considered, which significantly reduces the breadth of application of the method and, as a consequence, its practical value.
==================================================

Focused review:

1. The paper does not compare to other methods mentioned in literature review on incorporate Human Edits as Feedback to improve text summarization, for example how the used unlikelihood learning method is compared to Reinforcement learning based and constrastive learning based. The title seems to be a bit overselling.
2. The numbers shown in the experiment results look every close, some claims might not be true given the unclear significance. For example, in Table 3, the claims about comparing different variant of the proposed methods (SALT_x)
3. The evaluation majorly rely on automatic metric (ROUGE) to evaluate the summarization system, which has been proved to unreliable in many areas. The human evaluation is just a quick sanity check with very small (25) samples. It would be nice to expand this to larger one, and also show some case analysis on the model behaviors.
4. The paper mainly focus on single datasets, it would be nice to bring some experiments on other public dataset in the appendix to the main text.

Review Point: 1. The paper does not compare to other methods mentioned in literature review on incorporate Human Edits as Feedback to improve text summarization, for example how the used unlikelihood learning method is compared to Reinforcement learning based and constrastive learning based. The title seems to be a bit overselling.
Review Point: 2. The numbers shown in the experiment results look every close, some claims might not be true given the unclear significance. For example, in Table 3, the claims about comparing different variant of the proposed methods (SALT_x) 3. The evaluation majorly rely on automatic metric (ROUGE) to evaluate the summarization system, which has been proved to unreliable in many areas. The human evaluation is just a quick sanity check with very small (25) samples. It would be nice to expand this to larger one, and also show some case analysis on the model behaviors.
Review Point: 4. The paper mainly focus on single datasets, it would be nice to bring some experiments on other public dataset in the appendix to the main text.
==================================================

Focused review:

1.	The text in the figures is too small for easy readability and needs enlargement for better clarity.
2.	A more thorough justification is needed for the chosen metric of representation harm, given its critical importance to the paper's analysis.
3.	The necessity for different data setups in Sections 5 and 6, particularly the choice of causal mediation analysis for Section 6, lacks proper motivation and explanation.
4.	The paper's primary limitation is the absence of a comprehensive large-scale study, especially in the realm of self-supervised learning (SSL).

Review Point: 1. The text in the figures is too small for easy readability and needs enlargement for better clarity.
Review Point: 2. A more thorough justification is needed for the chosen metric of representation harm, given its critical importance to the paper's analysis.
Review Point: 3. The necessity for different data setups in Sections 5 and 6, particularly the choice of causal mediation analysis for Section 6, lacks proper motivation and explanation.
Review Point: 4. The paper's primary limitation is the absence of a comprehensive large-scale study, especially in the realm of self-supervised learning (SSL).
==================================================

Focused review:

1.	Could the optimal Q-matrix be theoretically symmetric? Would it be feasible to introduce symmetry constraints during the learning process, or alternatively, to learn only half of the matrix (e.g., the upper triangular portion)?
2.	Is there a theoretical relationship between $\mathcal{L}_{lls}$ and the vanilla cross-entropy loss?
If the above issues are adequately addressed, I will consider increasing my score further.
Small issues:
1.	The citation format used in this paper appears incorrect, please refine it carefully.
2.	Line 214-215: $-H(p_{lls}, \hat{p})$ - > $H(p_{lls}, \hat{p})$, $-H(\hat{p}, p_{lls})$ - > $H(\hat{p}, p_{lls})$

Review Point: 1. Could the optimal Q-matrix be theoretically symmetric? Would it be feasible to introduce symmetry constraints during the learning process, or alternatively, to learn only half of the matrix (e.g., the upper triangular portion)?
Review Point: 2. Is there a theoretical relationship between $\mathcal{L}_{lls}$ and the vanilla cross-entropy loss? If the above issues are adequately addressed, I will consider increasing my score further. Small issues:
Review Point: 1. The citation format used in this paper appears incorrect, please refine it carefully.
Review Point: 2. Line 214-215: $-H(p_{lls}, \hat{p})$ - > $H(p_{lls}, \hat{p})$, $-H(\hat{p}, p_{lls})$ - > $H(\hat{p}, p_{lls})$
==================================================

Focused review:

Based on the proposed technique, the amount of data that can be transmitted is limited. Therefore, it is crucial to prioritize foreground objects and bring them in a sparse manner to distinguish the overall shape, rather than bringing only a few parts. Regarding this aspect, I wonder if there are any criteria or tendencies for determining which points should be selected and what rules should be followed. Additionally, I am curious about the extent of performance degradation when background information is included instead of solely focusing on the foreground.
If we follow the logic mentioned above, I wonder if we can expect sufficient performance improvement in early or late collaboration by only bringing foreground information and adjusting it according to the available bandwidth.
- In Figure 4(a), there is a section where the performance of the proposed method rises sharply. I am curious about the reason behind this phenomenon and how the performance of the proposed method would differ from existing technologies if the communication volume is lower than this point.
- On Page 7, Line 373, it is mentioned that the proposed method shows improvements of 5.7%, 7.3%, and 12.8%, but it is unclear which methods are being compared.
- On Page 7, Line 377, the term "late collaboration" is used. I am curious about which specific technique this refers to.

Review Point: - In Figure 4(a), there is a section where the performance of the proposed method rises sharply. I am curious about the reason behind this phenomenon and how the performance of the proposed method would differ from existing technologies if the communication volume is lower than this point.
Review Point: - On Page 7, Line 373, it is mentioned that the proposed method shows improvements of 5.7%, 7.3%, and 12.8%, but it is unclear which methods are being compared.
Review Point: - On Page 7, Line 377, the term "late collaboration" is used. I am curious about which specific technique this refers to.
==================================================

Focused review:

1. Related works not well addressed. The long-tail categories studied in the paper is the same as the node-level imbalanced-class problem in graph. The imbalanced class problem has been studied intensively for graphs, which is closely related to this work but not sufficiently discussed in its related works. The paper lacks a thorough review of related literature. Some missing related works are [1-6].
2. Following the above point, the experiments should include some of the missing imbalanced class baselines.
3. The correctness of Corollary 1 is unclear. Why can contrastive learning guarantee to learn the predictors $f_1^{(l)}, . . . , f_T^{(l)}$ with $Range(f_1^{(l)}, . . . , f_T^{(l)}) < Range(f_1, . . . , f_T)$? In its proof, why do we only need to compare the relationship between $\sum _t 1/(n_t^{(l)})$ and $\sum _t 1/(n_t)$? And how is the special case of all nodes in one hypertask generalized to prove $\sum _t 1/(n_t^{(l)})\leq \sum _t 1/(n_t)$? The proof should be clearly given step-by-step instead of ambiguously stated.
[1] Imgcl: Revisiting graph contrastive learning on imbalanced node classification
[2] Boosting-GNN: boosting algorithm for graph networks on imbalanced node classification
[3] Graph neural network with curriculum learning for imbalanced node classification
[4] Co-Modality Graph Contrastive Learning for Imbalanced Node Classification
[5] Diving into Unified Data-Model Sparsity for Class-Imbalanced Graph Representation Learning
[6] TAM: topology-aware margin loss for class-imbalanced node classification

Review Point: 1. Related works not well addressed. The long-tail categories studied in the paper is the same as the node-level imbalanced-class problem in graph. The imbalanced class problem has been studied intensively for graphs, which is closely related to this work but not sufficiently discussed in its related works. The paper lacks a thorough review of related literature. Some missing related works are [1-6].
Review Point: 2. Following the above point, the experiments should include some of the missing imbalanced class baselines.
Review Point: 3. The correctness of Corollary 1 is unclear. Why can contrastive learning guarantee to learn the predictors $f_1^{(l)}, .
==================================================

Focused review:

1. The scope of this paper is limited.
- In this paper, the authors focus only on the subpopulation poisoning attacks. To the best of my knowledge, this particular attack type (rather than the general data poisoning) is still not yet a widely recognized threat.
- In particular, this paper only focuses on the label flipping subpopulation poisoning attack. It further limits the generalizability of the ideas in this paper.
- The main finding (i.e., that more complex models are more vulnerable to such attacks) seems to be expected. More importantly, the authors do not provide insights on how to exploit some of the understandings found in this paper.
2. There are some potential over-claims.
- Line 19-21: To the best of my knowledge, Theorem 1 is only related to locally dependent learners instead of overparameterized models, not to mention model capacity.
- Line 41-44: missing the type of backdoor attacks [1].
- Line 130-131: please provide references or experiments to show that the previous findings are not necessarily true in subpopulation poisoning attacks.
3. Theorem 1 seems to be a straightforward extension of the one proposed in [2].
4. The authors should discuss potential applications of their findings, instead of simply highlighting the need for more attention for defenses.
5. The authors should also conduct experiments on other types of poisoning attacks, instead of just the label flipping subpopulation poisoning attack.
Minor Comments
1. There are still many typos (e.g., Line 183, Line 204). References
1. Backdoor Learning: A Survey.
2. Subpopulation data poisoning attacks.
PS: I am not very familiar with subpopulation poisoning attacks, although I did a lot of work on data poisoning and its defenses. Please feel free to correct me if I have any misunderstanding. I am willing to increase my scores if the authors can address (parts of) my concerns.

Review Point: - In this paper, the authors focus only on the subpopulation poisoning attacks. To the best of my knowledge, this particular attack type (rather than the general data poisoning) is still not yet a widely recognized threat.
Review Point: - In particular, this paper only focuses on the label flipping subpopulation poisoning attack. It further limits the generalizability of the ideas in this paper.
Review Point: - The main finding (i.e., that more complex models are more vulnerable to such attacks) seems to be expected. More importantly, the authors do not provide insights on how to exploit some of the understandings found in this paper.
Review Point: - Line 19-21: To the best of my knowledge, Theorem 1 is only related to locally dependent learners instead of overparameterized models, not to mention model capacity.
Review Point: - Line 41-44: missing the type of backdoor attacks [1].
Review Point: - Line 130-131: please provide references or experiments to show that the previous findings are not necessarily true in subpopulation poisoning attacks.
Review Point: 3. Theorem 1 seems to be a straightforward extension of the one proposed in [2].
Review Point: 4. The authors should discuss potential applications of their findings, instead of simply highlighting the need for more attention for defenses.
Review Point: 2. Subpopulation data poisoning attacks. PS: I am not very familiar with subpopulation poisoning attacks, although I did a lot of work on data poisoning and its defenses. Please feel free to correct me if I have any misunderstanding. I am willing to increase my scores if the authors can address (parts of) my concerns.
==================================================

Focused review:

My main concern is on the positioning of the paper with respect to similar work on alleviating GNN redundancy, i.e., RFGNN (Chen et al., 2022), and the weak experimental results.
### Comparison with RFGNN
- This work argues to alleviate over-squashing based on the results from RFGNN (Chen et al., 2022). However, as the authors argue, their proposed GNN architecture is different from RFGNN. Hence the logic is incomplete, i.e., it is not clear whether the proposed architecture alleviates over-squashing based on the same logic as RFGNN.
- Upon reading Appendix A, the authors seem to claim that RFGNN introduces more redundancy compared to the proposed work. Since there is no clear explanation of how redundancy is harmful to GNN tasks, it is hard for me to understand the benefit of the proposed DAG-MLP.
- Furthermore, the authors do not compare the expressive power of DAG-MLP compared to RFGNN. One might argue that RFGNN might be more expressive than the proposed DAG-MLP at the cost of introducing more redundancy.
- In addition, the authors claim speed-up of RFGNN as another benefit. I wonder if the authors could empirically show this in a meaningful scenario, e.g., large-scale graphs.
### Weak experiments (TUDataset)
- Overall, I think TUDataset is not good enough for evaluating the performance of DAG-MLP in practical scenarios. Especially, to validate the ability of DAG-MLP to alleviate over-squashing, I strongly suggest the long-range graph benchmark (Dwivedi et al., 2022) to run the proposed DAG-MLP.
- The proposed work underperforms compared to the PathGNN. While the authors argue that PathGNN takes exponential running time, the actual running time is not reported. Hence it is hard to tell whether the issue is practically relevant.
- The statistical box plot in Appendix F should be similarly drawn for the baselines to make a fair comparison.
- The authors use four versions of DAG-MLP (0/1-NT, fixed single height/combined heights) while the relevant baselines have usually one or two versions (PathGNN has three versions, but DAG-MLP is not directly compared due to computational complexity). This makes the comparison unfair especially for TUDataset with high variance scores.
- The list of baselines is not comprehensive enough to check whether if performance improvement of the proposed DAG-MLP is practically relevant.

Review Point: - Upon reading Appendix A, the authors seem to claim that RFGNN introduces more redundancy compared to the proposed work. Since there is no clear explanation of how redundancy is harmful to GNN tasks, it is hard for me to understand the benefit of the proposed DAG-MLP.
Review Point: - Furthermore, the authors do not compare the expressive power of DAG-MLP compared to RFGNN. One might argue that RFGNN might be more expressive than the proposed DAG-MLP at the cost of introducing more redundancy.
Review Point: - In addition, the authors claim speed-up of RFGNN as another benefit. I wonder if the authors could empirically show this in a meaningful scenario, e.g., large-scale graphs. ### Weak experiments (TUDataset) - Overall, I think TUDataset is not good enough for evaluating the performance of DAG-MLP in practical scenarios. Especially, to validate the ability of DAG-MLP to alleviate over-squashing, I strongly suggest the long-range graph benchmark (Dwivedi et al., 2022) to run the proposed DAG-MLP.
Review Point: - The proposed work underperforms compared to the PathGNN. While the authors argue that PathGNN takes exponential running time, the actual running time is not reported. Hence it is hard to tell whether the issue is practically relevant.
Review Point: - The statistical box plot in Appendix F should be similarly drawn for the baselines to make a fair comparison.
Review Point: - The authors use four versions of DAG-MLP (0/1-NT, fixed single height/combined heights) while the relevant baselines have usually one or two versions (PathGNN has three versions, but DAG-MLP is not directly compared due to computational complexity). This makes the comparison unfair especially for TUDataset with high variance scores.
Review Point: - The list of baselines is not comprehensive enough to check whether if performance improvement of the proposed DAG-MLP is practically relevant.
==================================================

Focused review:

- The reliance on LLMs as recovery tools introduces significant computational cost, which may limit the scalability of Genshin for real-time or resource-constrained applications. LLMs (e.g. Llama, Vicuna, GPT) themselves should be robust enough to the token-level perturbations. I do not see the necessity to use LLMs as an intermediate agent for input recovery and send the input to an LM (i.e. BERT or RoBERTa).
- The experimental setting is limited in terms of tasks and models. This paper did not justify the necessity of using an LM for inference while LLMs are available, which makes the experiment setting a bit confusing where only LMs are being evaluated.

Review Point: - The reliance on LLMs as recovery tools introduces significant computational cost, which may limit the scalability of Genshin for real-time or resource-constrained applications. LLMs (e.g. Llama, Vicuna, GPT) themselves should be robust enough to the token-level perturbations. I do not see the necessity to use LLMs as an intermediate agent for input recovery and send the input to an LM (i.e. BERT or RoBERTa).
Review Point: - The experimental setting is limited in terms of tasks and models. This paper did not justify the necessity of using an LM for inference while LLMs are available, which makes the experiment setting a bit confusing where only LMs are being evaluated.
==================================================

Focused review:

-	Expanding from image-level to region-level instruction tuning seems like a natural progression, and the approach is straightforward without providing a fresh perspective. Some other papers also explore the region-level large language models [1] but lack the performance comparison.
-	It appears that while this paper utilized more datasets for training, the improvement in results is relatively marginal, as shown in Table 5.
-	This work lacks a comparison of parameters. The current models seem to be quite large, especially large language models. A comparison should be conducted at the same parameter level, e.g., Table 6.
[1] ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning.

Review Point: - Expanding from image-level to region-level instruction tuning seems like a natural progression, and the approach is straightforward without providing a fresh perspective. Some other papers also explore the region-level large language models [1] but lack the performance comparison.
Review Point: - It appears that while this paper utilized more datasets for training, the improvement in results is relatively marginal, as shown in Table 5.
Review Point: - This work lacks a comparison of parameters. The current models seem to be quite large, especially large language models. A comparison should be conducted at the same parameter level, e.g., Table 6. [1] ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning.
==================================================

Focused review:

1. The paper should address a fundamental question: why should we use LLMs for IoT data tasks, which are typically numerical in nature? While LLMs excel at handling textual data, they are not known for expertise in processing numerical data. Given the high cost associated with LLMs, what unique advantages do they offer for IoT tasks, especially when many other solutions are already available?
2. The paper lacks a baseline comparison with traditional approaches for IoT tasks. Without evaluating the gains or advantages of LLMs over these conventional methods (e.g., in terms of accuracy, efficiency, or cost), the benefits of using LLMs for IoT remain unclear.
3. Although the paper incorporates certain adaptations, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG) to make LLMs work for IoT tasks, these methods are standard. The paper’s novelty appears limited in this regard, and it would benefit from additional details and examples to illustrate precisely how these steps are implemented.

Review Point: 1. The paper should address a fundamental question: why should we use LLMs for IoT data tasks, which are typically numerical in nature? While LLMs excel at handling textual data, they are not known for expertise in processing numerical data. Given the high cost associated with LLMs, what unique advantages do they offer for IoT tasks, especially when many other solutions are already available?
Review Point: 2. The paper lacks a baseline comparison with traditional approaches for IoT tasks. Without evaluating the gains or advantages of LLMs over these conventional methods (e.g., in terms of accuracy, efficiency, or cost), the benefits of using LLMs for IoT remain unclear.
Review Point: 3. Although the paper incorporates certain adaptations, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG) to make LLMs work for IoT tasks, these methods are standard. The paper’s novelty appears limited in this regard, and it would benefit from additional details and examples to illustrate precisely how these steps are implemented.
==================================================

Focused review:

- Most the techniques are existing, and most of the baselines works use the same motivation. The GNN part is similar to GAT actually.
- The paper does not provide a clear explanation of how the dual fusion mechanism works, and what are the benefits of fusing cross-modal embeddings from different layers.

Review Point: - Most the techniques are existing, and most of the baselines works use the same motivation. The GNN part is similar to GAT actually.
Review Point: - The paper does not provide a clear explanation of how the dual fusion mechanism works, and what are the benefits of fusing cross-modal embeddings from different layers.
==================================================

Focused review:

Motivation:
The authors' main MOTIVATION consists in solving the following two problems (the problems mentioned in the second paragraph of the introduction are not sure whether they are motivations, so they are placed in the Question section): 1) Existing work has predominantly relied on supervised learning, often dealing with limited data from a few categories (introduction, first line of the third paragraph); 2) Limitations of the EEG feature extractor: convolution in the temporal and spatial dimensions respectively (the second sentence from the bottom of the third paragraph of introduction)
W1:Indeed most of the existing EEG tasks (emotion recognition, motor imagery) are supervised learning, but Decoding tasks (such as EEG2speech, EEG2Image) are different from traditional EEG tasks. The essence of the task is cross-modal conversion rather than Classification, self-supervised learning is a very common method in decoding tasks. Comparisons with supervised learning for traditional tasks are therefore unreasonable.
W2: Many existing EEG feature extraction methods focus on the spatial-temporal relationship of EEG, such as methods based on spatial-temporal graph, or spatial-temporal attention, or spatial-temporal convolution that is very similar to TSconv in NICE (Such as TSception). However, these very common methods are not mentioned in this paper, so this motivation is unreasonable. Experiment：
W1: The main experiment only included one baseline, but there has been a lot of work on EEG vision decoding, such as Mind-Vis in CVPR. It is difficult to demonstrate the superiority of NICE by only comparing it with one baseline on a not commonly used dataset.
W2: There is no analysis of why NICE achieves better results than BraVL.
W3: In the Encoder comparison experiment, the baseline compared with TSConv did not include methods used for EEG feature extraction in the past five years.

Review Point: 1) Existing work has predominantly relied on supervised learning, often dealing with limited data from a few categories (introduction, first line of the third paragraph);
Review Point: 2) Limitations of the EEG feature extractor: convolution in the temporal and spatial dimensions respectively (the second sentence from the bottom of the third paragraph of introduction) W1:Indeed most of the existing EEG tasks (emotion recognition, motor imagery) are supervised learning, but Decoding tasks (such as EEG2speech, EEG2Image) are different from traditional EEG tasks. The essence of the task is cross-modal conversion rather than Classification, self-supervised learning is a very common method in decoding tasks. Comparisons with supervised learning for traditional tasks are therefore unreasonable.
==================================================

Focused review:

1. Although the KSL is smaller compared to the size of the model, it must have some sort of slow-down associated with it since it appears as an additional layer with an additional step across K subcomponents. What is the speed reduction in using this method?
2. This paper makes multiple references to VAEs as inspiration for the latent vector $z$, but this connection is never formally introduced, nor are any details about what is being referred to in VAEs. Some formal background and direct linking would strengthen the work.
3. The notation and writing is not always the most clear, where some key variables are not clearly defined, and some motivation is not clearly written. For example, latent “knowledge” vector $z$ is not clearly defined nor is its length $K$, and the notion of knowledge is redefined several times in the text, including as a “latent relative concept” or “co-occurence pattern of tokens with similar semantics”.
4. The published parameter settings for each baseline may not be the fair comparison here, what may be more fair is scaling the baselines according to the parameter budget or throughput associated with the DST method.
5. The LLM experiments are not compared to few-shot/zero-shot prompting despite these models being able to perform in-context learning. The LLM experiments (Table 4) need some sort of baseline to compare to, like in Table 3.
6. $L_{KDL}$ is not ablated to show its usefulness in this work.
7. Some code or pseudocode would strengthen knowing how the KSL/KDM is actually implemented. For example, it is unclear how the selection process works for the Waz matrices, and the minimum operation in KDM is also unclear as to how this is differentiated.

Review Point: 1. Although the KSL is smaller compared to the size of the model, it must have some sort of slow-down associated with it since it appears as an additional layer with an additional step across K subcomponents. What is the speed reduction in using this method?
Review Point: 2. This paper makes multiple references to VAEs as inspiration for the latent vector $z$, but this connection is never formally introduced, nor are any details about what is being referred to in VAEs. Some formal background and direct linking would strengthen the work.
Review Point: 3. The notation and writing is not always the most clear, where some key variables are not clearly defined, and some motivation is not clearly written. For example, latent “knowledge” vector $z$ is not clearly defined nor is its length $K$, and the notion of knowledge is redefined several times in the text, including as a “latent relative concept” or “co-occurence pattern of tokens with similar semantics”.
Review Point: 4. The published parameter settings for each baseline may not be the fair comparison here, what may be more fair is scaling the baselines according to the parameter budget or throughput associated with the DST method.
Review Point: 5. The LLM experiments are not compared to few-shot/zero-shot prompting despite these models being able to perform in-context learning. The LLM experiments (Table 4) need some sort of baseline to compare to, like in Table 3.
Review Point: 6. $L_{KDL}$ is not ablated to show its usefulness in this work.
Review Point: 7. Some code or pseudocode would strengthen knowing how the KSL/KDM is actually implemented. For example, it is unclear how the selection process works for the Waz matrices, and the minimum operation in KDM is also unclear as to how this is differentiated.
==================================================

Focused review:

1.	Feature selection techniques are very common in machine learning and also represent a standard paradigm. While using the Fisher score to measure the importance of neural network parameters is beneficial, it remains a straightforward application and lacks innovation.
2.	Although the authors' model achieves excellent results on multiple datasets, it is not state-of-the-art, and there is a lack of corresponding model comparisons in Table 1, such as IFANet (CVPR 2024).
3.	Although the authors provide a comparison of the trainable layer information structure distribution in Figure 3, there is a lack of detailed explanation regarding what the input 'x' and output 'y' represent, along with the corresponding spatial dimensions, which is confusing. For example, does 'x' refer to the feature map of a single layer or a specific convolutional parameter? Does 'y' refer to the corresponding logit or something else? However, this lack of clarity detracts from the overall understanding of the results.
4.	Table 4 presents informative structure identification (ISI) with different structure Fisher scores. It would be beneficial to supplement this with the selection of different informative structures using (TOP-K) or the Fisher scores that are discarded, to more intuitively measure the differences and effectiveness of this method.
5.	The authors used support images as pseudo-query images to form support-query (pseudo-support) image pairs, and then they progressively increased the number of support samples to construct training pairs. However, the authors lack an explanation of why support images are used as pseudo-query images instead of directly using query images. Additionally, are the support and query (pseudo-support) images the same during training?
[1] IFANet: Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining

Review Point: 1. Feature selection techniques are very common in machine learning and also represent a standard paradigm. While using the Fisher score to measure the importance of neural network parameters is beneficial, it remains a straightforward application and lacks innovation.
Review Point: 2. Although the authors' model achieves excellent results on multiple datasets, it is not state-of-the-art, and there is a lack of corresponding model comparisons in Table 1, such as IFANet (CVPR 2024).
Review Point: 3. Although the authors provide a comparison of the trainable layer information structure distribution in Figure 3, there is a lack of detailed explanation regarding what the input 'x' and output 'y' represent, along with the corresponding spatial dimensions, which is confusing. For example, does 'x' refer to the feature map of a single layer or a specific convolutional parameter? Does 'y' refer to the corresponding logit or something else? However, this lack of clarity detracts from the overall understanding of the results.
Review Point: 4. Table 4 presents informative structure identification (ISI) with different structure Fisher scores. It would be beneficial to supplement this with the selection of different informative structures using (TOP-K) or the Fisher scores that are discarded, to more intuitively measure the differences and effectiveness of this method.
Review Point: 5. The authors used support images as pseudo-query images to form support-query (pseudo-support) image pairs, and then they progressively increased the number of support samples to construct training pairs. However, the authors lack an explanation of why support images are used as pseudo-query images instead of directly using query images. Additionally, are the support and query (pseudo-support) images the same during training? [1] IFANet: Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining
==================================================

Focused review:

1. The model’s effectiveness relies heavily on the internal property predictor, which may be less reliable for out-of-distribution samples. This dependence could reduce fidelity in less representative scenarios.
2. Although the model improves conditioning performance, it’s unclear how it balances molecule diversity and property, diversity is also a crucial metric in molecular generation.

Review Point: 1. The model’s effectiveness relies heavily on the internal property predictor, which may be less reliable for out-of-distribution samples. This dependence could reduce fidelity in less representative scenarios.
Review Point: 2. Although the model improves conditioning performance, it’s unclear how it balances molecule diversity and property, diversity is also a crucial metric in molecular generation.
==================================================

Focused review:

1. There are also fine-grained human action datasets, such as fine-gym.
2. In this dataset, action categories are highly related with objects. Then object bias may appear, which means object will help the model understand actions. I wonder about the task difficulty. Furthermore, it may cause researcher focus on objects rather than actions.
3. Following the point 2, can you just classify the 'verb' category?
4. No correponding baseline model is proposed.

Review Point: 1. There are also fine-grained human action datasets, such as fine-gym.
Review Point: 2. In this dataset, action categories are highly related with objects. Then object bias may appear, which means object will help the model understand actions. I wonder about the task difficulty. Furthermore, it may cause researcher focus on objects rather than actions.
Review Point: 3. Following the point 2, can you just classify the 'verb' category?
==================================================

Focused review:

- The link between VOC and the latter presented approach of modified expert iteration is unclear. Sections 2 and 3 are not well connected together.
- Incremental contribution. The method is an extension of existing work (STaR). More justification or analysis of this approach's novelty would strengthen the paper (see question 2).
- There a few places where writing could be improved (see the writing suggestions below).
- There are a few inconsistencies or unclear statements (see the clarity comments below).
- A potential limitation, not mentioned in the paper, is that while shorter responses may be preferred from the computational costs point of view, they may not necessarily be more human friendly. The paper would benefit from a small human study assessing the qualitative aspects of the generated reasoning chains with Rational Metareasoning.
- Experiments could include additional baselines and ablation studies (see below for details).
**Comments on clarity**
- Line 159: the statement: *“Initially, in the exploration phase, we approximate the optimal policy by using rejection sampling on our student policy $\pi_\theta$”* is unclear. First, the notion of an optimal policy $\hat{\pi}^*$ hasn’t been defined. Second, in rejection sampling, all reasoning chains with the reward above a certain threshold should be retained. Yet, the proposed algorithm only selects a single reasoning chain that maximises the reward. The motivation for this choice is unclear (see question 2).
- It seems that there are a few inconsistencies in the presentation of Algorithm 1. Assignment of $\pi_0$ is missing. In line 2, the entire dataset $\mathcal{D}$ is assigned to $\mathcal{D}\_n$, but from the comment and the main text it follows that $\mathcal{D}\_n$ is subsampled from $\mathcal{D}$. The reward function in the algorithm should should be made dependent on $\pi_{n-1}$, i.e. $\mathcal{R}\_{\pi_{n-1}}$. Storing the rewards in $r_{i,k}$ (line 7 of Algorithm 1) seems redundant given the later $\arg\max$ again uses $\mathcal{R}$. The input to this argmax should be $\mathcal{R}\_{\pi_{n-1}}(x_i, z_{i,k}, y_i)$ rather than $\mathcal{R}(x, z_{i,k}, y)$, I believe. The quantifier $\forall i$ is also missing in this step. **Experiments**
- It would be interesting to compare Metareasoning with Direct Few-Shot prompting, where the LLM is explicitly instructed to provide concise responses (e.g. “Keep your answer concise”.). It has been previously demonstrated that such a statement shortens the expected answer length, often without sacrificing the performance [1, 2].
- The batching technique with increasing $T$ is a design choice which should be tested in an additional ablation study.
- The proposed expert-iteration algorithm could be additionally compared to other fine-tunning algorithms, like PPO, to better motivate the particular choice of the training method.
**Minor writing suggestions**
- Line 032: “… many of these methods reduce costs at the expense of performance”—this statement would benefit by adding an example with an appropriate reference similar to the juxtaposed approach of chain-of-thought.
- Lines 071-075: the word “while” is repeated 4 times in 3 consecutive sentences, consider rephrasing.
- Line 097: There is no equation 3.1 in the paper **References**
[1] https://arxiv.org/pdf/2401.05618
[2] https://arxiv.org/pdf/2407.19825

Review Point: - The link between VOC and the latter presented approach of modified expert iteration is unclear. Sections 2 and 3 are not well connected together.
Review Point: - Incremental contribution. The method is an extension of existing work (STaR). More justification or analysis of this approach's novelty would strengthen the paper (see question 2).
Review Point: - There a few places where writing could be improved (see the writing suggestions below).
Review Point: - There are a few inconsistencies or unclear statements (see the clarity comments below).
Review Point: - A potential limitation, not mentioned in the paper, is that while shorter responses may be preferred from the computational costs point of view, they may not necessarily be more human friendly. The paper would benefit from a small human study assessing the qualitative aspects of the generated reasoning chains with Rational Metareasoning.
Review Point: - Experiments could include additional baselines and ablation studies (see below for details). **Comments on clarity** - Line 159: the statement: *“Initially, in the exploration phase, we approximate the optimal policy by using rejection sampling on our student policy $\pi_\theta$”* is unclear. First, the notion of an optimal policy $\hat{\pi}^*$ hasn’t been defined. Second, in rejection sampling, all reasoning chains with the reward above a certain threshold should be retained. Yet, the proposed algorithm only selects a single reasoning chain that maximises the reward. The motivation for this choice is unclear (see question 2).
Review Point: - The batching technique with increasing $T$ is a design choice which should be tested in an additional ablation study.
Review Point: - The proposed expert-iteration algorithm could be additionally compared to other fine-tunning algorithms, like PPO, to better motivate the particular choice of the training method. **Minor writing suggestions** - Line 032: “… many of these methods reduce costs at the expense of performance”—this statement would benefit by adding an example with an appropriate reference similar to the juxtaposed approach of chain-of-thought.
Review Point: - Lines 071-075: the word “while” is repeated 4 times in 3 consecutive sentences, consider rephrasing.
Review Point: - Line 097: There is no equation 3.1 in the paper **References** [1] https://arxiv.org/pdf/2401.05618 [2] https://arxiv.org/pdf/2407.19825
==================================================

Focused review:

1. Obviously, as with the original token merging and its subsequent forms, this method inherits the one same major downside: that of somewhat degraded performance. The more tokens that are merged, the faster the model performs, but the quality of the also tends to decline, and this seems to be a tradeoff (Tab. 2)
2. Another weakness is induced by the nature of the problem: time-series. In order to ensure that tokens are merged while causal relationship between observations is maintaining, setting $k = 1$ in the decoder restricts it from merging a wider set of tokens.
3. Converse to Strength 1, while the relevance of this work is broad, little analysis is done with the representations learnt: namely, the possible use of other measures of similarity measures for merging and unmerging, and the resulting impact on downstream tasks.
* For instance, if my understanding is correct (see Question 5), it would make more sense to use a quantitatively informed measure to choose tokens to further merge/retain as is, such as measuring the informativeness of tokens or using better heuristics for merging [4]
4. How would this technique translate to video such as human gait analysis or datasets like EPIC Kitchens [3], for example, all time-series? Is there a reason for not experimenting on videos which still offer the causal, time-series essence? Because, such datasets are sufficiently long temporally, offering to provide more insights into Fig 4. (A recent, relevant work [5] might be of interest).
5. Another serious weakness I find is lack of comparison to other token merging algorithms. Have the authors applied current merging algorithms ([1, 2]) to time series just for comparison?
### References
1. Xie, Enze, et al. "SegFormer: Simple and efficient design for semantic segmentation with transformers." Advances in neural information processing systems 34 (2021): 12077-12090.
2. Tran, Hoai-Chau, et al. "Accelerating Transformers with Spectrum-Preserving Token Merging." arXiv preprint arXiv:2405.16148 (2024).
3. Damen, Dima, et al. "Scaling egocentric vision: The epic-kitchens dataset." Proceedings of the European conference on computer vision (ECCV). 2018.
4. Choi, Joonmyung, et al. "vid-TLDR: Training Free Token merging for Light-weight Video Transformer." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
5. Lee, Seon Ho, et al. "Video token merging for long-form video understanding." (2024).

Review Point: 1. Obviously, as with the original token merging and its subsequent forms, this method inherits the one same major downside: that of somewhat degraded performance. The more tokens that are merged, the faster the model performs, but the quality of the also tends to decline, and this seems to be a tradeoff (Tab. 2) 2. Another weakness is induced by the nature of the problem: time-series. In order to ensure that tokens are merged while causal relationship between observations is maintaining, setting $k = 1$ in the decoder restricts it from merging a wider set of tokens.
Review Point: 3. Converse to Strength 1, while the relevance of this work is broad, little analysis is done with the representations learnt: namely, the possible use of other measures of similarity measures for merging and unmerging, and the resulting impact on downstream tasks.
Review Point: 5. Another serious weakness I find is lack of comparison to other token merging algorithms. Have the authors applied current merging algorithms ([1, 2]) to time series just for comparison? ### References 1. Xie, Enze, et al. "SegFormer: Simple and efficient design for semantic segmentation with transformers." Advances in neural information processing systems 34 (2021): 12077-12090.
Review Point: 2. Tran, Hoai-Chau, et al. "Accelerating Transformers with Spectrum-Preserving Token Merging." arXiv preprint arXiv:2405.16148 (2024).
Review Point: 3. Damen, Dima, et al. "Scaling egocentric vision: The epic-kitchens dataset." Proceedings of the European conference on computer vision (ECCV). 2018.
Review Point: 4. Choi, Joonmyung, et al. "vid-TLDR: Training Free Token merging for Light-weight Video Transformer." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
Review Point: 5. Lee, Seon Ho, et al. "Video token merging for long-form video understanding." (2024).
==================================================

Focused review:

The main weaknesses/limitations of this paper can be summarized as follows:
1. The theoretical contribution of this paper appears to be limited. In particular,
a) The main theoretical contribution is the introduction of problem (4) which is a standard log likelihood maximization. Furthermore, it is not well justified why the selection of this optimization is proper for the control of complex systems. The authors should better justify how it relates to the original problem (1) and whether the conditioning in Eq. (4) occurs based on an underlying derivation or if it is ad-hoc approach.
b) The classifier-free guidance free idea has also been presented in a related approach in [R1]. To the reviewer's best understanding, the difference between the current paper's approach and [R1] is only on how labeling works.
2. The related work section is short and only emphasizing in few works, rather than providing a general overview of the areas. For example, in Section 2.1, a large body of literature on deep learning based control is omitted. In addition, only two references are provided for finite-time control methods. The authors are encouraged to provide a more complete overview of the related literature, as this is of great importance for the reader to understand the motivation and importance of a proposed method.
3. It is unclear whether a running state cost or constraints can be incorporated through the proposed formulation, although such specifications are often crucial to be met in complex physical systems. The problem formulation in Eq. (1) only includes a terminal state cost, and similarly, in Eq. (4) the conditioning is only on the initial and terminal states.
4. It seems that in Eq. (4) there is also a conditioning on the "optimization goal" J which is the desired cost. Nevertheless, such an approach might encounter the following limitations: i) It is often very hard to "predict" what a good cost is - especially in complex physical systems. ii) If the cost J used for conditioning is worse (higher) than the optimal cost of Eq. (1), then the proposed approach might "force" the resulting policy to be worse than it should. On the other hand, if the guess for the optimization goal is too good to be feasible (too low), then no trajectories will satisfy this conditioning. The authors are encouraged to comment on this issue.
5. The actual implementation of the proposed methodology is not clearly explained in the paper. An algorithm figure is missing showing the steps and how the described components are integrated in practice (e.g., inpainting, inverse dynamics).
6. The advantages of the presented method are only shown in two systems. The authors are encouraged to explore more complex physical systems with performance specifications encoded throughout the tasks, constraints, etc.
[R1] Li, A., Ding, Z., Dieng, A. B., & Beeson, R. (2024). Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model. arXiv preprint arXiv:2403.05571.

Review Point: 2. The related work section is short and only emphasizing in few works, rather than providing a general overview of the areas. For example, in Section 2.1, a large body of literature on deep learning based control is omitted. In addition, only two references are provided for finite-time control methods. The authors are encouraged to provide a more complete overview of the related literature, as this is of great importance for the reader to understand the motivation and importance of a proposed method.
Review Point: 3. It is unclear whether a running state cost or constraints can be incorporated through the proposed formulation, although such specifications are often crucial to be met in complex physical systems. The problem formulation in Eq. (1) only includes a terminal state cost, and similarly, in Eq. (4) the conditioning is only on the initial and terminal states.
Review Point: 5. The actual implementation of the proposed methodology is not clearly explained in the paper. An algorithm figure is missing showing the steps and how the described components are integrated in practice (e.g., inpainting, inverse dynamics).
Review Point: 6. The advantages of the presented method are only shown in two systems. The authors are encouraged to explore more complex physical systems with performance specifications encoded throughout the tasks, constraints, etc. [R1] Li, A., Ding, Z., Dieng, A. B., & Beeson, R. (2024). Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model. arXiv preprint arXiv:2403.05571.
==================================================

Focused review:

At a high level, it's unclear to me what this paper accomplishes. It does not offer new insights about pruning, nor does it permit more efficient model inference or training in practice. Accuracy gains over other pruning methods are claimed, but they are fairly marginal, leaving me unconvinced that this is even useful as a proof of concept, in the way that e.g. the original lottery ticket hypothesis paper was.
More specifically:
> However, these models also bring important challenges related to computational needs, storage cost, and training efficiency. The resource-intensive nature of these large networks have spurred a growing interest in techniques that can reduce the size and computational complexity associated with training and deploying these models.
- This sentence from the introduction seems to promise a method that improves the efficiency of practical large models. Like other pruning papers, this one does not provide that. As the authors note themselves, translating unstructured sparsity into efficiency or reduced complexity in practice is very difficult, if not impossible, with current tools. Also, this paper does not evaluate SSA on models at a scale that would benefit from shrinking in practice.
> The probabilistic inclusion of extra parameters early in the tuning process allows for gradient information to bleed through into the target subnetwork, encouraging robust adaptation and avoiding the drastic performance collapse observed with one-shot pruning methods.
> Allowing other parameters to become active allows for gradient information to contribute to optimization of the target subnetwork which can help to encourage avoidance of local minima
- Several claims (^) of this nature are made, but as far as I can tell there is little evidence in the paper to support them. There are no robustness evaluations, as far as I can tell, and avoiding the drastic performance collapse of one-shot pruning is not unique to SSA.
- Table 1 should have full-network accuracy on it. It should also ideally have error bars, since the margins are so slim. Would also be nice to see CIFAR-10 results here, to allow for easier comparison to canonical papers in the field, like the lottery ticket hypothesis paper.
- Table 2 claims that SSA requires fewer FLOPs than some competing methods, but this is not particularly meaningful, in that the method does not actually translate into improved performance on existing ML hardware (right?). Alternatively, one could argue that the burden of having to keep around float matrices of probabilities the same size as the network also outweighs theoretical improvements to FLOP counts, since practical models most in need of being pruned are now tens or even hundreds of gigabytes.
- The differences between SSA and regular Prune and Tune in Table 2 seem far too close to call for me. I'd need to see error bars here as well.
Bits and bobs (no bearing on decision):
- There's a missing citation (marked by a ?) on page 8.

Review Point: - Table 1 should have full-network accuracy on it. It should also ideally have error bars, since the margins are so slim. Would also be nice to see CIFAR-10 results here, to allow for easier comparison to canonical papers in the field, like the lottery ticket hypothesis paper.
Review Point: - Table 2 claims that SSA requires fewer FLOPs than some competing methods, but this is not particularly meaningful, in that the method does not actually translate into improved performance on existing ML hardware (right?). Alternatively, one could argue that the burden of having to keep around float matrices of probabilities the same size as the network also outweighs theoretical improvements to FLOP counts, since practical models most in need of being pruned are now tens or even hundreds of gigabytes.
Review Point: - The differences between SSA and regular Prune and Tune in Table 2 seem far too close to call for me. I'd need to see error bars here as well. Bits and bobs (no bearing on decision):
Review Point: - There's a missing citation (marked by a ?) on page 8.
==================================================

Focused review:

1. The presentation could be improved as some figures, such as Figures 2, 3, and 5, are overly large and impact readability.
2. Despite the inclusion of many metrics, several tables exhibit issues:
* In Table 2, under the section "Generative model comparison," the comparison between Scale-wise Autoregressive models (M-VAR and VAR) seems unfair. For example, the last two rows show that M-VAR (depth 32) with 3B parameters outperforms VAR (depth 30) with 2B parameters, but the parameter count for M-VAR is 50% higher.
* Additionally, inference time increases from 0.7s to 1s (a 43% increase) despite only slightly better FID and IS scores.
3. Table 6 appears to lack significant information and could be made more concise for clarity.
4. It is suggested that the data in Table 1 be illustrated as a figure to better highlight this critical motivation behind the work.

Review Point: 1. The presentation could be improved as some figures, such as Figures 2, 3, and 5, are overly large and impact readability.
Review Point: 2. Despite the inclusion of many metrics, several tables exhibit issues:
Review Point: * In Table 2, under the section "Generative model comparison," the comparison between Scale-wise Autoregressive models (M-VAR and VAR) seems unfair. For example, the last two rows show that M-VAR (depth 32) with 3B parameters outperforms VAR (depth 30) with 2B parameters, but the parameter count for M-VAR is 50% higher.
Review Point: * Additionally, inference time increases from 0.7s to 1s (a 43% increase) despite only slightly better FID and IS scores.
Review Point: 3. Table 6 appears to lack significant information and could be made more concise for clarity.
Review Point: 4. It is suggested that the data in Table 1 be illustrated as a figure to better highlight this critical motivation behind the work.
==================================================

Focused review:

Despite the impressive points that the authors intended to make, some facts might undermine the validity of the claims.
1. The first part of the claims are made by direct prompt ChatGPT/GPT4. However, some gaps between the performances are not significant.
2. Some claims are too general to be valid, please check the question part.

Review Point: 1. The first part of the claims are made by direct prompt ChatGPT/GPT4. However, some gaps between the performances are not significant.
Review Point: 2. Some claims are too general to be valid, please check the question part.
==================================================

Focused review:

1. The proposed improvements in the paper are rather straightforward. Learned Gene Normalization refers to the layer normalization, Positional Embeddings refer to standard sinusoidal position embeddings. The authors stack 50 cells into a single sequence, which from my perspective, creates sudo bulks and lowers the granularity instead of modeling intercellular dependencies. It would be great if the authors could elaborate more on how the proposed strategy can help with modeling intercellular dependencies.
2. Some statements in the manuscript are not put correctly. For example, in line 231, the authors state that CellPLM and scGPT are built on MLM objective and predict raw gene expression values. In fact, CellPLM conducts library size normalization and log1p on the counts, and scGPT utilizes next token prediction with data binning. Note that the binning strategy has already been employed in scGPT. The authors should explicitly acknowledge where their approach uses similar techniques (like binning) to prior work.
3. The authors mention that *downstream evaluation pipelines that do not reflect the biological challenges in the field* in the abstract. However, in the experiments, the authors only include gene imputation and cell clustering tasks, which is quite limited. Please consider tasks like gene perturbation prediction, gene regulatory network inference, cell-cell communication, etc.

Review Point: 1. The proposed improvements in the paper are rather straightforward. Learned Gene Normalization refers to the layer normalization, Positional Embeddings refer to standard sinusoidal position embeddings. The authors stack 50 cells into a single sequence, which from my perspective, creates sudo bulks and lowers the granularity instead of modeling intercellular dependencies. It would be great if the authors could elaborate more on how the proposed strategy can help with modeling intercellular dependencies.
Review Point: 2. Some statements in the manuscript are not put correctly. For example, in line 231, the authors state that CellPLM and scGPT are built on MLM objective and predict raw gene expression values. In fact, CellPLM conducts library size normalization and log1p on the counts, and scGPT utilizes next token prediction with data binning. Note that the binning strategy has already been employed in scGPT. The authors should explicitly acknowledge where their approach uses similar techniques (like binning) to prior work.
Review Point: 3. The authors mention that *downstream evaluation pipelines that do not reflect the biological challenges in the field* in the abstract. However, in the experiments, the authors only include gene imputation and cell clustering tasks, which is quite limited. Please consider tasks like gene perturbation prediction, gene regulatory network inference, cell-cell communication, etc.
==================================================

Focused review:

1. The novelty of this paper is incremental. The paper merely combines the post-hoc explainers with fidelity metric and stability metric. 2. The experimental part is somewhat not convincing. * The experiments use the fidelity and stability metric to measure the proposed approach, which is optimized for those two metrics. Therefore, the metrics might not be sufficient to measure the proposed approach. Other measurements for model interpretability should be included. * The experimental results show that the proposed approach with regularizers outperforms the normally trained model without regularizers. This result is straightforward. Also, it is not surprising to see the results in user study: the regularized model achieves better interpretability than the normal model. The proposed approach should be compared with other interpreting methods in user study. 3. Minor comment: e is not defined in Equation 2.

Review Point: 1. The novelty of this paper is incremental. The paper merely combines the post-hoc explainers with fidelity metric and stability metric.
Review Point: * The experiments use the fidelity and stability metric to measure the proposed approach, which is optimized for those two metrics. Therefore, the metrics might not be sufficient to measure the proposed approach. Other measurements for model interpretability should be included.
Review Point: * The experimental results show that the proposed approach with regularizers outperforms the normally trained model without regularizers. This result is straightforward. Also, it is not surprising to see the results in user study: the regularized model achieves better interpretability than the normal model. The proposed approach should be compared with other interpreting methods in user study.
Review Point: 3. Minor comment: e is not defined in Equation 2.
==================================================

Focused review:

1. The paper primarily focuses on citation networks, which are inherently homophilic. The applicability of the proposed KNN-based graph generation method to other types of textual networks, such as academic or social networks, is not explored.
2. The core contribution of the paper lies in using KNN to approximate graphs, but this approach seems to be more suited for homophilic networks. In more diverse scenarios, such as heterogeneous academic networks with different types of nodes, the use of a unified nearest-neighbor metric may introduce significant bias. This limits the method’s applicability to broader graph structures. It is recommended to try a broader extension of the for KNN design.
3. While the paper provides strong theoretical and experimental support, the experiments on the validation of KNN could be more comprehensive. (See Questions)

Review Point: 1. The paper primarily focuses on citation networks, which are inherently homophilic. The applicability of the proposed KNN-based graph generation method to other types of textual networks, such as academic or social networks, is not explored.
Review Point: 2. The core contribution of the paper lies in using KNN to approximate graphs, but this approach seems to be more suited for homophilic networks. In more diverse scenarios, such as heterogeneous academic networks with different types of nodes, the use of a unified nearest-neighbor metric may introduce significant bias. This limits the method’s applicability to broader graph structures. It is recommended to try a broader extension of the for KNN design.
Review Point: 3. While the paper provides strong theoretical and experimental support, the experiments on the validation of KNN could be more comprehensive. (See Questions)
==================================================

Focused review:

1) Inconsistent notation
: In the explanation of AHR (184) you are referring are T_l but in the algorithm, it is T_i which is the same for P
: what does the * refer to in algorithm 1?
: What is J_l?
: ^ in explanation and ' is used in Figure 1 are interchangeably used for generated output
: what is \mathcal{T} in Figure 1?
2) Is there any explanation for how the memory is reduced to 0.1t
3) It is claimed that the complexity reduces to 10%, but no empirical evidence is provided to validate that hypothesis
4) Evaluation metric: It is unclear to readers, (line 373) does the accuracy represents the accuracy on the only last task after training on all tasks or is average on all previous tasks.
5) How does the number of exemplars decrease over time? You are representing the exemplars in latent space? Does it mean reducing the number of classes?
(Table 3) Why the different numbers of epochs for AHR and others? Please be clear on the size of latent and raw ? is 150 (latent) better than 20 (raw)
6) It would be clearer to the readers if there was some explanation of how CPSEM and RFA create incremental embeddings
7) The main objective of this paper is to reduce the size of exemplars in memory. In the related work section, the authors focus on mainly describing the current replay mechanism without mentioning how the current strategies fall short in reducing the size and its relation to AHR
8) There is no comparative analysis of the work with state-of-the-art replay methods such as:
I) Rolnick, David, et al. "Experience replay for continual learning." Advances in neural information processing systems 32 (2019).
II) Buzzega, Pietro, et al. "Dark experience for general continual learning: a strong, simple baseline." Advances in neural information processing systems 33 (2020): 15920-15930.
which makes it challenging to assess the significance of the work in the literature.
### Comments on evaluation:
I am mainly concerned about the accuracies for CIFAR100 and mimiImageNet. There are various works [FeTrIL [1] by Petit, Grégoire, et al., FeCAM [2] by Goswami et al] utilizing ResNet-18/32 achieving higher accuracy of more than 65% even in exemplar-free settings. I wonder how with an exemplar, the model is not able to maintain that accuracy.
[1] Petit, Grégoire, et al. "Fetril: Feature translation for exemplar-free class-incremental learning." Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2023.
[2] Goswami, Dipam, et al. "Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning." Advances in Neural Information Processing Systems 36 (2024).

Review Point: 1) Inconsistent notation : In the explanation of AHR (184) you are referring are T_l but in the algorithm, it is T_i which is the same for P : what does the * refer to in algorithm 1? : What is J_l? : ^ in explanation and ' is used in Figure 1 are interchangeably used for generated output : what is \mathcal{T} in Figure 1?
Review Point: 2) Is there any explanation for how the memory is reduced to 0.1t
Review Point: 3) It is claimed that the complexity reduces to 10%, but no empirical evidence is provided to validate that hypothesis
Review Point: 4) Evaluation metric: It is unclear to readers, (line 373) does the accuracy represents the accuracy on the only last task after training on all tasks or is average on all previous tasks.
Review Point: 5) How does the number of exemplars decrease over time? You are representing the exemplars in latent space? Does it mean reducing the number of classes? (Table 3) Why the different numbers of epochs for AHR and others? Please be clear on the size of latent and raw ? is 150 (latent) better than 20 (raw)
Review Point: 6) It would be clearer to the readers if there was some explanation of how CPSEM and RFA create incremental embeddings
Review Point: 7) The main objective of this paper is to reduce the size of exemplars in memory. In the related work section, the authors focus on mainly describing the current replay mechanism without mentioning how the current strategies fall short in reducing the size and its relation to AHR
==================================================

Focused review:

1. In definition 2, the augmented sequence is with any permutation $S$. Is this permutation random or fixed during training? If it is random, it seems impossible to achieve perfect accuracy for next-token prediction in Theorem 3, and the monotone pattern in Figure 3(b) might also be questionable. If it is fixed, some clarification is helpful.
2. It would be better to include discussion regarding a recent related work [1].
[1] How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad. Abbe et al. June 2024.

Review Point: 1. In definition 2, the augmented sequence is with any permutation $S$. Is this permutation random or fixed during training? If it is random, it seems impossible to achieve perfect accuracy for next-token prediction in Theorem 3, and the monotone pattern in Figure 3(b) might also be questionable. If it is fixed, some clarification is helpful.
Review Point: 2. It would be better to include discussion regarding a recent related work [1]. [1] How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad. Abbe et al. June 2024.
==================================================

Focused review:

1.I believe the evaluation of the instructionally tuned WILDLLAMA is too limited to demonstrate the effectiveness. In addition to MT-Bench, I strongly suggest that you can follow InstructEval (Chia et al., 2023) to evaluate WILDLLAMA on MMLU, DROP, Human-eval, and BBH. The performance superiority is not the thing to worry. Such benchmark results can help you better assess the coverage and diversity of the released WILDCHAT dataset.
2. I suggest that the authors should find a taxonomy for analyzing the task coverage, i.e. Flan, of the proposed WILDCHAT. The diverse coverage on different tasks might be more significant than the quantity. The t-SNE visualization on the diversity is not that intuitive as task coverage.

Review Point: 1.I believe the evaluation of the instructionally tuned WILDLLAMA is too limited to demonstrate the effectiveness. In addition to MT-Bench, I strongly suggest that you can follow InstructEval (Chia et al., 2023) to evaluate WILDLLAMA on MMLU, DROP, Human-eval, and BBH. The performance superiority is not the thing to worry. Such benchmark results can help you better assess the coverage and diversity of the released WILDCHAT dataset.
Review Point: 2. I suggest that the authors should find a taxonomy for analyzing the task coverage, i.e. Flan, of the proposed WILDCHAT. The diverse coverage on different tasks might be more significant than the quantity. The t-SNE visualization on the diversity is not that intuitive as task coverage.
==================================================

Focused review:

- The paper focuses on small language models, mostly GPT2, and for some experiments, llama2-7B-Chat. I am unsure how the results scale for even larger and deeper models and whether they could be applied to current LLM architectures (e.g., llama-3).
- The Authors include an invalid link within the reproducibility statement for code and experiments.
- As an informed outsider, the paper seems to be very technical, and I struggle to see the main insides you draw from the theory and empirical findings. Maybe a short elaboration on how and why these suffixes work could clarify that.

Review Point: - The paper focuses on small language models, mostly GPT2, and for some experiments, llama2-7B-Chat. I am unsure how the results scale for even larger and deeper models and whether they could be applied to current LLM architectures (e.g., llama-3).
Review Point: - The Authors include an invalid link within the reproducibility statement for code and experiments.
Review Point: - As an informed outsider, the paper seems to be very technical, and I struggle to see the main insides you draw from the theory and empirical findings. Maybe a short elaboration on how and why these suffixes work could clarify that.
==================================================

Focused review:

Some concerns: 1. One big concern is, the proposed method is combined with existing trigger-set-based IP protection methods to support the black-box verification, however, if the existing trigger-set-based method is strong enough to verify the suspect model do we need an extra step (i.e., the proposed method) to confirm? Or is this two-step verification necessary? I think the authors should provide some discussions or data to claim it. 2. The training process of the passport-free branch and the passport-aware branch is in an alternative way. What's the training cost? Why the authors use this training strategy, rather than train them simultaneously? More details are appreciated. 3. The details of the trigger-set-based method are missing. What existing method did the author use? How many special sets of data are used to identify a suspect model? Will this impact the DNN performance? Could the authors provide some detailed evaluation results of this part?

Review Point: 2. The training process of the passport-free branch and the passport-aware branch is in an alternative way. What's the training cost? Why the authors use this training strategy, rather than train them simultaneously? More details are appreciated.
Review Point: 3. The details of the trigger-set-based method are missing. What existing method did the author use? How many special sets of data are used to identify a suspect model? Will this impact the DNN performance? Could the authors provide some detailed evaluation results of this part?
==================================================

Focused review:

1. The current evaluation is not with dynamic and realistic environments and a more extensive evaluation in these settings would be beneficial. Some possible trials on auto driving or robotic manipulation is welcome.
2. Though this paper is mainly on LLM, some quick comparison with the traditional/classic planning methods is also welcome, at least could show some potential improvement room.
3. Limited analysis of model internals, further analysis on the performance gap or other factors will help the authors understand.

Review Point: 1. The current evaluation is not with dynamic and realistic environments and a more extensive evaluation in these settings would be beneficial. Some possible trials on auto driving or robotic manipulation is welcome.
Review Point: 2. Though this paper is mainly on LLM, some quick comparison with the traditional/classic planning methods is also welcome, at least could show some potential improvement room.
Review Point: 3. Limited analysis of model internals, further analysis on the performance gap or other factors will help the authors understand.
==================================================

Focused review:

* First of all, the overall structure and writing of this paper necessitate meticulous reorganization and refinement. Some paragraph is confusing and hard to follow due to poor organization. Especially in Section 4 and Section 5, the important conclusion in these paragraphs need to be highlighted and summarized. In Section 5, the transition from Monte Carlo (MC) pretraining to emphasizing both actor and critic regularization is perplexing, especially since these regularization are not introduced in the methods section. And the title "Application to Adroit Environments" is incongruous as the methodology differs from the prior parts.
* The two parts of pretraining and regularization that the authors want to underscore appear to be incremental additions rather than naturally integrated components. This disjointed presentation detracts from the coherence of the paper and needs to be addressed.
* Regarding the methodology, the paper's primary emphasis appears to be on the use of Monte Carlo (MC) estimates as pretraining targets. However, the results in Appendix A.3 indicate that the efficiency and performance during pretraining stem from the Behavioral Cloning (BC) loss rather than MC. MC only contributes to stability during subsequent fine-tuning. Furthermore, in Section 5, the authors assert that pretraining is less critical than both regularization techniques. Consequently, I am unconvinced about the significance of this work.
* The experiments are also limited in variety of dataset types and domains. For instance, BC pretraining may depend on data quality; therefore, additional dataset types such as "medium-replay" and "random" datasets are necessary to substantiate the importance of this work. Moreover, further ablation studies are required to validate the paper's claims. Current results in Figures 3, 5, and 6, which only base on a single dataset, are unconvincing. Additionally, the inclusion of more domains, such as AntMaze, would be beneficial.

Review Point: * First of all, the overall structure and writing of this paper necessitate meticulous reorganization and refinement. Some paragraph is confusing and hard to follow due to poor organization. Especially in Section 4 and Section 5, the important conclusion in these paragraphs need to be highlighted and summarized. In Section 5, the transition from Monte Carlo (MC) pretraining to emphasizing both actor and critic regularization is perplexing, especially since these regularization are not introduced in the methods section. And the title "Application to Adroit Environments" is incongruous as the methodology differs from the prior parts.
Review Point: * The two parts of pretraining and regularization that the authors want to underscore appear to be incremental additions rather than naturally integrated components. This disjointed presentation detracts from the coherence of the paper and needs to be addressed.
Review Point: * Regarding the methodology, the paper's primary emphasis appears to be on the use of Monte Carlo (MC) estimates as pretraining targets. However, the results in Appendix A.3 indicate that the efficiency and performance during pretraining stem from the Behavioral Cloning (BC) loss rather than MC. MC only contributes to stability during subsequent fine-tuning. Furthermore, in Section 5, the authors assert that pretraining is less critical than both regularization techniques. Consequently, I am unconvinced about the significance of this work.
Review Point: * The experiments are also limited in variety of dataset types and domains. For instance, BC pretraining may depend on data quality; therefore, additional dataset types such as "medium-replay" and "random" datasets are necessary to substantiate the importance of this work. Moreover, further ablation studies are required to validate the paper's claims. Current results in Figures 3, 5, and 6, which only base on a single dataset, are unconvincing. Additionally, the inclusion of more domains, such as AntMaze, would be beneficial.
==================================================

Focused review:

Certain aspects of the paper, which are emphasized as being novel, are not in fact new to this work. Given the large number of papers about watermarking, it is extremely reasonable that the authors missed these, but it nonetheless undermines the claims of novelty:
- I'm aware of at least one other work that also recognized and addressed the dilemma between unforgeability and robustness: "Pseudorandom error-correcting codes," Section 2.6.
- In the same paper, they also consider adversaries that make random deletions. The paper "Edit Distance Robust Watermarks for Language Models" also considers adversaries that make a broader class of edits.
Neither of the above papers contain experiments, so this paper could have some claim to novelty in this regard. However, editing adversaries are widely considered in basically every paper that does watermarking robustness experiments --- for instance, "Provable Robust Watermarking for AI-Generated Text" even has a robustness experiment that they call the "Editing attack."
In any case, the experiments leave much to be desired:
- They do not compare the quality-robustness trade-off to existing schemes at all.
- They only use perplexity to measure quality, which is known to prefer repetitive text and therefore may be favorable to watermarks. There is work on more useful quality evaluations for watermarks, such as the paper "MARKMYWORDS: Analyzing and Evaluating Language Model Watermarks." They do not theoretically analyze their quality, either, and I suspect that they would only achieve the degenerate 1-distortion freeness under their definition if they did. (This is because their definition of distortion-freeness allows the distinguisher to make many queries to the model before making their decision, as in the definitions of Christ et al. and Fairoze et al.)
- There do not appear to be any experiments demonstrating the actual detectability of their watermark. And their calculations suggest using 1000s of tokens, which is extremely inefficient.
Finally, a minor comment: I found the discussion around public detectability misleading. It is not the case that schemes aside from Fairoze et al. are inherently "secret-key," as one can always simply publish the key and achieve a fully functional scheme. The difference is in the *unforgeability* despite the fact that the detection key is made public.

Review Point: - I'm aware of at least one other work that also recognized and addressed the dilemma between unforgeability and robustness: "Pseudorandom error-correcting codes," Section 2.6.
Review Point: - In the same paper, they also consider adversaries that make random deletions. The paper "Edit Distance Robust Watermarks for Language Models" also considers adversaries that make a broader class of edits. Neither of the above papers contain experiments, so this paper could have some claim to novelty in this regard. However, editing adversaries are widely considered in basically every paper that does watermarking robustness experiments --- for instance, "Provable Robust Watermarking for AI-Generated Text" even has a robustness experiment that they call the "Editing attack." In any case, the experiments leave much to be desired:
Review Point: - They do not compare the quality-robustness trade-off to existing schemes at all.
==================================================

Focused review:

1. In Table 2, the authors compared the distillation performance of their proposed method with SRe2L in Tiny ImageNet, ImageNet-1K, and ImageNet-21K datasets. However, I am curious about the performance when randomly sampling an equal number of images from the source dataset, which was not explicitly shown in the table (e.g., randomly selecting 200 images from ImageNet-1K and training on resnet-18).
2. The validation accuracy for DeiT-Tiny in Table 15 appears to be subpar. What do you think could be the reason for this result?
3. Given that other dataset distillation approaches have shown effectiveness at higher distillation ratios, could the authors provide experimental results for this method on the Tiny ImageNet dataset with smaller compression ratios (e.g., IPC=1 and IPC=10)?

Review Point: 1. In Table 2, the authors compared the distillation performance of their proposed method with SRe2L in Tiny ImageNet, ImageNet-1K, and ImageNet-21K datasets. However, I am curious about the performance when randomly sampling an equal number of images from the source dataset, which was not explicitly shown in the table (e.g., randomly selecting 200 images from ImageNet-1K and training on resnet-18).
Review Point: 2. The validation accuracy for DeiT-Tiny in Table 15 appears to be subpar. What do you think could be the reason for this result?
Review Point: 3. Given that other dataset distillation approaches have shown effectiveness at higher distillation ratios, could the authors provide experimental results for this method on the Tiny ImageNet dataset with smaller compression ratios (e.g., IPC=1 and IPC=10)?
==================================================

Focused review:

- Proposed mechanisms all rely on pre-trained models, for filtering (Imagebind), and for annotations (PANNs), these pre-trained models can propagate errors in the pipeline, and it might help to understand better to what extent these errors are inherited in the processing pipeline. One suggestion is to leverage human in the loop to examine a manageable subset.
- For the benchmark experiment in Section 4.1 audio-video retrieval, it is stated that image features are extracted from Imagebind, and video features from InternVid, however, it is not clear how audio features are acquired. It might worth stating explicitly with one or two sentences that Imagebind or Freebind architecture is utilized and pre-trained with proposed datasets. Similarly, it might worth adding a sentence or two to describe CLIPSep so that it is easier for the reader to follow.
- It is worth adding another downstream benchmark for audio only tasks, there are several benchmark such as HEAR [1] that can be utilized. Even including a subset of these tasks can help provide a more thorough view of proposed dataset.
- In section 4, missing separation in (2) Vision-queried sound.
[1] Turian, J., Shier, J., Khan, H. R., Raj, B., Schuller, B. W., Steinmetz, C. J., ... & Bisk, Y. (2022, July). Hear: Holistic evaluation of audio representations. In *NeurIPS 2021 Competitions and Demonstrations Track* (pp. 125-145). PMLR.

Review Point: - Proposed mechanisms all rely on pre-trained models, for filtering (Imagebind), and for annotations (PANNs), these pre-trained models can propagate errors in the pipeline, and it might help to understand better to what extent these errors are inherited in the processing pipeline. One suggestion is to leverage human in the loop to examine a manageable subset.
Review Point: - For the benchmark experiment in Section 4.1 audio-video retrieval, it is stated that image features are extracted from Imagebind, and video features from InternVid, however, it is not clear how audio features are acquired. It might worth stating explicitly with one or two sentences that Imagebind or Freebind architecture is utilized and pre-trained with proposed datasets. Similarly, it might worth adding a sentence or two to describe CLIPSep so that it is easier for the reader to follow.
Review Point: - It is worth adding another downstream benchmark for audio only tasks, there are several benchmark such as HEAR [1] that can be utilized. Even including a subset of these tasks can help provide a more thorough view of proposed dataset.
Review Point: - In section 4, missing separation in (2) Vision-queried sound. [1] Turian, J., Shier, J., Khan, H. R., Raj, B., Schuller, B. W., Steinmetz, C. J., ... & Bisk, Y. (2022, July). Hear: Holistic evaluation of audio representations. In *NeurIPS 2021 Competitions and Demonstrations Track* (pp. 125-145). PMLR.
==================================================

Focused review:

- The major weakness is that LLM doesn’t provide more information than the visual cue. This is different from the problem in explaining language models with language models since the visualization technique in the vision domain itself could demonstrate the receptive field of the neurons already and can be much more precise and immune from language model bias.
- Presentation is not clear and precise.
- ViT’s important information comes from the attention mechanism. How does the proposed work be used to examine the attention maps?

Review Point: - The major weakness is that LLM doesn’t provide more information than the visual cue. This is different from the problem in explaining language models with language models since the visualization technique in the vision domain itself could demonstrate the receptive field of the neurons already and can be much more precise and immune from language model bias.
Review Point: - ViT’s important information comes from the attention mechanism. How does the proposed work be used to examine the attention maps?
==================================================

Focused review:

: 1. The proposed method is a two-stage optimization strategy, which is a bit difficult to balance the two steps optimization. Could it be end-to-end training? 2. Although it is intuitive that including multiple local prompts helps, for different categories, the features and their positions are not the same.

Review Point: 2. Although it is intuitive that including multiple local prompts helps, for different categories, the features and their positions are not the same.
==================================================

Focused review:

1. The analyses are very superficial, as only trends in standard benchmarks are measured and discussed. It is unclear what significance do t-SNE graphs show. The authors did not attempt to arrive at any form of empirical laws for typical scaling law works.
2. The main strategy has no novelty, as it's simply a mixture of different datasets. It seems very straightforward that adjusting the mixture will result in different capabilities on benchmarks, as well as the general rule of thumb that "more data under proper learning set-up results in better performance." The authors did not attempt to give any theoretical analysis of what they have observed, which further weakens the novelty claim.
3. Some wordings in the manuscript are unclear and unfit for an academic context. e.g. data amount, 100 thousand samples.

Review Point: 1. The analyses are very superficial, as only trends in standard benchmarks are measured and discussed. It is unclear what significance do t-SNE graphs show. The authors did not attempt to arrive at any form of empirical laws for typical scaling law works.
Review Point: 2. The main strategy has no novelty, as it's simply a mixture of different datasets. It seems very straightforward that adjusting the mixture will result in different capabilities on benchmarks, as well as the general rule of thumb that "more data under proper learning set-up results in better performance." The authors did not attempt to give any theoretical analysis of what they have observed, which further weakens the novelty claim.
Review Point: 3. Some wordings in the manuscript are unclear and unfit for an academic context. e.g. data amount, 100 thousand samples.
==================================================

Focused review:

1.	in my comprehension, the sentence-level prompts are actually latent vectors output from the MLP layer, so it is hard to make sure the prompts work as expected as demonstrated in Figure 1(c), i.e., decoupling the multiple objects and attributes of query image, and correctly integrating the process of object removal or attribute modification.
2.	It is difficult to understand the pi’ in prompt alignment loss. Whether each reference image has an auxiliary text prompt? As a result, the Figure 2(a) involves two training stages (ITC loss to optimize p and prompt alignment loss to optimize pi’)? Furthermore, during the optimization of pi’, the text encoder is frozen, so the image encoder learns to align with the frozen text encoder; while in optimization of pi, the text encoder is not frozen, so the image encoder learns to align with the updated text encoder. I find it hard to understand how the prompt alignment loss works and it seems very tricky to achieve a good performance.

Review Point: 1. in my comprehension, the sentence-level prompts are actually latent vectors output from the MLP layer, so it is hard to make sure the prompts work as expected as demonstrated in Figure 1(c), i.e., decoupling the multiple objects and attributes of query image, and correctly integrating the process of object removal or attribute modification.
==================================================

Focused review:

1. The images in the four selected datasets seem relatively simple, with very clean backgrounds. Have you considered comparing your proposed model with baseline models on more realistic image datasets, such as the COCO dataset?
2. Given that the GAP mechanism involves multiple steps (e.g., saliency map generation, inhibition of return), have you compared its computational performance with other baseline models?
3. Why can GAP-Abstractor improve OOD generalization of same-different relation and more abstract relations?

Review Point: 1. The images in the four selected datasets seem relatively simple, with very clean backgrounds. Have you considered comparing your proposed model with baseline models on more realistic image datasets, such as the COCO dataset?
Review Point: 2. Given that the GAP mechanism involves multiple steps (e.g., saliency map generation, inhibition of return), have you compared its computational performance with other baseline models?
Review Point: 3. Why can GAP-Abstractor improve OOD generalization of same-different relation and more abstract relations?
==================================================

Focused review:

- In Section 3.4, the paper discusses two alternative training strategies: one that directly incorporates all loss terms and another that initially trains with cross-entropy (CE) loss before introducing the proposed losses. However, guidance on when to choose between these strategies in general cases is lacking. Additional discussion on how this choice could affect the model’s performance or usability would be beneficial.
- The proposed method mainly includes two loss terms, i.e., the Sep loss and Clu loss, upon the OE loss. As discussed in the paper (e.g., the abstract), the Sep loss reflects the main motivation of neural collapse (NC). The ablation studies in Table 5 show that these two terms perform with different significances. The behaviours and importance of these two loss teams are unclear.
- Although the proposed method performs well on ImageNet-1k (Table 1), its performance is less consistent on other benchmarks, as seen in Table 2 and particularly Table 3, where broader comparisons are provided. Further explanation or analysis regarding this variability would be valuable to understand the method’s limitations.
- More analysis of the learned representation space, such as visualizations of the embedding space, would provide insight into how well the method achieves feature separation and could strengthen the evaluation of its effectiveness.

Review Point: - In Section 3.4, the paper discusses two alternative training strategies: one that directly incorporates all loss terms and another that initially trains with cross-entropy (CE) loss before introducing the proposed losses. However, guidance on when to choose between these strategies in general cases is lacking. Additional discussion on how this choice could affect the model’s performance or usability would be beneficial.
Review Point: - The proposed method mainly includes two loss terms, i.e., the Sep loss and Clu loss, upon the OE loss. As discussed in the paper (e.g., the abstract), the Sep loss reflects the main motivation of neural collapse (NC). The ablation studies in Table 5 show that these two terms perform with different significances. The behaviours and importance of these two loss teams are unclear.
Review Point: - Although the proposed method performs well on ImageNet-1k (Table 1), its performance is less consistent on other benchmarks, as seen in Table 2 and particularly Table 3, where broader comparisons are provided. Further explanation or analysis regarding this variability would be valuable to understand the method’s limitations.
Review Point: - More analysis of the learned representation space, such as visualizations of the embedding space, would provide insight into how well the method achieves feature separation and could strengthen the evaluation of its effectiveness.
==================================================

Focused review:

- The language and presentation of the paper could be further improved, particularly in figures and visual representations. (e.g. Figure 2).
- While the paper is designed for link prediction tasks, the motivation behind applying LLwLC in link prediction is not well-explained, especially given that the propagation process appears to be a learnable spectral graph filter. It would be helpful to clarify why LLwLC is particularly suited for link prediction tasks.
- Though the authors have demonstrated that the expressiveness LLwLC propagation is superior to MPNNs on k-regular graphs, they are still a limited subset of all the 1-WL isomorphism graphs.
- The implementation of the proposed model is not accessible, which makes it difficult for others to reproduce the experiments and further validate the results.

Review Point: - The language and presentation of the paper could be further improved, particularly in figures and visual representations. (e.g. Figure 2).
Review Point: - While the paper is designed for link prediction tasks, the motivation behind applying LLwLC in link prediction is not well-explained, especially given that the propagation process appears to be a learnable spectral graph filter. It would be helpful to clarify why LLwLC is particularly suited for link prediction tasks.
Review Point: - Though the authors have demonstrated that the expressiveness LLwLC propagation is superior to MPNNs on k-regular graphs, they are still a limited subset of all the 1-WL isomorphism graphs.
Review Point: - The implementation of the proposed model is not accessible, which makes it difficult for others to reproduce the experiments and further validate the results.
==================================================

Focused review:

- While the idea appears practical and well-founded, the rationale behind including specific steps and how each contributes to overall performance are missing. Furthermore, it is unclear why the proposed method would outperform SemMAE and AutoMAE, which also use adaptive mask generation. The authors are encouraged to provide some insights into why their approach may offer an advantage over these similar methods.
- The multi-level optimization presented appears to depend heavily on the previous steps. While the problem formulation in Eq. (4) integrates all sub-objectives, each optimization step must wait for the completion of the previous one. This approach seems a bit cumbersome, potentially leading to cache and memory issues (which may not be fully resolved by standard packages) and slowing down the process due to the sequential nature of the steps.
- There are no analyses of the generated masks during or after training, nor comparisons with the outputs of other mask generators (e.g., SemMAE's and AutoMAE's). The reviewer believes presenting this analysis is crucial, as it could provide insights into training dynamics, specifically which parts of images should be visible or reconstructed to impact performance.
- In training, stages 2 and 3 utilize downstream task data, which seems to be used more extensively than in standard practice, where downstream data is only introduced during the fine-tuning stage. This incurs an unfair evaluation.

Review Point: - While the idea appears practical and well-founded, the rationale behind including specific steps and how each contributes to overall performance are missing. Furthermore, it is unclear why the proposed method would outperform SemMAE and AutoMAE, which also use adaptive mask generation. The authors are encouraged to provide some insights into why their approach may offer an advantage over these similar methods.
Review Point: - The multi-level optimization presented appears to depend heavily on the previous steps. While the problem formulation in Eq. (4) integrates all sub-objectives, each optimization step must wait for the completion of the previous one. This approach seems a bit cumbersome, potentially leading to cache and memory issues (which may not be fully resolved by standard packages) and slowing down the process due to the sequential nature of the steps.
Review Point: - There are no analyses of the generated masks during or after training, nor comparisons with the outputs of other mask generators (e.g., SemMAE's and AutoMAE's). The reviewer believes presenting this analysis is crucial, as it could provide insights into training dynamics, specifically which parts of images should be visible or reconstructed to impact performance.
Review Point: - In training, stages 2 and 3 utilize downstream task data, which seems to be used more extensively than in standard practice, where downstream data is only introduced during the fine-tuning stage. This incurs an unfair evaluation.
==================================================

Focused review:

- Is there any kind of guarantee the answers having correct integration of retrieved results present in the Top k answer without touching upon the core mechanism of model generation? A more convincing approach is to further ensure the groundness of the next generation after a generating error show up.
- The paper claims that the input-output pair is automatically constructed, how the label of groundedness of the augmented answer was assigned automatically? This is the point that needs to be clarify.
- And I did not find the description about whether these baselines use the same generated model, which is important of comparison in the experiment.
- The proposed method is more like expand the space to choose the answer by the help of a trained verifier. So is there a more integreted and concise structure to build this model. For example, aligning directly during retrieval and generation to ensure the correctness of each step.

Review Point: - Is there any kind of guarantee the answers having correct integration of retrieved results present in the Top k answer without touching upon the core mechanism of model generation? A more convincing approach is to further ensure the groundness of the next generation after a generating error show up.
Review Point: - The paper claims that the input-output pair is automatically constructed, how the label of groundedness of the augmented answer was assigned automatically? This is the point that needs to be clarify.
Review Point: - And I did not find the description about whether these baselines use the same generated model, which is important of comparison in the experiment.
Review Point: - The proposed method is more like expand the space to choose the answer by the help of a trained verifier. So is there a more integreted and concise structure to build this model. For example, aligning directly during retrieval and generation to ensure the correctness of each step.
==================================================

Focused review:

1. The writing and organization of this paper can be improved. This work primarily follows *Xu et al. (2022)* which proposed the self-reinforcement effect. So, I think there should be more comparisons between your work and this one, like what's your improvements over their work, and why you propose two new self-reinforcement metrics apart from their TP, IP, WR metrics. But I can hardly find the words or comparisons about *Xu et al. (2022)* in the paper.
I think this paper is a bit hard for readers from other fields to understand easily.
2. It is incremental work. The two kinds of new measurements of the self-reinforcement effect are rather interesting and nice. But the proposed method is trivial. The forgetting mechanism is highly similar to the *penalized sampling* proposed by *Keskar et al. (2019)*.
3. The experiments are not comprehensive and convincing. The improvements are marginal. On some metrics, the performance of the proposed method even is worse than the baselines. I cannot confirm the effectiveness of the method according to current experimental results.

Review Point: 1. The writing and organization of this paper can be improved. This work primarily follows *Xu et al. (2022)* which proposed the self-reinforcement effect. So, I think there should be more comparisons between your work and this one, like what's your improvements over their work, and why you propose two new self-reinforcement metrics apart from their TP, IP, WR metrics. But I can hardly find the words or comparisons about *Xu et al. (2022)* in the paper. I think this paper is a bit hard for readers from other fields to understand easily.
Review Point: 2. It is incremental work. The two kinds of new measurements of the self-reinforcement effect are rather interesting and nice. But the proposed method is trivial. The forgetting mechanism is highly similar to the *penalized sampling* proposed by *Keskar et al. (2019)*.
Review Point: 3. The experiments are not comprehensive and convincing. The improvements are marginal. On some metrics, the performance of the proposed method even is worse than the baselines. I cannot confirm the effectiveness of the method according to current experimental results.
==================================================

Focused review:

- Despite the title, I am having hard time locating how exactly the proposed approach helps bounding the propagation of error in SDE systems. I assume what is meant by propagation is propagation through time. The approach appears to develop bounds for standalone time points, ignoring how these errors warp when passed through the SDE kernel and accummulate.
- Another limitation of the approach is that it ignores the impact of the estimation error, which is a fundamental issue in quite many system identification problems especially under stochasticity. In the absence of a link to estimation error, it becomes a bit far-fetched to assess the real-world significance of the developed solution.
- The final weakness is the limited scope of the presented experiments. With due respect to the originality and rigor of the presented theoretical material as admitted above, I still think that the chosen set of SDEs is rather trivial. Specifically, their input dimensionality is too small and the level of difficulty is too limited to assess the significance of the presented solution. Three of the five SDE systems have analytical solutions, where the presented methods would not bring any value-added. I would start from dynamics such as the stochastic Lorenz attractor for which numerical solvers fail to deliver accurate results even in the deterministic variant. Only in such cases I can make sense of the true importance of developing rigorous recursive error bounds. I do expect that the presented material will also apply reasonably well to such cases but unfortunately those results are missing in the current version of the work.

Review Point: - Despite the title, I am having hard time locating how exactly the proposed approach helps bounding the propagation of error in SDE systems. I assume what is meant by propagation is propagation through time. The approach appears to develop bounds for standalone time points, ignoring how these errors warp when passed through the SDE kernel and accummulate.
Review Point: - Another limitation of the approach is that it ignores the impact of the estimation error, which is a fundamental issue in quite many system identification problems especially under stochasticity. In the absence of a link to estimation error, it becomes a bit far-fetched to assess the real-world significance of the developed solution.
==================================================

Focused review:

I have some concerns about the novelty and experimental analysis of the proposed approach. In general, some design choices (e.g., assuming to know the model shift), are interesting, but they could be better explored.
**[Contributions]** In Section 1, the statement “the first optimal algorithm for any robust recourse problem” is misleading. There are other settings in which recourse robustness is required in which your method might **not** achieve the optimal (see [1] for an exhaustive list). Moreover, one could argue that in practice most models are deep neural networks, thus making the paper’s approach lose any guarantees.
**[Learning-Augmented Framework]** The authors consider a setting in which they have access to a prediction over the new model parameters $\hat{\theta}$. In practice, one could simply compute solutions for robust recourse using standard methods (e.g., ROAR, RBR) by considering the updated model $f_\hat{\theta}$. This consideration undermines the overall contribution to the approach (since both objective (3) and metrics (5), (6) are very similar to related cited literature). I recommend that the authors discuss the limitations/issues arising from obtaining a prediction of the model update and/or propose a computational model to describe them better.
**[Algorithm 1]** Section 3.1 would benefit a computational complexity analysis of Algorithm 1. It might as well achieve the global optimum, but if it is exponential in the number of features we could settle for approximate solutions. Lastly, in the case of a local approximation, Algorithm 1 might not be optimal anymore, and the authors should discuss the limitations of their approach in such a scenario.
**[Experimental Analysis with $\hat{\theta}$]**
- Following the [**Learning-Augmented Framework**] comment, it is not clear if the analysis in Section 4 considers competitors (ROAR and RBR) using the predicted model $f_\hat{\theta}$ or the original $f_\theta$. The latter case would penalize the competitors and it would result in an unfair comparison between the approaches.
- In Section 4 (Figure 2), the authors consider a simple setting in which the $\alpha$ parameter in Algorithm 1 is the same used to perturb the model. I believe it is a simplifying assumption since in practice we might have access to a noisy estimator of $\alpha$.
- The experiments could benefit from additional baselines using adversarial min-max objectives (e.g., Dominguez-Olmedo et al., 2022 can be run without the causal structure, by only considering the $\epsilon$-ball) or other forms of robustness (see Section 4 of [1] for a fairly large list).
[1] Jiang, Junqi, et al. "Robust counterfactual explanations in machine learning: A survey." IJCAI (2024).

Review Point: - In Section 4 (Figure 2), the authors consider a simple setting in which the $\alpha$ parameter in Algorithm 1 is the same used to perturb the model. I believe it is a simplifying assumption since in practice we might have access to a noisy estimator of $\alpha$.
Review Point: - The experiments could benefit from additional baselines using adversarial min-max objectives (e.g., Dominguez-Olmedo et al., 2022 can be run without the causal structure, by only considering the $\epsilon$-ball) or other forms of robustness (see Section 4 of [1] for a fairly large list). [1] Jiang, Junqi, et al. "Robust counterfactual explanations in machine learning: A survey." IJCAI (2024).
==================================================

Focused review:

1. I am concerned that the theoretical contribution of this work appears to have a somewhat narrow application scope in its current presentation, as it focuses specifically on matrix normalization methods in GCP for deep neural networks, particularly on the matrix logarithm and matrix power. I suggest that the authors further clarify the practical significance of matrix normalization and the presented theories.
2. There are some other matrix normalization methods in GCP beyond matrix logarithm and matrix power. Popular choices [1] include matrix logarithm, element-wise power, matrix square-root and matrix power normalization. Is the element-wise power normalization applicable to the presented theories? Does it require additional analysis?
3. In Figure 1, in addition to showing the gap between LEM and PEM, it would be beneficial to highlight the consequences of this gap.
[1] Wang, Qilong, et al. "What deep CNNs benefit from global covariance pooling: An optimization perspective." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.

Review Point: 1. I am concerned that the theoretical contribution of this work appears to have a somewhat narrow application scope in its current presentation, as it focuses specifically on matrix normalization methods in GCP for deep neural networks, particularly on the matrix logarithm and matrix power. I suggest that the authors further clarify the practical significance of matrix normalization and the presented theories.
Review Point: 2. There are some other matrix normalization methods in GCP beyond matrix logarithm and matrix power. Popular choices [1] include matrix logarithm, element-wise power, matrix square-root and matrix power normalization. Is the element-wise power normalization applicable to the presented theories? Does it require additional analysis?
Review Point: 3. In Figure 1, in addition to showing the gap between LEM and PEM, it would be beneficial to highlight the consequences of this gap. [1] Wang, Qilong, et al. "What deep CNNs benefit from global covariance pooling: An optimization perspective." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
==================================================

Focused review:

- It is unclear why GILL cannot be compared on VIST and MMDialog.
* GILL trains on CC3M image-text pairs dataset, and the images and captions are also used in this paper.
* (please correct me if I am wrong) GILL does not finetune on VIST / MMDialog while this paper does.
- Lack of the synthetic caption baseline.
* Despite the paper argues the scarcity of the descriptive captions in the existing datasets and it is hard to automatically generate high-quality captions, there is no quantitative numbers justifying that the synthetic caption has poor quality.
- The baseline experiments are not properly explained, and there can be as simple but stronger baselines for comparison
* Comparison with MiniGPT-4: the authors do not clearly explain the inputs and outputs used to finetune MiniGPT-4. A proper baseline would be to use MiniGPT-4 to caption the training images and train it to output both a text (story) and a description (prompt for SD).
- The design choices are not well-ablated/well-justified.
* The proposed voken dropping for CFG is not ablated. The simplest alternative is to just use the default empty feature of the SD in inference. An alternative is to learn the empty features, without messing the weights in the Feature Mapper.
* Why the number of vokens $n=8$? In Supp. Fig. 6, $n=16$ is consistently better than $n=8$ (significantly better in $IS$). Also, given the trend (especially given the large leap for IS from 8$\rightarrow$16), why not ablating $n>16$? Furthermore, there are 77 tokens for the text encoder in Stable Diffusion -- in Table 6, MiniGPT-5 performs worse than Stable Diffusion, is this because $n$ is too small?
- The name of the proposed approach is confusing and needs justification.
The proposed method is named MiniGPT-5. However, the naming is confusing and needs better justification -- there is no GPT-5 from OpenAI available yet. The naming itself can make people confused on whether OpenAI has released GPT-5 or not. Also, it is unclear why the authors name the approach this way.

Review Point: - It is unclear why GILL cannot be compared on VIST and MMDialog.
Review Point: * GILL trains on CC3M image-text pairs dataset, and the images and captions are also used in this paper.
Review Point: * (please correct me if I am wrong) GILL does not finetune on VIST / MMDialog while this paper does.
Review Point: * Despite the paper argues the scarcity of the descriptive captions in the existing datasets and it is hard to automatically generate high-quality captions, there is no quantitative numbers justifying that the synthetic caption has poor quality.
Review Point: - The baseline experiments are not properly explained, and there can be as simple but stronger baselines for comparison * Comparison with MiniGPT-4: the authors do not clearly explain the inputs and outputs used to finetune MiniGPT-4. A proper baseline would be to use MiniGPT-4 to caption the training images and train it to output both a text (story) and a description (prompt for SD).
Review Point: * The proposed voken dropping for CFG is not ablated. The simplest alternative is to just use the default empty feature of the SD in inference. An alternative is to learn the empty features, without messing the weights in the Feature Mapper.
Review Point: * Why the number of vokens $n=8$? In Supp. Fig. 6, $n=16$ is consistently better than $n=8$ (significantly better in $IS$). Also, given the trend (especially given the large leap for IS from 8$\rightarrow$16), why not ablating $n>16$? Furthermore, there are 77 tokens for the text encoder in Stable Diffusion -- in Table 6, MiniGPT-5 performs worse than Stable Diffusion, is this because $n$ is too small?
Review Point: - The name of the proposed approach is confusing and needs justification. The proposed method is named MiniGPT-5. However, the naming is confusing and needs better justification -- there is no GPT-5 from OpenAI available yet. The naming itself can make people confused on whether OpenAI has released GPT-5 or not. Also, it is unclear why the authors name the approach this way.
==================================================

Focused review:

: The claims are not well-supported by the current experiments. The detailed comments are below. 1. The mirror ICNN uses a duplication input pair. So its weights U_i and W_i are larger than those of the basic ICNN. What is a fair comparison between ICNN and mirror ICNN? It needs to give more detailed descriptions on the issue. At the end of the first paragraph in subsection 4.3, it claims that it uses the same quantity of hidden neurons. What does it mean? 2. It is better to include ICNN, modified ICNN by Chen et al. 2018, and the proposed mirror ICNN in all the comparisons. For example, it needs to compare with the modified ICNN by Chen et al. 2018 in Figure 4. 3. The compared neural network are all shallow. It is better to test with deep networks. 4. Which activation function is used in the experiment? Does the used activation function affect the results? 5. In Figure 5 and Table 1, why the modified ICNN proposed by Chen et al. (2018) performs so poor? It is known that the modified ICNN gets more representation power than the basic ICNN. 6. There are four propositions in the paper. It should distinguish between what propositions proposed in this paper and what propositions in others. 7. It is better to use three-line tables.

Review Point: 1. The mirror ICNN uses a duplication input pair. So its weights U_i and W_i are larger than those of the basic ICNN. What is a fair comparison between ICNN and mirror ICNN? It needs to give more detailed descriptions on the issue. At the end of the first paragraph in subsection 4.3, it claims that it uses the same quantity of hidden neurons. What does it mean?
Review Point: 2. It is better to include ICNN, modified ICNN by Chen et al. 2018, and the proposed mirror ICNN in all the comparisons. For example, it needs to compare with the modified ICNN by Chen et al. 2018 in Figure 4.
Review Point: 3. The compared neural network are all shallow. It is better to test with deep networks.
Review Point: 4. Which activation function is used in the experiment? Does the used activation function affect the results?
Review Point: 5. In Figure 5 and Table 1, why the modified ICNN proposed by Chen et al. (2018) performs so poor? It is known that the modified ICNN gets more representation power than the basic ICNN.
Review Point: 6. There are four propositions in the paper. It should distinguish between what propositions proposed in this paper and what propositions in others.
==================================================

Focused review:

1. While the paper effectively examines the Kalamang and Nepali languages, its findings may not generalize across the diversity of low-resource languages. XLR languages can vary significantly in structure, script, and available resources, and focusing only on these two could limit the applicability of the conclusions. It would be nice to do similar analysis on low-resource languages too.
2. The paper introduces typological prompts as a promising way to improve linguistic tasks but does not compare this approach against other techniques that incorporate linguistic typology (e.g., embeddings based on typological features from databases like [WALS](https://arxiv.org/abs/2010.03920))

Review Point: 1. While the paper effectively examines the Kalamang and Nepali languages, its findings may not generalize across the diversity of low-resource languages. XLR languages can vary significantly in structure, script, and available resources, and focusing only on these two could limit the applicability of the conclusions. It would be nice to do similar analysis on low-resource languages too.
==================================================

Focused review:

-	The paper misses to describe essential properties of the created dataset (e.g. number of (distinct) entities per type etc. – see detailed comment below), which complicates the assessment of the resource created
-	Details of the benchmark and the evaluation protocol are missing, which limits the reproducibility.
-	I am not entirely sure whether all necessary measures have been taken to ensure the anonymity of persons / authors of the texts collected.

Review Point: - The paper misses to describe essential properties of the created dataset (e.g. number of (distinct) entities per type etc. – see detailed comment below), which complicates the assessment of the resource created - Details of the benchmark and the evaluation protocol are missing, which limits the reproducibility.
Review Point: - I am not entirely sure whether all necessary measures have been taken to ensure the anonymity of persons / authors of the texts collected.
==================================================

Focused review:

While I am pleased with the paper and see no major flaws, I'll list out a couple of minor issues I would only perhaps use if I had to be very nit-picky on the margins given a host of other papers I deem of similar quality.
- MeLLo requires storing information externally, therefore it would appear to me that it is only useful when a limited amount of facts need to be edited. In the event that perhaps a large portion of the model's knowledge needs to be edited, it would appear to me that MeLLo might not scale as well as other methods that are benchmarked against.
- MQuAKE is limited to only 2-4 hop questions, which would appear somewhat limited given the size of the models generally used nowadays.

Review Point: - MeLLo requires storing information externally, therefore it would appear to me that it is only useful when a limited amount of facts need to be edited. In the event that perhaps a large portion of the model's knowledge needs to be edited, it would appear to me that MeLLo might not scale as well as other methods that are benchmarked against.
Review Point: - MQuAKE is limited to only 2-4 hop questions, which would appear somewhat limited given the size of the models generally used nowadays.
==================================================

Focused review:

The writing is confusing and difficult to understand. Not only the expression of writing is confusing, but also many important clarifications about technique can not be found in the paper. We list these issues as follows.
+ The evaluation of zero-shot transfer. In the main body of paper, the authors only list the datasets and metrics used for evaluation, but don't mention why and how.
+ For object centric finetuning, the authors don't mention how to conduct slot attention and top-k mlp decoder (the structure details) and why use these module. What is the loss function? Where do the output images come from in Figure 3? Are the output masked images just the output of DINOv2?
In summary, I am inclined to reject this submission in the current presentation.

Review Point: + The evaluation of zero-shot transfer. In the main body of paper, the authors only list the datasets and metrics used for evaluation, but don't mention why and how.
Review Point: + For object centric finetuning, the authors don't mention how to conduct slot attention and top-k mlp decoder (the structure details) and why use these module. What is the loss function? Where do the output images come from in Figure 3? Are the output masked images just the output of DINOv2? In summary, I am inclined to reject this submission in the current presentation.
==================================================

Focused review:

Some concerns remain over the empirical evaluation, as detailed in the comments.
1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to "randomly stor[e] a few examples of old classes" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases.
Basically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples?
In summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.
3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that "The performance drops with removing Lco ("w/o CO"). It demonstrates that it is helpful to handle medical rare words" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed.
Minor grammatical/spelling concerns: (Line 97) "Medica intent" (Line 124) "Contrastive Reply Networks" (reply or replay?)
(Line 161) "...top n example(s)" etc.

Review Point: 3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that "The performance drops with removing Lco ("w/o CO"). It demonstrates that it is helpful to handle medical rare words" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed. Minor grammatical/spelling concerns: (Line 97) "Medica intent" (Line 124) "Contrastive Reply Networks" (reply or replay?) (Line 161) "...top n example(s)" etc.
==================================================

Focused review:

Problems:
1. The authors wrote: “We crop sub-images of the individuals and assign them identifiable labels, such as person names.” in lIne243. I would like to know how to assign each individual a person's name. Is the name randomly generated via Large language models e.g. GPT4?
2. Why the results of Qwen-VL-Chat are not included in Table2 and Table3. Qwen-VL-Chat should be a baseline since the proposed IDA-VLM is built based on it.
3. More models should be included in Table1. There are a lot of popular open-sourced VLMs e.g. LLaVa. These models should be included in Table1 to enhance the benchmark. and facilate future reserch.
4. In the Table4, more standard benchmarks should be included. Only evaluating IDA-VLM on two benchmarks is not enough. The authors should include more benchmarks e.g TextVQA and MMBench for a more comprehensive evaluation. Issues:
1.The font in the figures is too small, which makes it hard for readers to read. It is highly recommended to increase the font size in figures.

Review Point: 2. Why the results of Qwen-VL-Chat are not included in Table2 and Table3. Qwen-VL-Chat should be a baseline since the proposed IDA-VLM is built based on it.
Review Point: 3. More models should be included in Table1. There are a lot of popular open-sourced VLMs e.g. LLaVa. These models should be included in Table1 to enhance the benchmark. and facilate future reserch.
Review Point: 4. In the Table4, more standard benchmarks should be included. Only evaluating IDA-VLM on two benchmarks is not enough. The authors should include more benchmarks e.g TextVQA and MMBench for a more comprehensive evaluation. Issues:
Review Point: 1.The font in the figures is too small, which makes it hard for readers to read. It is highly recommended to increase the font size in figures.
==================================================

Focused review:

1. **Lack of a clearly defined dataset**. Since the paper is submitted to the primary area of datasets and benchmarks, I would expect to see a complete dataset or benchmark with detailed descriptions of the components. However, as far as I can see from the paper, it only contains three cases. In case I misunderstood the work, could you please include an anonymous link to the complete dataset with clear documentation?
2. **Lack of extensive experiments.** The paper provides three case studies as the only experimental results, which seem to be insufficient for drawing meaningful conclusions. The settings for the experiments are not clearly stated or studied either.
- Could you clarify the settings of the experiments, such as the benchmarking workflow? For example, whether the experiments were conducted in a zero-shot manner.
- I would also suggest further investigation into few-shot learning, Chain-of-Thought techniques (or even fine-tuning if time permits) to gain deeper understanding of the phenomenon.
- Experiments with modified prompts stating the syntactic rules of Wuxing with concrete examples might also be meaningful to investigate if this helps improve performance.
3. **The discussion and related work sections could be significantly improved.** Instead of listing (loosely) related work indiscriminately, such as works in quantum computing, it might be better to group the work into key topics related to the paper and discuss their relations with each other and limitations.

Review Point: 1. **Lack of a clearly defined dataset**. Since the paper is submitted to the primary area of datasets and benchmarks, I would expect to see a complete dataset or benchmark with detailed descriptions of the components. However, as far as I can see from the paper, it only contains three cases. In case I misunderstood the work, could you please include an anonymous link to the complete dataset with clear documentation?
Review Point: 2. **Lack of extensive experiments.** The paper provides three case studies as the only experimental results, which seem to be insufficient for drawing meaningful conclusions. The settings for the experiments are not clearly stated or studied either.
Review Point: - Could you clarify the settings of the experiments, such as the benchmarking workflow? For example, whether the experiments were conducted in a zero-shot manner.
Review Point: - I would also suggest further investigation into few-shot learning, Chain-of-Thought techniques (or even fine-tuning if time permits) to gain deeper understanding of the phenomenon.
Review Point: - Experiments with modified prompts stating the syntactic rules of Wuxing with concrete examples might also be meaningful to investigate if this helps improve performance.
Review Point: 3. **The discussion and related work sections could be significantly improved.** Instead of listing (loosely) related work indiscriminately, such as works in quantum computing, it might be better to group the work into key topics related to the paper and discuss their relations with each other and limitations.
==================================================

Focused review:

- Although the paper conducted experiments on well-known benchmarks, these datasets have already reached near-saturation performance levels and are relatively small in size. It would be beneficial if the authors could explain their rationale for choosing these specific datasets and discuss whether they considered using larger, more challenging datasets, such as PDBbind, BindingDB, and KIBA. This would provide valuable insight into their dataset selection process and the potential for expanding their evaluation.
- In addition to the methods compared in the paper, comparison with more recent state-of-the-art models [1,2,3,4] is recommended. Could the authors explain their rationale for choosing the current baselines and why recent models were excluded from the experimental design? Additionally, it would be beneficial to compare the performance of the latest models on the same dataset; if a direct comparison is challenging, a discussion on how these recent models might better predict DTI based on model structure would be valuable.
- While the theoretical framework is well-defined, the paper could benefit from more practical insights into the model's implementation, such as an analysis of the model's time complexity and memory requirements. Additionally, a discussion on how the proposed model can be scaled for large and complex datasets would provide valuable insights into its applicability beyond controlled benchmarks.
- Although benchmark performance is discussed, the model's performance in terms of interpretability and its utility in real-world applications are not fully addressed, which may limit the paper's impact on practical DTI applications that require more than predictive accuracy. It would be beneficial to include a case study analyzing which specific characteristics of DTI the model captures to improve predictive performance, providing further insights into its practical applicability and interpretability.
[1] Zhang, Zuolong, et al. "Enhancing generalizability and performance in drug–target interaction identification by integrating pharmacophore and pre-trained models." Bioinformatics 40.Supplement_1 (2024): i539-i547.
[2] Ahmed, Khandakar Tanvir, Md Istiaq Ansari, and Wei Zhang. "DTI-LM: language model powered drug–target interaction prediction." Bioinformatics 40.9 (2024): btae533.
[3] He, Haohuai, Guanxing Chen, and Calvin Yu-Chian Chen. "NHGNN-DTA: a node-adaptive hybrid graph neural network for interpretable drug–target binding affinity prediction." Bioinformatics 39.6 (2023): btad355.
[4] Zhang, Qi, et al. "FMCA-DTI: A Fragment-oriented method based on a Multihead Cross Attention mechanism to improve Drug-Target Interaction prediction." Bioinformatics (2024): btae347.

Review Point: - Although the paper conducted experiments on well-known benchmarks, these datasets have already reached near-saturation performance levels and are relatively small in size. It would be beneficial if the authors could explain their rationale for choosing these specific datasets and discuss whether they considered using larger, more challenging datasets, such as PDBbind, BindingDB, and KIBA. This would provide valuable insight into their dataset selection process and the potential for expanding their evaluation.
Review Point: - In addition to the methods compared in the paper, comparison with more recent state-of-the-art models [1,2,3,4] is recommended. Could the authors explain their rationale for choosing the current baselines and why recent models were excluded from the experimental design? Additionally, it would be beneficial to compare the performance of the latest models on the same dataset; if a direct comparison is challenging, a discussion on how these recent models might better predict DTI based on model structure would be valuable.
Review Point: - While the theoretical framework is well-defined, the paper could benefit from more practical insights into the model's implementation, such as an analysis of the model's time complexity and memory requirements. Additionally, a discussion on how the proposed model can be scaled for large and complex datasets would provide valuable insights into its applicability beyond controlled benchmarks.
==================================================

Focused review:

:
There are some concerns regarding the method description and designs: 1) As described in the ASR update strategy, the rehearsal samples for previous tasks are based on the samples with high and low AS scores (the samples with middle AS scores are discarded) while for the current task the rehearsal samples are uniformly sampled from the corresponding data stream sorted by AS scores. Such difference between previous tasks and current task should be experimentally verified (for instance, why can't the current task follow the same principle as the previous tasks to select the rehearsal samples); 2) In line 5 of Algo.1, should it be noted that B^i_{t-}1 is already sorted according to the AS?
There are other concerns regarding the experimental settings and results: 1) As mentioned in Sec. 4.2, the mixup technique in LUMP is also adopted for the proposed method in the experiments on SplitCIFAR-100 and SplitTiny-ImageNet, there should be experimental results of excluding such mixup technique from the proposed method in order to demonstrate its pure contribution; 2) In order to better demonstrate the contribution of using ASR to update the reply buffer and its generalizabilty, there should be experiments of replacing the rehearsal buffer update strategy in the related works (for both supervised continual learning and continual self-supervised learning baselines that also adopt rehearsal buffer) by the proposed ASR update strategy.
The idea behind "augmentation stability of each sample is positively correlated with its relative position in corresponding category distribution" is not well proven or verified. Though Fig.1 tries to serve such purpose to show such idea, but it is not enough, can the authors provide more solid discussion or even theoretical proof for such idea if it is possible? Moreover, currently there is a hidden assumption that the distribution of each category is single mode, but how if the category distribution is multi-modal (in which it is very likely to happen in more complicated datasets), will AS still be effective as a proxy to the relative positive in a category distribution?
From my own research experience, for supervised continual learning, different strategies of rehearsal example selection (e.g. random or uniform) do not contribute significant difference to the final performance, can authors provide more discussion on the impact of particularly having both representative and discriminative rehearsal samples to the overall performance?

Review Point: 1) As described in the ASR update strategy, the rehearsal samples for previous tasks are based on the samples with high and low AS scores (the samples with middle AS scores are discarded) while for the current task the rehearsal samples are uniformly sampled from the corresponding data stream sorted by AS scores. Such difference between previous tasks and current task should be experimentally verified (for instance, why can't the current task follow the same principle as the previous tasks to select the rehearsal samples);
Review Point: 2) In line 5 of Algo.1, should it be noted that B^i_{t-}1 is already sorted according to the AS? There are other concerns regarding the experimental settings and results:
Review Point: 1) As mentioned in Sec. 4.2, the mixup technique in LUMP is also adopted for the proposed method in the experiments on SplitCIFAR-100 and SplitTiny-ImageNet, there should be experimental results of excluding such mixup technique from the proposed method in order to demonstrate its pure contribution;
==================================================

Focused review:

1. Comparison to the state-of-the-art baselines from natural images in the tasks like RES(Referring Expression Segmentation)[1]. The paper claims superior performance over SOTA baselines. However, it would be beneficial to see more direct comparisons, including visual examples and error analysis, to better understand where and how FLanS outperforms existing methods.
2. Lack of theoretical analysis, as a paper submitted to ICLR.
3.More details of making such a dataset.
[1] Liu, Chang, Henghui Ding, and Xudong Jiang. "GRES: Generalized referring expression segmentation."Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

Review Point: 1. Comparison to the state-of-the-art baselines from natural images in the tasks like RES(Referring Expression Segmentation)[1]. The paper claims superior performance over SOTA baselines. However, it would be beneficial to see more direct comparisons, including visual examples and error analysis, to better understand where and how FLanS outperforms existing methods.
Review Point: 2. Lack of theoretical analysis, as a paper submitted to ICLR.
Review Point: 3.More details of making such a dataset. [1] Liu, Chang, Henghui Ding, and Xudong Jiang. "GRES: Generalized referring expression segmentation."Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
==================================================

Focused review:

1. The training processing of the proposed method is more complicated. It has multiple training stages and losses.
2. Why is your model significantly better than other models in Figure3? This question needs to be explained clearly.

Review Point: 1. The training processing of the proposed method is more complicated. It has multiple training stages and losses.
Review Point: 2. Why is your model significantly better than other models in Figure3? This question needs to be explained clearly.
==================================================

Focused review:

1. The paper is motivated by the observation that the perceptual granularity gap between pre-trained models and downstream segmentation tasks necessitates significant parameter adjustments. However, for models like SAM[1], which are pre-trained on segmentation tasks, there is a lack of evidence to support that these observations still hold.
2. The necessity of additional midstream adaptation is not entirely clear, as the proposed intermediate tasks resemble data augmentation, which could potentially be integrated into the fine-tuning phase.
[1] Kirillov A, Mintun E, Ravi N, et al. Segment anything[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4015-4026

Review Point: 1. The paper is motivated by the observation that the perceptual granularity gap between pre-trained models and downstream segmentation tasks necessitates significant parameter adjustments. However, for models like SAM[1], which are pre-trained on segmentation tasks, there is a lack of evidence to support that these observations still hold.
Review Point: 2. The necessity of additional midstream adaptation is not entirely clear, as the proposed intermediate tasks resemble data augmentation, which could potentially be integrated into the fine-tuning phase. [1] Kirillov A, Mintun E, Ravi N, et al. Segment anything[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4015-4026
==================================================

Focused review:

1. While the audio-to-latents module improves the audio-motion correlation, there is no mention of how different audio characteristics (e.g., background noise, varying loudness) might impact the model’s performance, which could be critical for real-world applications.
2. The paper lacks a detailed analysis of potential failure modes or scenarios where Loopy may struggle. Highlighting these cases would provide a more balanced view of the model's robustness and limitations.

Review Point: 1. While the audio-to-latents module improves the audio-motion correlation, there is no mention of how different audio characteristics (e.g., background noise, varying loudness) might impact the model’s performance, which could be critical for real-world applications.
Review Point: 2. The paper lacks a detailed analysis of potential failure modes or scenarios where Loopy may struggle. Highlighting these cases would provide a more balanced view of the model's robustness and limitations.
==================================================

Focused review:

+ As the adapter in LORA are low-rank linear projection of parameters in attention modules, a combination of task-common and task-specific LORA adapters seems to be equivalent to just Mixture of LORA as addition of linear transformations is still a linear transformation (rows 2 and 3). Thus, it is unclear why decomposing learnable parameters would improve performance.
+ The experimental section misses some studies to show the effective of hyper-parameters in the model. For examples, what is the how number of adapters in task-common and task-specific modules effect the performances? What is the impact of rank or the number of tasks toward final performance? These experiments would offer better insight into how robust the proposed method is under different setting.
+ The performance seems to saturate when being applied to strong base model such as T5-Large. It seems to suggest that the effect of task-common components vanish when the base model can generalize well toward different downstream tasks. This could be an interesting phenomena can be study using stronger base model such as llama and llama2.

Review Point: + As the adapter in LORA are low-rank linear projection of parameters in attention modules, a combination of task-common and task-specific LORA adapters seems to be equivalent to just Mixture of LORA as addition of linear transformations is still a linear transformation (rows 2 and 3). Thus, it is unclear why decomposing learnable parameters would improve performance.
Review Point: + The experimental section misses some studies to show the effective of hyper-parameters in the model. For examples, what is the how number of adapters in task-common and task-specific modules effect the performances? What is the impact of rank or the number of tasks toward final performance? These experiments would offer better insight into how robust the proposed method is under different setting.
Review Point: + The performance seems to saturate when being applied to strong base model such as T5-Large. It seems to suggest that the effect of task-common components vanish when the base model can generalize well toward different downstream tasks. This could be an interesting phenomena can be study using stronger base model such as llama and llama2.
==================================================


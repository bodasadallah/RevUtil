Focused review:

weaknesses as well. W1: My biggest concern is the study on the relation between NTR and DIS (domain invariance score). Fig. 2 shows Res1 block has the lowest NTR and highest DIS. Compared to Res1 block, Res3 block shows higher NTR and lower DIS. If NTR and DIS are the only criteria, why do we need to use Res3 block feature for alignment? Why we cannot align Res2 and Res1 as well?
W2: The proposed architecture modification does not seem to align the mid-level feature only. By minimizing L_{em} we align the mid-level feature. However, minimizing L_{s,n} and L_{t,n} also align the high-level feature as f_n is attached on top of Res4 and GAP. This is a self-contraction with the claim made from Fig. 2.
W3: The improvement from each component is not very surprising. BoW-style mid-level feature + L_{em} only improves 0.4 ~ 0.5 points on top of OVANet. The results make me doubt that the proposed BoW-style feature alignment is really optimal. In addition, the proposed pretext task improves only 0.4 ~ 0.6 points compared to the rotation task.
W4: Presentation quality should be improved. In general, the paper is not straightforward to read. For example, it is hard to understand why NTR in eq. (1) measures negative transfer risk. Another confusion is that do we need to more align the layers with higher DIS or layers with lower DIS.
The authors did not really address the limitations of their work. Please describe what the failure modes of the proposed method are and when the failures happen. They have not discussed the potential negative societal impact of their work.


Review Point: as well.W1: My biggest concern is the study on the relation between NTR and DIS (domain invariance score). Fig. 2 shows Res1 block has the lowest NTR and highest DIS. Compared to Res1 block, Res3 block shows higher NTR and lower DIS. If NTR and DIS are the only criteria, why do we need to use Res3 block feature for alignment? Why we cannot align Res2 and Res1 as well?
Review Point: W2: The proposed architecture modification does not seem to align the mid-level feature only. By minimizing L_{em} we align the mid-level feature. However, minimizing L_{s,n} and L_{t,n} also align the high-level feature as f_n is attached on top of Res4 and GAP. This is a self-contraction with the claim made from Fig.
Review Point: 2.W3: The improvement from each component is not very surprising. BoW-style mid-level feature + L_{em} only improves 0.4 ~ 0.5 points on top of OVANet. The results make me doubt that the proposed BoW-style feature alignment is really optimal. In addition, the proposed pretext task improves only 0.4 ~ 0.6 points compared to the rotation task.
Review Point: W4: Presentation quality should be improved. In general, the paper is not straightforward to read. For example, it is hard to understand why NTR in eq. (1) measures negative transfer risk. Another confusion is that do we need to more align the layers with higher DIS or layers with lower DIS. The authors did not really address the limitations of their work. Please describe what the failure modes of the proposed method are and when the failures happen. They have not discussed the potential negative societal impact of their work.
==================================================

Focused review:

Weaknesses / places I was confused
I am confused about the duality claimed in (3) and proved in Appendix A.7. It seems that the authors only prove weak duality (
p
L
∗
≥
d
L
∗
), but then in the subsequent paragraphs they claim that they prove strong duality
p
L
∗
=
d
L
∗
as guaranteed in Lemma 1. However, the Lemma 1 seems to me unrelated to the arguments in Appendix A.7? In Appendix A.4 it is shown there is no duality gap, but only for the case of L = 3 layers. I would also appreciate clarification with what is new here compared to the paper Wang et al. '21, which is cited at the end and proves there is no duality gap in the same setting, but for any L. (Also, Corollary 1 of this paper states that the analysis extends to any number of layers, but no proof is given.)
The authors state that ResNets are special cases of this parallel network architecture. However, I am unconvinced by this because it requires tying weights (see Appendix A.2) and fixing weights to equal the identity in the parallel network. Doesn't this mean that the optimization problem for finding the minimum-cost ResNet is completely different from the optimization problem for finding the minimum-cost parallel network architecture?
I found Theorem 1 to be difficult to read. Several new variables have been introduced right before stating this theorem. Specifically "we first enumerate all possible signs and diagonal matrices for the ReLU layers and denote them as
I
j
1
,
D
1
i
j
1
, and
D
2
l
respectively, where
j
1
∈
[
m
1
]
,
i
∈
[
P
1
]
,
l
∈
[
P
2
]
". This notation is not very clear to me, and also it took some work to understand that the authors must assume that the data lies in a rank
r
subspace of the
d
-dimensional input space, where effectively
r
=
O
(
1
)
for the algorithm to run in polynomial time.
Since the data must lies in a
r
=
O
(
1
)
-dimensional subspace of
R
d
for the exact algorithm to run in polynomial time, it seems like a stretch to advertise in the abstract that the algorithm runs in polynomial-time in feature dimension. This is true, but if the rank
r
≪
d
, it seems that one can preprocess the data and project it onto the lower-dimensional space and now treat
d
as
d
=
O
(
1
)
.
The sentence "Based on the analysis above, exponential complexity is unavoidable for deep networks when the data matrix is full rank, i.e.,
r
a
n
k
(
X
)
=
m
i
n
(
n
,
d
)
" is not justified. The previous analysis was an upper bound on running times, and it does not say anything about the (unlikely) possibility that there is a better algorithm that runs in time
poly
(
n
,
d
)
time for full-rank data.
Section 3.2 claims "Here, we provide a complete explanation for the representational power of three-layer networks by comparing with the two-layer results in Pilanci & Ergen (2020)". However, (1) it is already known in the literature that two-layer networks are universal representers, so Section 3.2 must be about the cost of representing with 2 vs. 3 layers. And (2) it is already known in the literature that there are functions that take exponentially more neurons to represent with 2 layers than with 3 layers. In short, I am unsure what is contributed by Section 3.2.
Finally, why do the authors take a path regularization instead of an L2 regularization? It would be nice to have an intuitive explanation of why the former yields a better convex problem than the latter.
Strengths
An approximate algorithm is given with a runtime that depends as
n
κ
, where
κ
effective rank of the data. This addresses one of the weaknesses above. The idea here is nice: take a low-rank approximation of the data, and run the exact convex solver on it.
The authors provide experiments on small datasets and small networks, showing that learning with the convex program can yield better/faster solutions than training with SGD or Adam. I found Table 3 in the appendix particularly impressive. However, I would like to know -- if you take a very wide, overparametrized network, and train it with SGD/Adam, will it still be outperformed by the convex program?
Typos
Footnote 2: "all proofs are provided in the supplementary file", should say in appendix A.3
"with three-layer layers"
why is
w
3
k
∈
R
K
? I think this is a typo
"therefore we show that strong duality holds in this case, i.e.,
p
3
∗
≥
d
3
∗
as detailed in Appendix A.4". Should be
p
3
∗
=
d
3
∗


Review Point: / places I was confused I am confused about the duality claimed in (3) and proved in Appendix A.7. It seems that the authors only prove weak duality ( p L ∗ ≥ d L ∗ ), but then in the subsequent paragraphs they claim that they prove strong duality p L ∗ = d L ∗ as guaranteed in Lemma 1. However, the Lemma 1 seems to me unrelated to the arguments in Appendix A.7? In Appendix A.4 it is shown there is no duality gap, but only for the case of L = 3 layers. I would also appreciate clarification with what is new here compared to the paper Wang et al. '21, which is cited at the end and proves there is no duality gap in the same setting, but for any L. (Also, Corollary 1 of this paper states that the analysis extends to any number of layers, but no proof is given.) The authors state that ResNets are special cases of this parallel network architecture. However, I am unconvinced by this because it requires tying weights (see Appendix A.2) and fixing weights to equal the identity in the parallel network. Doesn't this mean that the optimization problem for finding the minimum-cost ResNet is completely different from the optimization problem for finding the minimum-cost parallel network architecture? I found Theorem 1 to be difficult to read. Several new variables have been introduced right before stating this theorem. Specifically "we first enumerate all possible signs and diagonal matrices for the ReLU layers and denote them as I j 1 , D 1 i j 1 , and D 2 l respectively, where j 1 ∈ [ m 1 ] , i ∈ [ P 1 ] , l ∈ [ P 2 ] ". This notation is not very clear to me, and also it took some work to understand that the authors must assume that the data lies in a rank r subspace of the d -dimensional input space, where effectively r = O ( 1 ) for the algorithm to run in polynomial time. Since the data must lies in a r = O ( 1 ) -dimensional subspace of R d for the exact algorithm to run in polynomial time, it seems like a stretch to advertise in the abstract that the algorithm runs in polynomial-time in feature dimension. This is true, but if the rank r ≪ d , it seems that one can preprocess the data and project it onto the lower-dimensional space and now treat d as d = O ( 1 ) . The sentence "Based on the analysis above, exponential complexity is unavoidable for deep networks when the data matrix is full rank, i.e., r a n k ( X ) = m i n ( n , d ) " is not justified. The previous analysis was an upper bound on running times, and it does not say anything about the (unlikely) possibility that there is a better algorithm that runs in time poly ( n , d ) time for full-rank data. Section 3.2 claims "Here, we provide a complete explanation for the representational power of three-layer networks by comparing with the two-layer results in Pilanci & Ergen (2020)". However, (1) it is already known in the literature that two-layer networks are universal representers, so Section 3.2 must be about the cost of representing with 2 vs.
Review Point: 3 layers. And (2) it is already known in the literature that there are functions that take exponentially more neurons to represent with 2 layers than with 3 layers. In short, I am unsure what is contributed by Section 3.2. Finally, why do the authors take a path regularization instead of an L2 regularization? It would be nice to have an intuitive explanation of why the former yields a better convex problem than the latter. Strengths An approximate algorithm is given with a runtime that depends as n κ , where κ effective rank of the data. This addresses one of the weaknesses above. The idea here is nice: take a low-rank approximation of the data, and run the exact convex solver on it. The authors provide experiments on small datasets and small networks, showing that learning with the convex program can yield better/faster solutions than training with SGD or Adam. I found Table 3 in the appendix particularly impressive. However, I would like to know -- if you take a very wide, overparametrized network, and train it with SGD/Adam, will it still be outperformed by the convex program? Typos Footnote 2: "all proofs are provided in the supplementary file", should say in appendix A.3 "with three-layer layers" why is w 3 k ∈ R K ? I think this is a typo "therefore we show that strong duality holds in this case, i.e., p 3 ∗ ≥ d 3 ∗ as detailed in Appendix A.4". Should be p 3 ∗ = d 3 ∗
==================================================

Focused review:

Weaknesses: 1. The input of the mentioned segmentation requires the object located at the center. I think this requirement is too hard to fulfill in the real world, and the proposed method didn’t consider this. 2. Although the paper is focusing on point clouds completion, statistics and comparison of completion performance are missing in the paper.
The authors have not addressed the limitations or potential negative societal impact. My suggestions are listed in the weaknesses parts.


Review Point: 1. The input of the mentioned segmentation requires the object located at the center. I think this requirement is too hard to fulfill in the real world, and the proposed method didn’t consider this.
Review Point: 2. Although the paper is focusing on point clouds completion, statistics and comparison of completion performance are missing in the paper. The authors have not addressed the limitations or potential negative societal impact. My suggestions are listed in the weaknesses parts.
==================================================

Focused review:

- As can be seen from Lemma 3, the higher the outlier proportion, the local conditioning becomes worse. Also, if the noise level tends to 0, the problem becomes non-smooth. It is not clear if the proposed algorithm still well-behaves in such extreme cases. - Some work on robust online regression had been previously discussed, for example: + Briegel, Thomas, and Volker Tresp. "Robust neural network regression for offline and online learning." Advances in Neural Information Processing Systems. 2000. It is not clear why such references are not mentioned and compared in the paper. ======== Update after rebuttal: I thank the authors for providing feedback to all reviewers' comments. Some of my concerns have been addressed. Therefore, I have upgraded my rating for this work to "7. Accept".

Review Point: - As can be seen from Lemma 3, the higher the outlier proportion, the local conditioning becomes worse. Also, if the noise level tends to 0, the problem becomes non-smooth. It is not clear if the proposed algorithm still well-behaves in such extreme cases.
Review Point: - Some work on robust online regression had been previously discussed, for example:
Review Point: + Briegel, Thomas, and Volker Tresp. "Robust neural network regression for offline and online learning." Advances in Neural Information Processing Systems. 2000. It is not clear why such references are not mentioned and compared in the paper. ======== Update after rebuttal: I thank the authors for providing feedback to all reviewers' comments. Some of my concerns have been addressed. Therefore, I have upgraded my rating for this work to "7. Accept".
==================================================

Focused review:

L1) I suggest updating the title, ".. for Multimodal Fusion" since this work limits to bi-modal and audio-visual classification tasks as the authors mentioned in the Conclusion. They fail to show how to scale to more than two modality tasks and if it is also effective other than audio-visual classification tasks.
L2) In the Abstract, the authors argued that the bottleneck forces "to collate and condense the most relevant information in each modality and only share what is necessary." Since the authors evaluate the method using macro metrics for the task, I suggest toning down to "most relevant" -> "relevant" and "only share" -> "share". The current explanation seems to be too strong without sufficient evidence.
L3) Although the attention bottlenecks have the advantage of computational cost, this is sensitive to the fusion layer
L
f
. In some cases, the proposed method, attention bottlenecks are indifferent from vanilla cross-attention as shown in Figure 3. This means if we adjust
L
for the smaller or larger dataset, the hyperparameter
L
f
would be critical to its performance. The authors omit this aspect in the Limitations of the Conclusion section.


Review Point: L1) I suggest updating the title, ".. for Multimodal Fusion" since this work limits to bi-modal and audio-visual classification tasks as the authors mentioned in the Conclusion. They fail to show how to scale to more than two modality tasks and if it is also effective other than audio-visual classification tasks.
Review Point: L2) In the Abstract, the authors argued that the bottleneck forces "to collate and condense the most relevant information in each modality and only share what is necessary." Since the authors evaluate the method using macro metrics for the task, I suggest toning down to "most relevant" -> "relevant" and "only share" -> "share". The current explanation seems to be too strong without sufficient evidence.
Review Point: L3) Although the attention bottlenecks have the advantage of computational cost, this is sensitive to the fusion layer L f . In some cases, the proposed method, attention bottlenecks are indifferent from vanilla cross-attention as shown in Figure 3. This means if we adjust L for the smaller or larger dataset, the hyperparameter L f would be critical to its performance. The authors omit this aspect in the Limitations of the Conclusion section.
==================================================

Focused review:

There are some concerns: 1. In line 82, authors should provide more explanations why they assumed linear constrains. How does it compare with non-linear combination in terms of performance and optimization speed. 2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability. 3. How does the cheap upsampling method (bicubic in the paper) influence the result? What is the limitations of upscaling factor, say will it fail if the factor is 8? 4. More comparisons and results from RAISR should be presented. 5. Experiments on image denoising and deblocking is very limited, lacking quantitative comparisons on benchmarks and intuitive explanation of this generalization.

Review Point: There are some concerns:1. In line 82, authors should provide more explanations why they assumed linear constrains. How does it compare with non-linear combination in terms of performance and optimization speed.
Review Point: 2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability.
Review Point: 3. How does the cheap upsampling method (bicubic in the paper) influence the result? What is the limitations of upscaling factor, say will it fail if the factor is 8?
Review Point: 4. More comparisons and results from RAISR should be presented.
Review Point: 5. Experiments on image denoising and deblocking is very limited, lacking quantitative comparisons on benchmarks and intuitive explanation of this generalization.
==================================================

Focused review:

Weaknesses: W1: The paper provides informal versions of Theorems 1 and 2 only. I cannot find the formal versions as well as their proofs in the appendix. I would recommend the authors to provide more details. This will help to make points such as "comparable approximation power as the Signature" in Theorem 2 more rigorous.
W2: How to choose activation
σ
? There seems also no explanation about this in Section 4.
W3: From time complexity analysis in Section 3.2, the runtime speed up seems significant. However, the experiment in Section 4.1 reveals using Randomized Signature yields less than an order of magnitude speed up (3 times in particular). This can be because
k
is quite large compared to
M
. The authors should provide more detailed runtime comparisons between the two types of signature in the experiments in Section 4.3.
W4: Most experiments in Section 4.3 are on data with
d
≤
5
. How about data with higher number of dimensions such as more than 10? It would be interesting to see scalability to
d
in both runtime and quality of Randomized Signature. Could the authors provide a summary of the 128 UCI data sets for time series classification? This will help to better understand impact of
d
on the performance.
W5: Randomized Signature + Rocket performs worse than Rocket, due to limitations of the update scheme in Algorithm 1. The authors already provided some justifications in the last paragraph of Section 4.4.


Review Point: W1: The paper provides informal versions of Theorems 1 and 2 only. I cannot find the formal versions as well as their proofs in the appendix. I would recommend the authors to provide more details. This will help to make points such as "comparable approximation power as the Signature" in Theorem 2 more rigorous.
Review Point: W2: How to choose activation σ ? There seems also no explanation about this in Section 4.
Review Point: W3: From time complexity analysis in Section 3.2, the runtime speed up seems significant. However, the experiment in Section 4.1 reveals using Randomized Signature yields less than an order of magnitude speed up (3 times in particular). This can be because k is quite large compared to M . The authors should provide more detailed runtime comparisons between the two types of signature in the experiments in Section 4.3.
Review Point: W4: Most experiments in Section 4.3 are on data with d ≤ 5 . How about data with higher number of dimensions such as more than 10? It would be interesting to see scalability to d in both runtime and quality of Randomized Signature. Could the authors provide a summary of the 128 UCI data sets for time series classification? This will help to better understand impact of d on the performance.
Review Point: W5: Randomized Signature + Rocket performs worse than Rocket, due to limitations of the update scheme in Algorithm 1. The authors already provided some justifications in the last paragraph of Section 4.4.
==================================================

Focused review:

1. My only concern is that this is a highly theoretical paper and most of the details are kept in the appendix. Due to the nature of the conference it might be hard for readers to go through such details. I would urge the authors to submit the full version to a journal. 2. The code does not open-sourced. It might be good if the authors could release the code and the datasets for ease of reproducibility.

Review Point: 1. My only concern is that this is a highly theoretical paper and most of the details are kept in the appendix. Due to the nature of the conference it might be hard for readers to go through such details. I would urge the authors to submit the full version to a journal.
Review Point: 2. The code does not open-sourced. It might be good if the authors could release the code and the datasets for ease of reproducibility.
==================================================

Focused review:

- Experimental results seem to be on a single run. Given that some of the differences are not very large, ideally results from multiple runs are included to show the variance in metrics. - "Clean" accuracy and accuracy in the case of small perturbations is worse than MACER; this could compromise practical application where the clean accuracy is also important.

Review Point: - Experimental results seem to be on a single run. Given that some of the differences are not very large, ideally results from multiple runs are included to show the variance in metrics.
Review Point: - "Clean" accuracy and accuracy in the case of small perturbations is worse than MACER; this could compromise practical application where the clean accuracy is also important.
==================================================

Focused review:

Weaknesses: I would have loved to see some experiments on real tasks where these embeddings are used as input beyond the experiments presented in the paper.  That would have made the paper far stronger.
- General Discussion: Even with the aforementioned weakness, I think this is a nice paper to have at ACL.
I have read the author response. 

Review Point: I would have loved to see some experiments on real tasks where these embeddings are used as input beyond the experiments presented in the paper. That would have made the paper far stronger.
Review Point: - General Discussion: Even with the aforementioned weakness, I think this is a nice paper to have at ACL. I have read the author response.
==================================================

Focused review:

Weakness:
My biggest criticism is regarding the current literature review, and therefore experimental design seems completely outdated and ill-suited for contribution to the field of 3D MR brain tumor segmentation. Recent papers by Isensee et al., (2021, 2020) have clearly demonstrated that a well-configured plain U-Net (including its 3D variant) surpasses most existing approaches (to till-date) on a wide range of diverse tasks, including brain tumor segmentation. There have already been numerous high-impact publications that have shown state-of-the-art benchmark performance utilizing 3D-UNets. Some of the most notable one’s includes:
-- Isensee, Fabian, et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nature methods 18.2 (2021): 203-211.
-- Isensee, Fabian, et al. "nnU-net for brain tumor segmentation." International MICCAI Brainlesion Workshop. Springer, Cham, 2020.
-- Myronenko, Andriy. "3D MRI brain tumor segmentation using autoencoder regularization." International MICCAI Brainlesion Workshop. Springer, Cham, 2018
-- Feng, Xue, et al. "Brain tumor segmentation using an ensemble of 3d u-nets and overall survival prediction using radiomic features." Frontiers in computational neuroscience 14 (2020): 25.
-- Islam, Mobarakol, et al. "Brain tumor segmentation and survival prediction using 3D attention UNet." International MICCAI Brainlesion Workshop. Springer, Cham, 2019.
More importantly, recent BraTS challenges (2017-2021) have added more clinically relevant tasks/sub-challenges such as predicting the tumor recurrence or overall survival (Bakas, et al., 2018; Baid et al., 2021), emphasizing the clinical relevance of the brain tumor segmentation task. However, for some reason, the authors did not reference any of these works and chose to still experiment with a sub-task of the challenge. At this point in time, these incremental improvements in architectural designs have severely limited or no real-world value. Without sufficient experimentation on different sub-tasks or with many different organs such as in Isensee et al. 2021, there is no way for me to evaluate the impact of the paper and whether it is actually better than previous approaches.
-- Bakas, Spyridon, et al. "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge." arXiv preprint arXiv:1811.02629 (2018).
-- Baid, Ujjwal, et al. "The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification." arXiv preprint arXiv:2107.02314 (2021).
Dataset description, implementation details, and experimental results have been very poorly presented.
-- For instance, the BRATS 2020 challenge dataset consists of training (369), validation (125), and test (166) images. But, the author’s report in their paper is 250, and it’s not clear what is their train, validation, and test splits.
-- Along with the Dice score, sensitivity, specificity, and Hausdorff distance should be reported, which is a commonly used evaluation metric on this challenge.
-- I don’t see any explanation regarding their experimental results. In Fig. 10, how does the original image looks like and what classes do they belong to?
-- Further, the paper doesn’t show any comparison against the state-of-the-art methods or any ablation experiments.


Review Point: My biggest criticism is regarding the current literature review, and therefore experimental design seems completely outdated and ill-suited for contribution to the field of 3D MR brain tumor segmentation. Recent papers by Isensee et al., (2021, 2020) have clearly demonstrated that a well-configured plain U-Net (including its 3D variant) surpasses most existing approaches (to till-date) on a wide range of diverse tasks, including brain tumor segmentation. There have already been numerous high-impact publications that have shown state-of-the-art benchmark performance utilizing 3D-UNets. Some of the most notable one’s includes: -- Isensee, Fabian, et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nature methods 18.2 (2021): 203-211. -- Isensee, Fabian, et al. "nnU-net for brain tumor segmentation." International MICCAI Brainlesion Workshop. Springer, Cham, 2020. -- Myronenko, Andriy. "3D MRI brain tumor segmentation using autoencoder regularization." International MICCAI Brainlesion Workshop. Springer, Cham, 2018 -- Feng, Xue, et al. "Brain tumor segmentation using an ensemble of 3d u-nets and overall survival prediction using radiomic features." Frontiers in computational neuroscience 14 (2020):
Review Point: 25. -- Islam, Mobarakol, et al. "Brain tumor segmentation and survival prediction using 3D attention UNet." International MICCAI Brainlesion Workshop. Springer, Cham, 2019. More importantly, recent BraTS challenges (2017-2021) have added more clinically relevant tasks/sub-challenges such as predicting the tumor recurrence or overall survival (Bakas, et al., 2018; Baid et al., 2021), emphasizing the clinical relevance of the brain tumor segmentation task. However, for some reason, the authors did not reference any of these works and chose to still experiment with a sub-task of the challenge. At this point in time, these incremental improvements in architectural designs have severely limited or no real-world value. Without sufficient experimentation on different sub-tasks or with many different organs such as in Isensee et al. 2021, there is no way for me to evaluate the impact of the paper and whether it is actually better than previous approaches. -- Bakas, Spyridon, et al. "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge." arXiv preprint arXiv:1811.02629 (2018). -- Baid, Ujjwal, et al. "The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification." arXiv preprint arXiv:2107.02314 (2021). Dataset description, implementation details, and experimental results have been very poorly presented. -- For instance, the BRATS 2020 challenge dataset consists of training (369), validation (125), and test (166) images. But, the author’s report in their paper is 250, and it’s not clear what is their train, validation, and test splits. -- Along with the Dice score, sensitivity, specificity, and Hausdorff distance should be reported, which is a commonly used evaluation metric on this challenge. -- I don’t see any explanation regarding their experimental results. In Fig. 10, how does the original image looks like and what classes do they belong to? -- Further, the paper doesn’t show any comparison against the state-of-the-art methods or any ablation experiments.
==================================================

Focused review:

Weaknesses:
The biggest concern for me is the efficiency of the proposed approach. To build such a ‘virtual’ surrogate model, the authors have to train K models on N datasets respectively. The building process can be expensive. I guess that that is why there don’t test and report the results on ImageNet-1K dataset. However, almost all other approaches listed in this paper work pretty well on ImageNet-1K dataset.
The effectiveness of the proposed ICE can be affected by the following factors: model architecture difference, datasets, label sets, and input sizes. The impact of all these factors should be studied. ICE could be very sensitive to some of them. Especially, the authors report the results on CIFAR10 dataset where the datasets used to build surrogate models is overlapped with the target one. To what extent the overlap will affect the final transferability?
The authors have a good summary of different attack approaches in Tab. 1. Regarding this table, I have a question about the input resolution. The author states that other attack approaches require information about the input resolution while the proposed one does not. Why it is so? For other approaches, they can create transferable adversarial examples at a different scale. I assume it is the responsibility of the target model to resize the adversarial examples to a specific size (which is true in real-world applications). If it is not the case, what size of adversarial example the proposed ICE should create to attack the target model?
In the experimental section 4.5.4, the study shows that the performances of ICE will be damaged by the increase of the number of gradient ascent steps. What about if we use more attack iterations? Is the transferability of the adversarial examples decreased further?
In some cases, the authors also report the results on epsilon= 8/255 (a popular setting). Most experiments are conducted on the setting where the perturbation range is epsilon= 15/255. Is there a particular reason for this choice?
In Figure 5 in the appendix, the authors visualize the created noise created by ICE. They are visually different from the ones created by others. Does the difference tell us anything about the effectiveness of the proposed method? Can the author comment on why it is different?


Review Point: The biggest concern for me is the efficiency of the proposed approach. To build such a ‘virtual’ surrogate model, the authors have to train K models on N datasets respectively. The building process can be expensive. I guess that that is why there don’t test and report the results on ImageNet-1K dataset. However, almost all other approaches listed in this paper work pretty well on ImageNet-1K dataset. The effectiveness of the proposed ICE can be affected by the following factors: model architecture difference, datasets, label sets, and input sizes. The impact of all these factors should be studied. ICE could be very sensitive to some of them. Especially, the authors report the results on CIFAR10 dataset where the datasets used to build surrogate models is overlapped with the target one. To what extent the overlap will affect the final transferability? The authors have a good summary of different attack approaches in Tab.
Review Point: 1. Regarding this table, I have a question about the input resolution. The author states that other attack approaches require information about the input resolution while the proposed one does not. Why it is so? For other approaches, they can create transferable adversarial examples at a different scale. I assume it is the responsibility of the target model to resize the adversarial examples to a specific size (which is true in real-world applications). If it is not the case, what size of adversarial example the proposed ICE should create to attack the target model? In the experimental section 4.5.4, the study shows that the performances of ICE will be damaged by the increase of the number of gradient ascent steps. What about if we use more attack iterations? Is the transferability of the adversarial examples decreased further? In some cases, the authors also report the results on epsilon= 8/255 (a popular setting). Most experiments are conducted on the setting where the perturbation range is epsilon= 15/255. Is there a particular reason for this choice? In Figure 5 in the appendix, the authors visualize the created noise created by ICE. They are visually different from the ones created by others. Does the difference tell us anything about the effectiveness of the proposed method? Can the author comment on why it is different?
==================================================

Focused review:

weakness of this work is in its experiments. The results shown in Figs. 2 & 5 seem unimpressive, and this work contains zero comparisons to any other methods or variants of the model. It is unacceptable that it does not include a comparison to explicit model learning. The results in Figs. 4 & 6 are qualitatively interesting but somewhat hard to interpret; they seem to indicate that the model which is learned is only vaguely related to ground-truth prediction. The comparison between architectures in Fig. 5 shows no significant difference; furthermore, without sharing the amount of data used to train the two architectures it is impossible to evaluate (as inductive biases will be washed out with sufficient data). The number of environment steps used may be computable from the number of training generations, etc in the Appendix but should be explicitly stated in the main body.  It is also clear that with an expressive policy which is able to distinguish between real and generated observations, there is no reason at all that the implicit "model" should need to make forward predictions at all. In that case the policy as a whole reduces to a recurrent neural network policy. It would be important to include a discussion of this limitation.   Clarity The writing quality in this work is high and I enjoyed reading it. However, there are a few details which could use elaboration: - All experiments should include the number of environment samples used. - The observation space for the cartpole example is not explicitly stated.   Significance Currently the significance of this paper is low-medium. It has a clever idea but it does not establish it well enough to motivate follow-on work by others.

Review Point: of this work is in its experiments. The results shown in Figs.
Review Point: 2 & 5 seem unimpressive, and this work contains zero comparisons to any other methods or variants of the model. It is unacceptable that it does not include a comparison to explicit model learning. The results in Figs.
Review Point: 4 & 6 are qualitatively interesting but somewhat hard to interpret; they seem to indicate that the model which is learned is only vaguely related to ground-truth prediction. The comparison between architectures in Fig. 5 shows no significant difference; furthermore, without sharing the amount of data used to train the two architectures it is impossible to evaluate (as inductive biases will be washed out with sufficient data). The number of environment steps used may be computable from the number of training generations, etc in the Appendix but should be explicitly stated in the main body. It is also clear that with an expressive policy which is able to distinguish between real and generated observations, there is no reason at all that the implicit "model" should need to make forward predictions at all. In that case the policy as a whole reduces to a recurrent neural network policy. It would be important to include a discussion of this limitation. Clarity The writing quality in this work is high and I enjoyed reading it. However, there are a few details which could use elaboration:
Review Point: - All experiments should include the number of environment samples used.
Review Point: - The observation space for the cartpole example is not explicitly stated. Significance Currently the significance of this paper is low-medium. It has a clever idea but it does not establish it well enough to motivate follow-on work by others.
==================================================

Focused review:

Weaknesses
The clarity and quality of presentation could be improved, which is a weakness that is hopefully straightforward to fix. Below, I will detail some points that were particularly unclear to me, in the hope that it is useful for the authors as they craft a response. I put explicit questions in "bullet points", with the remaining text as context. I am willing to increase my score if these points can be sufficiently clarified.
(1) Unclear significance of analysis of required rates for
(
ϵ
,
λ
)
As I understand it, the main goal of the analysis is to understand the extra error incurred by using a finite-differencing approach, above and beyond using analytic derivatives: As in Theorem 1, if this error decays fast enough, then one can use empirical derivatives while preserving
O
p
(
n
−
1
/
2
)
rates of estimation. This seems like a very clear type of result, but for an application where the analytic derivative is already well known. Meanwhile, the results in the remaining sections do not go "all the way" to a result like Theorem 1, but instead stop at giving some side-by-side comparison of the analytic and empirical derivatives.
It seems that there are two conclusions the reader is supposed to draw from this analysis, in the context of the remainder of the work: First, that
(
ϵ
,
λ
)
can decay slower than we might generically "expect", for problems with special structure. Second, that this special structure is present in the dynamic treatment regime (DTR) functional, but not in the policy optimization functional. This second point is implied to be surprising, because all three problems exhibit some double-robustness structure (see lines 279-280).
I had trouble drawing such conclusions, though I suspect this is mostly an issue of presentation / clarity.
(1a) First, it seems in several places that the reader should have a "baseline" result in mind, to contrast with the results presented here, but this baseline was not entirely clear. A few examples:
Line 188: "can be a slower rate than implied by the generic analysis of finite differences". What kind of rate would that be, and is there a reference for such results?
Lines 190-191: "potential improvement...could be on the order of generic rate improvements implied by a central difference scheme". What is this referring to?
Lines 278-280: "does not appear that rate-double robustness would admit weaker numerical requirements on
ϵ
". Weaker requirements than what? The "generic analysis" referenced above?
Are there "conservative" rates on
(
ϵ
,
λ
)
that will always preserve
O
P
(
n
−
1
/
2
)
rates of estimation, obtainable via some generic analysis?
The first three questions are about contextualizing the results, but the last one this is important for clarifying whether (a) there is always a generic approach to derive rates on
(
ϵ
,
λ
)
that preserve
O
P
(
n
−
1
/
2
)
convergence, and this is just an improved analysis for specific estimands that shows slower rates are possible, or (b) it is generally necessary to do a "Theorem 1-style" analysis to verify
O
P
(
n
−
1
/
2
)
convergence. The latter conclusion seems much more restrictive than the former.
(1b) Second, conclusions are often made by comparing the form of the empirical and analytical derivatives directly, but these were somewhat difficult to follow:
Proposition 2 (DTR) "verifies that the requirements...are similar in
ϵ
as in the case of a single-timestep", but there is no
O
(
ϵ
2
)
term in either Proposition 1 or Corollary 1. Could you clarify what is meant here?
Propositions 3 & 4 differ not only in an additive term, but also in the usage of perturbed nuisances, which makes them difficult to compare directly (this also applies to Corollary 1, as noted on line 170). Is there a reason why a direct comparison (e.g., isolating only an additive difference) is unnecessary here?
(2) Unclear significance of limitations of Empirical Gateaux derivatives:
As outlined in the introduction, constructive / algorithmic approaches to bias adjustment are very appealing, particularly for problems where small changes require re-derivation of the analytic derivative. This paper strikes an appropriate note of humility in the conclusion, giving limitations of Empirical Gateaux derivatives as a "completely general approach" (lines 329-335), namely the fact that (a) pathwise differentiability and (b) the second-order nature of the remainder must be verified analytically. However, these limitations do seem to undercut the general value of the approach. With that in mind, a few relevant questions
Are there existing scenarios where the analytic form of the gateaux derivative is non-obvious, but where these conditions (pathwise differentiability, second-order remainder) can nonetheless be verified to hold? Or does verifying these conditions always require derivation of the analytic form?
More broadly, are there scenarios where we can apply this approach (with appropriately conservative rates on
(
ϵ
,
λ
)
) and have confidence in achieving
O
P
(
n
−
1
/
2
)
rates, without deriving the analytic derivative? E.g., would the constrained MDP with arbitrary linear constraints be such an example?
If there exist some space of problems where the answer to these questions is "yes", then it would go a long way towards mitigating the impact of these limitations.
Comments on soundness: Regarding technical soundness, I have a (hopefully minor) question or two on Lemma 1
In the display following line 563, it is claimed that the following holds due to Cauchy-Schwarz. I'm not sure I see the application of CS here: Is there another reason why we would expect the cross term
2
E
[
(
μ
~
ϵ
(
X
)
−
μ
~
(
X
)
)
(
μ
~
(
X
)
−
μ
(
X
)
)
]
to be non-positive?
E
(
μ
~
ϵ
(
X
)
−
μ
(
X
)
)
2
≤
E
(
μ
~
ϵ
(
X
)
−
μ
~
(
X
)
)
2
+
E
(
μ
~
(
X
)
−
μ
(
X
)
)
2
In the display following line 562, the last inequality seems like a non-trivial jump, would you mind walking through the logic explicitly?
Otherwise, the proofs seem correct to me. Note that I only read through the proofs for Section 3 in depth, and only skimmed the proofs of other relevant results (e.g., Propositions 3 and 4). While I am well-versed in the causal inference literature, I am not otherwise an expert on non-parametric / semi-parametric statistics, so I may have missed something.
As an aside, it may be helpful to include a citation in the proof for some of the assumed results regarding kernels. [58] is referenced in the main text, referring to kernel smoothing more broadly, but it seems some of the prerequisite results could be cited more precisely (e.g., Lemma 25.1 of [58] appears to be a relevant result)
Other Minor Feedback
I consider the following points to be minor feedback re: presentation / notation / possible typos, and they did not meaningfully influence my score, and they do not require an explicit response from the authors (some are stated as questions only because I am unsure if they are typos).
Suggestions on clarity:
(Lines 15-19) This and some other sentences are a bit long and difficult to parse, and could perhaps be split into multiple sentences.
(Line 71) Is the introduction of projections onto the semi-parametric model necessary, given Remark 1's statement that this work focuses on nonparametric models? CLvdL (Equation 2.2) seems to refers to Luedtke, Carone, and van der Laan (2015) as a reference for the equation on between lines 71-72 holding generally in a non-parametric model.
Other typos / inconsistencies
Example 1 uses
E
P
for the outer expectation, but not for the inner expectation.
Proposition 1 uses $\tilde{\mathbb{E}}{\tilde{P}\epsilon}
i
n
o
n
e
p
l
a
c
e
,
p
e
r
h
a
p
s
t
h
e
t
i
l
d
e
o
n
\mathbb{E}$ was not intended?
Line 65, what is the observation
o
~
? This is not referenced anywhere, I assume this is meant to be
o
′
.
Footnote 1, should the kernel be
λ
−
d
K
(
u
/
λ
)
instead of
h
−
d
K
(
u
/
λ
)
? There is also a reference to an
O
(
h
J
)
error term on line 568 that should perhaps be
O
(
λ
β
)
?
Algorithm 1: Should it be
P
~
on lines 3-4? Should it likewise be
P
ϵ
,
λ
i
instead of
P
ϵ
i
?
Lemma 1: Should
e
~
ϵ
(
X
)
=
p
~
ϵ
(
A
=
1
,
X
)
/
p
~
(
X
)
instead of the current formulation, which uses
p
~
ϵ
(
A
=
1
∣
X
)
in the numerator? This would also make it consistent with usage in the proof (see e.g., line 561)
Line 154, there seems to be a missing parenthesis
Line 149 "analyses of from kernel density estimation"
Assumption 1 (iv), should the equation refer to
μ
~
ϵ
or just
μ
~
? Additionally, should the product-rate condition be
o
p
(
n
−
1
)
as written or
o
p
(
n
−
1
/
2
)
?
Line 561, says to bound perturbed
e
one should "argue similarly", seemingly in reference to the (later) bound on the perturbed
μ
, perhaps the order was swapped.
Line 562, following equation, third equality, missing a square on the final
p
~
(
A
=
1
,
x
)
term.
Line 572,
E
[
Γ
(
O
;
e
ϵ
,
μ
)
]
should be
E
[
Γ
(
O
;
e
~
ϵ
,
μ
~
)
]
Line 637, indicator is missing an
ϵ
on the left-hand side
Supplement, Section D.2, the reference is to citation [20], but I believe this should be citation [18]
Undefined notation:
Line 66, I did not see a definition of the function
g
(
u
)
.
I'm unsure if the dimension
d
was precisely defined prior to usage in Lemma 1, though it is fairly obvious from context.
Eq. 7: I'm not sure if
μ
a
is defined anywhere, aside from being the optimization variable, and similar for
μ
∗
(
s
,
a
)
in Equation 8.
ν
is used a few times in the proofs without being defined (end of equation starting on 576, end of equation starting on 562), presumably referring to a strong-overlap constant.
I think the authors do a fine job of explaining limitations of the approach.


Review Point: The clarity and quality of presentation could be improved, which is a weakness that is hopefully straightforward to fix. Below, I will detail some points that were particularly unclear to me, in the hope that it is useful for the authors as they craft a response. I put explicit questions in "bullet points", with the remaining text as context. I am willing to increase my score if these points can be sufficiently clarified. (1) Unclear significance of analysis of required rates for ( ϵ , λ ) As I understand it, the main goal of the analysis is to understand the extra error incurred by using a finite-differencing approach, above and beyond using analytic derivatives: As in Theorem 1, if this error decays fast enough, then one can use empirical derivatives while preserving O p ( n − 1 / 2 ) rates of estimation. This seems like a very clear type of result, but for an application where the analytic derivative is already well known. Meanwhile, the results in the remaining sections do not go "all the way" to a result like Theorem 1, but instead stop at giving some side-by-side comparison of the analytic and empirical derivatives. It seems that there are two conclusions the reader is supposed to draw from this analysis, in the context of the remainder of the work: First, that ( ϵ , λ ) can decay slower than we might generically "expect", for problems with special structure. Second, that this special structure is present in the dynamic treatment regime (DTR) functional, but not in the policy optimization functional. This second point is implied to be surprising, because all three problems exhibit some double-robustness structure (see lines 279-280). I had trouble drawing such conclusions, though I suspect this is mostly an issue of presentation / clarity. (1a) First, it seems in several places that the reader should have a "baseline" result in mind, to contrast with the results presented here, but this baseline was not entirely clear. A few examples: Line 188: "can be a slower rate than implied by the generic analysis of finite differences". What kind of rate would that be, and is there a reference for such results? Lines 190-191: "potential improvement...could be on the order of generic rate improvements implied by a central difference scheme". What is this referring to? Lines 278-280: "does not appear that rate-double robustness would admit weaker numerical requirements on ϵ ". Weaker requirements than what? The "generic analysis" referenced above? Are there "conservative" rates on ( ϵ , λ ) that will always preserve O P ( n − 1 / 2 ) rates of estimation, obtainable via some generic analysis? The first three questions are about contextualizing the results, but the last one this is important for clarifying whether (a) there is always a generic approach to derive rates on ( ϵ , λ ) that preserve O P ( n − 1 / 2 ) convergence, and this is just an improved analysis for specific estimands that shows slower rates are possible, or (b) it is generally necessary to do a "Theorem 1-style" analysis to verify O P ( n − 1 / 2 ) convergence. The latter conclusion seems much more restrictive than the former. (1b) Second, conclusions are often made by comparing the form of the empirical and analytical derivatives directly, but these were somewhat difficult to follow: Proposition 2 (DTR) "verifies that the requirements...are similar in ϵ as in the case of a single-timestep", but there is no O ( ϵ 2 ) term in either Proposition 1 or Corollary 1. Could you clarify what is meant here? Propositions 3 & 4 differ not only in an additive term, but also in the usage of perturbed nuisances, which makes them difficult to compare directly (this also applies to Corollary 1, as noted on line 170). Is there a reason why a direct comparison (e.g., isolating only an additive difference) is unnecessary here? (2) Unclear significance of limitations of Empirical Gateaux derivatives: As outlined in the introduction, constructive / algorithmic approaches to bias adjustment are very appealing, particularly for problems where small changes require re-derivation of the analytic derivative. This paper strikes an appropriate note of humility in the conclusion, giving limitations of Empirical Gateaux derivatives as a "completely general approach" (lines 329-335), namely the fact that (a) pathwise differentiability and (b) the second-order nature of the remainder must be verified analytically. However, these limitations do seem to undercut the general value of the approach. With that in mind, a few relevant questions Are there existing scenarios where the analytic form of the gateaux derivative is non-obvious, but where these conditions (pathwise differentiability, second-order remainder) can nonetheless be verified to hold? Or does verifying these conditions always require derivation of the analytic form? More broadly, are there scenarios where we can apply this approach (with appropriately conservative rates on ( ϵ , λ ) ) and have confidence in achieving O P ( n − 1 / 2 ) rates, without deriving the analytic derivative? E.g., would the constrained MDP with arbitrary linear constraints be such an example? If there exist some space of problems where the answer to these questions is "yes", then it would go a long way towards mitigating the impact of these limitations. Comments on soundness: Regarding technical soundness, I have a (hopefully minor) question or two on Lemma 1 In the display following line 563, it is claimed that the following holds due to Cauchy-Schwarz. I'm not sure I see the application of CS here: Is there another reason why we would expect the cross term 2 E [ ( μ ~ ϵ ( X ) − μ ~ ( X ) ) ( μ ~ ( X ) − μ ( X ) ) ] to be non-positive? E ( μ ~ ϵ ( X ) − μ ( X ) ) 2 ≤ E ( μ ~ ϵ ( X ) − μ ~ ( X ) ) 2 + E ( μ ~ ( X ) − μ ( X ) ) 2 In the display following line 562, the last inequality seems like a non-trivial jump, would you mind walking through the logic explicitly? Otherwise, the proofs seem correct to me. Note that I only read through the proofs for Section 3 in depth, and only skimmed the proofs of other relevant results (e.g., Propositions 3 and 4). While I am well-versed in the causal inference literature, I am not otherwise an expert on non-parametric / semi-parametric statistics, so I may have missed something. As an aside, it may be helpful to include a citation in the proof for some of the assumed results regarding kernels. [58] is referenced in the main text, referring to kernel smoothing more broadly, but it seems some of the prerequisite results could be cited more precisely (e.g., Lemma 25.1 of [58] appears to be a relevant result) Other Minor Feedback I consider the following points to be minor feedback re: presentation / notation / possible typos, and they did not meaningfully influence my score, and they do not require an explicit response from the authors (some are stated as questions only because I am unsure if they are typos). Suggestions on clarity: (Lines 15-19) This and some other sentences are a bit long and difficult to parse, and could perhaps be split into multiple sentences. (Line 71) Is the introduction of projections onto the semi-parametric model necessary, given Remark 1's statement that this work focuses on nonparametric models? CLvdL (Equation 2.2) seems to refers to Luedtke, Carone, and van der Laan (2015) as a reference for the equation on between lines 71-72 holding generally in a non-parametric model. Other typos / inconsistencies Example 1 uses E P for the outer expectation, but not for the inner expectation. Proposition 1 uses $\tilde{\mathbb{E}}{\tilde{P}\epsilon} i n o n e p l a c e , p e r h a p s t h e t i l d e o n \mathbb{E}$ was not intended? Line 65, what is the observation o ~ ? This is not referenced anywhere, I assume this is meant to be o ′ . Footnote 1, should the kernel be λ − d K ( u / λ ) instead of h − d K ( u / λ ) ? There is also a reference to an O ( h J ) error term on line 568 that should perhaps be O ( λ β ) ? Algorithm 1: Should it be P ~ on lines 3-4? Should it likewise be P ϵ , λ i instead of P ϵ i ? Lemma 1: Should e ~ ϵ ( X ) = p ~ ϵ ( A = 1 , X ) / p ~ ( X ) instead of the current formulation, which uses p ~ ϵ ( A = 1 ∣ X ) in the numerator? This would also make it consistent with usage in the proof (see e.g., line 561) Line 154, there seems to be a missing parenthesis Line 149 "analyses of from kernel density estimation" Assumption 1 (iv), should the equation refer to μ ~ ϵ or just μ ~ ? Additionally, should the product-rate condition be o p ( n − 1 ) as written or o p ( n − 1 / 2 ) ? Line 561, says to bound perturbed e one should "argue similarly", seemingly in reference to the (later) bound on the perturbed μ , perhaps the order was swapped. Line 562, following equation, third equality, missing a square on the final p ~ ( A = 1 , x ) term. Line 572, E [ Γ ( O ; e ϵ , μ ) ] should be E [ Γ ( O ; e ~ ϵ , μ ~ ) ] Line 637, indicator is missing an ϵ on the left-hand side Supplement, Section D.2, the reference is to citation [20], but I believe this should be citation [18] Undefined notation: Line 66, I did not see a definition of the function g ( u ) . I'm unsure if the dimension d was precisely defined prior to usage in Lemma 1, though it is fairly obvious from context. Eq.
Review Point: 7: I'm not sure if μ a is defined anywhere, aside from being the optimization variable, and similar for μ ∗ ( s , a ) in Equation 8. ν is used a few times in the proofs without being defined (end of equation starting on 576, end of equation starting on 562), presumably referring to a strong-overlap constant. I think the authors do a fine job of explaining limitations of the approach.
==================================================

Focused review:

No major concerns, added minor points in the Comments section. I thought this was a well-written paper overall with good contributions. The only thing I would have liked to see was similar few-shot experiments in non-classification tasks too, like some kind of structured prediction, QA, text generation. However, I agree that this is probably out of the scope of the current work. 
1. Start Section 4 by mentioning you are listing baseline models, it's a bit abrupt currently. 
2. Any reason you did not consider the other GLUE / SUPERGLUE tasks? 
3. I think you should rename "null hypothesis" to something else, since "null hypothesis" could have a different meaning and is not specific to label verbalization. 
4. The results section would be easier to read if there were bold-marked takeaways as paragraph headers, or at the beginning / end of each paragraph. 

Review Point: No major concerns, added minor points in the Comments section. I thought this was a well-written paper overall with good contributions. The only thing I would have liked to see was similar few-shot experiments in non-classification tasks too, like some kind of structured prediction, QA, text generation. However, I agree that this is probably out of the scope of the current work.
Review Point: 1. Start Section 4 by mentioning you are listing baseline models, it's a bit abrupt currently.
Review Point: 2. Any reason you did not consider the other GLUE / SUPERGLUE tasks?
Review Point: 3. I think you should rename "null hypothesis" to something else, since "null hypothesis" could have a different meaning and is not specific to label verbalization.
Review Point: 4. The results section would be easier to read if there were bold-marked takeaways as paragraph headers, or at the beginning / end of each paragraph.
==================================================

Focused review:

Weaknesses: 1. It is not clear if the dependence on
ε
is tight. Compared to the lower bound in central DP, there is a
1
/
ε
1.5
gap. 2. Time complexity is not discussed here.
This paper is theoretical and does not have a direct negative societal impact.


Review Point: 1. It is not clear if the dependence on ε is tight. Compared to the lower bound in central DP, there is a 1 / ε 1.5 gap.
Review Point: 2. Time complexity is not discussed here. This paper is theoretical and does not have a direct negative societal impact.
==================================================

Focused review:

Weaknesses
This work is presented as an alternative to standard empirical risk minimization (ERM) or the setup of mixup (Zhang et al, ICLR 2018), where ERM is replaced by vicinal risk minimization (VRM, Chappelle et al, NIPS 2000). What learning strategy is used here, in opposition to these two works, is never stated.
The experimental setup is not clear, which limits reproducibility and makes difficult to interpret the obtained results. What exactly is considered as in-distribution and what samples are OoD and how are they generated? (see detailed comments)
The proposed comparisons against ERM and mixup are not necessarily relevant. ERM is the standard approach to train a model and mixup proposes a data augmentation strategy to make a model more robust to adversarial samples. This work, instead focuses on the detection of OOD samples. As such, it should compare itself with methods addressing the same problem and not directly against ERM and mixup (table 1).
Detailed comments
Please acknowledge previous work on angle-based outlier detection [1], as it closely relates to this work.
The angular margin is estimated w.r.t the decision boundary (see eqs. 3 and 4). Therefore, there is an error in the illustration in Fig. 1a.
Differently from mixup, in this work \lambda does not follow a Beta distribution. Moreover, no details are provided on how it is chosen. Please comment.
If the label should not change, second line of Eq. 2 could be omitted. Otherwise, there is no guarantee that \hat{y} will have the same value. This would only hold when \lambda=1 or 0 and y_i=y_j=1 or 0.
Eq 5 implies that new data is being generated, i.e. data augmentation. Is this the case? What happends with the original samples? I suppose they are undesired since they have high variance.
Could the authors motivate why the angular margin needs to be used coupled with another metric (eq. 7) and not on its own?
The experimental setup is not clear. The paper misses to clearly establish what is an OoD sample/set on each of the experiments. In which way Gaussian and uniform noise are used for this purpose?
At inference time, when is a sample considered OOD?
The AUROC is not a good measure in OOD detection problems, since usually the majority class dominates. AUPRC should be favored. Interestingly, it is mixup that fairs best in that scenario, despite not being a method designed for OOD detection. Please comment
Minor
There are typos in the plots in figure 1b,c,d
Table 2 is unnecessary and may be omitted
References [1] H.-P. Kriegel, M. S hubert, and A. Zimek. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’08, pages 444–452. ACM, 2008.


Review Point: This work is presented as an alternative to standard empirical risk minimization (ERM) or the setup of mixup (Zhang et al, ICLR 2018), where ERM is replaced by vicinal risk minimization (VRM, Chappelle et al, NIPS 2000). What learning strategy is used here, in opposition to these two works, is never stated. The experimental setup is not clear, which limits reproducibility and makes difficult to interpret the obtained results. What exactly is considered as in-distribution and what samples are OoD and how are they generated? (see detailed comments) The proposed comparisons against ERM and mixup are not necessarily relevant. ERM is the standard approach to train a model and mixup proposes a data augmentation strategy to make a model more robust to adversarial samples. This work, instead focuses on the detection of OOD samples. As such, it should compare itself with methods addressing the same problem and not directly against ERM and mixup (table 1). Detailed comments Please acknowledge previous work on angle-based outlier detection [1], as it closely relates to this work. The angular margin is estimated w.r.t the decision boundary (see eqs. 3 and 4). Therefore, there is an error in the illustration in Fig. 1a. Differently from mixup, in this work \lambda does not follow a Beta distribution. Moreover, no details are provided on how it is chosen. Please comment. If the label should not change, second line of Eq.
Review Point: 2 could be omitted. Otherwise, there is no guarantee that \hat{y} will have the same value. This would only hold when \lambda=1 or 0 and y_i=y_j=1 or 0. Eq 5 implies that new data is being generated, i.e. data augmentation. Is this the case? What happends with the original samples? I suppose they are undesired since they have high variance. Could the authors motivate why the angular margin needs to be used coupled with another metric (eq. 7) and not on its own? The experimental setup is not clear. The paper misses to clearly establish what is an OoD sample/set on each of the experiments. In which way Gaussian and uniform noise are used for this purpose? At inference time, when is a sample considered OOD? The AUROC is not a good measure in OOD detection problems, since usually the majority class dominates. AUPRC should be favored. Interestingly, it is mixup that fairs best in that scenario, despite not being a method designed for OOD detection. Please comment Minor There are typos in the plots in figure 1b,c,d Table 2 is unnecessary and may be omitted References [1] H.-P. Kriegel, M. S hubert, and A. Zimek. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’08, pages 444–452. ACM, 2008.
==================================================

Focused review:

1. My major concern is that the improvement in l1 radius for Gaussian smoothing is somewhat limited. This paper also obtains better results for subspace perturbation, but it is not a common scenario in practice. I understand that it is important to consider the case where one model is used to defend against multiple types of threads, but this result might not be strong enough. 2. One thing that is not clear to me is how the l1 radius is calculated for Cohen et al. in Figure 2. In their paper, there is no mention of the l1 radius at all. Did the authors use bounds from other papers? It should be made clear. 3. Minor: the authors used "higher-order" in their paper but only discussed first-order in detail.

Review Point: 1. My major concern is that the improvement in l1 radius for Gaussian smoothing is somewhat limited. This paper also obtains better results for subspace perturbation, but it is not a common scenario in practice. I understand that it is important to consider the case where one model is used to defend against multiple types of threads, but this result might not be strong enough.
Review Point: 2. One thing that is not clear to me is how the l1 radius is calculated for Cohen et al. in Figure 2. In their paper, there is no mention of the l1 radius at all. Did the authors use bounds from other papers? It should be made clear.
Review Point: 3. Minor: the authors used "higher-order" in their paper but only discussed first-order in detail.
==================================================

Focused review:

1. The authors use exact inverses rather than learning approximate inverses with e.g. an autoencoder. Without some mechanism behind it, the use of exact inverses is just as biologically implausible as the use of transpose weights. I think it is important to show that the method is robust to the use of learned inverses. 2. The method hinges crucially on the orthogonality regularizer. Given that this paper's focus is biological plausibility, I think it is important to show or at least propose how this orthogonality condition could be imposed biologically. The authors "hypothesise that lateral inhibitory learning could aid in sufficient orthogonalization of synaptic weight matrices" -- but it is not clear to me that such a mechanism would enforce sufficient orthogonality.

Review Point: 1. The authors use exact inverses rather than learning approximate inverses with e.g. an autoencoder. Without some mechanism behind it, the use of exact inverses is just as biologically implausible as the use of transpose weights. I think it is important to show that the method is robust to the use of learned inverses.
Review Point: 2. The method hinges crucially on the orthogonality regularizer. Given that this paper's focus is biological plausibility, I think it is important to show or at least propose how this orthogonality condition could be imposed biologically. The authors "hypothesise that lateral inhibitory learning could aid in sufficient orthogonalization of synaptic weight matrices" -- but it is not clear to me that such a mechanism would enforce sufficient orthogonality.
==================================================

Focused review:

1) I appreciate the methods supported by theoretical grounds. However, considering that the effectiveness of the proposed framework is verified by empirical results, I believe that statements describing the experimental settings are still necessary. For example, in the application parts, the pseudo-code that describes how the proposed method is deployed in the existing model is desired. Currently, it’s not easy to understand how to use the proposed method to assist existing applications, like GANs. 2)The experimental results are not completely convincing. Although the results are from several different datasets, it seems that most of the images are down-scaled. For example, the images from ImageNet is down-scaled to 64*64, which are much easier than the high-resolution version of them. So I am not sure if the proposed method can also work well in high resolution cases, e.g. images from FFHQ dataset. Such modifications could also lead to different statistical results. It seems that some of the results of the compared methods are directly copied from the original paper, which means inconsistent experimental settings. I thus suspect the fairness of the comparisons. Please clarify this.

Review Point: 1) I appreciate the methods supported by theoretical grounds. However, considering that the effectiveness of the proposed framework is verified by empirical results, I believe that statements describing the experimental settings are still necessary. For example, in the application parts, the pseudo-code that describes how the proposed method is deployed in the existing model is desired. Currently, it’s not easy to understand how to use the proposed method to assist existing applications, like GANs.
Review Point: 2)The experimental results are not completely convincing. Although the results are from several different datasets, it seems that most of the images are down-scaled. For example, the images from ImageNet is down-scaled to 64*64, which are much easier than the high-resolution version of them. So I am not sure if the proposed method can also work well in high resolution cases, e.g. images from FFHQ dataset. Such modifications could also lead to different statistical results. It seems that some of the results of the compared methods are directly copied from the original paper, which means inconsistent experimental settings. I thus suspect the fairness of the comparisons. Please clarify this.
==================================================

Focused review:

Weakness
While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it.
The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained.
I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.).
While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean).
Question to authors
During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error.
I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample
m
has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level (
ϵ
m
,
k
) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable.
[1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188.
[2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594.
Small error: right above page 3 equation (3), should it be
ϵ
m
,
k
∗
?


Review Point: While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it. The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained. I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.). While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean). Question to authors During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error. I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample m has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level ( ϵ m , k ) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable. [1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows.
Review Point: 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188. [2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery.
Review Point: 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594. Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?
==================================================

Focused review:

UPDATE: I appreciate the author's clarification on the comparison with local DP, the notations \phi, \mu, and the additional experiments on examining the effect of loss balancing factor. As an additional suggestion, it would be better if authors can include more related work on the topic of learning robust representation, such as "Zhu, Sicheng, Xiao Zhang, and David Evans. Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization", "Garg, Shivam, et al. A spectral view of adversarially robust features.", "Pensia, Ankit, Varun Jog, and Po-Ling Loh. Extracting robust and accurate features via a robust information bottleneck" and so on. While I believe these studies are different from this paper, I think this topic is highly relevant to the topic of learning fair representation. 1. The performance highly depends on the choice of classifiers/parameters (e.g., loss balancing factors) and datasets, neither theoretical guarantee nor the guidance on the selection of classifiers/parameters is provided, which is one of the main reasons that limit my score. The performance is only compared with the BASE case. I wonder if authors can compare (at least empirically) with other fair representation learning methods. 2. The individual fairness studied in the paper — treating similar people similarly — is highly related to differential privacy and robustness ML. In essence, all are trying to guarantee that any small perturbation of training data cannot change the output significantly. Their relations have been studied extensively in the literature. It is thus NOT surprising to see the use of robust learning in providing certificates for individual fairness. Moreover, most of the methods in the paper are adopted from the existing work (e.g., the use of logical constraints, the formulation of min-max optimization, etc.). I am concerned that the work doesn’t have significant novelty and contribution. It would be helpful if authors can explain how their framework differs fundamentally from the existing work, especially those in the domain of robust and (local) differential privacy. 3. When the fairness constraint is imposed in machine learning, typically there should be a tradeoff between accuracy and fairness. I believe there is also such a tradeoff in the proposed framework, which is an important criterion but has not been discussed in the paper. However, as shown in the experiments, both accuracy and fairness can be improved simultaneously in many cases (e.g., HEALTH dataset). It seems that fairness can be attained "for free" without losing accuracy. I wonder if authors can explain why this can happen. I suggest authors conducting more experiments to examine the impact of loss balancing factor on the performance, which I believe plays a critical role in balancing fairness-accuracy tradeoff.

Review Point: UPDATE: I appreciate the author's clarification on the comparison with local DP, the notations \phi, \mu, and the additional experiments on examining the effect of loss balancing factor. As an additional suggestion, it would be better if authors can include more related work on the topic of learning robust representation, such as "Zhu, Sicheng, Xiao Zhang, and David Evans. Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization", "Garg, Shivam, et al. A spectral view of adversarially robust features.", "Pensia, Ankit, Varun Jog, and Po-Ling Loh. Extracting robust and accurate features via a robust information bottleneck" and so on. While I believe these studies are different from this paper, I think this topic is highly relevant to the topic of learning fair representation.
Review Point: 1. The performance highly depends on the choice of classifiers/parameters (e.g., loss balancing factors) and datasets, neither theoretical guarantee nor the guidance on the selection of classifiers/parameters is provided, which is one of the main reasons that limit my score. The performance is only compared with the BASE case. I wonder if authors can compare (at least empirically) with other fair representation learning methods.
Review Point: 2. The individual fairness studied in the paper — treating similar people similarly — is highly related to differential privacy and robustness ML. In essence, all are trying to guarantee that any small perturbation of training data cannot change the output significantly. Their relations have been studied extensively in the literature. It is thus NOT surprising to see the use of robust learning in providing certificates for individual fairness. Moreover, most of the methods in the paper are adopted from the existing work (e.g., the use of logical constraints, the formulation of min-max optimization, etc.). I am concerned that the work doesn’t have significant novelty and contribution. It would be helpful if authors can explain how their framework differs fundamentally from the existing work, especially those in the domain of robust and (local) differential privacy.
Review Point: 3. When the fairness constraint is imposed in machine learning, typically there should be a tradeoff between accuracy and fairness. I believe there is also such a tradeoff in the proposed framework, which is an important criterion but has not been discussed in the paper. However, as shown in the experiments, both accuracy and fairness can be improved simultaneously in many cases (e.g., HEALTH dataset). It seems that fairness can be attained "for free" without losing accuracy. I wonder if authors can explain why this can happen. I suggest authors conducting more experiments to examine the impact of loss balancing factor on the performance, which I believe plays a critical role in balancing fairness-accuracy tradeoff.
==================================================

Focused review:

Weaknesses:
The paper starts with grandiose claims of tackling "open-ended learning". However, open-ended learning involves learning to perform across diverse environments. But the definition of open-ended learning in this work seems restricted only to learning different skills in a given environment. In experiments, it is mostly restricted to goal conditioned environments and learning a goal conditioned policy.
"But where do goals come from? Almost always, they are sampled from a fixed distribution over a predefined goal space; i.e. they come from an engineer." There are numerous works where the goals are NOT generated from a fixed distribution (listed in the references below)
The previous statement makes us believe that in this work, the goals are not generated from a fixed distribution. However, a few paragraphs later, the authors note that "In this second challenge — the one we focus on — agents must learn to organize their own learning trajectories by prioritizing goals with the objective of maximizing long-term skill mastery." i.e, this work focuses on learning a goal conditioned policy from pre-defined goals.
"In social episodes, a social partner suggests a novel goal to the agent and decomposes it into two consecutive sub-goals: 1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities." The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments.
One of the contributions listed is: "an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion". This seems like a standard active learning setting and not a novel contribution.
References: [1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020 [2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017 [3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021 [4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021 [5] Automatic curriculum learning through value disagreement. Zhang et al, 2020 [6] Exploration via hindsight goal generation. Ren et al, 2019 [7] Automatic goal generation for reinforcement learning agents. Florensa et al, 2018


Review Point: The paper starts with grandiose claims of tackling "open-ended learning". However, open-ended learning involves learning to perform across diverse environments. But the definition of open-ended learning in this work seems restricted only to learning different skills in a given environment. In experiments, it is mostly restricted to goal conditioned environments and learning a goal conditioned policy. "But where do goals come from? Almost always, they are sampled from a fixed distribution over a predefined goal space; i.e. they come from an engineer." There are numerous works where the goals are NOT generated from a fixed distribution (listed in the references below) The previous statement makes us believe that in this work, the goals are not generated from a fixed distribution. However, a few paragraphs later, the authors note that "In this second challenge — the one we focus on — agents must learn to organize their own learning trajectories by prioritizing goals with the objective of maximizing long-term skill mastery." i.e, this work focuses on learning a goal conditioned policy from pre-defined goals. "In social episodes, a social partner suggests a novel goal to the agent and decomposes it into two consecutive sub-goals:
Review Point: 1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities." The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments. One of the contributions listed is: "an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion". This seems like a standard active learning setting and not a novel contribution. References: [1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020 [2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017 [3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021 [4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021 [5] Automatic curriculum learning through value disagreement. Zhang et al, 2020 [6] Exploration via hindsight goal generation. Ren et al, 2019 [7] Automatic goal generation for reinforcement learning agents. Florensa et al, 2018
==================================================

Focused review:

1. In Tab. 2, I find that the CC (Cluster Classification) outperforms the model in the paper with the same ResNet50 teacher net. It would be better to compare the results under the same setting, from same teacher net. 2. The results of '4.4 Experiments Comparing Self-Supervised Methods' seems not demonstrate the effectiveness of the model compression method itself, since the model is trained from the best self-supervised learning teacher model. When the teacher self-supervised learning is better, the student model should be better as well. As shown in Tab. 3, we can see MoCo outperforms other baselines, so it is not the contribution of the model itself but the teacher network. 3. Since it is a model compression method, I think it would be better to compare the model parameter and FLOPS between the student and teacher networks.

Review Point: 1. In Tab. 2, I find that the CC (Cluster Classification) outperforms the model in the paper with the same ResNet50 teacher net. It would be better to compare the results under the same setting, from same teacher net.
Review Point: 2. The results of '4.4 Experiments Comparing Self-Supervised Methods' seems not demonstrate the effectiveness of the model compression method itself, since the model is trained from the best self-supervised learning teacher model. When the teacher self-supervised learning is better, the student model should be better as well. As shown in Tab. 3, we can see MoCo outperforms other baselines, so it is not the contribution of the model itself but the teacher network.
Review Point: 3. Since it is a model compression method, I think it would be better to compare the model parameter and FLOPS between the student and teacher networks.
==================================================

Focused review:

weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly,
t
∈
R
while
y
,
f
(
θ
)
∈
R
n
, where
n
is the size of the training set, so that the operation
y
−
t
−
f
(
θ
)
is not well-defined. Moreover, even if
y
,
f
(
θ
)
∈
R
, the inequality
ψ
t
(
θ
)
≤
0
implies
l
(
θ
)
≤
t
2
/
2
, rather than
(
θ
)
≤
t
. Since, in general, the training problem will be defined for
y
∈
R
n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are "reasonable". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- "Issue 1: Is non-convex deep neural networks always better?": "is" should be "are". - "While some experiments show that certain local optima are equivalent and yield similar learning performance" -- this should be supported by a reference. - "However, the derivation of strong duality in the literature requires the planted model assumption" --- what do you mean by "planted model assumption"? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - "In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only
1
/
2
m
representation power." -- A statement like this only makes sense under some definition of "representation power". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - "Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient." --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - "... too many minimization pooling layers will damage the representation power of the neural network" --- why? Can the authors expand on this observation?
Section 5: - "... if we restrict the network output to be smaller than the network labels, i.e.,
f
(
θ
)
≤
y
" --- note that this observation requires
y
≥
0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): "conveyers" -> "converges".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - "It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly.
References
[1] Robbins, Herbert, and Sutton Monro. "A stochastic approximation method." The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. "Learning representations by back-propagating errors." nature 323.6088 (1986): 533-536.


Review Point: which are summarized in the following points: Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader. The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq.
Review Point: 4 and Eq.5. Firstly, t ∈ R while y , f ( θ ) ∈ R n , where n is the size of the training set, so that the operation y − t − f ( θ ) is not well-defined. Moreover, even if y , f ( θ ) ∈ R , the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2 , rather than ( θ ) ≤ t . Since, in general, the training problem will be defined for y ∈ R n , the derivations in the text should handle this general case. The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are "reasonable". The experiments as given have several notable flaws: Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship. Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods. Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well. Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution. Minor Comments Introduction:
Review Point: - The correct reference for SGD is the seminal paper by Robbins and Monro [1].
Review Point: - The correct reference for backpropagation is Rumelhart et al. [2] - "Issue 1: Is non-convex deep neural networks always better?": "is" should be "are".
Review Point: - "While some experiments show that certain local optima are equivalent and yield similar learning performance" -- this should be supported by a reference.
Review Point: - "However, the derivation of strong duality in the literature requires the planted model assumption" --- what do you mean by "planted model assumption"? The only necessary assumption for these works is that the shallow network is sufficiently wide. Section 4:
Review Point: - "In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m representation power." -- A statement like this only makes sense under some definition of "representation power". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power.
Review Point: - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere.
Review Point: - "Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient." --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex.
Review Point: - "... too many minimization pooling layers will damage the representation power of the neural network" --- why? Can the authors expand on this observation? Section 5:
Review Point: - "... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y " --- note that this observation requires y ≥ 0 , which does not appear to be explicitly mentioned.
Review Point: - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere. Figure 6:
Review Point: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text.
Review Point: - "It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References [1] Robbins, Herbert, and Sutton Monro. "A stochastic approximation method." The annals of mathematical statistics (1951): 400-407. [2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. "Learning representations by back-propagating errors." nature 323.6088 (1986): 533-536.
==================================================

Focused review:

- I understand the paper is not about ensembles but for ensembles for adversarial robustness, but I would like to see some papers that discuss ensembles and variants being cited in related works, since ensembles follow a very rich and established line of research. Namely [1,2,3,4,5]. - Eqn 3 and 4 are similar (to an extent) to the motivation of the proposed objective in [4]. I would like to see that being addressed, and how they are different. - NIT: in alg 1. i prefer \alpha being used instead of "lr" to denote learning rate - Resnet-20 is not a standard resnet variant to my knowledge. Is this similar to resnet-18? - Ensembles are very closely related to Bayesian NN, I would like to see a comment about - Whitebox accuracy performance is very similar to the baselines, but still good. - Fig 5. is also interesting, but is it possible to see other baselines with adv training as well on the same plot? - No error bars in any of the plots is very concerning to me. I much rather prefer seeing error bars to see beyond just the mean performance. [1] https://www.researchgate.net/publication/3191841_Neural_Network_Ensembles [2] https://arxiv.org/abs/1612.01474 [3] https://dl.acm.org/doi/abs/10.1145/1015330.1015385?casa_token=4-SXLz4nfOMAAAAA%3Av-L3CG89mjfUS4bqlPP8jq5evYx0i3uB99un8kwEocktvRSSkKcf4hRFA8shpqR_cHzt2tqbxRFN9A [4] https://arxiv.org/abs/2003.04514 [5] https://arxiv.org/abs/2001.10995

Review Point: - I understand the paper is not about ensembles but for ensembles for adversarial robustness, but I would like to see some papers that discuss ensembles and variants being cited in related works, since ensembles follow a very rich and established line of research. Namely [1,2,3,4,5].
Review Point: - Eqn 3 and 4 are similar (to an extent) to the motivation of the proposed objective in [4]. I would like to see that being addressed, and how they are different.
Review Point: - NIT: in alg 1. i prefer \alpha being used instead of "lr" to denote learning rate - Resnet-20 is not a standard resnet variant to my knowledge. Is this similar to resnet-18?
Review Point: - Ensembles are very closely related to Bayesian NN, I would like to see a comment about - Whitebox accuracy performance is very similar to the baselines, but still good.
Review Point: - Fig 5. is also interesting, but is it possible to see other baselines with adv training as well on the same plot?
Review Point: - No error bars in any of the plots is very concerning to me. I much rather prefer seeing error bars to see beyond just the mean performance. [1] https://www.researchgate.net/publication/3191841_Neural_Network_Ensembles [2] https://arxiv.org/abs/1612.01474 [3] https://dl.acm.org/doi/abs/10.1145/1015330.1015385?casa_token=4-SXLz4nfOMAAAAA%3Av-L3CG89mjfUS4bqlPP8jq5evYx0i3uB99un8kwEocktvRSSkKcf4hRFA8shpqR_cHzt2tqbxRFN9A [4] https://arxiv.org/abs/2003.04514 [5] https://arxiv.org/abs/2001.10995
==================================================

Focused review:

1. While the presented model can be posed as a transfer learning problem. This paper is more about concept drift. Therefore, the title Transfer learning via l1 Regularization is a bit too broad and can be misleading for some readers. 2. In the experiments on concept drift and "transfer learning", only Lasso is the only method studied and compared. However, Lasso is not considered as a state-of-the-art (SOTA) for concept drift and transfer learning. Lasso is designed for neither of these problem. On the other hand, there are many other methods for concept drift and transfer learning, with some discussed in the Related Work section but none is compared against in the experiment. 3. There are many existing works on concept drift (e.g. twitter activities, anomaly detection), as the authors have cited. However, this paper studies only synthetic concept drift problems. It is not clear whether the proposed solution can deal with concept drift in real data successfully. 4. Similar to the above, there are many transfer learning benchmarks and methods. However, this paper studies only a synthetic example without comparing to any transfer learning methods (as said above, Lasso is not designed for transfer learning). 5. The end of Sec. 4.2 states that Transfer Lasso showed the best accuracy in feature screening. However, previous works on Lasso screening are not cited or compared, e.g. Ren et al. "Safe feature screening for generalized LASSO." TPAMI 40.12 (2017): 2992-3006. 6. Section 4.3 follows the experiments in [17]. However, the presented results did not include [17] (and related works on the same data) in comparison. 7. Line 253: how was the data divided into 30 batches? 8. Line 258: What is the cause of such computational instability for binary features? What are the ways to mitigate this problem? 9. Figure 5-right: annotations on the colours used are missing. 10. Minor issues. Typo. Line 179: unchaing

Review Point: 1. While the presented model can be posed as a transfer learning problem. This paper is more about concept drift. Therefore, the title Transfer learning via l1 Regularization is a bit too broad and can be misleading for some readers.
Review Point: 2. In the experiments on concept drift and "transfer learning", only Lasso is the only method studied and compared. However, Lasso is not considered as a state-of-the-art (SOTA) for concept drift and transfer learning. Lasso is designed for neither of these problem. On the other hand, there are many other methods for concept drift and transfer learning, with some discussed in the Related Work section but none is compared against in the experiment.
Review Point: 3. There are many existing works on concept drift (e.g. twitter activities, anomaly detection), as the authors have cited. However, this paper studies only synthetic concept drift problems. It is not clear whether the proposed solution can deal with concept drift in real data successfully.
Review Point: 4. Similar to the above, there are many transfer learning benchmarks and methods. However, this paper studies only a synthetic example without comparing to any transfer learning methods (as said above, Lasso is not designed for transfer learning).
Review Point: 5. The end of Sec. 4.2 states that Transfer Lasso showed the best accuracy in feature screening. However, previous works on Lasso screening are not cited or compared, e.g. Ren et al. "Safe feature screening for generalized LASSO." TPAMI 40.12 (2017): 2992-3006.
Review Point: 6. Section 4.3 follows the experiments in [17]. However, the presented results did not include [17] (and related works on the same data) in comparison.
Review Point: 7. Line 253: how was the data divided into 30 batches?
Review Point: 8. Line 258: What is the cause of such computational instability for binary features? What are the ways to mitigate this problem?
Review Point: 9. Figure 5-right: annotations on the colours used are missing.
==================================================

Focused review:

-The paper suffers from several weaknesses, in particular with regards to its clarity. The method is described in a very confusing way, making it challenging to understand the building blocks and how they are arranged. Providing e.g. pseudo code describing the whole process, and more specifically the training process would greatly help comprehension. For example: the sampling process for S or where Q(z) computation fits in the whole training process. Figures also lack clarity, in particular Figure 1 which doesn't provide a clear distinction between the 2 compared strategies. - Authors miss an important category of related work relying on class similarity graphs (or knowledge bases) to generate unseen features. Such methods learn to generate features by implicitly exploiting similarities between classes. This contradicts the claim in the introduction l23 stating that there is no existing method leveraging shared concepts across classes. See relation to prior works for more details. - It is mentioned multiple times that the approach uniquely concatenates features instead of averaging them, yet use the classification strategy of DAZLE, which compares attribute specific features to each attribute. Therefore, the feature construction and sampling stage isn't clear. Is a feature vector constructed per attribute? What happens if an attribute is missing? Is S sampled such that all attributes are present?

Review Point: -The paper suffers from several weaknesses, in particular with regards to its clarity. The method is described in a very confusing way, making it challenging to understand the building blocks and how they are arranged. Providing e.g. pseudo code describing the whole process, and more specifically the training process would greatly help comprehension. For example: the sampling process for S or where Q(z) computation fits in the whole training process. Figures also lack clarity, in particular Figure 1 which doesn't provide a clear distinction between the 2 compared strategies.
Review Point: - Authors miss an important category of related work relying on class similarity graphs (or knowledge bases) to generate unseen features. Such methods learn to generate features by implicitly exploiting similarities between classes. This contradicts the claim in the introduction l23 stating that there is no existing method leveraging shared concepts across classes. See relation to prior works for more details.
Review Point: - It is mentioned multiple times that the approach uniquely concatenates features instead of averaging them, yet use the classification strategy of DAZLE, which compares attribute specific features to each attribute. Therefore, the feature construction and sampling stage isn't clear. Is a feature vector constructed per attribute? What happens if an attribute is missing? Is S sampled such that all attributes are present?
==================================================

Focused review:

1. The authors claimed that this is first to introduce an attention-aware feature aggregation module for video-text transformers. However, ViLBERT introduced co-attentional transformer layer for image-text modeling. ActBERT introduced Tangled Transformer for video-text modeling. The authors are suggested adding more discussions. 2. What is the value of $\lambda$ on different architectures and datasets? 3. It seems some of the losses have been studied in Zhang et al. [21]. Can the authors summarize the differences between this paper and [21]? 4. In Table 2, the results significantly outperform previous state-of-the-arts. However, when compared to HSE [21] in Table 1, COOT without AF, CMC, CoT outperforms HSE with a clear margin. How do the authors obtain these improvements?

Review Point: 1. The authors claimed that this is first to introduce an attention-aware feature aggregation module for video-text transformers. However, ViLBERT introduced co-attentional transformer layer for image-text modeling. ActBERT introduced Tangled Transformer for video-text modeling. The authors are suggested adding more discussions.
Review Point: 2. What is the value of $\lambda$ on different architectures and datasets?
Review Point: 3. It seems some of the losses have been studied in Zhang et al. [21]. Can the authors summarize the differences between this paper and [21]?
Review Point: 4. In Table 2, the results significantly outperform previous state-of-the-arts. However, when compared to HSE [21] in Table 1, COOT without AF, CMC, CoT outperforms HSE with a clear margin. How do the authors obtain these improvements?
==================================================

Focused review:

Weakness
The experimental results in Table 3 do not provide convincing evidence demonstrating that Tanh-like activation functions are virtually unaffected by non-IIDness.
As illustrated in Table 11, Tanh-like activation functions show a much more significant accuracy drop when the data heterogeneity across clients becomes larger (Dirichlet parameter
α
gets smaller). This trend becomes more significant when more clients participate per round. While these results support the claim that Tanh-like activation functions are robust to partial participation, but do not support the claim that Tanh-like functions are robust to the non-IIDness of each client.
The authors claim that the issue of client drift becomes severe since ReLU-based activation functions take fewer features than Tanh-like activation functions. However, Results in Figure 3 and Figure 4 rather show that Tanh-like activation functions have more client drift phenomenon: 1) low CKA similarity between clients, 2) large weight divergence between clients. Note that large weight divergence is known to be a cause of degenerated server accuracy and has been studied in many works [1, 2, 3]. Therefore, these experiments do not support the claim and should provide more convincing evidence.
Need more additional discussions and experiments:
Why are the accuracies of ResNet-based architectures and EfficientNet lower thanConvNet4?
200 rounds of communication seems too short for the convergence of the model. The authors should provide convergence plots or report the results after the model is trained with enough communication rounds.
There are no discussions about why large difference in accuracy among ReLU-based activation functions or among Tanh-like activation functions.
In Table 4, when
N
=
20
, why Leaky ReLU performs better with
R
=
0.3
than
R
=
0.4
?
ReLU-based activation functions and Tanh-like activation functions may have different optimal learning rates. Authors should compare models trained with each optimal local learning rate.
Reference
[1] S.P. Karimireddy et al., SCAFFOLD: Stochastic Controlled Averaging for Federated Learning, ICML, 2020.
[2] D.A.E. Acar et al., Federated Learning Based on Dynamic Regularization, ICLR, 2021.
[3] L. Gao et al., FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction, CVPR, 2022.


Review Point: The experimental results in Table 3 do not provide convincing evidence demonstrating that Tanh-like activation functions are virtually unaffected by non-IIDness. As illustrated in Table 11, Tanh-like activation functions show a much more significant accuracy drop when the data heterogeneity across clients becomes larger (Dirichlet parameter α gets smaller). This trend becomes more significant when more clients participate per round. While these results support the claim that Tanh-like activation functions are robust to partial participation, but do not support the claim that Tanh-like functions are robust to the non-IIDness of each client. The authors claim that the issue of client drift becomes severe since ReLU-based activation functions take fewer features than Tanh-like activation functions. However, Results in Figure 3 and Figure 4 rather show that Tanh-like activation functions have more client drift phenomenon:
Review Point: 1) low CKA similarity between clients, 2) large weight divergence between clients. Note that large weight divergence is known to be a cause of degenerated server accuracy and has been studied in many works [1, 2, 3]. Therefore, these experiments do not support the claim and should provide more convincing evidence. Need more additional discussions and experiments: Why are the accuracies of ResNet-based architectures and EfficientNet lower thanConvNet4?
Review Point: 200 rounds of communication seems too short for the convergence of the model. The authors should provide convergence plots or report the results after the model is trained with enough communication rounds. There are no discussions about why large difference in accuracy among ReLU-based activation functions or among Tanh-like activation functions. In Table 4, when N = 20 , why Leaky ReLU performs better with R = 0.3 than R = 0.4 ? ReLU-based activation functions and Tanh-like activation functions may have different optimal learning rates. Authors should compare models trained with each optimal local learning rate. Reference [1] S.P. Karimireddy et al., SCAFFOLD: Stochastic Controlled Averaging for Federated Learning, ICML, 2020. [2] D.A.E. Acar et al., Federated Learning Based on Dynamic Regularization, ICLR, 2021. [3] L. Gao et al., FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction, CVPR, 2022.
==================================================

Focused review:

- I wonder what is the total computational complexity compared to other methods (e.g., emerging convolutions). If I imagine the Woodbury flow working on a mobile device, the number of operations could cause a significant power demand. - Following on that, I am worried that the total computational complexity is much higher for other approaches. This could limit the usability of the proposed transformation.

Review Point: - I wonder what is the total computational complexity compared to other methods (e.g., emerging convolutions). If I imagine the Woodbury flow working on a mobile device, the number of operations could cause a significant power demand.
Review Point: - Following on that, I am worried that the total computational complexity is much higher for other approaches. This could limit the usability of the proposed transformation.
==================================================

Focused review:

- The established relationship between spectrally relaxed graph cuts and the reconstruction term of the ELBO (claim 4.1) feels fragile; please see below for additional details. I am also confused about the implications of claim 4.1 in a more general sense: does the proof rely on a specific distribution of the latent variables? If not, does that mean the statement is also true for VGAEs, which use a very similar generative model? - Section 2 provides a good summary about graph cuts, but more intuition about the ratio cut objective and additional details about spectral clustering would have been helpful; maybe they can be added to the supplementary material. Especially the role of Eq.(4) does not become immediately clear. - Section 3 heavily builds upon results in [12], but in many cases the necessary context is missing. For example, it is not possible to understand why Eq.(8) is Dirichlet-distributed or why Eqs.(9-10) describe a logistic normal distribution without consulting external references. I encourage the authors to update this section with additional details to make it more self-contained. - The quality of the Laplace approximation proposed in section 3 is unclear. Is it possible to make a theoretical statement in terms of an error bound? - The experimental evaluation could be improved in two ways: (1) The description of the datasets is very brief, simply referring to [8] is not enough; (2) The evaluation distinguishes x-AE and x-VAE, but it is nowhere mentioned what the difference is.

Review Point: - The established relationship between spectrally relaxed graph cuts and the reconstruction term of the ELBO (claim 4.1) feels fragile; please see below for additional details. I am also confused about the implications of claim 4.1 in a more general sense: does the proof rely on a specific distribution of the latent variables? If not, does that mean the statement is also true for VGAEs, which use a very similar generative model?
Review Point: - Section 2 provides a good summary about graph cuts, but more intuition about the ratio cut objective and additional details about spectral clustering would have been helpful; maybe they can be added to the supplementary material. Especially the role of Eq.(4) does not become immediately clear.
Review Point: - Section 3 heavily builds upon results in [12], but in many cases the necessary context is missing. For example, it is not possible to understand why Eq.(8) is Dirichlet-distributed or why Eqs.(9-10) describe a logistic normal distribution without consulting external references. I encourage the authors to update this section with additional details to make it more self-contained.
Review Point: - The quality of the Laplace approximation proposed in section 3 is unclear. Is it possible to make a theoretical statement in terms of an error bound?
Review Point: - The experimental evaluation could be improved in two ways: (1) The description of the datasets is very brief, simply referring to [8] is not enough; (2) The evaluation distinguishes x-AE and x-VAE, but it is nowhere mentioned what the difference is.
==================================================

Focused review:

weaknesses of different methods. - The paper is well-written. - The proposed method is clearly superior in all tasks.  Potential weaknesses - Motivation: how does modeling these time series vary or similar to modeling audio, images, or text? The paper mentions time series are more dynamic, involving temporal irregularities such as discontinuities and varying levels of rate of change in different regions, perhaps motivate a bit more why a fixed âsamplingâ scheme of "halving" the sequence addresses the challenges in the domain or other considerations for future work. - The method assumes fixed-length sequences.  Is this common in time series problems and datasets? - How much are the given anchor points (distributed across the sequence) giving the long-term structure versus the model learning them? - [masking] The masking scheme is not described.  The figures seem to show that the unmasked positions are those in order with how the divide-and-conquer scheme would proceed.  Does make it harder for certain baselines to cope with?  Is the masking per time step, that is all dimensions within a time step is masked if a tilmestep is masked? One of the baselines, the GRUI paper, uses random masking across both time steps and dimensions.  Given the divide-and-conquer scheme it might not be directly applicable, but would a variation of it be?  One question the reader might have is that if for example if the unmasked positions are less well distributed throughout the sequence, how would it affect the performance?   In line 235 âRobustness to percentage of missing valuesâ, the word âpercentageâ could be a bit misleading because itâs not a random percentage but portion in a pre-determined ordering. - [baseline] Would another baseline be generating auto-regressively but using the divide and conquer ordering, without adversarial loss? - Out of scope for this paper, perhaps question for future work, would some modification of the Transformer architectures be a competitive candidate for capturing these long-term dependencies?  Clarifying questions: - In the complexity section (line 139), why is only the backward hidden states updated when the step is imputed and not he forward states also. - From the description of the text (line 131 to 133), would line 10 (would include how g^{(r)} is obtained?) and 11 be swapped? - In Figure 2, would g_1 and g_2 be reversed since the subscript r refers to the resolution (size of gap)? 

Review Point: - The proposed method is clearly superior in all tasks. Potential weaknesses - Motivation: how does modeling these time series vary or similar to modeling audio, images, or text? The paper mentions time series are more dynamic, involving temporal irregularities such as discontinuities and varying levels of rate of change in different regions, perhaps motivate a bit more why a fixed âsamplingâ scheme of "halving" the sequence addresses the challenges in the domain or other considerations for future work.
Review Point: - The method assumes fixed-length sequences. Is this common in time series problems and datasets?
Review Point: - How much are the given anchor points (distributed across the sequence) giving the long-term structure versus the model learning them?
Review Point: - [masking] The masking scheme is not described. The figures seem to show that the unmasked positions are those in order with how the divide-and-conquer scheme would proceed. Does make it harder for certain baselines to cope with? Is the masking per time step, that is all dimensions within a time step is masked if a tilmestep is masked? One of the baselines, the GRUI paper, uses random masking across both time steps and dimensions. Given the divide-and-conquer scheme it might not be directly applicable, but would a variation of it be? One question the reader might have is that if for example if the unmasked positions are less well distributed throughout the sequence, how would it affect the performance? In line 235 âRobustness to percentage of missing valuesâ, the word âpercentageâ could be a bit misleading because itâs not a random percentage but portion in a pre-determined ordering.
Review Point: - [baseline] Would another baseline be generating auto-regressively but using the divide and conquer ordering, without adversarial loss?
Review Point: - Out of scope for this paper, perhaps question for future work, would some modification of the Transformer architectures be a competitive candidate for capturing these long-term dependencies? Clarifying questions:
Review Point: - In the complexity section (line 139), why is only the backward hidden states updated when the step is imputed and not he forward states also.
Review Point: - From the description of the text (line 131 to 133), would line 10 (would include how g^{(r)} is obtained?) and 11 be swapped?
Review Point: - In Figure 2, would g_1 and g_2 be reversed since the subscript r refers to the resolution (size of gap)?
==================================================

Focused review:

Weakness]: (1) There is a large gap in the proof of Theorem 1. (2) Missing discussion of the line of research using random matrix theory to understand the input-output Jacobian [1], which also consider the operator norm of the input-out Jacobian and draws a very similar conclusion, e.g., the squared operator norm must grow linearly with the number of layers; see eq (17) and follow up discussion in [1].

In what follows, I elaborate (1) and (2) since they are related.
The biggest issue I see in the proof is the equation above (A.1) on page 11. The authors mixed the calculation of finite width networks (on the left of the equation) and infinite width network calculation together (on the right). More precisely, the authors exchange the order of the two limits
lim
w
i
d
t
h
→
∞
and
lim sup
x
α
→
x
. The exchangeability of the two limits is questionable to me. In the order:
lim
w
i
d
t
h
→
∞
lim sup
x
α
→
, we need to handle a product of random matrices (if we compute the Jacobian). This is indeed a core contribution of [1], who uses free probability theory to compute the whole spectrum of the singular values of the Jacobian (assuming certain free independence of the matrices). If we swap the limits (we shouldn't do this without justification) to
lim sup
x
α
→
x
β
lim
w
i
d
t
h
→
, the problem itself is reduced to computing the derivative of the composed correlation map, which is much simpler. I think these two limits are not unchangeable in general. E.g., using the order
lim sup
x
α
→
x
β
lim
w
i
d
t
h
→
, both critical gaussian and orthogonal initialization give the same answer. But using the order
lim
w
i
d
t
h
→
∞
lim sup
x
α
→
, gaussian and orthogonal initialization can give different answers, see eq (17) vs (22) in [1].

Several Qs:
Q1:
How Theorem 1 leads to the four possible cases after it needs more discussion. In addition, what are the new insights quotient the existing ones from the order-chaotic analysis? It seems: the first case corresponds to the chaotic phase, the second case corresponds to the order phase. The third/fourth cases seem to be a finer analysis of the critical regime.
Q2: Remark1 the critical initialization. Several works have already identified the issue of the polynomial rate convergence of the correlation to 1 for Relu and smooth functions; see Proposition 1 in [2]; sec B.3. in [3].
Q3: I can't find places to explain the legends "upper bound", "largest found".
Q4: How does Thm1 imply eq (4.1)? Do you assume the operator norm is bounded by O(1)?
[1] Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice, https://arxiv.org/pdf/1711.04735.pdf [2] On the Impact of the Activation Function on Deep Neural Networks Training, https://arxiv.org/abs/1902.06853 [3] Disentangling Trainability and Generalization in Deep Neural Networks, https://arxiv.org/abs/1912.13053
Minors comments: 1.) What is the domain of the inputs? It seems they are lying in the same sphere, not mentioned in the paper.


Review Point: (1) There is a large gap in the proof of Theorem 1. (2) Missing discussion of the line of research using random matrix theory to understand the input-output Jacobian [1], which also consider the operator norm of the input-out Jacobian and draws a very similar conclusion, e.g., the squared operator norm must grow linearly with the number of layers; see eq (17) and follow up discussion in [1]. In what follows, I elaborate (1) and (2) since they are related. The biggest issue I see in the proof is the equation above (A.1) on page 11. The authors mixed the calculation of finite width networks (on the left of the equation) and infinite width network calculation together (on the right). More precisely, the authors exchange the order of the two limits lim w i d t h → ∞ and lim sup x α → x . The exchangeability of the two limits is questionable to me. In the order: lim w i d t h → ∞ lim sup x α → , we need to handle a product of random matrices (if we compute the Jacobian). This is indeed a core contribution of [1], who uses free probability theory to compute the whole spectrum of the singular values of the Jacobian (assuming certain free independence of the matrices). If we swap the limits (we shouldn't do this without justification) to lim sup x α → x β lim w i d t h → , the problem itself is reduced to computing the derivative of the composed correlation map, which is much simpler. I think these two limits are not unchangeable in general. E.g., using the order lim sup x α → x β lim w i d t h → , both critical gaussian and orthogonal initialization give the same answer. But using the order lim w i d t h → ∞ lim sup x α → , gaussian and orthogonal initialization can give different answers, see eq (17) vs (22) in [1]. Several Qs:
Review Point: Q1: How Theorem 1 leads to the four possible cases after it needs more discussion. In addition, what are the new insights quotient the existing ones from the order-chaotic analysis? It seems: the first case corresponds to the chaotic phase, the second case corresponds to the order phase. The third/fourth cases seem to be a finer analysis of the critical regime.
Review Point: Q2: Remark1 the critical initialization. Several works have already identified the issue of the polynomial rate convergence of the correlation to 1 for Relu and smooth functions; see Proposition 1 in [2]; sec B.3. in [3].
Review Point: Q3: I can't find places to explain the legends "upper bound", "largest found".
Review Point: Q4: How does Thm1 imply eq (4.1)? Do you assume the operator norm is bounded by O(1)? [1] Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice, https://arxiv.org/pdf/1711.04735.pdf [2] On the Impact of the Activation Function on Deep Neural Networks Training, https://arxiv.org/abs/1902.06853 [3] Disentangling Trainability and Generalization in Deep Neural Networks, https://arxiv.org/abs/1912.13053 Minors comments:
Review Point: 1.) What is the domain of the inputs? It seems they are lying in the same sphere, not mentioned in the paper.
==================================================

Focused review:

weaknesses of the method. Are there any caveats to practitioners due to some violation of the assumptions given in Appendix. B or for any other reasons?  Clarity: the writing is highly technical and rather dense, which I understand is necessary for some parts. However, I believe the manuscript would be readable to a broader audience if Sections 2 and 3 are augmented with more intuitive explanations of the motivations and their proposed methods. Many details of the derivations could be moved to the appendix and the resultant space could be used to highlight the key machinery which enabled efficient inference and to develop intuitions. Many terms and notations are not defined in text (as raised in "other comments" below).   Significance: the empirical results support the practical utility of the method. I am not sure, however, if the experiments on synthetic datasets, support the theoretical insights presented in the paper. I believe that the method is quite complex and recommend that the authors release the codes to maximize the impact.   Other comments: - line 47 - 48 "over-parametrization invariably overfits the data and results in worse performance": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].  - line 71: $\beta$ is never defined. It denotes the set of model parameters, right? - line 149-150 "the convergence to the asymptotically correct distribution allows ... obtain better point estimates in non-convex optimization.": this is only true if the assumptions in Appendix. B are satisfied, isn't it? How realistic are these assumptions in practice?  - line 1: MCMC is never defined: Markov Chain Monte Carlo  - line 77: typo "gxc lobal"=> "global" - eq.4: $\mathcal{N}$ and $\mathcal{L}$ are not defined. Normal and Laplace I suppose. You need to define them, please.  - Table 2: using the letter `a` to denote the difference in used models is confusing.  - too many acronyms are used.   References: [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. "A convergence theory for deep learning via over-parameterization." arXiv preprint arXiv:1811.03962 (2018).   ----------------------------------------------------------------------  I am grateful that the authors have addressed most of the concerns about the paper, and have updated my score accordingly. I would like to recommend for acceptance provided that the authors reflect the given clarifications in the paper. 

Review Point: of the method. Are there any caveats to practitioners due to some violation of the assumptions given in Appendix. B or for any other reasons? Clarity: the writing is highly technical and rather dense, which I understand is necessary for some parts. However, I believe the manuscript would be readable to a broader audience if Sections 2 and 3 are augmented with more intuitive explanations of the motivations and their proposed methods. Many details of the derivations could be moved to the appendix and the resultant space could be used to highlight the key machinery which enabled efficient inference and to develop intuitions. Many terms and notations are not defined in text (as raised in "other comments" below). Significance: the empirical results support the practical utility of the method. I am not sure, however, if the experiments on synthetic datasets, support the theoretical insights presented in the paper. I believe that the method is quite complex and recommend that the authors release the codes to maximize the impact. Other comments:
Review Point: - line 47 - 48 "over-parametrization invariably overfits the data and results in worse performance": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].
Review Point: - line 71: $\beta$ is never defined. It denotes the set of model parameters, right?
Review Point: - line 149-150 "the convergence to the asymptotically correct distribution allows ... obtain better point estimates in non-convex optimization.": this is only true if the assumptions in Appendix. B are satisfied, isn't it? How realistic are these assumptions in practice?
Review Point: - line 1: MCMC is never defined: Markov Chain Monte Carlo - line 77: typo "gxc lobal"=> "global" - eq.4: $\mathcal{N}$ and $\mathcal{L}$ are not defined. Normal and Laplace I suppose. You need to define them, please.
Review Point: - Table 2: using the letter `a` to denote the difference in used models is confusing.
Review Point: - too many acronyms are used. References: [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. "A convergence theory for deep learning via over-parameterization." arXiv preprint arXiv:1811.03962 (2018). ---------------------------------------------------------------------- I am grateful that the authors have addressed most of the concerns about the paper, and have updated my score accordingly. I would like to recommend for acceptance provided that the authors reflect the given clarifications in the paper.
==================================================

Focused review:

The main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section). 
- I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.
- Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention "no significant" difference, which instead is referring to the statistical significance. I would suggest to revise this part.
- It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?
- On which data is fine-tuned the model for the "Knowledge Distillation" - Please, add an intro paragraph to section 4.
- The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?
- In line 303 the authors mention "sentence transformers". Why are the authors mentioning this? Is it possible to add a citation?
- There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.
- I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones? 
- 

Review Point: The main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section).
Review Point: - I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.
Review Point: - Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention "no significant" difference, which instead is referring to the statistical significance. I would suggest to revise this part.
Review Point: - It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?
Review Point: - On which data is fine-tuned the model for the "Knowledge Distillation" - Please, add an intro paragraph to section 4.
Review Point: - The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?
Review Point: - In line 303 the authors mention "sentence transformers". Why are the authors mentioning this? Is it possible to add a citation?
Review Point: - There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.
Review Point: - I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones?
==================================================

Focused review:

The following four points are considered to be weaknesses. 1. The purpose of the study and the claim of the study is difficult to understand even after reading the abstraction and introduction. For example, the word "deep dynamic models" is not well understood. Also, there is little background explanation of why the authors focus on the discrete-time dynamics model. 2. There is a lack of discussion about the results of the experiments. Please state exactly what you are trying to argue from the results of each experiment. 3. There is a lack of comparison with the methods of previous studies [30]. Isn't it possible to model a stochastic discrete system as a continuous deterministic system and get the same level of performance? 4. The details of the training model and settings, such as the number of samples used for training, are not described. Also, no program is attached. Therefore, reproducibility is not guaranteed.

Review Point: 1. The purpose of the study and the claim of the study is difficult to understand even after reading the abstraction and introduction. For example, the word "deep dynamic models" is not well understood. Also, there is little background explanation of why the authors focus on the discrete-time dynamics model.
Review Point: 2. There is a lack of discussion about the results of the experiments. Please state exactly what you are trying to argue from the results of each experiment.
Review Point: 3. There is a lack of comparison with the methods of previous studies [30]. Isn't it possible to model a stochastic discrete system as a continuous deterministic system and get the same level of performance?
Review Point: 4. The details of the training model and settings, such as the number of samples used for training, are not described. Also, no program is attached. Therefore, reproducibility is not guaranteed.
==================================================

Focused review:

Weaknesses:
The authors distill a ViT-B/16 into a RN50. While the former takes more FLOPs than the later, ViT-B/32 is cheaper than RN50, and the CLIP version of that architecture outperforms both the author's RN50 and the CLIP RN50 model. While this doesn't totally undercut the presented results, it's a bit strange to me why the authors chose the particular distillation pair that they did --- it seems like ViT-B/32 (or even EfficientNet or MobileNet) would have been a good choice for student, and perhaps L/14-336px CLIP as the teacher would have made a more compelling setup.
The ablations suggest that the cosine similarity loss isn't really required: the best zero-shot imagenet performance is actually achieved only with the cross-modal loss, and the other datasets seem to be within a close margin. This isn't really a /negative/, per-say, but a bit contrary to the story told.
It would have been nice to see linear probe results for the other datasets like Pets37 as well.
Even assuming that RN50 is the best choice of student, the empirical results are somewhat unimpressive. Specifically, I believe that the authors choose anchor points to be task-specific prompts and the author's model is domain adapted to (unlabelled) dataset-specific images, i.e., there's reason to believe that the author's method is specifically tuned to the tasks described. RN50-CLIP, which is not tuned to the specific tasks described, achieves only slightly worse imagenet linear probe accuracy: 73.3 vs 74.8.
The zero shot results in table 3 are interesting, but a bit incomplete. The ViT-B/32-CLIP model smaller than the author's distilled RN50 model achieves only slightly worse performance on these tasks, but it isn't presented. Again, this observation doesn't invalidate the author's results, it's just that one really needs to buy that RN50 specifically is an interesting student model.
Overall, the authors clearly "win" on this particular architecture by a few accuracy points on linear probe imagenet, but it's not clear that this is really the best architecture to distill to (particularly because ViT-B/32 is smaller and there's already a CLIP model for that). To the author's credit, they recognize this on L288, but I would have preferred to see that the author's method generalizes to other teacher/student models, instead of just this one combination.
Overall, the authors have a promising core result: it's possible to distill the knowledge of a CLIP model into a smaller set of weights. And, the innovation that enables this distillation (that image-text distances should be preserved in addition to image-image distances) is novel. However, the empirical results are slightly underwhelming: the authors model, by virtue of the selection of the anchor points and distillation over unlabelled images from the corpora of interest, is tuned to this particular set of datasets (vs. CLIP that isn't). Their performance improvement over CLIP is somewhat small in magnitude (73.3 vs 74.8). I would have liked to have seen: 1) distillation to significantly more efficient architectures vs. RN50 (ViT-B/32 CLIP is already smaller) and with better teachers than ViT-B/16; and 2) linear probe evaluations on unseen tasks, where the images haven't been seen by the distillation model at training time.
Presentation fixes:
It should be specified that Table 3 results are all zero shot.
L71: week --> weak
The broader impact statement is a bit hollow -- I would have appreciated a fuller discussion of embedded systems and/or low resource computation
Table 4's caption has an incomplete sentence.
The authors mainly focus on making higher performance, lower-resource versions of existing CLIP models. Given that CLIP's negative impacts have been discussed in the original work and follow ups, I don't think significant additional discussion is needed here.
I did think that the "Broader impacts" section was a bit short: can more information about FLOPs/energy use/on-device computation be discussed?


Review Point: The authors distill a ViT-B/16 into a RN50. While the former takes more FLOPs than the later, ViT-B/32 is cheaper than RN50, and the CLIP version of that architecture outperforms both the author's RN50 and the CLIP RN50 model. While this doesn't totally undercut the presented results, it's a bit strange to me why the authors chose the particular distillation pair that they did --- it seems like ViT-B/32 (or even EfficientNet or MobileNet) would have been a good choice for student, and perhaps L/14-336px CLIP as the teacher would have made a more compelling setup. The ablations suggest that the cosine similarity loss isn't really required: the best zero-shot imagenet performance is actually achieved only with the cross-modal loss, and the other datasets seem to be within a close margin. This isn't really a /negative/, per-say, but a bit contrary to the story told. It would have been nice to see linear probe results for the other datasets like Pets37 as well. Even assuming that RN50 is the best choice of student, the empirical results are somewhat unimpressive. Specifically, I believe that the authors choose anchor points to be task-specific prompts and the author's model is domain adapted to (unlabelled) dataset-specific images, i.e., there's reason to believe that the author's method is specifically tuned to the tasks described. RN50-CLIP, which is not tuned to the specific tasks described, achieves only slightly worse imagenet linear probe accuracy: 73.3 vs 74.8. The zero shot results in table 3 are interesting, but a bit incomplete. The ViT-B/32-CLIP model smaller than the author's distilled RN50 model achieves only slightly worse performance on these tasks, but it isn't presented. Again, this observation doesn't invalidate the author's results, it's just that one really needs to buy that RN50 specifically is an interesting student model. Overall, the authors clearly "win" on this particular architecture by a few accuracy points on linear probe imagenet, but it's not clear that this is really the best architecture to distill to (particularly because ViT-B/32 is smaller and there's already a CLIP model for that). To the author's credit, they recognize this on L288, but I would have preferred to see that the author's method generalizes to other teacher/student models, instead of just this one combination. Overall, the authors have a promising core result: it's possible to distill the knowledge of a CLIP model into a smaller set of weights. And, the innovation that enables this distillation (that image-text distances should be preserved in addition to image-image distances) is novel. However, the empirical results are slightly underwhelming: the authors model, by virtue of the selection of the anchor points and distillation over unlabelled images from the corpora of interest, is tuned to this particular set of datasets (vs. CLIP that isn't). Their performance improvement over CLIP is somewhat small in magnitude (73.3 vs 74.8). I would have liked to have seen:
Review Point: 1) distillation to significantly more efficient architectures vs. RN50 (ViT-B/32 CLIP is already smaller) and with better teachers than ViT-B/16; and 2) linear probe evaluations on unseen tasks, where the images haven't been seen by the distillation model at training time. Presentation fixes: It should be specified that Table 3 results are all zero shot.
Review Point: L71: week --> weak The broader impact statement is a bit hollow -- I would have appreciated a fuller discussion of embedded systems and/or low resource computation Table 4's caption has an incomplete sentence. The authors mainly focus on making higher performance, lower-resource versions of existing CLIP models. Given that CLIP's negative impacts have been discussed in the original work and follow ups, I don't think significant additional discussion is needed here. I did think that the "Broader impacts" section was a bit short: can more information about FLOPs/energy use/on-device computation be discussed?
==================================================

Focused review:

Weaknesses:  The attention vector is simply the summation of two attention vectors of each part. Maybe the attention vector could be calculated in a more appropriate approach. For the supervised attention mechanism, two strategies are proposed. 
Both of them are quite straightforward. Some more complicated strategies can work better and can be tried.
- General Discussion:  Although there are some places that can be improved, this paper proposed a quite effective framework, and the performance is good. The experiment is solid. It can be considered to be accepted. 

Review Point: The attention vector is simply the summation of two attention vectors of each part. Maybe the attention vector could be calculated in a more appropriate approach. For the supervised attention mechanism, two strategies are proposed. Both of them are quite straightforward. Some more complicated strategies can work better and can be tried.
Review Point: - General Discussion: Although there are some places that can be improved, this paper proposed a quite effective framework, and the performance is good. The experiment is solid. It can be considered to be accepted.
==================================================

Focused review:

1. As the paper claimed, the uncertainty is modelled based on individual reward function and transition probability. It seems ref [12] has the same definition of reward and transition function (eq.3 in ref[12]), but they do not use the description like ‘nature’ agent and they use minmax to formalize the objective. This paper looks like another application of [12] and use a different optimizing trick. As such, it is critical to provide detailed discussion about the difference between the proposed method and ref [12]. 2. Introducing MADDPG (section 3.2) to optimize the hard objective is certainly of interest. The paper gives the gradient of policy. However, it might be better to give the theoretical guarantee of the discrepancy between the learned policy and true policy as well as the equilibrium guarantee by using such an optimization framework (MADDPG). 3. It might be better to report standard errors in tables. Experiment part lacks details (like the number of agents etc.) and experimental environments are too simple to support the effectiveness of your method. It is better to extend the experiment to more complex environments with multiple kinds of uncertainty.

Review Point: 1. As the paper claimed, the uncertainty is modelled based on individual reward function and transition probability. It seems ref [12] has the same definition of reward and transition function (eq.3 in ref[12]), but they do not use the description like ‘nature’ agent and they use minmax to formalize the objective. This paper looks like another application of [12] and use a different optimizing trick. As such, it is critical to provide detailed discussion about the difference between the proposed method and ref [12].
Review Point: 2. Introducing MADDPG (section 3.2) to optimize the hard objective is certainly of interest. The paper gives the gradient of policy. However, it might be better to give the theoretical guarantee of the discrepancy between the learned policy and true policy as well as the equilibrium guarantee by using such an optimization framework (MADDPG).
Review Point: 3. It might be better to report standard errors in tables. Experiment part lacks details (like the number of agents etc.) and experimental environments are too simple to support the effectiveness of your method. It is better to extend the experiment to more complex environments with multiple kinds of uncertainty.
==================================================

Focused review:

1. A major concern that I have is, the authors consider only shifts in annotations as the noise. However, real-world annotations include other types of noises like missing annotations or duplicate annotations. The authors do not consider this in their discussion. From the outset, it seems that their current method cannot accommodate these additional noises. From this perspective, I would say that the paper is incomplete in modeling different types of annotation noises. 2. It is not clear why the authors approximate pdf phi and Psi with Gaussian distribution. 3. It is not clear why eta_ri term is non-central chi-squared distribution. 4. As far as I understand, small shifts in annotations will not affect performance much, since neural networks can be robust if receptive size of the network is large enough. Can the authors discuss this more in detail. 5. The proposed method seems to be too specific to the counting problem. Can this method be extended to other problems in vision like object detection.

Review Point: 1. A major concern that I have is, the authors consider only shifts in annotations as the noise. However, real-world annotations include other types of noises like missing annotations or duplicate annotations. The authors do not consider this in their discussion. From the outset, it seems that their current method cannot accommodate these additional noises. From this perspective, I would say that the paper is incomplete in modeling different types of annotation noises.
Review Point: 2. It is not clear why the authors approximate pdf phi and Psi with Gaussian distribution.
Review Point: 3. It is not clear why eta_ri term is non-central chi-squared distribution.
Review Point: 4. As far as I understand, small shifts in annotations will not affect performance much, since neural networks can be robust if receptive size of the network is large enough. Can the authors discuss this more in detail.
Review Point: 5. The proposed method seems to be too specific to the counting problem. Can this method be extended to other problems in vision like object detection.
==================================================

Focused review:

Weakness & Comment  The proposed model is somewhat artificial. In particular, all the responses are assumed to be correct, whereas noisy response is common in practice. Hence, it limits to connect the considering problem to a motivating example in practice. In addition, there is a line of works e.g., [BA 2008, 13], that study the query complexity even under noisy responses (which lead to interesting applications) although there is no privacy concern. However, there is no justification or motivation on the noiseless response.   - Minor comments  Iâm not sure that the notion of the random seed Y is necessary.  Line 34: identity function > indicator function Line 44: $R^i := \emptyset$ > $R^1 := \emptyset$ Line 46: âguessâ > âguessâ Line 61: any an adversary > any adversary Line 163: $L \log (1/\epsilon)- L (\log L -1)$ > $L \log (1/\epsilon)- L (\log L -1) - 1$ Eq. 5.7: ${\cal E}_{i, j}$ is used without defining it.  -- [BA 2008]: Ben-Or, Michael, and Avinatan Hassidim. "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)." Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on. IEEE, 2008. 

Review Point: & Comment The proposed model is somewhat artificial. In particular, all the responses are assumed to be correct, whereas noisy response is common in practice. Hence, it limits to connect the considering problem to a motivating example in practice. In addition, there is a line of works e.g., [BA 2008, 13], that study the query complexity even under noisy responses (which lead to interesting applications) although there is no privacy concern. However, there is no justification or motivation on the noiseless response.
Review Point: - Minor comments Iâm not sure that the notion of the random seed Y is necessary. Line 34: identity function > indicator function Line 44: $R^i := \emptyset$ > $R^1 := \emptyset$ Line 46: âguessâ > âguessâ Line 61: any an adversary > any adversary Line 163: $L \log (1/\epsilon)- L (\log L -1)$ > $L \log (1/\epsilon)- L (\log L -1) - 1$ Eq. 5.7: ${\cal E}_{i, j}$ is used without defining it. -- [BA 2008]: Ben-Or, Michael, and Avinatan Hassidim. "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)." Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on. IEEE, 2008.
==================================================

Focused review:

Weaknesses ***
The presentation of Section 3 is poor, making it hard to follow. In addition, some expressions and equations are not rigorous. For instance, how to obtain the cluster center of the ID samples in Equation 2? The loss described in Equation 11 only enlarges the gap between positive and negative samples, which should not be called a contrastive learning loss.
Based on my understanding, the major contribution of this paper is to introduce prompt learning techniques into OSSL. However, an important ablation is missing, where the visual encoder is fine-tuned instead of prompting and the rest parts of the framework remain the same. This could better demonstrate the advantages of prompting and supports the central claim of this paper.
Since prompt learning generally needs a strong pre-trained model, what is the performance of the proposed method when the labeled data is much less than the unlabeled ones?
It is not clear to me what is the difference between the proposed binary classifier updating method and the one suggested in [1]. It would be good if the authors could elaborate on this.
The claimed low computational cost is not well supported. 1) Although they deal with unlabeled data by only fine-tuning the prompt, they have to pre-train the whole network on the labeled data first. In contrast, previous works train the model using both labeled & unlabeled data only once (consequently the #Paras in Table 1 is unfair). 2) There is no other strong evidence in the paper to demonstrate that the total computational cost of OpenPrompt is lower than in previous works.
[1] Exploiting a joint embedding space for generalized zero-shot semantic segmentation, ICCV 2021


Review Point: *** The presentation of Section 3 is poor, making it hard to follow. In addition, some expressions and equations are not rigorous. For instance, how to obtain the cluster center of the ID samples in Equation 2? The loss described in Equation 11 only enlarges the gap between positive and negative samples, which should not be called a contrastive learning loss. Based on my understanding, the major contribution of this paper is to introduce prompt learning techniques into OSSL. However, an important ablation is missing, where the visual encoder is fine-tuned instead of prompting and the rest parts of the framework remain the same. This could better demonstrate the advantages of prompting and supports the central claim of this paper. Since prompt learning generally needs a strong pre-trained model, what is the performance of the proposed method when the labeled data is much less than the unlabeled ones? It is not clear to me what is the difference between the proposed binary classifier updating method and the one suggested in [1]. It would be good if the authors could elaborate on this. The claimed low computational cost is not well supported.
Review Point: 1) Although they deal with unlabeled data by only fine-tuning the prompt, they have to pre-train the whole network on the labeled data first. In contrast, previous works train the model using both labeled & unlabeled data only once (consequently the #Paras in Table 1 is unfair).
Review Point: 2) There is no other strong evidence in the paper to demonstrate that the total computational cost of OpenPrompt is lower than in previous works. [1] Exploiting a joint embedding space for generalized zero-shot semantic segmentation, ICCV 2021
==================================================

Focused review:

Weaknesses
I do not understand the emphasis on the method being black box. Not because it is not accurate, but because it doesn’t seem to play that big of a role. The authors mention that for white-box distillation methods, one can use intermediate features. But it has been shown that the original distillation procedure, which is done on class logits, typically outperforms all other baselines. This was shown in CRD [1], which has been cited by the authors. The accurate tone of the paper should be about performing a more effective knowledge distillation.
Even after the effectiveness of the proposed tricks, the paper pretty much reads like that; a collection of tricks which empirically improve the performance of the student. I would be fine with a simple fix to an existing distillation procedure which then improves the performance, as long as it was telling us something about the previous procedure; maybe there was an issue with the method which people had missed and this new way of doing the distillation fixes that.
The batch-level alignment loss is a popular technique to distill knowledge. But the authors have not acknowledged that. In particular, in section 3.2.1, the authors should mention that the loss that they are explaining is already introduced in the Relational Knowledge Distillation [2], a paper that they have already cited in the related work section but not mentioned when Eq. 5 and 6 are being used.
References
[1] Contrastive Representation Distillation. Tian et al. ICLR 2020 [2] Relational Knowledge Distillation. Park et al. CVPR 2019.


Review Point: I do not understand the emphasis on the method being black box. Not because it is not accurate, but because it doesn’t seem to play that big of a role. The authors mention that for white-box distillation methods, one can use intermediate features. But it has been shown that the original distillation procedure, which is done on class logits, typically outperforms all other baselines. This was shown in CRD [1], which has been cited by the authors. The accurate tone of the paper should be about performing a more effective knowledge distillation. Even after the effectiveness of the proposed tricks, the paper pretty much reads like that; a collection of tricks which empirically improve the performance of the student. I would be fine with a simple fix to an existing distillation procedure which then improves the performance, as long as it was telling us something about the previous procedure; maybe there was an issue with the method which people had missed and this new way of doing the distillation fixes that. The batch-level alignment loss is a popular technique to distill knowledge. But the authors have not acknowledged that. In particular, in section 3.2.1, the authors should mention that the loss that they are explaining is already introduced in the Relational Knowledge Distillation [2], a paper that they have already cited in the related work section but not mentioned when Eq.
Review Point: 5 and 6 are being used. References [1] Contrastive Representation Distillation. Tian et al. ICLR 2020 [2] Relational Knowledge Distillation. Park et al. CVPR 2019.
==================================================

Focused review:

Weakness:
slim contribution: addition of one parameter to an existing network
only tested out on a single prototype network from 2017, which hasn't been SOTA for a long time. No comparisons to more recent prototype baselines [1,2] or approaches such as meta-learning [3], or self-supervision [4]. This lack of comparison makes the claims much less substantial. Big prototypes improve on vanilla prototypes, but do they make Euclidean prototypes competitive wrt modern methods?
While the writing is generally clear, there are many mistakes and at times clumsy formulations getting in the way of clarity and rigor
The way the prototypes are initialized (3.3) is not very clear to me at all. What is the benefit of this approach instead of simply computing the average center and radius? And if it is a contribution, why isn't it evaluated?
Suggestions:
The authors could turn the extreme simplicity of their approach into a strength if they adapted their method to a variety of prototype-based algorithms, including more recent and better-performing ones. Spheres exist in hyperbolic space, for example.
Details:
abstract:
dense vectors = prototype?
the radius of the sphere is not constant if I understand it properly
what is an atactic manifold?
the authors are constantly the world "metric" to mean "distance in a metric space." This makes for some confusing sentences introduction:
what is the meaning of derivative in "derivative prototype-based methods"?
how are Big prototypes easier to "model"?
Fig1:
the fact that the embeddings change as well makes the figures hard to understand.
are the center and radius really learned separately and not end-to-end? This is not my understanding from the rest of the paper Related work
type of NMS
"find the true location of the prototypes of the entire dataset" I don't think there is such a thing as a true embedding for a prototype.
3 Methods
"One big prototype is represented by two terms" -> parameters
why limit yourself to the case in which the data is a vector of dimension L? it could be an image, for example
"We now introduce big prototypes, which are a series of hyperspheres " -> a set of hyperspheres
3.3 is quite confusing to read. For example, z^j_n is never defined.
Equation 5: \tilde{r} is indexed by n, and the sum at the denominator is not indexed. It makes it look like the \tilde{r} could be canceled out, when it is obviously not the case
Algo1:
Init: And why not just average over Dn?
Table1 P,R,F never defined. "spaticularly"
[1] Gao et al. Hybrid attention-based prototypical networks for noisy few-shot relation classification. AAAI, 2019.
[2] Mettes et al. Hyperspherical prototype networks. NeurIPS 2019
[3] Rusu et al. Meta-learning with latent embedding optimization. ICLR 2019
[4] Gidaris et al. Dynamic few-shot visual learning without forgetting CVPR 2018


Review Point: slim contribution: addition of one parameter to an existing network only tested out on a single prototype network from 2017, which hasn't been SOTA for a long time. No comparisons to more recent prototype baselines [1,2] or approaches such as meta-learning [3], or self-supervision [4]. This lack of comparison makes the claims much less substantial. Big prototypes improve on vanilla prototypes, but do they make Euclidean prototypes competitive wrt modern methods? While the writing is generally clear, there are many mistakes and at times clumsy formulations getting in the way of clarity and rigor The way the prototypes are initialized (3.3) is not very clear to me at all. What is the benefit of this approach instead of simply computing the average center and radius? And if it is a contribution, why isn't it evaluated? Suggestions: The authors could turn the extreme simplicity of their approach into a strength if they adapted their method to a variety of prototype-based algorithms, including more recent and better-performing ones. Spheres exist in hyperbolic space, for example. Details: abstract: dense vectors = prototype? the radius of the sphere is not constant if I understand it properly what is an atactic manifold? the authors are constantly the world "metric" to mean "distance in a metric space." This makes for some confusing sentences introduction: what is the meaning of derivative in "derivative prototype-based methods"? how are Big prototypes easier to "model"?
Review Point: Fig1: the fact that the embeddings change as well makes the figures hard to understand. are the center and radius really learned separately and not end-to-end? This is not my understanding from the rest of the paper Related work type of NMS "find the true location of the prototypes of the entire dataset" I don't think there is such a thing as a true embedding for a prototype.
Review Point: 3 Methods "One big prototype is represented by two terms" -> parameters why limit yourself to the case in which the data is a vector of dimension L? it could be an image, for example "We now introduce big prototypes, which are a series of hyperspheres " -> a set of hyperspheres 3.3 is quite confusing to read. For example, z^j_n is never defined. Equation 5: \tilde{r} is indexed by n, and the sum at the denominator is not indexed. It makes it look like the \tilde{r} could be canceled out, when it is obviously not the case Algo1: Init: And why not just average over Dn? Table1 P,R,F never defined. "spaticularly" [1] Gao et al. Hybrid attention-based prototypical networks for noisy few-shot relation classification. AAAI, 2019. [2] Mettes et al. Hyperspherical prototype networks. NeurIPS 2019 [3] Rusu et al. Meta-learning with latent embedding optimization. ICLR 2019 [4] Gidaris et al. Dynamic few-shot visual learning without forgetting CVPR 2018
==================================================

Focused review:

1. The proposed method is based on a pre-defined causal graph, which has limitations if the causal graph is unavailable. In the experimental results sections, the authors only showed the results with the graph constructed by the PC algorithm. It is not clear how the way of graph construction affects the final results. 2. The optimization details for the objective function is missed. 3. This paper lacks the complexity analysis for the proposed method. 4. Only validating the proposed method on one real dataset (i.e., Adult) cannot guarantee its applicability in the wide spectrum of real-world applications. 5. The assumption about the mutual independency of the exogenous variables are too strong to be satisfied in real-world applications.

Review Point: 1. The proposed method is based on a pre-defined causal graph, which has limitations if the causal graph is unavailable. In the experimental results sections, the authors only showed the results with the graph constructed by the PC algorithm. It is not clear how the way of graph construction affects the final results.
Review Point: 2. The optimization details for the objective function is missed.
Review Point: 3. This paper lacks the complexity analysis for the proposed method.
Review Point: 4. Only validating the proposed method on one real dataset (i.e., Adult) cannot guarantee its applicability in the wide spectrum of real-world applications.
Review Point: 5. The assumption about the mutual independency of the exogenous variables are too strong to be satisfied in real-world applications.
==================================================

Focused review:

Weaknesses: 1.The problem formulation is somewhat unclear in the statement and introduction examples. 2. More baselines or self variants should be compared to better prove the effectiveness.
Detailed comments:
The problem definition of keyword search on incomplete graphs is ambiguous and confusing. The KS-GNN mostly optimizes on node similarity and the inference stage tends to select the top-k most similar results towards the query keyword set. However, the problem itself seems more like a combinatorial one, or say, set optimization, node-set selection with minimal distance measurement. Are the targets here equivalent?
The baseline approach seems much inferior to KS-GNN. It would be great to include some variants of the KS-GNN that delete some of the module or training objectives to confirm the contribution of each component.
Table 2 with missing edges is supposed to be more challenging than the task in Table 1. However, a lot of models perform even better (or comparable) which seems strange. Also, the claim of “KS-GNN has no significant effect” does not apply to the Toy and Video datasets for correctness.
It is hard to conclude from Figure 4 on the benefit of keyword frequency regularization. That is, it’s better to show the performance scores along with the visualization.
Figure 3: The notations of the figure (especially function f and g) are confusing.


Review Point: 1.The problem formulation is somewhat unclear in the statement and introduction examples.
Review Point: 2. More baselines or self variants should be compared to better prove the effectiveness. Detailed comments: The problem definition of keyword search on incomplete graphs is ambiguous and confusing. The KS-GNN mostly optimizes on node similarity and the inference stage tends to select the top-k most similar results towards the query keyword set. However, the problem itself seems more like a combinatorial one, or say, set optimization, node-set selection with minimal distance measurement. Are the targets here equivalent? The baseline approach seems much inferior to KS-GNN. It would be great to include some variants of the KS-GNN that delete some of the module or training objectives to confirm the contribution of each component. Table 2 with missing edges is supposed to be more challenging than the task in Table 1. However, a lot of models perform even better (or comparable) which seems strange. Also, the claim of “KS-GNN has no significant effect” does not apply to the Toy and Video datasets for correctness. It is hard to conclude from Figure 4 on the benefit of keyword frequency regularization. That is, it’s better to show the performance scores along with the visualization. Figure 3: The notations of the figure (especially function f and g) are confusing.
==================================================

Focused review:

Weaknesses (including main points to be addressed in a rebuttal, numbered for the authors’ convenience):
(TL;DR: state the exact page of Nagaragan and Kolter where Proposition D.2 appears, substantially rewrite and clarify Section D) The paper claims that their bounds supersume the uniform convergence approach, but It is very hard to make sense of the “proofs”. The claim is also misleading in general since no bound analogous to uniform convergence bounds is derived for any concrete architecture. There are errors in the “definitions” as well. Definition 3.1 is not very well written, but it is still technically possible to make sense of it. On the other hand, Section D makes very little sense. 1.1 The “proofs” of propositions D.2 and D.3 (which are almost exactly identical) merely seem to go back and forth through tautologies. Although I admit I may have missed something due to my lack of familiarity with PAC Bayesian bounds, I am quite certain that the proofs are at best very unclearly executed: Proposition D.2 seems to be a reformulation of the concept of uniform generalization bounds taken from Nagarajan and Kolter 19. The authors say that NK19 failed to provide a proof, but it seems unlikely that the result in NK19 needs proof and the current treatment is messy. Prop. D.2 states that \Phi_u needs to fulfil definition D.1, which includes equation (14). This seems to be an assumption of the proposition. The proposition statement than says that equation (14) (from the assumed definition D.1) is equivalent to another trivial reformulation of it.
1.2 The nearly identical Corollaries D.1 and D.2 are not better. First of all, the corollaries claim to use Theorem 3.1 but do not explicitly state where they do so, though I believe it is exactly after the line “for any hypothesis h… the definition of the parametric function… bound”. The conclusion there does not seem sound either: Theorem 3.1 is implicitly used there in a form that doesn’t involve the probability over h’ being drawn from the prior. It is also not clear to me why the definition of mu is different depending on whether h=h’ or not (please explain). 2. In Section 4.2 (experiments), you claim that the set H “corresponds to the hypotheses h_w that can be obtained from this initialization” (the initialization being that of He et al. 2015). This initialization requires drawing from gaussian distributions, but corollary 3.1 only applies to a uniform distribution on H. Am I correct in assuming that you use the scaling/normalizing factor of the normal initializations from He et al. 2015 to define lower and upper thresholds that define a uniform distribution over the weights? If so, this needs to be explained thoroughly. It is also very unclear how this would maintain the benefits of He initialization since uniformly sampling from a hypercube is radically different from sampling from a multivariate gaussian in terms of dimensional dependence. 3. It seems like most of the experiments are conducted at a regime where there is so much data that the generalization gap is near zero, which makes the bounds far less informative.


Review Point: (including main points to be addressed in a rebuttal, numbered for the authors’ convenience): (TL;DR: state the exact page of Nagaragan and Kolter where Proposition D.2 appears, substantially rewrite and clarify Section D) The paper claims that their bounds supersume the uniform convergence approach, but It is very hard to make sense of the “proofs”. The claim is also misleading in general since no bound analogous to uniform convergence bounds is derived for any concrete architecture. There are errors in the “definitions” as well. Definition 3.1 is not very well written, but it is still technically possible to make sense of it. On the other hand, Section D makes very little sense. 1.1 The “proofs” of propositions D.2 and D.3 (which are almost exactly identical) merely seem to go back and forth through tautologies. Although I admit I may have missed something due to my lack of familiarity with PAC Bayesian bounds, I am quite certain that the proofs are at best very unclearly executed: Proposition D.2 seems to be a reformulation of the concept of uniform generalization bounds taken from Nagarajan and Kolter 19. The authors say that NK19 failed to provide a proof, but it seems unlikely that the result in NK19 needs proof and the current treatment is messy. Prop. D.2 states that \Phi_u needs to fulfil definition D.1, which includes equation (14). This seems to be an assumption of the proposition. The proposition statement than says that equation (14) (from the assumed definition D.1) is equivalent to another trivial reformulation of it. 1.2 The nearly identical Corollaries D.1 and D.2 are not better. First of all, the corollaries claim to use Theorem 3.1 but do not explicitly state where they do so, though I believe it is exactly after the line “for any hypothesis h… the definition of the parametric function… bound”. The conclusion there does not seem sound either: Theorem 3.1 is implicitly used there in a form that doesn’t involve the probability over h’ being drawn from the prior. It is also not clear to me why the definition of mu is different depending on whether h=h’ or not (please explain).
Review Point: 2. In Section 4.2 (experiments), you claim that the set H “corresponds to the hypotheses h_w that can be obtained from this initialization” (the initialization being that of He et al. 2015). This initialization requires drawing from gaussian distributions, but corollary 3.1 only applies to a uniform distribution on H. Am I correct in assuming that you use the scaling/normalizing factor of the normal initializations from He et al. 2015 to define lower and upper thresholds that define a uniform distribution over the weights? If so, this needs to be explained thoroughly. It is also very unclear how this would maintain the benefits of He initialization since uniformly sampling from a hypercube is radically different from sampling from a multivariate gaussian in terms of dimensional dependence.
Review Point: 3. It seems like most of the experiments are conducted at a regime where there is so much data that the generalization gap is near zero, which makes the bounds far less informative.
==================================================

Focused review:

Weaknesses
1 Fidelity of the explanation. The knowledge graph is treated as a ground truth during the explanation, while the model may not use any object or concept in the ConceptNet. For instance, the scene classification model may use the local color, texture, the whole illumination, or even the background (non-object) as core logic to make the decision. The proposed method could not cover the explanation of these cases.
2 Missing the relationship between objects. A model may not use the existence of objects but their spatial relationship or correlation of them with the background. The proposed method could not be used to explain reasons outside of the predefined knowledge graph. Please consider comparing your method with "A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts, CVPR 2021" which consider the relationship between objects (concepts) during the explanation.
3 Generalization. The author claim it is a method of image classification (title) while only showing experiments and method of scene classification (a little bit overclaiming). The proposed method needs a predefined knowledge graph, which may be hard to apply to the general image classification task.
4 Time and effort cost. If the user wants to modify one image error, how long will it take? It looks like the method needs a lot of extra training or effort to explain even a single image.


Review Point: 1 Fidelity of the explanation. The knowledge graph is treated as a ground truth during the explanation, while the model may not use any object or concept in the ConceptNet. For instance, the scene classification model may use the local color, texture, the whole illumination, or even the background (non-object) as core logic to make the decision. The proposed method could not cover the explanation of these cases.
Review Point: 2 Missing the relationship between objects. A model may not use the existence of objects but their spatial relationship or correlation of them with the background. The proposed method could not be used to explain reasons outside of the predefined knowledge graph. Please consider comparing your method with "A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts, CVPR 2021" which consider the relationship between objects (concepts) during the explanation.
Review Point: 3 Generalization. The author claim it is a method of image classification (title) while only showing experiments and method of scene classification (a little bit overclaiming). The proposed method needs a predefined knowledge graph, which may be hard to apply to the general image classification task.
Review Point: 4 Time and effort cost. If the user wants to modify one image error, how long will it take? It looks like the method needs a lot of extra training or effort to explain even a single image.
==================================================

Focused review:

Weaknesses:
GPM performs fairly close to or marginally better than HAT and EWC on most datasets.
GPM trades-off between ACC and BWT metrics by strictly controlling forgetting. Hence, it may not be able to identify tasks with very similar structure (or repeated tasks) and may not re-use the Core Gradient Space from previous tasks to improve performance on them (BWT). So, while it minimizes negative BWT, it also restricts positive BWT.
Potential improvements and clarifications:
The authors have repeatedly emphasized that their method performs task-free inference. However, doesn't this just mean that the tasks used are such that the inputs encode the task identity and do not require a separate task identifier. Further, on page 6, under Network Architectures, the authors state that apart from permuted MNIST tasks, they use the multi-head setting, i.e., each task has a separate classifier. How does this allow task-identifier free inference?
All models have been trained using plain SGD. Would it be possible to extend the method to other optimizers, e.g., Adam?
3 runs are too few to average over. These are supervised learning experiments, so it should definitely be possible to use more runs (at least 5, but preferably 10).
At first glance, it seems that the method may have scalability issues since it relies on performing SVD. However figure 2 counter-intuitively shows GPM to be both fast and memory efficient. Is this because these graphs in figure 2 are per-epoch? What about in between tasks when GPM needs to run SVD on all layers? Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task.
Lastly, no experiments with a single class per task have been performed. This setting is known to be quite challenging in general and induces significantly more catastrophic forgetting (see Kamra et al, 2017). In this setting it is also generally harder to just set batch-norm parameters using just the first task.
[Kamra et al, 2017] Deep Generative Dual Memory Network for Continual Learning. Nitin Kamra, Umang Gupta and Yan Liu. ArXiv 2017.
Please respond to the 5 potential improvements and clarifications mentioned above. I will be happy to raise the score further if the above concerns are addressed.
-------- Post-rebuttal edit -------
The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines. I believe that the paper is strong and makes a significant technical contribution to the field of CL. Hence, I recommend acceptance and I am updating my score to reflect the same.


Review Point: GPM performs fairly close to or marginally better than HAT and EWC on most datasets. GPM trades-off between ACC and BWT metrics by strictly controlling forgetting. Hence, it may not be able to identify tasks with very similar structure (or repeated tasks) and may not re-use the Core Gradient Space from previous tasks to improve performance on them (BWT). So, while it minimizes negative BWT, it also restricts positive BWT. Potential improvements and clarifications: The authors have repeatedly emphasized that their method performs task-free inference. However, doesn't this just mean that the tasks used are such that the inputs encode the task identity and do not require a separate task identifier. Further, on page 6, under Network Architectures, the authors state that apart from permuted MNIST tasks, they use the multi-head setting, i.e., each task has a separate classifier. How does this allow task-identifier free inference? All models have been trained using plain SGD. Would it be possible to extend the method to other optimizers, e.g., Adam?
Review Point: 3 runs are too few to average over. These are supervised learning experiments, so it should definitely be possible to use more runs (at least 5, but preferably 10). At first glance, it seems that the method may have scalability issues since it relies on performing SVD. However figure 2 counter-intuitively shows GPM to be both fast and memory efficient. Is this because these graphs in figure 2 are per-epoch? What about in between tasks when GPM needs to run SVD on all layers? Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task. Lastly, no experiments with a single class per task have been performed. This setting is known to be quite challenging in general and induces significantly more catastrophic forgetting (see Kamra et al, 2017). In this setting it is also generally harder to just set batch-norm parameters using just the first task. [Kamra et al, 2017] Deep Generative Dual Memory Network for Continual Learning. Nitin Kamra, Umang Gupta and Yan Liu. ArXiv 2017. Please respond to the 5 potential improvements and clarifications mentioned above. I will be happy to raise the score further if the above concerns are addressed. -------- Post-rebuttal edit ------- The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines. I believe that the paper is strong and makes a significant technical contribution to the field of CL. Hence, I recommend acceptance and I am updating my score to reflect the same.
==================================================

Focused review:

(1) Many notations are missing, which makes the paper really hard to follow. For example, what are T_u, T_v in Prop. 1. What are all the bold notations in Eq. 3? The multiplication there is matrix product or vector component-wise product. (2) The whole story line is not clear. Different types of motivations come out here and there. For example, the arguments claimed in the abstract, the introduction and the overview in the Section 3 are not consistent. ------ Thank the authors for preparing the response. I have thoroughly checked the paper again and other reviewers' comments. I think the paper indeed considers some really complex and general structures in the hypergraph data. However, I still found it very hard to follow the core idea and thus hardly improve my evaluation. I will even increase my confidence score while keeping my evaluation as marginally tending to reject this paper. My specific criticisms are as the follows: 1. A lot of notations and their explanations have to be clearly highlighted, including the ones I previously mentioned and the other reviewers mentioned. 2. The motivating example is not clear enough and the formulation in G-MPNN/MPNN-R is not clear enough why they can capture the missing information in the motivating example. Also, as just the other reviewers said, in experiments, it is still not clear how to model the data that seems to be standard graphs that allows your models to indeed capture more information. 3. I still do not see why MPNN-R and G-MPNN, two very different models, should be compressed together within so limited space to make neither of them well explained. Moreover, these two models seem to have two different application domains. MPNN-R and its baselines used for standard hypergraphs. G-MPNN and its baselines used for comparison seem to be applied to heterogeneous graphs instead of natural hypergraphs. I guess that the authors in default assume that every reader knows the transformation from heterogeneous graphs to hypergraphs, which I do not think is the case. 4. I agree this paper unifies many models and generalizes many models. However, if the fundamental reason why such unification and generalization really brings benefit (not just empirical evaluation) is not shown clearly, I think such contributions are not valid and just incremental. Overall, I think that maybe there is indeed something useful in the paper. The exposition and notations are too unclear to make me (or maybe others) really appreciate this work.

Review Point: (1) Many notations are missing, which makes the paper really hard to follow. For example, what are T_u, T_v in Prop.
Review Point: 1. What are all the bold notations in Eq. 3? The multiplication there is matrix product or vector component-wise product. (2) The whole story line is not clear. Different types of motivations come out here and there. For example, the arguments claimed in the abstract, the introduction and the overview in the Section 3 are not consistent. ------ Thank the authors for preparing the response. I have thoroughly checked the paper again and other reviewers' comments. I think the paper indeed considers some really complex and general structures in the hypergraph data. However, I still found it very hard to follow the core idea and thus hardly improve my evaluation. I will even increase my confidence score while keeping my evaluation as marginally tending to reject this paper. My specific criticisms are as the follows:
Review Point: 1. A lot of notations and their explanations have to be clearly highlighted, including the ones I previously mentioned and the other reviewers mentioned.
Review Point: 2. The motivating example is not clear enough and the formulation in G-MPNN/MPNN-R is not clear enough why they can capture the missing information in the motivating example. Also, as just the other reviewers said, in experiments, it is still not clear how to model the data that seems to be standard graphs that allows your models to indeed capture more information.
Review Point: 3. I still do not see why MPNN-R and G-MPNN, two very different models, should be compressed together within so limited space to make neither of them well explained. Moreover, these two models seem to have two different application domains. MPNN-R and its baselines used for standard hypergraphs. G-MPNN and its baselines used for comparison seem to be applied to heterogeneous graphs instead of natural hypergraphs. I guess that the authors in default assume that every reader knows the transformation from heterogeneous graphs to hypergraphs, which I do not think is the case.
Review Point: 4. I agree this paper unifies many models and generalizes many models. However, if the fundamental reason why such unification and generalization really brings benefit (not just empirical evaluation) is not shown clearly, I think such contributions are not valid and just incremental. Overall, I think that maybe there is indeed something useful in the paper. The exposition and notations are too unclear to make me (or maybe others) really appreciate this work.
==================================================

Focused review:

Weaknesses -The testing 3D scene is synthetic and simple. From my understanding, the scenes only contain a bunch of cubes of different colors. Testing on the simple and synthetic case is a good start, and it would be great if we can later test the model on real and/or complex scenes.  Comments after the rebuttal ************************************ Thank authors for the rebuttal.Â  After reading reviews and rebuttal, I decided to lower the score to 6, as I noticed something not satisfying of this paper. 1) I first thought modeling a 3D scene by a generative query network is pretty novel, but the idea is actually from GQN paper. This is not a deficiency, but I previously mistook this as a strength of the paper; 2) The comparison between PoE and APoE in the rebuttal is not satisfying. For small |M| like 2, APoE is (almost) not saving time or space. For larger |M|, we need a performance comparison with PoE. I strongly recommend adding PoE result.  That said, I still think this paper is marginally above the acceptance threshold since it has the potential to be a good paper as the authors address the concerns from the reviewers.

Review Point: -The testing 3D scene is synthetic and simple. From my understanding, the scenes only contain a bunch of cubes of different colors. Testing on the simple and synthetic case is a good start, and it would be great if we can later test the model on real and/or complex scenes. Comments after the rebuttal ************************************ Thank authors for the rebuttal.Â After reading reviews and rebuttal, I decided to lower the score to 6, as I noticed something not satisfying of this paper.
Review Point: 1) I first thought modeling a 3D scene by a generative query network is pretty novel, but the idea is actually from GQN paper. This is not a deficiency, but I previously mistook this as a strength of the paper;
Review Point: 2) The comparison between PoE and APoE in the rebuttal is not satisfying. For small |M| like 2, APoE is (almost) not saving time or space. For larger |M|, we need a performance comparison with PoE. I strongly recommend adding PoE result. That said, I still think this paper is marginally above the acceptance threshold since it has the potential to be a good paper as the authors address the concerns from the reviewers.
==================================================

Focused review:

I have some pretty minor comments: - In the introduction and S2.2, the authors discuss both the time and memory issues with naive score matching. In the experimental section, only Table 2 has results on the memory saved, while the other results do not. Does the proposed method also improve memory usage on other models and datasets? - I'm pretty surprised that previous work does not exist for Lemma and Theorem 1. Although this is not my area of expertise, I would have assumed that this theory would already exist in an area such as Finite Difference Methods for Differential Equations. I know most of the teaching material focuses on the one-dimensional case, but I would be surprised if multivariate versions do not exist.

Review Point: - In the introduction and S2.2, the authors discuss both the time and memory issues with naive score matching. In the experimental section, only Table 2 has results on the memory saved, while the other results do not. Does the proposed method also improve memory usage on other models and datasets?
Review Point: - I'm pretty surprised that previous work does not exist for Lemma and Theorem 1. Although this is not my area of expertise, I would have assumed that this theory would already exist in an area such as Finite Difference Methods for Differential Equations. I know most of the teaching material focuses on the one-dimensional case, but I would be surprised if multivariate versions do not exist.
==================================================

Focused review:

Weaknesses
The problem setting is very limited with vocabulary of 200 words and the max utterance length is 20 tokens. Yet the paper makes very strong claims on answering two fundamental questions: RQ1. Does the inclusion of ToM in language acquisition models improve the performance and learned language of said models? (internal ToM mechanism) RQ2. How do our models’ learned languages adapt to discriminate between more visually and semantically similar images? (external environmental pressure)
The model design is presented as it is. There is no discussion on alternative designs and whether the design choice achieves the desired goals. For example, why LSTM is used. Would transformer or pretrained language model be a better choice.


Review Point: The problem setting is very limited with vocabulary of 200 words and the max utterance length is 20 tokens. Yet the paper makes very strong claims on answering two fundamental questions:
Review Point: RQ1. Does the inclusion of ToM in language acquisition models improve the performance and learned language of said models? (internal ToM mechanism) RQ2. How do our models’ learned languages adapt to discriminate between more visually and semantically similar images? (external environmental pressure) The model design is presented as it is. There is no discussion on alternative designs and whether the design choice achieves the desired goals. For example, why LSTM is used. Would transformer or pretrained language model be a better choice.
==================================================

Focused review:

Weaknesses:  - I am not entirely convinced by any of the analysis presented in the paper. Section 5.1 seems to particularly lack rigor, as it ignores most of the terms when studying an objective being optimized. The authors acknowledge this, but have still included this analysis in the paper. The analysis in Section 5.2 also make a number of approximations (these are more reasonable, however).  - The paper would benefit from some minor edits in English style/grammar. For example, in line 16-17: âWhile, many research works takeâ -> omit the âwhileâ.   In the end, I feel that the simplicity of the method, coupled with its strong performance on a large number of datasets merits an acceptance. However, the paper could be greatly improved with a better / more rigorous analysis of why it works well.   Other comments:  - Figure 2: What exact visualization technique is being used here? Would be good to mention that in the figure caption.  - The authors criticise L-softmax and A-softmax for requiring an annealing-like training procedure, but this method itself has a very specific learning rate schedule for experiments on CIFAR and Face Verification.  

Review Point: - I am not entirely convinced by any of the analysis presented in the paper. Section 5.1 seems to particularly lack rigor, as it ignores most of the terms when studying an objective being optimized. The authors acknowledge this, but have still included this analysis in the paper. The analysis in Section 5.2 also make a number of approximations (these are more reasonable, however).
Review Point: - The paper would benefit from some minor edits in English style/grammar. For example, in line 16-17: âWhile, many research works takeâ -> omit the âwhileâ. In the end, I feel that the simplicity of the method, coupled with its strong performance on a large number of datasets merits an acceptance. However, the paper could be greatly improved with a better / more rigorous analysis of why it works well. Other comments:
Review Point: - Figure 2: What exact visualization technique is being used here? Would be good to mention that in the figure caption.
Review Point: - The authors criticise L-softmax and A-softmax for requiring an annealing-like training procedure, but this method itself has a very specific learning rate schedule for experiments on CIFAR and Face Verification.
==================================================

Focused review:

Weaknesses:
1.The innovation of the article seems limited to me, mainly since the work shares the same perspective as [2]. Both the models build upon the probabilistic formulation and applies the Hilbert-Schmidt Independence Criteria (HSIC). It may be good to clarify a bit more on how novel the paper is compared from [2].
2.There is a lack of qualitative experiments to demonstrate the validity of the conditional independence model. a)It is better to provide some illustrative experimental results to demonstrate that minimising HSICcond-i could indeed perform better than minimising HSIC_HOOD. Possibly, one toy dataset can be used to demonstrate the separability of inlier features and outlier features.
b)The authors propose a new test metric, however, lacking the correctness test and comparative experiments with other metrices. It may be better to provide some visualization results or schematic diagram, which could make readers easier to understand.
3.Current experimental results seem not very convincing to me. Some critical comparative results are missing. a)Under the setting of unseen OOD training data, DIN [34], Mahalanobis distance [31], Energy [36], their original papers did not use fake/augmented OOD training data. These settings need to be clarified in the paper. Moreover, the impact of using different augmentation methods on the result could be explored in the ablation.
b)In CIFAR-100, the experimental setup appears to be consistent with that of HOOD [2]. However, in Table 1 (unseen OOD training data), the HOOD’s results are missing. In [2], the results of HOOD are superior to those of Conditional-I's.
c)In Table 2 (unseen OOD training data), the HOOD’s results are also missing.
d)There are missing both Conditional-i-generative and HOOD results for the NLP OOD detection tasks. As missing the results of the most relevant methods, the present experiments could not convince me of the validity of the improvements.
4.The memory bank architecture is one contribution, but the authors do not provide quantitative results of introducing the memory bank architecture.
The authors seemed not to discuss the limitations of the proposed model.


Review Point: 1.The innovation of the article seems limited to me, mainly since the work shares the same perspective as [2]. Both the models build upon the probabilistic formulation and applies the Hilbert-Schmidt Independence Criteria (HSIC). It may be good to clarify a bit more on how novel the paper is compared from [2].
Review Point: 2.There is a lack of qualitative experiments to demonstrate the validity of the conditional independence model. a)It is better to provide some illustrative experimental results to demonstrate that minimising HSICcond-i could indeed perform better than minimising HSIC_HOOD. Possibly, one toy dataset can be used to demonstrate the separability of inlier features and outlier features. b)The authors propose a new test metric, however, lacking the correctness test and comparative experiments with other metrices. It may be better to provide some visualization results or schematic diagram, which could make readers easier to understand. 3.Current experimental results seem not very convincing to me. Some critical comparative results are missing. a)Under the setting of unseen OOD training data, DIN [34], Mahalanobis distance [31], Energy [36], their original papers did not use fake/augmented OOD training data. These settings need to be clarified in the paper. Moreover, the impact of using different augmentation methods on the result could be explored in the ablation. b)In CIFAR-100, the experimental setup appears to be consistent with that of HOOD [2]. However, in Table 1 (unseen OOD training data), the HOOD’s results are missing. In [2], the results of HOOD are superior to those of Conditional-I's. c)In Table 2 (unseen OOD training data), the HOOD’s results are also missing. d)There are missing both Conditional-i-generative and HOOD results for the NLP OOD detection tasks. As missing the results of the most relevant methods, the present experiments could not convince me of the validity of the improvements. 4.The memory bank architecture is one contribution, but the authors do not provide quantitative results of introducing the memory bank architecture. The authors seemed not to discuss the limitations of the proposed model.
==================================================

Focused review:

Weaknesses: - The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive? - The method focuses on learning cluster granularity for the object only, and not for the background. - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)
A few comments on the text: - The phrase "coarse-grained images" is inaccurate, the "coarse-grained" adjective should refer to the clustering and not the images (in the intro). - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro. - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)


Review Point: - The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive?
Review Point: - The method focuses on learning cluster granularity for the object only, and not for the background.
Review Point: - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline) A few comments on the text:
Review Point: - The phrase "coarse-grained images" is inaccurate, the "coarse-grained" adjective should refer to the clustering and not the images (in the intro).
Review Point: - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro.
Review Point: - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)
==================================================

Focused review:

Weaknesses 1. The main (and only) theoretical result in the paper provides utility guarantees for the proposed algorithm only when the features and noise are Gaussian. This is a strong requirement on the data, especially given that previous algorithms don’t need this assumption as well. Moreover, the authors should compare the rates achieved by their procedure to existing rates in the literature. 2. Experiments: the experimental results in the paper don’t provide a convincing argument for their algorithms. First, all of the experiments are done over synthetic data. Moreover, the authors only consider low-dimensional datasets where d<30 and therefore it is not clear if the same improvements hold for high-dimensional problems. Finally, it is not clear whether the authors used any hyper-parameter tuning for DP-GD (or DP-SGD); this could result in significantly better results for DP-GD. 3. Writing: I encourage the authors to improve the writing in this paper. For example, the introduction could use more work on setting up the problem, stating the main results and comparing to previous work, before moving on to present the algorithm (which is done too soon in the current version).
More:
Typo (first sentence): “is a standard”
First paragraph in page 4 has m. What is m? Should that be n?


Review Point: 1. The main (and only) theoretical result in the paper provides utility guarantees for the proposed algorithm only when the features and noise are Gaussian. This is a strong requirement on the data, especially given that previous algorithms don’t need this assumption as well. Moreover, the authors should compare the rates achieved by their procedure to existing rates in the literature.
Review Point: 2. Experiments: the experimental results in the paper don’t provide a convincing argument for their algorithms. First, all of the experiments are done over synthetic data. Moreover, the authors only consider low-dimensional datasets where d<30 and therefore it is not clear if the same improvements hold for high-dimensional problems. Finally, it is not clear whether the authors used any hyper-parameter tuning for DP-GD (or DP-SGD); this could result in significantly better results for DP-GD.
Review Point: 3. Writing: I encourage the authors to improve the writing in this paper. For example, the introduction could use more work on setting up the problem, stating the main results and comparing to previous work, before moving on to present the algorithm (which is done too soon in the current version). More: Typo (first sentence): “is a standard” First paragraph in page 4 has m. What is m? Should that be n?
==================================================

Focused review:

* The presented claims only hold for a locally linear classifier * The claim that the linearity assumption is only violated close to a set of measure 0 is correct. This set has a codimension of 2 in a space of multi-thousand dimensions. Every point that is epsilon-close to this set lies on the codimensional 2 subset after it is dilated by an epsilon ball. It is not clear why this should not cover a large portion of the image domain. * It is not clear why the presented game is practically relevant. That both players add a perturbation to each sample does not seem to reflect the scenario where a classifier is released and the attacker tries to fool it after its release.

Review Point: * The presented claims only hold for a locally linear classifier * The claim that the linearity assumption is only violated close to a set of measure 0 is correct. This set has a codimension of 2 in a space of multi-thousand dimensions. Every point that is epsilon-close to this set lies on the codimensional 2 subset after it is dilated by an epsilon ball. It is not clear why this should not cover a large portion of the image domain.
Review Point: * It is not clear why the presented game is practically relevant. That both players add a perturbation to each sample does not seem to reflect the scenario where a classifier is released and the attacker tries to fool it after its release.
==================================================

Focused review:

Weakness:
The technical novelty of the proposed method is somewhat incremental. The proposed feature map token is just a simple way to perform global attention from the extracted features of CNN backbone. Some of the tricks used for performance improvement like L1-based 3D loss are incremental.
Although there is a graphical illustration of the FMT, it is still not clear about the specific operation. Concrete equations can be provided to make it clear about the actual operations.
There are some details about the overall framework are missing. For example, how many (image) frames are used to reconstruct a 3D pose? Are the previous t frames used to reconstruct the (t+1) 3D pose? These settings should be made clear. Also, in the experiments, are these settings keep the same for all the competing methods?
Direct regression from heatmap (or 2D pose) to 3D pose with a simple neural-network based design is not a significant contribution. This is common in the literature of general 3d human pose estimation.
The experimental evaluation is weak. 1) Only one ego-HPE method (i.e., reference [21]) is used for comparison on the Ego-dataset. Reference [21] was published in 2019. It may not represent the state-of-the-art. 2) 3d human pose estimation from image/video is a well-established field with many recent works using vision-transformer for achieving impressive results. It is important to also include the recent state-of-the-art (image-based and video-based) outside-in 3d pose estimation methods for a comprehensive evaluation on the datasets used in the experiments. There are many works also focus on occlusion-robust pose estimation. Otherwise, the result comparison is not convincing, and it is difficult to justify why a specific ego-focused HPE method is needed.
How the proposed method can address the occlusion issue? Some results and visualization should be provided.
The computational complexity aspect (e.g., FLOPs) of the method should be discussed and compared with sota methods.


Review Point: The technical novelty of the proposed method is somewhat incremental. The proposed feature map token is just a simple way to perform global attention from the extracted features of CNN backbone. Some of the tricks used for performance improvement like L1-based 3D loss are incremental. Although there is a graphical illustration of the FMT, it is still not clear about the specific operation. Concrete equations can be provided to make it clear about the actual operations. There are some details about the overall framework are missing. For example, how many (image) frames are used to reconstruct a 3D pose? Are the previous t frames used to reconstruct the (t+1) 3D pose? These settings should be made clear. Also, in the experiments, are these settings keep the same for all the competing methods? Direct regression from heatmap (or 2D pose) to 3D pose with a simple neural-network based design is not a significant contribution. This is common in the literature of general 3d human pose estimation. The experimental evaluation is weak.
Review Point: 1) Only one ego-HPE method (i.e., reference [21]) is used for comparison on the Ego-dataset. Reference [21] was published in 2019. It may not represent the state-of-the-art.
Review Point: 2) 3d human pose estimation from image/video is a well-established field with many recent works using vision-transformer for achieving impressive results. It is important to also include the recent state-of-the-art (image-based and video-based) outside-in 3d pose estimation methods for a comprehensive evaluation on the datasets used in the experiments. There are many works also focus on occlusion-robust pose estimation. Otherwise, the result comparison is not convincing, and it is difficult to justify why a specific ego-focused HPE method is needed. How the proposed method can address the occlusion issue? Some results and visualization should be provided. The computational complexity aspect (e.g., FLOPs) of the method should be discussed and compared with sota methods.
==================================================

Focused review:

After reviewer discussions and reading rebuttal, I strongly believe this paper should not be accepted at NeurIPS. 1) Concern on over-smoothing and sub-optimality of the method is not addressed. 2) Concern on convergence is detrimental for this paper (as shared by R2) a) The paper claims convergence, but this is never demonstrated empirically. I asked for empirical support but authors fail to respond. b) The theoretical assumption for convergence rely on strong assumption and do not hold for common GNN architectures. 3) Clarity and reproducibility are of serious concern (as shared by R4). The description of the algorithm is unclear and important details are missing. This makes the paper not reproducible and the results are questionable. I assume reproducibility is a very important issue for NeurIPS. 4) Many state of the art baselines are missing. This shows concern for whether the method is actually good in performance. The author response results (Fig. 1) show they do not outperform previous methods. Not to mention the paper is not reproducible, so results are questionable. 5) Assignment of credit is unfair. The paper simply builds upon implicit function theorem but never explicitly discuss that. Moreover, related work on such equilibrium models are missing. 6) Many other points. ---------------------------------------- 1. The motivation is not convincing. It's not compelling that GNN should converge to an equilibrium. In fact, literature show that deeper GNNs often suffer from 'over-smoothing', i.e., learning too much noise from large neighborhood and perform worse than shallow GNN. Implicit GNN, which tries to iterate until converge, would also suffer from over-smoothing. [1]-[4] address this issue and make deeper GNNs also generalize well. Unfortunately, authors fail to discuss and compare with this line of works. 2. The proposed GNN architecture is rather weak, with only 1 linear layer plus phi. The conditions for uniqueness of equilibrium also relies on this weak form of architecture. Analysis for more reasonable architecture is desired. 3. Clarity is of concern. The description of the algorithm and how the chain rule is applied to estimate gradient is confusing. The implicit function theorem is not defined. line 172, Perron-Frobenius (PF) eigenvalue is never defined. Line 230, q is not defined. Line 233, phi(z)' never used anywhere in the algorithm. (8) algorithm is not clearly defined. 4. Error analysis for gradient estimation and runtime is not provided. 5. Motivation of heterogeneous graphs is not compelling. It's not clear how the proposed method can benefit heterogeneous graphs compared to other methods. 6. State-of-the-art baselines [5] for experimental evaluation is missing. 7. It's not clear how helpful the projected GD is. Ablation study is desired. 8. Convergence is not demonstrated empirically. Can authors show that implicit GNN indeed converge to equilibrium? References [1] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018. [2] Predict then Propagate: Graph Neural Networks meet Personalized PageRank. ICLR 2019. [3] DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. ICLR 2020. [4] Simple and Deep Graph Convolutional Networks. ICML 2020. [5] Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels. NeurIPS 2019.

Review Point: After reviewer discussions and reading rebuttal, I strongly believe this paper should not be accepted at NeurIPS.
Review Point: 1) Concern on over-smoothing and sub-optimality of the method is not addressed.
Review Point: 2) Concern on convergence is detrimental for this paper (as shared by R2) a) The paper claims convergence, but this is never demonstrated empirically. I asked for empirical support but authors fail to respond. b) The theoretical assumption for convergence rely on strong assumption and do not hold for common GNN architectures. 3) Clarity and reproducibility are of serious concern (as shared by R4). The description of the algorithm is unclear and important details are missing. This makes the paper not reproducible and the results are questionable. I assume reproducibility is a very important issue for NeurIPS. 4) Many state of the art baselines are missing. This shows concern for whether the method is actually good in performance. The author response results (Fig. 1) show they do not outperform previous methods. Not to mention the paper is not reproducible, so results are questionable. 5) Assignment of credit is unfair. The paper simply builds upon implicit function theorem but never explicitly discuss that. Moreover, related work on such equilibrium models are missing. 6) Many other points. ---------------------------------------- 1. The motivation is not convincing. It's not compelling that GNN should converge to an equilibrium. In fact, literature show that deeper GNNs often suffer from 'over-smoothing', i.e., learning too much noise from large neighborhood and perform worse than shallow GNN. Implicit GNN, which tries to iterate until converge, would also suffer from over-smoothing. [1]-[4] address this issue and make deeper GNNs also generalize well. Unfortunately, authors fail to discuss and compare with this line of works. 2. The proposed GNN architecture is rather weak, with only 1 linear layer plus phi. The conditions for uniqueness of equilibrium also relies on this weak form of architecture. Analysis for more reasonable architecture is desired. 3. Clarity is of concern. The description of the algorithm and how the chain rule is applied to estimate gradient is confusing. The implicit function theorem is not defined. line 172, Perron-Frobenius (PF) eigenvalue is never defined. Line 230, q is not defined. Line 233, phi(z)' never used anywhere in the algorithm. (8) algorithm is not clearly defined. 4. Error analysis for gradient estimation and runtime is not provided. 5. Motivation of heterogeneous graphs is not compelling. It's not clear how the proposed method can benefit heterogeneous graphs compared to other methods. 6. State-of-the-art baselines [5] for experimental evaluation is missing. 7. It's not clear how helpful the projected GD is. Ablation study is desired. 8. Convergence is not demonstrated empirically. Can authors show that implicit GNN indeed converge to equilibrium? References [1] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018. [2] Predict then Propagate: Graph Neural Networks meet Personalized PageRank. ICLR 2019. [3] DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. ICLR 2020. [4] Simple and Deep Graph Convolutional Networks. ICML 2020. [5] Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels. NeurIPS 2019.
==================================================

Focused review:

weaknesses of the paper:
Sure, if we focus only on the low-order frequencies trainability is better. Also, I dare say that the insight is not exactly surprising although very intriguing. However, what exactly is the message? That we should have only low frequencies? Or that we should have some high frequencies? That skip connections are good for better training? I believe that in many ways, the message is incomplete if one leaves out expressivity and it would be nice to extend the theory to say something about the potency of the neural network on learning patterns and generalizing. The authors already comment on this in the 'future work' lines. I think that this should become current work, otherwise the work is incomplete, at least from the current perspective.
I have the feeling there are places where the analysis is imprecise, although it could be that I also misunderstood.
For one, the crux of the analysis is that the neural network nonlinearities are expressed in Fourier series (sec 3.2). Then, in the next section 3.3 the paper says that in practice nonlinearities are not polynomial and might not have a convergent Taylor expansion. So, instead a Chebyshev approximation is opted for. However, it is not clear if the Chebyshev approximation suffices or what are the limits of it? but I think this must be elaborated further.
Also, what are these Chebyshev approximations per nonlinearity? I think it is quite important to clarify this, considering there are nonlinearities that all but very similar, e.g., the ReLU and the leaky ReLU. What is the big difference between the two in terms of the described analysis?
I find it hard to understand often what the analysis tries to say, either the analysis is incomplete, the writing generally unclear or I simply don't understand some details. I list my comments by order of reading (not importance).
Throughout the paper there is a clear desire to connect rougness with layer depth. However, in all equations and analysis the depth is not explicitly present. For instance, in equations 4-6 there is only the degree of the polynomial K, but no layer variable or index. From what I gather, the (implicit) argument is that by the successive stacking of layers, the corresponding low/high order frequencies get stronger or weaker, relatively. Then, the objective is to compare the corresponding low and high frequencies for different layers, showing that for deeper layers the higher frequencies get stronger because of the recursion. This is how depth is 'qualitatively' introduced as a variable. Is this indeed the intention? If yes, I think it can be written more explicitly.
I find figure 1 a bit perplexing. Again, I understand what is the message, but it is hard for me to connect it to the theory, since the theory makes only indirect references to the specific nonlinearities. Also, what is 'ReLU->ResNet' supposed to stand for? ResNet is a ReLU when including skip connections? And what is 'Linear->ReLU'? To put it otherwise, adding skip connections or a ReLU nonlinearity are discrete design choices. However, the figure has continuous axis. So, what exactly is illustrated? The 'vertical' axis corresponds to the t variable in the Fourier coefficient. What about the other axis?
The related work points to Li et al and their spectral analysis to ground the proposed research. However, it is not explained what these observations are and how they relate to the current paper. It would be nice for the reader to add a short explanation.
Do we expect a difference by considering 1D slides, instead of 2D slides as motivated by Li et al? Why yes, why no?
It is not explained why are the mean path are empirically zero-functions. I infer that this is the case because at any location of the loss surface, if we take a small ball around it there will be an equal amount of parameters for which there is a higher or lower loss value? However, wouldn't this imply already a strong gradient (about 1, if I am not mistaken)?
What I find a bit confusing is that in equation 4 and 5 we apply the nonlinearity \phi on p(t). However, p(t) are the 1-D slices of our neural network f defined in the preamble of section 3. I would assume that the nonlinearities would then already be inside p(t). In fact, in the preamble of 3 there is also a mention of \phi and how p(t) is a polynomial when \phi is the identity function. Maybe I have misunderstood something here.
There is an attempt to connect exploding gradients to blueshifting. However, this is not entirely clear to me. Indeed, one can say that simultaneously we have blueshifting in the gradient and at the same time exploding gradients. Does this mean that one cause the other, however? Couldn't one have explosions by having disproportionately large low order frequencies (not that it is the case, just wondering)? Or some other phenomenon.
There is a connection to exploding gradients, however, in deep networks vanishing gradients also important (maybe more so). Can the analysis address also vanishing gradients?
It is not exactly clear why the frequency dependent signal averaging is wekaner than exponetial downweights. The explanation is very brief and a bit vague (law of large numbers, exponential decay). Is there a more precise qualitative or quantitative argument here? Can one still call the method as more 'global' in saying something about roughness, given that all coefficients are computed per layer? Of course, each layer's coefficient are influenced by all previous layers, but is this enough to paint the method 'global'?
Is somehow K (polynomial order) connected to L (number of layers)? Or is this relation the way I described above?
Perhaps relevant to the previous point, and taking the position of the devil's advocate, in a way what is put forward by the paper is a re-interpretation of existing knowledge. While certainly very intriguing, is there a new insight on trainability for a new type of method/technique that can improve trainability? What about wide layers and them being easier/harder to train?
The text in p. 6 on Fig 5 (In figure 5, we use the power law ... ) is unclear to me.
In p. 7 there is a great misalignment between the figure references and the figure locations in the paper.
How is leaky ReLU connected to other nonlinearities? How precisely does it make a difference? How difference is the Chebyshev polynomial?
What does it mean 'making the networking more linear'? Do you mean increasing the \alpha hyperparameter till it becomes 1, in which case you have a linear function?
In general, I find the paper quite interesting and with valuable potential contributions, but incomplete and not ready for publication at this stage. I believe it would worth it if the authors took the time to revisit the crispness of the message as well as the writing. Of course, I am more than happy to revisit my recommendation if the authors produce a convincing argument.


Review Point: of the paper: Sure, if we focus only on the low-order frequencies trainability is better. Also, I dare say that the insight is not exactly surprising although very intriguing. However, what exactly is the message? That we should have only low frequencies? Or that we should have some high frequencies? That skip connections are good for better training? I believe that in many ways, the message is incomplete if one leaves out expressivity and it would be nice to extend the theory to say something about the potency of the neural network on learning patterns and generalizing. The authors already comment on this in the 'future work' lines. I think that this should become current work, otherwise the work is incomplete, at least from the current perspective. I have the feeling there are places where the analysis is imprecise, although it could be that I also misunderstood. For one, the crux of the analysis is that the neural network nonlinearities are expressed in Fourier series (sec 3.2). Then, in the next section 3.3 the paper says that in practice nonlinearities are not polynomial and might not have a convergent Taylor expansion. So, instead a Chebyshev approximation is opted for. However, it is not clear if the Chebyshev approximation suffices or what are the limits of it? but I think this must be elaborated further. Also, what are these Chebyshev approximations per nonlinearity? I think it is quite important to clarify this, considering there are nonlinearities that all but very similar, e.g., the ReLU and the leaky ReLU. What is the big difference between the two in terms of the described analysis? I find it hard to understand often what the analysis tries to say, either the analysis is incomplete, the writing generally unclear or I simply don't understand some details. I list my comments by order of reading (not importance). Throughout the paper there is a clear desire to connect rougness with layer depth. However, in all equations and analysis the depth is not explicitly present. For instance, in equations 4-6 there is only the degree of the polynomial K, but no layer variable or index. From what I gather, the (implicit) argument is that by the successive stacking of layers, the corresponding low/high order frequencies get stronger or weaker, relatively. Then, the objective is to compare the corresponding low and high frequencies for different layers, showing that for deeper layers the higher frequencies get stronger because of the recursion. This is how depth is 'qualitatively' introduced as a variable. Is this indeed the intention? If yes, I think it can be written more explicitly. I find figure 1 a bit perplexing. Again, I understand what is the message, but it is hard for me to connect it to the theory, since the theory makes only indirect references to the specific nonlinearities. Also, what is 'ReLU->ResNet' supposed to stand for? ResNet is a ReLU when including skip connections? And what is 'Linear->ReLU'? To put it otherwise, adding skip connections or a ReLU nonlinearity are discrete design choices. However, the figure has continuous axis. So, what exactly is illustrated? The 'vertical' axis corresponds to the t variable in the Fourier coefficient. What about the other axis? The related work points to Li et al and their spectral analysis to ground the proposed research. However, it is not explained what these observations are and how they relate to the current paper. It would be nice for the reader to add a short explanation. Do we expect a difference by considering 1D slides, instead of 2D slides as motivated by Li et al? Why yes, why no? It is not explained why are the mean path are empirically zero-functions. I infer that this is the case because at any location of the loss surface, if we take a small ball around it there will be an equal amount of parameters for which there is a higher or lower loss value? However, wouldn't this imply already a strong gradient (about 1, if I am not mistaken)? What I find a bit confusing is that in equation 4 and 5 we apply the nonlinearity \phi on p(t). However, p(t) are the 1-D slices of our neural network f defined in the preamble of section 3. I would assume that the nonlinearities would then already be inside p(t). In fact, in the preamble of 3 there is also a mention of \phi and how p(t) is a polynomial when \phi is the identity function. Maybe I have misunderstood something here. There is an attempt to connect exploding gradients to blueshifting. However, this is not entirely clear to me. Indeed, one can say that simultaneously we have blueshifting in the gradient and at the same time exploding gradients. Does this mean that one cause the other, however? Couldn't one have explosions by having disproportionately large low order frequencies (not that it is the case, just wondering)? Or some other phenomenon. There is a connection to exploding gradients, however, in deep networks vanishing gradients also important (maybe more so). Can the analysis address also vanishing gradients? It is not exactly clear why the frequency dependent signal averaging is wekaner than exponetial downweights. The explanation is very brief and a bit vague (law of large numbers, exponential decay). Is there a more precise qualitative or quantitative argument here? Can one still call the method as more 'global' in saying something about roughness, given that all coefficients are computed per layer? Of course, each layer's coefficient are influenced by all previous layers, but is this enough to paint the method 'global'? Is somehow K (polynomial order) connected to L (number of layers)? Or is this relation the way I described above? Perhaps relevant to the previous point, and taking the position of the devil's advocate, in a way what is put forward by the paper is a re-interpretation of existing knowledge. While certainly very intriguing, is there a new insight on trainability for a new type of method/technique that can improve trainability? What about wide layers and them being easier/harder to train? The text in p.
Review Point: 6 on Fig 5 (In figure 5, we use the power law ... ) is unclear to me. In p.
Review Point: 7 there is a great misalignment between the figure references and the figure locations in the paper. How is leaky ReLU connected to other nonlinearities? How precisely does it make a difference? How difference is the Chebyshev polynomial? What does it mean 'making the networking more linear'? Do you mean increasing the \alpha hyperparameter till it becomes 1, in which case you have a linear function? In general, I find the paper quite interesting and with valuable potential contributions, but incomplete and not ready for publication at this stage. I believe it would worth it if the authors took the time to revisit the crispness of the message as well as the writing. Of course, I am more than happy to revisit my recommendation if the authors produce a convincing argument.
==================================================

Focused review:

- DVP needs to perform training at test time (25-50 epochs) per testing sequence. - The reviewer understands that the Figure 8 provides some insights "when to stop", however, it is unclear how it will change or is it sensitive to the length of videos (longer videos). - It is interesting to see how DVP perform on video with different length?

Review Point: - DVP needs to perform training at test time (25-50 epochs) per testing sequence.
Review Point: - The reviewer understands that the Figure 8 provides some insights "when to stop", however, it is unclear how it will change or is it sensitive to the length of videos (longer videos).
Review Point: - It is interesting to see how DVP perform on video with different length?
==================================================

Focused review:

Weaknesses:
1.This paper lacks novelty and is only a combination of some existing approaches, such as Qu et al. (2020). Moreover, I find that the equations are similar.
2.The motivation is not clear at all. The introduction should be carefully revised to make this paper easy to follow.
3.I find the experimental analysis is vague, and why the model works better is not clear. No case studies and no detailed ablation analysis.


Review Point: 1.This paper lacks novelty and is only a combination of some existing approaches, such as Qu et al. (2020). Moreover, I find that the equations are similar.
Review Point: 2.The motivation is not clear at all. The introduction should be carefully revised to make this paper easy to follow.
Review Point: 3.I find the experimental analysis is vague, and why the model works better is not clear. No case studies and no detailed ablation analysis.
==================================================

Focused review:

- The proposed method is not explained with adequate details (more below).
- The results (Table 3, 4) seem to be from single runs, hence are not trustworthy. They should be given as the average of multiple runs with different random seeds. 
- 006-007: The sentence sounds weird. Maybe delete the "better" at the end - 017: You can hint at which components you refer to - 017: jointly - 060: how do you define a difficult word?
- 091-93: it just repeats the same sentence with no clarification. then you jump into the tiny details.   - 131: capable of learning - 135: space before ( - 152-155: more suitable then what? e.g., isn't T5 also suitable?
- 188-191: why would you need to generate both of them? isn't simplification the point of this task?
- 225: in -> of - 250-252: what is the hypothesis? why do you think sharing decoders among two tasks would help? the motivation is missing - 267: focusd -> focused - 285-287: repeating the same sentence - 304: Don't you only have the corrupted text as the input for the reconstruction (also as shown in Fig 2)? But this equation tells you do language modeling on the original text?   - In general for Sec 3.2: you mention that only some parameters are shared, but then you use the same \theta as the model parameters for each sub task. This is confusing. Please separate the parameters for clarity.  - 317: dived -> divided - 340: I'm very confused how the complexity "c". what's the range of c? how is it defined? is it learned during training without supervision and how? how do you control for it?
- 350: what exactly is a query? the output vector from the cross-attention?
- 433: The model is heavily based on MASS, but it is not described properly.   - 440-441: Why don't you tune your lambda parameters on the dev test for fair comparison? Please also add how you initialize the parameters that are not part of MASS, e.g., the ones in Eq. 8.
- 547: sharing 571: extra space before comma 

Review Point: - The proposed method is not explained with adequate details (more below).
Review Point: - The results (Table 3, 4) seem to be from single runs, hence are not trustworthy. They should be given as the average of multiple runs with different random seeds.
Review Point: - 006-007: The sentence sounds weird. Maybe delete the "better" at the end - 017: You can hint at which components you refer to - 017: jointly - 060: how do you define a difficult word?
Review Point: - 091-93: it just repeats the same sentence with no clarification. then you jump into the tiny details.
Review Point: - 131: capable of learning - 135: space before ( - 152-155: more suitable then what? e.g., isn't T5 also suitable? - 188-191: why would you need to generate both of them? isn't simplification the point of this task? - 225: in -> of - 250-252: what is the hypothesis? why do you think sharing decoders among two tasks would help? the motivation is missing - 267: focusd -> focused - 285-287: repeating the same sentence - 304: Don't you only have the corrupted text as the input for the reconstruction (also as shown in Fig 2)? But this equation tells you do language modeling on the original text? - In general for Sec 3.2: you mention that only some parameters are shared, but then you use the same \theta as the model parameters for each sub task. This is confusing. Please separate the parameters for clarity. - 317: dived -> divided - 340: I'm very confused how the complexity "c". what's the range of c? how is it defined? is it learned during training without supervision and how? how do you control for it? - 350: what exactly is a query? the output vector from the cross-attention? - 433: The model is heavily based on MASS, but it is not described properly. - 440-441: Why don't you tune your lambda parameters on the dev test for fair comparison? Please also add how you initialize the parameters that are not part of MASS, e.g., the ones in Eq. 8. - 547: sharing 571: extra space before comma
==================================================

Focused review:

# Weaknesses 1. The proposed algorithm to compute the terms required for EM is computationally intensive and scales poorly with the latent space dimensionality and model size. This limits the empirical evaluations to only small neural networks and 1 dimensional latent spaces. 2. The analytical computations are only possible when working with Gaussian priors and likelihoods for the VAE. It also may not be possible to analytically compute the terms derived in the paper when the nonlinearities are not affine. 3. The derivation requires assuming that the likelihood of the data conditioned on latent has a constant covariance with respect to the latent. However, this is not a huge problem because in practice, the covariance is often made constant for optimization stability.

Review Point: # Weaknesses 1. The proposed algorithm to compute the terms required for EM is computationally intensive and scales poorly with the latent space dimensionality and model size. This limits the empirical evaluations to only small neural networks and 1 dimensional latent spaces.
Review Point: 2. The analytical computations are only possible when working with Gaussian priors and likelihoods for the VAE. It also may not be possible to analytically compute the terms derived in the paper when the nonlinearities are not affine.
Review Point: 3. The derivation requires assuming that the likelihood of the data conditioned on latent has a constant covariance with respect to the latent. However, this is not a huge problem because in practice, the covariance is often made constant for optimization stability.
==================================================

Focused review:

Weaknesses
There is little insight into when the proposed method may break or may not work as well as other competing methods. How does the efficacy of the proposed method vary with the size of the validation dataset? When would the single-step approximation be assumed in eq. (9). Is there any relevant form of data augmentation that may be hard to parametrize in a differentiable way? If so and if such transformations were to be critical for some applications, then wouldn't the competing approaches based on discrete optimization be more preferable?
It is unclear what the benefits of the Bayesian approach are. For example, one could set the noise term
η
=
0
in eq.(13) and I suspect that the method may still work reasonably well (not shown in the manuscript). The noise injection may potentially improve the diversity of sampled augmentations, but I feel this should be shown to substantiate the benefits of the Bayesian approach.
Minor comments
line 103: "differential" => "differentiable"
line 124: "we pick k<< n for simplicity" -> what do we lose from this assumption?
line 182: "pseudo one-step update" -> why pseudo?
line 204: "second-order derivative … computed more efficiently" => efficient in time or space?
line 232: no space after '.'
line 255: "Tab. 3 and Tab. 2" => Tab. 2 and Tab. 3
The main limitations in my view are the lack of insights into the limitations of the proposed approach as elaborated above. NA for societal impact.


Review Point: There is little insight into when the proposed method may break or may not work as well as other competing methods. How does the efficacy of the proposed method vary with the size of the validation dataset? When would the single-step approximation be assumed in eq. (9). Is there any relevant form of data augmentation that may be hard to parametrize in a differentiable way? If so and if such transformations were to be critical for some applications, then wouldn't the competing approaches based on discrete optimization be more preferable? It is unclear what the benefits of the Bayesian approach are. For example, one could set the noise term η = 0 in eq.(13) and I suspect that the method may still work reasonably well (not shown in the manuscript). The noise injection may potentially improve the diversity of sampled augmentations, but I feel this should be shown to substantiate the benefits of the Bayesian approach. Minor comments line 103: "differential" => "differentiable" line 124: "we pick k<< n for simplicity" -> what do we lose from this assumption? line 182: "pseudo one-step update" -> why pseudo? line 204: "second-order derivative … computed more efficiently" => efficient in time or space? line 232: no space after '.' line 255: "Tab.
Review Point: 3 and Tab. 2" => Tab. 2 and Tab. 3 The main limitations in my view are the lack of insights into the limitations of the proposed approach as elaborated above. NA for societal impact.
==================================================

Focused review:

Weaknesses: - Theoretical analyses are not particularly difficult, even if they do provide some insights. That is, the analyses are what I would expect any competent grad student to be able to come up with within the context of a homework assignment. I would consider the contributions there to be worthy of a posted note / arXiv article. - Section 4 is interesting, but does not provide any actionable advice to the practitioner, unlike Theorem 4. The conclusion I took was that the learned function f needs to achieve a compression rate of \zeta / m with a false positive rate F_p and false negative rate F_n. To know if my deep neural network (for example) can do that, I would have to actually train a fixed size network and then empirically measure its errors. But if I have to do that, the current theory on standard Bloom filters would provide me with an estimate of the equivalent Bloom filter that achieves the same error false positive as the learned Bloom filter. - To reiterate the above point, the analysis of Section 4 doesn't change how I would build, evaluate, and decide on whether to use learned Bloom filters. - The analytical approach of Section 4 gets confusing by starting with a fixed f with known \zeta, F_p, F_n, and then drawing the conclusion for an a priori fixed F_p, F_n (lines 231-233) before fixing the learned function f (lines 235-237). In practice, one typically fixes the function class (e.g. parameterized neural networks with the same architecture) *first* and measures F_p, F_n after. For such settings where \zeta and b are fixed a priori, one would be advised to minimize the learned Bloom filter's overall false positive (F_p + (1-F_p)\alpha^{b/F_n}) in the function class. An interesting analysis would then be to say whether this is feasible, and how it compares to the log loss function. Experiments can then conducted to back this up. This could constitute actionable advice to practitioners. Similarly for the sandwiched learned Bloom filter. - Claim (first para of Section 3.2) that "this methodology requires significant additional assumptions" seems too extreme to me. The only additional assumption is that the test set be drawn from the same distribution as the query set, which is natural for many machine learning settings where the train, validation, test sets are typically assumed to be from the same iid distribution. (If this assumption is in fact too hard to satisfy, then Theorem 4 isn't very useful too.) - Inequality on line 310 has wrong sign; compare inequality line 227 --- base \alpha < 1. - No empirical validation. I would have like to see some experiments where the bounds are validated.

Review Point: - Theoretical analyses are not particularly difficult, even if they do provide some insights. That is, the analyses are what I would expect any competent grad student to be able to come up with within the context of a homework assignment. I would consider the contributions there to be worthy of a posted note / arXiv article.
Review Point: - Section 4 is interesting, but does not provide any actionable advice to the practitioner, unlike Theorem 4. The conclusion I took was that the learned function f needs to achieve a compression rate of \zeta / m with a false positive rate F_p and false negative rate F_n. To know if my deep neural network (for example) can do that, I would have to actually train a fixed size network and then empirically measure its errors. But if I have to do that, the current theory on standard Bloom filters would provide me with an estimate of the equivalent Bloom filter that achieves the same error false positive as the learned Bloom filter.
Review Point: - To reiterate the above point, the analysis of Section 4 doesn't change how I would build, evaluate, and decide on whether to use learned Bloom filters.
Review Point: - The analytical approach of Section 4 gets confusing by starting with a fixed f with known \zeta, F_p, F_n, and then drawing the conclusion for an a priori fixed F_p, F_n (lines 231-233) before fixing the learned function f (lines 235-237). In practice, one typically fixes the function class (e.g. parameterized neural networks with the same architecture) *first* and measures F_p, F_n after. For such settings where \zeta and b are fixed a priori, one would be advised to minimize the learned Bloom filter's overall false positive (F_p + (1-F_p)\alpha^{b/F_n}) in the function class. An interesting analysis would then be to say whether this is feasible, and how it compares to the log loss function. Experiments can then conducted to back this up. This could constitute actionable advice to practitioners. Similarly for the sandwiched learned Bloom filter.
Review Point: - Claim (first para of Section 3.2) that "this methodology requires significant additional assumptions" seems too extreme to me. The only additional assumption is that the test set be drawn from the same distribution as the query set, which is natural for many machine learning settings where the train, validation, test sets are typically assumed to be from the same iid distribution. (If this assumption is in fact too hard to satisfy, then Theorem 4 isn't very useful too.) - Inequality on line 310 has wrong sign; compare inequality line 227 --- base \alpha < 1.
Review Point: - No empirical validation. I would have like to see some experiments where the bounds are validated.
==================================================

Focused review:

- Despite the effectiveness of the proposed approach as proven by extensive experiments, the technical novelties are fairly limited. Applying dynamic operations for dynamic mask generation has been previously explored in the segmentation literature, e.g. conditional batch normalization [1], adaptive instance normalization [2], but are not discussed here. What is the benefit of using dynamic convolution as opposed to other conditional operations? - The Fig.4 in SOLO illustrates the location-awareness of each learned kernel, i.e. different instance is activated at different channel (among S^2 channels) depending on the position of the grid in the image. It would be interesting to also provide a similar plot for DFIS (each subfigure corresponds to the mask produced by one of the S^2 kernels) to study whether such position-aware property still holds in DFIS. - Eqn(1) should be M_i,j = G_i,j * F instead. - Typo: threshing in L207 References: [1] Sofiiuk et al. “AdaptIS: Adaptive instance selection network”, in ICCV 2019. [2] Yang et al. “Efficient video object segmentation via network modulation”, in CVPR 2018.

Review Point: - Despite the effectiveness of the proposed approach as proven by extensive experiments, the technical novelties are fairly limited. Applying dynamic operations for dynamic mask generation has been previously explored in the segmentation literature, e.g. conditional batch normalization [1], adaptive instance normalization [2], but are not discussed here. What is the benefit of using dynamic convolution as opposed to other conditional operations?
Review Point: - The Fig.4 in SOLO illustrates the location-awareness of each learned kernel, i.e. different instance is activated at different channel (among S^2 channels) depending on the position of the grid in the image. It would be interesting to also provide a similar plot for DFIS (each subfigure corresponds to the mask produced by one of the S^2 kernels) to study whether such position-aware property still holds in DFIS.
Review Point: - Eqn(1) should be M_i,j = G_i,j * F instead.
Review Point: - Typo: threshing in L207 References: [1] Sofiiuk et al. “AdaptIS: Adaptive instance selection network”, in ICCV 2019. [2] Yang et al. “Efficient video object segmentation via network modulation”, in CVPR 2018.
==================================================

Focused review:

Weakness: 1. Lack of discussion with previous multi-branched network structure. Since the current paper focus on the design of a multi-branched block structure, a detailed comparison and discussion should be included, like ResNeXt[1], GoogleNet Family and Inception Family. Although they did not include self-attention in their branches, some of the networks do have the mixed kernel size (1x1 Vs 5x5), which is also another form of low/high frequency mixer. Also, the multi-branched network seems to have better generalization and optimization properties [2] compared with the single-branched counter parts. A simple citation is insufficient to highlight your improvement over them. In addition, as the paper is largely driven by the low/high pass filter design in signal processing, I would encourage the author incudes some reference on this topic. 2. Problems on ablation study. To be scientific, ablation study means quantifying the influence of each modular design by removal/changing of one component from the entire system. But in table 5 row 1-4, the authors are accurately adding one component at each time. I know that a lot of paper take this style of ablation study, but scientifically, this is not a valid ablation study. Better fix this.
[1] Aggregated Residual Transformations for Deep Neural Networks (CVPR 2017) [2] Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less Non-Convex (ICML 2019)


Review Point: 1. Lack of discussion with previous multi-branched network structure. Since the current paper focus on the design of a multi-branched block structure, a detailed comparison and discussion should be included, like ResNeXt[1], GoogleNet Family and Inception Family. Although they did not include self-attention in their branches, some of the networks do have the mixed kernel size (1x1 Vs 5x5), which is also another form of low/high frequency mixer. Also, the multi-branched network seems to have better generalization and optimization properties [2] compared with the single-branched counter parts. A simple citation is insufficient to highlight your improvement over them. In addition, as the paper is largely driven by the low/high pass filter design in signal processing, I would encourage the author incudes some reference on this topic.
Review Point: 2. Problems on ablation study. To be scientific, ablation study means quantifying the influence of each modular design by removal/changing of one component from the entire system. But in table 5 row 1-4, the authors are accurately adding one component at each time. I know that a lot of paper take this style of ablation study, but scientifically, this is not a valid ablation study. Better fix this. [1] Aggregated Residual Transformations for Deep Neural Networks (CVPR 2017) [2] Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less Non-Convex (ICML 2019)
==================================================

Focused review:

Weaknesses:
Although the use of likelihoods instead of loss functions is not a common practice in deep learning, its advantages have been thoroughly studied in statistics, econometrics and other disciplines, as also discussed in the related work of the paper. Hence, the novelty mainly lies in the application of these ideas in deep learning and the employment of some likelihoods better suited for the respective problems (i.e. softmax and rho-estimators).
The paper is interesting however I found it somewhat difficult to read. In my view it tries to pack many different aspects and applications of the main idea (use of likelihood) into a very limited space. In fact, there are too many cross-references to the supplemental material, to the point that it seems that most of the paper is described in the supplemental material. On a similar note, due to the fact that four different application domains are considered, there numerous methods, metrics and datasets involved in each one of them which are not sufficiently covered in the text. Additionally, many of the proposed methods/improvements/variants on each domain are not explained in sufficient detail (e.g. AE+S and PCA+S in Sec. 5.2). I would expect some more principled and thorough guidance on how to use the likelihood functions and, regarding the conditioning and dimensionality, strategies on how to choose among the various options.
Also some editing is required, for example the likelihood of the softmax is not provided as the respective sentence after eq. 4 is suddenly interrupted (see also the comments below).
Minor comments
the text in the figure is very small, making it very difficult to read in typical zoom levels (~100%)
Figure 3: the text does not correspond to the figure for the intermediate case
Figure 4, caption: include reference to left, middle and right panel
Table 1: there is no reference of this table in the text. Also, the three dots should be replaced with the actual setting.
Rating Justification:
I think that the overall idea of the paper is interesting and provides improved data modeling which leads to important advantages of the estimated models. However, possibly due to space limitations, the paper does not explain in sufficient detail important aspect of applying the proposed idea in the domains considered.
Rating and comments after the rebuttal
I think that in the revised version the paper has addressed many of the weaknesses pointed out in our reviews, hence I increase my rating to 6. Nevertheless, the paper still packs too much information which makes it difficult to read and appreciate. Regarding novelty, although I agree with other reviews that the core idea is not novel I think that it is important that the paper stresses the applicability and usefulness of considering likelihoods in deep learning models, as it appears to be not fully appreciated currently. Overall, I think that the paper would shine as a journal paper while it is only a borderline submission in its current form.


Review Point: Although the use of likelihoods instead of loss functions is not a common practice in deep learning, its advantages have been thoroughly studied in statistics, econometrics and other disciplines, as also discussed in the related work of the paper. Hence, the novelty mainly lies in the application of these ideas in deep learning and the employment of some likelihoods better suited for the respective problems (i.e. softmax and rho-estimators). The paper is interesting however I found it somewhat difficult to read. In my view it tries to pack many different aspects and applications of the main idea (use of likelihood) into a very limited space. In fact, there are too many cross-references to the supplemental material, to the point that it seems that most of the paper is described in the supplemental material. On a similar note, due to the fact that four different application domains are considered, there numerous methods, metrics and datasets involved in each one of them which are not sufficiently covered in the text. Additionally, many of the proposed methods/improvements/variants on each domain are not explained in sufficient detail (e.g. AE+S and PCA+S in Sec. 5.2). I would expect some more principled and thorough guidance on how to use the likelihood functions and, regarding the conditioning and dimensionality, strategies on how to choose among the various options. Also some editing is required, for example the likelihood of the softmax is not provided as the respective sentence after eq.
Review Point: 4 is suddenly interrupted (see also the comments below). Minor comments the text in the figure is very small, making it very difficult to read in typical zoom levels (~100%) Figure 3: the text does not correspond to the figure for the intermediate case Figure 4, caption: include reference to left, middle and right panel Table 1: there is no reference of this table in the text. Also, the three dots should be replaced with the actual setting. Rating Justification: I think that the overall idea of the paper is interesting and provides improved data modeling which leads to important advantages of the estimated models. However, possibly due to space limitations, the paper does not explain in sufficient detail important aspect of applying the proposed idea in the domains considered. Rating and comments after the rebuttal I think that in the revised version the paper has addressed many of the weaknesses pointed out in our reviews, hence I increase my rating to 6. Nevertheless, the paper still packs too much information which makes it difficult to read and appreciate. Regarding novelty, although I agree with other reviews that the core idea is not novel I think that it is important that the paper stresses the applicability and usefulness of considering likelihoods in deep learning models, as it appears to be not fully appreciated currently. Overall, I think that the paper would shine as a journal paper while it is only a borderline submission in its current form.
==================================================

Focused review:

Not so much weaknesses as areas of additional curiosity: 1) The carry forward data imputation of the dataset may be teaching the auxiliary tasks to predict the most recent value by default. Is this a strength or a weakness? 2) For what patients is this model serving well? One might imagine that this model biases for patients with a lot of observed data in the auxiliary tasks, i.e. longer stays in the ICU. 3) Can we reason about model convergence? How well does the model overcome local optimum? Specifically computing the weights of auxiliary tasks may lead to degenerate solution. How sensitive are the learned weights to dataset noise or bad initialization? --- I read all other reviews and author feedback. As the authors answered my questions sufficiently, I am keeping my score the same.

Review Point: 1) The carry forward data imputation of the dataset may be teaching the auxiliary tasks to predict the most recent value by default. Is this a strength or a weakness?
Review Point: 2) For what patients is this model serving well? One might imagine that this model biases for patients with a lot of observed data in the auxiliary tasks, i.e. longer stays in the ICU.
Review Point: 3) Can we reason about model convergence? How well does the model overcome local optimum? Specifically computing the weights of auxiliary tasks may lead to degenerate solution. How sensitive are the learned weights to dataset noise or bad initialization? --- I read all other reviews and author feedback. As the authors answered my questions sufficiently, I am keeping my score the same.
==================================================

Focused review:

The three main weaknesses I found was: 1) Lack of strong motivation for why adaptive discretization is needed in the first place? Not all RL algorithms need assume a discrete state space, and perform quite well in continuous spaces, so I do not know why there would be a need to "simplify" such continuous spaces in the first place. 2) This paper could have been put into greater context w.r.t. related literature, especially all the literature on this topic that's older than the last couples years. See the "Relation to prior work:" section below. 3) The experiments appears to show no difference between the methods' performances (except for the red baseline)?

Review Point: 1) Lack of strong motivation for why adaptive discretization is needed in the first place? Not all RL algorithms need assume a discrete state space, and perform quite well in continuous spaces, so I do not know why there would be a need to "simplify" such continuous spaces in the first place.
Review Point: 2) This paper could have been put into greater context w.r.t. related literature, especially all the literature on this topic that's older than the last couples years. See the "Relation to prior work:" section below.
Review Point: 3) The experiments appears to show no difference between the methods' performances (except for the red baseline)?
==================================================

Focused review:

- The method requires additional information to what is typically provided in a semantic segmentation dataset in that it needs to know whether masks are complete or incomplete for training. - While the ablation study shows the importance of the unpaired data for finetuning but also highlights the fact that the invisible component of the objects are typically small (i.e., far fewer pixels than the visible part). It would be interesting to see a plot of accuracy versus percentage occlusion. - The multiple plausible completions is never really tested given that the experiments are only run on rigid objects (I'm considering pedestrians here as rigid given their size and viewpoint in the chosen datasets). It would have been interesting to see results on articulated objects (such as animals) or objects where there is a greater variation in possible object shape.

Review Point: - The method requires additional information to what is typically provided in a semantic segmentation dataset in that it needs to know whether masks are complete or incomplete for training.
Review Point: - While the ablation study shows the importance of the unpaired data for finetuning but also highlights the fact that the invisible component of the objects are typically small (i.e., far fewer pixels than the visible part). It would be interesting to see a plot of accuracy versus percentage occlusion.
Review Point: - The multiple plausible completions is never really tested given that the experiments are only run on rigid objects (I'm considering pedestrians here as rigid given their size and viewpoint in the chosen datasets). It would have been interesting to see results on articulated objects (such as animals) or objects where there is a greater variation in possible object shape.
==================================================

Focused review:

Weakness: There are several points in the article that confused me, as follows:
In Table 4, the number of parameters of Pretraining + BCT is 43.59, while the number of parameters of Pretraining + BCT + Fix Trunk is 2.29. It seems unreasonable.
As can be seen from Table 4, it is the model distillation operation that provides the largest performance improvement across multi-resolution and same resolution task. What puzzles me is: 1) whether distillation is capable of such a significant improvement and 2) if the distillation operation can improve to such an extent, then the innovation points claimed in this paper may not be reliable and more gains come from the distillation operation.
The task of this paper is to obtain a better cross-resolution representation, and I think it may be more intuitive to visualize it using t-SNE.


Review Point: There are several points in the article that confused me, as follows: In Table 4, the number of parameters of Pretraining + BCT is 43.59, while the number of parameters of Pretraining + BCT + Fix Trunk is 2.29. It seems unreasonable. As can be seen from Table 4, it is the model distillation operation that provides the largest performance improvement across multi-resolution and same resolution task. What puzzles me is:
Review Point: 1) whether distillation is capable of such a significant improvement and 2) if the distillation operation can improve to such an extent, then the innovation points claimed in this paper may not be reliable and more gains come from the distillation operation. The task of this paper is to obtain a better cross-resolution representation, and I think it may be more intuitive to visualize it using t-SNE.
==================================================

Focused review:

Weaknesses:
By looking at the table of the quantitative results (Table 2), it seems hard to draw a conclusion at first sight, and the description of the results in Section 5.3 is not helpful either. It may be better if it is clearly called out that t-SNE and UMAP are good at local quality metrics, topological autoencoder and At-SNE are good at global quality metrics, while UMATO is good at both global and local metrics.
The visualization in Fig. 3 is interesting but I am not sure what information is conveyed or what conclusion is drawn for the three non-synthetic datasets. If the authors decide to keep these figures, more interpretations will be helpful for readers.
The algorithm description can be clearer by using some illustrative examples, besides using the actual Spheres data in Fig. 1 and Fig. 2. For example, building a graph, sort the data points in descending order of connectivity, and selecting the hubs: these steps can be well explained with an illustration, while using actual data such as Fig. 1 and Fig. 2 would make readers guess what is happening.
In Section 4.3, there are many descriptive steps in words that are vague (e.g. the description of an edge with point
p
and
q
), and I don't precisely know what they refer to without a picture.
In Fig. 2(C), it shows "LOCAL INIT" which looks like the state before running the local optimization, while (A) (B) (D) show the states after the corresponding steps, but the descriptions in the figure title haven't clarified their meanings. In particular, what do (B) and (C) refer to exactly?
Questions during rebuttal period: Please address #2 and #5 in the weaknesses above.


Review Point: By looking at the table of the quantitative results (Table 2), it seems hard to draw a conclusion at first sight, and the description of the results in Section 5.3 is not helpful either. It may be better if it is clearly called out that t-SNE and UMAP are good at local quality metrics, topological autoencoder and At-SNE are good at global quality metrics, while UMATO is good at both global and local metrics. The visualization in Fig. 3 is interesting but I am not sure what information is conveyed or what conclusion is drawn for the three non-synthetic datasets. If the authors decide to keep these figures, more interpretations will be helpful for readers. The algorithm description can be clearer by using some illustrative examples, besides using the actual Spheres data in Fig. 1 and Fig.
Review Point: 2. For example, building a graph, sort the data points in descending order of connectivity, and selecting the hubs: these steps can be well explained with an illustration, while using actual data such as Fig. 1 and Fig. 2 would make readers guess what is happening. In Section 4.3, there are many descriptive steps in words that are vague (e.g. the description of an edge with point p and q ), and I don't precisely know what they refer to without a picture. In Fig. 2(C), it shows "LOCAL INIT" which looks like the state before running the local optimization, while (A) (B) (D) show the states after the corresponding steps, but the descriptions in the figure title haven't clarified their meanings. In particular, what do (B) and (C) refer to exactly? Questions during rebuttal period: Please address #2 and #5 in the weaknesses above.
==================================================

Focused review:

I will list the detailed weakness in the next section.  In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable. 
1. " a smoother loss" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting?  There is a text overlap between 0.00 and 0.03 around the origin.
2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?
3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is "our method is also the **best** among all the baseline methods", which is a contradiction to me. Later the paper suggests that "outperforming the **baselines** of our implementation.", what is the difference between "baseline" and "your re-implemented results".  4. Some introduction to the datasets might also be helpful, such as the size of each dataset.  5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?
6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?
7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here.  8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting?  9. Line 22, "Bert" --> "BERT" 

Review Point: I will list the detailed weakness in the next section. In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable.
Review Point: 1. " a smoother loss" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting? There is a text overlap between 0.00 and 0.03 around the origin.
Review Point: 2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?
Review Point: 3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is "our method is also the **best** among all the baseline methods", which is a contradiction to me. Later the paper suggests that "outperforming the **baselines** of our implementation.", what is the difference between "baseline" and "your re-implemented results".
Review Point: 4. Some introduction to the datasets might also be helpful, such as the size of each dataset.
Review Point: 5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?
Review Point: 6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?
Review Point: 7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here.
Review Point: 8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting?
==================================================

Focused review:

weaknesses of the approach (e.g., the fact that algorithms do not enjoy optimality guarantees).  I was surprised by the fact that there is no comparison with safe methods that guarantee a monotonic policy performance improvement: e.g., in policy iteration [Kakade and Langford, 2002; Pirotta et al, 2013a] or policy gradient [Pirotta et al, 2013b, etc.]. Your approach is definitely different but, often, the constraint cost can be incorporated in the reward (it the case of your example).  The mentioned methods are guaranteed to learn policies that are always better than the baseline one. Then, if the baseline policy is good enough (satisfies the constraints reformulated in terms of bad reward) these approaches are guaranteed to recover safe policy over the entire learning process.  I think you should mention these works in the paper highlighting similarities and advantages of your approach. Please comment on this.  Please add a reference for the fact that the first hitting time is upper bounded by a finite quantity (you talk about standard notion).  I overall like the paper and I think it will be a good fit for NIPS subject to the fact that the authors will take into account the aforementioned comments about the presentation.  - Line 706: fnction -> function - Policy Distillation algorithm in page 24 contains a broken reference    [Bertsekas, 1995] D. P. Bertsekas, Dynamic Programming and Optimal Control, Vol. II. Athena Scientific ISBN: 1-886529-13-2 [Pirotta et al. 2013a] M Pirotta, M Restelli, A Pecorino, D Calandriello. Safe Policy Iteration, ICML 2013 [Pirotta et al. 2013b] M Pirotta, M Restelli, L Bascetta. Adaptive step-size for policy gradient methods, NIPS 2013  ------------------------ After feedback Thank you very much for the feedback. Now it is clear to me the difference between safe approaches and the proposed algorithm. I suggest you add this explanation to the paper. Hitting time: The assumption of boundness of the first hitting time is very restrictive. I underestimated it during the review because I was not sure about the meaning of the term "uniformly bounded". I checked the reference and uniform boundness of the first hitting time means that the time to reach the absorbing state is almost surely bounded for any stationary policy. I start noticing that due to the settings you considered, every stationary policy induces an absorbing Markov chain (by definition of proper policy, [Bertsekas, 1995, Chap 2, Def 1.1, pag 80]). For absorbing Markov chains it is known that the boundness of the first hitting time (with probability 1 the time to reach the absorbing state is bounded) implies that there are no loops in the chain (every state is visited at most once with probability 1) [see for example App. D of Fruit and Lazaric, Exploration-Exploitation in MDPs with Options, 2017]. You can assume this property but I would like to see this comment in the paper because I believe that the strong implication of this assumption is not well known in general. Moreover, you need to clearly distinguish the theoretical analysis from the empirical results because this assumption seems to be not satisfied in your experiments.

Review Point: of the approach (e.g., the fact that algorithms do not enjoy optimality guarantees). I was surprised by the fact that there is no comparison with safe methods that guarantee a monotonic policy performance improvement: e.g., in policy iteration [Kakade and Langford, 2002; Pirotta et al, 2013a] or policy gradient [Pirotta et al, 2013b, etc.]. Your approach is definitely different but, often, the constraint cost can be incorporated in the reward (it the case of your example). The mentioned methods are guaranteed to learn policies that are always better than the baseline one. Then, if the baseline policy is good enough (satisfies the constraints reformulated in terms of bad reward) these approaches are guaranteed to recover safe policy over the entire learning process. I think you should mention these works in the paper highlighting similarities and advantages of your approach. Please comment on this. Please add a reference for the fact that the first hitting time is upper bounded by a finite quantity (you talk about standard notion). I overall like the paper and I think it will be a good fit for NIPS subject to the fact that the authors will take into account the aforementioned comments about the presentation.
Review Point: - Line 706: fnction -> function - Policy Distillation algorithm in page 24 contains a broken reference [Bertsekas, 1995] D. P. Bertsekas, Dynamic Programming and Optimal Control, Vol. II. Athena Scientific ISBN: 1-886529-13-2 [Pirotta et al. 2013a] M Pirotta, M Restelli, A Pecorino, D Calandriello. Safe Policy Iteration, ICML 2013 [Pirotta et al. 2013b] M Pirotta, M Restelli, L Bascetta. Adaptive step-size for policy gradient methods, NIPS 2013 ------------------------ After feedback Thank you very much for the feedback. Now it is clear to me the difference between safe approaches and the proposed algorithm. I suggest you add this explanation to the paper. Hitting time: The assumption of boundness of the first hitting time is very restrictive. I underestimated it during the review because I was not sure about the meaning of the term "uniformly bounded". I checked the reference and uniform boundness of the first hitting time means that the time to reach the absorbing state is almost surely bounded for any stationary policy. I start noticing that due to the settings you considered, every stationary policy induces an absorbing Markov chain (by definition of proper policy, [Bertsekas, 1995, Chap 2, Def 1.1, pag 80]). For absorbing Markov chains it is known that the boundness of the first hitting time (with probability 1 the time to reach the absorbing state is bounded) implies that there are no loops in the chain (every state is visited at most once with probability 1) [see for example App. D of Fruit and Lazaric, Exploration-Exploitation in MDPs with Options, 2017]. You can assume this property but I would like to see this comment in the paper because I believe that the strong implication of this assumption is not well known in general. Moreover, you need to clearly distinguish the theoretical analysis from the empirical results because this assumption seems to be not satisfied in your experiments.
==================================================

Focused review:

1. What does K means in the beginning of introduction? It makes me confusion. 2. The compared method in experiments is just the standard HMC. Is there any existing methods as a baseline to illustrate the advantages of their proposed method?

Review Point: 1. What does K means in the beginning of introduction? It makes me confusion.
Review Point: 2. The compared method in experiments is just the standard HMC. Is there any existing methods as a baseline to illustrate the advantages of their proposed method?
==================================================

Focused review:

Weaknesses:
(W1) The paper is poorly written: there is a large number of typos, broken sentences and bad use of English. The paper would benefit from thorough proofreading and rewriting. Some sentences make no sense or are hard to understand. For reference, here is an incomplete list of sentences and typos:
severe accuracy challenges but diminishing
1 corruption benchmarks
pushing network binarization research to be accurate and efficient
Binarization technology compresses
to fully exert the generic of binarization technology.
We train the 1× number of training epochs
which requires specifically studied in binarization research.
(W2) Clarity: The paper should introduce the concepts more smoothly and walk the reader through them. Many concepts are assumed to be known and dropped without reference. The paper is therefore hard to read. Examples of unclear/unexplained concepts or claims are:
“The most aggressive quantization technology” according to whom?
“imaging modality task”: what is that? how is it defined?
the 1-bit specialization of quantization” - missing reference
“requiring specified local structures” such as?
“model is exported in the ONNX” → Is this a common format? Where does it come from?
What do the colours in Table 2-3 represent?
(W3) The paper would benefit from more rigour. E.g.,
Popcount is never defined
Examples of
α
and
w
should be provided
“The quadratic mean form is uniformly applied in BiBench to unify all metrics.” Why is this a good choice?
(W3) The intuition behind the evaluation metrics in Eq. 2 - 9 is unclear. Why not consider standard deviation as well?
(W4) The related work analysis should be more extensive and explain what are the choices determining the current selection of algorithms. A concurrent work [1] presents other models for binarization in its related work section. How are the algorithms in Table 1 chosen? Why are they representative?
[1] Shang, Y., Xu, D., Zong, Z. and Yan, Y., 2022. Network Binarization via Contrastive Learning. ECCV
(W5) As a benchmark, it should probably compare with other, simpler strategies to make the network more compact. For instance, one such strategy could be dropout or model quantization. There is no need to be exhaustive there, but there should be the possibility for researchers working on Network binarization to assess their methods on more traditional techniques.
(W5) It is not clear to what extent the chosen datasets and tasks are challenging or representative. The paper should elaborate more on why some of the tasks have been chosen. The description in Section 3.1 assumes the reader knows the tasks but does not provide additional information on why they are representative.
(W6) Code: The implementation is not available. A benchmark should provide the code (in this case anonymous) for reproducibility. Moreover, the code has to be clear, well-documented, and easy to run.


Review Point: (W1) The paper is poorly written: there is a large number of typos, broken sentences and bad use of English. The paper would benefit from thorough proofreading and rewriting. Some sentences make no sense or are hard to understand. For reference, here is an incomplete list of sentences and typos: severe accuracy challenges but diminishing 1 corruption benchmarks pushing network binarization research to be accurate and efficient Binarization technology compresses to fully exert the generic of binarization technology. We train the 1× number of training epochs which requires specifically studied in binarization research.
Review Point: (W2) Clarity: The paper should introduce the concepts more smoothly and walk the reader through them. Many concepts are assumed to be known and dropped without reference. The paper is therefore hard to read. Examples of unclear/unexplained concepts or claims are: “The most aggressive quantization technology” according to whom? “imaging modality task”: what is that? how is it defined? the 1-bit specialization of quantization” - missing reference “requiring specified local structures” such as? “model is exported in the ONNX” → Is this a common format? Where does it come from? What do the colours in Table 2-3 represent?
Review Point: (W3) The paper would benefit from more rigour. E.g., Popcount is never defined Examples of α and w should be provided “The quadratic mean form is uniformly applied in BiBench to unify all metrics.” Why is this a good choice?
Review Point: 2 - 9 is unclear. Why not consider standard deviation as well?
Review Point: (W4) The related work analysis should be more extensive and explain what are the choices determining the current selection of algorithms. A concurrent work [1] presents other models for binarization in its related work section. How are the algorithms in Table 1 chosen? Why are they representative? [1] Shang, Y., Xu, D., Zong, Z. and Yan, Y., 2022. Network Binarization via Contrastive Learning. ECCV
Review Point: (W5) As a benchmark, it should probably compare with other, simpler strategies to make the network more compact. For instance, one such strategy could be dropout or model quantization. There is no need to be exhaustive there, but there should be the possibility for researchers working on Network binarization to assess their methods on more traditional techniques.
Review Point: (W5) It is not clear to what extent the chosen datasets and tasks are challenging or representative. The paper should elaborate more on why some of the tasks have been chosen. The description in Section 3.1 assumes the reader knows the tasks but does not provide additional information on why they are representative.
Review Point: (W6) Code: The implementation is not available. A benchmark should provide the code (in this case anonymous) for reproducibility. Moreover, the code has to be clear, well-documented, and easy to run.
==================================================

Focused review:

weaknesses: - The paper has serious clarity issues: (1) It is not clear what "unsupervised" means in the paper. The groundtruth bounding box annotations are provided and there is physics supervision. Which part is unsupervised? Does "unsupervised" mean the lack of 3D bounding box annotations? If so, how is the top part of Fig. 2 trained? (2) It is not clear how the fine-tuning is performed. I searched through the paper and supplementary material, but I couldn't find any detail about the fine-tuning procedure. (3) The details of the REINFORCE algorithm is missing (the supplementary material doesn't add much). How many samples are drawn? What are the distributions that we sample from? How is the loss computed? etc. (4) The evaluation metric is not clear. It seems the stability score is computed by averaging the stability scores of all primitives (according to line 180). Suppose only one box is predicted correctly and the stability score is 1 for that box. Does that mean the overall stability score is 1?  - Another main issue is that the results are not that impressive. The physics supervision, which is the main point of this paper, does not help much. For instance, in Table 2, we have 0.200 vs 0.206 or 0.256 vs 0.261. The fine-tuning procedure is not clear so I cannot comment on those results at this point.  Rebuttal Requests: Please clarify all of the items mentioned above. I will upgrade the rating if these issues are clarified.

Review Point: - The paper has serious clarity issues: (1) It is not clear what "unsupervised" means in the paper. The groundtruth bounding box annotations are provided and there is physics supervision. Which part is unsupervised? Does "unsupervised" mean the lack of 3D bounding box annotations? If so, how is the top part of Fig. 2 trained? (2) It is not clear how the fine-tuning is performed. I searched through the paper and supplementary material, but I couldn't find any detail about the fine-tuning procedure. (3) The details of the REINFORCE algorithm is missing (the supplementary material doesn't add much). How many samples are drawn? What are the distributions that we sample from? How is the loss computed? etc. (4) The evaluation metric is not clear. It seems the stability score is computed by averaging the stability scores of all primitives (according to line 180). Suppose only one box is predicted correctly and the stability score is 1 for that box. Does that mean the overall stability score is 1?
Review Point: - Another main issue is that the results are not that impressive. The physics supervision, which is the main point of this paper, does not help much. For instance, in Table 2, we have 0.200 vs 0.206 or 0.256 vs 0.261. The fine-tuning procedure is not clear so I cannot comment on those results at this point. Rebuttal Requests: Please clarify all of the items mentioned above. I will upgrade the rating if these issues are clarified.
==================================================

Focused review:

Weaknesses:
W1. This paper presents a neat and interesting observation, and a set of experiments to test their hypothesis. However, the implications of this work don't seem very widespread or practically useful for future work. Suppose the hypothesis that model-free agents build maps without explicitly being trained to build them is true. Who cares? Will this change how you train embodied agents in the future?
W2. It's disappointing that the experiments with vision-based agents didn't work out. The hypothesis makes sense, but effectively, the experiments in this paper are on grid-world environments. It would have been nice to show the hypothesis holds with vision-based agents or in more complex tasks like rearrangement.


Review Point: W1. This paper presents a neat and interesting observation, and a set of experiments to test their hypothesis. However, the implications of this work don't seem very widespread or practically useful for future work. Suppose the hypothesis that model-free agents build maps without explicitly being trained to build them is true. Who cares? Will this change how you train embodied agents in the future?
Review Point: W2. It's disappointing that the experiments with vision-based agents didn't work out. The hypothesis makes sense, but effectively, the experiments in this paper are on grid-world environments. It would have been nice to show the hypothesis holds with vision-based agents or in more complex tasks like rearrangement.
==================================================

Focused review:

Weaknesses
W1: The submission has somewhat limited novelty. It can be seen as an application of depthwise convolutions to group-CNNs. Depthwise convolutions are quite popular since at least 2017 (Xception, Mobilenets); and the (non-separable) group-CNN utilized seems the same as Finzi et al [1], with sinusoidal activations. More importantly, the idea of separating the group-convolution along the group dimension has also appeared in Lengyel and van Gemert [2], although for a different flavor of G-CNN.
W2: I see several issues with the experimental section. The most problematic is that all comparisons are against the paper's own baselines. While this makes for fair comparisons in the sense that the data pipeline, architectures and training schedule are all the same (although I have reservations about the number of parameters, see W3), I'm not sure if the conclusions hold in general. For example Weiler and Cesa [3] seems to show significantly better performance than the proposed method on rotated MNIST and CIFAR10/100; can the proposed method improve those results? If not, what would be the applications where the method is useful? Finzi et al [1] also show favorable results on a molecular property prediction, how does the submission fare in that task? When comparing with Finzi et al, I believe there should also be an ablation to disentangle the effects of the separable convolutions and the use of SIREN instead of a regular MLP.
W3: Please report the number of parameters for each model trained. If I understand correctly, the number of channels per layer is kept constant, so the non-separable models have several times more parameters than the separable, is that correct? In that case could the lower performance on non-separable be explained by overfitting or slower convergence? I think both constant number of channels per layer or constant number of parameters are informative and should be reported (Weiler and Cesa [3] do something similar for CIFAR).
W4: In Table 5, I believe a few experiments are missing and would be needed to disentangle the effects of the separation along the group dimension and the channel dimensions. I suggest to show, for each group, the performance when the convolutions are separable over the channel (depthwise) but not the group dimension. And for the baseline, it would be interesting to also see the performance for the separable depthwise version.
Questions
Q1: For the rotated MNIST, it is shown that approximating the convolution with random samples is superior, however the other MNIST variations seem to fall back to the discretization. Why is this the case? Do the random sampling approximation perform worse on the higher dimensional groups?
Q2: The SIM(2) MNIST experiment is described as limited to 2, 4, 6, or 8 elements for each subgroup. Does this also refer to the way the dataset is constructed, or is it created with random rotations and scaling sampled from the continuous internal?
References
[1] Finzi et al, "Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data", ICML'20.
[2] Lengyel and van Gemert, "Exploiting Learned Symmetries in Group Equivariant Convolutions", ICIP'21.
[3] Weiler and Cesa, "General E(2) - Equivariant Steerable CNNs", NeurIPS'19.


Review Point: W1: The submission has somewhat limited novelty. It can be seen as an application of depthwise convolutions to group-CNNs. Depthwise convolutions are quite popular since at least 2017 (Xception, Mobilenets); and the (non-separable) group-CNN utilized seems the same as Finzi et al [1], with sinusoidal activations. More importantly, the idea of separating the group-convolution along the group dimension has also appeared in Lengyel and van Gemert [2], although for a different flavor of G-CNN.
Review Point: W2: I see several issues with the experimental section. The most problematic is that all comparisons are against the paper's own baselines. While this makes for fair comparisons in the sense that the data pipeline, architectures and training schedule are all the same (although I have reservations about the number of parameters, see W3), I'm not sure if the conclusions hold in general. For example Weiler and Cesa [3] seems to show significantly better performance than the proposed method on rotated MNIST and CIFAR10/100; can the proposed method improve those results? If not, what would be the applications where the method is useful? Finzi et al [1] also show favorable results on a molecular property prediction, how does the submission fare in that task? When comparing with Finzi et al, I believe there should also be an ablation to disentangle the effects of the separable convolutions and the use of SIREN instead of a regular MLP.
Review Point: W3: Please report the number of parameters for each model trained. If I understand correctly, the number of channels per layer is kept constant, so the non-separable models have several times more parameters than the separable, is that correct? In that case could the lower performance on non-separable be explained by overfitting or slower convergence? I think both constant number of channels per layer or constant number of parameters are informative and should be reported (Weiler and Cesa [3] do something similar for CIFAR).
Review Point: W4: In Table 5, I believe a few experiments are missing and would be needed to disentangle the effects of the separation along the group dimension and the channel dimensions. I suggest to show, for each group, the performance when the convolutions are separable over the channel (depthwise) but not the group dimension. And for the baseline, it would be interesting to also see the performance for the separable depthwise version. Questions Q1: For the rotated MNIST, it is shown that approximating the convolution with random samples is superior, however the other MNIST variations seem to fall back to the discretization. Why is this the case? Do the random sampling approximation perform worse on the higher dimensional groups?
Review Point: Q2: The SIM(2) MNIST experiment is described as limited to 2, 4, 6, or 8 elements for each subgroup. Does this also refer to the way the dataset is constructed, or is it created with random rotations and scaling sampled from the continuous internal? References [1] Finzi et al, "Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data", ICML'20. [2] Lengyel and van Gemert, "Exploiting Learned Symmetries in Group Equivariant Convolutions", ICIP'21. [3] Weiler and Cesa, "General E(2) - Equivariant Steerable CNNs", NeurIPS'19.
==================================================

Focused review:

Weaknesses 1. I found the experiments rather confusing. Can the authors explain why they chose to show the relative improvement instead accuracy? Also, what is the definition of the relative improvement, is it with respect to the last query? A simple definition would improve the flow. 2. The experiments showing the accuracy vs the number of labels show that the difference with other method is quite small in the range of 1-2% 3. Some of the experiments are stopped too soon when the accuracy curve is still in a sharp slope. It would be interesting if at convergence the TiDal methods indeed reach a higher accuracy then the other methods.


Review Point: 1. I found the experiments rather confusing. Can the authors explain why they chose to show the relative improvement instead accuracy? Also, what is the definition of the relative improvement, is it with respect to the last query? A simple definition would improve the flow.
Review Point: 2. The experiments showing the accuracy vs the number of labels show that the difference with other method is quite small in the range of 1-2% 3. Some of the experiments are stopped too soon when the accuracy curve is still in a sharp slope. It would be interesting if at convergence the TiDal methods indeed reach a higher accuracy then the other methods.
==================================================

Focused review:

Weaknesses:
The overall clarity of the paper can be further improved. I encourage the authors to fully discuss and present the key ideas and results in the main paper rather than putting all the content together with limited explanation and discussion. Due to the space limit, the authors may want to consider moving some minor parts of the paper, e.g., model-based learning, to the appendix.
The proposed algorithms are not well justified in my opinion, e.g., why using recursive TS? How does Farsighter model the uncertainty in exact k steps? Again, this is related to the clarity of the paper, and some technical details need to be clarified or justified (see below for more detailed comments).
The paper can be benefited from either (1) presenting some theoretical backup for their algorithmic design or (2) conducting additional experiments on some interpretable toy environments to help fully understand and validate the algorithmic design (see below for more comments).
Questions/comments that can improve the overall quality of the paper, if adequately addressed in the revision:
[1. Clarity]
1.1 Algorithm 1, line 5, when updating the parameters
θ
with Eqn. (2), you would need to use
γ
k
instead of
γ
in (2), is that right? If not, why not?
1.2 Algorithm 1, line 7, how is the Q-variance measured here? And how to select the Q-variance target
ϵ
in practice? In addition, it seems that whenever the
k
changes, Algorithm 1 would need to empty the data buffer. If
k
keeps changing during the course of learning, would that pose any problems or hurt data efficiency as the buffer may only contain recent 1-2 episode(s) of data?
1.3 In the model-based part, it is not clear to me why solving the nested optimization problem in Eqn. (5) is equivalent to solving the joint optimization problem of higher dimension? What simplification or relaxation was made here (if any)?
1.4 It could be quite challenging to fully discuss and put all different cases into a conference paper with limited space. My general suggestion is that the authors could consider moving minor parts of the paper to the appendix, e.g., delaying the discussion on model-based learning to appendix since the results from model-based experiments were not included in the main paper. By doing so, the authors can have more space to fully discuss their ideas and justify their algorithmic design, or/and include more experimental results.
[2. Algorithmic Design]
2.1 The discussion on one-step uncertainty and multi-step uncertainty is unclear and inaccurate. For example, the authors argued that “Bootstrapped DQN (BootDQN) [1] only considers the uncertainty in next one-step”, which is incorrect technically. BootDQN and its enhanced version Bootstrapped DQN with randomized Priors (BootDQNP) [2], have shown to induce temporally-consistent exploration or deep exploration in the literature, which guides the agent to explore the environment for multiple steps. Specifically, BootDQN/BootDQNP learns an ensemble of M Q-functions that approximate the posterior, and the members of the ensemble would have a larger disagreement in Q values on rarely-visited state-action pairs. Consider a toy example where M = 2 (
Q
1
and
Q
2
), and a state
s
i
which is reachable from some initial state
s
0
and is
i
>
1
steps away from
s
0
. Assume the agent has only seen
s
i
once. In this case,
Q
1
(
s
i
,
⋅
)
would be very different from
Q
2
(
s
i
,
⋅
)
due to high uncertainty on
s
i
. We can further assume
Q
1
(
s
i
,
⋅
)
>
Q
∗
(
s
i
,
⋅
)
and
Q
2
(
s
i
,
⋅
)
<
Q
∗
(
s
i
,
⋅
)
. The “optimism” or overestimation of
Q
1
(
s
i
,
⋅
)
would be propagated back to
Q
1
(
s
0
,
⋅
)
from regular TD updates since
Q
1
,
Q
2
are trained and maintained independently. As a result,
Q
1
(
s
0
,
a
i
)
would be bumped up on
a
i
that leads to the state
s
i
. When TS is applied and
Q
1
is sampled, following greedily the
Q
1
function in one episode would drive the agent to explore state
s
i
, which is
i
>
1
steps away by assumption. In summary, BootDQN/BootDQNP is already doing multi-step exploration in an implicit way, which is referred to as deep exploration in the literature (see Section 5.3.2 and 5.3.3 of [3] for more discussion). In terms of the number of steps
k
, or the planning horizon, one may reason by using the expected length of an episode in the TD update with discount factor
γ
, i.e., roughly
k
∝
1
1
−
γ
, which can be adjusted by changing the discount factor. Therefore, claiming that BootDQN only considers the uncertainty in the next step is incorrect. In fact, the discussion/example here would also apply to other algorithms that can estimate the Bayesian posterior well and are able to propagate properly uncertainty across states. Such an algorithm, when combined with TS, would perform exploration over multiple steps by driving the agent towards states of optimistic values that are
k
>
1
steps away.
2.2 TS in RL is typically applied by sampling one model, e.g., Q-function, from the approximate posterior, and following that sampled model (or Q-function) in the entire next episode to induce efficient exploration. In other words, models or Q-functions are typically resampled per episode [1-4] instead of being resampled per timestep (as in recursive TS used in the paper). One reason is that we would like the agent to have a notion of “commitment” [4] when exploring the environment. Back to the toy example above, following
Q
1
drives the agent to an under-explored state
s
i
while following
Q
2
pushes the agent away from
s
i
. If the agent switches between
Q
1
and
Q
2
multiple times in one episode by resampling per step, it would hinder exploration as the agent did not really commit to any of its “beliefs”, i.e.,
Q
1
or
Q
2
. I am not saying that recursive TS (by resampling per step) cannot be used as a purely empirical method, but clearly it breaks the commitment. Since this is not how TS would usually be applied in RL, I encourage the authors to further justify and explain their design choices of using recursive TS for exploration.
2.3 The proposed algorithm uses (1) recursive TS by resampling a model (e.g., Q-function) at every single step for exploration, and (2) multi-step TD update by aggregating the rewards observed in
k
steps for model updates. However, the interplay between (1) and (2) is not well explained in the paper. Combining with my comments 2.1 and 2.2, it is not clear why the proposed algorithm can estimate the uncertainty in exact
k
future steps. Can you validate that? theoretically or/and empirically?
2.4 What would happen if the algorithm resamples a model every
k
steps? i.e., periodically applying TS every k-step instead of every single step. Would that improve or downgrade the performance? What would happen if the algorithm only resamples per episode? It would be interesting to compare them in the experiments.
2.5 Adaptively changing the value of
k
during training does not seem to be ideal. In practice, it can be tricky to select or finetune the variance target value
ϵ
. If the Bayesian posterior is maintained and estimated accurately throughout the training, it would concentrate as the agent accumulates more and more data, balancing naturally between exploration vs. exploitation. It is therefore unclear why adaptive-k is needed and more clarification can be very helpful.
[3. Experiments]
3.1 The paper can be benefited from additional experimentation on more interpretable and smaller/toy environments to further validate the proposed algorithms. Deep-sea and/or Cartpole swing-up in [2] can be good examples that require multi-step and temporally-consistent exploration. The two environments were made available in bsuite [5]. Particularly, I feel Deep-sea can be a great testbed for evaluating multi-step uncertainty or planning in exact
k
steps. The Deep-sea environments are grid-like and indexed by a problem scale
N
, which require the agent to estimate the uncertainty in the next
N
steps and take actions accordingly. Ideally, the parameter
k
in Farsighter should scale linearly with the problem size
N
, and it would be interesting to see how well Farsighter can perform on Deep-sea, especially when compared with BootDQN/BootDQNP which leverages TS, and OB2I which uses UCB exploration bonus.
3.2 Could you plot the value of
k
in adaptive Farsighter against the “number of training frames” on Montezuma’s Revenge? It would be informative to see how
k
varies during the course of training.
Minor comments:
In Section 3.1, the paper used notation
τ
but without defining it.
In Eqn. (3), the definition of target value
y
i
is not clear. I assume it is the same target value as in (2), but it would be better to make this clear.
Typo in the first sentence of Section 4.5, change “model-free cases” to “model-based cases”.
Figure 4(a) only shows 3e5+ steps of training, but in the text, it mentioned “Farsighter achieves almost 100% success rate and it only takes around 100 million steps”, which seems inconsistent and cannot be told from the figure.
[1] Osband, Ian, et al. "Deep exploration via bootstrapped DQN." Advances in neural information processing systems 29 (2016).
[2] Osband, Ian, John Aslanides, and Albin Cassirer. "Randomized prior functions for deep reinforcement learning." Advances in Neural Information Processing Systems 31 (2018).
[3] Osband, Ian, et al. "Deep Exploration via Randomized Value Functions." J. Mach. Learn. Res. 20.124 (2019): 1-62.
[4] Dimakopoulou, Maria, and Benjamin Van Roy. "Coordinated exploration in concurrent reinforcement learning." International Conference on Machine Learning. PMLR, 2018.
[5] Osband, Ian, et al. "Behaviour suite for reinforcement learning." arXiv preprint arXiv:1908.03568 (2019).


Review Point: The overall clarity of the paper can be further improved. I encourage the authors to fully discuss and present the key ideas and results in the main paper rather than putting all the content together with limited explanation and discussion. Due to the space limit, the authors may want to consider moving some minor parts of the paper, e.g., model-based learning, to the appendix. The proposed algorithms are not well justified in my opinion, e.g., why using recursive TS? How does Farsighter model the uncertainty in exact k steps? Again, this is related to the clarity of the paper, and some technical details need to be clarified or justified (see below for more detailed comments). The paper can be benefited from either (1) presenting some theoretical backup for their algorithmic design or (2) conducting additional experiments on some interpretable toy environments to help fully understand and validate the algorithmic design (see below for more comments). Questions/comments that can improve the overall quality of the paper, if adequately addressed in the revision:
Review Point: [1. Clarity] 1.1 Algorithm 1, line 5, when updating the parameters θ with Eqn. (2), you would need to use γ k instead of γ in (2), is that right? If not, why not? 1.2 Algorithm 1, line 7, how is the Q-variance measured here? And how to select the Q-variance target ϵ in practice? In addition, it seems that whenever the k changes, Algorithm 1 would need to empty the data buffer. If k keeps changing during the course of learning, would that pose any problems or hurt data efficiency as the buffer may only contain recent 1-2 episode(s) of data? 1.3 In the model-based part, it is not clear to me why solving the nested optimization problem in Eqn. (5) is equivalent to solving the joint optimization problem of higher dimension? What simplification or relaxation was made here (if any)? 1.4 It could be quite challenging to fully discuss and put all different cases into a conference paper with limited space. My general suggestion is that the authors could consider moving minor parts of the paper to the appendix, e.g., delaying the discussion on model-based learning to appendix since the results from model-based experiments were not included in the main paper. By doing so, the authors can have more space to fully discuss their ideas and justify their algorithmic design, or/and include more experimental results.
Review Point: [2. Algorithmic Design] 2.1 The discussion on one-step uncertainty and multi-step uncertainty is unclear and inaccurate. For example, the authors argued that “Bootstrapped DQN (BootDQN) [1] only considers the uncertainty in next one-step”, which is incorrect technically. BootDQN and its enhanced version Bootstrapped DQN with randomized Priors (BootDQNP) [2], have shown to induce temporally-consistent exploration or deep exploration in the literature, which guides the agent to explore the environment for multiple steps. Specifically, BootDQN/BootDQNP learns an ensemble of M Q-functions that approximate the posterior, and the members of the ensemble would have a larger disagreement in Q values on rarely-visited state-action pairs. Consider a toy example where M = 2 ( Q 1 and Q 2 ), and a state s i which is reachable from some initial state s 0 and is i > 1 steps away from s 0 . Assume the agent has only seen s i once. In this case, Q 1 ( s i , ⋅ ) would be very different from Q 2 ( s i , ⋅ ) due to high uncertainty on s i . We can further assume Q 1 ( s i , ⋅ ) > Q ∗ ( s i , ⋅ ) and Q 2 ( s i , ⋅ ) < Q ∗ ( s i , ⋅ ) . The “optimism” or overestimation of Q 1 ( s i , ⋅ ) would be propagated back to Q 1 ( s 0 , ⋅ ) from regular TD updates since Q 1 , Q 2 are trained and maintained independently. As a result, Q 1 ( s 0 , a i ) would be bumped up on a i that leads to the state s i . When TS is applied and Q 1 is sampled, following greedily the Q 1 function in one episode would drive the agent to explore state s i , which is i > 1 steps away by assumption. In summary, BootDQN/BootDQNP is already doing multi-step exploration in an implicit way, which is referred to as deep exploration in the literature (see Section 5.3.2 and 5.3.3 of [3] for more discussion). In terms of the number of steps k , or the planning horizon, one may reason by using the expected length of an episode in the TD update with discount factor γ , i.e., roughly k ∝ 1 1 − γ , which can be adjusted by changing the discount factor. Therefore, claiming that BootDQN only considers the uncertainty in the next step is incorrect. In fact, the discussion/example here would also apply to other algorithms that can estimate the Bayesian posterior well and are able to propagate properly uncertainty across states. Such an algorithm, when combined with TS, would perform exploration over multiple steps by driving the agent towards states of optimistic values that are k > 1 steps away. 2.2 TS in RL is typically applied by sampling one model, e.g., Q-function, from the approximate posterior, and following that sampled model (or Q-function) in the entire next episode to induce efficient exploration. In other words, models or Q-functions are typically resampled per episode [1-4] instead of being resampled per timestep (as in recursive TS used in the paper). One reason is that we would like the agent to have a notion of “commitment” [4] when exploring the environment. Back to the toy example above, following Q 1 drives the agent to an under-explored state s i while following Q 2 pushes the agent away from s i . If the agent switches between Q 1 and Q 2 multiple times in one episode by resampling per step, it would hinder exploration as the agent did not really commit to any of its “beliefs”, i.e., Q 1 or Q 2 . I am not saying that recursive TS (by resampling per step) cannot be used as a purely empirical method, but clearly it breaks the commitment. Since this is not how TS would usually be applied in RL, I encourage the authors to further justify and explain their design choices of using recursive TS for exploration. 2.3 The proposed algorithm uses (1) recursive TS by resampling a model (e.g., Q-function) at every single step for exploration, and (2) multi-step TD update by aggregating the rewards observed in k steps for model updates. However, the interplay between (1) and (2) is not well explained in the paper. Combining with my comments 2.1 and 2.2, it is not clear why the proposed algorithm can estimate the uncertainty in exact k future steps. Can you validate that? theoretically or/and empirically? 2.4 What would happen if the algorithm resamples a model every k steps? i.e., periodically applying TS every k-step instead of every single step. Would that improve or downgrade the performance? What would happen if the algorithm only resamples per episode? It would be interesting to compare them in the experiments. 2.5 Adaptively changing the value of k during training does not seem to be ideal. In practice, it can be tricky to select or finetune the variance target value ϵ . If the Bayesian posterior is maintained and estimated accurately throughout the training, it would concentrate as the agent accumulates more and more data, balancing naturally between exploration vs. exploitation. It is therefore unclear why adaptive-k is needed and more clarification can be very helpful.
Review Point: [3. Experiments] 3.1 The paper can be benefited from additional experimentation on more interpretable and smaller/toy environments to further validate the proposed algorithms. Deep-sea and/or Cartpole swing-up in [2] can be good examples that require multi-step and temporally-consistent exploration. The two environments were made available in bsuite [5]. Particularly, I feel Deep-sea can be a great testbed for evaluating multi-step uncertainty or planning in exact k steps. The Deep-sea environments are grid-like and indexed by a problem scale N , which require the agent to estimate the uncertainty in the next N steps and take actions accordingly. Ideally, the parameter k in Farsighter should scale linearly with the problem size N , and it would be interesting to see how well Farsighter can perform on Deep-sea, especially when compared with BootDQN/BootDQNP which leverages TS, and OB2I which uses UCB exploration bonus. 3.2 Could you plot the value of k in adaptive Farsighter against the “number of training frames” on Montezuma’s Revenge? It would be informative to see how k varies during the course of training. Minor comments: In Section 3.1, the paper used notation τ but without defining it. In Eqn. (3), the definition of target value y i is not clear. I assume it is the same target value as in (2), but it would be better to make this clear. Typo in the first sentence of Section 4.5, change “model-free cases” to “model-based cases”. Figure 4(a) only shows 3e5+ steps of training, but in the text, it mentioned “Farsighter achieves almost 100% success rate and it only takes around 100 million steps”, which seems inconsistent and cannot be told from the figure. [1] Osband, Ian, et al. "Deep exploration via bootstrapped DQN." Advances in neural information processing systems 29 (2016). [2] Osband, Ian, John Aslanides, and Albin Cassirer. "Randomized prior functions for deep reinforcement learning." Advances in Neural Information Processing Systems 31 (2018). [3] Osband, Ian, et al. "Deep Exploration via Randomized Value Functions." J. Mach. Learn. Res. 20.124 (2019): 1-62. [4] Dimakopoulou, Maria, and Benjamin Van Roy. "Coordinated exploration in concurrent reinforcement learning." International Conference on Machine Learning. PMLR, 2018. [5] Osband, Ian, et al. "Behaviour suite for reinforcement learning." arXiv preprint arXiv:1908.03568 (2019).
==================================================

Focused review:

weakness is the lack of clear motivation: the authors consider a very specific setup, with two proximable functions and one non-bilinear joint term, certain assumptions about each of the functions, and I don't feel that the choices were sufficiently motivated. Still, I vote for acceptance because, in my opinion, the topic is underexplored.
Additional comments
The authors wrote in Section 1.1 that they "propose a proximal-GDA algorithm", and I find this choice of words to be confusing: this algorithm is by far not new as it is a well-known special case of the Forward-Backward iteration, which dates back to (1979, "Splitting algorithms for the sum of two nonlinear operators"). I expect the authors to properly cite the related literature on Forward-Backaward and mention the guarantees that follow from the existing bounds on Forward-Backward algorithm (which has been quite extensively studied).
The Lyapunov function in Proposition 2 is quite interesting, but the quadratic term is not really explained. I think the paper would benefit from explaining why this term shows up in the proofs.
The bounds in Theorem 1 and other results seem to have a terrible dependency on the conditioning. In the strongly monotone case, the stepsize in GDA has to be of order O(1/(kappaL)), but here it is O(1/(kappa^3L)). This is not completely unexpected due to the nonconvexity, but I'm wondering if the authors think that the bounds are tight or they can be improved.
The bad conditioning in the bounds seems to be coming from the update of GDA, and a better update should lead to faster rates. I think that the extragradient algorithm should have the right conditioning with stepsize O(1/L), and several methods have been proposed recently that achieve O(kappa) bounds: 1) "Reducing noise in GAN training with variance reduced extragradient", 2) "Revisiting stochastic extragradient", 3) "On the convergence of single-call stochastic extra-gradient methods". I'd appreciate if the authors could comment on that.
The authors wrote "The Kurdyka-Łojasiewicz (KŁ) geometry <...> has been shown to hold ubiquitously for most practical functions" -- I think this is a big overstatement, I haven't seen any example that would be relevant to the minmax optimization.
The authors assume that y*(x) is differentiable in Section 4. First of all, please make this assumption more explicit as it's very easy to miss, particularly so because the other assumptions are presented in Assumption 1. Secondly, I'm wondering if the assumption that y*(x) is differentiable is really necessary. The authors prove that y* is Lipschitz, so assuming differentiability makes the class of relevant objectives small. Secondly, as far as I can see, differentiability is only used in the proof of theorem 2, and the authors only need f1(x)=  y-y*(x)  ^2 to be differentiable and to satisfy   nabla f1(x)   <= const * f1(x). It might be possible to relax the assumption a little bit (for example   x  ^2 is differentiable everywhere but   x   isn't).
Remark 1 is not precisely correct: to relax the assumption, function h would have to be strongly concave with constant mu>L as otherwise adding gamma*h to f with gamma<1 will not make f strongly concave.
Minor issues
Abstract: "it is lack of understanding" -> "it lacks understanding" p.2, "the entire variable sequences of proximal-GDA have a unique limit point" -> "sequence ... has" p.6, "proximal-GDA admits a very important Lyapunov function" -- it's a bit strange to call the Lyapunov function "very important"


Review Point: is the lack of clear motivation: the authors consider a very specific setup, with two proximable functions and one non-bilinear joint term, certain assumptions about each of the functions, and I don't feel that the choices were sufficiently motivated. Still, I vote for acceptance because, in my opinion, the topic is underexplored. Additional comments The authors wrote in Section 1.1 that they "propose a proximal-GDA algorithm", and I find this choice of words to be confusing: this algorithm is by far not new as it is a well-known special case of the Forward-Backward iteration, which dates back to (1979, "Splitting algorithms for the sum of two nonlinear operators"). I expect the authors to properly cite the related literature on Forward-Backaward and mention the guarantees that follow from the existing bounds on Forward-Backward algorithm (which has been quite extensively studied). The Lyapunov function in Proposition 2 is quite interesting, but the quadratic term is not really explained. I think the paper would benefit from explaining why this term shows up in the proofs. The bounds in Theorem 1 and other results seem to have a terrible dependency on the conditioning. In the strongly monotone case, the stepsize in GDA has to be of order O(1/(kappaL)), but here it is O(1/(kappa^3L)). This is not completely unexpected due to the nonconvexity, but I'm wondering if the authors think that the bounds are tight or they can be improved. The bad conditioning in the bounds seems to be coming from the update of GDA, and a better update should lead to faster rates. I think that the extragradient algorithm should have the right conditioning with stepsize O(1/L), and several methods have been proposed recently that achieve O(kappa) bounds:
Review Point: 1) "Reducing noise in GAN training with variance reduced extragradient", 2) "Revisiting stochastic extragradient", 3) "On the convergence of single-call stochastic extra-gradient methods". I'd appreciate if the authors could comment on that. The authors wrote "The Kurdyka-Łojasiewicz (KŁ) geometry <...> has been shown to hold ubiquitously for most practical functions" -- I think this is a big overstatement, I haven't seen any example that would be relevant to the minmax optimization. The authors assume that y*(x) is differentiable in Section 4. First of all, please make this assumption more explicit as it's very easy to miss, particularly so because the other assumptions are presented in Assumption 1. Secondly, I'm wondering if the assumption that y*(x) is differentiable is really necessary. The authors prove that y* is Lipschitz, so assuming differentiability makes the class of relevant objectives small. Secondly, as far as I can see, differentiability is only used in the proof of theorem 2, and the authors only need f1(x)= y-y*(x) ^2 to be differentiable and to satisfy nabla f1(x) <= const * f1(x). It might be possible to relax the assumption a little bit (for example x ^2 is differentiable everywhere but x isn't). Remark 1 is not precisely correct: to relax the assumption, function h would have to be strongly concave with constant mu>L as otherwise adding gamma*h to f with gamma<1 will not make f strongly concave. Minor issues Abstract: "it is lack of understanding" -> "it lacks understanding" p.2, "the entire variable sequences of proximal-GDA have a unique limit point" -> "sequence ... has" p.6, "proximal-GDA admits a very important Lyapunov function" -- it's a bit strange to call the Lyapunov function "very important"
==================================================

Focused review:

Weaknesses 1. While the paper claim the importance of language modeling capability of pre-trained models, the authors did not conduct experments on generation tasks that are more likely to require a well-performing language model. Experiments on word similarity and SquAD in section 5.3 cannot really reflect the capability of language modeling. The authors may consider to include tasks like language modeling, machine translation or text sumarization to strenghen this part, as this is one of the main motivations of COCO-LM. 2. Analysis of SCL in section 5.2 regarding few-shot abaility looks not convincing. The paper claims that a more regularized representation space by SCL may result in better generalization ability in few-shot scenarios. However, results in Figure 7(c) and (d) do not meet our expectation such that COCO-LM achieves much more improvements with less labels and the improvements will gradually disappear with more labels. Besides, the authors may check if COCO-LM brings benefits to sentence retrieval tasks with the learned anisotropy text representations. 3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works.
Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance? 2. In Table 2, it looks like COCO-LM especially affects the performance on CoLA and RTE hence the final performance. Can the authors provide some explanation on how the proposed pre-training tasks affect the two different GLEU tasks? 3. In section 5.1, the authors say that the benefits of the stop gradient operation are more on stability. What stability, the training process? If so, are there any learning curves of COCO-LM with and without stop gradient during pre-training to support this claim? 4. In section 5.2, the term “Data Argumentation” seems wrong. Did the authors mean data augmentation?
Typos 1. Check the term “Argumentation” in line 164, 252, and 314. 2. Line 283, “a unbalanced task”, should be “an unbalanced task”. 3. Line 326, “contrast pairs”, should be “contrastive pairs” to be consistent throughout the paper?


Review Point: 1. While the paper claim the importance of language modeling capability of pre-trained models, the authors did not conduct experments on generation tasks that are more likely to require a well-performing language model. Experiments on word similarity and SquAD in section 5.3 cannot really reflect the capability of language modeling. The authors may consider to include tasks like language modeling, machine translation or text sumarization to strenghen this part, as this is one of the main motivations of COCO-LM.
Review Point: 2. Analysis of SCL in section 5.2 regarding few-shot abaility looks not convincing. The paper claims that a more regularized representation space by SCL may result in better generalization ability in few-shot scenarios. However, results in Figure 7(c) and (d) do not meet our expectation such that COCO-LM achieves much more improvements with less labels and the improvements will gradually disappear with more labels. Besides, the authors may check if COCO-LM brings benefits to sentence retrieval tasks with the learned anisotropy text representations.
Review Point: 3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works. Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance?
Review Point: 2. In Table 2, it looks like COCO-LM especially affects the performance on CoLA and RTE hence the final performance. Can the authors provide some explanation on how the proposed pre-training tasks affect the two different GLEU tasks?
Review Point: 3. In section 5.1, the authors say that the benefits of the stop gradient operation are more on stability. What stability, the training process? If so, are there any learning curves of COCO-LM with and without stop gradient during pre-training to support this claim?
Review Point: 4. In section 5.2, the term “Data Argumentation” seems wrong. Did the authors mean data augmentation? Typos 1. Check the term “Argumentation” in line 164, 252, and 314.
Review Point: 2. Line 283, “a unbalanced task”, should be “an unbalanced task”.
Review Point: 3. Line 326, “contrast pairs”, should be “contrastive pairs” to be consistent throughout the paper?
==================================================

Focused review:

Weaknesses
W1 - Method
W1.1: One major drawback of this optimization-based solution is that the same optimization procedure is also required at inference time, even though the authors show that even one iteration can already produce accurate results. How fast is each optimization iteration?
W1.2: Does this optimization always yield a good solution? It seems to me the camera viewpoint can fall into local minima with BFGS optimization as well. Moreover, it relies heavily on the predicted correspondences, and cannot be robust against erroneous correspondences as a learning approach could with learned priors.
W1.3: Another drawback is the way the correspondences are predicted using points. It does not guarantee continuity, as is also shown in the visualizations where the points can get messy in some parts. CSM [15] and part segmentation [16] have the advantage of producing smoother correspondences with an image. This also makes more concerned with the robustness of the optimization against errors in correspondence prediction.
W1.4: One more major limitation is the need for a category template shape similar to [15] and [16]. This has largely simplified the problem of category-specific shape reconstruction and at the same time restricted the reconstructed shape from deviating too much from the template. The reconstructed airplanes for example are almost always the same despite the variation of the inputs (see Fig 9 in the sup. mat.).
W2 - Evaluation
The evaluation has a number of limitations.
W2.1: Why not compare with UMR[16]? It obtains part correspondences through a self-supervised part segmentation model, and does not require a template shape.
W2.2: The numerical results are not very convincing. Mask mIoU only measures the shape from one single viewpoint, and does not evaluate the 3D shape. Keypoint reprojection is also very sparse and noisy, and may not be indicative of the 3D shape accuracy. It is unclear whether the comparison on these noisy metrics is actually meaningful.
W2.3: The evaluation on Pascal3D+ with 3D mIoU is more meaningful (although the 3D shape is also "pseudo GT"), but the gap is relatively small. But more importantly, I wonder how well the template shape alone achieves in terms of metric?
W2.4: Most of the visual results are visualized in the input viewpoint, which makes it hard to tell the how good the reconstructed 3D shapes are. From the only one novel view provided in Fig. 2 and 3 and in the sup. mat., the reconstructed 3D shapes do not look very plausible. The birds for example appear quite flat. The authors should provide videos or visualizations with multiple viewpoints to showcase the reconstructed 3D shapes. How do they compare to UMR and ACMR visually?
It probably also makes sense to plot distribution of the predicted viewpoints. Is the model confused with some viewpoint, eg front vs back, left vs right?
It could also help with understanding the pose and shape prediction to color the predicted mesh with the same color pattern in the template mesh.
Clarification needed
How exactly is texture symmetry enforced? The method samples texture from the input image using the predicted 2D points. In line 182, it briefly describes the texture symmetry enforced by averaging textures from two symmetric viewpoints. Does it mean taking the average of the sampled colors of two symmetric points (defined by the template)? If so, does this also rely on the visibility prediction? More details should be provided.
This is interesting because texture symmetry alone can provide pretty strong cue for shape and pose. Does the model utilize this cue for reconstruction?
There are 9 loss terms in total (if not more). How are they balanced?
Additional comments
Eq (3):
V
should be
T
, input
C
should be lower case
c
.
Line 207: "vertical flipping"? ie, about the horizontal axis and the objects would be upside down?

Post-Rebuttal
I appreciate the authors' efforts in the rebuttal, which has addressed many of my questions, including the computational cost, comparison with UMR, and template shape baseline performance on Pascal3D+. Regarding the robustness, it is still not clear how robust the correspondence prediction and subsequently the pose estimation are. This is hard to quantify, but more discussion on failure cases in the final version would be very helpful.
Overall, this paper has sufficient interesting material for acceptance. I will keep my original review as borderline accept.
A few failure examples are provided in the supplementary, but they all seem very challenging cases, eg flying birds, heavy occlusion and small crops. Are there failure cases on more regular images? Does the viewpoint prediction get stuck in local minima? Does the correspondence prediction fail dramatically, and what happens in the shape and pose optimization then?


Review Point: W1 - Method W1.1: One major drawback of this optimization-based solution is that the same optimization procedure is also required at inference time, even though the authors show that even one iteration can already produce accurate results. How fast is each optimization iteration?
Review Point: W1.2: Does this optimization always yield a good solution? It seems to me the camera viewpoint can fall into local minima with BFGS optimization as well. Moreover, it relies heavily on the predicted correspondences, and cannot be robust against erroneous correspondences as a learning approach could with learned priors.
Review Point: W1.3: Another drawback is the way the correspondences are predicted using points. It does not guarantee continuity, as is also shown in the visualizations where the points can get messy in some parts. CSM [15] and part segmentation [16] have the advantage of producing smoother correspondences with an image. This also makes more concerned with the robustness of the optimization against errors in correspondence prediction.
Review Point: W1.4: One more major limitation is the need for a category template shape similar to [15] and [16]. This has largely simplified the problem of category-specific shape reconstruction and at the same time restricted the reconstructed shape from deviating too much from the template. The reconstructed airplanes for example are almost always the same despite the variation of the inputs (see Fig 9 in the sup. mat.). W2 - Evaluation The evaluation has a number of limitations.
Review Point: W2.1: Why not compare with UMR[16]? It obtains part correspondences through a self-supervised part segmentation model, and does not require a template shape.
Review Point: W2.2: The numerical results are not very convincing. Mask mIoU only measures the shape from one single viewpoint, and does not evaluate the 3D shape. Keypoint reprojection is also very sparse and noisy, and may not be indicative of the 3D shape accuracy. It is unclear whether the comparison on these noisy metrics is actually meaningful.
Review Point: W2.3: The evaluation on Pascal3D+ with 3D mIoU is more meaningful (although the 3D shape is also "pseudo GT"), but the gap is relatively small. But more importantly, I wonder how well the template shape alone achieves in terms of metric?
Review Point: W2.4: Most of the visual results are visualized in the input viewpoint, which makes it hard to tell the how good the reconstructed 3D shapes are. From the only one novel view provided in Fig. 2 and 3 and in the sup. mat., the reconstructed 3D shapes do not look very plausible. The birds for example appear quite flat. The authors should provide videos or visualizations with multiple viewpoints to showcase the reconstructed 3D shapes. How do they compare to UMR and ACMR visually? It probably also makes sense to plot distribution of the predicted viewpoints. Is the model confused with some viewpoint, eg front vs back, left vs right? It could also help with understanding the pose and shape prediction to color the predicted mesh with the same color pattern in the template mesh. Clarification needed How exactly is texture symmetry enforced? The method samples texture from the input image using the predicted 2D points. In line 182, it briefly describes the texture symmetry enforced by averaging textures from two symmetric viewpoints. Does it mean taking the average of the sampled colors of two symmetric points (defined by the template)? If so, does this also rely on the visibility prediction? More details should be provided. This is interesting because texture symmetry alone can provide pretty strong cue for shape and pose. Does the model utilize this cue for reconstruction? There are 9 loss terms in total (if not more). How are they balanced? Additional comments Eq (3): V should be T , input C should be lower case c . Line 207: "vertical flipping"? ie, about the horizontal axis and the objects would be upside down? Post-Rebuttal I appreciate the authors' efforts in the rebuttal, which has addressed many of my questions, including the computational cost, comparison with UMR, and template shape baseline performance on Pascal3D+. Regarding the robustness, it is still not clear how robust the correspondence prediction and subsequently the pose estimation are. This is hard to quantify, but more discussion on failure cases in the final version would be very helpful. Overall, this paper has sufficient interesting material for acceptance. I will keep my original review as borderline accept. A few failure examples are provided in the supplementary, but they all seem very challenging cases, eg flying birds, heavy occlusion and small crops. Are there failure cases on more regular images? Does the viewpoint prediction get stuck in local minima? Does the correspondence prediction fail dramatically, and what happens in the shape and pose optimization then?
==================================================

Focused review:

1. There is the following recent paper that came up with optimal first order methods for the same overconstrained L2 regression: https://proceedings.icml.cc/static/paper_files/icml/2020/3430-Paper.pdf. In the above paper, the authors used a fixed sketch (in contrast to the current work in which authors used different sketching matrices generated independently in each iteration) which achieved similar or better convergence rate than this paper. The above paper was based on a larger class of pre-conditioned quasi-Newton like methods. While the analyses of these two papers are somewhat different from each other, I am wondering why would one care about the variable sketches (i.e. the current work), rather than the fixed sketch (i.e. the paper mentioned above) that performs better both in terms of the convergence rate and the computation (as one has to generate S only once)? 2. Can this analysis be also extended to accommodate count-sketch or any other sparse sketching matrices ?

Review Point: 1. There is the following recent paper that came up with optimal first order methods for the same overconstrained L2 regression: https://proceedings.icml.cc/static/paper_files/icml/2020/3430-Paper.pdf. In the above paper, the authors used a fixed sketch (in contrast to the current work in which authors used different sketching matrices generated independently in each iteration) which achieved similar or better convergence rate than this paper. The above paper was based on a larger class of pre-conditioned quasi-Newton like methods. While the analyses of these two papers are somewhat different from each other, I am wondering why would one care about the variable sketches (i.e. the current work), rather than the fixed sketch (i.e. the paper mentioned above) that performs better both in terms of the convergence rate and the computation (as one has to generate S only once)?
Review Point: 2. Can this analysis be also extended to accommodate count-sketch or any other sparse sketching matrices ?
==================================================

Focused review:

Weaknesses
The intuition figure 1 is a bit hard to fully understand. For the first row, when scaling up the sample size, does it mean we keep the same number of pre-train graphs but we vary the same portion of sample size from each pre-train graph dataset? For the bottom row, the total number of pre-train graphs in the paper is eleven and the number of graphs in the figure is only up to ten. The author should provide what is the benchmark results if we use all the possible pre-train datasets to properly compare
Directly using contrastive loss to define and measure predictive uncertainty is not questionable.
The notation of this paper is a bit messy. For example, both scaler and vector use non-bolded lowercase (degree vector and node degree) which is not conventional.
The selection mechanism is a bit complicated and heavily handcrafted. Currently, the graph selection policy uses a combination of 6 loss terms in total. Besides, loss terms work on a very different scale and are combined together with an additional time-adaptive parameter. I am not sure if this design is reasonable. Empirically, there is no proper ablation study on the choice of which graph property to be included in the pre-train graph selection criteria.
There is an additional proximal regularization term in the model design, which aims to better preserve the knowledge or information contained in previous input data when we train on new incoming data, a similar design component in the continue learning paradigm. I am not able to fully follow the rationale of this design. The pre-training problem is completely different from the catastrophic forgetting setting since we will normally shuffle the training order of the samples after each epoch. The claim of "previous input data will be forgotten or covered by new incoming data" is invalid if we shuffle the data training order. Besides, the
θ
parameters learned from the first
j
graphs, does it mean in terms of the memory complexity of the model will be
j
times larger since we need to store the previous training iteration of the model?
Experimental results-wise, I have several concerns: 1) It seems the work is directly established on top of the GCC paper, but with completely different suits of pre-training datasets and downstream datasets. The choice of dataset section seems arbitrary and suspicious. Could the author directly work on the precious experiment setting in GCC? or what's the reason behind the complete switch of datasets?
The baseline uses are quite outdated, using baselines only coming before the year 2020. A lot of recent work in terms of GNN pre-training or graph data augmentation should be included [1] - [8]. 3) One suggestion to the authors, since the paper uses a lot of downstream tasks and lots of numbers in the result table, a better way to present the results can be considered. For example, providing an average rank number across all the datasets for each method will be informative and helpful to deliver the message to readers. 3) I will suggest including the full pre-training dataset results in the experiment table to see the effectiveness of the pre-train graph selection scheme. Or should I interpret the GCC results as the full pre-training dataset results? 4) Can the author provide the training time comparison? It seems that to fully
Can the author comment on the relationship of this work to some recent adaptive graph positive sample generation work? It seems they all achieve a similar end goal by finding the proper training samples [3] [5] [6] [7].
[1] Hu, Ziniu, et al. "GPT-GNN: Generative pre-training of graph neural networks." KDD 2020.
[2] Xu, Dongkuan, et al. "Infogcl: Information-aware graph contrastive learning." NeurIPS 2021.
[3] Zhu, Yanqiao, et al. "Graph contrastive learning with adaptive augmentation." Proceedings of the Web Conference 2021. 2021.
[4] Zhu, Yanqiao, et al. "An Empirical Study of Graph Contrastive Learning." NeurIPS 2021.
[5] You, Yuning, et al. "Graph contrastive learning automated." ICML 2021.
[6] Lee N, Lee J, Park C. Augmentation-free self-supervised learning on graphs. In Proc. of the AAAI Conference on Artificial Intelligence 2022.
[7] Han et al. "G-Mixup: Graph Data Augmentation for Graph Classification." ICML 2022.
[8] Hou, Zhenyu, et al. "GraphMAE: Self-Supervised Masked Graph Autoencoders." KDD 2022.


Review Point: The intuition figure 1 is a bit hard to fully understand. For the first row, when scaling up the sample size, does it mean we keep the same number of pre-train graphs but we vary the same portion of sample size from each pre-train graph dataset? For the bottom row, the total number of pre-train graphs in the paper is eleven and the number of graphs in the figure is only up to ten. The author should provide what is the benchmark results if we use all the possible pre-train datasets to properly compare Directly using contrastive loss to define and measure predictive uncertainty is not questionable. The notation of this paper is a bit messy. For example, both scaler and vector use non-bolded lowercase (degree vector and node degree) which is not conventional. The selection mechanism is a bit complicated and heavily handcrafted. Currently, the graph selection policy uses a combination of 6 loss terms in total. Besides, loss terms work on a very different scale and are combined together with an additional time-adaptive parameter. I am not sure if this design is reasonable. Empirically, there is no proper ablation study on the choice of which graph property to be included in the pre-train graph selection criteria. There is an additional proximal regularization term in the model design, which aims to better preserve the knowledge or information contained in previous input data when we train on new incoming data, a similar design component in the continue learning paradigm. I am not able to fully follow the rationale of this design. The pre-training problem is completely different from the catastrophic forgetting setting since we will normally shuffle the training order of the samples after each epoch. The claim of "previous input data will be forgotten or covered by new incoming data" is invalid if we shuffle the data training order. Besides, the θ parameters learned from the first j graphs, does it mean in terms of the memory complexity of the model will be j times larger since we need to store the previous training iteration of the model? Experimental results-wise, I have several concerns:
Review Point: 1) It seems the work is directly established on top of the GCC paper, but with completely different suits of pre-training datasets and downstream datasets. The choice of dataset section seems arbitrary and suspicious. Could the author directly work on the precious experiment setting in GCC? or what's the reason behind the complete switch of datasets? The baseline uses are quite outdated, using baselines only coming before the year 2020. A lot of recent work in terms of GNN pre-training or graph data augmentation should be included [1] - [8].
Review Point: 3) One suggestion to the authors, since the paper uses a lot of downstream tasks and lots of numbers in the result table, a better way to present the results can be considered. For example, providing an average rank number across all the datasets for each method will be informative and helpful to deliver the message to readers.
Review Point: 3) I will suggest including the full pre-training dataset results in the experiment table to see the effectiveness of the pre-train graph selection scheme. Or should I interpret the GCC results as the full pre-training dataset results?
Review Point: 4) Can the author provide the training time comparison? It seems that to fully Can the author comment on the relationship of this work to some recent adaptive graph positive sample generation work? It seems they all achieve a similar end goal by finding the proper training samples [3] [5] [6] [7]. [1] Hu, Ziniu, et al. "GPT-GNN: Generative pre-training of graph neural networks." KDD 2020. [2] Xu, Dongkuan, et al. "Infogcl: Information-aware graph contrastive learning." NeurIPS 2021. [3] Zhu, Yanqiao, et al. "Graph contrastive learning with adaptive augmentation." Proceedings of the Web Conference 2021. 2021. [4] Zhu, Yanqiao, et al. "An Empirical Study of Graph Contrastive Learning." NeurIPS 2021. [5] You, Yuning, et al. "Graph contrastive learning automated." ICML 2021. [6] Lee N, Lee J, Park C. Augmentation-free self-supervised learning on graphs. In Proc. of the AAAI Conference on Artificial Intelligence 2022. [7] Han et al. "G-Mixup: Graph Data Augmentation for Graph Classification." ICML 2022. [8] Hou, Zhenyu, et al. "GraphMAE: Self-Supervised Masked Graph Autoencoders." KDD 2022.
==================================================

Focused review:

Weaknesses:
Clarity.
I have several concerns about the paper, but first I have fundamental questions that I was not able to resolve based on the paper or its appendices.
The usual way to finetune BERT for a downstream task is to finetune all of its parameters while learning the new parameters added for the downstream task. The usual way to use ELMo is to keep the ELMo parameters frozen and learn only the layer weights. Unfortunately, I was not able to determine based on the paper whether the experiments involve full finetuning or only learning of the layer weights. It's not at all obvious based on context, so this detail really needs to be clarified for each experiment in the paper. For example, for probing tasks like those in 4.1, researchers will often freeze the pretrained model parameters and only learn the probing classifier parameters, while for the NLP tasks considered in 4.2, researchers typically do full fine-tuning of all parameters. Therefore, it's really essential for the paper to clarify this detail. 
Relatedly, what model architecture is used for the probing tasks? BERT can provide a vector at each layer for each position in the sequence, including the CLS token position. What vector or function of vectors is used as input to the classifier? Some people average over positions and layers when using BERT for probing tasks, while others average over positions in the final layer, and others just use the hidden vector at the CLS position (either taking the final layer or averaging over layers). For BERMo there is also a choice among averaging over positions or choosing the CLS position, among other possibilities.  Also, what is the architecture of the probing classifier in 4.1 and the other classifiers in 4.2? E.g., do they have any hidden layers? 
However, even if the above questions are resolved, I still have significant concerns about the significance of the paper, described below.
Significance of the key methodological idea.
 
The central idea---using a learnable weighted combination of layers with BERT---is quite common in NLP. The submission cites some related work that has done this in the past, but there is other work that is uncited that also does this. For example, Peters et al. (2019; https://aclanthology.org/W19-4302/) compared learning layer weights in a weighted layer combination with BERT (with fixed BERT parameters) to full fine-tuning with BERT across several NLP tasks.  Tenney et al. (from ICLR 2019, different from the Tenney et al paper that is cited; https://arxiv.org/abs/1905.06316) also used a learned weighted combination of layers with BERT. I think it would be good for this submission to be rewritten in light of the central idea already being well known in the community. It's certainly less common in my experience to use a learnable weighted layer combination in the context of full finetuning, but I am unsure whether the experiments in the paper involve full finetuning. 
Mixed experimental results.
 
The learnable weighted combination (BERMo) outperforms BERT on semantic probing tasks, but not necessarily on the other probing tasks and not consistently on downstream NLP tasks in a standard setting (see Tables 1 and 2). Also, in the knowledge distillation experiments (Table 5), the results are similar between BERT and BERMo. 
Lack of motivation for the application of BERMo to parameter pruning experimental settings.
 
Many of the experiments in the paper consider various weight pruning methods from prior work. I was a little confused as to why a paper focusing on BERMo would include so many experiments with parameter pruning. Why is it natural to consider parameter pruning experiments when using BERMo? The pruning experiments were not motivated using an argument that relates to the BERMo architecture itself. 
It seems that the connection is possibly due to the relationship between skip connections and pruning? There is discussion throughout the paper about how the BERMo architecture achieves the benefits of skip/residual connections. However, this can be confusing to readers because the transformer architecture already contains residual connections for each layer. Would the authors argue that the layer-weighted architecture is a superior style of skip connections than the residual connections that are already in BERT via its use of the transformer architecture? The paper really needs to discuss why BERMo's skip connections are expected to be more amenable to pruning than the skip connections already present in BERT. 
Lack of analysis.
 
All the above being said, the pruning experiments do show some significant differences between BERT and BERMo. This is a potentially interesting result. Why might this be the case? It would be nice if the paper could include some ablation experiments that modify the architecture in various ways in order to determine what exactly it is about the BERMo architecture that makes it more amenable to pruning. 
I had a few other questions/suggestions:
Section 4 mentions a "mask learning rate" but it is not described -- what is it?
In Sec. 4.2.4, the paper states "Our approach gives better results on SQuAD". Is a difference of 0.16 F1 statistically significant?
For QQP, Tables 2 and 4 show accuracies being consistently higher than F1 scores, while Table 5 shows the opposite. Were the numbers in Table 5 inadvertently transposed?
Unfortunately, BERT-base is not a very strong model. I'd suggest using RoBERTa-base or DeBERTa-base instead if larger models cannot be used. 
Below are more minor notes:
The citations are formatted oddly: "(Vaswani et al. (2017))". Maybe this is due to a use of citet within parentheses? It should be "(Vaswani et al., 2017)" or "Vaswani et al. (2017)" depending on context.
Equation (1) is not contained in the flow of the paper but is rather set off in a figure. It's not common to render an equation in a floating item such as a figure unless there is a series of related equations shown together for comparative purposes. I'd suggest moving Equation (1) to be in the flow of the paper rather than off to the side. 
Equation (1) has bold, non-italicized h, while the caption has non-boldface, italicized h. 
Sec. 2.2: "to distinguishing" --> "to distinguish" or "for distinguishing"
Sec. 2.3.2: Use \log instead of log in Eq 3.
Sec. 3.1: The term "softmax normalized" is used but the equation shown immediately thereafter is just standard normalization---there is no softmax. 
Sec. 3.1: "features maps" --> "feature maps"
Sec. 4: "higher learning rates for the skip connections" -- do you mean learning rates for updating the alpha parameters?
Sec. 4: "various experiment" --> "various experiments"
Sec. 4.1: "5 trails" --> "5 trials"
Sec. 4.1: "our approach outperform" --> "our approach outperforms"
Sec. 4.2.2: "smaller dataset" --> "smaller datasets"
Sec. 4.2.2: "at the end the fine-pruning" --> "at the end of fine-pruning"
Sec. 4.2.2: "roughtly" --> "roughly"
Sec. 4.2.2: "is lesser epochs" --> "in lesser epochs"?
Sec. 4.2.3: "the both the" --> "both the"
Sec. 4.2.3: "weights retain" --> "weights retained"
References:
@inproceedings{peters-etal-2019-tune,     title = "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",     author = "Peters, Matthew E.  and       Ruder, Sebastian  and       Smith, Noah A.",     booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",     month = aug,     year = "2019",     address = "Florence, Italy",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/W19-4302",     doi = "10.18653/v1/W19-4302",     pages = "7--14",     abstract = "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.", }
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. "WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS", ICLR 2019.


Review Point: Clarity. I have several concerns about the paper, but first I have fundamental questions that I was not able to resolve based on the paper or its appendices. The usual way to finetune BERT for a downstream task is to finetune all of its parameters while learning the new parameters added for the downstream task. The usual way to use ELMo is to keep the ELMo parameters frozen and learn only the layer weights. Unfortunately, I was not able to determine based on the paper whether the experiments involve full finetuning or only learning of the layer weights. It's not at all obvious based on context, so this detail really needs to be clarified for each experiment in the paper. For example, for probing tasks like those in 4.1, researchers will often freeze the pretrained model parameters and only learn the probing classifier parameters, while for the NLP tasks considered in 4.2, researchers typically do full fine-tuning of all parameters. Therefore, it's really essential for the paper to clarify this detail. Relatedly, what model architecture is used for the probing tasks? BERT can provide a vector at each layer for each position in the sequence, including the CLS token position. What vector or function of vectors is used as input to the classifier? Some people average over positions and layers when using BERT for probing tasks, while others average over positions in the final layer, and others just use the hidden vector at the CLS position (either taking the final layer or averaging over layers). For BERMo there is also a choice among averaging over positions or choosing the CLS position, among other possibilities. Also, what is the architecture of the probing classifier in 4.1 and the other classifiers in 4.2? E.g., do they have any hidden layers? However, even if the above questions are resolved, I still have significant concerns about the significance of the paper, described below. Significance of the key methodological idea. The central idea---using a learnable weighted combination of layers with BERT---is quite common in NLP. The submission cites some related work that has done this in the past, but there is other work that is uncited that also does this. For example, Peters et al. (2019; https://aclanthology.org/W19-4302/) compared learning layer weights in a weighted layer combination with BERT (with fixed BERT parameters) to full fine-tuning with BERT across several NLP tasks. Tenney et al. (from ICLR 2019, different from the Tenney et al paper that is cited; https://arxiv.org/abs/1905.06316) also used a learned weighted combination of layers with BERT. I think it would be good for this submission to be rewritten in light of the central idea already being well known in the community. It's certainly less common in my experience to use a learnable weighted layer combination in the context of full finetuning, but I am unsure whether the experiments in the paper involve full finetuning. Mixed experimental results. The learnable weighted combination (BERMo) outperforms BERT on semantic probing tasks, but not necessarily on the other probing tasks and not consistently on downstream NLP tasks in a standard setting (see Tables 1 and 2). Also, in the knowledge distillation experiments (Table 5), the results are similar between BERT and BERMo. Lack of motivation for the application of BERMo to parameter pruning experimental settings. Many of the experiments in the paper consider various weight pruning methods from prior work. I was a little confused as to why a paper focusing on BERMo would include so many experiments with parameter pruning. Why is it natural to consider parameter pruning experiments when using BERMo? The pruning experiments were not motivated using an argument that relates to the BERMo architecture itself. It seems that the connection is possibly due to the relationship between skip connections and pruning? There is discussion throughout the paper about how the BERMo architecture achieves the benefits of skip/residual connections. However, this can be confusing to readers because the transformer architecture already contains residual connections for each layer. Would the authors argue that the layer-weighted architecture is a superior style of skip connections than the residual connections that are already in BERT via its use of the transformer architecture? The paper really needs to discuss why BERMo's skip connections are expected to be more amenable to pruning than the skip connections already present in BERT. Lack of analysis. All the above being said, the pruning experiments do show some significant differences between BERT and BERMo. This is a potentially interesting result. Why might this be the case? It would be nice if the paper could include some ablation experiments that modify the architecture in various ways in order to determine what exactly it is about the BERMo architecture that makes it more amenable to pruning. I had a few other questions/suggestions: Section 4 mentions a "mask learning rate" but it is not described -- what is it? In Sec. 4.2.4, the paper states "Our approach gives better results on SQuAD". Is a difference of 0.16 F1 statistically significant? For QQP, Tables 2 and 4 show accuracies being consistently higher than F1 scores, while Table 5 shows the opposite. Were the numbers in Table 5 inadvertently transposed? Unfortunately, BERT-base is not a very strong model. I'd suggest using RoBERTa-base or DeBERTa-base instead if larger models cannot be used. Below are more minor notes: The citations are formatted oddly: "(Vaswani et al. (2017))". Maybe this is due to a use of citet within parentheses? It should be "(Vaswani et al., 2017)" or "Vaswani et al. (2017)" depending on context. Equation (1) is not contained in the flow of the paper but is rather set off in a figure. It's not common to render an equation in a floating item such as a figure unless there is a series of related equations shown together for comparative purposes. I'd suggest moving Equation (1) to be in the flow of the paper rather than off to the side. Equation (1) has bold, non-italicized h, while the caption has non-boldface, italicized h. Sec. 2.2: "to distinguishing" --> "to distinguish" or "for distinguishing" Sec. 2.3.2: Use \log instead of log in Eq 3. Sec. 3.1: The term "softmax normalized" is used but the equation shown immediately thereafter is just standard normalization---there is no softmax. Sec. 3.1: "features maps" --> "feature maps" Sec.
Review Point: 4: "higher learning rates for the skip connections" -- do you mean learning rates for updating the alpha parameters? Sec.
Review Point: 4: "various experiment" --> "various experiments" Sec. 4.1: "5 trails" --> "5 trials" Sec. 4.1: "our approach outperform" --> "our approach outperforms" Sec. 4.2.2: "smaller dataset" --> "smaller datasets" Sec. 4.2.2: "at the end the fine-pruning" --> "at the end of fine-pruning" Sec. 4.2.2: "roughtly" --> "roughly" Sec. 4.2.2: "is lesser epochs" --> "in lesser epochs"? Sec. 4.2.3: "the both the" --> "both the" Sec. 4.2.3: "weights retain" --> "weights retained" References: @inproceedings{peters-etal-2019-tune, title = "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks", author = "Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.", booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)", month = aug, year = "2019", address = "Florence, Italy", publisher = "Association for Computational Linguistics", url = "https://aclanthology.org/W19-4302", doi = "10.18653/v1/W19-4302", pages = "7--14", abstract = "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.", } Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. "WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS", ICLR 2019.
==================================================

Focused review:

Weaknesses:
Paper was slightly harder read and some useful information such as the definition of the studied approaches were left in the appendices. Reordering the information in the text would help here.
Most of the gains come from the framework over the spherical coordinate approach. This seems reasonable as using spherical coordinates over euclidean coordinates doesn't seem to be something a few layers of networks can't learn easily. 3
Some errors in the text:
a) In practice it help with --> helps with b) Eq. 6: second term in the second line seems to have errors c) Supervised Fine-Turning --> Fine-tuning d) \delta MRR was not defined in the text


Review Point: Paper was slightly harder read and some useful information such as the definition of the studied approaches were left in the appendices. Reordering the information in the text would help here. Most of the gains come from the framework over the spherical coordinate approach. This seems reasonable as using spherical coordinates over euclidean coordinates doesn't seem to be something a few layers of networks can't learn easily.
Review Point: 3 Some errors in the text: a) In practice it help with --> helps with b) Eq. 6: second term in the second line seems to have errors c) Supervised Fine-Turning --> Fine-tuning d) \delta MRR was not defined in the text
==================================================

Focused review:

Weaknesses:
The paper is not carefully written. Line 134 contains a missing reference. Line 260 uses an abbreviation TDR (I believe it is a typo of TRE?), which is not defined anywhere.
The authors analyze a different telescoping density-ratio estimator, rather than the one in [2], and acknowledge that the techniques used for the chain defined in the paper do not apply to the estimator of interest in [2]. This means that the analysis is only for the problem defined in this paper. The authors should provide some evidence that the chain defined in this paper has a superior or comparable performance to the estimator of interest in [2].
[1]Kato, M. and Teshima, T. Non-negative bregman divergence minimization for deep direct density ratio estimation. In International Conference on Machine Learning, pp. 5320 – 5333, 2021.
[2] Rhodes, B., Xu, K., and Gutmann, M. U. Telescoping density-ratio estimation. In Advances in Neural Information Processing Systems, 2020.


Review Point: The paper is not carefully written. Line 134 contains a missing reference. Line 260 uses an abbreviation TDR (I believe it is a typo of TRE?), which is not defined anywhere. The authors analyze a different telescoping density-ratio estimator, rather than the one in [2], and acknowledge that the techniques used for the chain defined in the paper do not apply to the estimator of interest in [2]. This means that the analysis is only for the problem defined in this paper. The authors should provide some evidence that the chain defined in this paper has a superior or comparable performance to the estimator of interest in [2]. [1]Kato, M. and Teshima, T. Non-negative bregman divergence minimization for deep direct density ratio estimation. In International Conference on Machine Learning, pp.
Review Point: 5320 – 5333, 2021. [2] Rhodes, B., Xu, K., and Gutmann, M. U. Telescoping density-ratio estimation. In Advances in Neural Information Processing Systems, 2020.
==================================================

Focused review:

Weakness:
rotation-equivariant: authors claims the proposed method to be rotation equivariant and results on ModelNet40 also support this claim. This is the most important property of the proposed method. It is advisable to have a better explanation of both proposed method and related method mentioned in related work, i.e. why some methods are not rotation-equivariant.
ModelNet40: results of [1] (Cohen etal. 2019) is missing in Table 1.
DOS experiments: (1). meaning of parameters R/C: in the experiments, R=1 and C=32 are used. Does this mean 32 spheres are combined into 1? (2). loss functions: from A.2 and Eq.5, \alpha=0.1 is used, does this mean loss(DOS) is less important than loss(FDOS)? If previous works using \alpha=1.0, could the authors provides the results using the same setting? In other words, how does disabling loss(FDOS) hurt? I'm also keen to know if it is fair to compare to other methods using additional loss term. (3) Effectiveness of the methods: On DOS experiments, authors use simplified version of the architecture (R=1) and reported strong performance than baselines. Does this mean that DOS estimation is too easy for the proposed method? (4) Spherical CNNs baseline: Considering the baselines use in DOS experiments, nearly all SOTA spherical CNNs are not compared.
Ref: [1] Gauge equivariant convolutional networks and the icosahedral CNN.


Review Point: rotation-equivariant: authors claims the proposed method to be rotation equivariant and results on ModelNet40 also support this claim. This is the most important property of the proposed method. It is advisable to have a better explanation of both proposed method and related method mentioned in related work, i.e. why some methods are not rotation-equivariant.
Review Point: ModelNet40: results of [1] (Cohen etal. 2019) is missing in Table 1. DOS experiments: (1). meaning of parameters R/C: in the experiments, R=1 and C=32 are used. Does this mean 32 spheres are combined into 1? (2). loss functions: from A.2 and Eq.5, \alpha=0.1 is used, does this mean loss(DOS) is less important than loss(FDOS)? If previous works using \alpha=1.0, could the authors provides the results using the same setting? In other words, how does disabling loss(FDOS) hurt? I'm also keen to know if it is fair to compare to other methods using additional loss term. (3) Effectiveness of the methods: On DOS experiments, authors use simplified version of the architecture (R=1) and reported strong performance than baselines. Does this mean that DOS estimation is too easy for the proposed method? (4) Spherical CNNs baseline: Considering the baselines use in DOS experiments, nearly all SOTA spherical CNNs are not compared. Ref: [1] Gauge equivariant convolutional networks and the icosahedral CNN.
==================================================

Focused review:

Weakness:
There are some overclaims. For example, the author claims “this is the first work that builds the PPML pipeline for GCN-based models”. As I know, there are multiple existing PPML for GCN based networks, like [1].
[1] Igamberdiev, T., & Habernal, I. (2021). Privacy-preserving graph convolutional networks for text classification. arXiv preprint arXiv:2102. 09604.
The difference between CNN and GCN is not clear. Although the paper uses figure 1 to highlight that GCN has one more matrix multiplication, the GCN and CNN have same operators. How to design efficient homomorphic encryption matrix multiplication is not novel. Please try to explain the difference compared to existing works like [2][3][4]. It is important to know why existing techniques in FHE-based CNN or [2][3][4] do not work for the GCN. It is also important to know what is the motivation of the proposed AMA-based matrix multiplication. Is it specific for GCN?
[2]Chiang, J. (2022). A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks (Inference). arXiv preprint arXiv:2201.12577.
[3] Jiang X, Kim M, Lauter K, Song Y. Secure Outsourced Matrix Computation and Application to Neural Networks. Conf Comput Commun Secur. 2018 Oct;2018:1209-1222. doi: 10.1145/3243734.3243837. PMID: 31404438; PMCID: PMC6689419.
[4] Cui, J., Chen, C., Lyu, L., Yang, C., & Li, W. (2021). Exploiting Data Sparsity in Secure Cross-Platform Social Recommendation. Advances in Neural Information Processing Systems, 34, 10524–10534.
Paper checklist. The reproduction is not clear to me. It is still not clear why the activation pruning does not significantly hurt the accuracy.
There are some overclaims. For example, the author claims “this is the first work that builds the PPML pipeline for GCN-based models”. As I know, there are multiple existing PPML for GCN based networks, like [1].


Review Point: There are some overclaims. For example, the author claims “this is the first work that builds the PPML pipeline for GCN-based models”. As I know, there are multiple existing PPML for GCN based networks, like [1]. [1] Igamberdiev, T., & Habernal, I. (2021). Privacy-preserving graph convolutional networks for text classification. arXiv preprint arXiv:2102. 09604. The difference between CNN and GCN is not clear. Although the paper uses figure 1 to highlight that GCN has one more matrix multiplication, the GCN and CNN have same operators. How to design efficient homomorphic encryption matrix multiplication is not novel. Please try to explain the difference compared to existing works like [2][3][4]. It is important to know why existing techniques in FHE-based CNN or [2][3][4] do not work for the GCN. It is also important to know what is the motivation of the proposed AMA-based matrix multiplication. Is it specific for GCN? [2]Chiang, J. (2022). A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks (Inference). arXiv preprint arXiv:2201.12577. [3] Jiang X, Kim M, Lauter K, Song Y. Secure Outsourced Matrix Computation and Application to Neural Networks. Conf Comput Commun Secur.
Review Point: PMC6689419. [4] Cui, J., Chen, C., Lyu, L., Yang, C., & Li, W. (2021). Exploiting Data Sparsity in Secure Cross-Platform Social Recommendation. Advances in Neural Information Processing Systems, 34, 10524–10534. Paper checklist. The reproduction is not clear to me. It is still not clear why the activation pruning does not significantly hurt the accuracy. There are some overclaims. For example, the author claims “this is the first work that builds the PPML pipeline for GCN-based models”. As I know, there are multiple existing PPML for GCN based networks, like [1].
==================================================

Focused review:

Weaknesses/Questions
The paper states that ‘In other words, it is the hypothesis that any natural task can be expressed as a reward signal’. The definition of a natural task can vary significantly. It would also bring up the question of whether the corresponding objective functions are ‘natural’ or if they are part of ‘what we mean by goals and purposes’.
I think there should be clarification on why the state space has to be the same between the original and modified MDPs to showcase that the reward hypothesis holds.
3.This also leads to the question of if there are scalar rewards conditioned on the objectives then would that be equivalent to the original MORL or Risk Sensitive RL reward functions or if there are reward function approximators that approximate such equivalent rewards then would the reward hypothesis hold. Could there even be such approximators?
Coming to modal reward function, there isn’t a specific example that showcases how the reward function might be dependent on the transition function.


Review Point: The paper states that ‘In other words, it is the hypothesis that any natural task can be expressed as a reward signal’. The definition of a natural task can vary significantly. It would also bring up the question of whether the corresponding objective functions are ‘natural’ or if they are part of ‘what we mean by goals and purposes’. I think there should be clarification on why the state space has to be the same between the original and modified MDPs to showcase that the reward hypothesis holds.
Review Point: 3.This also leads to the question of if there are scalar rewards conditioned on the objectives then would that be equivalent to the original MORL or Risk Sensitive RL reward functions or if there are reward function approximators that approximate such equivalent rewards then would the reward hypothesis hold. Could there even be such approximators? Coming to modal reward function, there isn’t a specific example that showcases how the reward function might be dependent on the transition function.
==================================================

Focused review:

- As the main novelty lies in the clustering of different patterns, the usage of K-means in embedding space G could be explained better. Maybe also a discussion of other possible choices and their comparison would be helpful. - As several previous approaches were also restricted only to Continuous Game of Life, another application could make the main topic of diversity search (and meta-diversity search) more task-independent. For example, it is not clear how useful the same clustering approach would help in different complex dynamical systems.

Review Point: - As the main novelty lies in the clustering of different patterns, the usage of K-means in embedding space G could be explained better. Maybe also a discussion of other possible choices and their comparison would be helpful.
Review Point: - As several previous approaches were also restricted only to Continuous Game of Life, another application could make the main topic of diversity search (and meta-diversity search) more task-independent. For example, it is not clear how useful the same clustering approach would help in different complex dynamical systems.
==================================================

Focused review:

1. The novelty of the method is rather incremental, and heuristic in the selecting k number. 2. The problem of GAN to be addressed in the paper is unclear. 3. No comparison of similar works using critics for rejection sampling, e.g., [2].

Review Point: 1. The novelty of the method is rather incremental, and heuristic in the selecting k number.
Review Point: 2. The problem of GAN to be addressed in the paper is unclear.
Review Point: 3. No comparison of similar works using critics for rejection sampling, e.g., [2].
==================================================

Focused review:

- How much does the spatial and temporal potentials matter? The paper conducts experiments on DLC semi (supervised + gaussian regularization) and DGP, however the influence of spatial and temporal potentials are not evaluated independently. This seems like an informative ablation study to do, especially since the paper claims the difference with prior work is that prior work does not consider temporal and spatial priors. There is a recent work OptiFlex by Liu et al which also uses temporal information, this should be cited. (Although the proposed approach is much simpler than OptiFlex) - The experimental protocol is good, but it is not compared against a previously published semi-supervised pose prediction approach. Only copared against a fully supervised method (DLC) and a baseline semi-supervised method which is an ablative version of the proposed approach (no temporal and structural priors). In the supplemental authors claim that this is a form of previous weakly-supervised approach, however the cited paper is from 1972 (is this a typo?). You would think there's another semi-supervised approach for animal 2D pose estimation, as in this area limited labeled data seems like a very common problem. However I am not an expert in animal 2D pose estimation so I do not have any papers to suggest. From what I know fo 2D human pose estimation this is not as studied as it is ok to assume there are lots labeled data. So possibly there is no other baseline. As I am not suggesting a paper to compare against, I am not taking points away from this, but if the authors can find a previously published work on this the paper may be more convincing. If this is the first instantiation the writing may also reflect this. - Writing is well written, however, much of the detail is in the supplementary that just from reading the paper the detail of the model eludes the reader. I think there are ways to incorporate some more information from the supplemental to the main paper. For example conditional CAE, it's easy to say that one just appends the detected keypoints as an image as additional channel. The detail of the gaussian prior on the heatmap detection also may be incorporated a little more.

Review Point: - How much does the spatial and temporal potentials matter? The paper conducts experiments on DLC semi (supervised + gaussian regularization) and DGP, however the influence of spatial and temporal potentials are not evaluated independently. This seems like an informative ablation study to do, especially since the paper claims the difference with prior work is that prior work does not consider temporal and spatial priors. There is a recent work OptiFlex by Liu et al which also uses temporal information, this should be cited. (Although the proposed approach is much simpler than OptiFlex) - The experimental protocol is good, but it is not compared against a previously published semi-supervised pose prediction approach. Only copared against a fully supervised method (DLC) and a baseline semi-supervised method which is an ablative version of the proposed approach (no temporal and structural priors). In the supplemental authors claim that this is a form of previous weakly-supervised approach, however the cited paper is from 1972 (is this a typo?). You would think there's another semi-supervised approach for animal 2D pose estimation, as in this area limited labeled data seems like a very common problem. However I am not an expert in animal 2D pose estimation so I do not have any papers to suggest. From what I know fo 2D human pose estimation this is not as studied as it is ok to assume there are lots labeled data. So possibly there is no other baseline. As I am not suggesting a paper to compare against, I am not taking points away from this, but if the authors can find a previously published work on this the paper may be more convincing. If this is the first instantiation the writing may also reflect this.
Review Point: - Writing is well written, however, much of the detail is in the supplementary that just from reading the paper the detail of the model eludes the reader. I think there are ways to incorporate some more information from the supplemental to the main paper. For example conditional CAE, it's easy to say that one just appends the detected keypoints as an image as additional channel. The detail of the gaussian prior on the heatmap detection also may be incorporated a little more.
==================================================

Focused review:

- For experimental results in Table 4, the authors did not report the results of [16] for 3-shot. From the results, can we conclude that the proposed method does not have better performance than [16]? The authors fail to provide a detailed analysis on this. - The authors take IoU>0.7 as positives and 0.2<IoU<0.3 as negatives. How does this parameter determined? This is not included in the current version. - The authors fail to compare against state-of-the-art hard negative mining methods in the literature, although they have not been directly applied to few-shot object detection.

Review Point: - For experimental results in Table 4, the authors did not report the results of [16] for 3-shot. From the results, can we conclude that the proposed method does not have better performance than [16]? The authors fail to provide a detailed analysis on this.
Review Point: - The authors take IoU>0.7 as positives and 0.2<IoU<0.3 as negatives. How does this parameter determined? This is not included in the current version.
Review Point: - The authors fail to compare against state-of-the-art hard negative mining methods in the literature, although they have not been directly applied to few-shot object detection.
==================================================

Focused review:

Weaknesses:
The notations are inconsistent and confusing at times. For instance,
M
denotes both the total number of Monte-Carlo sampling steps and the obtained mask from FlowX.
In the initial assessment using Shapley value approximations, the removal of one edge causes the removal of multiple message flows. The contribution scores are then uniformly averaged and assigned to the different message flows. Note that this can cause spurious distribution of scores causing higher importance to message flows that were not useful for a given node- or graph-classification task, especially in the final layers.
The notion of refining the Shapley importance scores is intuitive, but the use of optimization tricks like exponential scaling and encouraging discreteness can bias the refinement step to obtain edge masks that achieve better scores using the evaluation metrics in Sec. 4.
What does the weight vector
w
represent in Eq. 9? Is it just some kind of mapper function that translates Shapley scores to edge mask scores?
The flow importances are converted to layer edge importance by simply summing up the scores of all flows sharing a particular layer edge as the message carrier. Would this cause the distribution of the layer edge importance scores to be skewed?
The authors did not use any baselines when comparing different explanation methods in Sec. 4 and why were the scores calculated using just 100 samples? Was it for computational complexity? If yes, then mentioning the error bar in the results would increase the significance of Sec. 4.
In Table 2, FlowX* performs on-par with FlowX on many datasets which questions the use of the refinement stage. An intuitive explanation for such behavior would be great.
Thank you for submitting a well-thought work!


Review Point: The notations are inconsistent and confusing at times. For instance, M denotes both the total number of Monte-Carlo sampling steps and the obtained mask from FlowX. In the initial assessment using Shapley value approximations, the removal of one edge causes the removal of multiple message flows. The contribution scores are then uniformly averaged and assigned to the different message flows. Note that this can cause spurious distribution of scores causing higher importance to message flows that were not useful for a given node- or graph-classification task, especially in the final layers. The notion of refining the Shapley importance scores is intuitive, but the use of optimization tricks like exponential scaling and encouraging discreteness can bias the refinement step to obtain edge masks that achieve better scores using the evaluation metrics in Sec.
Review Point: 4. What does the weight vector w represent in Eq. 9? Is it just some kind of mapper function that translates Shapley scores to edge mask scores? The flow importances are converted to layer edge importance by simply summing up the scores of all flows sharing a particular layer edge as the message carrier. Would this cause the distribution of the layer edge importance scores to be skewed? The authors did not use any baselines when comparing different explanation methods in Sec. 4 and why were the scores calculated using just 100 samples? Was it for computational complexity? If yes, then mentioning the error bar in the results would increase the significance of Sec.
Review Point: 4. In Table 2, FlowX* performs on-par with FlowX on many datasets which questions the use of the refinement stage. An intuitive explanation for such behavior would be great. Thank you for submitting a well-thought work!
==================================================

Focused review:

Suggestions for improvement: - A discussion of computational costs compared to MAML would be interesting. How much slower is meta-training with ALPHA? - More emphasis could be put on the ALPHA+Random algorithm. Could you perhaps look at per task final adaptive parameter norms and compare ALPHA+Random, pure MAML and the main result? I suspect that all MAML variants substantially modify parameters based on the very biased training samples, while ALPHA relies heavily on model "prior" inductive bias. - Perhaps another set of boxplots in the appendix could be plotted with deviations from the initialization, if informative. Both learning rates and regularization coefficient matter in relation to the scale of gradients at each step, and aren't directly comparable if gradient scale falls dramatically. - I am curious how the authors rationalize and interpret negative learning rates, especially for variants which include MAML. - Should some L2 regularization be added, or be meta-learned, for the parameter initialization as well? Negative learning rates tell me that some amount of meta-overfitting may be due to the initialization being updated via MAML. It may turn out that optimal results are somewhere in between Random and MAML initializations, e.g. MAML with some (meta-learned) weight decay. - A discussion of possible limitations of proposed method would be interesting. Can the authors imagine conditions in which ALPHA would be detrimental or lead to divergence? Some failure cases mentioned in code comments could be brought into the main paper.

Review Point: Suggestions for improvement:- A discussion of computational costs compared to MAML would be interesting. How much slower is meta-training with ALPHA?
Review Point: - More emphasis could be put on the ALPHA+Random algorithm. Could you perhaps look at per task final adaptive parameter norms and compare ALPHA+Random, pure MAML and the main result? I suspect that all MAML variants substantially modify parameters based on the very biased training samples, while ALPHA relies heavily on model "prior" inductive bias.
Review Point: - Perhaps another set of boxplots in the appendix could be plotted with deviations from the initialization, if informative. Both learning rates and regularization coefficient matter in relation to the scale of gradients at each step, and aren't directly comparable if gradient scale falls dramatically.
Review Point: - I am curious how the authors rationalize and interpret negative learning rates, especially for variants which include MAML.
Review Point: - Should some L2 regularization be added, or be meta-learned, for the parameter initialization as well? Negative learning rates tell me that some amount of meta-overfitting may be due to the initialization being updated via MAML. It may turn out that optimal results are somewhere in between Random and MAML initializations, e.g. MAML with some (meta-learned) weight decay.
Review Point: - A discussion of possible limitations of proposed method would be interesting. Can the authors imagine conditions in which ALPHA would be detrimental or lead to divergence? Some failure cases mentioned in code comments could be brought into the main paper.
==================================================

Focused review:

weaknesses: - The first weakness of the paper is that the paper is unclear in its method and missing details. -- While Section 3.1 is clear, the rest of Section 3 is unclear at several occasions. In Section 3.2, a link is made with deformable convolutions with the note that the trajectory convolution can be implemented in this manner (lines 152-153). Section 3.4 in turn discusses the implementation by extending separable 3D-convolutional networks. Does this mean that the proposed trajectory convolution combines both notions? If so, these subsections should be combined and rewritten to better describe how the proposed approach incorporates both. If not, the distinction should be made more clear. -- Section 3.3 discusses the addition of motion information by adding offset coordinates as features. This is also done e.g. in Dense Trajectories themselves and in TDD. From Section 3.3, it is however unclear how this is performed in this context. To what are these offsets added and where in the network are they used? The explanation on lines 159-160 is not intuitive. A figure would also really help here. -- Section 3.5 is not clear at all. The method jumps from using a small 3D CNN to estimate estimate trajectories, to preliminary experiments, to MotionNet, to FlowNet-SD, to replacing optical flow. The same subsection discusses multiple topics in half a page without proper explanation and details, making it unnecessarily hard to read. Please rewrite this subsection and again consider a figure to make things clear to the reader. -- On a general note, it remains unclear from the paper which elements form a trajectory. I.e. which points are tracked, how densely this is done, etc. Without these details, the paper remains abstract and hard to interpret.  - The second weakness of the paper is that the experiments are not fully convincing in three aspects. -- The first aspect is that the proposed trajectory convolution does not add much to the current 3D convolutional network setup. While large improvements are not always a requirement, it is difficult to assess whether the improvements in Tables 1 through 3 are even due to the method, or due to randomness within the networks. Rerunning the baselines and methods multiple times with different seeds followed by significance tests seem like a requirement for these ablation studies given the small improvements. On face value, the current improvements are small.  -- The second aspect is that it remains unknown why the method works and when it fails. Next to the missing details on the trajectories themselves, their impact also remains abstract and unknown. Figure 2 provides a qualitative result, but does not really add to the understanding of the trajectory convolution itself. What do the trajectories focus on? Where and when are the trajectories used within a video? For which actions do they work and why? For which actions to they not work? How different are the learned trajectories from straight lines through the temporal axis? The lack of such analyses limit the impact and insight into the proposed network.  The listed strengths and weaknesses of the paper paint an overall interesting work. The trajectory convolution is a natural extension from the current literature on action recognition. Furthermore, the paper is clearly written. The paper does however have several weaknesses that require attention and warrant discussion. The method itself is not well explained and requires thorough rewriting, while the experiments are not fully convincing due to lack of effect. It should however be noted that the method itself makes sense within the current action recognition literature and the lack of big improvements should not be a direct cause for rejection, as it might still be an interesting result for researchers investigating similar avenues.

Review Point: - The first weakness of the paper is that the paper is unclear in its method and missing details. -- While Section 3.1 is clear, the rest of Section 3 is unclear at several occasions. In Section 3.2, a link is made with deformable convolutions with the note that the trajectory convolution can be implemented in this manner (lines 152-153). Section 3.4 in turn discusses the implementation by extending separable 3D-convolutional networks. Does this mean that the proposed trajectory convolution combines both notions? If so, these subsections should be combined and rewritten to better describe how the proposed approach incorporates both. If not, the distinction should be made more clear. -- Section 3.3 discusses the addition of motion information by adding offset coordinates as features. This is also done e.g. in Dense Trajectories themselves and in TDD. From Section 3.3, it is however unclear how this is performed in this context. To what are these offsets added and where in the network are they used? The explanation on lines 159-160 is not intuitive. A figure would also really help here. -- Section 3.5 is not clear at all. The method jumps from using a small 3D CNN to estimate estimate trajectories, to preliminary experiments, to MotionNet, to FlowNet-SD, to replacing optical flow. The same subsection discusses multiple topics in half a page without proper explanation and details, making it unnecessarily hard to read. Please rewrite this subsection and again consider a figure to make things clear to the reader. -- On a general note, it remains unclear from the paper which elements form a trajectory. I.e. which points are tracked, how densely this is done, etc. Without these details, the paper remains abstract and hard to interpret.
Review Point: - The second weakness of the paper is that the experiments are not fully convincing in three aspects. -- The first aspect is that the proposed trajectory convolution does not add much to the current 3D convolutional network setup. While large improvements are not always a requirement, it is difficult to assess whether the improvements in Tables 1 through 3 are even due to the method, or due to randomness within the networks. Rerunning the baselines and methods multiple times with different seeds followed by significance tests seem like a requirement for these ablation studies given the small improvements. On face value, the current improvements are small. -- The second aspect is that it remains unknown why the method works and when it fails. Next to the missing details on the trajectories themselves, their impact also remains abstract and unknown. Figure 2 provides a qualitative result, but does not really add to the understanding of the trajectory convolution itself. What do the trajectories focus on? Where and when are the trajectories used within a video? For which actions do they work and why? For which actions to they not work? How different are the learned trajectories from straight lines through the temporal axis? The lack of such analyses limit the impact and insight into the proposed network. The listed strengths and weaknesses of the paper paint an overall interesting work. The trajectory convolution is a natural extension from the current literature on action recognition. Furthermore, the paper is clearly written. The paper does however have several weaknesses that require attention and warrant discussion. The method itself is not well explained and requires thorough rewriting, while the experiments are not fully convincing due to lack of effect. It should however be noted that the method itself makes sense within the current action recognition literature and the lack of big improvements should not be a direct cause for rejection, as it might still be an interesting result for researchers investigating similar avenues.
==================================================

Focused review:

weaknesses of different approaches.
5) Some formulations are not quite clear, such as âlimited stochasticityâ vs âpowerful decoderâ in lines 88 and 96. Also the statement in line 111 about âapproximately optimizing the KL divergenceâ and the corresponding footnote looks a bit too abstract - so do the authors optimize it or not?
6) In the bibliography the authors tend to ignore the ICLR conference and list many officially published papers as arxiv.
7) Putting a whole section on cross-domain relations to the appendix is not good practice at all. I realize itâs difficult to fit all content to 8 pages, but itâs the job of the authors to organize the paper in such a way that all important contributions fit into the main paper.

Overall, I am in the borderline mode. The results are quite good, but the novelty seems limited.

Review Point: of different approaches.5) Some formulations are not quite clear, such as âlimited stochasticityâ vs âpowerful decoderâ in lines 88 and 96. Also the statement in line 111 about âapproximately optimizing the KL divergenceâ and the corresponding footnote looks a bit too abstract - so do the authors optimize it or not?
Review Point: 6) In the bibliography the authors tend to ignore the ICLR conference and list many officially published papers as arxiv.
Review Point: 7) Putting a whole section on cross-domain relations to the appendix is not good practice at all. I realize itâs difficult to fit all content to 8 pages, but itâs the job of the authors to organize the paper in such a way that all important contributions fit into the main paper. Overall, I am in the borderline mode. The results are quite good, but the novelty seems limited.
==================================================

Focused review:

- As acknowledged by the authors, there are important computational limitations that prevent the EM approach from being use to train really powerful DLVMs. - Similar insight was present in a paper presented at last year's ICML, which limits the novelty and "thought-provokingness" of the paper (see "prior work" box). - I think that the experiments are quite limited, in particular VAEs with more expressive posteriors could be compared to as well (e.g. IWAE, normalising flow posterior).

Review Point: - As acknowledged by the authors, there are important computational limitations that prevent the EM approach from being use to train really powerful DLVMs.
Review Point: - Similar insight was present in a paper presented at last year's ICML, which limits the novelty and "thought-provokingness" of the paper (see "prior work" box).
Review Point: - I think that the experiments are quite limited, in particular VAEs with more expressive posteriors could be compared to as well (e.g. IWAE, normalising flow posterior).
==================================================

Focused review:

1) The author uses meta learning to learn the memory. However, there are some other works [1][2] uses a memory consisting of raw features. So What is the advantages of the meta-learning strategy to learn memory? 2) The ablation study lacks the experiments to verify the effectiveness of instance-wise FiLM models. 3) The Meta-Neighborhoods is semi-parametric, and is a generalizatio of k-nearest-neighbors. So what is parameters number and the time complexity of the proposed method compared to the baseline and k-nearest-neighbors, [1] Meta-learning with memory-augmented neural networks. ICML 2016. [2] Memory matching networks for one-shot image recognition. CVPR 2018. *******************After Rebuttal************************* I have carefully looked into all the reviews and rebuttals. The rebuttal addresses my concerns. I think the authors have performed significant experiments. It produces a good performance for large-scale tasks, 200-class classification on Tiny-Imagenet. It further reports the results on 1000-class ImageNet classification and also achieves significant gains. I agree with the other reviewers that the idea of combining single parameter initialization with non-parametric approaches for classification is interesting. So I will keep my score.

Review Point: 1) The author uses meta learning to learn the memory. However, there are some other works [1][2] uses a memory consisting of raw features. So What is the advantages of the meta-learning strategy to learn memory?
Review Point: 2) The ablation study lacks the experiments to verify the effectiveness of instance-wise FiLM models.
Review Point: 3) The Meta-Neighborhoods is semi-parametric, and is a generalizatio of k-nearest-neighbors. So what is parameters number and the time complexity of the proposed method compared to the baseline and k-nearest-neighbors, [1] Meta-learning with memory-augmented neural networks. ICML 2016. [2] Memory matching networks for one-shot image recognition. CVPR 2018. *******************After Rebuttal************************* I have carefully looked into all the reviews and rebuttals. The rebuttal addresses my concerns. I think the authors have performed significant experiments. It produces a good performance for large-scale tasks, 200-class classification on Tiny-Imagenet. It further reports the results on 1000-class ImageNet classification and also achieves significant gains. I agree with the other reviewers that the idea of combining single parameter initialization with non-parametric approaches for classification is interesting. So I will keep my score.
==================================================

Focused review:

Weakness: 1. I’m confused about the application scenarios of the proposed method. The authors did not show any application value, though they claimed that MAXENT has a large number of applications in applied machine learning. The proposed method is not easy to understand and implement. The author should at least add one real data application. 2. Differences between the proposed method and the existing methods (Good, 1970; Zhu et al,. 1997; Pandey & Dukkipati, 2013) should be stressed more. The differences can be presented theoretically and numerically.


Review Point: 1. I’m confused about the application scenarios of the proposed method. The authors did not show any application value, though they claimed that MAXENT has a large number of applications in applied machine learning. The proposed method is not easy to understand and implement. The author should at least add one real data application.
Review Point: 2. Differences between the proposed method and the existing methods (Good, 1970; Zhu et al,. 1997; Pandey & Dukkipati, 2013) should be stressed more. The differences can be presented theoretically and numerically.
==================================================

Focused review:

Weaknesses: Post rebuttal: The authors have addressed most of my concerns in the revised version.
The current writing makes the paper a bit hard to follow as notations and algorithms are all in the appendix. Also, some notations are missing such as n, \mathcal{M}. I recommend the authors to explain the notations early in the method section and clarify the benefits when using algorithms 2 to 3 and then to algorithm 1. The main text should be self-contained per se.
Why the paper limits itself to pruning only? Parameters sharing is also an effective approach for model compression and it can be done in a similar manner using learnable masks in [0,1].
It is a bit unclear to me why the authors used the term “heterogeneous structures”. Please clarify a bit more on this. How do you ensure the normalized masks in Eq. 4 belong to the same distribution? And why is this necessary?


Review Point: Post rebuttal: The authors have addressed most of my concerns in the revised version. The current writing makes the paper a bit hard to follow as notations and algorithms are all in the appendix. Also, some notations are missing such as n, \mathcal{M}. I recommend the authors to explain the notations early in the method section and clarify the benefits when using algorithms 2 to 3 and then to algorithm 1. The main text should be self-contained per se. Why the paper limits itself to pruning only? Parameters sharing is also an effective approach for model compression and it can be done in a similar manner using learnable masks in [0,1]. It is a bit unclear to me why the authors used the term “heterogeneous structures”. Please clarify a bit more on this. How do you ensure the normalized masks in Eq.
Review Point: 4 belong to the same distribution? And why is this necessary?
==================================================

Focused review:

- As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate. While this is certainly related, is it the only key factor for this? It will be useful if the authors could provide more discussion on this. - While the reviewer understands that this is a theoretical work, it might be useful to provide numerical results to demonstrate the convergence and the block-wise bound. - tau^hat_1 appears to play an important role in the results. It would be useful to provide some intuition about this.

Review Point: - As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate. While this is certainly related, is it the only key factor for this? It will be useful if the authors could provide more discussion on this.
Review Point: - While the reviewer understands that this is a theoretical work, it might be useful to provide numerical results to demonstrate the convergence and the block-wise bound.
Review Point: - tau^hat_1 appears to play an important role in the results. It would be useful to provide some intuition about this.
==================================================

Focused review:

1. The title seems misleading, the method from my understanding is a way to use GNNs for learning natural laws, or understand GNNs in terms of symbolic models, rather than extracting symbolic models from general deep learning models. If the latter is their main claim, it would be helpful if the authors show the utility of this model on more general tasks, other than natural laws. 2. The paper lacks a strong related work section, missing key details about models that have been used previously for relevant tasks. For example, the authors have not discussed how other papers like Neural Relational Inference or using Generative Models for their tasks. 3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model. 4. It would be helpful if the authors address the natural question how utilizing this model for a general deep learning model is more interpretable or generalizable especially in the cases of noise? 5. The paper needs to add more details in the main draft. Key details are in the appendix, the authors should include majority of Section A.1 in the main paper rather than the appendix. The simulation details should be explained more, are the authors generating time series datasets, the number of samples, length of the time series, how the trajectory is processed etc. While some details are covered in the appendix, these details are essential to understand the task and model.

Review Point: 1. The title seems misleading, the method from my understanding is a way to use GNNs for learning natural laws, or understand GNNs in terms of symbolic models, rather than extracting symbolic models from general deep learning models. If the latter is their main claim, it would be helpful if the authors show the utility of this model on more general tasks, other than natural laws.
Review Point: 2. The paper lacks a strong related work section, missing key details about models that have been used previously for relevant tasks. For example, the authors have not discussed how other papers like Neural Relational Inference or using Generative Models for their tasks.
Review Point: 3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model.
Review Point: 4. It would be helpful if the authors address the natural question how utilizing this model for a general deep learning model is more interpretable or generalizable especially in the cases of noise?
Review Point: 5. The paper needs to add more details in the main draft. Key details are in the appendix, the authors should include majority of Section A.1 in the main paper rather than the appendix. The simulation details should be explained more, are the authors generating time series datasets, the number of samples, length of the time series, how the trajectory is processed etc. While some details are covered in the appendix, these details are essential to understand the task and model.
==================================================

Focused review:

- The contrastive learning framework is the same as SimCLR. - Graph augmentation methods, such as DropNode, DropEdge, FeatureMask, have been adopted in previous GNNs work, such as [1,2]. [1] DROPEDGE: TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION. [2] STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS.

Review Point: - The contrastive learning framework is the same as SimCLR.
Review Point: - Graph augmentation methods, such as DropNode, DropEdge, FeatureMask, have been adopted in previous GNNs work, such as [1,2]. [1] DROPEDGE: TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION. [2] STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS.
==================================================

Focused review:

- The idea of using negative samples is not new and is still widely used in metric learning (triplet loss, contrastive loss etc..) / representation learning (Discriminative unsupervised feature learning, Representation learning with contrastive predictive coding etc..). Even though the setting is different here, I think the related work section should at least mention and discuss some of those works. - I randomly checked 1 mathematical derivation (l.130), and it was incorrect. The beta_+ given does not allow to set eta_- to 0 (in fact the solution would rather be 1/beta_+). Hence I wonder how this value was set during experiments (not discussed after). - I found some experiments confusing. For instance, the even and overlapping MNIST. Why should ACGAN P/R curves be any different between the two experiments: ACGAN is trained on the full MNIST dataset regardless of the positive/negative split, isn'it ? Even if the set of digits over which you evaluate the scores may slightly differ in both cases, this shouldn't make such a difference on MNIST. - Experiments are exclusively carried out on low-resolution images (at most 32x32), which makes it hard to evaluate how the method would perform in more "realistic" settings. - I also wonder how the method would behave if a domain shift was also introduced between positive and negative classes : say you include CIFAR images as negative samples when learning classes from CelebA. Would the performances be enhanced, or could they be degraded w.r.t a vanilla LSGAN trained on positive samples only ?

Review Point: - The idea of using negative samples is not new and is still widely used in metric learning (triplet loss, contrastive loss etc..) / representation learning (Discriminative unsupervised feature learning, Representation learning with contrastive predictive coding etc..). Even though the setting is different here, I think the related work section should at least mention and discuss some of those works.
Review Point: - I randomly checked 1 mathematical derivation (l.130), and it was incorrect. The beta_+ given does not allow to set eta_- to 0 (in fact the solution would rather be 1/beta_+). Hence I wonder how this value was set during experiments (not discussed after).
Review Point: - I found some experiments confusing. For instance, the even and overlapping MNIST. Why should ACGAN P/R curves be any different between the two experiments: ACGAN is trained on the full MNIST dataset regardless of the positive/negative split, isn'it ? Even if the set of digits over which you evaluate the scores may slightly differ in both cases, this shouldn't make such a difference on MNIST.
Review Point: - Experiments are exclusively carried out on low-resolution images (at most 32x32), which makes it hard to evaluate how the method would perform in more "realistic" settings.
Review Point: - I also wonder how the method would behave if a domain shift was also introduced between positive and negative classes : say you include CIFAR images as negative samples when learning classes from CelebA. Would the performances be enhanced, or could they be degraded w.r.t a vanilla LSGAN trained on positive samples only ?
==================================================

Focused review:

Weaknesses:
My major concern is the novelty of this paper. This paper takes a lot of space in Method to explain three popular kinds of video representation learning paradigms and proposes the so-called co-training manner that simply combines two paradigms together. Actually, the so-called co-training manner is widely-used in the multi-modal pertaining community where the contrastive loss and ITM loss are usually used together to boost performance. Therefore, the reviewer cannot regard the so-called co-training manner as one contribution of this paper. More importantly, although the studied false positive and false negative issues are interesting and practical for video representation learning, the proposed method and actual effect lack novelty and guarantee. First, this paper tries to tackle the false positive issue by penalizing the unconfident positive pairs. Such a reweighting strategy on contrastive loss is not new and has been used in other cross-modal learning methods such as [1]. Second, the authors claim that both the false positive and false negative issues are solved using the predicted correspondence confidence in Fig. 3. However, the reviewer cannot understand why the false negative issue can also be tackled by using the estimated confidences [1] Robust Audio-Visual Instance Discrimination, CVPR 2021
The other concern is the effectiveness of the proposed method. As shown in Table 2-5, the compared baselines are all before 2021. The recently proposed video representation learning methods should be included to further show the effectiveness. Second, as shown in Table 7, the proposed method owns limited robustness improvement compared to the baseline even in the hand-crafted high noise scenario (50%). The reviewer doubts the robustness and necessity of the proposed method which penalizes the noisy pair by using the predicted correspondence confidences


Review Point: My major concern is the novelty of this paper. This paper takes a lot of space in Method to explain three popular kinds of video representation learning paradigms and proposes the so-called co-training manner that simply combines two paradigms together. Actually, the so-called co-training manner is widely-used in the multi-modal pertaining community where the contrastive loss and ITM loss are usually used together to boost performance. Therefore, the reviewer cannot regard the so-called co-training manner as one contribution of this paper. More importantly, although the studied false positive and false negative issues are interesting and practical for video representation learning, the proposed method and actual effect lack novelty and guarantee. First, this paper tries to tackle the false positive issue by penalizing the unconfident positive pairs. Such a reweighting strategy on contrastive loss is not new and has been used in other cross-modal learning methods such as [1]. Second, the authors claim that both the false positive and false negative issues are solved using the predicted correspondence confidence in Fig.
Review Point: 3. However, the reviewer cannot understand why the false negative issue can also be tackled by using the estimated confidences [1] Robust Audio-Visual Instance Discrimination, CVPR 2021 The other concern is the effectiveness of the proposed method. As shown in Table 2-5, the compared baselines are all before 2021. The recently proposed video representation learning methods should be included to further show the effectiveness. Second, as shown in Table 7, the proposed method owns limited robustness improvement compared to the baseline even in the hand-crafted high noise scenario (50%). The reviewer doubts the robustness and necessity of the proposed method which penalizes the noisy pair by using the predicted correspondence confidences
==================================================

Focused review:

Weaknesses:
The proposed method is based on the analysis on the two specific datasets, FB15k-237 and WN18. It is unclear if this method can be applied in other KGs.
How do the authors get the hierarchical information of relations? Is this hierarchy also available for other KGs?
Based on the proposed model in Eq. 4 and 5, it seems that the proposed new relation RelatedTo and new entity Country are unnecessary, as the model is actually learning to make France and Iran close to each other. Moreover, in Section 4.1, if the authors force
F
r
a
n
c
e
+
H
a
s
A
t
t
r
i
b
u
t
e
≈
c
a
p
i
t
a
l
and
P
a
r
i
s
+
R
e
l
a
t
e
d
T
o
≈
c
a
p
i
t
a
l
and these two equations should also hold for other (capital, country) pair, this again means that the model actually learns to make all capitals close to each other and all countries close to each other. If this is the case, then why not directly make the entities belonging to the same category close to each other? Why introducing new relations and entities?
According to the experimental results, the proposed method does not performs well on FB15k-237 dataset.
The language and figures in this paper need to be improved.


Review Point: The proposed method is based on the analysis on the two specific datasets, FB15k-237 and WN18. It is unclear if this method can be applied in other KGs. How do the authors get the hierarchical information of relations? Is this hierarchy also available for other KGs? Based on the proposed model in Eq.
Review Point: 4 and 5, it seems that the proposed new relation RelatedTo and new entity Country are unnecessary, as the model is actually learning to make France and Iran close to each other. Moreover, in Section 4.1, if the authors force F r a n c e + H a s A t t r i b u t e ≈ c a p i t a l and P a r i s + R e l a t e d T o ≈ c a p i t a l and these two equations should also hold for other (capital, country) pair, this again means that the model actually learns to make all capitals close to each other and all countries close to each other. If this is the case, then why not directly make the entities belonging to the same category close to each other? Why introducing new relations and entities? According to the experimental results, the proposed method does not performs well on FB15k-237 dataset. The language and figures in this paper need to be improved.
==================================================

Focused review:

A human evaluation of the "gaps" filled by the model would have been a plus for the paper. 
A better evaluation of the output produced by the proposed method would be beneficial for the reader. For instance focusing on: - How often invalid constraints are finally hypothesized by the model?
- How grammatical are the "filled gaps" proposed by the model given that in principle the model does not modifies human constraints?
- Which is the impact of using different ratios of raw/augmented sentences for training?
Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs?
The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints: - Boosting Neural Machine Translation with Similar Translations. Jitao Xu, Josep Crego, Jean Senellart - Training Neural Machine Translation to Apply Terminology Constraints. Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan - Neural Machine Translation Decoding with Terminology Constraints. Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, Bill Byrne - Levenshtein Transformer. Jiatao Gu, Changhan Wang, Jake Zhao Typos: 261: in the abve equation "are" used to guarantee that all 

Review Point: A human evaluation of the "gaps" filled by the model would have been a plus for the paper. A better evaluation of the output produced by the proposed method would be beneficial for the reader. For instance focusing on:
Review Point: - How often invalid constraints are finally hypothesized by the model?
Review Point: - How grammatical are the "filled gaps" proposed by the model given that in principle the model does not modifies human constraints?
Review Point: - Which is the impact of using different ratios of raw/augmented sentences for training? Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs? The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints:
Review Point: - Boosting Neural Machine Translation with Similar Translations. Jitao Xu, Josep Crego, Jean Senellart - Training Neural Machine Translation to Apply Terminology Constraints. Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan - Neural Machine Translation Decoding with Terminology Constraints. Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, Bill Byrne - Levenshtein Transformer. Jiatao Gu, Changhan Wang, Jake Zhao Typos: 261: in the abve equation "are" used to guarantee that all
==================================================

Focused review:

weakness of the non-local (NL) module [31] that the correlations across channels are less taken into account, and then formulate the compact generalized non-local (CGNL) module to remedy the issue through summarizing the previous methods of NL and bilinear pooling [14] in a unified manner. The CGNL is evaluated on thorough experiments for action and fine-grained classification tasks, exhibiting promising performance competitive to the state-of-the-arts.  Positives: + The paper is well organized and easy to follow. + The generalized formulation (8,9) to unify bilinear pooling and non-local module is theoretically sound. + Good performance.  Negatives: - Less discussion on the linear version of CGNL using dot product for f. - Missing fundamental comparison to the simple ResBlock.  The authors nicely present the generalized formulation toward CGNL by unifying the two previous works of bilinear pooling and non-local module. Though the kernelized (non-linear) correlation function f is well theoretically motivated, the actual form of f that achieves the better empirical performance is a âlinearâ form (dot product). In this regard, the reviewer has the following concerns.  - Less discussion about the linear form. If the reviewer correctly understands the CGNL formulation, the linear function f of dot product f (line 204) can greatly simplify the CGNL into Y = X * W_theta * tr[(X*W_phi)â * (X*W_g)] = X * W_theta * tr[(XâX)* W_g* W_phiâ]   = s * X * W_theta, where s = tr[(XâX) * W_g * W_phiâ]= tr[(XâX)* W] is just a scalar and W = W_g*W_phiâ. This reformulation would be beneficial from the following viewpoints. > It reduces the parameters from {W_theta, W_phi, W_g} to {W_theta, W}, which facilitates the implementation. > It is closely related to squeeze-and-excitation (SE) module [9]. The above formulation can be regarded as a bilinear extension of SE from âsqueezeâ viewpoint since it âsqueezesâ the feature map X into the bilinear form of XâX while SE simply employs an average-pooling.   Such discussions as above would help the readers to further understand the methods and to further extend the method.  - Missing comparison. Based on the above discussion, one can think that the baseline for the linear CGNL is a simple ResBlock of  Z = BatchNorm( X * W_z ) + X, while the linear CGNL is  Z = BatchNorm( s * X * W_theta * W_z ) + X    = BatchNorm( s * X * W_tz ) + X. The only difference is the scaling factor s that is also build on X. Through batch normalization, such a scaling might be less effective (during the training) and thus by comparing these closely-related methods, the authors have to clarify its effectiveness of CGNL empirically. Due to this concern, the reviewer can not fairly evaluate the impact of the method on classification performance.  [After Rebuttal] The reviewer appreciates the authorsâ efforts to perform the comparison experiments in such a short rebuttal period. The comparison with the standard ResBlock clarifies the effectiveness of the proposed method as well as helps us to further understand how it works. 

Review Point: of the non-local (NL) module [31] that the correlations across channels are less taken into account, and then formulate the compact generalized non-local (CGNL) module to remedy the issue through summarizing the previous methods of NL and bilinear pooling [14] in a unified manner. The CGNL is evaluated on thorough experiments for action and fine-grained classification tasks, exhibiting promising performance competitive to the state-of-the-arts. Positives:
Review Point: + The paper is well organized and easy to follow.
Review Point: + The generalized formulation (8,9) to unify bilinear pooling and non-local module is theoretically sound.
Review Point: + Good performance. Negatives:- Less discussion on the linear version of CGNL using dot product for f.
Review Point: - Missing fundamental comparison to the simple ResBlock. The authors nicely present the generalized formulation toward CGNL by unifying the two previous works of bilinear pooling and non-local module. Though the kernelized (non-linear) correlation function f is well theoretically motivated, the actual form of f that achieves the better empirical performance is a âlinearâ form (dot product). In this regard, the reviewer has the following concerns.
Review Point: - Less discussion about the linear form. If the reviewer correctly understands the CGNL formulation, the linear function f of dot product f (line 204) can greatly simplify the CGNL into Y = X * W_theta * tr[(X*W_phi)â * (X*W_g)] = X * W_theta * tr[(XâX)* W_g* W_phiâ] = s * X * W_theta, where s = tr[(XâX) * W_g * W_phiâ]= tr[(XâX)* W] is just a scalar and W = W_g*W_phiâ. This reformulation would be beneficial from the following viewpoints. > It reduces the parameters from {W_theta, W_phi, W_g} to {W_theta, W}, which facilitates the implementation. > It is closely related to squeeze-and-excitation (SE) module [9]. The above formulation can be regarded as a bilinear extension of SE from âsqueezeâ viewpoint since it âsqueezesâ the feature map X into the bilinear form of XâX while SE simply employs an average-pooling. Such discussions as above would help the readers to further understand the methods and to further extend the method.
Review Point: - Missing comparison. Based on the above discussion, one can think that the baseline for the linear CGNL is a simple ResBlock of Z = BatchNorm( X * W_z ) + X, while the linear CGNL is Z = BatchNorm( s * X * W_theta * W_z ) + X = BatchNorm( s * X * W_tz ) + X. The only difference is the scaling factor s that is also build on X. Through batch normalization, such a scaling might be less effective (during the training) and thus by comparing these closely-related methods, the authors have to clarify its effectiveness of CGNL empirically. Due to this concern, the reviewer can not fairly evaluate the impact of the method on classification performance. [After Rebuttal] The reviewer appreciates the authorsâ efforts to perform the comparison experiments in such a short rebuttal period. The comparison with the standard ResBlock clarifies the effectiveness of the proposed method as well as helps us to further understand how it works.
==================================================

Focused review:

Weakness- 1). Theoretical claims could be better explained/justified. It's not obvious upon reading this how all the theoretical contributions are related to the work. Theorem 1 and lemma 1 are related to TT framework, and theorem 2 is about explicit singular value formula for strided convolution. To help clear some ambiguity, it might be helpful to put the lemma in appendix and expand up the technical details of theorem 1 and the importance of it for the framework. Then explain how all the theoretical contributions fit into algorithm (1).
2). I believe the paper would benefit from additional experiment, similar to the one in Figure (1), which would include speed up versus some performance metrics (accuracy, ECE). This would be very useful from a practitioner's standpoint


Review Point: 1). Theoretical claims could be better explained/justified. It's not obvious upon reading this how all the theoretical contributions are related to the work. Theorem 1 and lemma 1 are related to TT framework, and theorem 2 is about explicit singular value formula for strided convolution. To help clear some ambiguity, it might be helpful to put the lemma in appendix and expand up the technical details of theorem 1 and the importance of it for the framework. Then explain how all the theoretical contributions fit into algorithm (1).
Review Point: 2). I believe the paper would benefit from additional experiment, similar to the one in Figure (1), which would include speed up versus some performance metrics (accuracy, ECE). This would be very useful from a practitioner's standpoint
==================================================

Focused review:

Negatives: - The paper uses non-standard terminology for a well-known problem in the literature. - Some of the claims made about prior work are not accurate. - The paper is unclear on when, exactly, in operation the unbiased risk estimator established. Related to this is the practical question of where, exactly, the unlabeled dataset comes from. - There is some question in my mind as to whether or not the comparison experiment reflected in Table 1 is fair, based on the way hyperparameters are set.

Review Point: Negatives:- The paper uses non-standard terminology for a well-known problem in the literature.
Review Point: - Some of the claims made about prior work are not accurate.
Review Point: - The paper is unclear on when, exactly, in operation the unbiased risk estimator established. Related to this is the practical question of where, exactly, the unlabeled dataset comes from.
Review Point: - There is some question in my mind as to whether or not the comparison experiment reflected in Table 1 is fair, based on the way hyperparameters are set.
==================================================

Focused review:

Weakness: 1. The paper has not been completed. 2. The experiments are limited and the analysis is lacking. Without enough experiments and analysis, the research is difficult to convince the readers to be a solid work. 3. The method description contains limited detailed information. It is difficult to transfer the authors' idea without details.


Review Point: 2. The experiments are limited and the analysis is lacking. Without enough experiments and analysis, the research is difficult to convince the readers to be a solid work.
Review Point: 3. The method description contains limited detailed information. It is difficult to transfer the authors' idea without details.
==================================================

Focused review:

Weakness:
• The authors argue that B-cos Transformer performs at least as well as the baseline ViTs under the same number of backbone layers in Section 5.1. However, the experimental setting is unfair without comparing the computational complexity to the baseline methods, especially when the authors replace the conventional tokenization module with a pre-trained B-cos DenseNet-121. Though deriving more explainable models, the proposed model should be toned down if they are too heavy to deploy. This is the same for Fig. 6, which also lacks comparisons of model complexity vs. Localisation/perturbation.
• I am also confused about where the good performance of B-cos Transformer comes from. The authors claim that the positional information design largely improves performance. However, the effectiveness of the positional information cannot be concluded from Fig. 5, with many other components modified. To fully explain the performance gain, comparisons have to be done by attaching the B-cos positional information to the conventional ViTs.
• I am also curious about how the third column of Fig. 7 is generated, as the visualizations do not seem to be normalized.


Review Point: • The authors argue that B-cos Transformer performs at least as well as the baseline ViTs under the same number of backbone layers in Section 5.1. However, the experimental setting is unfair without comparing the computational complexity to the baseline methods, especially when the authors replace the conventional tokenization module with a pre-trained B-cos DenseNet-121. Though deriving more explainable models, the proposed model should be toned down if they are too heavy to deploy. This is the same for Fig. 6, which also lacks comparisons of model complexity vs. Localisation/perturbation.
Review Point: • I am also confused about where the good performance of B-cos Transformer comes from. The authors claim that the positional information design largely improves performance. However, the effectiveness of the positional information cannot be concluded from Fig. 5, with many other components modified. To fully explain the performance gain, comparisons have to be done by attaching the B-cos positional information to the conventional ViTs.
Review Point: • I am also curious about how the third column of Fig. 7 is generated, as the visualizations do not seem to be normalized.
==================================================

Focused review:

- Despite the idea is somehow interesting, this paper just brings a new strategy to generate additional positive and negative pairs for contrastive loss. - Furthermore, this methodology is tailored to the situation of aligned pairs of images. Which is the performance of all the compared methods when images are not aligned? Or how sensitive the performance is to bad registrations? - I feel additional ablation studies are required. For example, in the local contrastive loss, the regions are sampled based on K (< min (W,H)), but I assume the value of K will be dependent on the application, as the context needed to capture the target across different datasets. However, authors show results only on ACDC. - Some of these datasets contain multiple classes. Nevertheless, whether the DSC is computed over one or multiple targets (and then averaged) is not explained. If multiple classes are used, how the sampling for local contrastive loss is performed? Does the fact the the same patch may contain information from several targets affect the local structure representation? - In some point of the paper, authors claim that the only use 2 images for training. As such, I think authors should also report the performance of the method in [63], as they achieve satisfactory results with only 1 training image. - According to results in Table 1, it seems that the local contrastive term without the domain specific knowledge (L_R) typically achieves better performance than the term integrating domain knowledge (L_D). - As a minor comment, it is hard to get the whole message when Table 1 in the main paper and Table 4 in supplemental material are supposed to show the same thing.

Review Point: - Despite the idea is somehow interesting, this paper just brings a new strategy to generate additional positive and negative pairs for contrastive loss.
Review Point: - Furthermore, this methodology is tailored to the situation of aligned pairs of images. Which is the performance of all the compared methods when images are not aligned? Or how sensitive the performance is to bad registrations?
Review Point: - I feel additional ablation studies are required. For example, in the local contrastive loss, the regions are sampled based on K (< min (W,H)), but I assume the value of K will be dependent on the application, as the context needed to capture the target across different datasets. However, authors show results only on ACDC.
Review Point: - Some of these datasets contain multiple classes. Nevertheless, whether the DSC is computed over one or multiple targets (and then averaged) is not explained. If multiple classes are used, how the sampling for local contrastive loss is performed? Does the fact the the same patch may contain information from several targets affect the local structure representation?
Review Point: - In some point of the paper, authors claim that the only use 2 images for training. As such, I think authors should also report the performance of the method in [63], as they achieve satisfactory results with only 1 training image.
Review Point: - According to results in Table 1, it seems that the local contrastive term without the domain specific knowledge (L_R) typically achieves better performance than the term integrating domain knowledge (L_D).
Review Point: - As a minor comment, it is hard to get the whole message when Table 1 in the main paper and Table 4 in supplemental material are supposed to show the same thing.
==================================================

Focused review:

Weaknesses:
Terminology: Introduction l. 24-26 "pixels near the peripheral of the object of interest can generally be challenging, but not relevant to topology." I think this statement is problematic. When considering the inverse e.g. in the case of a surface or vessel, that a foreground pixel changes to background. Such a scenario would immediately lead to a topology mismatch (Betti error 1).
Terminology: "topologically critical location" --> I find this terminology to be not optimally chosen. I agree that the warping concept appears to help with identifying pixels which may close loops or fill holes. However, considering the warping I do not see a guarantee that such "locations" (as in the exact location) which I understand to refer to individual or groups of pixels are indeed part of the real foreground, nor are these locations unique. A slightly varying warping may propose a set of different pixels.
The identified locations are more likely to be relevant to topological errors. --> this statement should be statistically supported. Compared to what exactly? Does this rely on the point estimate for any pixel? Or given a particularly trained network?
Theorem 1: The presentation of a well known definition from Kong et al. is trivial and could be presented in a different way.
Experimentation, lack of implementation details: In Table 2 and a dedicated section, the authors show an ablation study on the influence of lamda on the results. Lamda is a linear parameter, weighting the contribution of the new loss to the overall loss. Similarly, the studied baseline methods, e.g. TopoNet [24], DMT [25], and clDice [42] have a loss weighting parameter. It would be important to understand how and if the parameters of the baselines were chosen and experimented with. (I understand that the authors cannot train ablation studies for all baselines etc.) However, it is an important information to understand the results in Table 1.
Terminology: l. 34 "to force the neural network to memorize them" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact "critical point" as such in TopoNet [24].
Minor:
I find the method section to be a bit wordy, it could be compressed on the essential definitions.
There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l. 271 "This lemma is naturally generalized to 3D case."
l. 52 "language of topology" I find this to be an imprecise definition or formulation.
Note:
After rebuttal and discussion I increased the rating to 5.


Review Point: Terminology: Introduction l. 24-26 "pixels near the peripheral of the object of interest can generally be challenging, but not relevant to topology." I think this statement is problematic. When considering the inverse e.g. in the case of a surface or vessel, that a foreground pixel changes to background. Such a scenario would immediately lead to a topology mismatch (Betti error 1). Terminology: "topologically critical location" --> I find this terminology to be not optimally chosen. I agree that the warping concept appears to help with identifying pixels which may close loops or fill holes. However, considering the warping I do not see a guarantee that such "locations" (as in the exact location) which I understand to refer to individual or groups of pixels are indeed part of the real foreground, nor are these locations unique. A slightly varying warping may propose a set of different pixels. The identified locations are more likely to be relevant to topological errors. --> this statement should be statistically supported. Compared to what exactly? Does this rely on the point estimate for any pixel? Or given a particularly trained network? Theorem 1: The presentation of a well known definition from Kong et al. is trivial and could be presented in a different way. Experimentation, lack of implementation details: In Table 2 and a dedicated section, the authors show an ablation study on the influence of lamda on the results. Lamda is a linear parameter, weighting the contribution of the new loss to the overall loss. Similarly, the studied baseline methods, e.g. TopoNet [24], DMT [25], and clDice [42] have a loss weighting parameter. It would be important to understand how and if the parameters of the baselines were chosen and experimented with. (I understand that the authors cannot train ablation studies for all baselines etc.) However, it is an important information to understand the results in Table 1. Terminology: l.
Review Point: 34 "to force the neural network to memorize them" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact "critical point" as such in TopoNet [24]. Minor: I find the method section to be a bit wordy, it could be compressed on the essential definitions. There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l.
Review Point: 271 "This lemma is naturally generalized to 3D case." l.
Review Point: 52 "language of topology" I find this to be an imprecise definition or formulation. Note: After rebuttal and discussion I increased the rating to 5.
==================================================

Focused review:

the paper presents minor weaknesses: 1) it is too rich in content probably given the page limitations: there is a lot (too much) material that is just relegated to the appendix...an appendix of 90 pages is excessive..maybe I would suggest to submit the paper to a journal rather than a conference in order to allow the reviewers to have the time to go through the appendix material and to include more of the material in the actual paper. 2) I find the title not so in line with the main content of the paper. The title should probably be focused on the uniform analysis of distributed SGD-based methods which is the main topic of the paper. 3) the authors are not discussing the following methods which seem to be highly related to their problem framework: https://arxiv.org/abs/1611.02189 https://www.research-collection.ethz.ch/handle/20.500.11850/183454 http://proceedings.mlr.press/v80/duenner18a.html maybe they should point to those and cite those papers. 4) it is a bit disappointing that for the experiments the distributed setting is only 'simulated' on a single machine with a for loop... 5) improve the readability of the plots by increasing their size (they are currently a bit too small) ========== After Authors' Rebuttal ========== I would like to thank the authors for providing their comments/reply to each potential weakness point that I had underlined in this section. Regarding the first point, although there are no-strict limits on the appendix length enforced, I still find the current appendix's length (and content) to be excessive wrt the paper's length (and content) and still think that the content should have been re-organized between paper and appendix and submitted to a journal rather than a conference. At the same time, I agree with the authors regarding the fact that details on derivations for special cases are important and can be exploited in the future by other researchers (indeed I have never said to remove those details/derivations but just pointed out that most of the content is unfortunately and in my opinion wrongly relegated to the appendix....while the appendix's role in my opinion should be different). Regarding the second point, I suggest to switch back to the title of the 'working version' or find a new one that incorporates both these aspects without sacrificing the unified theory aspect. In the light of the authors's reply and taking into account also some of the other reviewers'votes, I confirm my (positive) vote for this paper.

Review Point: 1) it is too rich in content probably given the page limitations: there is a lot (too much) material that is just relegated to the appendix...an appendix of 90 pages is excessive..maybe I would suggest to submit the paper to a journal rather than a conference in order to allow the reviewers to have the time to go through the appendix material and to include more of the material in the actual paper.
Review Point: 2) I find the title not so in line with the main content of the paper. The title should probably be focused on the uniform analysis of distributed SGD-based methods which is the main topic of the paper.
Review Point: 3) the authors are not discussing the following methods which seem to be highly related to their problem framework: https://arxiv.org/abs/1611.02189 https://www.research-collection.ethz.ch/handle/20.500.11850/183454 http://proceedings.mlr.press/v80/duenner18a.html maybe they should point to those and cite those papers.
Review Point: 4) it is a bit disappointing that for the experiments the distributed setting is only 'simulated' on a single machine with a for loop...
Review Point: 5) improve the readability of the plots by increasing their size (they are currently a bit too small) ========== After Authors' Rebuttal ========== I would like to thank the authors for providing their comments/reply to each potential weakness point that I had underlined in this section. Regarding the first point, although there are no-strict limits on the appendix length enforced, I still find the current appendix's length (and content) to be excessive wrt the paper's length (and content) and still think that the content should have been re-organized between paper and appendix and submitted to a journal rather than a conference. At the same time, I agree with the authors regarding the fact that details on derivations for special cases are important and can be exploited in the future by other researchers (indeed I have never said to remove those details/derivations but just pointed out that most of the content is unfortunately and in my opinion wrongly relegated to the appendix....while the appendix's role in my opinion should be different). Regarding the second point, I suggest to switch back to the title of the 'working version' or find a new one that incorporates both these aspects without sacrificing the unified theory aspect. In the light of the authors's reply and taking into account also some of the other reviewers'votes, I confirm my (positive) vote for this paper.
==================================================

Focused review:

Weakness:
The unbalanced data scenario has not been properly explored by experiments. Under what circumstances can it be counted as an unbalanced data scenario, and what is the data ratio? Therefore, the experiments should not pay more attention to one given setting like TED, WMT, etc., but should construct unbalanced scenarios of different ratios by sampling data in one setting like WMT to verify this important issue.
There is a lack of a reasonable ablation study on the upsampling parameter T, so we cannot confirm whether the oversampling overfit phenomenon will occur, and to what extent will the upsampling reach.
Some baselines are missing in the experimental comparison, such as 1) giving different weights to the loss of unbalanced translation pairs so that in the later stages of training, there will be no situation where rich-resource pairs dominate the training loss; 2) the use of low-resource language pairs further finetune the multilingual model and use the method like R3F to maintain the generalization ability of the model.
In some low-resource language translations from 1.2->2.0, although the improvement of 0.8 can be claimed, it is insignificant in a practical sense.
Missing References:
Aghajanyan, Armen, et al. "Better Fine-Tuning by Reducing Representational Collapse." International Conference on Learning Representations. 2020.


Review Point: The unbalanced data scenario has not been properly explored by experiments. Under what circumstances can it be counted as an unbalanced data scenario, and what is the data ratio? Therefore, the experiments should not pay more attention to one given setting like TED, WMT, etc., but should construct unbalanced scenarios of different ratios by sampling data in one setting like WMT to verify this important issue. There is a lack of a reasonable ablation study on the upsampling parameter T, so we cannot confirm whether the oversampling overfit phenomenon will occur, and to what extent will the upsampling reach. Some baselines are missing in the experimental comparison, such as 1) giving different weights to the loss of unbalanced translation pairs so that in the later stages of training, there will be no situation where rich-resource pairs dominate the training loss;
Review Point: 2) the use of low-resource language pairs further finetune the multilingual model and use the method like R3F to maintain the generalization ability of the model. In some low-resource language translations from 1.2->2.0, although the improvement of 0.8 can be claimed, it is insignificant in a practical sense. Missing References: Aghajanyan, Armen, et al. "Better Fine-Tuning by Reducing Representational Collapse." International Conference on Learning Representations. 2020.
==================================================

Focused review:

Weaknesses:
I may be wrong on this, but I think the authors could say more about how expensive it is to estimate
μ
X
s
c
∣
X
s
=
x
s
repeatedly throughout the KernelSHAP WLS approximation for different coalitions
s
. I've elaborated on this in the "questions" section below.
Again I may be wrong on this, but I think the paper understates the difficulty of estimating
μ
X
s
c
∣
X
s
=
x
s
. I've elaborated on this in the "questions" section below.
The robustness results do not seem to be a property of RKHS-SHAP, but of Shapley values themselves when applied to kernel models for which the authors' assumptions hold. It's an interesting result and non-trivial to show, but it perhaps shouldn't be claimed to be related to RKHS-SHAP.
In discussing the conditional expectation estimate in Section 3, the authors write that they "utilise the arsenal of kernel methods to estimate the conditional expectations directly." I believe this is very similar (except for the focus on kernels) to the second method introduced by Frye et al. 2020 that has a very similar motivation - the supervised surrogate model. That method is easier to train than the conditional generative model, does not require Monte Carlo sampling, and has no risk of off-manifold examples because it bypasses the generative modeling task. Perhaps this deserves some discussion?
I thought the paper's exposition could greatly benefit from disentangling the two main elements of estimating Shapley values (and where RHKS-SHAP is helpful): 1) handling predictions with held-out features, and 2) accurately approximating Shapley values. Section 2 made it sound like this work would present a significant improvement over KernelSHAP (see lines 114-130), which led me to believe the improvement had to do with 2). Ultimately, it had to do with 1), which I don't understand as really being part of KernelSHAP. The main idea behind KernelSHAP is approximating Shapley values via weighted least squares, and the authors actually re-use this part. What's modified here is the sampling trick for handling held-out features, which isn't unique to KernelSHAP and also used by other Shapley value methods (e.g., IME).
A couple small things:
On line 125, the formula for the closed-form solution to the WLS problem solved by KernelSHAP seems a bit informal due to the presence of
∞
entries in
W
It may be helpful to say explicitly that the interventional and observational coalitional games can be estimated via the KME/CME specifically because of the model's linear dependence on the features
ψ
x
. Of course, what we really want is the expectation of the model's output, and it's a unique property of such methods that we can get that via the expectation of the
ψ
x
component.
There are a couple related works that seem like they should/could have been discussed here:
The various ways of estimating
E
[
f
(
X
)
∣
X
s
=
x
s
]
are discussed in a recent review paper [1], including parametric assumptions, generative modeling, and several more. The conditional generative model is not, I think, widely understood as a viable approach
The idea of "attribution priors" [2] has been discussed as a way to regularize models' dependencies during training, although it's mainly used with gradient-based methods rather than Shapley values. Regularizing explanations is a natural idea, but Shapley values are somewhat unique in making this intractable
ShapNets [3] also provide a fast Shapley value calculation, albeit for a specific modified DNN architecture, and one of the motivations was to enable explanation regularization. RKHS-SHAP provides something similar, but for kernel models
[1] Covert et al., "Explaining by removing: a unified approach to model explanation" (2021)
[2] Erion et al., "Improving permeance of deep learning models with axiomatic attribution priors and expected gradients" (2021)
[3] Wang et al., "Shapley explanation networks" (2021)
There are no negative societal impacts to this work.
As for methodological limitations, I believe those are primarily in the speed and reliability of the CME estimation (as discussed above). If my concerns are accurate, then I don't think these points are currently adequately discussed.


Review Point: I may be wrong on this, but I think the authors could say more about how expensive it is to estimate μ X s c ∣ X s = x s repeatedly throughout the KernelSHAP WLS approximation for different coalitions s . I've elaborated on this in the "questions" section below. Again I may be wrong on this, but I think the paper understates the difficulty of estimating μ X s c ∣ X s = x s . I've elaborated on this in the "questions" section below. The robustness results do not seem to be a property of RKHS-SHAP, but of Shapley values themselves when applied to kernel models for which the authors' assumptions hold. It's an interesting result and non-trivial to show, but it perhaps shouldn't be claimed to be related to RKHS-SHAP. In discussing the conditional expectation estimate in Section 3, the authors write that they "utilise the arsenal of kernel methods to estimate the conditional expectations directly." I believe this is very similar (except for the focus on kernels) to the second method introduced by Frye et al. 2020 that has a very similar motivation - the supervised surrogate model. That method is easier to train than the conditional generative model, does not require Monte Carlo sampling, and has no risk of off-manifold examples because it bypasses the generative modeling task. Perhaps this deserves some discussion? I thought the paper's exposition could greatly benefit from disentangling the two main elements of estimating Shapley values (and where RHKS-SHAP is helpful):
Review Point: 1) handling predictions with held-out features, and 2) accurately approximating Shapley values. Section 2 made it sound like this work would present a significant improvement over KernelSHAP (see lines 114-130), which led me to believe the improvement had to do with 2). Ultimately, it had to do with 1), which I don't understand as really being part of KernelSHAP. The main idea behind KernelSHAP is approximating Shapley values via weighted least squares, and the authors actually re-use this part. What's modified here is the sampling trick for handling held-out features, which isn't unique to KernelSHAP and also used by other Shapley value methods (e.g., IME). A couple small things: On line 125, the formula for the closed-form solution to the WLS problem solved by KernelSHAP seems a bit informal due to the presence of ∞ entries in W It may be helpful to say explicitly that the interventional and observational coalitional games can be estimated via the KME/CME specifically because of the model's linear dependence on the features ψ x . Of course, what we really want is the expectation of the model's output, and it's a unique property of such methods that we can get that via the expectation of the ψ x component. There are a couple related works that seem like they should/could have been discussed here: The various ways of estimating E [ f ( X ) ∣ X s = x s ] are discussed in a recent review paper [1], including parametric assumptions, generative modeling, and several more. The conditional generative model is not, I think, widely understood as a viable approach The idea of "attribution priors" [2] has been discussed as a way to regularize models' dependencies during training, although it's mainly used with gradient-based methods rather than Shapley values. Regularizing explanations is a natural idea, but Shapley values are somewhat unique in making this intractable ShapNets [3] also provide a fast Shapley value calculation, albeit for a specific modified DNN architecture, and one of the motivations was to enable explanation regularization. RKHS-SHAP provides something similar, but for kernel models [1] Covert et al., "Explaining by removing: a unified approach to model explanation" (2021) [2] Erion et al., "Improving permeance of deep learning models with axiomatic attribution priors and expected gradients" (2021) [3] Wang et al., "Shapley explanation networks" (2021) There are no negative societal impacts to this work. As for methodological limitations, I believe those are primarily in the speed and reliability of the CME estimation (as discussed above). If my concerns are accurate, then I don't think these points are currently adequately discussed.
==================================================

Focused review:

- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. 

Review Point: - Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
Review Point: - The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community.
Review Point: - The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
Review Point: - In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
Review Point: - In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.
==================================================

Focused review:

Weaknesses: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes "morph-fitting", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of "attract" and "repel" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.
I think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.
- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.
2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.
3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does "looking" - "look" + "walk" = "walking"? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings. 

Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes "morph-fitting", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of "attract" and "repel" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings. I think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.
Review Point: - Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.
Review Point: 2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.
Review Point: 3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does "looking" - "look" + "walk" = "walking"? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings.
==================================================

Focused review:

1. The overall claim (at least the first half of the paper) of the study being one of learning a bandit algorithm (as opposed to using a given bandit algorithm to learn the right actions in a given problem) is a bit misleading and somewhat overstated. In the end the study is about parameter-tuning a given class of algorithms (or a single algorithm with a tunable parameter). And if my reading of the experiments is correct (see more on this below), then they are all for the tuning of a single algorithm for a single instance of bandit. 2. The point of generalizing from $P$ to $\mathcal{P}$ is well taken, but while $P$ appears to be a joint distribution across all $K$ arms, it does not appear to be across time t. Is the assumption that rewards are independent across time? This would rule out Markov chains and other time-dependent processes as arms. 3. It is assumed that at each step the algorithm can sample m problem instances and run a policy on each in order to estimate the policy gradients. Is not clear to me what this assumption means in practice: a few application examples here would be very helpful. 4. Furthermore, it is not clear to me whether the regret from all these sampled instances are counted in the analysis. If not, then the regret analysis would be cheating, and if yes, then what really is the fundamental difference between this meta-learning and treating the overall distribution $\mathcal{P}$ as a single/ensemble bandit? 5. I fail to see the point of the experiments (see more comments below): in all three experiments, the problem seems to consist of a single bandit. The first two were said to include two bandit instances but they look identical; the third one has only one instance. ======= post rebuttal ============= The rebuttal addresses my points 2,3,4. On points 1&5, I may be misunderstanding the problem setting (not entirely I hope), but the rebuttal is not helping if that's the case. In particular, my point 5 on the experiment in 6.1 and 6.2, the rebuttal does not help at all: there are two bandit instances that are mirror images of each other, so why are they not considered the same instance -- surely we can just reindex/flip the arms, no? And in any case these arms are not pre-indexed and we just observe rewards, so for all intent and purposes these two instances would look identical to a player. I agree a single instance would not be very interesting, and that's exactly the point behind my question: why choose such an experiment?

Review Point: 1. The overall claim (at least the first half of the paper) of the study being one of learning a bandit algorithm (as opposed to using a given bandit algorithm to learn the right actions in a given problem) is a bit misleading and somewhat overstated. In the end the study is about parameter-tuning a given class of algorithms (or a single algorithm with a tunable parameter). And if my reading of the experiments is correct (see more on this below), then they are all for the tuning of a single algorithm for a single instance of bandit.
Review Point: 2. The point of generalizing from $P$ to $\mathcal{P}$ is well taken, but while $P$ appears to be a joint distribution across all $K$ arms, it does not appear to be across time t. Is the assumption that rewards are independent across time? This would rule out Markov chains and other time-dependent processes as arms.
Review Point: 3. It is assumed that at each step the algorithm can sample m problem instances and run a policy on each in order to estimate the policy gradients. Is not clear to me what this assumption means in practice: a few application examples here would be very helpful.
Review Point: 4. Furthermore, it is not clear to me whether the regret from all these sampled instances are counted in the analysis. If not, then the regret analysis would be cheating, and if yes, then what really is the fundamental difference between this meta-learning and treating the overall distribution $\mathcal{P}$ as a single/ensemble bandit?
Review Point: 5. I fail to see the point of the experiments (see more comments below): in all three experiments, the problem seems to consist of a single bandit. The first two were said to include two bandit instances but they look identical; the third one has only one instance. ======= post rebuttal ============= The rebuttal addresses my points 2,3,4. On points 1&5, I may be misunderstanding the problem setting (not entirely I hope), but the rebuttal is not helping if that's the case. In particular, my point 5 on the experiment in 6.1 and 6.2, the rebuttal does not help at all: there are two bandit instances that are mirror images of each other, so why are they not considered the same instance -- surely we can just reindex/flip the arms, no? And in any case these arms are not pre-indexed and we just observe rewards, so for all intent and purposes these two instances would look identical to a player. I agree a single instance would not be very interesting, and that's exactly the point behind my question: why choose such an experiment?
==================================================

Focused review:

- No crisp concept of containment, unlike box embeddings. - Instead rely on a KL divergence distance metric, and evaluate this purely on its ability to rank results to queries that have non-empty result-sets. - Not clear that this distance metric is calibrated, or that it could be used to implement an existential operator which resolves to true or false. - Some limitations of the union operation are discussed but I would like to see some empirical evaluation of exactly what this operation can or cannot do. While the application of De-Morgan's law is interesting, I believe it is not a-priori valid when applied to the negation and intersection operations presented here (which are not exact counterparts of logical negation and intersection). Also it is clear that the Beta distribution cannot represent the multimodal distributions that I would expect as a result of many union operations. - Comparison is made to other efforts to model compositional queries. Since the representation itself is new, I would also like to see comparison to the much larger literature from modeling individual triples (presented here as queries of type 1p).

Review Point: - Instead rely on a KL divergence distance metric, and evaluate this purely on its ability to rank results to queries that have non-empty result-sets.
Review Point: - Not clear that this distance metric is calibrated, or that it could be used to implement an existential operator which resolves to true or false.
Review Point: - Some limitations of the union operation are discussed but I would like to see some empirical evaluation of exactly what this operation can or cannot do. While the application of De-Morgan's law is interesting, I believe it is not a-priori valid when applied to the negation and intersection operations presented here (which are not exact counterparts of logical negation and intersection). Also it is clear that the Beta distribution cannot represent the multimodal distributions that I would expect as a result of many union operations.
Review Point: - Comparison is made to other efforts to model compositional queries. Since the representation itself is new, I would also like to see comparison to the much larger literature from modeling individual triples (presented here as queries of type 1p).
==================================================

Focused review:

On the other hand, there is no surprising result as far as I understand. Moreover, there is no numerical experiments. As the authors mentioned, their results can be useful for computing confidence intervals. This point could for example be discussed in an experimental section. Prop 2.1 is a side result as the authors said, and is obtained by imitating the proof of SL19. Moreover, the author only consider the case where the algorithm is initialized from a minimizer of f. Why? Same question for Prop 2.3 and 3.2. Th 1: Such purely asymptotic result may not be relevant for the neurips community. Prop 2.2. : The order of the bias is worse than the bias of the vanilla overdamped Langevin algorithm (O(h)). Since the result is not obtained similarly as the complexity result Prop 2.1 I am wondering if Prop 2.2 is tight. Th 2: I cannot evaluate this theorem due to a lack of clarity. First I don't understand the difference between biaised CLT and unbiased CLT. Why is the mean of the limit distribution not equal to the expectation to be computed? What is an "optimal" step size. Moreover, a better description of the set of \varphi considered would be appreciated, e.g. sufficient conditions to have \varphi = \mathcal A \phi for some \phi. Does the limit of \hat{\gamma}_n always exist? It is not clear to me. More importantly, the CLT proven does not show any improvement w.r.t. to CLT obtained for simpler versions of Langevin algorithm. Therefore Th 2 is indeed a negative result, which limits the impact of RLMC. Th 3. Same as Th. 1 Prop 3.1 seems to be an easy corollary of SL19, am I correct? Moreover, the constant are large.

Review Point: On the other hand, there is no surprising result as far as I understand. Moreover, there is no numerical experiments. As the authors mentioned, their results can be useful for computing confidence intervals. This point could for example be discussed in an experimental section. Prop 2.1 is a side result as the authors said, and is obtained by imitating the proof of SL19. Moreover, the author only consider the case where the algorithm is initialized from a minimizer of f. Why? Same question for Prop 2.3 and 3.2. Th 1: Such purely asymptotic result may not be relevant for the neurips community. Prop 2.2. : The order of the bias is worse than the bias of the vanilla overdamped Langevin algorithm (O(h)). Since the result is not obtained similarly as the complexity result Prop 2.1 I am wondering if Prop 2.2 is tight. Th 2: I cannot evaluate this theorem due to a lack of clarity. First I don't understand the difference between biaised CLT and unbiased CLT. Why is the mean of the limit distribution not equal to the expectation to be computed? What is an "optimal" step size. Moreover, a better description of the set of \varphi considered would be appreciated, e.g. sufficient conditions to have \varphi = \mathcal A \phi for some \phi. Does the limit of \hat{\gamma}_n always exist? It is not clear to me. More importantly, the CLT proven does not show any improvement w.r.t. to CLT obtained for simpler versions of Langevin algorithm. Therefore Th 2 is indeed a negative result, which limits the impact of RLMC. Th 3. Same as Th.
Review Point: 1 Prop 3.1 seems to be an easy corollary of SL19, am I correct? Moreover, the constant are large.
==================================================

Focused review:

- The networks need to be retrained after changing the pooling units to RNNpool - It is not explained if it is more difficult to train the networks when using such pooling, in the light of the fact that RNN's are harder to train than feed-forward convolutional networks. - Comparisons have not been made to methods of quantization, which also reduce memory and computational requirements without compromising on accuracy. In this regard there is only one paper [37] cited, which is just related to mixed precision networks. There are several others that deal with integers or just binary network, which could be cited: [Ref-1] "XNOR-net: Imagenet classification using binary convolutional neural networks." ECCV 2016 [Ref-2] "Data-Free Quantization through Weight Equalization and Bias Correction" ICCV 2019

Review Point: - The networks need to be retrained after changing the pooling units to RNNpool - It is not explained if it is more difficult to train the networks when using such pooling, in the light of the fact that RNN's are harder to train than feed-forward convolutional networks.
Review Point: - Comparisons have not been made to methods of quantization, which also reduce memory and computational requirements without compromising on accuracy. In this regard there is only one paper [37] cited, which is just related to mixed precision networks. There are several others that deal with integers or just binary network, which could be cited: [Ref-1] "XNOR-net: Imagenet classification using binary convolutional neural networks." ECCV 2016 [Ref-2] "Data-Free Quantization through Weight Equalization and Bias Correction" ICCV 2019
==================================================

Focused review:

Weaknesses
The part about modeling the number of atoms and the vertices of the molecular graph is very detailed. However, they only briefly mention how the edges and molecular properties could be modeled. In fact, they do not even mention how the edges of the generated vertices are inferred. They seem to use a heuristic technique, possibly just connecting vertices which are closer than a certain threshold. The authors refer for more details on this to section 4, but this information is missing there. Furthermore, the authors mention in section 4 that the atoms of the ligand are either carbon, nitrogen, oxygen, or fluorine. However, the ligand will most likely also contain hydrogen atoms, so how are they sampled? And how are the edge properties, like whether the bond is a single, double, or triple bond, determined? The method is only applied to a single dataset, i.e. CrossDocked2020. To get a better understanding, it would have been helpful to apply it to a toy dataset or showing a special case such as modeling the distribution for a fixed receptor. On CrossDocked2020 the molecules generated by the method have a worse binding affinity on average than those generated by GraphBP. However, the standard deviation of this number is high. The percentage of generated molecules that have a higher binding affinity than the reference is higher for the author's method, but this number is probably not a good performance measure due to the noise in the data.


Review Point: The part about modeling the number of atoms and the vertices of the molecular graph is very detailed. However, they only briefly mention how the edges and molecular properties could be modeled. In fact, they do not even mention how the edges of the generated vertices are inferred. They seem to use a heuristic technique, possibly just connecting vertices which are closer than a certain threshold. The authors refer for more details on this to section 4, but this information is missing there. Furthermore, the authors mention in section 4 that the atoms of the ligand are either carbon, nitrogen, oxygen, or fluorine. However, the ligand will most likely also contain hydrogen atoms, so how are they sampled? And how are the edge properties, like whether the bond is a single, double, or triple bond, determined? The method is only applied to a single dataset, i.e.
Review Point: CrossDocked2020. To get a better understanding, it would have been helpful to apply it to a toy dataset or showing a special case such as modeling the distribution for a fixed receptor. On CrossDocked2020 the molecules generated by the method have a worse binding affinity on average than those generated by GraphBP. However, the standard deviation of this number is high. The percentage of generated molecules that have a higher binding affinity than the reference is higher for the author's method, but this number is probably not a good performance measure due to the noise in the data.
==================================================

Focused review:

I am unconvinced of the significance and relevance of this work to the NeurIPS community. This work analyzes a 3-player game, each player with a 1-d "action" in a deterministic environment, i.e., the parameters of the game are fixed. If the authors assume the standard Euclidean regularizer, it seems as though the proofs in this paper can be mostly reasoned about by considering a linear dynamical system and basic results from variational inequalities (VI). For example given the known reference price dynamics, the Jacobian of the map F for an appropriate VI(F,X) is: J = [2\beta_1, -\delta_1, -\gamma_1] [-\delta_2, 2\beta_2, -\gamma_2] [-\theta_1, -\theta_2, 1]. Given assumptions on the parameters in the paper (line 90 and 113), this matrix is diagonally dominant, therefore, has eigenvalues with positive real part (note updating the system looks like x <-- x - step * (J * x + b) so positive means continuous time convergent). The authors assume the SNE exists in the interior of the set. With this assumption, the SNE is obviously unique because this linear system has a single fixed point. The upper left 2 x 2 can be shown to be positive definite (J+J^T > alpha * I > 0) which means that the appropriate VI that models the two firms is strongly monotone [1]. The solution to a strongly monotone VI is unique and a Nash equilibrium [2]. The point of this is simply to reason that the results are not very surprising. Given the complexity of the proof techniques used in the paper, I would have liked to have seen results that are general to n firms, consider a stochastic setting or unknown reference price dynamics, and/or examine a non-linear model. [1] "Projected Dynamical Systems and Variational Inequalities with Applications"; Nagurney & Zhang; '12 [2] "Convex Optimization, Game Theory, and Variational Inequality Theory"; Scutari, Palomar, Facchinei, Pang; '10 After rebuttal: 1. I see your point that the SNE w.r.t. a moving ref price is more nuanced than an NE w.r.t. a fixed ref price, but the moving ref price moves in an amount proportional to the firms; if the firms converge, then the ref price converges and is effectively fixed. My argument above was not only about uniqueness, the diagonal dominance also suggests convergence. 2. I understand decaying step sizes complicate analysis. Note fixed heterogenous step sizes are handled by VIs: x <-- x - step * D * (J * x + b) = x - step * (D * J * x + D * b) where D is a pos def diagonal matrix that accounts for differences in step sizes between players. Note diagonal dominance still holds for D * J and monotonicity of the upper left 2 x 2 still holds for a range of relative step sizes for the two firms. IMO, viewing the reference price as an OMD player is not a primary contribution. Any linear update rule can be written as gradient descent on some objective. Also, referring to it as OMD is a bit misleading; the ref price updates with vanilla online gradient descent. Maybe I missed a more general analysis. 3. The player loss functions in [4] are not fixed; they may change with time. Also, the view that opponents in a game can be viewed instead as a non-stationary environment is well known; you simply reverse this argument. All in all, this paper presents an analysis of a deterministic, 3-d, linear dynamical system. Complexities arise from the fact that a) the firms may use more general OMD updates (not necessarily OGD) which could make the system nonlinear and b) decaying step sizes while the ref price uses a fixed step size. I'm raising my score to marginally above as it appears the other reviewers are interested in this analysis. I've relayed my comments to the AC and I'm okay with acceptance if the other reviewers are.

Review Point: I am unconvinced of the significance and relevance of this work to the NeurIPS community. This work analyzes a 3-player game, each player with a 1-d "action" in a deterministic environment, i.e., the parameters of the game are fixed. If the authors assume the standard Euclidean regularizer, it seems as though the proofs in this paper can be mostly reasoned about by considering a linear dynamical system and basic results from variational inequalities (VI). For example given the known reference price dynamics, the Jacobian of the map F for an appropriate VI(F,X) is: J = [2\beta_1, -\delta_1, -\gamma_1] [-\delta_2, 2\beta_2, -\gamma_2] [-\theta_1, -\theta_2, 1]. Given assumptions on the parameters in the paper (line 90 and 113), this matrix is diagonally dominant, therefore, has eigenvalues with positive real part (note updating the system looks like x <-- x - step * (J * x + b) so positive means continuous time convergent). The authors assume the SNE exists in the interior of the set. With this assumption, the SNE is obviously unique because this linear system has a single fixed point. The upper left 2 x 2 can be shown to be positive definite (J+J^T > alpha * I > 0) which means that the appropriate VI that models the two firms is strongly monotone [1]. The solution to a strongly monotone VI is unique and a Nash equilibrium [2]. The point of this is simply to reason that the results are not very surprising. Given the complexity of the proof techniques used in the paper, I would have liked to have seen results that are general to n firms, consider a stochastic setting or unknown reference price dynamics, and/or examine a non-linear model. [1] "Projected Dynamical Systems and Variational Inequalities with Applications"; Nagurney & Zhang; '12 [2] "Convex Optimization, Game Theory, and Variational Inequality Theory"; Scutari, Palomar, Facchinei, Pang; '10 After rebuttal:
Review Point: 1. I see your point that the SNE w.r.t. a moving ref price is more nuanced than an NE w.r.t. a fixed ref price, but the moving ref price moves in an amount proportional to the firms; if the firms converge, then the ref price converges and is effectively fixed. My argument above was not only about uniqueness, the diagonal dominance also suggests convergence.
Review Point: 2. I understand decaying step sizes complicate analysis. Note fixed heterogenous step sizes are handled by VIs: x <-- x - step * D * (J * x + b) = x - step * (D * J * x + D * b) where D is a pos def diagonal matrix that accounts for differences in step sizes between players. Note diagonal dominance still holds for D * J and monotonicity of the upper left 2 x 2 still holds for a range of relative step sizes for the two firms. IMO, viewing the reference price as an OMD player is not a primary contribution. Any linear update rule can be written as gradient descent on some objective. Also, referring to it as OMD is a bit misleading; the ref price updates with vanilla online gradient descent. Maybe I missed a more general analysis.
Review Point: 3. The player loss functions in [4] are not fixed; they may change with time. Also, the view that opponents in a game can be viewed instead as a non-stationary environment is well known; you simply reverse this argument. All in all, this paper presents an analysis of a deterministic, 3-d, linear dynamical system. Complexities arise from the fact that a) the firms may use more general OMD updates (not necessarily OGD) which could make the system nonlinear and b) decaying step sizes while the ref price uses a fixed step size. I'm raising my score to marginally above as it appears the other reviewers are interested in this analysis. I've relayed my comments to the AC and I'm okay with acceptance if the other reviewers are.
==================================================

Focused review:

Weaknesses: w1. The main weakness is related to the gradient flow network, there are multiple things to model, a practical question is whether these gradient flow networks learn correctly and generalize, especially in a few data setting, it would make the paper stronger if the authors can provide any empirical evaluation of these networks to further verify.
w2: The optimization of the gradient flow networks are a little bit complicated, including both the inner- and outer-loop optimization process + many hyper-parameters, the authors should provide a detailed analysis in this aspect for real usage of this method in practice.
Overall, this paper is novel to me and I'm glad to see hyperbolic models in this setting, the method is clear and straightforward, though a detailed guidance of the method should be made available.


Review Point: w1. The main weakness is related to the gradient flow network, there are multiple things to model, a practical question is whether these gradient flow networks learn correctly and generalize, especially in a few data setting, it would make the paper stronger if the authors can provide any empirical evaluation of these networks to further verify.
Review Point: w2: The optimization of the gradient flow networks are a little bit complicated, including both the inner- and outer-loop optimization process + many hyper-parameters, the authors should provide a detailed analysis in this aspect for real usage of this method in practice. Overall, this paper is novel to me and I'm glad to see hyperbolic models in this setting, the method is clear and straightforward, though a detailed guidance of the method should be made available.
==================================================

Focused review:

1. The proposed method marginally improve over previous methods. 2. The proposed method is a combination of existing techniques. The main innovation I can see so far is the design of self-tuning networks for ensemble learning. 3. The paper claims that two sources of diversity jointly contribute to the overall ensemble model. Actually, there is a third source of diversity during training from p_t(\lambda_k) that controls the diversity of \lambda_k. Assuming p_t will not degenerate, how to effectively control the variance of p_t such that it can do a good local search job around \lambda_k? It would be great if there is a qualitative explanation that multiple ensemble members p_t(\lambda_k) for k=1,..., K work well independently and can jointly explore a wider space of lambda.

Review Point: 2. The proposed method is a combination of existing techniques. The main innovation I can see so far is the design of self-tuning networks for ensemble learning.
Review Point: 3. The paper claims that two sources of diversity jointly contribute to the overall ensemble model. Actually, there is a third source of diversity during training from p_t(\lambda_k) that controls the diversity of \lambda_k. Assuming p_t will not degenerate, how to effectively control the variance of p_t such that it can do a good local search job around \lambda_k? It would be great if there is a qualitative explanation that multiple ensemble members p_t(\lambda_k) for k=1,..., K work well independently and can jointly explore a wider space of lambda.
==================================================

Focused review:

The paper is overall interesting, well-written and makes a valuable contribution. I do, however, have some comments for the authors to consider (which in my mind, are potential limitations of the study):
- Comparison of the proposed unsupervised method with the supervised baseline is not suggestive because of the absence of augmentations in the supervised baseline. The authors should consider reporting performance on the decoding task when the supervised method employed data augmentations as well. - For completeness, the authors should also report how the hyperparameters for the linear decoder were determined. Ideally, I would’ve liked to see error bars for decoding accuracies as well (maybe by bootstrapping training set for the decoder?) - In future, the authors could also consider replacing the acc metric for decoding with better evaluation metrics for circular data, like circular correlation. This would treat the reach direction as a continuous variable (which it is) rather than as a discrete unordered variable (which it theoretically isn’t).
- The authors consider swapping only the block of variables belonging to the `content’ group. What would happen if the reconstruction term in the BlockSwap method swapped both the content and style of the augmented views? Does swapping only the content block necessarily facilitate disentanglement? If the BlockSwap was essential, does the proposed method required knowing the number of latent factors in advance. The authors could discuss these aspects in their conclusion/discussion. - When comparing the proposed SwapVAE against vanilla VAE, the authors should also consider reporting other metrics more commonly employed in VAE evaluation (likelihood etc.) and not just the reconstruction error (which can be trivially minimized). This is important, since the authors mention ‘generating realistic neural activity’ as a significant contribution of their paper. - The authors should also consider defining content and style more broadly as it relates to their specific neural application (e.g., as in Gabbay &Hosehn (2018)) where style is instance-specific(?) and content includes information that can be transferred among groups. More specifically, since their model is not sequential and does not capture the temporal dynamic structure, what do they really mean by ‘style’ represents the ‘movement dynamic’?


Review Point: The paper is overall interesting, well-written and makes a valuable contribution. I do, however, have some comments for the authors to consider (which in my mind, are potential limitations of the study):
Review Point: - Comparison of the proposed unsupervised method with the supervised baseline is not suggestive because of the absence of augmentations in the supervised baseline. The authors should consider reporting performance on the decoding task when the supervised method employed data augmentations as well.
Review Point: - For completeness, the authors should also report how the hyperparameters for the linear decoder were determined. Ideally, I would’ve liked to see error bars for decoding accuracies as well (maybe by bootstrapping training set for the decoder?) - In future, the authors could also consider replacing the acc metric for decoding with better evaluation metrics for circular data, like circular correlation. This would treat the reach direction as a continuous variable (which it is) rather than as a discrete unordered variable (which it theoretically isn’t).
Review Point: - The authors consider swapping only the block of variables belonging to the `content’ group. What would happen if the reconstruction term in the BlockSwap method swapped both the content and style of the augmented views? Does swapping only the content block necessarily facilitate disentanglement? If the BlockSwap was essential, does the proposed method required knowing the number of latent factors in advance. The authors could discuss these aspects in their conclusion/discussion.
Review Point: - When comparing the proposed SwapVAE against vanilla VAE, the authors should also consider reporting other metrics more commonly employed in VAE evaluation (likelihood etc.) and not just the reconstruction error (which can be trivially minimized). This is important, since the authors mention ‘generating realistic neural activity’ as a significant contribution of their paper.
Review Point: - The authors should also consider defining content and style more broadly as it relates to their specific neural application (e.g., as in Gabbay &Hosehn (2018)) where style is instance-specific(?) and content includes information that can be transferred among groups. More specifically, since their model is not sequential and does not capture the temporal dynamic structure, what do they really mean by ‘style’ represents the ‘movement dynamic’?
==================================================

Focused review:

weakness is the need for clarification of the actual derivation of the equivalence, which is currently quite unclear. Here are some specific issues (but my comments are not limited to these issues):
Eq. 15: no x on RHS.
Eq. 15/16 G() vs G[] (i.e. sometimes G has curly, sometimes square brackets)
The projection seems like the key step. Can we give some flavour of how that works in the main text, and whether any assumptions go in there (e.g. linearity).
It would be worth exploring the biology. Is there really this kind of adaptation present in e.g. head-direction cells? That would seem strange, as it would destabilise the bump. This mechanism would be a potential explanation.
It isn't clear how far the basic ideas here will extend (e.g. to learned representations, visual system etc.) But it is definitely an approach worth considering in future.


Review Point: is the need for clarification of the actual derivation of the equivalence, which is currently quite unclear. Here are some specific issues (but my comments are not limited to these issues): Eq.
Review Point: 15: no x on RHS. Eq. 15/16 G() vs G[] (i.e. sometimes G has curly, sometimes square brackets) The projection seems like the key step. Can we give some flavour of how that works in the main text, and whether any assumptions go in there (e.g. linearity). It would be worth exploring the biology. Is there really this kind of adaptation present in e.g. head-direction cells? That would seem strange, as it would destabilise the bump. This mechanism would be a potential explanation. It isn't clear how far the basic ideas here will extend (e.g. to learned representations, visual system etc.) But it is definitely an approach worth considering in future.
==================================================

Focused review:

Weakness:
1 The key issue of this paper is how this empirical finding can facilitate the SR task. In common sense, the restoration of textures, especially those regular patterns with strong perceptual prior, relies on the semantic information, or at least the similar patterns that are included in the training set. However, according to the findings in this paper, the model mainly learns the degradation-related information. Is this good or bad for the performance of SR? or in other words if the authors recommend the following researches to keep the global residual and the adversarial loss in the designing of SR network? 2 There lacks sufficient experiments to verify the performance of blind SR with the proposed DDR guidance. Detailed comparison against the state-of-the-art methods should be conducted.


Review Point: 1 The key issue of this paper is how this empirical finding can facilitate the SR task. In common sense, the restoration of textures, especially those regular patterns with strong perceptual prior, relies on the semantic information, or at least the similar patterns that are included in the training set. However, according to the findings in this paper, the model mainly learns the degradation-related information. Is this good or bad for the performance of SR? or in other words if the authors recommend the following researches to keep the global residual and the adversarial loss in the designing of SR network?
Review Point: 2 There lacks sufficient experiments to verify the performance of blind SR with the proposed DDR guidance. Detailed comparison against the state-of-the-art methods should be conducted.
==================================================

Focused review:

Weaknesses:
• Not clear what the algorithmic contribution is; the methods provided in section 2 and algorithm1 seem to be already existent.
• Recently developed EBM approaches have been effectively applied for HEP event detection but no new machine learning approaches have been proposed
• Ablation experiments to understand the effect of the effect of various components and especially the transformer network is crucial to understand the model performance
• How are the hyperparameters for the learning chosen?
• Do the results hold with multiple random initializations of the model?


Review Point: • Not clear what the algorithmic contribution is; the methods provided in section 2 and algorithm1 seem to be already existent.
Review Point: • Recently developed EBM approaches have been effectively applied for HEP event detection but no new machine learning approaches have been proposed • Ablation experiments to understand the effect of the effect of various components and especially the transformer network is crucial to understand the model performance • How are the hyperparameters for the learning chosen?
Review Point: • Do the results hold with multiple random initializations of the model?
==================================================

Focused review:

Weakness
I have two main concerns about the theoretical and practical aspects of the results.
Theoretical.
I am not sure the expectation in Thm.1 is taken w.r.t which random variables. What is the random variable in Eq. (3)?
I have not checked the proof but suppose that Thm.1. is soundly proven. I am not sure Thm 1. obtains exponential growth. The Thm. 1 states
E
[
a
n
/
a
n
+
1
]
>
1
where
a
n
is a factor of gradient variance in layer
n
. Although it is stated such lower-bound in expectation leads to an exponential growth in
a
n
, I am not sure about this. If there was no expectation, it would be possible to derive exponential growth. However, the expectation makes the recurrence complicated. Would the authors mind if I ask to elaborate on this?
My experiments show the norm of the gradient in early layers (close to input) grows at an exponential rate with depth. While the paper suggests an exponential growth in the norm of the gradient in deep layers (close to outputs).
Practical. The proposed algorithm is very similar to adaptive gradient clipping proposed by Brock, A., De, S., Smith, S. L., and Simonyan, K. in a paper titled "High-performance large-scale image recognition without normalization". What is the difference between the proposed method and the adaptive gradient clipping?


Review Point: I have two main concerns about the theoretical and practical aspects of the results. Theoretical. I am not sure the expectation in Thm.1 is taken w.r.t which random variables. What is the random variable in Eq. (3)? I have not checked the proof but suppose that Thm.1. is soundly proven. I am not sure Thm 1. obtains exponential growth. The Thm.
Review Point: 1 states E [ a n / a n + 1 ] > 1 where a n is a factor of gradient variance in layer n . Although it is stated such lower-bound in expectation leads to an exponential growth in a n , I am not sure about this. If there was no expectation, it would be possible to derive exponential growth. However, the expectation makes the recurrence complicated. Would the authors mind if I ask to elaborate on this? My experiments show the norm of the gradient in early layers (close to input) grows at an exponential rate with depth. While the paper suggests an exponential growth in the norm of the gradient in deep layers (close to outputs). Practical. The proposed algorithm is very similar to adaptive gradient clipping proposed by Brock, A., De, S., Smith, S. L., and Simonyan, K. in a paper titled "High-performance large-scale image recognition without normalization". What is the difference between the proposed method and the adaptive gradient clipping?
==================================================

Focused review:

===After rebuttal=== I read the reubttal and I think the proposed method has computational complexity issues and it should be compared with the naive solution of inteverening on the target variable (estimating MEC from finite sample size). Thus, I decided to keep my score unchanged. ================ - The main assumption of ICP may not be satisified in real world scenarios. In particular, in the linear model, it means that the variance of exognous noise of target variable should not be changed across environments. - It is not clear for me why we cannot intervene on the target variable and get enough samples to recover its parents. It might be a good idea to compare this solution with the proposed policies. - It is required to analyze the time complexity of the proposed policies mentioned in Section 4. - It is not clear whether each experiment consists of a single intervention or not in the proposed policies. It is better to clarify this issue.

Review Point: ===After rebuttal=== I read the reubttal and I think the proposed method has computational complexity issues and it should be compared with the naive solution of inteverening on the target variable (estimating MEC from finite sample size). Thus, I decided to keep my score unchanged. ================ - The main assumption of ICP may not be satisified in real world scenarios. In particular, in the linear model, it means that the variance of exognous noise of target variable should not be changed across environments.
Review Point: - It is not clear for me why we cannot intervene on the target variable and get enough samples to recover its parents. It might be a good idea to compare this solution with the proposed policies.
Review Point: - It is required to analyze the time complexity of the proposed policies mentioned in Section 4.
Review Point: - It is not clear whether each experiment consists of a single intervention or not in the proposed policies. It is better to clarify this issue.
==================================================

Focused review:

Weaknesses / Concerns / Comments:
The paper does not have sufficient contributions. There is no justification for the consistency term and it is not clear to me that why such a simple linear formulation can improve cell type fraction approximation.
Is there sample-to-sample correspondence between
B
i
and
B
i
s
i
m
?
While bulk measurements and simulated dataset (based on single-cell measurements) consist of different sources of noise and biological variabilities, can you justify why a linear mixture of these two cell measurements can enhance cell deconvolution?
Is there any equation missing after the third equation, Section 2 (page 2)?
In Section 2, 4th equation, why does the signature matrix which is a
c
x
n
matrix is a function of three indices, including sample id? Does not
S
represent the expression profiles across cell types?
Section 2.1.2, Eq. 4 should be written as
x
i
s
i
, not
s
i
x
i
.
Is the number of cell types given to the model apriori? How does the deconvolution performance change in the absence of having the true number of cell types in the bulk RNA-seq data?
In Figure 1.a, I think the first “+” operation should be “=”.
Why do you use a variational approach to learn signature matrix? Why not just an MLP, similar to
X
matrix estimation? Is there any plan to use the latent factors?
How many genes are profiled in datasets in Tables 1 & 2? Are you using the same subset of genes in Eq. 3?
I think reporting MSE (which is reported in the supplement) is more informative rather than reporting correlation between estimated variables and true ones. My suggestion is to report MSE results in the main text.


Review Point: / Concerns / Comments: The paper does not have sufficient contributions. There is no justification for the consistency term and it is not clear to me that why such a simple linear formulation can improve cell type fraction approximation. Is there sample-to-sample correspondence between B i and B i s i m ? While bulk measurements and simulated dataset (based on single-cell measurements) consist of different sources of noise and biological variabilities, can you justify why a linear mixture of these two cell measurements can enhance cell deconvolution? Is there any equation missing after the third equation, Section 2 (page 2)? In Section 2, 4th equation, why does the signature matrix which is a c x n matrix is a function of three indices, including sample id? Does not S represent the expression profiles across cell types? Section 2.1.2, Eq.
Review Point: 4 should be written as x i s i , not s i x i . Is the number of cell types given to the model apriori? How does the deconvolution performance change in the absence of having the true number of cell types in the bulk RNA-seq data? In Figure 1.a, I think the first “+” operation should be “=”. Why do you use a variational approach to learn signature matrix? Why not just an MLP, similar to X matrix estimation? Is there any plan to use the latent factors? How many genes are profiled in datasets in Tables 1 & 2? Are you using the same subset of genes in Eq. 3? I think reporting MSE (which is reported in the supplement) is more informative rather than reporting correlation between estimated variables and true ones. My suggestion is to report MSE results in the main text.
==================================================

Focused review:

Weaknesses:
A clear definition of N-way K-shot few-shot tasks is not provided.
What does interference mean in the context of Figure 1?
What is the definition of semantic features? Are they text-based features?
How are visual-feature class centers and label-derived semantic features computed?
What is the difference between D_b, D_v, and D_n datasets?
Effort has to be put in Section 3 to improve its clarity and readability. It is highly recommended to clearly define the problem to be addressed and the approach to be outlined along with any notation in the beginning of the section.
No discussion of the baselines and their settings is provided. Also, what is the meaning of dashes in Tables 1 and 2?
It is not clear why jointly learning visual and semantic features will not improve performance?
Some minor comments follow:
Abstract: "to incorporates" -> "to incorporate"
pg. 1: Please define acronyms before they are first used, e.g., FSL
pg. 2: "Few-shot classification methods exhibits" -> "Few-shot classification methods exhibit" "learning a discriminative representations" -> "learning discriminative representations"
pg. 3: "the these methods is" -> "these methods is"
Table 2 caption: "in Tabel1" -> "in Table 1"
pg. 9: "except" -> "expect"
pg. 6: "We computes" -> "We compute"


Review Point: A clear definition of N-way K-shot few-shot tasks is not provided. What does interference mean in the context of Figure 1? What is the definition of semantic features? Are they text-based features? How are visual-feature class centers and label-derived semantic features computed? What is the difference between D_b, D_v, and D_n datasets? Effort has to be put in Section 3 to improve its clarity and readability. It is highly recommended to clearly define the problem to be addressed and the approach to be outlined along with any notation in the beginning of the section. No discussion of the baselines and their settings is provided. Also, what is the meaning of dashes in Tables 1 and 2? It is not clear why jointly learning visual and semantic features will not improve performance? Some minor comments follow: Abstract: "to incorporates" -> "to incorporate" pg.
Review Point: 1: Please define acronyms before they are first used, e.g., FSL pg.
Review Point: 2: "Few-shot classification methods exhibits" -> "Few-shot classification methods exhibit" "learning a discriminative representations" -> "learning discriminative representations" pg.
Review Point: 3: "the these methods is" -> "these methods is" Table 2 caption: "in Tabel1" -> "in Table 1" pg.
==================================================

Focused review:

Weaknesses: - The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples.  - I find this statement in the supplemental section D.4 questionable: "Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states?  - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version.   - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model.    Minor comments and questions :  - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from "scratch". (Though Figure 3 makes it clear that pretrained embeddings have little impact).  - I think the authors risk overclaiming when they write "Existing language GANs... have shown little to no performance improvements over traditional language models", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM). 

Review Point: - The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples.
Review Point: - I find this statement in the supplemental section D.4 questionable: "Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states?
Review Point: - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version.
Review Point: - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model. Minor comments and questions :
Review Point: - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from "scratch". (Though Figure 3 makes it clear that pretrained embeddings have little impact).
Review Point: - I think the authors risk overclaiming when they write "Existing language GANs... have shown little to no performance improvements over traditional language models", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).
==================================================

Focused review:

Weaknesses: - Lack of novelty. The main innovation of this paper is the proposed easy2hard multi-modal fusion module, which contains a cross-modal attention module and a gating strategy. However, both of them have been used many times in previous MMT papers [1-4]. I don’t think the approach in this paper is significantly different from the previous ones. - Lack of explanation and verification of the proposed method. The motivation of this paper is good and reasonable, which aims to align textual and visual features at multiple granularities. However, there is no evidence that the proposed approach can achieve this goal. For example, the global-level alignment is just computing a gate to filter the textual features， which does not seem to learn cross-modal alignment information. I think it is not sufficient to justify the effectiveness of the proposed module only from the BLEU score and the case study. - Too many typos. Cross-model -> Cross-modal, among -> Among, prosed -> proposed,
α
n
i
->
α
f
i
, employe -> employ.
[1] On Vision Features in Multimodal Machine Translation. ACL 2022. [2] Neural Machine Translation with Phrase-Level Universal Visual Representations. ACL 2022. [3] Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation. ACL 2021. [4] Neural Machine Translation with Universal Visual Representation. ICLR 2020.


Review Point: - Lack of novelty. The main innovation of this paper is the proposed easy2hard multi-modal fusion module, which contains a cross-modal attention module and a gating strategy. However, both of them have been used many times in previous MMT papers [1-4]. I don’t think the approach in this paper is significantly different from the previous ones.
Review Point: - Lack of explanation and verification of the proposed method. The motivation of this paper is good and reasonable, which aims to align textual and visual features at multiple granularities. However, there is no evidence that the proposed approach can achieve this goal. For example, the global-level alignment is just computing a gate to filter the textual features， which does not seem to learn cross-modal alignment information. I think it is not sufficient to justify the effectiveness of the proposed module only from the BLEU score and the case study.
Review Point: - Too many typos. Cross-model -> Cross-modal, among -> Among, prosed -> proposed, α n i -> α f i , employe -> employ. [1] On Vision Features in Multimodal Machine Translation. ACL 2022. [2] Neural Machine Translation with Phrase-Level Universal Visual Representations. ACL 2022. [3] Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation. ACL 2021. [4] Neural Machine Translation with Universal Visual Representation. ICLR 2020.
==================================================

Focused review:

The key issue is in Eq. 6, which appears to indicate that the normalising flow transformation is applied at each timepoint independently. This renders the model somewhat trivial (a Kalman filter with nonlinear outputs), extensively studied (e.g. in the EKF literature). It is well known that linear dynamics in a Kalman filter is a discretisation of an underlying continuous dynamical system, and you could use unevenly spaced observations if desired. Here are some possible claims that would render the work more interesting: 1.) Inclusion of the Jacobean in the output transformation renders the ML solution a better characterisation of the Bayesian solution. 2.) Having a normalising flow that depends on the value of the Weiner process at all past time steps (which allows much richer temporal dependencies). 3.) Arguing that the presently described process is surprisingly effective, by looking at a more empirical examples, and comparing to SOTA performance from referenced papers. Eq 12 might do some of this, but it is extremely unclear.

Review Point: The key issue is in Eq. 6, which appears to indicate that the normalising flow transformation is applied at each timepoint independently. This renders the model somewhat trivial (a Kalman filter with nonlinear outputs), extensively studied (e.g. in the EKF literature). It is well known that linear dynamics in a Kalman filter is a discretisation of an underlying continuous dynamical system, and you could use unevenly spaced observations if desired. Here are some possible claims that would render the work more interesting:
Review Point: 1.) Inclusion of the Jacobean in the output transformation renders the ML solution a better characterisation of the Bayesian solution.
Review Point: 2.) Having a normalising flow that depends on the value of the Weiner process at all past time steps (which allows much richer temporal dependencies).
Review Point: 3.) Arguing that the presently described process is surprisingly effective, by looking at a more empirical examples, and comparing to SOTA performance from referenced papers. Eq 12 might do some of this, but it is extremely unclear.
==================================================

Focused review:

Weakness: 1. The only difference between using PTQ in LIC models and conventional neural networks is the format of task loss. This paper simply applies the PTQ method for the task of LIC, thus makes somewhat trivial technical contributions. 2. The theoretical derivations and the relevant conclusions in this paper have been given in the paper of AdaRound [1]. 3. The setting of this paper is to transform the neural network from FP32 to INT8. But actually this setting is defined by the author without any explanations. Some other choices can be adopted, e.g., INT4 or unsigned INT16. 4. There are some vague claims without evidence, such as, ‘Such inappropriate processing of bias may lead to catastrophic results. For example, having the bias in INT32 precision may cause data overflow of the INT32 accumulator; while setting bias as zero would degrade the model performance significantly.’ References: [1] Up or down? adaptive rounding for post-training quantization. Nagel et al., ICML 2020.


Review Point: 1. The only difference between using PTQ in LIC models and conventional neural networks is the format of task loss. This paper simply applies the PTQ method for the task of LIC, thus makes somewhat trivial technical contributions.
Review Point: 2. The theoretical derivations and the relevant conclusions in this paper have been given in the paper of AdaRound [1].
Review Point: 3. The setting of this paper is to transform the neural network from FP32 to INT8. But actually this setting is defined by the author without any explanations. Some other choices can be adopted, e.g., INT4 or unsigned INT16.
Review Point: 4. There are some vague claims without evidence, such as, ‘Such inappropriate processing of bias may lead to catastrophic results. For example, having the bias in INT32 precision may cause data overflow of the INT32 accumulator; while setting bias as zero would degrade the model performance significantly.’ References: [1] Up or down? adaptive rounding for post-training quantization. Nagel et al., ICML 2020.
==================================================

Focused review:

weaknesses of previous topic models. Even if incorporating the sequential information was already studied (e.g., HTMM), there is still room for improvement. The graph that one can construct at the end when the parameters are learned (Figure 4, left) is particularly interesting for interpretability. It seems that it can really improve the understanding of the topic content. The approach seems to outperform (recent) prior works in term of topic coherence, diversity, and clustering.
Cons:
Not a rejection criterion per se, but your approach is beaten in few configurations, e.g. NMI on TMN and 20NG with a high number of topic for clustering and TD score for 20NG, BNC and Reuters, again with high number of topics. Do you have an intuition on why your method performs slightly worst when the number of topics grows? Even if you provide the effect of the window size in supp.mat. on the 20NG dataset, it would be interesting to provide further experiments regarding this parameter, on more datasets. Similarly, you should provide more experiments to test the effect of the word embedding used on the performances of your model.
L35: “For example, LDA discovers word such as “station” in a topic which does not seem to be that insightful. » You do not introduce this example. Where does it come from? You cannot drop examples like that and expect that the reader will understand your point. L 108: “We also filter out the stop words, and words and dependency edges with low frequency among the whole collection to reduce noise.”: it should be in the experimental section, and you should add the parameters you use (which stop words, the threshold for selection, etc…). All your experiments must be reproduceable. The paper is not well written. There is a lot of grammatical errors, typos, and some sentences are not clear. Additional remarks: 2.2 : there should be a way to simplify the notation, this subsection is hard to follow. In the abstract and in the experiment section, you should use present tense, not past tense. Why using AVI rather than the usual expression (VAE)? L 100: “ we can donate a document” this is not clear, donate does not fit here L 104: “with nodes on the vocabulary” : should be “in the vocabulary” L 128: just based  based solely on the topic assignment ? L 136 : should be cast into  cast as L 210 : “Now, the only challenge to compute the ELBO loss Ld is at the last term in Eqaution: “ this sentence is not grammatically correct and Eqaution should be Equation L 58 : Graphic Neural Topic Modeling  Graph Neural Topic Modeling. This is the name of your method!

I've read the authors' response and the other reviews. The authors answered most of my remarks and I choose to keep my score of 7. I'd be interested to read the reactions of the other reviewers before, but they never came...
We can read "N/A" in the form. I guess developing a new topic modeling approach let the authors ignore this point.


Review Point: of previous topic models. Even if incorporating the sequential information was already studied (e.g., HTMM), there is still room for improvement. The graph that one can construct at the end when the parameters are learned (Figure 4, left) is particularly interesting for interpretability. It seems that it can really improve the understanding of the topic content. The approach seems to outperform (recent) prior works in term of topic coherence, diversity, and clustering. Cons: Not a rejection criterion per se, but your approach is beaten in few configurations, e.g. NMI on TMN and 20NG with a high number of topic for clustering and TD score for 20NG, BNC and Reuters, again with high number of topics. Do you have an intuition on why your method performs slightly worst when the number of topics grows? Even if you provide the effect of the window size in supp.mat. on the 20NG dataset, it would be interesting to provide further experiments regarding this parameter, on more datasets. Similarly, you should provide more experiments to test the effect of the word embedding used on the performances of your model.
Review Point: L35: “For example, LDA discovers word such as “station” in a topic which does not seem to be that insightful. » You do not introduce this example. Where does it come from? You cannot drop examples like that and expect that the reader will understand your point. L 108: “We also filter out the stop words, and words and dependency edges with low frequency among the whole collection to reduce noise.”: it should be in the experimental section, and you should add the parameters you use (which stop words, the threshold for selection, etc…). All your experiments must be reproduceable. The paper is not well written. There is a lot of grammatical errors, typos, and some sentences are not clear. Additional remarks: 2.2 : there should be a way to simplify the notation, this subsection is hard to follow. In the abstract and in the experiment section, you should use present tense, not past tense. Why using AVI rather than the usual expression (VAE)? L 100: “ we can donate a document” this is not clear, donate does not fit here L 104: “with nodes on the vocabulary” : should be “in the vocabulary” L 128: just based  based solely on the topic assignment ? L 136 : should be cast into  cast as L 210 : “Now, the only challenge to compute the ELBO loss Ld is at the last term in Eqaution: “ this sentence is not grammatically correct and Eqaution should be Equation L 58 : Graphic Neural Topic Modeling  Graph Neural Topic Modeling. This is the name of your method! I've read the authors' response and the other reviews. The authors answered most of my remarks and I choose to keep my score of 7. I'd be interested to read the reactions of the other reviewers before, but they never came... We can read "N/A" in the form. I guess developing a new topic modeling approach let the authors ignore this point.
==================================================

Focused review:

Weakness:  The major quality problem of this paper is clarity. In terms of clarity, there are several confusing places in the paper, especially in equation 9, 10, 11, 12.   1) What is s_{i,j} in these equations? In definition 1, the author mentions that s_{i,j} denotes edge weights in the graph, but what are their values exactly in the experiments?  Are they 0/1 or continuous values?   2) How is the diffusion map computed for structural embedding in 10 and 12? Is it using equation 1 only with the learned structural embedding and without text embedding?  3) Why is the diffusion convolution operator only applied to text embedding? Can it also be applied to structural embedding?  On the other hand, if the author wants to capture global information in the graph as claimed between line 191 and line 194, why not directly use the diffusion map in equation (1) on text embedding instead of applying the diffusion convolution operator in 4.2? It's confusing to me the relationship between equation (1) and equation (5), (6) and (7) in section 4.2. In other words, there are two methods that could capture the global information: equation (1), and equation (5)(6)(7). Equation (1) is applied to structural embedding in equation (10) and (12); equation (5)(6)(7) are applied to textual embeddings. The author should explain why they do so.   4) I wonder whether the structural embedding is really necessary in this case, since in the learning process, the structural embedding just involves a embedding table lookup. The author does not explain the point of using a structural embedding, especially in such a way. What if just use diffusion text embedding? I don't see any experimental results proving the effectiveness of structural embedding in Table 1.   5) What's the motivation of each part in equation (8)? For example, what's the motivation of maximizing the probability of textual embedding of vertex i conditioned on the diffusion map of structural embedding of vertex j in equation (12)?   6) In line 135, the author says "Initially the network only has a few active vertices, due to sparsity." How is "active vertices" defined here?   7) In 5.2, when the author trains the SVM classifier, do they also fine-tune the embeddings or just freeze them? There are many existing CNN and RNN based neural classifier for text classification. What if just use any of those off-the-shelf methods on the text embedding without the diffusion process and fine-tune those embedding? This is typically a strong baseline for text classification, but there is no corresponding experimental results in Figure 5.   8) In line 234, how those objective function weights obtained? Are they tuned on any development set? Has the author tried only using a subset of those objectives? It's not clear how important each of those four objectives is.   9) In line 315, the author attributes the result of Table 3 to "both structure and text information". However, the fact that the method picks vertex 3 is due to diffusion convolution operator, as explained in line 313.  Does the "structure" here mean the diffusion convolution operator or the structural embedding?   

Review Point: The major quality problem of this paper is clarity. In terms of clarity, there are several confusing places in the paper, especially in equation 9, 10, 11, 12.
Review Point: 1) What is s_{i,j} in these equations? In definition 1, the author mentions that s_{i,j} denotes edge weights in the graph, but what are their values exactly in the experiments? Are they 0/1 or continuous values?
Review Point: 2) How is the diffusion map computed for structural embedding in 10 and 12? Is it using equation 1 only with the learned structural embedding and without text embedding?
Review Point: 3) Why is the diffusion convolution operator only applied to text embedding? Can it also be applied to structural embedding? On the other hand, if the author wants to capture global information in the graph as claimed between line 191 and line 194, why not directly use the diffusion map in equation (1) on text embedding instead of applying the diffusion convolution operator in 4.2? It's confusing to me the relationship between equation (1) and equation (5), (6) and (7) in section 4.2. In other words, there are two methods that could capture the global information: equation (1), and equation (5)(6)(7). Equation (1) is applied to structural embedding in equation (10) and (12); equation (5)(6)(7) are applied to textual embeddings. The author should explain why they do so.
Review Point: 4) I wonder whether the structural embedding is really necessary in this case, since in the learning process, the structural embedding just involves a embedding table lookup. The author does not explain the point of using a structural embedding, especially in such a way. What if just use diffusion text embedding? I don't see any experimental results proving the effectiveness of structural embedding in Table 1.
Review Point: 5) What's the motivation of each part in equation (8)? For example, what's the motivation of maximizing the probability of textual embedding of vertex i conditioned on the diffusion map of structural embedding of vertex j in equation (12)?
Review Point: 6) In line 135, the author says "Initially the network only has a few active vertices, due to sparsity." How is "active vertices" defined here?
Review Point: 7) In 5.2, when the author trains the SVM classifier, do they also fine-tune the embeddings or just freeze them? There are many existing CNN and RNN based neural classifier for text classification. What if just use any of those off-the-shelf methods on the text embedding without the diffusion process and fine-tune those embedding? This is typically a strong baseline for text classification, but there is no corresponding experimental results in Figure 5.
Review Point: 8) In line 234, how those objective function weights obtained? Are they tuned on any development set? Has the author tried only using a subset of those objectives? It's not clear how important each of those four objectives is.
Review Point: 9) In line 315, the author attributes the result of Table 3 to "both structure and text information". However, the fact that the method picks vertex 3 is due to diffusion convolution operator, as explained in line 313. Does the "structure" here mean the diffusion convolution operator or the structural embedding?
==================================================

Focused review:

Weaknesses and Issues
W1) Need to clarify misleading "constant approximation guarantee"
The paper claims to have a "constant approximation guarantee," which is misleading. This guarantee is only with respect to the proposed selection criteria
f
(
S
)
, but there is no guarantee that a set
S
which maximizes
f
(
S
)
is actually a good training set. To the best of my knowledge, the correlation between
f
(
S
)
and test accuracy is merely heuristic and empirical. This point needs to be made much clearer in the text to prevent readers from conflating a "constant approximation guarantee" for maximizing
f
(
S
)
and the (non-existent) guarantee for maximizing test accuracy.
In contrast, active learning methods based on influence functions (see W2 below), actually try to directly maximize test set accuracy.
W2) Missing discussions and comparisons against other related works
The paper is missing discussions and comparisons against about other relevant active learning techniques such as Deep Bayesian Active Learning (Gal et al., 2017) and Active Learning via Influence Functions (Xu and Kazantsev, 2019). Note that DBAL can be considered as another "uncertainty" metric. However, active learning via influence functions does not seem to fall within the uncertainty vs. diversity framework that this submission describes. Therefore, I would be particularly interested in seeing a discussion and comparison between the proposed framework and active learning via influence functions.
W3) How does the proposed framework address the limitation that "no single subset selection criterion achieves the best performance"?
The authors write, "To address the first limitation, we develop a unified algorithm based on maximization of a submodular function, which can combine different selection objectives." The limitation that they refer to is "First, there is no single subset selection criterion that achieves the best performance on different classification datasets."
It is unclear to me how the proposed framework addresses this limitation. Putting the various selection objectives into a single selection criteria function
f
does not magically mean that the combined criteria achieves the best performance on different classification datasets. The authors' claim seems very misleading.
W4) Need better justification and discussion about
f
t
r
i
p
l
e
It is unclear to me how
f
t
r
i
p
l
e
is practically different from
f
d
i
v
e
r
s
i
t
y
. Are there situations where the two are very different? Is there a theoretical justification for why including both metrics is better than including just one of the two? Could you show a scatter plot of the two metrics (e.g., sample many different sets
S
, then plot
f
d
i
v
e
r
s
i
t
y
(
S
)
on the x-axis and
f
t
r
i
p
l
e
(
S
)
on the y-axis)?
Also, I'm not sure I fully understand what the "volume consumed by the embeddings of the triplet
i
,
j
,
k
" means. A more thorough discussion of this is warranted. How is
T
t
h
r
e
s
h
chosen? And why use a threshold on
t
(
i
,
j
,
k
)
, instead of a proportional penalty on
t
(
i
,
j
,
k
)
?
W5) How to select hyperparameters parameters like
λ
i
,
γ
,
T
t
h
r
e
s
h
, and
k
I would appreciate a discussion on how to select the hyperparameters mentioned throughout the paper. The values chosen seem rather arbitrary.
W6) What about active learning for regression?
The paper only describes active learning for classification problems. The authors should make this clearer from the start of the paper. Are any of the ideas proposed here generalizable to regression settings?
W7) Clarity Issues - please address each point individually in your response
In the "Subset selection and model evaluation" paragraph under "Section 3: Problem Statement", it is unclear what edges are included in
E
. For example, is
E
directed? Is
E
fully-connected? Only after reading through the end of the paper did I understand
E
to be the edge set of a
k
-degree nearest neighbor, where
k
is a hyperparameter. This point should be made clearer.
Why define the margin score as
m
(
x
)
=
1
−
(
P
(
Y
=
b
b
 
x
)
−
P
(
Y
=
s
b
 
x
)
)
? Why not define it more simply as
m
(
x
)
=
P
(
Y
=
b
b
 
x
)
−
P
(
Y
=
s
b
 
x
)
?
What unary function did you use for
f
d
i
v
e
r
s
i
t
y
in your experiments? Did you use the constant term?
Equation (7) says
M
=
(
G
,
I
p
)
, but then defines
I
c
. Looks like a typo.
The authors write that on ImageNet, "we found that 15% of all possible boundaries were covered." Help me interpret this statement. Is 15% surprising? Is this more or less than the authors expected?
In equation (10), what is
M
? It seems like
M
is used both as an integer (for the number of decision boundaries) and as a matroid. Please clarify.
In the "Evaluation" subsection under "Section 6: Experiments," the authors write, "Note that, on this dataset, accuracies with 80% and 90% subsets exceed that with the full dataset." Which dataset is this referring to?
In the Ablation study subsection, what are the standard deviations for the reported improvement percentages? Are these relative improvements, or absolute improvements? On which dataset(s) did the authors test the larger values of
k
?


Review Point: and Issues W1) Need to clarify misleading "constant approximation guarantee" The paper claims to have a "constant approximation guarantee," which is misleading. This guarantee is only with respect to the proposed selection criteria f ( S ) , but there is no guarantee that a set S which maximizes f ( S ) is actually a good training set. To the best of my knowledge, the correlation between f ( S ) and test accuracy is merely heuristic and empirical. This point needs to be made much clearer in the text to prevent readers from conflating a "constant approximation guarantee" for maximizing f ( S ) and the (non-existent) guarantee for maximizing test accuracy. In contrast, active learning methods based on influence functions (see W2 below), actually try to directly maximize test set accuracy.
Review Point: W2) Missing discussions and comparisons against other related works The paper is missing discussions and comparisons against about other relevant active learning techniques such as Deep Bayesian Active Learning (Gal et al., 2017) and Active Learning via Influence Functions (Xu and Kazantsev, 2019). Note that DBAL can be considered as another "uncertainty" metric. However, active learning via influence functions does not seem to fall within the uncertainty vs. diversity framework that this submission describes. Therefore, I would be particularly interested in seeing a discussion and comparison between the proposed framework and active learning via influence functions.
Review Point: W3) How does the proposed framework address the limitation that "no single subset selection criterion achieves the best performance"? The authors write, "To address the first limitation, we develop a unified algorithm based on maximization of a submodular function, which can combine different selection objectives." The limitation that they refer to is "First, there is no single subset selection criterion that achieves the best performance on different classification datasets." It is unclear to me how the proposed framework addresses this limitation. Putting the various selection objectives into a single selection criteria function f does not magically mean that the combined criteria achieves the best performance on different classification datasets. The authors' claim seems very misleading.
Review Point: W4) Need better justification and discussion about f t r i p l e It is unclear to me how f t r i p l e is practically different from f d i v e r s i t y . Are there situations where the two are very different? Is there a theoretical justification for why including both metrics is better than including just one of the two? Could you show a scatter plot of the two metrics (e.g., sample many different sets S , then plot f d i v e r s i t y ( S ) on the x-axis and f t r i p l e ( S ) on the y-axis)? Also, I'm not sure I fully understand what the "volume consumed by the embeddings of the triplet i , j , k " means. A more thorough discussion of this is warranted. How is T t h r e s h chosen? And why use a threshold on t ( i , j , k ) , instead of a proportional penalty on t ( i , j , k ) ?
Review Point: W5) How to select hyperparameters parameters like λ i , γ , T t h r e s h , and k I would appreciate a discussion on how to select the hyperparameters mentioned throughout the paper. The values chosen seem rather arbitrary.
Review Point: W6) What about active learning for regression? The paper only describes active learning for classification problems. The authors should make this clearer from the start of the paper. Are any of the ideas proposed here generalizable to regression settings?
Review Point: W7) Clarity Issues - please address each point individually in your response In the "Subset selection and model evaluation" paragraph under "Section 3: Problem Statement", it is unclear what edges are included in E . For example, is E directed? Is E fully-connected? Only after reading through the end of the paper did I understand E to be the edge set of a k -degree nearest neighbor, where k is a hyperparameter. This point should be made clearer. Why define the margin score as m ( x ) = 1 − ( P ( Y = b b x ) − P ( Y = s b x ) ) ? Why not define it more simply as m ( x ) = P ( Y = b b x ) − P ( Y = s b x ) ? What unary function did you use for f d i v e r s i t y in your experiments? Did you use the constant term? Equation (7) says M = ( G , I p ) , but then defines I c . Looks like a typo. The authors write that on ImageNet, "we found that 15% of all possible boundaries were covered." Help me interpret this statement. Is 15% surprising? Is this more or less than the authors expected? In equation (10), what is M ? It seems like M is used both as an integer (for the number of decision boundaries) and as a matroid. Please clarify. In the "Evaluation" subsection under "Section 6: Experiments," the authors write, "Note that, on this dataset, accuracies with 80% and 90% subsets exceed that with the full dataset." Which dataset is this referring to? In the Ablation study subsection, what are the standard deviations for the reported improvement percentages? Are these relative improvements, or absolute improvements? On which dataset(s) did the authors test the larger values of k ?
==================================================

Focused review:

1. The notations, equations in the method section are not clear. In Line 110 for instance, the equation $\Upsilon(x)=\{\Upsilon(x)_l\}$ is confusing. 2. The discriminator on the left side of Figure 1 is not the network used by the existing I2I methods (e.g., BicycleGAN concatenates the one-hot vector with the image as the input.) 3. Two highly-related frameworks targeting multi-domain I2I [1,2] are not cited, discussed, and compared in the paper. 4. In the table of Figure 3, it is not clear why training with partial adaptor performs worse than that of training without the adaptor? 5. Since the model is pre-trained from the BigGAN model trained on the natural images, what is the performance of the proposed method on the I2I tasks with unnatural images (e.g., face to artistic portrait)? [1] Lee, et al. "Drit++: Diverse image-to-image translation via disentangled representations.". [2] Choi et al. "StarGAN v2: Diverse image synthesis for multiple domains."

Review Point: 1. The notations, equations in the method section are not clear. In Line 110 for instance, the equation $\Upsilon(x)=\{\Upsilon(x)_l\}$ is confusing.
Review Point: 2. The discriminator on the left side of Figure 1 is not the network used by the existing I2I methods (e.g., BicycleGAN concatenates the one-hot vector with the image as the input.) 3. Two highly-related frameworks targeting multi-domain I2I [1,2] are not cited, discussed, and compared in the paper.
Review Point: 4. In the table of Figure 3, it is not clear why training with partial adaptor performs worse than that of training without the adaptor?
Review Point: 5. Since the model is pre-trained from the BigGAN model trained on the natural images, what is the performance of the proposed method on the I2I tasks with unnatural images (e.g., face to artistic portrait)? [1] Lee, et al. "Drit++: Diverse image-to-image translation via disentangled representations.". [2] Choi et al. "StarGAN v2: Diverse image synthesis for multiple domains."
==================================================

Focused review:

Weaknesses The proposed system uses HybrIK as a final mesh regressor (Section 4.1). However, with the same mesh regressor, the original HybrIK performs much better on 3DPW regardless of using 3DPW training set. See Table 2 of https://arxiv.org/pdf/2011.14672.pdf. The gap becomes very large when 3DPW is not used during the training stage. HybrIK without 3DPW achieves PA MPJPE 48.8, MPJPE 80.0, and MPVPE 94.5, while FeatER achieves PA MPJPE 54.5, MPJPE 88.4, and MPVPE 105.6. Given similarity between 3DPW training and testing sets, performance without using 3DPW training set reflects performance of methods in the wild better than using it. Human3.6M suffers from the same similarity as training and testing sets of Human3.6M are captured from the same studio and same camera viewpoints. Hence, 3D errors on 3DPW without using 3DPW as a training set is a major metric. On such a major metric, FeatER performs worse than HybrIK although FeatER uses the 3D mesh regression module of HybrIK. Although FeatER requires a smaller number of parameters and MACs, the gap seems not very large given the large 3D errors gap.


Review Point: The proposed system uses HybrIK as a final mesh regressor (Section 4.1). However, with the same mesh regressor, the original HybrIK performs much better on 3DPW regardless of using 3DPW training set. See Table 2 of https://arxiv.org/pdf/2011.14672.pdf. The gap becomes very large when 3DPW is not used during the training stage. HybrIK without 3DPW achieves PA MPJPE 48.8, MPJPE 80.0, and MPVPE 94.5, while FeatER achieves PA MPJPE 54.5, MPJPE 88.4, and MPVPE 105.6. Given similarity between 3DPW training and testing sets, performance without using 3DPW training set reflects performance of methods in the wild better than using it.
Review Point: Human3.6M suffers from the same similarity as training and testing sets of Human3.6M are captured from the same studio and same camera viewpoints. Hence, 3D errors on 3DPW without using 3DPW as a training set is a major metric. On such a major metric, FeatER performs worse than HybrIK although FeatER uses the 3D mesh regression module of HybrIK. Although FeatER requires a smaller number of parameters and MACs, the gap seems not very large given the large 3D errors gap.
==================================================

Focused review:

- It is unclear how the human annotations process is done (for example: how many annotators per sample and the acceptance criteria).
- The motivation of the three experimental settings is relatively weak. It can be better explained in the introduction. The settings are interesting if they are appropriately described and given a more substantial reason why they are important. For now, it looks like the authors are trying to explore new settings without supporting rationale.
- The authors do not explore the performance drop when using local entities in the experiments. 
Comments: 1. Please add more descriptions as the authors undergo the human annotation process. 
2. It would be better to add examples for each setting.
Questions: 1. What are the criteria for selecting the human annotation results for the post-edit stage? Did you apply additional quality assurance after adding the entities to the samples? And, how much changes after the post-edit stage? 
2. Why did the significant performance drop occur when using local entities in the experiments? 
3. Did you manage to measure the quality of the samples you collected using the proposed method? 

Review Point: - It is unclear how the human annotations process is done (for example: how many annotators per sample and the acceptance criteria).
Review Point: - The motivation of the three experimental settings is relatively weak. It can be better explained in the introduction. The settings are interesting if they are appropriately described and given a more substantial reason why they are important. For now, it looks like the authors are trying to explore new settings without supporting rationale.
Review Point: - The authors do not explore the performance drop when using local entities in the experiments. Comments:
Review Point: 1. Please add more descriptions as the authors undergo the human annotation process.
Review Point: 2. It would be better to add examples for each setting. Questions:
Review Point: 1. What are the criteria for selecting the human annotation results for the post-edit stage? Did you apply additional quality assurance after adding the entities to the samples? And, how much changes after the post-edit stage?
Review Point: 2. Why did the significant performance drop occur when using local entities in the experiments?
Review Point: 3. Did you manage to measure the quality of the samples you collected using the proposed method?
==================================================

Focused review:

With a double check on the paper after reading the rebuttal, I have the following concerns. 1. The major one is on correctness of the experimental comparison. I would like to share the results of the two most related blind-spot-based methods: N2S and Laine et al. [12], in their original implementations. The results tell a quite different story from what showed in the paper which used the versions with their own modifications. The original implementations of both N2S [1] and Laine [12] achieve much better results on BSD68, with more than 1.2dB PSNR, than that reported in the paper. Concretely, we use the model trained with the original authors' code and training scheme on BSD400, the same training dataset used in this paper. The test results on BSD68 are as follows: (1) Original N2S can achieve PSNR 28.12dB vs 26.98 reported in this paper with their own modifications. (2) Original Laine et al [12] can achieve PSNR 28.84dB vs. 27.15dB reported in the paper with their own modifications. In short, original implementations of N2S and [12] noticeably outperformed the proposed methods. There is a big gap between the results using original implementations and the one reported in the paper. In supplementary materials, It seems that the authors modified the original implementation (not sure, the description is not very clear). To be honest, I did not see good reasons why only include different results or the results from the modified implementations of the original papers, which are much lower than the results from their original implementations. *********************************************************************** 2. The claim in Section 3.1 is rather confusing. It says that "the denoising function f trained through mask-based blind-spot approaches is not strictly J-invariant, making Equation (2) not valid". Also, in the rebuttal, it says that "it is stated that the results in Table 1 indicate that the model f does not have the J-invariance, thus violating the assumption behind using the loss in Eqn. (3)." In N2S, during training, the denoising function f is J-invariant if we view f as the one equipped with masking. Thus, it does not violate Equation (2). Further, since Equation (2) is the loss for training which is not used in test, relating the J-invariance of a trained model in test to the conditions of Equation (2)(3) for training does not make sense.

Review Point: With a double check on the paper after reading the rebuttal, I have the following concerns.
Review Point: 1. The major one is on correctness of the experimental comparison. I would like to share the results of the two most related blind-spot-based methods: N2S and Laine et al. [12], in their original implementations. The results tell a quite different story from what showed in the paper which used the versions with their own modifications. The original implementations of both N2S [1] and Laine [12] achieve much better results on BSD68, with more than 1.2dB PSNR, than that reported in the paper. Concretely, we use the model trained with the original authors' code and training scheme on BSD400, the same training dataset used in this paper. The test results on BSD68 are as follows: (1) Original N2S can achieve PSNR 28.12dB vs 26.98 reported in this paper with their own modifications. (2) Original Laine et al [12] can achieve PSNR 28.84dB vs. 27.15dB reported in the paper with their own modifications. In short, original implementations of N2S and [12] noticeably outperformed the proposed methods. There is a big gap between the results using original implementations and the one reported in the paper. In supplementary materials, It seems that the authors modified the original implementation (not sure, the description is not very clear). To be honest, I did not see good reasons why only include different results or the results from the modified implementations of the original papers, which are much lower than the results from their original implementations. *********************************************************************** 2. The claim in Section 3.1 is rather confusing. It says that "the denoising function f trained through mask-based blind-spot approaches is not strictly J-invariant, making Equation (2) not valid". Also, in the rebuttal, it says that "it is stated that the results in Table 1 indicate that the model f does not have the J-invariance, thus violating the assumption behind using the loss in Eqn. (3)." In N2S, during training, the denoising function f is J-invariant if we view f as the one equipped with masking. Thus, it does not violate Equation (2). Further, since Equation (2) is the loss for training which is not used in test, relating the J-invariance of a trained model in test to the conditions of Equation (2)(3) for training does not make sense.
==================================================

Focused review:

Weakness
The paper fails to explain how different pretrained models influence off-line RL.
I don't see the point of including Wikipedia in the name. I assume that it's not a key element of their model and conclusions. They may also train their model on other corpora, e.g. c4. And the paper doesn't conduct an ablation study of using a different language-based corpus.
The interpretability of using a language model to help offline RL is weakly explained in the paper. It seems that the authors believe that the offline RL only benefits from a similar sequential structure. If so, I think of an experiment setting that may verify it. If a pretrained vision model is trained using continuous images with low resolutions, which can be constructed by dealing with videos crawled from youtube or other online channels, how will the model perform to help the off-line reinforcement learning?
The author should provide experiments based on language models like XL-Net and RoBerta with a similar parameter size. It may help to illustrate the interpretability of using language models to help offline RL as well.
This submission has relatively limited technical contributions, as most of the algorithmic components were proposed in previous papers.
Questions
The experiment results in Table. 3 are impressive but counter-intuitive. I wonder whether there is a probability that the GPU utilization is not fairly equal when running these experiments with DT, ChibiT, and GPT2. Because the experiments are done with a single V100 and models with very different parameter sizes. I hope that the authors can double-check it and provide the analysis based on steps with equal batches. The training time comparison experiment is not convincing enough for me.
The interpretability of why the pure language-based pretraining model performs better than the pretrained vision-language multimodal is very interesting but relatively shallow. Are there any possibilities that an experiment can be conducted to verify that the “natural” sequential nature of language and trajectories helps?


Review Point: The paper fails to explain how different pretrained models influence off-line RL. I don't see the point of including Wikipedia in the name. I assume that it's not a key element of their model and conclusions. They may also train their model on other corpora, e.g.
Review Point: c4. And the paper doesn't conduct an ablation study of using a different language-based corpus. The interpretability of using a language model to help offline RL is weakly explained in the paper. It seems that the authors believe that the offline RL only benefits from a similar sequential structure. If so, I think of an experiment setting that may verify it. If a pretrained vision model is trained using continuous images with low resolutions, which can be constructed by dealing with videos crawled from youtube or other online channels, how will the model perform to help the off-line reinforcement learning? The author should provide experiments based on language models like XL-Net and RoBerta with a similar parameter size. It may help to illustrate the interpretability of using language models to help offline RL as well. This submission has relatively limited technical contributions, as most of the algorithmic components were proposed in previous papers. Questions The experiment results in Table. 3 are impressive but counter-intuitive. I wonder whether there is a probability that the GPU utilization is not fairly equal when running these experiments with DT, ChibiT, and GPT2. Because the experiments are done with a single V100 and models with very different parameter sizes. I hope that the authors can double-check it and provide the analysis based on steps with equal batches. The training time comparison experiment is not convincing enough for me. The interpretability of why the pure language-based pretraining model performs better than the pretrained vision-language multimodal is very interesting but relatively shallow. Are there any possibilities that an experiment can be conducted to verify that the “natural” sequential nature of language and trajectories helps?
==================================================

Focused review:

Weakness:
Hyperspectral image reconstruction has attracted more and more attention in the past several years. Many papers do not study the mechanism behind the imaging process, the property of hyperspectral images and how to make the reconstructed images really useful, instead focus on adopting some deep learning techniques to learn the mapping from single-shot measurement to multiple channel image. This paper is one of them, and is not exciting with so many papers doing the same thing. I'd rather want to see a paper that works on reconstructing images with a wider spectral range and a much higher number of bands, more complex scenes consisting of multiple objects, and addressing a real-world problem such as distinguishing the fine differences of similar materials. These are the reason that makes hyperspectral image useful and the reconstruction of hyperspectral image meaningful.
The novel contribution on network mechanism and image representation learning is low. The design of the structure seems to be empirical. Although the proposed model is simple, its rationale has not been clearly explained, especially why and how it can produce high reconstruction quality.
The presentation of the paper shall be improved. There are some typos and errors that shall be fixed, e.g. "nad", "Fig. 6 (a)" (there is no Fig. 6 (a)), and "Tab ??".
More comments:
When introducing U-net, the properties of medical images are described, which is not closely relevant to this paper.
Discussion on the receptive field is not convincing. There are differences between medical image and hyperspectral image, but such difference is simply on the data and can be easily solved by introducing different training samples. It is not related to the network model. The authors commented that "medical image processing task is always more locally focused while the HSI reconstruction tends to be globally comprehensive". However, isn't HSI reconstruction also focus on local details? This has been highlighted in the experimental results, e.g. Figure 5 that shows more details of the scene?
In evaluating the quality of reconstructed spectral responses, has the authors considered the spectral angle distance between the estimated signal and the ground truth? This is more commonly used in remote sensing for quantitative evaluation.
Since the depth of network is highly important, an analysis on the impact of K shall be given.
"Within the chosen region (green patch), our reconstructed pixels yield the highest a correlation with the reference given by spectrometer, indicating the effectiveness of our method within waveband of green, i.e., 500nm-565nm". Why green patch? How is the overall quality of the reconstructed spectral responses?
Section 3.1 is highly similar to the description on CASSI in some published papers. Even if the papers may be written by the same authors, such similarity shall be reduced or avoided.


Review Point: Hyperspectral image reconstruction has attracted more and more attention in the past several years. Many papers do not study the mechanism behind the imaging process, the property of hyperspectral images and how to make the reconstructed images really useful, instead focus on adopting some deep learning techniques to learn the mapping from single-shot measurement to multiple channel image. This paper is one of them, and is not exciting with so many papers doing the same thing. I'd rather want to see a paper that works on reconstructing images with a wider spectral range and a much higher number of bands, more complex scenes consisting of multiple objects, and addressing a real-world problem such as distinguishing the fine differences of similar materials. These are the reason that makes hyperspectral image useful and the reconstruction of hyperspectral image meaningful. The novel contribution on network mechanism and image representation learning is low. The design of the structure seems to be empirical. Although the proposed model is simple, its rationale has not been clearly explained, especially why and how it can produce high reconstruction quality. The presentation of the paper shall be improved. There are some typos and errors that shall be fixed, e.g. "nad", "Fig.
Review Point: 6 (a)" (there is no Fig. 6 (a)), and "Tab ??". More comments: When introducing U-net, the properties of medical images are described, which is not closely relevant to this paper. Discussion on the receptive field is not convincing. There are differences between medical image and hyperspectral image, but such difference is simply on the data and can be easily solved by introducing different training samples. It is not related to the network model. The authors commented that "medical image processing task is always more locally focused while the HSI reconstruction tends to be globally comprehensive". However, isn't HSI reconstruction also focus on local details? This has been highlighted in the experimental results, e.g. Figure 5 that shows more details of the scene? In evaluating the quality of reconstructed spectral responses, has the authors considered the spectral angle distance between the estimated signal and the ground truth? This is more commonly used in remote sensing for quantitative evaluation. Since the depth of network is highly important, an analysis on the impact of K shall be given. "Within the chosen region (green patch), our reconstructed pixels yield the highest a correlation with the reference given by spectrometer, indicating the effectiveness of our method within waveband of green, i.e., 500nm-565nm". Why green patch? How is the overall quality of the reconstructed spectral responses? Section 3.1 is highly similar to the description on CASSI in some published papers. Even if the papers may be written by the same authors, such similarity shall be reduced or avoided.
==================================================

Focused review:

Major Comments: (1) Organization of theoretical results. * It is not clear to me the significance of Lemma 1 and Theorem 1, specifically, why they are surprising or important for CE (e.g., lemma 1 seem to be a standard application of LLN), and what insight can be derived from them (e.g., why should I care about the asymptotic rate of the stochastically bound O(1/nm)? Does it related to any operating characteristics of the model?). Comparing to these results, Proposition 1 seem to be more central to this paper, and may merit more detailed explanation. If authors agree, it may be beneficial to shift the emphasis in the presentation of the theoretical results, for example, move Lemma 1 to the Appendix. Unless Theorem 1 has crucial implications that is central to the story of the paper, I suggest author move it to the appendix also, or alternatively give a bit more explanation to justify its position in the paper. * As mentioned in the last paragraph, Proposition 1 seem to be more important and deserves its own background section. In particular, author should explain what $\alpha$ in Equation (2) represents, and explain how to compute it in theory and in practice, thereby providing context for Algorithm 1. It will also reader to understand the importance of Proposition 1 by explaining the connection between Var(K) and model's generalization ability, like what you illustrated empirically in Figure 4. (2) (Optionally,) adding baseline comparison to experiments 4.2, and more discussion. While I find the experiments interesting, it might be beneficial to add at least one standard baseline method (e.g., standard neural architecture search) to compare against primal formulation. So author can compare the difference in terms of resulting architecture, estimated kernel variance, testing error and total wall clock time used to better illustrate the benefit of the existing method. Minor: * line 68, "respectively.." (should be only one period) * line 148 cifar10/100 -> CIFAR-10/-100 * There's some relevant recent work on NTK for ensemble models, e.g., [1] and references therein. It appears rather relevant and should be included in Section 5 as well. [1] Bobby He, Balaji Lakshminarayanan, Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. (https://arxiv.org/abs/2007.05864)

Review Point: * It is not clear to me the significance of Lemma 1 and Theorem 1, specifically, why they are surprising or important for CE (e.g., lemma 1 seem to be a standard application of LLN), and what insight can be derived from them (e.g., why should I care about the asymptotic rate of the stochastically bound O(1/nm)? Does it related to any operating characteristics of the model?). Comparing to these results, Proposition 1 seem to be more central to this paper, and may merit more detailed explanation. If authors agree, it may be beneficial to shift the emphasis in the presentation of the theoretical results, for example, move Lemma 1 to the Appendix. Unless Theorem 1 has crucial implications that is central to the story of the paper, I suggest author move it to the appendix also, or alternatively give a bit more explanation to justify its position in the paper.
Review Point: * As mentioned in the last paragraph, Proposition 1 seem to be more important and deserves its own background section. In particular, author should explain what $\alpha$ in Equation (2) represents, and explain how to compute it in theory and in practice, thereby providing context for Algorithm 1. It will also reader to understand the importance of Proposition 1 by explaining the connection between Var(K) and model's generalization ability, like what you illustrated empirically in Figure 4. (2) (Optionally,) adding baseline comparison to experiments 4.2, and more discussion. While I find the experiments interesting, it might be beneficial to add at least one standard baseline method (e.g., standard neural architecture search) to compare against primal formulation. So author can compare the difference in terms of resulting architecture, estimated kernel variance, testing error and total wall clock time used to better illustrate the benefit of the existing method. Minor:
Review Point: * line 68, "respectively.." (should be only one period) * line 148 cifar10/100 -> CIFAR-10/-100 * There's some relevant recent work on NTK for ensemble models, e.g., [1] and references therein. It appears rather relevant and should be included in Section 5 as well. [1] Bobby He, Balaji Lakshminarayanan, Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. (https://arxiv.org/abs/2007.05864)
==================================================

Focused review:

weakness I see is that I'm not sure there is a lot of information gain from this paper. The results are more or less what I think most readers will expect. The findings seem in line with past work on generalization and with conventional wisdom -- ID works, OOD works worse, inductive biases can help on OOD, etc. That's not to say the present work is not valuable -- there are open questions here and the present paper is one of the most extensive studies I have seen on them. Just that the paper doesn't provide entirely unexpected answers. Because of this, I think the most valuable contribution of the paper may be the benchmark it provides, on top of which future studies may find something really new.
To elaborate further, the related work covers numerous papers that have come to roughly similar conclusions. Two more papers come to mind that also have similar conclusion but were not discussed:
Packer et al., “Assessing Generalization in Deep Reinforcement Learning”, 2019 -- This paper also studied ID vs OOD generalization but in the context of reinforcement learning. Figure 1 from Packer et al. shows their train/test setup, which is quite related to the current paper's settings in Figure 2. The conclusions are similar to the current paper: 1) extrapolation is harder than interpolation, 2) SOTA algorithms that are supposed to "solve" this problem fail.
Jahanian et al., "On the 'Steerability' of Generative Adversarial Networks", 2020 -- This paper studied the ability of GANs to extrapolate in their latent space. The finding is that they struggle to generate transformations that extend beyond the distribution seen during training. These results are similar to the findings in the current paper on the failure of VAE latent representations to extrapolate beyond the training data.
This is all to say I think there is ample prior literature that make the present conclusions unsurprising. But thorough work on this topic, and new benchmarks, is still valuable and that's what the current paper provides. I should also note that the finding about modularity is something I hadn't seen before, and I think that's a valuable contribution as well.
Aside from this, I think the paper is very solid. A few minor comments follow:
Using CelebGlow feels a bit awkward since it is a generative model fit to data, and then you are again fitting samples from this model with another generative model. I wonder if there could be some bias where the samples are easier to model with a VAE since they were generated with a related model (Glow)... It's probably all fine but some commentary on this could be useful.
Repeated reference to Hendrycks and Diettrich 2019.
I would say inductive biases 1 and 3 overlap: inductive bias 1, as it is implemented in the paper, could be considered a special case of transfer learning where the pretraining is done with VAEs. This could be clarified to avoid implying that these are independent inductive biases.
“if factors are located in a particular edge of the FoV hyper cube given by all FoVs” — “edge” —> “corner”?
“Here, we further see that, on average, the performances seem to increase as we increase the supervision signal.” — This is a bit vague I don’t see it fully reflected in the figure. This point could be made more precise. What does “increase the supervision signal” refer to?
“We find that the degree of downstream performance correlates weakly but positively with the degree of disentanglement (Pearson ρ = 0.63, Spearman ρ = 0.67)” — I’m not sure I would call these weak correlations. In many fields I believe this would be considered a strong correlation.
“Existing notions of disentanglement models with a readout MLP do not help to facilitate the learning of the underlying mechanisms in the tested datasets.” — I don’t understand this conclusion. The correlations are substantial. My understanding of the results is that greater disentanglement does correlate with better ability to identify the underlying mechanisms. Appendix Fig 7 seems to support this as well, with all but one of the correlations being positive.


Review Point: I see is that I'm not sure there is a lot of information gain from this paper. The results are more or less what I think most readers will expect. The findings seem in line with past work on generalization and with conventional wisdom -- ID works, OOD works worse, inductive biases can help on OOD, etc. That's not to say the present work is not valuable -- there are open questions here and the present paper is one of the most extensive studies I have seen on them. Just that the paper doesn't provide entirely unexpected answers. Because of this, I think the most valuable contribution of the paper may be the benchmark it provides, on top of which future studies may find something really new. To elaborate further, the related work covers numerous papers that have come to roughly similar conclusions. Two more papers come to mind that also have similar conclusion but were not discussed: Packer et al., “Assessing Generalization in Deep Reinforcement Learning”, 2019 -- This paper also studied ID vs OOD generalization but in the context of reinforcement learning. Figure 1 from Packer et al. shows their train/test setup, which is quite related to the current paper's settings in Figure 2. The conclusions are similar to the current paper:
Review Point: 1) extrapolation is harder than interpolation, 2) SOTA algorithms that are supposed to "solve" this problem fail. Jahanian et al., "On the 'Steerability' of Generative Adversarial Networks", 2020 -- This paper studied the ability of GANs to extrapolate in their latent space. The finding is that they struggle to generate transformations that extend beyond the distribution seen during training. These results are similar to the findings in the current paper on the failure of VAE latent representations to extrapolate beyond the training data. This is all to say I think there is ample prior literature that make the present conclusions unsurprising. But thorough work on this topic, and new benchmarks, is still valuable and that's what the current paper provides. I should also note that the finding about modularity is something I hadn't seen before, and I think that's a valuable contribution as well. Aside from this, I think the paper is very solid. A few minor comments follow: Using CelebGlow feels a bit awkward since it is a generative model fit to data, and then you are again fitting samples from this model with another generative model. I wonder if there could be some bias where the samples are easier to model with a VAE since they were generated with a related model (Glow)... It's probably all fine but some commentary on this could be useful. Repeated reference to Hendrycks and Diettrich 2019. I would say inductive biases 1 and 3 overlap: inductive bias 1, as it is implemented in the paper, could be considered a special case of transfer learning where the pretraining is done with VAEs. This could be clarified to avoid implying that these are independent inductive biases. “if factors are located in a particular edge of the FoV hyper cube given by all FoVs” — “edge” —> “corner”? “Here, we further see that, on average, the performances seem to increase as we increase the supervision signal.” — This is a bit vague I don’t see it fully reflected in the figure. This point could be made more precise. What does “increase the supervision signal” refer to? “We find that the degree of downstream performance correlates weakly but positively with the degree of disentanglement (Pearson ρ = 0.63, Spearman ρ = 0.67)” — I’m not sure I would call these weak correlations. In many fields I believe this would be considered a strong correlation. “Existing notions of disentanglement models with a readout MLP do not help to facilitate the learning of the underlying mechanisms in the tested datasets.” — I don’t understand this conclusion. The correlations are substantial. My understanding of the results is that greater disentanglement does correlate with better ability to identify the underlying mechanisms. Appendix Fig 7 seems to support this as well, with all but one of the correlations being positive.
==================================================

Focused review:

Weaknesses.
The paper represents the challenges in a vague way. In Section 1 Introduction, the paper illustrates the necessity of customizing Traffic Engineering schemes for various applications. However, it lacks concrete analysis of challenges that lie in translating a TE policy into a distributed protocol. Only a general description in Paragraph 2, such as "it requires the specification of exchange data, the processing of the data and the algorithm ...", is not solid and analytical enough to demonstrate how these challenge the design of a better TE scheme. The paper should show how difficult it is that to process per-application-level data and to design an algorithm. Furthermore, they do not explain how incorporating machine learning techniques mitigate those challenges.
The paper presents unclear descriptions of technical details. 1) In the second paragraph of Section 2, they claim that the proposed approach will learn two aspects, i.e., process and exchange the local state of switches and map the exchanged state into forwarding decisions. However, later in the fourth paragraph, they mention that the model should learn more than these two aspects. The model should also learn how to react to changes in the network, such as node and link failures and changes in monitored measures. They do not explain how the latter aspect is related to the former two aspects. 2) In Section 3, paragraph 5, does "to select the HNSAs necessary for this" indicates that there might be more HNSAs? Also, it is unclear what "this" refers to.
The evaluation part is not complete enough. First, the paper does not show the comparison of computing overhead compared to baselines. As a result, we do not know if the proposed one can be deployed in actual data center deployment. How long will it take for training? What's the computing overhead for inference? Second, they do not use traffic features from various applications, and thus we do not know if the proposed approach can adapt to various traffic features. It is crucial in Traffic Engineering, as stated in Section 1 Introduction.
The paper does not discuss the possible technical defects of their approach. For example, with wrong forwarding decisions accumulating, network congestion may happen. How does the approach handle problems that arise from intrinsic errors of the Machine Learning algorithm?


Review Point: The paper represents the challenges in a vague way. In Section 1 Introduction, the paper illustrates the necessity of customizing Traffic Engineering schemes for various applications. However, it lacks concrete analysis of challenges that lie in translating a TE policy into a distributed protocol. Only a general description in Paragraph 2, such as "it requires the specification of exchange data, the processing of the data and the algorithm ...", is not solid and analytical enough to demonstrate how these challenge the design of a better TE scheme. The paper should show how difficult it is that to process per-application-level data and to design an algorithm. Furthermore, they do not explain how incorporating machine learning techniques mitigate those challenges. The paper presents unclear descriptions of technical details.
Review Point: 1) In the second paragraph of Section 2, they claim that the proposed approach will learn two aspects, i.e., process and exchange the local state of switches and map the exchanged state into forwarding decisions. However, later in the fourth paragraph, they mention that the model should learn more than these two aspects. The model should also learn how to react to changes in the network, such as node and link failures and changes in monitored measures. They do not explain how the latter aspect is related to the former two aspects.
Review Point: 2) In Section 3, paragraph 5, does "to select the HNSAs necessary for this" indicates that there might be more HNSAs? Also, it is unclear what "this" refers to. The evaluation part is not complete enough. First, the paper does not show the comparison of computing overhead compared to baselines. As a result, we do not know if the proposed one can be deployed in actual data center deployment. How long will it take for training? What's the computing overhead for inference? Second, they do not use traffic features from various applications, and thus we do not know if the proposed approach can adapt to various traffic features. It is crucial in Traffic Engineering, as stated in Section 1 Introduction. The paper does not discuss the possible technical defects of their approach. For example, with wrong forwarding decisions accumulating, network congestion may happen. How does the approach handle problems that arise from intrinsic errors of the Machine Learning algorithm?
==================================================

Focused review:

1. I understand this paper is a learning theory paper. However, I still think having some experiments (even they are small empirical studies) should make this paper stronger and empirically convincing. 2. Empirically, modifying neural network architecture for a certain task is a very common practice, which also involves changing internal structure of the architecture beyond input and output maps. However, in this paper, it assumes the feature mapping is regular (Assumption 3.1) and doesn't study much of it. So I am worried that the paper is too restrict to input and output mapping. 3. Following 2, when designing a neural network architecture for a specific task, people often inject a particular "inductive bias" to it. So I am wondering, does the universal approximation claim on feature map suffice?

Review Point: 1. I understand this paper is a learning theory paper. However, I still think having some experiments (even they are small empirical studies) should make this paper stronger and empirically convincing.
Review Point: 2. Empirically, modifying neural network architecture for a certain task is a very common practice, which also involves changing internal structure of the architecture beyond input and output maps. However, in this paper, it assumes the feature mapping is regular (Assumption 3.1) and doesn't study much of it. So I am worried that the paper is too restrict to input and output mapping.
Review Point: 3. Following 2, when designing a neural network architecture for a specific task, people often inject a particular "inductive bias" to it. So I am wondering, does the universal approximation claim on feature map suffice?
==================================================

Focused review:

- The extensive set of assumptions limit the applicability (same terminal states for all tasks, same state space, binary rewards in terminal states). - In order to derive the policy the burden is on the engineer to describe the new task using boolean operations between previously learned ones (not a learning approach). - The abstract starts with a reference to lifelong learning, but there’s no relevant benchmark used. The paper does not address the problems specific to lifelong learning. - No discussion on how to benefit when a new task cannot be described as the result of boolean operations applied to the old ones. - The paper does not address a machine learning problem per se.

Review Point: - The extensive set of assumptions limit the applicability (same terminal states for all tasks, same state space, binary rewards in terminal states).
Review Point: - In order to derive the policy the burden is on the engineer to describe the new task using boolean operations between previously learned ones (not a learning approach).
Review Point: - The abstract starts with a reference to lifelong learning, but there’s no relevant benchmark used. The paper does not address the problems specific to lifelong learning.
Review Point: - No discussion on how to benefit when a new task cannot be described as the result of boolean operations applied to the old ones.
Review Point: - The paper does not address a machine learning problem per se.
==================================================

Focused review:

[Edit after Author Response] I thank the authors for acknowledging the suggestion for merging the tables, captioning and moving Algorithm 1 to the Appendix. 1) Discussing the findings of Table 7 and 8 and providing a reasoning for them is fairly important since this detail forms the crux of your simple idea. The author response does not elaborate or explain too much on this, but rather states the observations from Table 8. 2) While I thank the authors for performing more experiments on state of the art meta-learning approaches like MCT and mentioning that AQ on MCT reduces the drop of natural accuracy, the current results in the paper using other meta-learning approaches do have a large drop in the natural accuracy. This certainly diminishes the practical use of AQ. ------- I agree that the paper does have some good positive points. However I am slightly inclined towards a rejection currently primarily due to the following reasons: 1) The core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution for the NeurIPS community. 2) While it is common knowledge that adversarial training leads to a drop in the natural accuracy while improving the adversarial robustness accuracy, the drop is usually small and not very significant. In contrast, from results in Table 4,5 vs Table 2, it appears that using AQ causes a big drop (sometimes almost 15-20%) in the natural accuracy. While the adversarial robustness increases, such a big drop in natural accuracy is not supportive of their AQ technique being very useful practically. 3) Apart from providing some empirical results in Table 7, 8, the authors provide very little reasoning/exploration of why only perturbing the query data and not the support data or both the (query+support data) is used in their proposed technique. This seems to be the crux of their simple idea, but is not well justified. I would have liked some better intuitive explanation and possibly some theoretical justification as to why only fine-tuning in the outer-loop with adversarially perturbed query data is sufficient to make the model robust to previously unseen few-shot (support, query) pairs. 4) Instead of having so many small individual tables, I would rather have a single big table comparing different methods/experiments (especially since almost all the tables are over the same 2 datasets). There are 2 main reasons for this: a) First, having separate tables makes it difficult to compare the improvements of a technique with respect to other techniques whose results have been put in a separate table. b) There are several repeating results across different tables which seems to be unnecessarily used for space filling.

Review Point: [Edit after Author Response] I thank the authors for acknowledging the suggestion for merging the tables, captioning and moving Algorithm 1 to the Appendix.
Review Point: 1) Discussing the findings of Table 7 and 8 and providing a reasoning for them is fairly important since this detail forms the crux of your simple idea. The author response does not elaborate or explain too much on this, but rather states the observations from Table 8.
Review Point: 2) While I thank the authors for performing more experiments on state of the art meta-learning approaches like MCT and mentioning that AQ on MCT reduces the drop of natural accuracy, the current results in the paper using other meta-learning approaches do have a large drop in the natural accuracy. This certainly diminishes the practical use of AQ. ------- I agree that the paper does have some good positive points. However I am slightly inclined towards a rejection currently primarily due to the following reasons:
Review Point: 1) The core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution for the NeurIPS community.
Review Point: 2) While it is common knowledge that adversarial training leads to a drop in the natural accuracy while improving the adversarial robustness accuracy, the drop is usually small and not very significant. In contrast, from results in Table 4,5 vs Table 2, it appears that using AQ causes a big drop (sometimes almost 15-20%) in the natural accuracy. While the adversarial robustness increases, such a big drop in natural accuracy is not supportive of their AQ technique being very useful practically.
Review Point: 3) Apart from providing some empirical results in Table 7, 8, the authors provide very little reasoning/exploration of why only perturbing the query data and not the support data or both the (query+support data) is used in their proposed technique. This seems to be the crux of their simple idea, but is not well justified. I would have liked some better intuitive explanation and possibly some theoretical justification as to why only fine-tuning in the outer-loop with adversarially perturbed query data is sufficient to make the model robust to previously unseen few-shot (support, query) pairs.
Review Point: 4) Instead of having so many small individual tables, I would rather have a single big table comparing different methods/experiments (especially since almost all the tables are over the same 2 datasets). There are 2 main reasons for this: a) First, having separate tables makes it difficult to compare the improvements of a technique with respect to other techniques whose results have been put in a separate table. b) There are several repeating results across different tables which seems to be unnecessarily used for space filling.
==================================================

Focused review:

Weaknesses:
Missing important baselines in experiments: current experiments only compare to OCT and ignore a large amount of approaches for optimal decision trees developed over the last year. In particular, despite the focus on MIP and continuous features, the work should compare against prominent non-MIP-based optimal approaches and approaches that first convert continuous values to discrete values using a transformation that maintains optimality (e.g., assigning a single variable to each value). Some examples include [1-5] listed below.
Experiments are limited to trees of depth 2. This is a very shallow depth and many previous works have considered deeper trees.
Ablation study: the paper would benefit from an analysis of the impact of different components (e.g., performance without the sample reduction technique).
Minor points:
line 3, 36: "tailed" I think should be "tailored"?
line 104: "feature size" -> "number of features"?
line 214: "reduce" -> "reduces"
[1] Verwer, Sicco, and Yingqian Zhang. "Learning optimal classification trees using a binary linear program formulation." Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019. [2] Aghaei, Sina, Andrés Gómez, and Phebe Vayanos. "Strong optimal classification trees." arXiv preprint arXiv:2103.15965 (2021). [3] Aglin, Gaël, Siegfried Nijssen, and Pierre Schaus. "Learning optimal decision trees using caching branch-and-bound search." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020. [4] Hu, Hao, et al. "Learning optimal decision trees with maxsat and its integration in adaboost." IJCAI-PRICAI 2020, 29th International Joint Conference on Artificial Intelligence and the 17th Pacific Rim International Conference on Artificial Intelligence. 2020. [5] Shati, Pouya, Eldan Cohen, and Sheila McIlraith. "SAT-based approach for learning optimal decision trees with non-binary features." 27th International Conference on Principles and Practice of Constraint Programming (CP 2021). Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2021.
I did not identify any issues related to limitations and potential negative societal impact.


Review Point: Missing important baselines in experiments: current experiments only compare to OCT and ignore a large amount of approaches for optimal decision trees developed over the last year. In particular, despite the focus on MIP and continuous features, the work should compare against prominent non-MIP-based optimal approaches and approaches that first convert continuous values to discrete values using a transformation that maintains optimality (e.g., assigning a single variable to each value). Some examples include [1-5] listed below. Experiments are limited to trees of depth 2. This is a very shallow depth and many previous works have considered deeper trees. Ablation study: the paper would benefit from an analysis of the impact of different components (e.g., performance without the sample reduction technique). Minor points: line 3, 36: "tailed" I think should be "tailored"? line 104: "feature size" -> "number of features"? line 214: "reduce" -> "reduces" [1] Verwer, Sicco, and Yingqian Zhang. "Learning optimal classification trees using a binary linear program formulation." Proceedings of the AAAI conference on artificial intelligence. Vol.
Review Point: 33. No.01. 2019. [2] Aghaei, Sina, Andrés Gómez, and Phebe Vayanos. "Strong optimal classification trees." arXiv preprint arXiv:2103.15965 (2021). [3] Aglin, Gaël, Siegfried Nijssen, and Pierre Schaus. "Learning optimal decision trees using caching branch-and-bound search." Proceedings of the AAAI Conference on Artificial Intelligence. Vol.
Review Point: 34. No.04. 2020. [4] Hu, Hao, et al. "Learning optimal decision trees with maxsat and its integration in adaboost." IJCAI-PRICAI 2020, 29th International Joint Conference on Artificial Intelligence and the 17th Pacific Rim International Conference on Artificial Intelligence. 2020. [5] Shati, Pouya, Eldan Cohen, and Sheila McIlraith. "SAT-based approach for learning optimal decision trees with non-binary features." 27th International Conference on Principles and Practice of Constraint Programming (CP 2021). Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2021. I did not identify any issues related to limitations and potential negative societal impact.
==================================================

Focused review:

Weaknesses
Clarity:
Figure 1 is not very useful for demonstrating how the method works. There’s no indication of some risk prediction threshold (
η
) nor anything to indicate that actions are actually modified based on this threshold (
a
t
,
a
t
′
aren’t differentiated).
The contrastive classifier
F
θ
’s intuition isn’t described very well, and Eq. 2 just appears without explaining the terms. I understand that this derivation of
F
θ
makes Eq 3 convenient (and maybe the authors figured out Eq 3 before figuring out Eq 2), but this could be made more clear.
Experiments:
Assumption 1 is presented without empirical evidence it exists. Can the authors experiment and demonstrate how true Assumption 1 is?
It’s hard to say that RPT+DA is better, as the authors claim, due to the overlapping standard deviations.
Method and Framing:
Intro: “benefitting from the generalizability of risk prediction, the proposed approach can avoid safety violations much earlier in the training process” → It’s not clear intuitively how this risk prediction differs from the aforementioned prior work at this point in the intro, and therefore how it is more generalizable than prior work.
Method: “With safety constraints and risk predictions, the proposed approach (to be elaborated below) is expected to incorporate the strengths of both categories of safe RL techniques.” → by this point, it’s still not clear why this is better. The authors should make more clear, in the intro and method introduction, that their contrastive method is able to reason about future risk, while many current methods either only implicitly do (through reward modifications and TD-learning) or do not at all (binary classification with model-based RL). This would make the paper's contributions clearer, and make it better in my opinion.


Review Point: Clarity: Figure 1 is not very useful for demonstrating how the method works. There’s no indication of some risk prediction threshold ( η ) nor anything to indicate that actions are actually modified based on this threshold ( a t , a t ′ aren’t differentiated). The contrastive classifier F θ ’s intuition isn’t described very well, and Eq.
Review Point: 2 just appears without explaining the terms. I understand that this derivation of F θ makes Eq 3 convenient (and maybe the authors figured out Eq 3 before figuring out Eq 2), but this could be made more clear. Experiments: Assumption 1 is presented without empirical evidence it exists. Can the authors experiment and demonstrate how true Assumption 1 is? It’s hard to say that RPT+DA is better, as the authors claim, due to the overlapping standard deviations. Method and Framing: Intro: “benefitting from the generalizability of risk prediction, the proposed approach can avoid safety violations much earlier in the training process” → It’s not clear intuitively how this risk prediction differs from the aforementioned prior work at this point in the intro, and therefore how it is more generalizable than prior work. Method: “With safety constraints and risk predictions, the proposed approach (to be elaborated below) is expected to incorporate the strengths of both categories of safe RL techniques.” → by this point, it’s still not clear why this is better. The authors should make more clear, in the intro and method introduction, that their contrastive method is able to reason about future risk, while many current methods either only implicitly do (through reward modifications and TD-learning) or do not at all (binary classification with model-based RL). This would make the paper's contributions clearer, and make it better in my opinion.
==================================================

Focused review:

Weaknesses:
W1. The paper needs substantial effort to improve its presentation.
W2. Lack of novelty and technical depth
W3. Missing baselines


Review Point: W1. The paper needs substantial effort to improve its presentation.
Review Point: W2. Lack of novelty and technical depth W3. Missing baselines
==================================================

Focused review:

weaknesses of this approach (e.g., it does not take into account language compositionally).  I appreciate that the authors used different methods to extract influential objects: Human attention (in line with previous works), text explanation (to rely on another modality), and question parsing (to remove the need of extra annotation). As a complementary analysis, I would have compared object sets (Jaccard Distance) which are extracted with visual cues and text description. Indeed, the VQA-X dataset contains both information for each question/answer pairs.   The method is more or less correctly explained. The training details seems complete and allow for reproducibility.  The authors do not provide code source although they mentioned it in the reproducibility checklist.  The empirical results are quite convincing and the necessary baselines and ablation studies are correctly provided. The formatting is simple and clear! It would have been perfect to provide the error bar as the number of experiments remains low (and over a small number of epochs) The cherry on the cake would be to run similar experiments on VQAv1 / VQA-CP1?  To increase the impact of the paper, I would recommend extending the setting to either dense image captioning, or question answering (if possible).  I feel that the discussion section raise some excellent points:  - I really like table 4, that clearly show that the method perform as expected (I would have add HINT for being exhaustive)  - the ablation study is convincing But, a lot of open-questions are still left open and could have been discussed.  For instance, I would have appreciated a more in-depth analysis of model errors.  What about the model complexity? Why only reweighting L_{crit}. How does evolve L_crit and L_infl at training time?  On a more general note, I think the overall writing and paper architecture can be greatly improved. For instance,   - the introduction and related work can be partially merged and summarized.   - 4.2 starts by providing high-level intuition while 4.1 does not.  - Training details incorporate some result discussion   Generic questions (sorted by impact):  - What is the impact of |I|, do you have the performance ration according to the number of top |I| influential objects  - Eq1 is a modified version of GardCAM, however, the modifications are not highlighted (neither explained). For instance, why did the authors remove the ReLU  - Even if the weight sensitivity in equation 5 is well motivated, it is not supported by previous works. Thus, did you perform an ablation study? It would be very have been nice in the discussion section.  - What is the actual computation cost of the two losses? What is the relative additional time required? +5%, +20%, +200%?  - As you used heuristics to retrieve influential objects, did you try to estimate the impact of false negatives in the loss.  - How did you pick 0.6 for glove embedding similarity? Did you perform k-cross-validation? What is the potential impact  - Have you tried other influential loss (Eq3)? For instance, replacing the min with a mean or NDCG?   Remarks:  - I would use a different notation for SV(.,.,.) as it is not symmetric. For instance SV_{a}(v_i || v_j) would avoid confusion (I am using KL notation here)  - Non-formal expression should be avoided: Ex: "l32 What's worse" - The references section is full of format inconsistencies. Besides, some papers are published with proceeding but are referred to arxiv papers.  - 3.1 introduces non-important notation, e.g., function h(.) or f(.) that are never used in the paper. - Several subsections could be gathered together, or define as a paragraph: 2.1/2.2/2.3 ; 5.1/5.2/5.3, etc. It would have save space for more experiments   Conclusion: The paper introduces two losses to better tie influential objects and potential answers.  The method is convincing, and the experimental results are diverse and good. However, I still think that the paper requires further polishing to improve the readability. I would also advocate providing more element to support the proposed method and to analyze the strengths and weaknesses. Although the current experiences are quite convincing, I would advocate adding more analysis to definitely conclude the efficiency of the method.  ---------------------------------- The rebuttal was clearly written and insightful. it answered most of my questions, and the authors demonstrate their ability to update the paper accordingly. Therefore, I am happy to increase my score, and accept the paper    

Review Point: of this approach (e.g., it does not take into account language compositionally). I appreciate that the authors used different methods to extract influential objects: Human attention (in line with previous works), text explanation (to rely on another modality), and question parsing (to remove the need of extra annotation). As a complementary analysis, I would have compared object sets (Jaccard Distance) which are extracted with visual cues and text description. Indeed, the VQA-X dataset contains both information for each question/answer pairs. The method is more or less correctly explained. The training details seems complete and allow for reproducibility. The authors do not provide code source although they mentioned it in the reproducibility checklist. The empirical results are quite convincing and the necessary baselines and ablation studies are correctly provided. The formatting is simple and clear! It would have been perfect to provide the error bar as the number of experiments remains low (and over a small number of epochs) The cherry on the cake would be to run similar experiments on VQAv1 / VQA-CP1? To increase the impact of the paper, I would recommend extending the setting to either dense image captioning, or question answering (if possible). I feel that the discussion section raise some excellent points:
Review Point: - I really like table 4, that clearly show that the method perform as expected (I would have add HINT for being exhaustive) - the ablation study is convincing But, a lot of open-questions are still left open and could have been discussed. For instance, I would have appreciated a more in-depth analysis of model errors. What about the model complexity? Why only reweighting L_{crit}. How does evolve L_crit and L_infl at training time? On a more general note, I think the overall writing and paper architecture can be greatly improved. For instance, - the introduction and related work can be partially merged and summarized.
Review Point: - 4.2 starts by providing high-level intuition while 4.1 does not.
Review Point: - Training details incorporate some result discussion Generic questions (sorted by impact):
Review Point: - What is the impact of |I|, do you have the performance ration according to the number of top |I| influential objects - Eq1 is a modified version of GardCAM, however, the modifications are not highlighted (neither explained). For instance, why did the authors remove the ReLU - Even if the weight sensitivity in equation 5 is well motivated, it is not supported by previous works. Thus, did you perform an ablation study? It would be very have been nice in the discussion section.
Review Point: - What is the actual computation cost of the two losses? What is the relative additional time required? +5%, +20%, +200%?
Review Point: - As you used heuristics to retrieve influential objects, did you try to estimate the impact of false negatives in the loss.
Review Point: - How did you pick 0.6 for glove embedding similarity? Did you perform k-cross-validation? What is the potential impact - Have you tried other influential loss (Eq3)? For instance, replacing the min with a mean or NDCG? Remarks:
Review Point: - I would use a different notation for SV(.,.,.) as it is not symmetric. For instance SV_{a}(v_i || v_j) would avoid confusion (I am using KL notation here) - Non-formal expression should be avoided: Ex: "l32 What's worse" - The references section is full of format inconsistencies. Besides, some papers are published with proceeding but are referred to arxiv papers.
Review Point: - 3.1 introduces non-important notation, e.g., function h(.) or f(.) that are never used in the paper.
Review Point: - Several subsections could be gathered together, or define as a paragraph: 2.1/2.2/2.3 ; 5.1/5.2/5.3, etc. It would have save space for more experiments Conclusion: The paper introduces two losses to better tie influential objects and potential answers. The method is convincing, and the experimental results are diverse and good. However, I still think that the paper requires further polishing to improve the readability. I would also advocate providing more element to support the proposed method and to analyze the strengths and weaknesses. Although the current experiences are quite convincing, I would advocate adding more analysis to definitely conclude the efficiency of the method. ---------------------------------- The rebuttal was clearly written and insightful. it answered most of my questions, and the authors demonstrate their ability to update the paper accordingly. Therefore, I am happy to increase my score, and accept the paper
==================================================

Focused review:

* Sparsity seems to affect performance at test time only, scaling to large training datasets is done by a heuristic subsampling step (which is presumably suboptimal for large datasets), addressing this (which is an arguably interesting problem) is suggested as future work. * The impact of this sub-sampling on performance was not indicated. * Sparsity may be leading to improvements even where no subsampling is needed, but that is unclear from the results presented in the paper (for example, this could be the case in MSCOCO and Yelp medium, but for MSCOCO alphas are all below 1, and for Yelp medium authors only report two seemly arbitrary values). * Qualitative examples in Tables 3 and 4 did not tell me much, perhaps the authors could walk readers through the patterns they seem to recognise.

Review Point: * Sparsity seems to affect performance at test time only, scaling to large training datasets is done by a heuristic subsampling step (which is presumably suboptimal for large datasets), addressing this (which is an arguably interesting problem) is suggested as future work.
Review Point: * The impact of this sub-sampling on performance was not indicated.
Review Point: * Sparsity may be leading to improvements even where no subsampling is needed, but that is unclear from the results presented in the paper (for example, this could be the case in MSCOCO and Yelp medium, but for MSCOCO alphas are all below 1, and for Yelp medium authors only report two seemly arbitrary values).
Review Point: * Qualitative examples in Tables 3 and 4 did not tell me much, perhaps the authors could walk readers through the patterns they seem to recognise.
==================================================

Focused review:

- I believe clarity may be an issue for this paper. For instance, in the section 104-110 the authors seem to introduce the notation g(I_a,I_a^*) without defining it. I believe I understand that g will, in fact, be one of g_regret and g_pct but as these are introduced later it made for difficult reading. It could be worth spelling out exactly what “refinement” is (drawing additional samples, but how many in one go, etc.). In lines 134-140, the approximates come thick and fast. It could be worth including some discussion of why these approximations are made (e.g. unless I’ve missed something we could actually different the max function unless we’re sitting right on a discontinuity point) and how much they might impact the final algorithm. It’s just a suggestion but the authors might consider relegating some details of the GMRF to the appendix and expending more ink on the core of their approach, especially around 133-140. - It should be explicitly stated at line 82 that these bounds have been considered in prior work. E.g. the lower bound appears in reference [11] as PCE and the upper bound has appeared in several existing works (see [10] for example). - It’s a pity that `bed-lb` appears to equal or outpeform `mu`. The paper would be even stronger if a practical case where this doesn’t happen could be demonstrated. That said, I believe the paper stands without this additional experiment.

Review Point: - I believe clarity may be an issue for this paper. For instance, in the section 104-110 the authors seem to introduce the notation g(I_a,I_a^*) without defining it. I believe I understand that g will, in fact, be one of g_regret and g_pct but as these are introduced later it made for difficult reading. It could be worth spelling out exactly what “refinement” is (drawing additional samples, but how many in one go, etc.). In lines 134-140, the approximates come thick and fast. It could be worth including some discussion of why these approximations are made (e.g. unless I’ve missed something we could actually different the max function unless we’re sitting right on a discontinuity point) and how much they might impact the final algorithm. It’s just a suggestion but the authors might consider relegating some details of the GMRF to the appendix and expending more ink on the core of their approach, especially around 133-140.
Review Point: - It should be explicitly stated at line 82 that these bounds have been considered in prior work. E.g. the lower bound appears in reference [11] as PCE and the upper bound has appeared in several existing works (see [10] for example).
Review Point: - It’s a pity that `bed-lb` appears to equal or outpeform `mu`. The paper would be even stronger if a practical case where this doesn’t happen could be demonstrated. That said, I believe the paper stands without this additional experiment.
==================================================

Focused review:

Weakness:
The value of the proved generalisation bounds are limited in that: 1. It is based on a "theoretical version" of the REF complexity (eq. 2) which differs from eq. 3 that is actually used in practice. 2. The bounds are limited to convex or linear regression problems. 3. Even in these constrained cases, the tightness of the bounds are unclear; empirical validation in even toy-ish problems would help.
The main empirical results are based on correlations obtained from only varying the batch size and learning rate. It would be more helpful if the correlation can be shown across architectures (even for different channel configurations), which arguably more critically affect generalisation. While this is unclear from the paper, inferring from fig. 1's numbers across ResNet variants, the REF complexity seemed to be less well at capturing the test accuracy for varying architectures.


Review Point: The value of the proved generalisation bounds are limited in that:
Review Point: 1. It is based on a "theoretical version" of the REF complexity (eq. 2) which differs from eq.
Review Point: 2. The bounds are limited to convex or linear regression problems.
Review Point: 3. Even in these constrained cases, the tightness of the bounds are unclear; empirical validation in even toy-ish problems would help. The main empirical results are based on correlations obtained from only varying the batch size and learning rate. It would be more helpful if the correlation can be shown across architectures (even for different channel configurations), which arguably more critically affect generalisation. While this is unclear from the paper, inferring from fig. 1's numbers across ResNet variants, the REF complexity seemed to be less well at capturing the test accuracy for varying architectures.
==================================================

Focused review:

- While I understand the space limitations, I think the paper could greatly benefit from more explanation of the meaning of the bounds (perhaps in the appendix). - Line 122: it's not obvious to me that the smoothed bound is more stable, since the \gamma factor in the numerator is also larger. Some calculations here, or a very simple experiment, would greatly help the reader understand when smoothing would be desirable. - The above also applies for the discussion on overestimation starting on line 181, especially in the trade-off of reducing overestimation error and converging to a suboptimal value function. - The above applies for the combined smoothness + regularization algorithm

Review Point: - While I understand the space limitations, I think the paper could greatly benefit from more explanation of the meaning of the bounds (perhaps in the appendix).
Review Point: - Line 122: it's not obvious to me that the smoothed bound is more stable, since the \gamma factor in the numerator is also larger. Some calculations here, or a very simple experiment, would greatly help the reader understand when smoothing would be desirable.
Review Point: - The above also applies for the discussion on overestimation starting on line 181, especially in the trade-off of reducing overestimation error and converging to a suboptimal value function.
Review Point: - The above applies for the combined smoothness + regularization algorithm
==================================================

Focused review:

1. The assumption that the density is upper bounded by 1/sigma times the uniform is a relatively strong assumption when the instances x are high- dimensional. Imagine that the space of instances X is [0, 2]^d then the uniform distribution has density 1/2^d and hence the probability density over X is assumed to have density in the range [0, (1/sigma) (1/2^d)], which is a strong restriction for the distributions over X. 2. I don't think that the distributional assumption resembles the smoothed analysis model of Spielman and Teng. In the present paper the assumption of the authors is that the distribution over X is very close to the completely uniform distribution. In other words we start with the uniform distribution over X and then an adversary can move a small amount of mass in a worst-case way. On the other hand in Spielman and Teng we start from a worst-case instance and then we add a small amount of noise around that particular worst-case instance. An assumption that feels closer to the Spielman and Teng model would be that we start with an arbitrary distribution D and then we take the convolution of D with a Gaussian distribution or a uniform distribution over a ball with small radius. Post-Rebuttal ============================ Thanks a lot for the clarifications! I think a discussion of these points in the paper would help a lot. I change my score to strong accept.

Review Point: 1. The assumption that the density is upper bounded by 1/sigma times the uniform is a relatively strong assumption when the instances x are high- dimensional. Imagine that the space of instances X is [0, 2]^d then the uniform distribution has density 1/2^d and hence the probability density over X is assumed to have density in the range [0, (1/sigma) (1/2^d)], which is a strong restriction for the distributions over X.
Review Point: 2. I don't think that the distributional assumption resembles the smoothed analysis model of Spielman and Teng. In the present paper the assumption of the authors is that the distribution over X is very close to the completely uniform distribution. In other words we start with the uniform distribution over X and then an adversary can move a small amount of mass in a worst-case way. On the other hand in Spielman and Teng we start from a worst-case instance and then we add a small amount of noise around that particular worst-case instance. An assumption that feels closer to the Spielman and Teng model would be that we start with an arbitrary distribution D and then we take the convolution of D with a Gaussian distribution or a uniform distribution over a ball with small radius. Post-Rebuttal ============================ Thanks a lot for the clarifications! I think a discussion of these points in the paper would help a lot. I change my score to strong accept.
==================================================

Focused review:

Weakness: - The proposed approach may introduce new parameters to the network. - The proposed Semantic Difference Convolution (SDC) is similar to Central Difference Convolution (CDC). - The effectiveness of the semantic difference term needs more clarification.


Review Point: - The proposed approach may introduce new parameters to the network.
Review Point: - The proposed Semantic Difference Convolution (SDC) is similar to Central Difference Convolution (CDC).
Review Point: - The effectiveness of the semantic difference term needs more clarification.
==================================================

Focused review:

weakness of this work, in my opinion, is the lack of comprehensive experiments to show that triplet-BERT outperforms state-of-the-art methods in protein prediction tasks. The methods mentioned above were not used as comparison partners
In addition to a lack of comparison partners, the tasks used to assess the performance of triplet-BERT are not standard benchmark tasks like the broadly used TAPE tasks. I recommend the authors to use published datasets that have been designed as benchmark datasets in protein prediction. Of importance is that the data splits must have been created by the respective publications, to guarantee fair comparison between methods, e.g., data associated with the DeepLoc method to predict subcellular localization [RRef9].
The authors show the loss function in Equation 3 but it is unclear if/how it can be differentiated to propagate the gradients. No analysis with respect to this topic is provided in the manuscript.
The authors point to Figure 3, that shows cluster maps, as evidence that the fine-tuned encodings give more distinct clusters. This is not immediately obvious from the figure. If actual clusters are obtained, I would suggest using another standard metric to compare clustering results, such as silhouette coefficients or normalized mutual information (NMI).
Figure 4 is meant to highlight the performance of the attention mechanism in two proteins (at different layers). It is not clear how these results are later used, if at all. I would recommend adding more context around this figure, especially if it helps drive an important point, which at the present does not seem to be the case. Otherwise, it should be removed.
5. Problems with presentation of the text
Portions of the text were extracted verbatim from other sources without the proper acknowledgement. This is a serious problem and can be construed as a case of plagiarism. One of the unnamed sources is another submission to ICLR 2021. Some examples are listed below:
The next to last paragraph in page 3 is a match, with minor rephrasing, to the last part of the first paragraph in Section 3 (page 4) of another manuscript under review at ICLR 2021 (indicated as [RRef2] in Section 7 of this review. Example text:
In this submission In extension to this work, Vig et al. (2020) explored how this BERT model (Rao et al., 2019) was capable of discerning structural and functional properties about the protein. Vig et al. (2020) proved that the model was able to model long-range dependencies within the sequence of amino acids, it was also able to deduce information about the protein based on the folding structure, target binding sites,and additional complex biophysical properties. They concluded that the specific heads within the model attended to individual amino acids, as the attention similarity matrix was positively correlated to the expected substitution scores (i.e. BLOSUM62) for each amino acid. Vig et al. (2020) noted that the deeper layers of the BERT model focused relatively more attention on binding sites and contacts (i.e. high-level concepts). In contrast, information about the secondary structure (i.e. low-to mid-level concepts) within the protein was targeted evenly across each of the layers
In [RRef2] Vig et al. (2020) explored how this particular model was capable of discerning structural and functional properties about proteins. As the BERT transformer was able to model long-range dependencies within the protein, it was able to deduce information about the protein based on the folding structure, target binding sites, and additional complex biophysical properties. Vig et al. (2020) concluded that the specific heads within the model attended to individual amino acids, as the attention similarity matrix was highly correlated to the expected substitution scores (i.e. BLOSUM62) for each amino acid. They also noted that the deeper layers of the BERT model focused relatively more attention on binding sites and contacts (i.e. high-level concepts). In contrast, information about the secondary structure (i.e. low- to mid-level concepts) within the protein was targeted evenly across each of the layers.
There are many instances of sentences copied verbatim or with minor rephrasing between the current manuscript and RRef2]. Example text:
In this submission However, such methods have yet to be rigorously tested within the context of protein modelling.
In [RRef2] However, such methods have yet to be rigorously tested within the context of drug-protein modelling.
Another example:
In this submission These proteins were collected from the recently curated Pfam database (El-Gebali et al., 2018), which holds approximately thirty-one million protein domains, and forms the corpus used to train large sequence models as featured in TAPE (Rao et al., 2019). The architects of the Pfam database have organised the proteins into clusters that share evolutionary-related groups, also known as families
In [RRef2] “... protein sequences collected from the recently curated Pfam database (ElGebali et al., 2018), which has the proteins organised into clusters that share evolutionary-related groups (i.e. families). This corpus holds approximately thirty-one million protein domains that have been extensively used in bioinformatics. It now forms the corpus used to train large sequence models, such as TAPE (Rao et al., 2019)
Text extracted mostly verbatim from the abstract of another publication [RRef3]. Although there is a reference to [RRef3] at the beginning of the sentence, following citation etiquette, the text extracted verbatim should be highlighted as “... text …”. Example:
In this submission “... they introduced the Tasks Assessing Protein Embeddings (TAPE). They benchmarked the current state-of-the-art models to a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology.
In [RRef3] “... we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology.
Text extracted mostly verbatim from another publication [RRef4]. Similar issue to the one described above. Example:
In this submission Vig et al. (2020) noted that the deeper layers of the BERT model focused relatively more attention on binding sites and contacts (i.e. high-level concepts).
In [RRef4] As shown in Figure 8, deeper layers focus relatively more attention on binding sites and contacts (high-level concept)...
6. Questions to be addressed during rebuttal period
Can the authors enhance the experiments by:
Adding relevant comparison partners for each of the 4 tasks?
Using standard benchmark datasets?
How was triplet-BERT implemented? Please, provide details (libraries used, hardware on which it was executed, etc.)
Can you please expand on how the loss function is used to optimize the parameters of the method?
What can the authors say the problems with the presentation of the text, i.e., text extracted from other sources without proper acknowledgement?
7. Additional remarks: typos found in the text
protein sequences analysing -> analyses a triplet of proteins our organised -> are organised before embeddings a sentence -> not clear, rephrase (before embedding a sentence?) are encoded a BERT model -> not clear, rephrase The BERT model will then outputs the following -> output comprises of the Gloeobacter violaceus ... -> comprises the ... Figure 3b is the final encodings -> encoding Figures 3a-3b presents -> present the cluster map based on the triplet tuned encodings provide a more -> provides can we ascertain the critical parts -> we can ascertain
8. References
[RRef1] Esmaeil Nourani, Ehsaneddin Asgari, Alice C. McHardy, Mohammad R.K. Mofrad. “TripletProt: Deep Representation Learning of Proteins based on Siamese Networks”. bioRxiv 2020.05.11.088237; doi: https://doi.org/10.1101/2020.05.11.088237
[RRef2] Modelling Drug-target Binding Affinity Using A Bert Based Graph Neural Network. Under review at ICLR 2021. URL: https://openreview.net/forum?id=Zqf6RGp5lqf
[RRef3] Rao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. "Evaluating protein transfer learning with TAPE." In Advances in Neural Information Processing Systems, pp. 9689-9701. 2019.
[RRef4] Vig, Jesse, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. "Bertology meets biology: Interpreting attention in protein language models." arXiv preprint arXiv:2006.15222 (2020).
[RRef5] Joshi, Naman Goyal Jingfei Du Mandar, Danqi Chen Omer Levy Mike Lewis, Luke Zettlemoyer Veselin Stoyanov Yinhan Liu, and Myle Ott. "RoBERTa: A Robustly Optimized BERT Pretraining Approach." In Submitted to International Conference on Learning Representations. https://openreview. net/forum. 2020.
[RRef6] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.
[RRef7] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL)
[RRef8] Heinzinger, Michael, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes, and Burkhard Rost. "Modeling aspects of the language of life through transfer-learning protein sequences." BMC bioinformatics 20, no. 1 (2019): 723.
[RRef9] Almagro Armenteros, José Juan, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik Nielsen, and Ole Winther. "DeepLoc: prediction of protein subcellular localization using deep learning." Bioinformatics 33, no. 21 (2017): 3387-3395.


Review Point: of this work, in my opinion, is the lack of comprehensive experiments to show that triplet-BERT outperforms state-of-the-art methods in protein prediction tasks. The methods mentioned above were not used as comparison partners In addition to a lack of comparison partners, the tasks used to assess the performance of triplet-BERT are not standard benchmark tasks like the broadly used TAPE tasks. I recommend the authors to use published datasets that have been designed as benchmark datasets in protein prediction. Of importance is that the data splits must have been created by the respective publications, to guarantee fair comparison between methods, e.g., data associated with the DeepLoc method to predict subcellular localization [RRef9]. The authors show the loss function in Equation 3 but it is unclear if/how it can be differentiated to propagate the gradients. No analysis with respect to this topic is provided in the manuscript. The authors point to Figure 3, that shows cluster maps, as evidence that the fine-tuned encodings give more distinct clusters. This is not immediately obvious from the figure. If actual clusters are obtained, I would suggest using another standard metric to compare clustering results, such as silhouette coefficients or normalized mutual information (NMI). Figure 4 is meant to highlight the performance of the attention mechanism in two proteins (at different layers). It is not clear how these results are later used, if at all. I would recommend adding more context around this figure, especially if it helps drive an important point, which at the present does not seem to be the case. Otherwise, it should be removed.
Review Point: 5. Problems with presentation of the text Portions of the text were extracted verbatim from other sources without the proper acknowledgement. This is a serious problem and can be construed as a case of plagiarism. One of the unnamed sources is another submission to ICLR 2021. Some examples are listed below: The next to last paragraph in page 3 is a match, with minor rephrasing, to the last part of the first paragraph in Section 3 (page 4) of another manuscript under review at ICLR 2021 (indicated as [RRef2] in Section 7 of this review. Example text: In this submission In extension to this work, Vig et al. (2020) explored how this BERT model (Rao et al., 2019) was capable of discerning structural and functional properties about the protein. Vig et al. (2020) proved that the model was able to model long-range dependencies within the sequence of amino acids, it was also able to deduce information about the protein based on the folding structure, target binding sites,and additional complex biophysical properties. They concluded that the specific heads within the model attended to individual amino acids, as the attention similarity matrix was positively correlated to the expected substitution scores (i.e. BLOSUM62) for each amino acid. Vig et al. (2020) noted that the deeper layers of the BERT model focused relatively more attention on binding sites and contacts (i.e. high-level concepts). In contrast, information about the secondary structure (i.e. low-to mid-level concepts) within the protein was targeted evenly across each of the layers In [RRef2] Vig et al. (2020) explored how this particular model was capable of discerning structural and functional properties about proteins. As the BERT transformer was able to model long-range dependencies within the protein, it was able to deduce information about the protein based on the folding structure, target binding sites, and additional complex biophysical properties. Vig et al. (2020) concluded that the specific heads within the model attended to individual amino acids, as the attention similarity matrix was highly correlated to the expected substitution scores (i.e. BLOSUM62) for each amino acid. They also noted that the deeper layers of the BERT model focused relatively more attention on binding sites and contacts (i.e. high-level concepts). In contrast, information about the secondary structure (i.e. low- to mid-level concepts) within the protein was targeted evenly across each of the layers. There are many instances of sentences copied verbatim or with minor rephrasing between the current manuscript and RRef2]. Example text: In this submission However, such methods have yet to be rigorously tested within the context of protein modelling. In [RRef2] However, such methods have yet to be rigorously tested within the context of drug-protein modelling. Another example: In this submission These proteins were collected from the recently curated Pfam database (El-Gebali et al., 2018), which holds approximately thirty-one million protein domains, and forms the corpus used to train large sequence models as featured in TAPE (Rao et al., 2019). The architects of the Pfam database have organised the proteins into clusters that share evolutionary-related groups, also known as families In [RRef2] “... protein sequences collected from the recently curated Pfam database (ElGebali et al., 2018), which has the proteins organised into clusters that share evolutionary-related groups (i.e. families). This corpus holds approximately thirty-one million protein domains that have been extensively used in bioinformatics. It now forms the corpus used to train large sequence models, such as TAPE (Rao et al., 2019) Text extracted mostly verbatim from the abstract of another publication [RRef3]. Although there is a reference to [RRef3] at the beginning of the sentence, following citation etiquette, the text extracted verbatim should be highlighted as “... text …”. Example: In this submission “... they introduced the Tasks Assessing Protein Embeddings (TAPE). They benchmarked the current state-of-the-art models to a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. In [RRef3] “... we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. Text extracted mostly verbatim from another publication [RRef4]. Similar issue to the one described above. Example: In this submission Vig et al. (2020) noted that the deeper layers of the BERT model focused relatively more attention on binding sites and contacts (i.e. high-level concepts). In [RRef4] As shown in Figure 8, deeper layers focus relatively more attention on binding sites and contacts (high-level concept)... 6. Questions to be addressed during rebuttal period Can the authors enhance the experiments by: Adding relevant comparison partners for each of the 4 tasks? Using standard benchmark datasets? How was triplet-BERT implemented? Please, provide details (libraries used, hardware on which it was executed, etc.) Can you please expand on how the loss function is used to optimize the parameters of the method? What can the authors say the problems with the presentation of the text, i.e., text extracted from other sources without proper acknowledgement? 7. Additional remarks: typos found in the text protein sequences analysing -> analyses a triplet of proteins our organised -> are organised before embeddings a sentence -> not clear, rephrase (before embedding a sentence?) are encoded a BERT model -> not clear, rephrase The BERT model will then outputs the following -> output comprises of the Gloeobacter violaceus ... -> comprises the ... Figure 3b is the final encodings -> encoding Figures 3a-3b presents -> present the cluster map based on the triplet tuned encodings provide a more -> provides can we ascertain the critical parts -> we can ascertain 8. References [RRef1] Esmaeil Nourani, Ehsaneddin Asgari, Alice C. McHardy, Mohammad R.K. Mofrad. “TripletProt: Deep Representation Learning of Proteins based on Siamese Networks”. bioRxiv 2020.05.11.088237; doi: https://doi.org/10.1101/2020.05.11.088237 [RRef2] Modelling Drug-target Binding Affinity Using A Bert Based Graph Neural Network. Under review at ICLR 2021. URL: https://openreview.net/forum?id=Zqf6RGp5lqf [RRef3] Rao, Roshan, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. "Evaluating protein transfer learning with TAPE." In Advances in Neural Information Processing Systems, pp. 9689-9701. 2019. [RRef4] Vig, Jesse, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. "Bertology meets biology: Interpreting attention in protein language models." arXiv preprint arXiv:2006.15222 (2020). [RRef5] Joshi, Naman Goyal Jingfei Du Mandar, Danqi Chen Omer Levy Mike Lewis, Luke Zettlemoyer Veselin Stoyanov Yinhan Liu, and Myle Ott. "RoBERTa: A Robustly Optimized BERT Pretraining Approach." In Submitted to International Conference on Learning Representations. https://openreview. net/forum. 2020. [RRef6] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237. [RRef7] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL) [RRef8] Heinzinger, Michael, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes, and Burkhard Rost. "Modeling aspects of the language of life through transfer-learning protein sequences." BMC bioinformatics 20, no. 1 (2019): 723. [RRef9] Almagro Armenteros, José Juan, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik Nielsen, and Ole Winther. "DeepLoc: prediction of protein subcellular localization using deep learning." Bioinformatics 33, no. 21 (2017): 3387-3395.
==================================================

Focused review:

Weaknesses: - With limited space, the paper leaves quite some to wish for in terms of demonstrating the method application in actually uncovering (sets of interesting examples of) linguistic shift. Related work does a much better job at that, and the appendix of this draft does not contribute in this direction. A lot of space is dedicated to method equivalence, which is admirable, but it also shifts the weight of the contribution towards the first part of the writeup.  - Along similar lines, only Google Books and arXiv are used, while related work does more. Altogether this indicates a slightly unbalanced contribution, where the empirical work slightly suffers.  Questions: - Isn't the treatment of the "alignment method" slightly unfair with respect to dimensionality? If the corpora are available for building the embeddings, it seems a triviality to make the embeddings dimensions uniform.

Review Point: - With limited space, the paper leaves quite some to wish for in terms of demonstrating the method application in actually uncovering (sets of interesting examples of) linguistic shift. Related work does a much better job at that, and the appendix of this draft does not contribute in this direction. A lot of space is dedicated to method equivalence, which is admirable, but it also shifts the weight of the contribution towards the first part of the writeup.
Review Point: - Along similar lines, only Google Books and arXiv are used, while related work does more. Altogether this indicates a slightly unbalanced contribution, where the empirical work slightly suffers. Questions:
Review Point: - Isn't the treatment of the "alignment method" slightly unfair with respect to dimensionality? If the corpora are available for building the embeddings, it seems a triviality to make the embeddings dimensions uniform.
==================================================

Focused review:

Weakness:
The authors miss some important related works. Although relying on multi-view supervision, there is a line of previous works focusing on learning-shape-from-x without GT labels, e.g. [1], [2], [3]. Especially, [3] has proposed a quite similar framework, like disentangling canonical shapes and poses, self-supervised losses, as well as selecting the best hypothesis during inference. The major differences are the type of supervision (multi-view) and the network design (PointNet + regression). It also presents many observations mentioned in this paper. For example, symmetric objects like cars are difficult for such self-supervised frameworks.
Although there are no existing approaches to compare directly, the authors could provide more ablation studies to verify the design choices, which is kind of missing now. Although Sec 2.4 in Supp has presented some results, can the authors analyze the impact of equivariant networks on the shape and pose separately? For example, KPConv (or EPN) for shapes and EPN (or KPConv) for poses. Compared to [3], one key improvement of this paper is to introduce SE(3)-equivariant networks to generate consistent canonical shapes. Another question is how important it is to predict multiple hypotheses for 6D poses. Can the authors study the impact of the number of hypotheses (predefined groups)? And can the authors directly regress poses rather than using any hypothesis?
The second car in Figure 2 looks strange. The blue point cloud (canonical shape) in the 3rd column looks different from that in the 2nd column.
How stable is the training process? It seems that the network can easily get stuck in local minima (imperfect shapes will lead to wrong poses, and imperfect poses can also lead to imperfect shapes. This phenomenon might be most obvious for cars). Can the authors report the result of multiple trials of experiments to show the variance of the method?

Typos:
L152: there are two "j-th", and I think one should be "j'-th".
L203-L204:
g
x
=
Δ
g
x
0
⋅
g
x

References
[1] Unsupervised Learning of Shape and Pose with Differentiable Point Clouds
[2] Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction
[3] Weakly-supervised 3d shape completion in the wild


Review Point: The authors miss some important related works. Although relying on multi-view supervision, there is a line of previous works focusing on learning-shape-from-x without GT labels, e.g. [1], [2], [3]. Especially, [3] has proposed a quite similar framework, like disentangling canonical shapes and poses, self-supervised losses, as well as selecting the best hypothesis during inference. The major differences are the type of supervision (multi-view) and the network design (PointNet + regression). It also presents many observations mentioned in this paper. For example, symmetric objects like cars are difficult for such self-supervised frameworks. Although there are no existing approaches to compare directly, the authors could provide more ablation studies to verify the design choices, which is kind of missing now. Although Sec 2.4 in Supp has presented some results, can the authors analyze the impact of equivariant networks on the shape and pose separately? For example, KPConv (or EPN) for shapes and EPN (or KPConv) for poses. Compared to [3], one key improvement of this paper is to introduce SE(3)-equivariant networks to generate consistent canonical shapes. Another question is how important it is to predict multiple hypotheses for 6D poses. Can the authors study the impact of the number of hypotheses (predefined groups)? And can the authors directly regress poses rather than using any hypothesis? The second car in Figure 2 looks strange. The blue point cloud (canonical shape) in the 3rd column looks different from that in the 2nd column. How stable is the training process? It seems that the network can easily get stuck in local minima (imperfect shapes will lead to wrong poses, and imperfect poses can also lead to imperfect shapes. This phenomenon might be most obvious for cars). Can the authors report the result of multiple trials of experiments to show the variance of the method? Typos:
Review Point: L152: there are two "j-th", and I think one should be "j'-th". L203-L204: g x = Δ g x 0 ⋅ g x References [1] Unsupervised Learning of Shape and Pose with Differentiable Point Clouds [2] Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction [3] Weakly-supervised 3d shape completion in the wild
==================================================

Focused review:

- It seems there are underlying assumptions that are not clearly explained regarding equation 3 in Section 3: Input features x_j need to be independent, Otherwise the joint distribution of the input vector x belonging to regions R_k cannot be obtained by multiplying the individual pdfs of the input features x_j belonging to region R_k (note: region R_k equals a leaf k). If the same pdf is used for all x_j, then the assumption is that the input features x_j are iid (this is the case here) - Explain how the location parameters u_j is obtained. Is it the center of a region? - If enough data is available, a standard regression tree with enough leaves would also approximate a smooth link function. Maybe this approach would be advantageous in the small data regime? - The proposed method seems closer to “Uncertain Decision Trees (UDT)” than to “Soft trees” or “Smooth transition trees (STR)”. While Soft trees and STR have a soft branching mechanism based on a sigmoid at each node, the proposed PR trees have a soft assignment only at the leaves (last nodes). Uncertain Decision Trees propose also a soft assignment at the leaves, but use a different parametrization of the pdf (estimate the probability mass for discrete bins). Hence, it might be meaningful to compare the performance of PR trees to UDT. - The discussion of the empirical results is not convincing and contains some confusing statements regarding potential overfitting, please improve. - I think this work would get more attention in a journal format, which would give more room for detailed explanations.

Review Point: - It seems there are underlying assumptions that are not clearly explained regarding equation 3 in Section 3: Input features x_j need to be independent, Otherwise the joint distribution of the input vector x belonging to regions R_k cannot be obtained by multiplying the individual pdfs of the input features x_j belonging to region R_k (note: region R_k equals a leaf k). If the same pdf is used for all x_j, then the assumption is that the input features x_j are iid (this is the case here) - Explain how the location parameters u_j is obtained. Is it the center of a region?
Review Point: - If enough data is available, a standard regression tree with enough leaves would also approximate a smooth link function. Maybe this approach would be advantageous in the small data regime?
Review Point: - The proposed method seems closer to “Uncertain Decision Trees (UDT)” than to “Soft trees” or “Smooth transition trees (STR)”. While Soft trees and STR have a soft branching mechanism based on a sigmoid at each node, the proposed PR trees have a soft assignment only at the leaves (last nodes). Uncertain Decision Trees propose also a soft assignment at the leaves, but use a different parametrization of the pdf (estimate the probability mass for discrete bins). Hence, it might be meaningful to compare the performance of PR trees to UDT.
Review Point: - The discussion of the empirical results is not convincing and contains some confusing statements regarding potential overfitting, please improve.
Review Point: - I think this work would get more attention in a journal format, which would give more room for detailed explanations.
==================================================

Focused review:

weaknesses of the evaluation can be found in tasks 2 and 3, in my opinion. Showing only rank plots in the main paper is insufficient as they hide the degree of improvement. This becomes particularly clear if one considers the actual performance tables provided in appendix H (for task 2) where one can see that on many datasets the performance differences between METABU MF and the other methods are negligible. In fact, there seems to be almost no statistically significant differences with only a handful of exceptions. To me this is quite in contrast to the claims made by the authors in the main paper that their approach is statistically significantly better than the baselines. This might be true for the average rank, but it seems to be far from the truth when looking at the exact performances. Furthermore, there are no detailed results for task 3 at all in the appendix such that the rank plots are even less meaningful for task 3. The drastically varying p-levels in Tables 6-8 in the appendix also do not give a good impression. In my opinion, the authors should revise the evaluation to show concrete performance gains instead of rank plots. In particular, instead of showing the ranks of the methods over the time of the optimization, I suggest to plot the concrete performance of the corresponding solutions. However, from the results in the appendix I have doubts that such improvements exist in a non-negligible form.
Furthermore, it is unclear to me why (for task 3) the target distributions for AutoSklearn and PMF are different. In my eyes, the target distribution should be independent of the used AutoML method and only depend on the search space of the corresponding method. If this is meant by the authors, I strongly suggest to use the same search space for AutoSklearn and PMF as the results are hard to compare otherwise.
Lastly, I believe that some statements in the paper are formulated in a way making them at least debatable:
p. 2: "the ML meta-features have hardly been effective to achieve AutoML (Misir & Sebag, 2017) or even to distinguish among hard and easy datasets w.r.t. a given learning algorithm (Muñoz et al., 2018)" → I believe this claim to be wrong. Several works have shown that meta-features are well suited for algorithm recommendation even in the context of ML algorithms (including AutoSklearn 2.0 [1]).
p. 3: "By construction, it can be cheaply computed for any dataset." → Not true for landmarking features, which are notoriously expensive.
p. 6 goal of experiments: "The first goal is to measure the performance of the METABU meta-features" → meta-features themselves cannot have a performance.
Explanation of results
Task 1: "[...] the metrics based on the baselines are deterministic" → the landmarking baselines are certainly not deterministic as they also rely on training the respective ML algorithm whose performance is tracked
Task 2: "METABU dominates after the beginning; all approaches but the uniform sampler yield similar performances" → Contradiction. Does it dominate or is performance similar? Both statements cannot be true at the same time.
Ethics statement: I do not believe that the goal of AutoML is to reduce the computational resources needed to get peak performance from an ML portfolio. While I agree that AutoML methods are often faster in generating a high quality ML pipeline for a given dataset than experts as some competitions have shown, I highly doubt that they use less computational resources than the corresponding human experts would.
With all of the above in mind, I believe that one cannot assess the empirical quality of the proposed methodology based on the current state of the evaluation. Accordingly, claims made regarding the quality of the methodology are not well supported, in my opinion.
References
[1] Feurer, Matthias, et al. "Auto-sklearn 2.0: The next generation." arXiv preprint arXiv:2007.04074 (2020).
Novelty and Significance of Technical Algorithms, Models, or Theory
While the proposed methodology certainly is novel in the specific context of AutoML, it is well known in other contexts as acknowledged by the authors. I believe this to be completely okay for an empirical field such as AutoML. However, I believe that the authors could have focused more on giving explanations on other embedding approaches from the context of AutoML. Although the authors point to PMF and OBOE, the differences of the embedding learned in this work compared to the other embedding approaches should be explored more thoroughly on a methodological level. Thus, while the work is well contextualized wrt. to existing literature on a broad level, it lacks a good contextualization on a more nuanced level.
The significance of the methodology is hard to assess as the empirical evaluation is not reliable as stated under correctness. However, assuming that the methodology does indeed yield good meta-features, I deem it significant for the AutoML community.
Novelty and Significance of Empirical Advancements, Insights, or Datasets
Since I have doubts regarding the empirical evaluation as a whole in this work, I deem it to be non-significant. However, the results presented in 4.4 are certainly interesting and I have not seen an analysis of the intrinsic dimension of the OpenML CC-18 benchmark before. Thus, at least this part of the evaluation yields novel insights.
Minor Issues
different writings of AutoSklearn throughout the paper
in contrast with → in contrast to
"OT, the Wasserstein distance [...]" → OT seems to be out of place there.
p. 4: difference in notation between step 1 and step 2 - in step 1 "d" is the 2-Wasserstein distance whereas in step 2 "d_W^2" is the distance.

Update after Rebuttal
I would like to thank the authors for their strong rebuttal and excuse myself for misjudging that the learned embedding is performance preserving instead of rank preserving rendering my criticism of the evaluation task 1 infeasible. My remaining questions have been thoroughly answered and the requested changes to the evaluation have been added to the paper where applicable. As my main concern has been with the evaluation, all reasons why I could not suggest acceptance of this paper have been ruled out. Although I do not agree to all opinions presented in the paper and the rebuttal (e.g. computational resources), I do not have any further factual criticism. As such, I will adjust my rating correspondingly to suggest acceptance of this work.


Review Point: of the evaluation can be found in tasks 2 and 3, in my opinion. Showing only rank plots in the main paper is insufficient as they hide the degree of improvement. This becomes particularly clear if one considers the actual performance tables provided in appendix H (for task 2) where one can see that on many datasets the performance differences between METABU MF and the other methods are negligible. In fact, there seems to be almost no statistically significant differences with only a handful of exceptions. To me this is quite in contrast to the claims made by the authors in the main paper that their approach is statistically significantly better than the baselines. This might be true for the average rank, but it seems to be far from the truth when looking at the exact performances. Furthermore, there are no detailed results for task 3 at all in the appendix such that the rank plots are even less meaningful for task 3. The drastically varying p-levels in Tables 6-8 in the appendix also do not give a good impression. In my opinion, the authors should revise the evaluation to show concrete performance gains instead of rank plots. In particular, instead of showing the ranks of the methods over the time of the optimization, I suggest to plot the concrete performance of the corresponding solutions. However, from the results in the appendix I have doubts that such improvements exist in a non-negligible form. Furthermore, it is unclear to me why (for task 3) the target distributions for AutoSklearn and PMF are different. In my eyes, the target distribution should be independent of the used AutoML method and only depend on the search space of the corresponding method. If this is meant by the authors, I strongly suggest to use the same search space for AutoSklearn and PMF as the results are hard to compare otherwise. Lastly, I believe that some statements in the paper are formulated in a way making them at least debatable: p.
Review Point: 2: "the ML meta-features have hardly been effective to achieve AutoML (Misir & Sebag, 2017) or even to distinguish among hard and easy datasets w.r.t. a given learning algorithm (Muñoz et al., 2018)" → I believe this claim to be wrong. Several works have shown that meta-features are well suited for algorithm recommendation even in the context of ML algorithms (including AutoSklearn 2.0 [1]). p.
Review Point: 3: "By construction, it can be cheaply computed for any dataset." → Not true for landmarking features, which are notoriously expensive. p.
Review Point: 6 goal of experiments: "The first goal is to measure the performance of the METABU meta-features" → meta-features themselves cannot have a performance. Explanation of results Task 1: "[...] the metrics based on the baselines are deterministic" → the landmarking baselines are certainly not deterministic as they also rely on training the respective ML algorithm whose performance is tracked Task 2: "METABU dominates after the beginning; all approaches but the uniform sampler yield similar performances" → Contradiction. Does it dominate or is performance similar? Both statements cannot be true at the same time. Ethics statement: I do not believe that the goal of AutoML is to reduce the computational resources needed to get peak performance from an ML portfolio. While I agree that AutoML methods are often faster in generating a high quality ML pipeline for a given dataset than experts as some competitions have shown, I highly doubt that they use less computational resources than the corresponding human experts would. With all of the above in mind, I believe that one cannot assess the empirical quality of the proposed methodology based on the current state of the evaluation. Accordingly, claims made regarding the quality of the methodology are not well supported, in my opinion. References [1] Feurer, Matthias, et al. "Auto-sklearn 2.0: The next generation." arXiv preprint arXiv:2007.04074 (2020). Novelty and Significance of Technical Algorithms, Models, or Theory While the proposed methodology certainly is novel in the specific context of AutoML, it is well known in other contexts as acknowledged by the authors. I believe this to be completely okay for an empirical field such as AutoML. However, I believe that the authors could have focused more on giving explanations on other embedding approaches from the context of AutoML. Although the authors point to PMF and OBOE, the differences of the embedding learned in this work compared to the other embedding approaches should be explored more thoroughly on a methodological level. Thus, while the work is well contextualized wrt. to existing literature on a broad level, it lacks a good contextualization on a more nuanced level. The significance of the methodology is hard to assess as the empirical evaluation is not reliable as stated under correctness. However, assuming that the methodology does indeed yield good meta-features, I deem it significant for the AutoML community. Novelty and Significance of Empirical Advancements, Insights, or Datasets Since I have doubts regarding the empirical evaluation as a whole in this work, I deem it to be non-significant. However, the results presented in 4.4 are certainly interesting and I have not seen an analysis of the intrinsic dimension of the OpenML CC-18 benchmark before. Thus, at least this part of the evaluation yields novel insights. Minor Issues different writings of AutoSklearn throughout the paper in contrast with → in contrast to "OT, the Wasserstein distance [...]" → OT seems to be out of place there. p.
Review Point: 4: difference in notation between step 1 and step 2 - in step 1 "d" is the 2-Wasserstein distance whereas in step 2 "d_W^2" is the distance. Update after Rebuttal I would like to thank the authors for their strong rebuttal and excuse myself for misjudging that the learned embedding is performance preserving instead of rank preserving rendering my criticism of the evaluation task 1 infeasible. My remaining questions have been thoroughly answered and the requested changes to the evaluation have been added to the paper where applicable. As my main concern has been with the evaluation, all reasons why I could not suggest acceptance of this paper have been ruled out. Although I do not agree to all opinions presented in the paper and the rebuttal (e.g. computational resources), I do not have any further factual criticism. As such, I will adjust my rating correspondingly to suggest acceptance of this work.
==================================================

Focused review:

- The results are not applicable to some other important family of networks like ResNets. The analysis seems to be relying on the homogeneity. Does this mean that ResNets have different behavior? Or they have similar behaviors, but the analysis is limited by the current techniques available? - Can one have discrete-time guarantees? Especially since the analysis if for late training, one would like to have an analysis saying after how many steps the training roughly converges (stabilizes). Does it take exponential time to stabilize? - The generalization behavior is only briefly mentioned. The implications on margins are related, but more explicit discussion will be appreciated. I would also like to see more discussion on the effect of late training on generalization, e.g., does the late training after loss 1/#datapoints improve the generalization or can hurt the generalization? ============after response=============== The response clarified my questions on generalization and distinction from prior work. It doesn't adequately address the questions about discrete-time analysis and non-homogeneous networks, but I understand these two are quite beyond the scope of the paper, so I increase the score from 6 to 7.

Review Point: - The results are not applicable to some other important family of networks like ResNets. The analysis seems to be relying on the homogeneity. Does this mean that ResNets have different behavior? Or they have similar behaviors, but the analysis is limited by the current techniques available?
Review Point: - Can one have discrete-time guarantees? Especially since the analysis if for late training, one would like to have an analysis saying after how many steps the training roughly converges (stabilizes). Does it take exponential time to stabilize?
Review Point: - The generalization behavior is only briefly mentioned. The implications on margins are related, but more explicit discussion will be appreciated. I would also like to see more discussion on the effect of late training on generalization, e.g., does the late training after loss 1/#datapoints improve the generalization or can hurt the generalization? ============after response=============== The response clarified my questions on generalization and distinction from prior work. It doesn't adequately address the questions about discrete-time analysis and non-homogeneous networks, but I understand these two are quite beyond the scope of the paper, so I increase the score from 6 to 7.
==================================================

Focused review:

While there are important differences in the broad contributions, I feel that the significance of the proposed algorithms and experimental results is not so high. Specifically, I have several concerns about the contributions: - The proposed PTAS does not appear to be practical. In fact, this paper admits the weakness in lines 255 and 276. - While the derivation of the quantile-based algorithm would be a good contribution, the approximation factors are quite poor. In particular, I'm not sure how significant the factor 1000 of the second largest expected value is. - It's unclear how important the proposed algorithm is compared to the existing algorithm [KR18]. Of course, the differences are described in Section 1, the experimental results showed the KR algorithm was sometimes better than the proposed quantile-based algorithm. The paper did not provide an enough discussion of how to interpret the experimental results. - Related to the above concern, it seems that the experimental results do not strongly support the merits of the proposed method. From the results, it is unclear what the strengths of the proposed method are and thus what this paper wants to point out through the experiments.

Review Point: While there are important differences in the broad contributions, I feel that the significance of the proposed algorithms and experimental results is not so high. Specifically, I have several concerns about the contributions:
Review Point: - The proposed PTAS does not appear to be practical. In fact, this paper admits the weakness in lines 255 and 276.
Review Point: - While the derivation of the quantile-based algorithm would be a good contribution, the approximation factors are quite poor. In particular, I'm not sure how significant the factor 1000 of the second largest expected value is.
Review Point: - It's unclear how important the proposed algorithm is compared to the existing algorithm [KR18]. Of course, the differences are described in Section 1, the experimental results showed the KR algorithm was sometimes better than the proposed quantile-based algorithm. The paper did not provide an enough discussion of how to interpret the experimental results.
Review Point: - Related to the above concern, it seems that the experimental results do not strongly support the merits of the proposed method. From the results, it is unclear what the strengths of the proposed method are and thus what this paper wants to point out through the experiments.
==================================================

Focused review:

weakness of the general method is that the approach seems to alleviate computational complexity during execution of a learned policy but during the learning phase, however, the method seems not to alleviate the problem of explosive complexity in the number of agents which may cause tractability issues for large populations. Furthermore, the method seems reliant on the population size being large for the approximations to hold though the paper does not include a discussion on when the population size is reasonably large for the method to be valid. Finally, though the contribution is useful for the given setting, it is not clear how often the central assumption of non-decomposability occurs in multi-agent systems. (edited) There are also some other important questions that if addressed would improve the paper: In particular, though the setting of the paper is said to be a partially observable - it seems that 'distant' agents do not have an immediate effect on the agentsâ rewards since the rewards depend on variables that are defined locally  (though they may interact with the agent at some later time). In this sense each agent's observations fully capture all relevant state information - it doesn't seem as though extra information concerning the agent's immediate reward or state transition can be inferred by including global information.  Secondly, the authors introduce the notion of agent types as a way of introducing heterogeneity into the formalism. The notion of agent types, however, seems inconsistent since it is assumed that the agents have the same policy which would differ for different agent types if type is any meaningful parameter (i.e. determines the agents' rewards/state transition probabilities). Additionally, heterogeneity from agent types also seems not to fit with the idea of a global critic.  Lastly, the advantage function (15) as used in the cited paper is used in a team game setting when agents have the same reward function. There are a number of details on which the theory is built that are omitted - in particular:  1. The authors do not state conditions under which sufficient statistic is actually sufficient (is this derived from application of the law of large numbers?) - this also seems to impose conditions of homogeneity on the agents - without a clear statement of the conditions under which this holds, some other statements are hard to evaluate e.g. line 163: "which makes learning scalable even with very large number of agents" and "Crucially, the above computation is independent of agent population M"- it seems that these statements in fact only hold in cases when M is large. This also arises in lines 195-196 where M must be large for the approximation to be valid as well as in the proof B1.  2. The authors state that "Previous work shows that count tables are the sufficient statistic for planning" - this seems to be fairly fundamental, it would be of great benefit to the reader to have a citation and a reference to the result for this remark. The paper would also benefit from a discussion on the solution admitted by the method. Specifically, though the setting is non-cooperative since each agent maximises their individual rewards the examples discussed nonetheless suggest the authors seek to target cooperative or team games. Though cooperative and team-based games can be tackled in a decentralised way,  the stable points in non-cooperative game setting (i.e. Markov game) are described by Nash equilibria (Markov-perfect Nash equilibria) which are in general inefficient from a social welfare perspective. The authors ought to include a discussion on how they expect the payoffs from the learned policies which seem to be generated by Nash equilibrium strategies to perform in relation to the system goal in general and, possibly how they might overcome challenges of inefficiency from non-cooperative challenges.    3. What is the meaning of d - can the authors give an example?  4. Additionally, the authors state that "We assume that Qw is differentiable in all input parameters". This doesn't seem to be a reasonable assumption given that one of the inputs n^{sa} is a discrete integer count.    The reason for my score is the following. The paper is clearly written leading to a coherent exposition and its objectives are clearly stated. The contribution seems however only incremental given the existing (and cited) methods on difference rewards and in the absence of clear statements when the method is most applicable. The crucial element, however, is about the correctness of the method. I have two major concerns: i) about the differentiability of Q_w* and ii) about small population sizes. There's also a (minor) issue about the claim that the method can work with heterogeneous populations, where the method, which seems to depend on one single policy being learned, may break down. i) The assumption of differentiability of Q_w*, which appears to be (likely) invalidated in the current setting as one of its inputs is a discrete integer variable. This seems to me to be a concern as differentiation here is ill-defined. The authors have not addressed this issue in their rebuttal.   ii) The derivations of the Q function estimates rely on large populations (e.g. eqs. (5), (9)), and use previous results (e.g. citations [4] and [6] on the appendix) but those papers (beside having many notational inconsistencies) also rely on large populations.  Overall, the paper is well written and easy to follow, but I  have major concerns with the correctness of the method. Also, reading the previous papers that this paper heavily relies on, the contribution looks very incremental. 

Review Point: of the general method is that the approach seems to alleviate computational complexity during execution of a learned policy but during the learning phase, however, the method seems not to alleviate the problem of explosive complexity in the number of agents which may cause tractability issues for large populations. Furthermore, the method seems reliant on the population size being large for the approximations to hold though the paper does not include a discussion on when the population size is reasonably large for the method to be valid. Finally, though the contribution is useful for the given setting, it is not clear how often the central assumption of non-decomposability occurs in multi-agent systems. (edited) There are also some other important questions that if addressed would improve the paper: In particular, though the setting of the paper is said to be a partially observable - it seems that 'distant' agents do not have an immediate effect on the agentsâ rewards since the rewards depend on variables that are defined locally (though they may interact with the agent at some later time). In this sense each agent's observations fully capture all relevant state information - it doesn't seem as though extra information concerning the agent's immediate reward or state transition can be inferred by including global information. Secondly, the authors introduce the notion of agent types as a way of introducing heterogeneity into the formalism. The notion of agent types, however, seems inconsistent since it is assumed that the agents have the same policy which would differ for different agent types if type is any meaningful parameter (i.e. determines the agents' rewards/state transition probabilities). Additionally, heterogeneity from agent types also seems not to fit with the idea of a global critic. Lastly, the advantage function (15) as used in the cited paper is used in a team game setting when agents have the same reward function. There are a number of details on which the theory is built that are omitted - in particular:
Review Point: 1. The authors do not state conditions under which sufficient statistic is actually sufficient (is this derived from application of the law of large numbers?) - this also seems to impose conditions of homogeneity on the agents - without a clear statement of the conditions under which this holds, some other statements are hard to evaluate e.g. line 163: "which makes learning scalable even with very large number of agents" and "Crucially, the above computation is independent of agent population M"- it seems that these statements in fact only hold in cases when M is large. This also arises in lines 195-196 where M must be large for the approximation to be valid as well as in the proof B1.
Review Point: 2. The authors state that "Previous work shows that count tables are the sufficient statistic for planning" - this seems to be fairly fundamental, it would be of great benefit to the reader to have a citation and a reference to the result for this remark. The paper would also benefit from a discussion on the solution admitted by the method. Specifically, though the setting is non-cooperative since each agent maximises their individual rewards the examples discussed nonetheless suggest the authors seek to target cooperative or team games. Though cooperative and team-based games can be tackled in a decentralised way, the stable points in non-cooperative game setting (i.e. Markov game) are described by Nash equilibria (Markov-perfect Nash equilibria) which are in general inefficient from a social welfare perspective. The authors ought to include a discussion on how they expect the payoffs from the learned policies which seem to be generated by Nash equilibrium strategies to perform in relation to the system goal in general and, possibly how they might overcome challenges of inefficiency from non-cooperative challenges.
Review Point: 3. What is the meaning of d - can the authors give an example?
Review Point: 4. Additionally, the authors state that "We assume that Qw is differentiable in all input parameters". This doesn't seem to be a reasonable assumption given that one of the inputs n^{sa} is a discrete integer count. The reason for my score is the following. The paper is clearly written leading to a coherent exposition and its objectives are clearly stated. The contribution seems however only incremental given the existing (and cited) methods on difference rewards and in the absence of clear statements when the method is most applicable. The crucial element, however, is about the correctness of the method. I have two major concerns: i) about the differentiability of Q_w* and ii) about small population sizes. There's also a (minor) issue about the claim that the method can work with heterogeneous populations, where the method, which seems to depend on one single policy being learned, may break down. i) The assumption of differentiability of Q_w*, which appears to be (likely) invalidated in the current setting as one of its inputs is a discrete integer variable. This seems to me to be a concern as differentiation here is ill-defined. The authors have not addressed this issue in their rebuttal. ii) The derivations of the Q function estimates rely on large populations (e.g. eqs. (5), (9)), and use previous results (e.g. citations [4] and [6] on the appendix) but those papers (beside having many notational inconsistencies) also rely on large populations. Overall, the paper is well written and easy to follow, but I have major concerns with the correctness of the method. Also, reading the previous papers that this paper heavily relies on, the contribution looks very incremental.
==================================================

Focused review:

Weaknesses: - There is no theoretical result or analysis supporting the proposed method. - The method applies only when testing data is incomplete requiring complete training datasets with limits its application in many practical situations where training datasets are also incomplete. - The paper compares the proposed method only against simple and basic imputation methods. There are many other approaches in the literature to train classifiers on incomplete or complete training datasets and apply them to incomplete test datasets. See for example: o C. Caiafa, Z. Wang, J. Sole-Casals and Q. Zhao, "Learning from Incomplete Features by Simultaneous Training of Neural Networks and Sparse Coding," in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Nashville, TN, USA, 2021 pp. 2621-2630. o Marek Smieja, Lukasz Struski, Jacek Tabor, Bartosz Zielinski, and Przemyslaw Spurek. Processing of missing data by neural networks. In NeurIPS, 2018. - This paper considers only small datasets with small NNs (only 3-layer NNs). The paper would be benefited by considering also the application of the method to deeper architectures and larger datasets such as computer vision datasets (MNIST and CIFAR for example). - There is no explanation on why a maximum of P/2 features can be removed. Is there any theoretical explanation for such constraint? How the algorithms behaves when missing entries are more than P/2? - Results comparing classical training (MLP) with augmented data training (AMLP) does not consider any statistical analysis. In fact results are very similar and it is not clear if the differences are statistically significant.


Review Point: - There is no theoretical result or analysis supporting the proposed method.
Review Point: - The method applies only when testing data is incomplete requiring complete training datasets with limits its application in many practical situations where training datasets are also incomplete.
Review Point: - The paper compares the proposed method only against simple and basic imputation methods. There are many other approaches in the literature to train classifiers on incomplete or complete training datasets and apply them to incomplete test datasets. See for example: o C. Caiafa, Z. Wang, J. Sole-Casals and Q. Zhao, "Learning from Incomplete Features by Simultaneous Training of Neural Networks and Sparse Coding," in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Nashville, TN, USA, 2021 pp. 2621-2630. o Marek Smieja, Lukasz Struski, Jacek Tabor, Bartosz Zielinski, and Przemyslaw Spurek. Processing of missing data by neural networks. In NeurIPS, 2018.
Review Point: - This paper considers only small datasets with small NNs (only 3-layer NNs). The paper would be benefited by considering also the application of the method to deeper architectures and larger datasets such as computer vision datasets (MNIST and CIFAR for example).
Review Point: - There is no explanation on why a maximum of P/2 features can be removed. Is there any theoretical explanation for such constraint? How the algorithms behaves when missing entries are more than P/2?
Review Point: - Results comparing classical training (MLP) with augmented data training (AMLP) does not consider any statistical analysis. In fact results are very similar and it is not clear if the differences are statistically significant.
==================================================

Focused review:

Weaknesses:
1) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).
2) I don't think the baseline comparisons made in the experiments are appropriate. The proposal is a method to choose the appropriate number of bins in the estimate, and should be compared to other methods to do so instead of to an arbitrary choice of number of bins as is done in section 5.2. Without this comparison, I have no way to judge if this is a good autotuning method or not. Reasonable comparisons could be, e.g., choosing
b
by cross validation, or, in equal mass binning, choosing
b
so that each bin has a reasonable number of samples for the error
y
―
k
to not be too large.
3) While the focus of the paper is on bias, it should be noted that by searching over many different bin sizes, the variance of ECE_sweep may be inflated. If this is to such an extent that the gains in bias relative to other autotuning methods are washed out, then this estimator would not be good. To judge this requires at least that the variances for ECE_sweep are reported, but these are never mentioned in the main text.
4) Choice of law in simulation in section 3, which are used to illustrate the dependence of bias on the number of bins, not aligned with the laws/curves in figure 3. Taking the latter as representative of the sort of laws and calibration curves that arise in practice, there are two issues:
4a) The pdfs of
f
tend to be a lot more peaked near the end than the one explored in section 3 - this is borne out by the values of
α
,
β
in the fits in Table 1. Beta(1.1,1) is remarkably flat compared to the curves in Fig 3.
4b) There seem to be a few different qualitative properties of the calibration curves - monotone but with a large intercept at 0; those with an inflection point in the middle; and those with the bulk lying below the
y
=
x
line. In particular, all of them tend to have at least some region above the
y
=
x
line. The choice of curve
c
2
in section 3 doesn't completely align with any of these cases, but even if we make the case that it aligns with the third type, this leaves two qualitative behaviours unexplored.
In fact, the choice of laws is such that the error of the hard classifier that thresholds
f
at
1
/
2
is
26
%
. I don't think we're usually interested in the calibration of a predictor as poor as this in practice.
All of this makes me question the relevance of this simulation. Is the dependence of the bias on the number of bins as strong for the estimated laws as it is for these? Seeing the the equivalents of figs 7 and 8 for the laws from section 5 would go a long way in sorting this out.
5) Experiments: As I previously mentioned, I don't think the correct baselines are compared to. Instead of posing the method against other autotuning schemes, just one choice of the number of bins is taken. This already makes it near impossible to judge the efficacy of this method.
Despite this, even the data presented does not make a clear case for ECE_sweep. In Fig. 4 we see that the bias of EW_sweep is even worse than EW. This already means that the sweep estimate doesn't fix the issues of ECE_bin in all contexts. It is the case that EM_sweep has better bias than EM, but again, for samples large enough for the variances to be in control, it seems like these numbers are both converging to the same, so I don't see any distinct advantage when it comes to estimation. (of course, this is moot because this isn't the right comparison anyway)
Also, Fig. 5 is flawed because it compares EW and EM_sweep. It should either compare EM and EM_sweep, or EW and EW_sweep, I don't see why EW and EM_sweep are directly comparable.
Minor issues:
a) Algorithm (1) and the formula for ECE_sweep in section 4 don't compute the same thing. In algorithm (1), you find the largest
b
such that the resulting
y
―
k
is a monotone sequence, and return the ECE_bin for this number of bins. In the formula, you maximise the ECE_bin for all b that yield a monotone
y
―
k
. From the preceding text, I assumed that the quantity in Algorithm (1) is intended.
b) Why is the
L
p
norm definition of the ECEs introduced at all? In the paper only
p
=
2
is used throughout. I feel like the
p
just complicates things without adding much - even if you only present the
L
2
definition, the fact that a generic
p
can be used instead should be obvious to the audience.
c) Design considerations for ECE_sweep - it is worth noting that accuracy is not all that we want in an estimate of calibration error. For instance, one might reasonably want to add this as a regulariser when training a model in order to obtain better calibrated solutions. One issue with ECE_sweep is that it introduces a problem in that how the number of bins in the ECE_sweep estimate changes with a small change in model parameters seems very difficult to handle, which makes this a nondifferentiable loss. Broader issues of this form, and a discussion of how they may be mitigated, could lead to a more well rounded paper.

Comments:
a) Exact monotonicity in the ECE_sweep proposal - I find the argument stemming from the monotonicity of the true calibration curve, and the idea to use this to nail down a maximum binning size interesting. However, why should we demand exact monotonicity in the bin heights? Each
y
―
k
will have noise at the scale of roughly
b
/
n
,
(for equal mass binning with
b
bins), and in my opinion, violation of monotonicity at this scale should not be penalised. Also, what if a few
y
―
k
s decrease but most are increasing (i.e., the sequence has a few falling regions, but the bulk is increasing)? Perhaps instead of dealing with this crudely, the error of a shape constrained estimator may serve as a better proxy.
b) Isn't the procedure for parametrically fitting the pdf of
f
, and
E
[
Y
 
f
(
X
)
]
,
and then integrating the bias a completely different estimator for TCE of a model? In fact, if the laws are a good fit, as is claimed in section 5.1, then this plug in estimator might do well simply because the integration is exact. In fact, since the fit is parametric, this can further be automatically differentiated (if, say,
f
were a DNN), and thus used to train.
c) It would be interesting to see what number of bins are ultimately adopted in the ECE_sweep computations that are performed.

Overall opinion: The lack of comparison to appropriate baselines makes it near impossible for me to judge the validity of the proposed estimator. I feel like this is a deep methodological flaw when it comes to evaluating the main proposal of the paper. This is a real pity because I quite like some of the ideas in the paper.
Due to the inability to evaluate the main contribution of the paper, i am rating it a strong reject. I'd be completely open to re-rating it if appropriate comparisons are performed, and the case for the method is properly made.


Review Point: 1) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).
Review Point: 2) I don't think the baseline comparisons made in the experiments are appropriate. The proposal is a method to choose the appropriate number of bins in the estimate, and should be compared to other methods to do so instead of to an arbitrary choice of number of bins as is done in section 5.2. Without this comparison, I have no way to judge if this is a good autotuning method or not. Reasonable comparisons could be, e.g., choosing b by cross validation, or, in equal mass binning, choosing b so that each bin has a reasonable number of samples for the error y ― k to not be too large.
Review Point: 3) While the focus of the paper is on bias, it should be noted that by searching over many different bin sizes, the variance of ECE_sweep may be inflated. If this is to such an extent that the gains in bias relative to other autotuning methods are washed out, then this estimator would not be good. To judge this requires at least that the variances for ECE_sweep are reported, but these are never mentioned in the main text.
Review Point: 4) Choice of law in simulation in section 3, which are used to illustrate the dependence of bias on the number of bins, not aligned with the laws/curves in figure 3. Taking the latter as representative of the sort of laws and calibration curves that arise in practice, there are two issues: 4a) The pdfs of f tend to be a lot more peaked near the end than the one explored in section 3 - this is borne out by the values of α , β in the fits in Table 1. Beta(1.1,1) is remarkably flat compared to the curves in Fig 3. 4b) There seem to be a few different qualitative properties of the calibration curves - monotone but with a large intercept at 0; those with an inflection point in the middle; and those with the bulk lying below the y = x line. In particular, all of them tend to have at least some region above the y = x line. The choice of curve c 2 in section 3 doesn't completely align with any of these cases, but even if we make the case that it aligns with the third type, this leaves two qualitative behaviours unexplored. In fact, the choice of laws is such that the error of the hard classifier that thresholds f at 1 / 2 is 26 % . I don't think we're usually interested in the calibration of a predictor as poor as this in practice. All of this makes me question the relevance of this simulation. Is the dependence of the bias on the number of bins as strong for the estimated laws as it is for these? Seeing the the equivalents of figs 7 and 8 for the laws from section 5 would go a long way in sorting this out. 5) Experiments: As I previously mentioned, I don't think the correct baselines are compared to. Instead of posing the method against other autotuning schemes, just one choice of the number of bins is taken. This already makes it near impossible to judge the efficacy of this method. Despite this, even the data presented does not make a clear case for ECE_sweep. In Fig. 4 we see that the bias of EW_sweep is even worse than EW. This already means that the sweep estimate doesn't fix the issues of ECE_bin in all contexts. It is the case that EM_sweep has better bias than EM, but again, for samples large enough for the variances to be in control, it seems like these numbers are both converging to the same, so I don't see any distinct advantage when it comes to estimation. (of course, this is moot because this isn't the right comparison anyway) Also, Fig. 5 is flawed because it compares EW and EM_sweep. It should either compare EM and EM_sweep, or EW and EW_sweep, I don't see why EW and EM_sweep are directly comparable. Minor issues: a) Algorithm (1) and the formula for ECE_sweep in section 4 don't compute the same thing. In algorithm (1), you find the largest b such that the resulting y ― k is a monotone sequence, and return the ECE_bin for this number of bins. In the formula, you maximise the ECE_bin for all b that yield a monotone y ― k . From the preceding text, I assumed that the quantity in Algorithm (1) is intended. b) Why is the L p norm definition of the ECEs introduced at all? In the paper only p = 2 is used throughout. I feel like the p just complicates things without adding much - even if you only present the L 2 definition, the fact that a generic p can be used instead should be obvious to the audience. c) Design considerations for ECE_sweep - it is worth noting that accuracy is not all that we want in an estimate of calibration error. For instance, one might reasonably want to add this as a regulariser when training a model in order to obtain better calibrated solutions. One issue with ECE_sweep is that it introduces a problem in that how the number of bins in the ECE_sweep estimate changes with a small change in model parameters seems very difficult to handle, which makes this a nondifferentiable loss. Broader issues of this form, and a discussion of how they may be mitigated, could lead to a more well rounded paper. Comments: a) Exact monotonicity in the ECE_sweep proposal - I find the argument stemming from the monotonicity of the true calibration curve, and the idea to use this to nail down a maximum binning size interesting. However, why should we demand exact monotonicity in the bin heights? Each y ― k will have noise at the scale of roughly b / n , (for equal mass binning with b bins), and in my opinion, violation of monotonicity at this scale should not be penalised. Also, what if a few y ― k s decrease but most are increasing (i.e., the sequence has a few falling regions, but the bulk is increasing)? Perhaps instead of dealing with this crudely, the error of a shape constrained estimator may serve as a better proxy. b) Isn't the procedure for parametrically fitting the pdf of f , and E [ Y f ( X ) ] , and then integrating the bias a completely different estimator for TCE of a model? In fact, if the laws are a good fit, as is claimed in section 5.1, then this plug in estimator might do well simply because the integration is exact. In fact, since the fit is parametric, this can further be automatically differentiated (if, say, f were a DNN), and thus used to train. c) It would be interesting to see what number of bins are ultimately adopted in the ECE_sweep computations that are performed. Overall opinion: The lack of comparison to appropriate baselines makes it near impossible for me to judge the validity of the proposed estimator. I feel like this is a deep methodological flaw when it comes to evaluating the main proposal of the paper. This is a real pity because I quite like some of the ideas in the paper. Due to the inability to evaluate the main contribution of the paper, i am rating it a strong reject. I'd be completely open to re-rating it if appropriate comparisons are performed, and the case for the method is properly made.
==================================================

Focused review:

Weaknesses: 1. The literature review related to NAS should be enriched. 2. The explanation of formulae should be improved. For example, what does
∘
mean in the definition of
F
N
(
x
0
;
θ
)
? 3. The strategy for selecting weights in Section 4.2 is described as shallow. 4. Is the selection of the evolutionary architecture search algorithm a limitation of your work? In other words, can any other search algorithms work? 5. How do you define the network search space? 6. Testing data could be enriched. 7. More methods should be chosen to testify to your method, especially the latest ones. 8. Reference should be updated. Most of the cited literature in the current version was published before 2020, which would confuse the readers that your research topic is not essential anymore.


Review Point: 1. The literature review related to NAS should be enriched.
Review Point: 2. The explanation of formulae should be improved. For example, what does ∘ mean in the definition of F N ( x 0 ; θ ) ?
Review Point: 3. The strategy for selecting weights in Section 4.2 is described as shallow.
Review Point: 4. Is the selection of the evolutionary architecture search algorithm a limitation of your work? In other words, can any other search algorithms work?
Review Point: 7. More methods should be chosen to testify to your method, especially the latest ones.
Review Point: 8. Reference should be updated. Most of the cited literature in the current version was published before 2020, which would confuse the readers that your research topic is not essential anymore.
==================================================

Focused review:

1. I think the conclusion "the Lipschitz constant is tightly and inversely related to the loss" is somehow trivial. Could the authors explain to us whether there are no theoretical explanations before, or you just use some novel methods to reach this conclusion? What are the differences between your conclusion and the others (e.g. different settings)? 2. Since two types of problems are considered here, could the authors further explain some more connections between these two problems except for their similar forms? 3. I think the problems this paper tries to solve are not motivated very well. Some more discussions could be added to better show the importance of this problem.

Review Point: 1. I think the conclusion "the Lipschitz constant is tightly and inversely related to the loss" is somehow trivial. Could the authors explain to us whether there are no theoretical explanations before, or you just use some novel methods to reach this conclusion? What are the differences between your conclusion and the others (e.g. different settings)?
Review Point: 2. Since two types of problems are considered here, could the authors further explain some more connections between these two problems except for their similar forms?
Review Point: 3. I think the problems this paper tries to solve are not motivated very well. Some more discussions could be added to better show the importance of this problem.
==================================================

Focused review:

Weakness:
The proposed method is more like a surprisingly effective hack/trick instead of a systematic technique. The whole paper boils down to one sentence "Let's think step by step". While the paper has well demonstrated the effectiveness of this trick, it can be strengthened by answering a few related questions: 1) how to find the optimal "template"? Is "let's think step by step" happen to work or the model really understands it? 2) What's the mechanism behind this? The template does not provide any extra information and why it can trigger the LLM?
2 The full pipeline in Figure 2 has two steps which I am not sure if it is necessary. I have tried a few examples using the Open AI play ground and it seems that by appending "Let's think step by step", the model naturally generates the final answer "375".


Review Point: The proposed method is more like a surprisingly effective hack/trick instead of a systematic technique. The whole paper boils down to one sentence "Let's think step by step". While the paper has well demonstrated the effectiveness of this trick, it can be strengthened by answering a few related questions:
Review Point: 1) how to find the optimal "template"? Is "let's think step by step" happen to work or the model really understands it?
Review Point: 2) What's the mechanism behind this? The template does not provide any extra information and why it can trigger the LLM?
Review Point: 2 The full pipeline in Figure 2 has two steps which I am not sure if it is necessary. I have tried a few examples using the Open AI play ground and it seems that by appending "Let's think step by step", the model naturally generates the final answer "375".
==================================================

Focused review:

+ In the constrained case, the use of precond OGD seems essential -- one explores in the ellipsoid of the hessian of a self-concord barrier, which could be quite large in some directions. However, here one can readily play points outside the constraint set. Is the use of preconditioned GD necessary? Note that in context of the previous works it was possible to constrain the disturbance-action class solely in terms of a single frobenius norm, if it is so desired, without degradation of expressivity or the associated regret bound. + Usually in control settings, the reward/cost (even if changing) is often explicitly available, because the algorithm designer herself chooses the cost as a proxy for description of the optimal behavior. At the outset, the reviewer believes, the setting here is a natural extension of the recently studied questions in the community, and completely defensible because research in theory is often forward-looking. Yet, this raises some question on how broadly applicable the setting/results are.

Review Point: + In the constrained case, the use of precond OGD seems essential -- one explores in the ellipsoid of the hessian of a self-concord barrier, which could be quite large in some directions. However, here one can readily play points outside the constraint set. Is the use of preconditioned GD necessary? Note that in context of the previous works it was possible to constrain the disturbance-action class solely in terms of a single frobenius norm, if it is so desired, without degradation of expressivity or the associated regret bound.
Review Point: + Usually in control settings, the reward/cost (even if changing) is often explicitly available, because the algorithm designer herself chooses the cost as a proxy for description of the optimal behavior. At the outset, the reviewer believes, the setting here is a natural extension of the recently studied questions in the community, and completely defensible because research in theory is often forward-looking. Yet, this raises some question on how broadly applicable the setting/results are.
==================================================

Focused review:

1. While this paper describes a method to derive activation functions from kernels, it seems not covering much work on kernel feature expansion (e.g. random Fourier features). Given that the proposed approach is also based on the Fourier duality, it would be good to claim further differences between performing it at a feature level and performing it at an activation function level. 2. Although I appreciate this paper has included various tasks to show the goodness of approximation, some of the experiments can still be tuned to give better illustrations. For instance, a) to include methods based on kernel feature expansion as discussed above. b) both figure 1 and figure 2 shows the approximated distributions, it should be possible to perform quantitive comparisons on such toy datasets (e.g. compute divergence/discrepancy). It seems the approximation depends on the number of hidden units as well as the number of Monto-Carlo samples. c) For later out-of-distribution detection task, it would also be better if some results from the original GPs are included (maybe not on a huge image dataset considering computational costs).

Review Point: 1. While this paper describes a method to derive activation functions from kernels, it seems not covering much work on kernel feature expansion (e.g. random Fourier features). Given that the proposed approach is also based on the Fourier duality, it would be good to claim further differences between performing it at a feature level and performing it at an activation function level.
Review Point: 2. Although I appreciate this paper has included various tasks to show the goodness of approximation, some of the experiments can still be tuned to give better illustrations. For instance, a) to include methods based on kernel feature expansion as discussed above. b) both figure 1 and figure 2 shows the approximated distributions, it should be possible to perform quantitive comparisons on such toy datasets (e.g. compute divergence/discrepancy). It seems the approximation depends on the number of hidden units as well as the number of Monto-Carlo samples. c) For later out-of-distribution detection task, it would also be better if some results from the original GPs are included (maybe not on a huge image dataset considering computational costs).
==================================================

Focused review:


Here attach the comments about addressing my previous review.  A1: Would you give some suggestions based on current results about which part can be discarded first if computational efforts are constrained?
A2: "We believe any functions that first rise then decrease can also obtain similar results" -> Do you have any other instantiation? Also, if this argument can be clarified, this sentence should be added to the paper.
A3:  Based on the explanation, it seems that ND cannot be adapted to beam search, as it cannot achieve better performance. Thus, the discussions from Line494 may be problematic.
Overall, the revision towards my previous review is not clear and the arguments provided in the responses are not supported by sufficient discussion or experimental results. I would still keep my score to be 3. 

Review Point: A1: Would you give some suggestions based on current results about which part can be discarded first if computational efforts are constrained?
Review Point: A2: "We believe any functions that first rise then decrease can also obtain similar results" -> Do you have any other instantiation? Also, if this argument can be clarified, this sentence should be added to the paper.
Review Point: A3: Based on the explanation, it seems that ND cannot be adapted to beam search, as it cannot achieve better performance. Thus, the discussions from Line494 may be problematic. Overall, the revision towards my previous review is not clear and the arguments provided in the responses are not supported by sufficient discussion or experimental results. I would still keep my score to be 3.
==================================================

Focused review:

It is not clear if the initialization in Section 3.1 is realistic or not (i.e., is it similar to something that is used in practice). What is a concrete example of an initialization that satisfies the conditions mentioned in this section? Some of the notations and statements are not clear: 1. Lemma 4.1 part 4 does not seem to be used in the proof of Theorem 3.2. Is it used? The convergence to the global minimum follows by the convergence to 0 loss and not using Lemma 4.1 part 4. Is this correct? 2. After line 92, does lambda_F^2 and lambda_F^3 correspond to lambda_F to the power of 2 and 3? If so, then why are both assumptions needed (both give lower bounds on lambda_F)? 3. Can gamma be equal to 0 in equation (2), such that we get the ReLU activation and not Leaky ReLU?

Review Point: It is not clear if the initialization in Section 3.1 is realistic or not (i.e., is it similar to something that is used in practice). What is a concrete example of an initialization that satisfies the conditions mentioned in this section? Some of the notations and statements are not clear:
Review Point: 1. Lemma 4.1 part 4 does not seem to be used in the proof of Theorem 3.2. Is it used? The convergence to the global minimum follows by the convergence to 0 loss and not using Lemma 4.1 part 4. Is this correct?
Review Point: 2. After line 92, does lambda_F^2 and lambda_F^3 correspond to lambda_F to the power of 2 and 3? If so, then why are both assumptions needed (both give lower bounds on lambda_F)?
Review Point: 3. Can gamma be equal to 0 in equation (2), such that we get the ReLU activation and not Leaky ReLU?
==================================================

Focused review:

1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity. 
2) Lacking strong baselines both on entity-level factuality evaluation and summarization. 
1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced? 
2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated? 
3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement? 
4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7). 

Review Point: 1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity.
Review Point: 2) Lacking strong baselines both on entity-level factuality evaluation and summarization.
Review Point: 1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced?
Review Point: 2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated?
Review Point: 3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement?
Review Point: 4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7).
==================================================

Focused review:

Weakness:
Some phenomena can be further explained for the reader to get better understanding. For example, Why cannot the “more descriptive” BLIP-generated captions beat the performance of COCO dataset?
The paper evaluated the performance of representation learning only with classification-based transfer performance, while some other tasks can also be used to evaluate the capability of representations, i.e. image-text matching, and image textual grounding. The experiments from multiple different kinds of tasks can better support your claim.
Some experiments can be more completed. i.e. CLIP_s 2 and 10 in the left table of Fig. 5.
The proposed data augmentation methods are based on BLIP model which is also a supervised trained with large scale datasets. I doubt if the learned knowledge will be leaked during the data augmentation process especially in the image captioning process mentioned in Section 3.3. This can be view as a kind of knowledge distillation of BLIP model which makes it less supportive for the claim.


Review Point: Some phenomena can be further explained for the reader to get better understanding. For example, Why cannot the “more descriptive” BLIP-generated captions beat the performance of COCO dataset? The paper evaluated the performance of representation learning only with classification-based transfer performance, while some other tasks can also be used to evaluate the capability of representations, i.e. image-text matching, and image textual grounding. The experiments from multiple different kinds of tasks can better support your claim. Some experiments can be more completed. i.e. CLIP_s 2 and 10 in the left table of Fig.
Review Point: 5. The proposed data augmentation methods are based on BLIP model which is also a supervised trained with large scale datasets. I doubt if the learned knowledge will be leaked during the data augmentation process especially in the image captioning process mentioned in Section 3.3. This can be view as a kind of knowledge distillation of BLIP model which makes it less supportive for the claim.
==================================================

Focused review:

In my opinion, the paper is very good, but it still has two main weaknesses that slightly hinder its significance: - the accessibility for a more general ML audience and the fact that a lot of the content (including some evaluation) is in the Appendix - not enough emphasis on the strong assumptions, e.g. knowing the causal graph On the other hand, I can understand the first point is difficult to fix, also given the paper would probably fit better a journal in order to be properly self contained.

Review Point: In my opinion, the paper is very good, but it still has two main weaknesses that slightly hinder its significance:
Review Point: - the accessibility for a more general ML audience and the fact that a lot of the content (including some evaluation) is in the Appendix - not enough emphasis on the strong assumptions, e.g. knowing the causal graph On the other hand, I can understand the first point is difficult to fix, also given the paper would probably fit better a journal in order to be properly self contained.
==================================================

Focused review:

1. Although authors integrate many components such as the memory bank updating and the confidence threshold, the main ideas of this paper, Neighborhood Clustering (NC) and entropy separation (ES), seem to be very straightforward. NC is not new [1]. [1] Li S, Chen D, Liu B, et al. Memory-based neighbourhood embedding for visual recognition[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 6102-6111. 2. Will the memory bank F and V bring in lots of additional memory cost? If so, it would largely reduce the practicability of the proposed algorithm. Needs more explanation and discussions. 3. Missing reference ID in Table2-6.

Review Point: 1. Although authors integrate many components such as the memory bank updating and the confidence threshold, the main ideas of this paper, Neighborhood Clustering (NC) and entropy separation (ES), seem to be very straightforward. NC is not new [1]. [1] Li S, Chen D, Liu B, et al. Memory-based neighbourhood embedding for visual recognition[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 6102-6111.
Review Point: 2. Will the memory bank F and V bring in lots of additional memory cost? If so, it would largely reduce the practicability of the proposed algorithm. Needs more explanation and discussions.
==================================================

Focused review:

- Task-related head looks the same as the cross-attention structure in Transformer Decoder[61], and a similar structure is mentioned in Cait[57], so this contribution is optional. - Spatial-filling curve does not seem to improve the performance, and the SIS mode stated in the paper is equivalent to the current token embedding in ViT. - EAT increases by 0.5 points on average compared to the baseline in Table 1, but the reported highest Top-1 is 82.4 that is lower than current methods, e.g., SwinTransformer[36] and Cait[57]. Authors should compare with these methods or explain why it is not being compared. - Why do authors claim that "... so that only 1D operators are required" in Figure 3? Adding 2D operations, such as 2D-convolution in CPVT[16], CeiT[70], and PVTv2[a], has been shown to improve the performance of Transformer in image classification tasks. - Grammatical errors and typo issues lower the paper quality that can be seen in the following section.  
[a] Wang W, Xie E, Li X, et al. PVTv2:
Moreover, Here are more suggestions.
- Some quantitative results in Appendix E should be presented in Table 1 - Why does DCN dramatically damage performance in Table 4? - Figure 6 should be rearranged - How to integrate CV and NLP features is unclear in multi-modal experiments, and more details should be displayed. - Some grammar/typo issues.     - L12, L51: multi-modal      - L102: their ways     - L116: the definition of "h"     - L161: the weight     - L174: concepts     - L314: remove the comma after "format"


Review Point: - Task-related head looks the same as the cross-attention structure in Transformer Decoder[61], and a similar structure is mentioned in Cait[57], so this contribution is optional.
Review Point: - Spatial-filling curve does not seem to improve the performance, and the SIS mode stated in the paper is equivalent to the current token embedding in ViT.
Review Point: - EAT increases by 0.5 points on average compared to the baseline in Table 1, but the reported highest Top-1 is 82.4 that is lower than current methods, e.g., SwinTransformer[36] and Cait[57]. Authors should compare with these methods or explain why it is not being compared.
Review Point: - Why do authors claim that "... so that only 1D operators are required" in Figure 3? Adding 2D operations, such as 2D-convolution in CPVT[16], CeiT[70], and PVTv2[a], has been shown to improve the performance of Transformer in image classification tasks.
Review Point: - Grammatical errors and typo issues lower the paper quality that can be seen in the following section. [a] Wang W, Xie E, Li X, et al.
Review Point: - Some quantitative results in Appendix E should be presented in Table 1 - Why does DCN dramatically damage performance in Table 4?
Review Point: - Figure 6 should be rearranged - How to integrate CV and NLP features is unclear in multi-modal experiments, and more details should be displayed.
Review Point: - Some grammar/typo issues.- L12, L51: multi-modal - L102: their ways - L116: the definition of "h" - L161: the weight - L174: concepts - L314: remove the comma after "format"
==================================================

Focused review:

Theoretical grounding: - I'm not confident Assumption 5 is always satisfied; I think for DNNs on common datasets, it's nontrivial to find a baseline that is actually noninformative / unimportant. Would it be possible to test how well this is satisfied? - In the experiments, it's necessary to choose an interaction strength _threshold_ for combining pairwise interactions (e.g. top and bottom 10%). This choice seems critical to any user-facing visualization, especially since realistic models will probably have nonzero interaction strengths for all pairs. Although there are some results in Appendix I showing how quantitative results vary with the threshold, the paper does not discuss how to meaningfully choose this value. - I'm not 100% convinced that meaningful higher-order interactions will always be detected by merging pairwise interactions (though perhaps this is covered by Assumption 5). As a concrete example, imagine a function in R^40 like in the synthetic examples where x* is [1,1,...,1], x' is [-1,-1,...,-1], and there is a component in the function that takes a subset of dimensions (say the first 10) and returns 1 if and only if exactly half of those 10 dimensions have positive values. In that case, there is a meaningful interaction between those 10 dimensions, but I don't think it would be returned by the method. It would be great if you could either explain how I'm incorrect or how this example violates the method's explicitly stated assumptions. Empirical evaluation: - The paper introduces two new methods: ArchDetect (for finding sets of interacting features), and ArchAttribute (for computing feature set attributions), which together seem to result in compelling performance. Ironically, though, the paper doesn't make it clear whether this performance improvement is primarily because of ArchDetect, ArchAttribute, or their interaction :) - To fix this, I would strongly recommend showing results for ArchAttribute with a detection method other than ArchDetect (e.g. using Hessian components or Shapley interaction indices as pairwise interaction scores and then merging them using the same thresholding scheme). That way, we can better understand the source of the strength of the method. - It would be interesting to see if Hessians still perform poorly even when explaining many-times differentiable networks, e.g. those with softplus rather than ReLU activations.

Review Point: Theoretical grounding:- I'm not confident Assumption 5 is always satisfied; I think for DNNs on common datasets, it's nontrivial to find a baseline that is actually noninformative / unimportant. Would it be possible to test how well this is satisfied?
Review Point: - In the experiments, it's necessary to choose an interaction strength _threshold_ for combining pairwise interactions (e.g. top and bottom 10%). This choice seems critical to any user-facing visualization, especially since realistic models will probably have nonzero interaction strengths for all pairs. Although there are some results in Appendix I showing how quantitative results vary with the threshold, the paper does not discuss how to meaningfully choose this value.
Review Point: - I'm not 100% convinced that meaningful higher-order interactions will always be detected by merging pairwise interactions (though perhaps this is covered by Assumption 5). As a concrete example, imagine a function in R^40 like in the synthetic examples where x* is [1,1,...,1], x' is [-1,-1,...,-1], and there is a component in the function that takes a subset of dimensions (say the first 10) and returns 1 if and only if exactly half of those 10 dimensions have positive values. In that case, there is a meaningful interaction between those 10 dimensions, but I don't think it would be returned by the method. It would be great if you could either explain how I'm incorrect or how this example violates the method's explicitly stated assumptions. Empirical evaluation:
Review Point: - The paper introduces two new methods: ArchDetect (for finding sets of interacting features), and ArchAttribute (for computing feature set attributions), which together seem to result in compelling performance. Ironically, though, the paper doesn't make it clear whether this performance improvement is primarily because of ArchDetect, ArchAttribute, or their interaction :) - To fix this, I would strongly recommend showing results for ArchAttribute with a detection method other than ArchDetect (e.g. using Hessian components or Shapley interaction indices as pairwise interaction scores and then merging them using the same thresholding scheme). That way, we can better understand the source of the strength of the method. - It would be interesting to see if Hessians still perform poorly even when explaining many-times differentiable networks, e.g. those with softplus rather than ReLU activations.
==================================================

Focused review:

- The validation of the interpretability aspect of the proposed method seems rather preliminary. Quantitative validations or comparison with other relevant methods are absent. For example, evaluating the feature importace values in synthetic experiments where the underlying true feature ranking is known could be added e.g., Fig.4 in [1] might be a good option. Since the proposed method can also capture feature interaction, designing a similar synthetic scenario where the importance of pairwise interactions are known might also be worthwhile. - The advantages of the proposed method over the most relevant prior work [1] still remains unclear. It would be nice if the authors could include an example or an experiment in which some aspects of the two methods are compared in concrete terms. For example, a comparison of the two methods in the prognosis prediction task for breast cancer (as done in [1]) may be informative. [1] demystifying black-box models with symbolic metamodels, NeurIPS 2019

Review Point: - The validation of the interpretability aspect of the proposed method seems rather preliminary. Quantitative validations or comparison with other relevant methods are absent. For example, evaluating the feature importace values in synthetic experiments where the underlying true feature ranking is known could be added e.g., Fig.4 in [1] might be a good option. Since the proposed method can also capture feature interaction, designing a similar synthetic scenario where the importance of pairwise interactions are known might also be worthwhile.
Review Point: - The advantages of the proposed method over the most relevant prior work [1] still remains unclear. It would be nice if the authors could include an example or an experiment in which some aspects of the two methods are compared in concrete terms. For example, a comparison of the two methods in the prognosis prediction task for breast cancer (as done in [1]) may be informative. [1] demystifying black-box models with symbolic metamodels, NeurIPS 2019
==================================================

Focused review:

Some of the concerns are below: 1. The claims of proposing a 'generalized' interpolation may be too strong. What could be the real cases which cannot be resolved by the proposed methods should also be discussed. I believe exposure setting could be only one of the problem of the poor generalization ability. 2. It's unclear to me how equation(2) in line 103 is derived. And in line 109, how equ(4) can be degraded to equ(1) given using different frames? 3. For the restoration network, it seems the network is just trying to achieve multi-frame deblurring with a two-stage process. What exactly the function of the two-stage network? Could the author show some outputs from different stages? In line 154, the improvement in dB can only come from more parameters in the model, but not the intuitive idea illustrated in the paper. Please double check it or visualize it. 4. Also for the restoration network, in line 147, what is the temporal ambiguity, and why the authors utilize four frames but not just 2 frame? In line 143, does the author mean B1 and B2? 5. More results on real videos should be reported. Currently, the results are reported on synthetic data and most previous methods do not share the same assumptions as this paper. Results on real videos in the supplementary material look fine, but related contents are missing in the paper, especially user study.

Review Point: 1. The claims of proposing a 'generalized' interpolation may be too strong. What could be the real cases which cannot be resolved by the proposed methods should also be discussed. I believe exposure setting could be only one of the problem of the poor generalization ability.
Review Point: 2. It's unclear to me how equation(2) in line 103 is derived. And in line 109, how equ(4) can be degraded to equ(1) given using different frames?
Review Point: 3. For the restoration network, it seems the network is just trying to achieve multi-frame deblurring with a two-stage process. What exactly the function of the two-stage network? Could the author show some outputs from different stages? In line 154, the improvement in dB can only come from more parameters in the model, but not the intuitive idea illustrated in the paper. Please double check it or visualize it.
Review Point: 4. Also for the restoration network, in line 147, what is the temporal ambiguity, and why the authors utilize four frames but not just 2 frame? In line 143, does the author mean B1 and B2?
Review Point: 5. More results on real videos should be reported. Currently, the results are reported on synthetic data and most previous methods do not share the same assumptions as this paper. Results on real videos in the supplementary material look fine, but related contents are missing in the paper, especially user study.
==================================================

Focused review:

weakness is serious enough to reject the paper, as the authors fail to properly position their contribution with respect to their natural field.  From the relative entropy or entropy-regularized perspective, the contribution of the authors, whose technical part is located in Eqs. (5), (6) and (7) could be described as performing entropy-regularized RL, but using an average over the n=5 previous policies rather than just the previous one. This characterization is much simpler than the one they propose, and if it happens to be too simplified, the authors could explain what are the differences and why they matter.  By the way, from the above perspective, the formal writing of the expectation in (5), (6) and (7) is partly wrong or at least lacking rigor. Again, since this is the main contribution, a much clearer and rigorous description is mandatory.  Also, the fact that using relative entropy improves exploration is now well-known, thus the empirical results are not surprising to me at all. An empirical comparison between the authors framework and other entropy-regularized methods could have delivered much more interesting messages.  Other weaknesses contribute secundarily to my negative evaluation about this work:  - Section 3.3 and 3.4 are full of un-principled tricks and hyper-parameters - the influence of those tricks and hyper-parameters is not properly studied - results in Fig.5 contradict the literature: the authors found that DDPG with OU noise outperforms DDPP with parameter noise, whereas several papers have found the contrary. I'm afraid the empirical study lacks a lot of evaluation rigor: no number of seeds is specified, statistical significance is not assessed, etc.  In the background section, the paragraphs are too short and not so well written, they do not help much.  Finally, the paper suffers from a few more local formatting or writing problems, but this does not contribute significantly to my negative evaluation: - Subfigs (a) to (c) could be above subfigs (d) to (f). - "sufficient enough" is redundant ;) - l. 48 to 52: two sentences that are quite redundant - l.92: an experience replay => a replay buffer?  typos:  l. 7: preventing => prevents l. 33: its simpliciy => their l. 113: compared with => to l. 129 eq. (4) => Eq. l. 263: of all of the models => of all models  References:  V. et al. Mnih should be V. Mnih et al. Many references suffer from the same problem.  Supplementary: the content of S1 should stay in the main text rather than being in an appendix

Review Point: is serious enough to reject the paper, as the authors fail to properly position their contribution with respect to their natural field. From the relative entropy or entropy-regularized perspective, the contribution of the authors, whose technical part is located in Eqs. (5), (6) and (7) could be described as performing entropy-regularized RL, but using an average over the n=5 previous policies rather than just the previous one. This characterization is much simpler than the one they propose, and if it happens to be too simplified, the authors could explain what are the differences and why they matter. By the way, from the above perspective, the formal writing of the expectation in (5), (6) and (7) is partly wrong or at least lacking rigor. Again, since this is the main contribution, a much clearer and rigorous description is mandatory. Also, the fact that using relative entropy improves exploration is now well-known, thus the empirical results are not surprising to me at all. An empirical comparison between the authors framework and other entropy-regularized methods could have delivered much more interesting messages. Other weaknesses contribute secundarily to my negative evaluation about this work:
Review Point: - Section 3.3 and 3.4 are full of un-principled tricks and hyper-parameters - the influence of those tricks and hyper-parameters is not properly studied - results in Fig.5 contradict the literature: the authors found that DDPG with OU noise outperforms DDPP with parameter noise, whereas several papers have found the contrary. I'm afraid the empirical study lacks a lot of evaluation rigor: no number of seeds is specified, statistical significance is not assessed, etc. In the background section, the paragraphs are too short and not so well written, they do not help much. Finally, the paper suffers from a few more local formatting or writing problems, but this does not contribute significantly to my negative evaluation:
Review Point: - Subfigs (a) to (c) could be above subfigs (d) to (f).
Review Point: - "sufficient enough" is redundant ;) - l. 48 to 52: two sentences that are quite redundant - l.92: an experience replay => a replay buffer? typos: l. 7: preventing => prevents l. 33: its simpliciy => their l. 113: compared with => to l. 129 eq. (4) => Eq. l. 263: of all of the models => of all models References: V. et al. Mnih should be V. Mnih et al. Many references suffer from the same problem. Supplementary: the content of S1 should stay in the main text rather than being in an appendix
==================================================

Focused review:

. There are relatively few experiments; with such a new idea, it is important to validate that it is not simply these datasets or even these tasks that contain this kind of structure. Since there are many tasks at all of these timescales, the lack of further results is somewhat disappointing. 2. The authors do not fully describe the various hypotheses they imply. I wish not to be too harsh on this point, because it is endemic in NLP and much of ML. However, in this case it is especially important, because the authors are introducing a number of hypotheses: (a) Unsupervised neural representations from BERT can be disentangled into constituent representations that stand for different time scales. (b) This disentanglement can happen at the level of the *neuron*. Note that (a) does not imply this, because it could be that some transformation of the information that requires global knowledge of the given vector is required for disentanglement, i.e. rotation. (c) The tasks chosen to represent the timescales actually do operate on those timescales.

Review Point: . There are relatively few experiments; with such a new idea, it is important to validate that it is not simply these datasets or even these tasks that contain this kind of structure. Since there are many tasks at all of these timescales, the lack of further results is somewhat disappointing.
Review Point: 2. The authors do not fully describe the various hypotheses they imply. I wish not to be too harsh on this point, because it is endemic in NLP and much of ML. However, in this case it is especially important, because the authors are introducing a number of hypotheses: (a) Unsupervised neural representations from BERT can be disentangled into constituent representations that stand for different time scales. (b) This disentanglement can happen at the level of the *neuron*. Note that (a) does not imply this, because it could be that some transformation of the information that requires global knowledge of the given vector is required for disentanglement, i.e. rotation. (c) The tasks chosen to represent the timescales actually do operate on those timescales.
==================================================

Focused review:

Weakness:
Fig. 2(c) is unclear. The caption says the step (c) collects data using the model, however, the action
a
is sent back to the environment. I assume this means interaction with the real environment also happens to collect data. So is the model only used for value estimation in TD-learning? If so, since TD-learning only requires the value of the next state, why are three steps plotted here? Please clearly indicate whether each
r
,
a
and
z
are from the prediction model or the rollout trajectories.
What is the policy gradient? It seems a majority of the model updates in phase 2 and 3 are using TD-MPC method, however, the policy gradient as an important component in TD-MPC is not clearly explained. Only in Section 2 the paper briefly mentions that ‘The policy … and is optimized to maximize temporally weighted Q-values’. How specifically is the temporally weighted Q-values implemented and what’s the explicit form of the policy gradient?
Why
π
at the seeding phase and
Π
at the third phase? It’s not clear to me why the policies use different notations in the second and third phase. Please explain it in the paragraph.
Lack of experiments. For experiments, the most important baseline methods to compare for the proposed algorithm should be those falling in the category of model-based RL+IL setting as in Table 2. However, only the TD-MPC method is compared in the main results. The MWM and Dreamer-v2 should also be compared in the experiments across three task suites. The paper only reports partial results in Fig. 12. Moreover, given that the results in one (Meta-World) of the two environments show similar performances across three methods, it does not prove the strong performance gain of the proposed method.


Review Point: Fig. 2(c) is unclear. The caption says the step (c) collects data using the model, however, the action a is sent back to the environment. I assume this means interaction with the real environment also happens to collect data. So is the model only used for value estimation in TD-learning? If so, since TD-learning only requires the value of the next state, why are three steps plotted here? Please clearly indicate whether each r , a and z are from the prediction model or the rollout trajectories. What is the policy gradient? It seems a majority of the model updates in phase 2 and 3 are using TD-MPC method, however, the policy gradient as an important component in TD-MPC is not clearly explained. Only in Section 2 the paper briefly mentions that ‘The policy … and is optimized to maximize temporally weighted Q-values’. How specifically is the temporally weighted Q-values implemented and what’s the explicit form of the policy gradient? Why π at the seeding phase and Π at the third phase? It’s not clear to me why the policies use different notations in the second and third phase. Please explain it in the paragraph. Lack of experiments. For experiments, the most important baseline methods to compare for the proposed algorithm should be those falling in the category of model-based RL+IL setting as in Table 2. However, only the TD-MPC method is compared in the main results. The MWM and Dreamer-v2 should also be compared in the experiments across three task suites. The paper only reports partial results in Fig.
Review Point: 12. Moreover, given that the results in one (Meta-World) of the two environments show similar performances across three methods, it does not prove the strong performance gain of the proposed method.
==================================================

Focused review:

weaknesses, 1) For the motion token, the template-search region pair is based on the single frame, while the object trajectory is sequential. To make the trajectory invariant to the cropping, do all bounding boxes in one trajectory perform the same transformation, i.e., based on the cropping of the current frame? This point is somewhat confusing. 2) The fixed sampling interval of the trajectory is set to 15 and 8 for different datasets, is it the same setting for the training and inference phase? Besides, will it make a significant performance fluctuation when increasing or decreasing this value? I think the setting of this value needs to be explained.


Review Point: 1) For the motion token, the template-search region pair is based on the single frame, while the object trajectory is sequential. To make the trajectory invariant to the cropping, do all bounding boxes in one trajectory perform the same transformation, i.e., based on the cropping of the current frame? This point is somewhat confusing.
Review Point: 2) The fixed sampling interval of the trajectory is set to 15 and 8 for different datasets, is it the same setting for the training and inference phase? Besides, will it make a significant performance fluctuation when increasing or decreasing this value? I think the setting of this value needs to be explained.
==================================================

Focused review:

Weaknesses:
The main contribution seems to be the new transformer architecture combining context and spatial branches, resulting to a generic bloc that breaks the complexity. This contribution is mainly presented in the context of mobile segmentation application. I wonder if considering in a more generic way this contribution would implies more impact: 1) As you point with the small experiment on classification, this is very generic and can be used for classification, detection, segmentation (semantic, instances, ...); and 2) blocks are designed for mobile applications (MobileNet blocks) and could be extended to GPU systems with more classical operations without changing the O(WH) complexity. Extending experiments in this way would be interesting with may be more impact for the paper.


Review Point: The main contribution seems to be the new transformer architecture combining context and spatial branches, resulting to a generic bloc that breaks the complexity. This contribution is mainly presented in the context of mobile segmentation application. I wonder if considering in a more generic way this contribution would implies more impact:
Review Point: 1) As you point with the small experiment on classification, this is very generic and can be used for classification, detection, segmentation (semantic, instances, ...); and 2) blocks are designed for mobile applications (MobileNet blocks) and could be extended to GPU systems with more classical operations without changing the O(WH) complexity. Extending experiments in this way would be interesting with may be more impact for the paper.
==================================================

Focused review:

Weakness: 1. The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning. The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.” Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1. 2. The theory is a bit complicated and not easy to follow. 3. The experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.


Review Point: 1. The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning. The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.” Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1.
Review Point: 2. The theory is a bit complicated and not easy to follow.
Review Point: 3. The experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.
==================================================

Focused review:

Weakness.
About the motivation. In Line44-49, the authors claim that the teacher model in offline knowledge distillation does not work well because it has not been optimized for the student model. I can understand that this conclusion is drawn from the observations in Line41-43. However, even in if DML and the other offline knowledge distillation methods, the "teacher model" is trained to minimize the cross-entropy loss between teacher prediction and the labels, and the KL (of feat loss) between teacher prediction and student predictions. In these situations, the teacher model is also not optimized for the student model. To make it easy to be understood, when the teacher in DML minimizes the knowledge distillation loss, it does not optimize the loss for the student, it just optimizes it for itself. Hence, the problem that why an offline teacher works worse is still not clear.
In the proposed SHAKE, the teacher knowledge is firstly transferred to the proxy teacher, and then transferred to the student. Thus, it is similar to the teacher-assistant knowledge distillation (TAKD, AAAI2020)[1]. It will be better to discuss the difference and connection between SHAKE and TAKD.
One of the motivations of SHAKE is to reduce the fine-tuning overhead of the pretrained teacher. Recently, adapter-based methods have been proposed to efficiently fine-tune a pretrained model. It will be better to compare SHAKE with using these adapter methods for teacher fine-tuning.
In the detection experiments in Table 4., SHAKE+Review leads to 0.31 AP improvements than Review, which is not significant. Besides, both KD (logit) and Review are not knowledge distillation methods for object detection. It will be better if results with detection-oriented knowledge distillation methods, such as [2-3].
[1] Improved knowledge distillation via teacher assistant. AAAI2020
[2] Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors.ICLR2020.
[3] General instance distillation for object detection. CVPR2021.
I am happy to increase my rating if my questions are answered in the rebuttal.


Review Point: About the motivation. In Line44-49, the authors claim that the teacher model in offline knowledge distillation does not work well because it has not been optimized for the student model. I can understand that this conclusion is drawn from the observations in Line41-43. However, even in if DML and the other offline knowledge distillation methods, the "teacher model" is trained to minimize the cross-entropy loss between teacher prediction and the labels, and the KL (of feat loss) between teacher prediction and student predictions. In these situations, the teacher model is also not optimized for the student model. To make it easy to be understood, when the teacher in DML minimizes the knowledge distillation loss, it does not optimize the loss for the student, it just optimizes it for itself. Hence, the problem that why an offline teacher works worse is still not clear. In the proposed SHAKE, the teacher knowledge is firstly transferred to the proxy teacher, and then transferred to the student. Thus, it is similar to the teacher-assistant knowledge distillation (TAKD, AAAI2020)[1]. It will be better to discuss the difference and connection between SHAKE and TAKD. One of the motivations of SHAKE is to reduce the fine-tuning overhead of the pretrained teacher. Recently, adapter-based methods have been proposed to efficiently fine-tune a pretrained model. It will be better to compare SHAKE with using these adapter methods for teacher fine-tuning. In the detection experiments in Table 4., SHAKE+Review leads to 0.31 AP improvements than Review, which is not significant. Besides, both KD (logit) and Review are not knowledge distillation methods for object detection. It will be better if results with detection-oriented knowledge distillation methods, such as [2-3]. [1] Improved knowledge distillation via teacher assistant. AAAI2020 [2] Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors.ICLR2020. [3] General instance distillation for object detection.
Review Point: CVPR2021. I am happy to increase my rating if my questions are answered in the rebuttal.
==================================================

Focused review:

1. Questions about the structural causal model 1) I feel that the confounder set C can be interpreted as “object shapes and where to place them”. But I still do not have an intuitive way to interpret the image-specific context representation M. 2) Why is X -> M instead of M -> X? From my understanding, we sample object shapes and their locations to get M. And then later we sample object appearance (e.g., texture, lighting, etc.) to get X. 2. Implementation 1) Since the images in both VOC and COCO have different sizes and ratios, I wonder how the authors construct the confounder set C. 2) Is the segmentation mask X_m (L195) logits or probabilities? 3) I feel a bit confused about Eqn. (3). It seems that W_1 and W_2 are used as projection matrices, reducing the dimension from original spatial size (hw) to the number of class (n). I wonder if this is reasonable. And I think the projected embedding space can be any dimension, not necessarily to be n? 4) Why do the authors choose P(c) to be uniform? Using the actual object frequencies in the dataset to represent P(c) might be better? 3. Experiments 1) For Q1 in Table 1, more details are required. How exactly the segmentation mask is used in the network? What’s the dimension? Is it a soft mask with probability/logit, or a binary mask with one-hot label? What if the author constructed a self-attention mask similar to Eqn. (3)? 2) Since the proposed method requires iterative refinement, I think it should also compare with the Noisy Student training [A1]. For example, after the first time training of the segmentation model, the authors can then use it to generate pseudo labels. And then, use the pseudo labels to re-train the segmentation model. By comparing with this baseline, we can then know if the performance gain comes from causal intervention or simply from the iterative refinement of the segmentation model itself. 3) In Table, it seems that the proposed method has smaller gain when using stronger feature backbone. Does it mean that, stronger network can better handle the context (e.g., effectively exploit its advantage while discard its negative impact)? Reference [A1] Xie et al. CVPR 2020. Self-training with Noisy Student improves ImageNet classification

Review Point: 1. Questions about the structural causal model 1) I feel that the confounder set C can be interpreted as “object shapes and where to place them”. But I still do not have an intuitive way to interpret the image-specific context representation M.
Review Point: 2) Why is X -> M instead of M -> X? From my understanding, we sample object shapes and their locations to get M. And then later we sample object appearance (e.g., texture, lighting, etc.) to get X.
Review Point: 2. Implementation 1) Since the images in both VOC and COCO have different sizes and ratios, I wonder how the authors construct the confounder set C.
Review Point: 2) Is the segmentation mask X_m (L195) logits or probabilities?
Review Point: 3) I feel a bit confused about Eqn. (3). It seems that W_1 and W_2 are used as projection matrices, reducing the dimension from original spatial size (hw) to the number of class (n). I wonder if this is reasonable. And I think the projected embedding space can be any dimension, not necessarily to be n?
Review Point: 4) Why do the authors choose P(c) to be uniform? Using the actual object frequencies in the dataset to represent P(c) might be better?
Review Point: 3. Experiments 1) For Q1 in Table 1, more details are required. How exactly the segmentation mask is used in the network? What’s the dimension? Is it a soft mask with probability/logit, or a binary mask with one-hot label? What if the author constructed a self-attention mask similar to Eqn. (3)?
Review Point: 2) Since the proposed method requires iterative refinement, I think it should also compare with the Noisy Student training [A1]. For example, after the first time training of the segmentation model, the authors can then use it to generate pseudo labels. And then, use the pseudo labels to re-train the segmentation model. By comparing with this baseline, we can then know if the performance gain comes from causal intervention or simply from the iterative refinement of the segmentation model itself.
Review Point: 3) In Table, it seems that the proposed method has smaller gain when using stronger feature backbone. Does it mean that, stronger network can better handle the context (e.g., effectively exploit its advantage while discard its negative impact)? Reference [A1] Xie et al. CVPR 2020. Self-training with Noisy Student improves ImageNet classification
==================================================

Focused review:

Weaknesses
- Relationship to the existing work
I appreciate the authors summarize the relationship to the existing work in Section 4. I understand that the proposed generation procedure is different from the existing ones, but I don't understand why the difference is important. For example, the authors state that "Our STGG framework differs from this line of research since it proposes a new type of graph-based operations for generating the molecular graph", but do not clarify how it is different, and why the difference is important. Such a comparison is important for readers to understand the essence of the proposed method.
- No theoretical guarantee to generate valid molecules
I am not convinced with the mechanism to comply the valence rule, and I wonder if there is a theoretical guarantee that this mechanism can comply the rule or there is a counter-example where this mechanism cannot guarantee it. When constructing a ring, it is desirable that the tail atom has one remaining valence, and the ring closes by adding a residual edge. Is it possible that the tail atom has no remaining valence and the ring cannot be closed? For example, C*=C-C≡C seems not to be rejected by the mechanism, but we cannot close the ring. % This may be a question, rather than weakness. If there is any misunderstanding, please correct it.
- Relationship to the classical VAE+BO approaches
As discussed in Section 5.3, one of the major issues in the plogP optimization task is that unrealistic molecules can optimize the score. I consider there had been an implicit agreement that the optimized molecules should resemble the training data, which leads to the classical molecular optimization method combining VAE trained on the real-world molecules and Bayesian optimization. As far as I am aware of, the method by Kajino [Kajino, 19] achieves the best scores among the methods using this approach. While the proposed method can control the trade-off between the score and realisticness, it seems the proposed method is not better than VAE+BO approaches in this setting.
[Kajino, 19] Molecular Hypergraph Grammar with Its Application to Molecular Optimization, ICML-19.

Given the discussion below, all of my concerns have been addressed.


Review Point: - Relationship to the existing work I appreciate the authors summarize the relationship to the existing work in Section 4. I understand that the proposed generation procedure is different from the existing ones, but I don't understand why the difference is important. For example, the authors state that "Our STGG framework differs from this line of research since it proposes a new type of graph-based operations for generating the molecular graph", but do not clarify how it is different, and why the difference is important. Such a comparison is important for readers to understand the essence of the proposed method.
Review Point: - No theoretical guarantee to generate valid molecules I am not convinced with the mechanism to comply the valence rule, and I wonder if there is a theoretical guarantee that this mechanism can comply the rule or there is a counter-example where this mechanism cannot guarantee it. When constructing a ring, it is desirable that the tail atom has one remaining valence, and the ring closes by adding a residual edge. Is it possible that the tail atom has no remaining valence and the ring cannot be closed? For example, C*=C-C≡C seems not to be rejected by the mechanism, but we cannot close the ring. % This may be a question, rather than weakness. If there is any misunderstanding, please correct it.
Review Point: - Relationship to the classical VAE+BO approaches As discussed in Section 5.3, one of the major issues in the plogP optimization task is that unrealistic molecules can optimize the score. I consider there had been an implicit agreement that the optimized molecules should resemble the training data, which leads to the classical molecular optimization method combining VAE trained on the real-world molecules and Bayesian optimization. As far as I am aware of, the method by Kajino [Kajino, 19] achieves the best scores among the methods using this approach. While the proposed method can control the trade-off between the score and realisticness, it seems the proposed method is not better than VAE+BO approaches in this setting. [Kajino, 19] Molecular Hypergraph Grammar with Its Application to Molecular Optimization, ICML-19. Given the discussion below, all of my concerns have been addressed.
==================================================

Focused review:

weakness is not including baselines that address the overfitting in boosting with heuristics.  Ordered boosting is non-trivial, and it would be good to know how far simpler (heuristic) fixes go towards mitigating the problem.  Overall, I think this paper will spur new research.  As I read it, I easily came up with variations and alternatives that I wanted to see tried and compared.   DETAILED COMMENTS  The paper is already full of content, so the ideas for additional comparisons are really suggestions to consider.  * For both model estimations, why start at example 1?  Why not start   at an example that is 1% of the way into the training data, to help   reduce the risk of high variance estimates for early examples?  * The best alternative I've seen for fixing TS leakage, while reusing   the data sample, uses tools from differential privacy [1, 2].  How   does this compare to Ordered TS?  * Does importance-sampled voting [3] have the same target leakage   problem as gradient boosting?  This algorithm has a similar property   of only using part of the sequence of examples for a given model.   (I was very impressed by this algorithm when I used it; beat random   forests hands down for our situation.)  * How does ordered boosting compare to the subsampling trick mentioned   in l. 150?  * Yes, fixes that involve bagging (e.g., BagBoo [4]) add computational   time, but so does having multiple permuted sequences.  Seems worth a   (future?) comparison.  * Why not consider multiple permutations, and for each, split into   required data subsets to avoid or mitigate leakage?  Seems like it   would have the same computational cost as ordered boosting.  * Recommend checking out the Wilcoxon signed rank test for testing if   two algorithms are significantly different over a range of data   sets.  See [6].  * l. 61: "A categorical feature..."  * l. 73: "for each categorical *value*" ?  * l. 97: For clarity, consider explaining a bit more how novel values   in the test set are handled.  * The approach here reminds me a bit of Dawid's prequential analysis,   e.g., [5].  Could be worth checking those old papers to see if there   is a useful connection.  * l. 129: "we reveal" => "we describe" ?  * l. 131: "called ordered boosting"  * l. 135-137: The "shift" terminology seems less understandable than   talking about biased estimates.  * l. 174: "remind" => "recall" ?  * l. 203-204: "using one tree structure"; do you mean shared \sigma?  * Algorithm 1: only one random permutation?  * l. 237: Don't really understand what is meant by right hand side of   equality.  What is 2^j subscript denoting?  * l. 257: "tunning" => "tuning"  * l. 268: ", what is expected."  This reads awkwardly.  * l. 311: This reference is incomplete.   REFERENCES  [1] https://www.slideshare.net/SessionsEvents/misha-bilenko-principal-researcher-microsoft  [2] https://www.youtube.com/watch?v=7sZeTxIrnxs  [3] Breiman (1999).  Pasting small votes for classification in large databases and on-line.  Machine Learning 36(1):85--103.  [4] Pavlov et al. (2010).  BagBoo: A scalable hybrid bagging-the-boosting model.  In CIKM.  [5] Dawid (1984).  Present position and potential developments: Some personal views: Statistical Theory: The Prequential Approach.  Journal of the Royal Stastical Society, Series A, 147(2).  [6] Demsar (2006).  Statistical comparisons of classifiers over multiple data sets.  Journal of Machine Learning Research, 7:1--30. 

Review Point: is not including baselines that address the overfitting in boosting with heuristics. Ordered boosting is non-trivial, and it would be good to know how far simpler (heuristic) fixes go towards mitigating the problem. Overall, I think this paper will spur new research. As I read it, I easily came up with variations and alternatives that I wanted to see tried and compared. DETAILED COMMENTS The paper is already full of content, so the ideas for additional comparisons are really suggestions to consider.
Review Point: * For both model estimations, why start at example 1? Why not start at an example that is 1% of the way into the training data, to help reduce the risk of high variance estimates for early examples?
Review Point: * The best alternative I've seen for fixing TS leakage, while reusing the data sample, uses tools from differential privacy [1, 2]. How does this compare to Ordered TS?
Review Point: * Does importance-sampled voting [3] have the same target leakage problem as gradient boosting? This algorithm has a similar property of only using part of the sequence of examples for a given model. (I was very impressed by this algorithm when I used it; beat random forests hands down for our situation.) * How does ordered boosting compare to the subsampling trick mentioned in l. 150?
Review Point: * Yes, fixes that involve bagging (e.g., BagBoo [4]) add computational time, but so does having multiple permuted sequences. Seems worth a (future?) comparison.
Review Point: * Why not consider multiple permutations, and for each, split into required data subsets to avoid or mitigate leakage? Seems like it would have the same computational cost as ordered boosting.
Review Point: * Recommend checking out the Wilcoxon signed rank test for testing if two algorithms are significantly different over a range of data sets. See [6].
Review Point: * l.97: For clarity, consider explaining a bit more how novel values in the test set are handled.
Review Point: * The approach here reminds me a bit of Dawid's prequential analysis, e.g., [5]. Could be worth checking those old papers to see if there is a useful connection.
Review Point: * l. 131: "called ordered boosting" * l. 135-137: The "shift" terminology seems less understandable than talking about biased estimates.
Review Point: * l. 203-204: "using one tree structure"; do you mean shared \sigma?
Review Point: * l. 237: Don't really understand what is meant by right hand side of equality. What is 2^j subscript denoting?
Review Point: * l. 257: "tunning" => "tuning" * l. 268: ", what is expected." This reads awkwardly.
Review Point: * l. 311: This reference is incomplete. REFERENCES [1] https://www.slideshare.net/SessionsEvents/misha-bilenko-principal-researcher-microsoft [2] https://www.youtube.com/watch?v=7sZeTxIrnxs [3] Breiman (1999). Pasting small votes for classification in large databases and on-line. Machine Learning 36(1):85--103. [4] Pavlov et al. (2010). BagBoo: A scalable hybrid bagging-the-boosting model. In CIKM. [5] Dawid (1984). Present position and potential developments: Some personal views: Statistical Theory: The Prequential Approach. Journal of the Royal Stastical Society, Series A, 147(2). [6] Demsar (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1--30.
==================================================

Focused review:

Weakness:
1.The paper is not quite easy to follow. It is hard for people who are not familiar with reward free RL to get the main idea of the method.
2.The paper makes the low-rank assumption, but they do not justify whether it is reasonable.
3.The paper does not discuss it's limitation and conclude the paper.
4.It would be better to have a experiment to validate the effectiveness over baselines, even in a tabular game.
5.The collaborative MDP problem setting in the paper restrict the transition functions to be the same across MDPs, so it is equivalent to stating a single MDP with personalized reward function. The paper would have greater impact if it considers a more realistic setting where user transitions are non-deterministic and personalized as well.
Minor issues:
Introduction, first paragraph, "is smaller the number" --> "is smaller than the number";
Since you are borrowing the insight of collaborative filtering, in the motivating example, better briefly introduce the insight of your method. For example, stating that you can borrow the "exploration of similar users and assume that similar users generate similar responses."
Problem setting, 2nd paragraph, "A_h = \pi_h(S_h)" --> "A_h \sim \pi_h(S_h)"
Problem setting, "linear MDP setting" section, better state that \phi represents transition embedding and \psi represents reward embedding when first introduced.
Section 3.2, Phase 2 step 1, "query obtain samples" --> "query/obtain samples"?
An inconsistent notation: in problem setting you use e_i for different embedding dimensions, but in section 6 you state that i represents a user.
Questions to authors:
Main points mentioned in weakness;
Do you have explanation on why linear MDP has total complexity O(N+d)?
Should it be G_h in the last two lines of Algorithm 1 instead of G?


Review Point: 1.The paper is not quite easy to follow. It is hard for people who are not familiar with reward free RL to get the main idea of the method.
Review Point: 2.The paper makes the low-rank assumption, but they do not justify whether it is reasonable.
Review Point: 3.The paper does not discuss it's limitation and conclude the paper.
Review Point: 4.It would be better to have a experiment to validate the effectiveness over baselines, even in a tabular game.
Review Point: 5.The collaborative MDP problem setting in the paper restrict the transition functions to be the same across MDPs, so it is equivalent to stating a single MDP with personalized reward function. The paper would have greater impact if it considers a more realistic setting where user transitions are non-deterministic and personalized as well. Minor issues: Introduction, first paragraph, "is smaller the number" --> "is smaller than the number"; Since you are borrowing the insight of collaborative filtering, in the motivating example, better briefly introduce the insight of your method. For example, stating that you can borrow the "exploration of similar users and assume that similar users generate similar responses." Problem setting, 2nd paragraph, "A_h = \pi_h(S_h)" --> "A_h \sim \pi_h(S_h)" Problem setting, "linear MDP setting" section, better state that \phi represents transition embedding and \psi represents reward embedding when first introduced. Section 3.2, Phase 2 step 1, "query obtain samples" --> "query/obtain samples"? An inconsistent notation: in problem setting you use e_i for different embedding dimensions, but in section 6 you state that i represents a user. Questions to authors: Main points mentioned in weakness; Do you have explanation on why linear MDP has total complexity O(N+d)? Should it be G_h in the last two lines of Algorithm 1 instead of G?
==================================================

Focused review:

- Central parts of the paper are unclear eg. in line 80 \log P_M (X; \theta) should be the negative cross entropy. - The proposed objective Eq. 2 in line 128, requires the optimisation over both the parameters of the transformation \phi and the shared model \theta_S. The effect on the number of parameters vs. prior work eg. AlignFlow (Grover et. al. 2019) has not been discussed clearly. - The paper is sparse in quantitative results and does not compare to important prior work based on GANs [1]. The only quantitative results are on adaptation from USPS to MNIST in line 268. However, prior work [1] achieves 96.5% accuracy in comparison to the 55% accuracy achieved by the proposed method. - The empirical evaluation is restricted to small datasets eg. moons, MNIST and USPS. It would be desirable to evaluate the proposed approach on the more complex Facades/Maps/Cityscapes using the MSE metric to facilitate comparison with AlignFlow and [1]. - The shared model (\theta_s) is trained on two datasets simultaneously. It is unclear how the inductive bias from each of the datasets influence the shared space. [1] CyCADA: Cycle-Consistent Adversarial Domain Adaptation, ICML 2018.

Review Point: - Central parts of the paper are unclear eg. in line 80 \log P_M (X; \theta) should be the negative cross entropy.
Review Point: 2 in line 128, requires the optimisation over both the parameters of the transformation \phi and the shared model \theta_S. The effect on the number of parameters vs. prior work eg. AlignFlow (Grover et. al. 2019) has not been discussed clearly.
Review Point: - The paper is sparse in quantitative results and does not compare to important prior work based on GANs [1]. The only quantitative results are on adaptation from USPS to MNIST in line 268. However, prior work [1] achieves 96.5% accuracy in comparison to the 55% accuracy achieved by the proposed method.
Review Point: - The empirical evaluation is restricted to small datasets eg. moons, MNIST and USPS. It would be desirable to evaluate the proposed approach on the more complex Facades/Maps/Cityscapes using the MSE metric to facilitate comparison with AlignFlow and [1].
Review Point: - The shared model (\theta_s) is trained on two datasets simultaneously. It is unclear how the inductive bias from each of the datasets influence the shared space. [1] CyCADA: Cycle-Consistent Adversarial Domain Adaptation, ICML 2018.
==================================================

Focused review:

Overall, I really liked the experiments in the paper, but some of the analysis could be made more complete. In particular, I had the following questions about the experiments: 1. In the experiments of Section 4.1, a possible explanation for the model's performance could be that it's not due to implicit reasoning but due to the distractor subject leaking the answer: For example, given A mammal has a belly button and A whale eats fish, the model can infer that a whale has a belly button simply by combining these facts instead of doing implicit reasoning. 2. What happens if you do not do "context" dropout at training time (removing certain parts of the context 50% of the times etc.)? 3. One possible explanation for why the hypothesis-only results are poor is because the hypothesis-only conditions are only seen in 20% of the examples. What happens if the model is re-trained with only hypothesis information and labels, and then evaluated in the hypothesis only mode? 4. To isolate the effect of pre-trained representations, why do the authors choose to use a different architecture (ESIM) instead of using RoBERTa with randomly initialized weights? 5. In the counting experiments, is the model really counting? A possible explanation is that the model doesn't really count but just looks at vector space similaries? For e.g. *Mick Jagger is a member of Beatles* is predicted as False, simply because it is far away in representation space from Ringo Starr and John Lennon, and not because the model counts. This could be tested by dropping the quantity fact and assessing performance. (To prevent train/test mismatch the model would need to be re-trained without the quantity fact).

Review Point: Overall, I really liked the experiments in the paper, but some of the analysis could be made more complete. In particular, I had the following questions about the experiments:
Review Point: 1. In the experiments of Section 4.1, a possible explanation for the model's performance could be that it's not due to implicit reasoning but due to the distractor subject leaking the answer: For example, given A mammal has a belly button and A whale eats fish, the model can infer that a whale has a belly button simply by combining these facts instead of doing implicit reasoning.
Review Point: 2. What happens if you do not do "context" dropout at training time (removing certain parts of the context 50% of the times etc.)?
Review Point: 3. One possible explanation for why the hypothesis-only results are poor is because the hypothesis-only conditions are only seen in 20% of the examples. What happens if the model is re-trained with only hypothesis information and labels, and then evaluated in the hypothesis only mode?
Review Point: 4. To isolate the effect of pre-trained representations, why do the authors choose to use a different architecture (ESIM) instead of using RoBERTa with randomly initialized weights?
Review Point: 5. In the counting experiments, is the model really counting? A possible explanation is that the model doesn't really count but just looks at vector space similaries? For e.g. *Mick Jagger is a member of Beatles* is predicted as False, simply because it is far away in representation space from Ringo Starr and John Lennon, and not because the model counts. This could be tested by dropping the quantity fact and assessing performance. (To prevent train/test mismatch the model would need to be re-trained without the quantity fact).
==================================================

Focused review:

Weaknesses:
The paper is not well written. (1) Lots of space wasted on trivial points, like prop.1 that simply says that the function is well defined. Or the list of operations later that doesn't really helps understand the work proposed. Also Prop. 2 is trivial from definition (eq. 1). (2) The main technical part of the paper is glossed over, 3 lines in bullet point 3 in page 6. Everything up to that point was simply replacing an integral with an average. (3) The whole mathematical machinery feels redundant or under-utilized. How is it different then convolutions on a non-uniform grid?
The experimental part should compare to previous works such as Jiang et al "CONVOLUTIONAL NEURAL NETWORKS ON NONUNIFORM GEOMETRICAL SIGNALS USING EUCLIDEAN SPECTRAL TRANSFORMATION"


Review Point: The paper is not well written. (1) Lots of space wasted on trivial points, like prop.1 that simply says that the function is well defined. Or the list of operations later that doesn't really helps understand the work proposed. Also Prop.
Review Point: 2 is trivial from definition (eq. 1). (2) The main technical part of the paper is glossed over, 3 lines in bullet point 3 in page 6. Everything up to that point was simply replacing an integral with an average. (3) The whole mathematical machinery feels redundant or under-utilized. How is it different then convolutions on a non-uniform grid? The experimental part should compare to previous works such as Jiang et al "CONVOLUTIONAL NEURAL NETWORKS ON NONUNIFORM GEOMETRICAL SIGNALS USING EUCLIDEAN SPECTRAL TRANSFORMATION"
==================================================

Focused review:

1) Lack of interpretability: There could be more of a discussion of why "semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers".  This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's  behavior. 

Review Point: 1) Lack of interpretability: There could be more of a discussion of why "semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others.
Review Point: 2) It will be interesting to how this method scales with respect to more complex mathematical questions.
Review Point: 3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior.
==================================================

Focused review:

1. It is hard to adapt existing works with 3D shape generation (like GRASS, PointEdit, SAGNet) to the current setting. 2. Maybe it could be also useful to see timings on network inference. 3. It is very interesting to see what would it be if one makes a slight modification and replaces the 6-DoF pose prediction with 9-DoF, that is augmented with scale vector prediction.

Review Point: 1. It is hard to adapt existing works with 3D shape generation (like GRASS, PointEdit, SAGNet) to the current setting.
Review Point: 2. Maybe it could be also useful to see timings on network inference.
Review Point: 3. It is very interesting to see what would it be if one makes a slight modification and replaces the 6-DoF pose prediction with 9-DoF, that is augmented with scale vector prediction.
==================================================

Focused review:

- I found this paper was trying to pack too much into a single paper: the personalization and the interpretability works are totally orthogonal, and I'd say should have gone into separate papers. - Although this is a minor weakness, the evaluation domains used are fairly simple. Even the Taxi domain that is labeled as "real world", is a toy grid world. So, it's unclear how do the PNT approach scale to actual real world domains. [edit after rebuttal: I just wanted to clarify, that this is more of a criticism on the language used in the paper, than on the experiments per se. I think experiments in small domains are fine, but please do not call them "real world".]

Review Point: - I found this paper was trying to pack too much into a single paper: the personalization and the interpretability works are totally orthogonal, and I'd say should have gone into separate papers.
Review Point: - Although this is a minor weakness, the evaluation domains used are fairly simple. Even the Taxi domain that is labeled as "real world", is a toy grid world. So, it's unclear how do the PNT approach scale to actual real world domains. [edit after rebuttal: I just wanted to clarify, that this is more of a criticism on the language used in the paper, than on the experiments per se. I think experiments in small domains are fine, but please do not call them "real world".]
==================================================

Focused review:

The overall writing is poor. The whole paper lacks logic when organizing their content and is poorly structured, making it difficult to read, especially when they describe their experimental results. It is hard to learn something useful because of the writing. 
Suggestions: 1. Add more figures to clarify the ideas. 
2. Leave the training details (like how many hours it takes to train) in a single section like "implementation details". 
3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result. 

Review Point: The overall writing is poor. The whole paper lacks logic when organizing their content and is poorly structured, making it difficult to read, especially when they describe their experimental results. It is hard to learn something useful because of the writing. Suggestions:
Review Point: 2. Leave the training details (like how many hours it takes to train) in a single section like "implementation details".
Review Point: 3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result.
==================================================

Focused review:

Weakness: 1) In the beginning of the paper, authors often mention that previous works lack the flexibility compared to their work. It is not clear what does it mean and thus makes it harder to understand their explanation. 2) It is not clear regarding the choice of 20 distribution sets. Can we control the number of distribution sets for each class? What if you select only few number of distribution set? 3) The role of Tranfer Matrix T is not discussed or elaborated. 4) It is not clear how to form the target distribution H. How do you formulate H? 5) There is no discussion on how to generate x_H from H and what does x_H constitute of? 6) Despite the significant improvement, it is not clear how this proposed method boost the transferability of the adversarial examples.
As per my understanding, authors briefly addressed the limitations and negative impact in their work.


Review Point: 1) In the beginning of the paper, authors often mention that previous works lack the flexibility compared to their work. It is not clear what does it mean and thus makes it harder to understand their explanation.
Review Point: 2) It is not clear regarding the choice of 20 distribution sets. Can we control the number of distribution sets for each class? What if you select only few number of distribution set?
Review Point: 3) The role of Tranfer Matrix T is not discussed or elaborated.
Review Point: 4) It is not clear how to form the target distribution H. How do you formulate H?
Review Point: 5) There is no discussion on how to generate x_H from H and what does x_H constitute of?
Review Point: 6) Despite the significant improvement, it is not clear how this proposed method boost the transferability of the adversarial examples. As per my understanding, authors briefly addressed the limitations and negative impact in their work.
==================================================

Focused review:

Weaknesses:
The proposed setting and algorithm are novel. In particular, the subtle connection between backdoored networks and sparse adversarial example generation can inspire further research in this direction.
The analysis in the paper is logical, and it guides the reader well through the thought-process behind the algorithm.
The experimental results are comprehensive. Although the experiments do not contain the most recent backdoor detection methods as baselines, this reviewer believes that the current comparison baselines (neural cleanse [5] and DL-TND [6]) are enough to provide the bigger picture. This is since almost all existing methods are typically designed for the white-box scenario, and as such, they have a huge advantage compared to the current method.
The paper is generally well-written, although some parts need proofreading. See the minor comments about this below.
There are two gray areas regarding this submission that needs to be clarified further:
Although the authors talk about the practicality of the black-box scenario in the introduction, this assumption needs to be justified further.
I strongly suggest providing a concrete example, where the use-case of this scenario is explained. In this example, please specify the training data, the model, the user, and why the user who does not own anything from the training data to the model should be worried about backdoors?
2. Another interesting question that is not been explored is the white-box performance of the algorithm.
Currently, the only part where the black-box scenario is being dealt with is the adversarial example generation. Now, assuming that the user has access to the model parameters, what does the current approach provide in contrast to existing methods? Asked the other way around, can someone use the Monte-Carlo gradient estimation in conjunction with existing methods to make them black-box? Since being "black-box" is considered as the strength of the proposed method compared to existing ones, these questions need to be answered.
Further Questions:
Can you clarify what this sentence below Eq. (3) means: "Additional difficulty comes from the fact that..."?
At the bottom of page 4 the paper reads: "Following this, the optimization in Eq. (3) converts to..." I am guessing that you meant only the first part of Eq. (3), right? Otherwise Eq. (4) is missing a
 
 
m
 
 
term.
For the empirical study of Section 3.2, do you solve Eq. (6) in a white-box setting, or using the Monte-Carlo gradient estimator? How about Figure 6, is this figure generated in the black-box or white-box setting?
Does Eq. (7) mean that there are two ways to get the GAP: (1) by generating adversarial perturbations for multiple inputs (as the AVEA does) (2) by generating multiple perturbations for a single input?
Does the ablation study on "the impact of trigger size" mean that a scattered trigger is more likely to circumvent the detector than a sparse trigger?
The results reported in Appendix J indicate that in some cases the performance of the black-box detector is better than the white-box baselines, especially neural cleanse. What do you speculate is the reason behind this?
Minor Comments/Suggestions:
Across the paper, the index "
i
" has been used to point to training samples (Eq. (2)), validation samples (Eq. (3)), and class (Algorithm 1). Consider using a different index for each one of these to avoid misunderstanding.
Consider using different line styles and a bigger plot size for AUROC figures.
Consider adding a table of contents and/or explanation of different parts of the Appendix. Right now there are sudden jumps from one section to the next with no explanations.
In the second sentence after Eq. (15)
q
has been used to point out the first and last dimensions of
X
. The second one needs to be changed to
p
.
Omit the
′
in Eq. (16).
In Eq. (19)
N
b
→
∞
, not
N
.
In the explanations that follow Eq. (24),
K
is used instead of
k
.
In Eq. (27) there is a
S
^
missing in the second
log
.
Figure 13 is colliding with the text.


Review Point: The proposed setting and algorithm are novel. In particular, the subtle connection between backdoored networks and sparse adversarial example generation can inspire further research in this direction. The analysis in the paper is logical, and it guides the reader well through the thought-process behind the algorithm. The experimental results are comprehensive. Although the experiments do not contain the most recent backdoor detection methods as baselines, this reviewer believes that the current comparison baselines (neural cleanse [5] and DL-TND [6]) are enough to provide the bigger picture. This is since almost all existing methods are typically designed for the white-box scenario, and as such, they have a huge advantage compared to the current method. The paper is generally well-written, although some parts need proofreading. See the minor comments about this below. There are two gray areas regarding this submission that needs to be clarified further: Although the authors talk about the practicality of the black-box scenario in the introduction, this assumption needs to be justified further. I strongly suggest providing a concrete example, where the use-case of this scenario is explained. In this example, please specify the training data, the model, the user, and why the user who does not own anything from the training data to the model should be worried about backdoors?
Review Point: 2. Another interesting question that is not been explored is the white-box performance of the algorithm. Currently, the only part where the black-box scenario is being dealt with is the adversarial example generation. Now, assuming that the user has access to the model parameters, what does the current approach provide in contrast to existing methods? Asked the other way around, can someone use the Monte-Carlo gradient estimation in conjunction with existing methods to make them black-box? Since being "black-box" is considered as the strength of the proposed method compared to existing ones, these questions need to be answered. Further Questions: Can you clarify what this sentence below Eq. (3) means: "Additional difficulty comes from the fact that..."? At the bottom of page 4 the paper reads: "Following this, the optimization in Eq. (3) converts to..." I am guessing that you meant only the first part of Eq. (3), right? Otherwise Eq. (4) is missing a m term. For the empirical study of Section 3.2, do you solve Eq. (6) in a white-box setting, or using the Monte-Carlo gradient estimator? How about Figure 6, is this figure generated in the black-box or white-box setting? Does Eq. (7) mean that there are two ways to get the GAP: (1) by generating adversarial perturbations for multiple inputs (as the AVEA does) (2) by generating multiple perturbations for a single input? Does the ablation study on "the impact of trigger size" mean that a scattered trigger is more likely to circumvent the detector than a sparse trigger? The results reported in Appendix J indicate that in some cases the performance of the black-box detector is better than the white-box baselines, especially neural cleanse. What do you speculate is the reason behind this? Minor Comments/Suggestions: Across the paper, the index " i " has been used to point to training samples (Eq. (2)), validation samples (Eq. (3)), and class (Algorithm 1). Consider using a different index for each one of these to avoid misunderstanding. Consider using different line styles and a bigger plot size for AUROC figures. Consider adding a table of contents and/or explanation of different parts of the Appendix. Right now there are sudden jumps from one section to the next with no explanations. In the second sentence after Eq. (15) q has been used to point out the first and last dimensions of X . The second one needs to be changed to p . Omit the ′ in Eq. (16). In Eq. (19) N b → ∞ , not N . In the explanations that follow Eq. (24), K is used instead of k . In Eq. (27) there is a S ^ missing in the second log . Figure 13 is colliding with the text.
==================================================

Focused review:

Weaknesses: - The experiments are on very small datasets and in toy settings. - Some parts of the theory are insufficiently explored. For example, under what scenarios can we expect invertibility of
E
z
[
G
(
z
)
G
(
z
)
T
]
? Perhaps this could be shown to hold in simple settings, e.g.,
G
(
z
)
:=
ReLU
(
W
z
)
with the WDC assumption. Tools from NTK theory could potentially be helpful here, since the entries are of the form
E
z
[
ReLU
(
⟨
w
i
,
z
⟩
)
ReLU
(
⟨
w
j
,
z
⟩
)
]
. - I have some concerns about some of the claimed relevance of this approach to transfer learning. In particular, I was a bit confused by the experimental setup of the generative prior derived from MNIST VAE. What is the broader claim about the relationship between this framework for dictionary learning with generative coefficient priors and transfer learning, and how does this experiment comment on this relationship? Is the claim that dictionary learning with generative priors can be phrased as learning the last linear layer of a generative model?
Clarity: Overall, the paper was fairly well-written and easy to follow in most parts. Here are some typos that I found:
Top of page 2: “atoms, simultaneously” -> “atoms, while simultaneously”
Bottom of page 5: a transpose on
E
[
G
(
z
)
G
(
z
)
]
is missing and a parenthesis on
E
[
A
s
G
(
z
)
−
A
∗
G
(
z
∗
)
G
(
z
∗
)
T
]
as well.
In the appendix, it may be best to use notation that shows
Π
i
=
d
1
W
i
,
+
,
z
’s dependence on
z
, e.g.
W
z
:=
Π
i
=
d
1
W
i
,
+
,
z
In the update rule in Algorithm1, should the projection operator be applied to
A
s
or to
A
s
−
η
g
^
s
?
Novelty and significance: To the reviewer’s knowledge, this work is the first to incorporate generative neural network priors in the dictionary learning setting. In terms of analysis, the theory is a combination of previous work on dictionary learning from Arora et al [1], along with theory from Bora et al [2] and Hand and Voroninski [3]. While new theoretical tools aren’t provided, the combination of these ideas is novel.
Minor comments:
For convergence guarantees of optimization over
z
in compressive sensing or denoising, one may want to cite Huang et al [4] and Heckel et al [5].
Recent work has shown that the logarithmic factor in the WDC can be relaxed to a constant factor [6].
References:
[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Motiga. Simple, effiicent, and neural algorithms for sparse coding. JMLR
[2] Ashish Bora, Ail Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using generative models. ICML
[3] Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by empirical risk. COLT
[4] Wen Huang, Reinhard Heckel, Paul Hand, Vladislav Voroninski. A provably convergent scheme for compressive sensing under random generative priors. Journal of Fourier Analysis and Applications
[5] Reinhard Heckel, Wen Huang, Paul Hand, Vladislav Voroninski. Rate-optimal denoising with deep neural networks. Information and Inference
[6] Constantinos Daskalakis, Dhruv Rohatgi, Manolis Zampetakis. Constant-expansion suffices for compressed sensing with generative priors. NeurIPS


Review Point: - The experiments are on very small datasets and in toy settings.
Review Point: - Some parts of the theory are insufficiently explored. For example, under what scenarios can we expect invertibility of E z [ G ( z ) G ( z ) T ] ? Perhaps this could be shown to hold in simple settings, e.g., G ( z ) := ReLU ( W z ) with the WDC assumption. Tools from NTK theory could potentially be helpful here, since the entries are of the form E z [ ReLU ( ⟨ w i , z ⟩ ) ReLU ( ⟨ w j , z ⟩ ) ] .
Review Point: - I have some concerns about some of the claimed relevance of this approach to transfer learning. In particular, I was a bit confused by the experimental setup of the generative prior derived from MNIST VAE. What is the broader claim about the relationship between this framework for dictionary learning with generative coefficient priors and transfer learning, and how does this experiment comment on this relationship? Is the claim that dictionary learning with generative priors can be phrased as learning the last linear layer of a generative model? Clarity: Overall, the paper was fairly well-written and easy to follow in most parts. Here are some typos that I found: Top of page 2: “atoms, simultaneously” -> “atoms, while simultaneously” Bottom of page 5: a transpose on E [ G ( z ) G ( z ) ] is missing and a parenthesis on E [ A s G ( z ) − A ∗ G ( z ∗ ) G ( z ∗ ) T ] as well. In the appendix, it may be best to use notation that shows Π i = d 1 W i , + , z ’s dependence on z , e.g. W z := Π i = d 1 W i , + , z In the update rule in Algorithm1, should the projection operator be applied to A s or to A s − η g ^ s ? Novelty and significance: To the reviewer’s knowledge, this work is the first to incorporate generative neural network priors in the dictionary learning setting. In terms of analysis, the theory is a combination of previous work on dictionary learning from Arora et al [1], along with theory from Bora et al [2] and Hand and Voroninski [3]. While new theoretical tools aren’t provided, the combination of these ideas is novel. Minor comments: For convergence guarantees of optimization over z in compressive sensing or denoising, one may want to cite Huang et al [4] and Heckel et al [5]. Recent work has shown that the logarithmic factor in the WDC can be relaxed to a constant factor [6]. References: [1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Motiga. Simple, effiicent, and neural algorithms for sparse coding. JMLR [2] Ashish Bora, Ail Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using generative models. ICML [3] Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by empirical risk. COLT [4] Wen Huang, Reinhard Heckel, Paul Hand, Vladislav Voroninski. A provably convergent scheme for compressive sensing under random generative priors. Journal of Fourier Analysis and Applications [5] Reinhard Heckel, Wen Huang, Paul Hand, Vladislav Voroninski. Rate-optimal denoising with deep neural networks. Information and Inference [6] Constantinos Daskalakis, Dhruv Rohatgi, Manolis Zampetakis. Constant-expansion suffices for compressed sensing with generative priors. NeurIPS
==================================================

Focused review:

Weaknesses:
1.The introduction and the abstract can be carefully revised, which is hard to follow. Lots of background is missing, which makes the readers very confused.
2.The experimental results show that the improvement is very small; why? And can you explain it? Could you provide some case studies？
3.Some small typos such as ", Several works also" >, "several works also"


Review Point: 1.The introduction and the abstract can be carefully revised, which is hard to follow. Lots of background is missing, which makes the readers very confused.
Review Point: 2.The experimental results show that the improvement is very small; why? And can you explain it? Could you provide some case studies？ 3.Some small typos such as ", Several works also" >, "several works also"
==================================================

Focused review:

Weaknesses: It looks complicated to assess the practical impact of the paper. On the one hand, the thermodynamic limit and the Gaussianity assumption may be hard to check in practice and it is not straightforward to extrapolate what happens in the finite dimensional case. The idea of identifying the problem's phase transitions is conceptually clear but it is not explicitly specified in the paper how this can help the practitioner. The paper only compares the AMP approach to alternate least squares without mention, for example, positive results obtained in the spectral method literature. Finally, it is not easy to understand if the obtained results only regard the AMP method or generalize to any inference method.     Questions: - Is the analysis restricted to the AMP inference? In other words, could a tensor that is hard to infer via AMP approach be easily identifiable by other methods (or the other way round)?   - Are the easy-hard-impossible phases be related with conditions on the rank of the tensor?  - In the introduction the authors mention the fact that tensor decomposition is in general harder in the symmetric than in the non-symmetric case. How is this connected with recent findings about the `nice' landscape of the objective function associated with the decomposition of symmetric (orthogonal) order-4 tensors [1]?  - The Gaussian assumption looks crucial for the analysis and seems to be guaranteed in the limit r << N. Is this a typical situation in practice? Is always possible to compute the `effective' variance for non-gaussian outputs? Is there a finite-N expansion that characterize the departure from Gaussianity in the non-ideal case? - For the themodynamic limit to hold, should one require N_alpha / N  =  O(1) for all alpha? - Given an observed tensor, is it possible to determine the particular phase it belongs to?  [1] Rong Ge and Tengyu Ma, 2017, On the Optimization Landscape of Tensor Decompositions  

Review Point: It looks complicated to assess the practical impact of the paper. On the one hand, the thermodynamic limit and the Gaussianity assumption may be hard to check in practice and it is not straightforward to extrapolate what happens in the finite dimensional case. The idea of identifying the problem's phase transitions is conceptually clear but it is not explicitly specified in the paper how this can help the practitioner. The paper only compares the AMP approach to alternate least squares without mention, for example, positive results obtained in the spectral method literature. Finally, it is not easy to understand if the obtained results only regard the AMP method or generalize to any inference method. Questions:
Review Point: - Is the analysis restricted to the AMP inference? In other words, could a tensor that is hard to infer via AMP approach be easily identifiable by other methods (or the other way round)?
Review Point: - Are the easy-hard-impossible phases be related with conditions on the rank of the tensor?
Review Point: - In the introduction the authors mention the fact that tensor decomposition is in general harder in the symmetric than in the non-symmetric case. How is this connected with recent findings about the `nice' landscape of the objective function associated with the decomposition of symmetric (orthogonal) order-4 tensors [1]?
Review Point: - The Gaussian assumption looks crucial for the analysis and seems to be guaranteed in the limit r << N. Is this a typical situation in practice? Is always possible to compute the `effective' variance for non-gaussian outputs? Is there a finite-N expansion that characterize the departure from Gaussianity in the non-ideal case?
Review Point: - For the themodynamic limit to hold, should one require N_alpha / N = O(1) for all alpha?
Review Point: - Given an observed tensor, is it possible to determine the particular phase it belongs to? [1] Rong Ge and Tengyu Ma, 2017, On the Optimization Landscape of Tensor Decompositions
==================================================

Focused review:

- Training this method is probably involved and expensive. For example, the chamfer requires reconstructing the full shape S{A,B}^\prime and then finding closest points during training. Since implicit functions are trained point-wise, I am not sure how it is implemented in practice. There is no mention of computational time to train the model. - The results on ShapeNet look nice. I wonder how this would look like for more complex objects, like humans. I would have been much more convinced if there would be a sanity check testing the method on established benchmarks like Faust[4] and DFaust Bogo et al. CVPR'17 where ground truth correspondence is available. Sure those have the same topology but it would validate the approach. - I am doubtful about how useful the approach is. The shapes are scaled and perfectly aligned. It would be good to see the correspondence accuracy of just finding the closest points after aligning the data. Also the unsupervised segmentation network does not require the contributions mentioned here and the performance is already quite good 86.1 vs 88.0. It would be nice to see segmentation results for the baseline as well. ** Post Rebuttal ** Thanks for the rebuttal, it was very helpful. The results on FAUST are appreciated as it shows that the method can work on more complex shapes such as humans and also that the method is robust (to an extent) to noisy scans (FAUST scans are noisy with holes etc.). The authors addressed my main concerns and the results presented in the rebuttal look good. Hence, I'm updating my score.

Review Point: - Training this method is probably involved and expensive. For example, the chamfer requires reconstructing the full shape S{A,B}^\prime and then finding closest points during training. Since implicit functions are trained point-wise, I am not sure how it is implemented in practice. There is no mention of computational time to train the model.
Review Point: - The results on ShapeNet look nice. I wonder how this would look like for more complex objects, like humans. I would have been much more convinced if there would be a sanity check testing the method on established benchmarks like Faust[4] and DFaust Bogo et al. CVPR'17 where ground truth correspondence is available. Sure those have the same topology but it would validate the approach.
Review Point: - I am doubtful about how useful the approach is. The shapes are scaled and perfectly aligned. It would be good to see the correspondence accuracy of just finding the closest points after aligning the data. Also the unsupervised segmentation network does not require the contributions mentioned here and the performance is already quite good 86.1 vs 88.0. It would be nice to see segmentation results for the baseline as well. ** Post Rebuttal ** Thanks for the rebuttal, it was very helpful. The results on FAUST are appreciated as it shows that the method can work on more complex shapes such as humans and also that the method is robust (to an extent) to noisy scans (FAUST scans are noisy with holes etc.). The authors addressed my main concerns and the results presented in the rebuttal look good. Hence, I'm updating my score.
==================================================

Focused review:

* In terms of novelty, note that both the motivation for the model as well as the initial parts of it hold similarities to some prior works. See detailed description in the relation to prior work section. * I would be happy to see results about generalization not only for the CLEVR dataset, but also for natural images datasets where there is larger variance both in the language and visual complexity. There are multiple datasets for generalization in VQA that can be used for that such as CP-VQA and also some splits of GQA. For the CLEVR dataset, the model is basically based on using an object detector to recognize the objects and their properties and build a semantic graph that represents the image. While other approaches that are compared to for this task use object detectors as well, there are many approaches for CLEVR (such as the Neural Module Network, Relation Network, MAC and FiLM) that do not use such strong supervision and therefore the comparison between these approaches in the experimental section is not completely valid. For better comparability, I would be interested to see generalization results when these models are also being fed with at least object-based bounding-boxes representations instead of the earlier commonly used spatial features, as is very common in VQA in the last years (see bottom-up attention networks). * This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.

Review Point: * In terms of novelty, note that both the motivation for the model as well as the initial parts of it hold similarities to some prior works. See detailed description in the relation to prior work section.
Review Point: * I would be happy to see results about generalization not only for the CLEVR dataset, but also for natural images datasets where there is larger variance both in the language and visual complexity. There are multiple datasets for generalization in VQA that can be used for that such as CP-VQA and also some splits of GQA. For the CLEVR dataset, the model is basically based on using an object detector to recognize the objects and their properties and build a semantic graph that represents the image. While other approaches that are compared to for this task use object detectors as well, there are many approaches for CLEVR (such as the Neural Module Network, Relation Network, MAC and FiLM) that do not use such strong supervision and therefore the comparison between these approaches in the experimental section is not completely valid. For better comparability, I would be interested to see generalization results when these models are also being fed with at least object-based bounding-boxes representations instead of the earlier commonly used spatial features, as is very common in VQA in the last years (see bottom-up attention networks).
Review Point: * This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.
==================================================

Focused review:

weaknesses:
1.) Lack of computational cost analysis. According to the implementational details, the overall training process has two steps, which is likely to increase the computational burden. To this end, the authors are suggested to conduct some analysis on this issue.
2.) The comparison experiments with some recent and important methods are missing. For example, the following three papers also focus on Auto-ML based semantic segmentation:
[1] FasterSeg: Searching for Faster Real-time Semantic Segmentation, ICLR 2020;
[2] Graph-guided Architecture Search for Real-time Semantic Segmentation, CVPR 2020;
[3] Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation, CVPR 2019.
As the Auto Seg-Loss is particularly designed for Auto-ML based semantic segmentation, the comparison with the related methods is required.
In addition, there are some other recent semantic segmentation methods (proposed in 2019 and 2020) focusing on the PASCAL VOC and Cityscapes datasets, although they are not related to Auto-ML. As an effective semantic segmentation method, the overall segmentation performance of the model is supposed to be competitive. To this end, the authors are also suggested to make comparisons with these SOTA semantic segmentation methods.
3.) The writing of the abstract needs to be improved. In the current manuscript, the abstract is too brief. To better attract the interests of the readers, the authors can first introduce the background of the problem studied in this paper. Next, some detailed motivations can also be included.


Review Point: 1.) Lack of computational cost analysis. According to the implementational details, the overall training process has two steps, which is likely to increase the computational burden. To this end, the authors are suggested to conduct some analysis on this issue.
Review Point: 2.) The comparison experiments with some recent and important methods are missing. For example, the following three papers also focus on Auto-ML based semantic segmentation: [1] FasterSeg: Searching for Faster Real-time Semantic Segmentation, ICLR 2020; [2] Graph-guided Architecture Search for Real-time Semantic Segmentation, CVPR 2020; [3] Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation, CVPR 2019. As the Auto Seg-Loss is particularly designed for Auto-ML based semantic segmentation, the comparison with the related methods is required. In addition, there are some other recent semantic segmentation methods (proposed in 2019 and 2020) focusing on the PASCAL VOC and Cityscapes datasets, although they are not related to Auto-ML. As an effective semantic segmentation method, the overall segmentation performance of the model is supposed to be competitive. To this end, the authors are also suggested to make comparisons with these SOTA semantic segmentation methods.
Review Point: 3.) The writing of the abstract needs to be improved. In the current manuscript, the abstract is too brief. To better attract the interests of the readers, the authors can first introduce the background of the problem studied in this paper. Next, some detailed motivations can also be included.
==================================================

Focused review:

- one of the motivations for stochastic NFs is that they overcome topological constraints. In the remainder of the paper, such constraints were not carefully formalized nor a reasoning provided for what makes NFs fail in such scenarios and how SNFs are particularly suited to fix these issues. Put differently, I wonder if the shortcomings of RNVPs in Figure 3 can be overcome with other architectures for normalizing flows, such as invertible resnets, iaf, maf etc. - the empirical evaluation in the context of existing works is largely restricted. For example, the schemes in [21, 35] could very well be applied here as well. Similarly, in the setup for MNIST/fashion datasets Table 3, I would have expected the default use implementation of flows for approximating intractable posteriors in a VAE is an inverse autoregressive flow (IAF) but instead the authors choose it to be a Real-NVP. - the unbiased guarantees for estimating expectations are only in the asymptotic limit. I would have been curious to see an analysis of the bias-variance tradeoff in the empirical evaluations. - related work such as A-NICE-MC [35] take special care to ensure that detailed balance is satisfied while using flows as proposals for MCMC. In the current work, it is unclear if such conditions are being satisfied in practice for SNFs.

Review Point: - one of the motivations for stochastic NFs is that they overcome topological constraints. In the remainder of the paper, such constraints were not carefully formalized nor a reasoning provided for what makes NFs fail in such scenarios and how SNFs are particularly suited to fix these issues. Put differently, I wonder if the shortcomings of RNVPs in Figure 3 can be overcome with other architectures for normalizing flows, such as invertible resnets, iaf, maf etc.
Review Point: - the empirical evaluation in the context of existing works is largely restricted. For example, the schemes in [21, 35] could very well be applied here as well. Similarly, in the setup for MNIST/fashion datasets Table 3, I would have expected the default use implementation of flows for approximating intractable posteriors in a VAE is an inverse autoregressive flow (IAF) but instead the authors choose it to be a Real-NVP.
Review Point: - the unbiased guarantees for estimating expectations are only in the asymptotic limit. I would have been curious to see an analysis of the bias-variance tradeoff in the empirical evaluations.
Review Point: - related work such as A-NICE-MC [35] take special care to ensure that detailed balance is satisfied while using flows as proposals for MCMC. In the current work, it is unclear if such conditions are being satisfied in practice for SNFs.
==================================================

Focused review:

* In the related work section, the comparison between the present approach and the two most similar approaches [18, 44] is lackluster. In the case of [44] this is particularly important, since there was no empirical comparison to that work. There is also no explanation as to why such a comparison was not done. * The experimental results are done on only one MRI dataset, and images are cropped to a small size (128x128) for computational reasons (raw k-space data in fastMRI dataset is of size 368x640). Therefore, the experiments are done on a highly simplified setting, and it is hard to infer how well these results would scale to a more practical setting. * The differences between the AlphaZero approach and the proposed method are statistically significant but the effect is small, particularly considering the gap with respect to the adaptive greedy method. * The analysis of greedy vs. non-greedy is interesting, but the conclusions that can be drawn from it are still limited. It would have been valuable to study whether properties of the data favor the use of greedy policies. It is possible that the higher variance of the non-greedy method offers only a partial explanation for this result.

Review Point: * In the related work section, the comparison between the present approach and the two most similar approaches [18, 44] is lackluster. In the case of [44] this is particularly important, since there was no empirical comparison to that work. There is also no explanation as to why such a comparison was not done.
Review Point: * The experimental results are done on only one MRI dataset, and images are cropped to a small size (128x128) for computational reasons (raw k-space data in fastMRI dataset is of size 368x640). Therefore, the experiments are done on a highly simplified setting, and it is hard to infer how well these results would scale to a more practical setting.
Review Point: * The differences between the AlphaZero approach and the proposed method are statistically significant but the effect is small, particularly considering the gap with respect to the adaptive greedy method.
Review Point: * The analysis of greedy vs. non-greedy is interesting, but the conclusions that can be drawn from it are still limited. It would have been valuable to study whether properties of the data favor the use of greedy policies. It is possible that the higher variance of the non-greedy method offers only a partial explanation for this result.
==================================================

Focused review:

The method itself requires tweaks for the objectives to make it applicable, which might not be obvious in some cases. The paper jumps to "large scale" experiments directly after introducing the method. It would be ideal to have some "toy examples" which illustrate and evaluate the proposed method and contrast them against SSM and DSM. * Specifically, it is claimed that the proposed method is insensitive to a wide range of \epsilon but there is no experiment that supports this claim. * It would be nice to see the actual trace plot of the SM loss for FD and non-FD objectives, which show how good the approximations are and provide empirical evidence of the validity of Theorem 2. For the large-scale experiments: * Does FD-SSM refer to FD-SSMVR? * Why sometimes only FD-SSM is reported and sometimes only FD-DSM is report? ########## # Update # ########## The additional plot Fig. A(b) looks great to me. The y-axis of the current version is the time but I assume both are run for the same iterations. It would be good to use iterations I guess because we just want to check if FD affect the convergence rather than comparing the computational efficacy. But overall, the response looks good to me. Thanks!

Review Point: The method itself requires tweaks for the objectives to make it applicable, which might not be obvious in some cases. The paper jumps to "large scale" experiments directly after introducing the method. It would be ideal to have some "toy examples" which illustrate and evaluate the proposed method and contrast them against SSM and DSM.
Review Point: * Specifically, it is claimed that the proposed method is insensitive to a wide range of \epsilon but there is no experiment that supports this claim.
Review Point: * It would be nice to see the actual trace plot of the SM loss for FD and non-FD objectives, which show how good the approximations are and provide empirical evidence of the validity of Theorem 2. For the large-scale experiments:
Review Point: * Why sometimes only FD-SSM is reported and sometimes only FD-DSM is report? ########## # Update # ########## The additional plot Fig. A(b) looks great to me. The y-axis of the current version is the time but I assume both are run for the same iterations. It would be good to use iterations I guess because we just want to check if FD affect the convergence rather than comparing the computational efficacy. But overall, the response looks good to me. Thanks!
==================================================

Focused review:

1. Despite the strengths as mentioned before, the overall novelty is not enough for NeurIPS. The proposed method is highly related to contrastive learning and simCLR, so it can be seen as an incremental research. 2. The proposed method is limited to image data, as the transformation augmentation cannot applied to other types of data, such as temporal data. So I suggest the author add some keywords such as visual or image to the paper title. 3. The paper does not clearly explain why these augmentations are helpful for OOD detection. 4. It is better to report the performance variance in each setting.

Review Point: 1. Despite the strengths as mentioned before, the overall novelty is not enough for NeurIPS. The proposed method is highly related to contrastive learning and simCLR, so it can be seen as an incremental research.
Review Point: 2. The proposed method is limited to image data, as the transformation augmentation cannot applied to other types of data, such as temporal data. So I suggest the author add some keywords such as visual or image to the paper title.
Review Point: 3. The paper does not clearly explain why these augmentations are helpful for OOD detection.
Review Point: 4. It is better to report the performance variance in each setting.
==================================================

Focused review:

1. The "training speed" analyzed in this paper could be very different from the previous works. Equation (2) assumes the update follows a Bayesian updating procedure. However, stochastic optimization are adopted for most of thee gradient-based optimization algorithms. 2. The experiments are conducted only in toy dataset such as MNIST-fashion. I am wondering does the proposed model selection approach scale beyond MNIST-like dataset? It would also be interesting to compare the proposed approach with other neural network pruning methods. 3. There should be more baselines in the experiments sections. For instance, for feature dimension selection, the proposed approach should at least compare with LASSO. More simple baselines could provide a better overview.

Review Point: 1. The "training speed" analyzed in this paper could be very different from the previous works. Equation (2) assumes the update follows a Bayesian updating procedure. However, stochastic optimization are adopted for most of thee gradient-based optimization algorithms.
Review Point: 2. The experiments are conducted only in toy dataset such as MNIST-fashion. I am wondering does the proposed model selection approach scale beyond MNIST-like dataset? It would also be interesting to compare the proposed approach with other neural network pruning methods.
Review Point: 3. There should be more baselines in the experiments sections. For instance, for feature dimension selection, the proposed approach should at least compare with LASSO. More simple baselines could provide a better overview.
==================================================

Focused review:

Weaknesses: W1: The method seems ad-hoc with no theoretical justification. Is is possible to provide a proof for a simple case (e.g. 1-dimensional time series) that RevIN indeed handles distribution shift over time?
W2: Experiments are on 4 data sets only. To validate the merit of RevIN, I would like to see experimental results on more real-world data sets, for instance those taken from UCI Repository.
Other remarks: R1: In the first paragraph of Section 3.1, it looks like output sequence
Y
needs to have the same length as the input sequence
X
. Does this have to be the case?
R2: ECL data set originally has a lot of dimensions and a few instances. After preprocessing, what are the length and dimensionality of the new time series?


Review Point: W1: The method seems ad-hoc with no theoretical justification. Is is possible to provide a proof for a simple case (e.g. 1-dimensional time series) that RevIN indeed handles distribution shift over time?
Review Point: W2: Experiments are on 4 data sets only. To validate the merit of RevIN, I would like to see experimental results on more real-world data sets, for instance those taken from UCI Repository. Other remarks:
Review Point: R1: In the first paragraph of Section 3.1, it looks like output sequence Y needs to have the same length as the input sequence X . Does this have to be the case?
Review Point: R2: ECL data set originally has a lot of dimensions and a few instances. After preprocessing, what are the length and dimensionality of the new time series?
==================================================

Focused review:

Weaknesses:
The fact that larger models memorize more has been shown abundantly in the past (not theoretically, but this paper only looks at linear regression theoretically), for instance in [1] and [2] this is shown for GPT-2 and BERT families, respectively.
The conclusion that downsizing models is better than adding noise for privacy is too broad and even detrimental. I think it is important to clarify that smaller models only memorize less, 'on average'. I wonder if the observation of figure 3c holds only when we look at average memorization. I would guess if we look at individual sample memorization, the worst case would be better on the noise addition method (which is similar to DP). I think calling smaller models a privacy mitigation/defense could have negative consequences.
This final point is not completely relevant, but as far as the broad conclusion of smaller models are better for privacy goes, there is another way in which it is not accurate. Recent work has shown that when DP-SGD is used for training models, it is in fact better to have larger, pre-trained models that you fine-tune on [3,4]. I understand that this is outside the scope of this paper.
Summary: The paper formalizes empirical observations and existing heuristics, but other than that I do not see any added insights that would change the way we deploy or think about models. Yes smaller models are better in terms of memorization, when you have smaller data, but as datasets get larger we need to scale up. I will increase my score if my understanding of parts of the paper is lacking and I have misunderstood something.
[1] Carlini N, Tramer F, Wallace E, Jagielski M, Herbert-Voss A, Lee K, Roberts A, Brown T, Song D, Erlingsson U, Oprea A. Extracting training data from large language models. In30th USENIX Security Symposium (USENIX Security 21) 2021 (pp. 2633-2650).
[2] Mireshghallah F, Goyal K, Uniyal A, Berg-Kirkpatrick T, Shokri R. Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks. arXiv e-prints. 2022 Mar:arXiv-2203.
[3] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12. (ICLR)
[4] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13.


Review Point: The fact that larger models memorize more has been shown abundantly in the past (not theoretically, but this paper only looks at linear regression theoretically), for instance in [1] and [2] this is shown for GPT-2 and BERT families, respectively. The conclusion that downsizing models is better than adding noise for privacy is too broad and even detrimental. I think it is important to clarify that smaller models only memorize less, 'on average'. I wonder if the observation of figure 3c holds only when we look at average memorization. I would guess if we look at individual sample memorization, the worst case would be better on the noise addition method (which is similar to DP). I think calling smaller models a privacy mitigation/defense could have negative consequences. This final point is not completely relevant, but as far as the broad conclusion of smaller models are better for privacy goes, there is another way in which it is not accurate. Recent work has shown that when DP-SGD is used for training models, it is in fact better to have larger, pre-trained models that you fine-tune on [3,4]. I understand that this is outside the scope of this paper. Summary: The paper formalizes empirical observations and existing heuristics, but other than that I do not see any added insights that would change the way we deploy or think about models. Yes smaller models are better in terms of memorization, when you have smaller data, but as datasets get larger we need to scale up. I will increase my score if my understanding of parts of the paper is lacking and I have misunderstood something. [1] Carlini N, Tramer F, Wallace E, Jagielski M, Herbert-Voss A, Lee K, Roberts A, Brown T, Song D, Erlingsson U, Oprea A. Extracting training data from large language models. In30th USENIX Security Symposium (USENIX Security 21) 2021 (pp. 2633-2650). [2] Mireshghallah F, Goyal K, Uniyal A, Berg-Kirkpatrick T, Shokri R. Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks. arXiv e-prints.
Review Point: 2022 Mar:arXiv-2203. [3] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679.
Review Point: 2021 Oct 12. (ICLR) [4] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500.
==================================================

Focused review:

1. Although the author has done a comparative experiment between the multi-modal and the GECA methods on the proposed benchmark, the design and analysis of the experiment are confusing and difficult to understand. 2. Besides, there are lots of inconsistencies in the experiment section, such as the title of Table1 claim that “Models fail on all splits except C and F”, which is inconsistent with the data in rows C and F. The poor performance of the two methods on most split datasets could explain that advances are needed in neural architectures for compositional learning. 3. However, I am not quite sure why the author can draw the following conclusions based on the experiment and analysis of split A-I: artifacts in SCAN are not central to the nature of compositional generalization, gSCAN removes these artifacts by introducing more sophisticated semantics through grounding. I think that seems a little bit unconvincing.

Review Point: 1. Although the author has done a comparative experiment between the multi-modal and the GECA methods on the proposed benchmark, the design and analysis of the experiment are confusing and difficult to understand.
Review Point: 2. Besides, there are lots of inconsistencies in the experiment section, such as the title of Table1 claim that “Models fail on all splits except C and F”, which is inconsistent with the data in rows C and F. The poor performance of the two methods on most split datasets could explain that advances are needed in neural architectures for compositional learning.
Review Point: 3. However, I am not quite sure why the author can draw the following conclusions based on the experiment and analysis of split A-I: artifacts in SCAN are not central to the nature of compositional generalization, gSCAN removes these artifacts by introducing more sophisticated semantics through grounding. I think that seems a little bit unconvincing.
==================================================

Focused review:

Weakness: 1) The sample-efficiency of the algorithm has very limited insight given previous works. 2) It will be more clear for the readers to understand the contribution of the paper if the paper can explicitly analyze how much computation cost is reduced, rather than just from the reduction of planning calls. In deed, the new algorithm still require to calculate a new covariance matrix in each iteration. When re-planning is triggered, it requires solving a QCQP, which is not required in Algorithm 1. 3) It is not very clear how strong the cross product structure in Assumption 3 is. Does it includes the case where there is only finite number of possible
w
?


Review Point: 1) The sample-efficiency of the algorithm has very limited insight given previous works.
Review Point: 2) It will be more clear for the readers to understand the contribution of the paper if the paper can explicitly analyze how much computation cost is reduced, rather than just from the reduction of planning calls. In deed, the new algorithm still require to calculate a new covariance matrix in each iteration. When re-planning is triggered, it requires solving a QCQP, which is not required in Algorithm 1.
Review Point: 3) It is not very clear how strong the cross product structure in Assumption 3 is. Does it includes the case where there is only finite number of possible w ?
==================================================

Focused review:

1.Citation: Please cite the journal or the conference version for your reference instead of the Arxiv one.
2.Expression: Please express things in a unified way. In the Local Graph Linking Analysis section, some tables are written in snake case, some are in Capitalisation case, some are “quoted” case, and some even in Big_snake case. It’s not easy to read through.


Review Point: 1.Citation: Please cite the journal or the conference version for your reference instead of the Arxiv one.
Review Point: 2.Expression: Please express things in a unified way. In the Local Graph Linking Analysis section, some tables are written in snake case, some are in Capitalisation case, some are “quoted” case, and some even in Big_snake case. It’s not easy to read through.
==================================================

Focused review:

Weaknesses: - Do all the baseline LSTM models in the paper use recurrent projection units (Sak et al. 2014, Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling)? It should be easy to implement and it is effective to improve performance with the same number of parameters. - LSTM + 1-D conv should be used as the baseline in Table 3 and 5 since it is shown to be better than LSTM alone in Table 1. - It is kind of surprising that SRU + 1-D conv achieves better accuracy than LSTM + 1-D conv. It is worth discussing the reason / hypothesis in the paper. - In Table 6, it would be useful to also measure the computation time of LSTM AM for comparison. - The proposed system should be compared to a standard LSTM AM + WFST decoder graph with a (pruned) n-gram LM, which is shown to be efficient for embedded speech recognition in previous works (e.g. McGraw et al. 2016, Personalized speech recognition on mobile devices), especially now that the RNN LM is the computation bottleneck of the entire system as is shown in Figure 2. In fact, the speed of RNN LM could be further improved by 8-bit quantization, which typically does not degrade the WER.  Clarity: In general, the presentation in the paper is clear and well-organized, except for the following questions: - In Table 2, what is the system performance with HCLM? - Does the system in Table 6 use characters or word pieces? Both should be shown for comparison. - How much is the WER different between different beam width in Figure 2? Does using half-precision in LM hurt WER? - It would be clearer to mention in Table 2 that 3.60 (for Deep Speech 2) is WER (instead of CER).  Originality: The problem is well-motivated: multi-time step parallel processing for AM would be very useful for embedded system recognition, but SRU alone does not work. It is novel to find that simply combining 1-D conv with SRU substantially improves the quality of SRU, leading to better performance than LSTM. Other tricks and analyses in the paper to improve speed and accuracy are also useful.   Significance: The results are very encouraging in terms of both accuracy and speed. Both the idea and the experiments would be of interest to the field and motivate future work.  It is a good submission. I recommend an accept, although the issues mentioned above should be addressed.

Review Point: - Do all the baseline LSTM models in the paper use recurrent projection units (Sak et al. 2014, Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling)? It should be easy to implement and it is effective to improve performance with the same number of parameters.
Review Point: - LSTM + 1-D conv should be used as the baseline in Table 3 and 5 since it is shown to be better than LSTM alone in Table 1.
Review Point: - It is kind of surprising that SRU + 1-D conv achieves better accuracy than LSTM + 1-D conv. It is worth discussing the reason / hypothesis in the paper.
Review Point: - In Table 6, it would be useful to also measure the computation time of LSTM AM for comparison.
Review Point: - The proposed system should be compared to a standard LSTM AM + WFST decoder graph with a (pruned) n-gram LM, which is shown to be efficient for embedded speech recognition in previous works (e.g. McGraw et al. 2016, Personalized speech recognition on mobile devices), especially now that the RNN LM is the computation bottleneck of the entire system as is shown in Figure 2. In fact, the speed of RNN LM could be further improved by 8-bit quantization, which typically does not degrade the WER. Clarity: In general, the presentation in the paper is clear and well-organized, except for the following questions:
Review Point: - In Table 2, what is the system performance with HCLM?
Review Point: - Does the system in Table 6 use characters or word pieces? Both should be shown for comparison.
Review Point: - How much is the WER different between different beam width in Figure 2? Does using half-precision in LM hurt WER?
Review Point: - It would be clearer to mention in Table 2 that 3.60 (for Deep Speech 2) is WER (instead of CER). Originality: The problem is well-motivated: multi-time step parallel processing for AM would be very useful for embedded system recognition, but SRU alone does not work. It is novel to find that simply combining 1-D conv with SRU substantially improves the quality of SRU, leading to better performance than LSTM. Other tricks and analyses in the paper to improve speed and accuracy are also useful. Significance: The results are very encouraging in terms of both accuracy and speed. Both the idea and the experiments would be of interest to the field and motivate future work. It is a good submission. I recommend an accept, although the issues mentioned above should be addressed.
==================================================

Focused review:

**Exposition** - There are several places where the exposition is not clear (see the “clarity section” below). **Novelty** - One could argue that this paper is an application of DIP to video. Yet, I think the insight is interesting and differs from recovering clean images in the original DIP paper. **Method** - When I see that the title of deep video prior, I thought there will be some specific design exploiting the temporal dimension of video. Instead, the method applies the CNN training for each frame independently. I am wondering whether alternative designs could be applicable for the problem of blind video consistency as well. For example, there could two other simple methods to consider. - 1) One can have a straightforward extension of DIP for video by using some kind of 3D (spatio-temporal) CNN. The same argument that fitting the temporal flicker takes longer than fitting the contents of the processed video still applies. - 2) Explicitly using flow as in the “An internal learning approach to video inpainting, ICCV 2019” paper. - The proposed method requires 2 seconds per frame for frames with 800 x 480 resolution on a NVIDIA RTX 2080 Ti. I think it would be better to acknowledge slow runtime as a limitation by comparing the runtime speed with other methods. I don’t know how other methods’ runtime performances are in this specific hardware setup. However, from [Lai et al. 2018], “…. the execution speed achieves 418 FPS on (Titan X) GPU for videos with a resolution of 1280 × 720”. This suggests that the proposed method may be around 1,000 times slower than that of [Lai et al. 2018]. **Evaluation** - As discussed in the paper, the perceptual similarity and warping error are competing metrics. Showing one data point cannot reveal the whole story. I would suggest showing a *curve* in the perceptual similarity and warping error space. For example, one can plot the two errors along the training iterations. - Why using PSNR as the perceptual similarity? It does not reflect the perceptual similarity between the predicted and processed frames. I think the metric used in [Lai et al. 2018] (LPIPS) makes more sense here. - I have concerns about “L244: In this paper, we fix the number of training epochs for each task empirically.” How did you determine the number of training epochs? As there is no mention of a validation set in the paper, it sounds like this is tuned on the *testing set*. What are these numbers for different tasks? Are they very different (and therefore sensitive) to different tasks. This is important because the goal for this problem is to enforce temporal consistency on videos that are processed by *unseen* application. That is, the method should work without the knowledge of accessing the information about the task. If one needs to tune the hyperparameters for each task, then it is no longer a “blind” video temporal consistency method. - There were failure cases shown in the paper. It would be good to have some to highlight the limitations of the method.

Review Point: **Exposition** - There are several places where the exposition is not clear (see the “clarity section” below). **Novelty** - One could argue that this paper is an application of DIP to video. Yet, I think the insight is interesting and differs from recovering clean images in the original DIP paper. **Method** - When I see that the title of deep video prior, I thought there will be some specific design exploiting the temporal dimension of video. Instead, the method applies the CNN training for each frame independently. I am wondering whether alternative designs could be applicable for the problem of blind video consistency as well. For example, there could two other simple methods to consider.
Review Point: - 1) One can have a straightforward extension of DIP for video by using some kind of 3D (spatio-temporal) CNN. The same argument that fitting the temporal flicker takes longer than fitting the contents of the processed video still applies.
Review Point: - 2) Explicitly using flow as in the “An internal learning approach to video inpainting, ICCV 2019” paper.
Review Point: - The proposed method requires 2 seconds per frame for frames with 800 x 480 resolution on a NVIDIA RTX 2080 Ti. I think it would be better to acknowledge slow runtime as a limitation by comparing the runtime speed with other methods. I don’t know how other methods’ runtime performances are in this specific hardware setup. However, from [Lai et al. 2018], “…. the execution speed achieves 418 FPS on (Titan X) GPU for videos with a resolution of 1280 × 720”. This suggests that the proposed method may be around 1,000 times slower than that of [Lai et al. 2018]. **Evaluation** - As discussed in the paper, the perceptual similarity and warping error are competing metrics. Showing one data point cannot reveal the whole story. I would suggest showing a *curve* in the perceptual similarity and warping error space. For example, one can plot the two errors along the training iterations.
Review Point: - Why using PSNR as the perceptual similarity? It does not reflect the perceptual similarity between the predicted and processed frames. I think the metric used in [Lai et al. 2018] (LPIPS) makes more sense here.
Review Point: - I have concerns about “L244: In this paper, we fix the number of training epochs for each task empirically.” How did you determine the number of training epochs? As there is no mention of a validation set in the paper, it sounds like this is tuned on the *testing set*. What are these numbers for different tasks? Are they very different (and therefore sensitive) to different tasks. This is important because the goal for this problem is to enforce temporal consistency on videos that are processed by *unseen* application. That is, the method should work without the knowledge of accessing the information about the task. If one needs to tune the hyperparameters for each task, then it is no longer a “blind” video temporal consistency method.
Review Point: - There were failure cases shown in the paper. It would be good to have some to highlight the limitations of the method.
==================================================

Focused review:

Weaknesses: 1. The key question is âDo we need to make capsule networks explicitly equivariant?â My answer is âmost likely noâ. Because the idea of capsule networks is not to push for a specific and restricted type of equivariance. It allows freedom in the pose vector and choses the routing algorithm such that the data speak for itself and the pose matrices automatically capture the ârightâ variances in the lower level features.  2. Continuing the previous comment, arenât we unnecessarily restricting the capsule networks? Unfortunately, in the experiments the group CapNet is deeper than the simple CapNet and the authors do not provide the number of parameters in the group CapNet. So, we cannot judge how much enforcing the equivariance strictly is beneficial. Because the original CapNet does capture some variations in the data. 3. Unfortunately, the authors omit a key baseline in the experiments: the group-CNN. Not only it is not clear how much extra accuracy is due to CapNets, but also, they do not validate their argument in Section 4.  I certainly enjoyed reading this paper, mainly because of the insights and rigor that it brings to the capsule networks idea. Unfortunately, there are doubts about about necessity of this idea and the experiments do not provide any solid evidence either.  ======================== After reading the authors' rebuttal:  I still do not understand why the proposed model is not directly comparable to the original G-CNN.   In the rebuttal, the authors talk about keeping interpretability. But the current version of the paper does not have any experimental validation of these claims.  As I mentioned in the original review, I like this paper, but I think it is not ready for publication at the current shape.

Review Point: 1. The key question is âDo we need to make capsule networks explicitly equivariant?â My answer is âmost likely noâ. Because the idea of capsule networks is not to push for a specific and restricted type of equivariance. It allows freedom in the pose vector and choses the routing algorithm such that the data speak for itself and the pose matrices automatically capture the ârightâ variances in the lower level features.
Review Point: 2. Continuing the previous comment, arenât we unnecessarily restricting the capsule networks? Unfortunately, in the experiments the group CapNet is deeper than the simple CapNet and the authors do not provide the number of parameters in the group CapNet. So, we cannot judge how much enforcing the equivariance strictly is beneficial. Because the original CapNet does capture some variations in the data.
Review Point: 3. Unfortunately, the authors omit a key baseline in the experiments: the group-CNN. Not only it is not clear how much extra accuracy is due to CapNets, but also, they do not validate their argument in Section 4. I certainly enjoyed reading this paper, mainly because of the insights and rigor that it brings to the capsule networks idea. Unfortunately, there are doubts about about necessity of this idea and the experiments do not provide any solid evidence either. ======================== After reading the authors' rebuttal: I still do not understand why the proposed model is not directly comparable to the original G-CNN. In the rebuttal, the authors talk about keeping interpretability. But the current version of the paper does not have any experimental validation of these claims. As I mentioned in the original review, I like this paper, but I think it is not ready for publication at the current shape.
==================================================

Focused review:

Weaknesses]
The proposed idea novelty is limited. The framework of NIA is similar to [Zhao et al. (2022)] since the two major steps of both methods are generalizable implicit body representation and sparse views blending. The core design of NIA is the neural appearance blending module (sec. 3.2), yet no ablation study to discuss and analyze why such a design helps.
Three assumptions exist in this work: 1) the calibration parameters of the multi-view input images are known; 2) the foreground human region masks are known; 3) the fitted SMPL models are available as prior. The first two assumptions are also seen in other methods; however, yet the third assumption is new in this work. The third assumption seems too strong since the SMPL model estimation is a vital factor affecting human NeRF rendering. Therefore, while assuming the fitted SMPL models are available as a prior, it is doubtful that the comparison experiments are fair enough compared with other methods. Notice that Table 4 (i) shows that the NIA w/o image-based rendering (with the third assumption) has already better than most other methods.
In sec 3.3, the description of “the 3D coordinates of the posed SMPL vertices in both reference and target spaces are known by the nature of motion-tracked SMPL model” is another strong assumption that makes the NIA available to avoid the finetuning step. It is unclear why the such assumption is available while dealing with the unseen subject with unseen poses.
The comparison experiments are not convincing. It seems that some existing methods are reimplemented since the reported results of these methods (degraded) in this paper are not the same as their published ones. It is better to annotate which results are reimplemented and which are not.
Some implementation details are not clear:
The limitations of the sparse input images. Are there any constraints for view directions?
The feature representation of P_n is not defined;
The design of two MLPs in equation (1) is different from NeRF [Mildenhall et al. (2020)]. Why is such a design reasonable, and what is its advantages?
The architecture of F_B in equation (2). Does (2) means that it has one single model for a specific value of N?
SE(3), x^can, k-th part sampling method, function R(), function t() in sec. 3.3;
Reference: [Zhao et al. (2022)] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In CVPR, 2022.


Review Point: The proposed idea novelty is limited. The framework of NIA is similar to [Zhao et al. (2022)] since the two major steps of both methods are generalizable implicit body representation and sparse views blending. The core design of NIA is the neural appearance blending module (sec. 3.2), yet no ablation study to discuss and analyze why such a design helps. Three assumptions exist in this work:
Review Point: 1) the calibration parameters of the multi-view input images are known;
Review Point: 3) the fitted SMPL models are available as prior. The first two assumptions are also seen in other methods; however, yet the third assumption is new in this work. The third assumption seems too strong since the SMPL model estimation is a vital factor affecting human NeRF rendering. Therefore, while assuming the fitted SMPL models are available as a prior, it is doubtful that the comparison experiments are fair enough compared with other methods. Notice that Table 4 (i) shows that the NIA w/o image-based rendering (with the third assumption) has already better than most other methods. In sec 3.3, the description of “the 3D coordinates of the posed SMPL vertices in both reference and target spaces are known by the nature of motion-tracked SMPL model” is another strong assumption that makes the NIA available to avoid the finetuning step. It is unclear why the such assumption is available while dealing with the unseen subject with unseen poses. The comparison experiments are not convincing. It seems that some existing methods are reimplemented since the reported results of these methods (degraded) in this paper are not the same as their published ones. It is better to annotate which results are reimplemented and which are not. Some implementation details are not clear: The limitations of the sparse input images. Are there any constraints for view directions? The feature representation of P_n is not defined; The design of two MLPs in equation (1) is different from NeRF [Mildenhall et al. (2020)]. Why is such a design reasonable, and what is its advantages? The architecture of F_B in equation (2). Does (2) means that it has one single model for a specific value of N? SE(3), x^can, k-th part sampling method, function R(), function t() in sec. 3.3; Reference: [Zhao et al. (2022)] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In CVPR, 2022.
==================================================

Focused review:

Weaknesses ---
W1. The paper claims (e.g., in the abstract and the conclusion) to show that the proposed TTLM is a generalization of 2nd-order RNNs, RACs, and MI-RNNs. The TTLM model takes the form given in Eq (2). In order to say that TTLM generalizes another model, that model must be expressible in the form given in Eq (2). However, according to Claim 4.1 and 4.2, in order to be able to express 2nd-order RNNs and MI-RNNs additional nonlinearities need to be inserted into the formula in Eq (2), therefore yielding a model which is not expressible as a TTLM (i.e., as in Eq (2)). Therefore, it is incorrect to say that TTLM generalizes those models! If TTLM truly generalized these models, there shouldn't be a need to further modify Eq (2).
This is a bit like saying that the set of linear functions (i.e., matrices) generalize functions
f
:
R
m
→
R
n
of the form
f
(
x
)
=
σ
(
A
x
)
, where
A
is a matrix
σ
is a non-linear functions.
W2. The paper is full of typos, missing/redundant commas and periods, and notational inconsistencies that a careful proof reading should have caught; there are two inconsistencies already in the first sentence in Sec 1.
W3. On page 1, you say that "we propose a novel Tensor Train Language Model, as a first attempt to apply tensor networks on language modeling tasks". This makes it sound like applying tensor networks, and tensor trains in particular, to language modeling is new. But that's not the case. For example, Miller et al. (2021) use matrix product states (which is the same as tensor trains) with language modeling as a potential application. Your contribution needs to be clarified.
W4. The paper is difficult to follow in several places. For example:
The discussion in Sec 3.3 doesn't make sense. You insert Eq (4) into Eq (3) to get Eq (5)---which is the same as Eq (3). What is the point of this calculation?
Below Eq (5) you show diagrammatic notation and say that it represents the tensor
A
---but doesn't that tensor network graph correspond to the contraction in Eq (2)?
The discussion at the start of Sec 4 is difficult to follow. TNLM is never defined.
How exactly is the slice operator
T
[
i
]
defined? This is unclear below Eq (6). Is the 2nd or 3rd index kept fixed when slicing?
In Fig 3: Plot (a) shows that
f
θ
(
x
(
j
)
)
is contracted with the tensor
M
(
j
)
. But then (b) and (c) seem to indicate that
M
(
j
)
itself is a reshaped version (a matrix, rather than tensor) of a contraction that involves
f
θ
(
x
(
j
)
)
. Also, since TTML-Large and TTML-Tiny add additional operations like reshape, it's not immediately clear that they can even be expressed on the form in Eq (2). Again, if additional functions need to be added into Eq (2) to make the model expressible in that format, then it's not a TTLM.
In Fig 4: The caption refers to labels (RNNs-100, RNNs-200, etc) that don't appear in the figure.
W5. The experiment results aren't represented properly. For example, you say that "TTLM-Large obtains the best PPL among these models," but that's not true. Two of the LSTM-based methods achieve lower PPL in Table 1 (for PTB). For the PTB dataset, the PPL for TTLM-Large is bolded even though it's not the best number.


Review Point: --- W1. The paper claims (e.g., in the abstract and the conclusion) to show that the proposed TTLM is a generalization of 2nd-order RNNs, RACs, and MI-RNNs. The TTLM model takes the form given in Eq (2). In order to say that TTLM generalizes another model, that model must be expressible in the form given in Eq (2). However, according to Claim 4.1 and 4.2, in order to be able to express 2nd-order RNNs and MI-RNNs additional nonlinearities need to be inserted into the formula in Eq (2), therefore yielding a model which is not expressible as a TTLM (i.e., as in Eq (2)). Therefore, it is incorrect to say that TTLM generalizes those models! If TTLM truly generalized these models, there shouldn't be a need to further modify Eq (2). This is a bit like saying that the set of linear functions (i.e., matrices) generalize functions f : R m → R n of the form f ( x ) = σ ( A x ) , where A is a matrix σ is a non-linear functions.
Review Point: W2. The paper is full of typos, missing/redundant commas and periods, and notational inconsistencies that a careful proof reading should have caught; there are two inconsistencies already in the first sentence in Sec 1.
Review Point: W3. On page 1, you say that "we propose a novel Tensor Train Language Model, as a first attempt to apply tensor networks on language modeling tasks". This makes it sound like applying tensor networks, and tensor trains in particular, to language modeling is new. But that's not the case. For example, Miller et al. (2021) use matrix product states (which is the same as tensor trains) with language modeling as a potential application. Your contribution needs to be clarified.
Review Point: W4. The paper is difficult to follow in several places. For example: The discussion in Sec 3.3 doesn't make sense. You insert Eq (4) into Eq (3) to get Eq (5)---which is the same as Eq (3). What is the point of this calculation? Below Eq (5) you show diagrammatic notation and say that it represents the tensor A ---but doesn't that tensor network graph correspond to the contraction in Eq (2)? The discussion at the start of Sec 4 is difficult to follow. TNLM is never defined. How exactly is the slice operator T [ i ] defined? This is unclear below Eq (6). Is the 2nd or 3rd index kept fixed when slicing? In Fig 3: Plot (a) shows that f θ ( x ( j ) ) is contracted with the tensor M ( j ) . But then (b) and (c) seem to indicate that M ( j ) itself is a reshaped version (a matrix, rather than tensor) of a contraction that involves f θ ( x ( j ) ) . Also, since TTML-Large and TTML-Tiny add additional operations like reshape, it's not immediately clear that they can even be expressed on the form in Eq (2). Again, if additional functions need to be added into Eq (2) to make the model expressible in that format, then it's not a TTLM. In Fig 4: The caption refers to labels (RNNs-100, RNNs-200, etc) that don't appear in the figure.
Review Point: W5. The experiment results aren't represented properly. For example, you say that "TTLM-Large obtains the best PPL among these models," but that's not true. Two of the LSTM-based methods achieve lower PPL in Table 1 (for PTB). For the PTB dataset, the PPL for TTLM-Large is bolded even though it's not the best number.
==================================================

Focused review:

weakness of the paper are as follows (from my perspective):  * (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods. * (weakness) currently, the paper does not seem to have a detailed discussion on how their generative model was trained to produce images containing the goal information. The authors do clarify this on their feedback and it would be useful if they also add this discussion on their next version of the paper. More importantly, including this discussion is useful for the Deep RL community. * (weakness) their current approach of training the generative model relies on manually annotating the goal images, which may prevent scalability of the algorithm. Addressing this could make their approach be more impactful.

Review Point: * (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods.
Review Point: * (weakness) currently, the paper does not seem to have a detailed discussion on how their generative model was trained to produce images containing the goal information. The authors do clarify this on their feedback and it would be useful if they also add this discussion on their next version of the paper. More importantly, including this discussion is useful for the Deep RL community.
Review Point: * (weakness) their current approach of training the generative model relies on manually annotating the goal images, which may prevent scalability of the algorithm. Addressing this could make their approach be more impactful.
==================================================

Focused review:

Weaknesses:
The technical contribution is limited. Essentially, the proposed method is just to apply the existing PointConv operator in both spatial and temporal dimensions. Also, regarding the literature of spatial-temporal point cloud processing, it would be nice if the paper can add some discussion/comparison with PointRNN [A1], which I think is the closest work to this paper though to my knowledge it is not published yet (has been on arXiv for more than a year).
The proposed method seems to be problematic in modeling temporal dynamics. Specifically, the PointConv operator in Eq. 1 is to aggregate features within a neighborhood (weighted summation of the features), which does not consider the order of the points in the neighborhood, i.e., permutation-invariant. In the spatial point processing case, this is fine in some applications where we do not consider the direction but only distances of neighborhood points. However, in the temporal domain (i.e., Eq. 3), I feel using PointConv does not make much sense as the order of time should matter in general. Essentially, Eq.3 tells us that, if the centroid point is at frame t, then its corresponding neighbor point at frame t-H has the same distance measure as its corresponding neighbor point at frame t+H, which means that the direction of time is not considered. I feel that the formulation needs to inform the network of the time flow, i.e., the order of time. Also, in Eq 2, the neighborhood also considers some points within a small time window but discards the order of the time again. As a result, I do not think the spatial-temporal point processing proposed in this paper is properly modeling temporal dynamics. Even simple GNN/PointNet + RNN [Li et al 2018b, A2] network structure considers the order of time in the RNN.
A big problem I feel about the proposed method is that it relies on a strong assumption about the point correspondences in time, which largely limits the practical impact of this paper. In this paper, the proposed method is shown to reason for high-level entities (e.g., represent an object or a Starcraft unit as a single point), which is fine as the correspondences can be obtained by object tracking in practice. However, how about the scenario where each point does not represent a high-level object but represent low-level observations, e.g., 3D point clouds obtained by LiDAR or stereo reconstruction? In such cases, it is extremely difficult to obtain the point correspondences across time as there could be no correspondences at all in the real-world. Then the proposed method might have problems applied to such data which is widely used in many applications such as point cloud-based object detection, SLAM, point cloud-based classification/segmentation/prediction. In contrast, Minkowski Networks [Choy 2019] that this paper is compared to and also the PointRNN [A1] and SPF [A2] can be applied to these more challenging scenarios.
I am a bit concerned about the dataset used in the experiments as the data in the Starcraft II dataset seems to be generated randomly, which is hard to be reproduced and compared by the follow-up work. Would it be possible that evaluating the proposed method on the test set of a few public benchmarks (e.g., traffic prediction) that others can easily compare with even without re-implemented the proposed method?
The re-implementation of the Minkowski Networks is somewhat concerning. The paper claims that it is hard to find a suitable kernel size and vocalization resolution, and will lead to a prohibitively high number of network parameters if increasing the kernel size. This leads me to wonder how large scale the data is, e.g., how many points and how many frames the method needs to process. If the scale of the data is not too big, there should not be a problem for the Minkowski Networks I guess. In the original paper of the Minkowski Networks, they evaluated their method on a sequence of large-scale LiDAR point cloud data (usually with 50k number of points per frame), which seems fine and obtains strong performance. It would be nice if a more detailed explanation of why the Minkowski Networks cannot be properly tuned.
Justification:
My decision is made mainly because I feel the proposed method seems to have some flaws and limitations. Also, I am not fully convinced by the experimental results due to the concern about data and re-implementation, and the technical contribution is limited too. However, I would be happy to change my mind if there are any misunderstandings.
References
[A1] H. Fan and Y. Yang. PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing. arXiv 2019
[A2] Weng et al. Inverting the Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting. CoRL 2020
Post-rebuttal Review
As there is no response submitted by the authors, I would like to stick to my original rating to reject this paper.


Review Point: The technical contribution is limited. Essentially, the proposed method is just to apply the existing PointConv operator in both spatial and temporal dimensions. Also, regarding the literature of spatial-temporal point cloud processing, it would be nice if the paper can add some discussion/comparison with PointRNN [A1], which I think is the closest work to this paper though to my knowledge it is not published yet (has been on arXiv for more than a year). The proposed method seems to be problematic in modeling temporal dynamics. Specifically, the PointConv operator in Eq.
Review Point: 1 is to aggregate features within a neighborhood (weighted summation of the features), which does not consider the order of the points in the neighborhood, i.e., permutation-invariant. In the spatial point processing case, this is fine in some applications where we do not consider the direction but only distances of neighborhood points. However, in the temporal domain (i.e., Eq. 3), I feel using PointConv does not make much sense as the order of time should matter in general. Essentially, Eq.3 tells us that, if the centroid point is at frame t, then its corresponding neighbor point at frame t-H has the same distance measure as its corresponding neighbor point at frame t+H, which means that the direction of time is not considered. I feel that the formulation needs to inform the network of the time flow, i.e., the order of time. Also, in Eq 2, the neighborhood also considers some points within a small time window but discards the order of the time again. As a result, I do not think the spatial-temporal point processing proposed in this paper is properly modeling temporal dynamics. Even simple GNN/PointNet + RNN [Li et al 2018b, A2] network structure considers the order of time in the RNN. A big problem I feel about the proposed method is that it relies on a strong assumption about the point correspondences in time, which largely limits the practical impact of this paper. In this paper, the proposed method is shown to reason for high-level entities (e.g., represent an object or a Starcraft unit as a single point), which is fine as the correspondences can be obtained by object tracking in practice. However, how about the scenario where each point does not represent a high-level object but represent low-level observations, e.g., 3D point clouds obtained by LiDAR or stereo reconstruction? In such cases, it is extremely difficult to obtain the point correspondences across time as there could be no correspondences at all in the real-world. Then the proposed method might have problems applied to such data which is widely used in many applications such as point cloud-based object detection, SLAM, point cloud-based classification/segmentation/prediction. In contrast, Minkowski Networks [Choy 2019] that this paper is compared to and also the PointRNN [A1] and SPF [A2] can be applied to these more challenging scenarios. I am a bit concerned about the dataset used in the experiments as the data in the Starcraft II dataset seems to be generated randomly, which is hard to be reproduced and compared by the follow-up work. Would it be possible that evaluating the proposed method on the test set of a few public benchmarks (e.g., traffic prediction) that others can easily compare with even without re-implemented the proposed method? The re-implementation of the Minkowski Networks is somewhat concerning. The paper claims that it is hard to find a suitable kernel size and vocalization resolution, and will lead to a prohibitively high number of network parameters if increasing the kernel size. This leads me to wonder how large scale the data is, e.g., how many points and how many frames the method needs to process. If the scale of the data is not too big, there should not be a problem for the Minkowski Networks I guess. In the original paper of the Minkowski Networks, they evaluated their method on a sequence of large-scale LiDAR point cloud data (usually with 50k number of points per frame), which seems fine and obtains strong performance. It would be nice if a more detailed explanation of why the Minkowski Networks cannot be properly tuned. Justification: My decision is made mainly because I feel the proposed method seems to have some flaws and limitations. Also, I am not fully convinced by the experimental results due to the concern about data and re-implementation, and the technical contribution is limited too. However, I would be happy to change my mind if there are any misunderstandings. References [A1] H. Fan and Y. Yang. PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing. arXiv 2019 [A2] Weng et al. Inverting the Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting. CoRL 2020 Post-rebuttal Review As there is no response submitted by the authors, I would like to stick to my original rating to reject this paper.
==================================================

Focused review:

weakness of the work is in its similarity to RARL. Although the proposed approach of simultaneous gradient ascent-descent is reasonable, an advantage of this over RARL would be in better optimization. However this is not analyzed theoretically or experimentally. The authors do mention that the difficulty in optimization may explain why Domain Randomization (DR) demonstrates better worst-case performance than M2TD3, and I feel that this highlights a pretty big problem with the proposed approach: the optimization can become too difficult to achieve the stated objective. As a result, I feel that analyzing this aspect of the work would greatly strengthen the reasons for adoption of this algorithm.
Summary of points and some additional notes are provided below.
Strengths:
Thorough empirical analysis
Details of algorithm well-explained and sensible
I felt that the authors explained the limitations and did sufficient comparison with existing literature
Weaknesses:
Need to be able to set simulator settings randomly, and we get robustness only to these settings. This is common in the literature but it still faces similar issues as domain randomization, etc., in that simulation to real setting changes need to be known in advanced.
There is no theoretical analysis of the convergence properties of the proposed algorithm for simple settings. These results may be impossible in the deep neural network regime but it would inspire confidence in the algorithm to have some result showing that we can expect convergence with direct or linear parameterization (similar to those done by [1], [2], etc)
Writing:
Would probably be better to put Related Works after describing the algorithm because it was hard to understand all the details that were being compared without knowing the algorithm first.
L228:
max
η
J
t
should be
min
η
J
t
?
Notationally I found the use of
ω
and
η
as the uncertainty parameters confusing. I think it would make the reading easier if only one letter is used.
Minor:
abstract has a few grammatical errors
L37 — extra “has been reported”
L265: appears to have a typo “optimize theta and omega optimizes (2) …”
[1] Agarwal, Alekh, et al. "Optimality and approximation with policy gradient methods in markov decision processes." Conference on Learning Theory. PMLR, 2020. [2] Qiu, Shuang, et al. "On finite-time convergence of actor-critic algorithm." IEEE Journal on Selected Areas in Information Theory 2.2 (2021): 652-664.
I think the authors do a reasonable job of discussing and addressing limitations of the work.
One other limitation that I see is in designing of the set
Ω
such that
ω
∗
∈
Ω
, and also to have
ω
be a configurable parameter in the simulation environment. This is common practice in robust approaches, however.
Also as noted by the authors, small dimensions (1-3) of uncertainty parameters were tested due to the exponentially growing size of the uncertainty parameter set with increasing dimensions. Two improvements in this area could be to design the set of parameters in a more directed manner (than a uniform cover), or to do the search in this space could more efficiently than exhaustive search.


Review Point: of the work is in its similarity to RARL. Although the proposed approach of simultaneous gradient ascent-descent is reasonable, an advantage of this over RARL would be in better optimization. However this is not analyzed theoretically or experimentally. The authors do mention that the difficulty in optimization may explain why Domain Randomization (DR) demonstrates better worst-case performance than M2TD3, and I feel that this highlights a pretty big problem with the proposed approach: the optimization can become too difficult to achieve the stated objective. As a result, I feel that analyzing this aspect of the work would greatly strengthen the reasons for adoption of this algorithm. Summary of points and some additional notes are provided below. Strengths: Thorough empirical analysis Details of algorithm well-explained and sensible I felt that the authors explained the limitations and did sufficient comparison with existing literature Weaknesses: Need to be able to set simulator settings randomly, and we get robustness only to these settings. This is common in the literature but it still faces similar issues as domain randomization, etc., in that simulation to real setting changes need to be known in advanced. There is no theoretical analysis of the convergence properties of the proposed algorithm for simple settings. These results may be impossible in the deep neural network regime but it would inspire confidence in the algorithm to have some result showing that we can expect convergence with direct or linear parameterization (similar to those done by [1], [2], etc) Writing: Would probably be better to put Related Works after describing the algorithm because it was hard to understand all the details that were being compared without knowing the algorithm first.
Review Point: L228: max η J t should be min η J t ? Notationally I found the use of ω and η as the uncertainty parameters confusing. I think it would make the reading easier if only one letter is used. Minor: abstract has a few grammatical errors L37 — extra “has been reported” L265: appears to have a typo “optimize theta and omega optimizes (2) …” [1] Agarwal, Alekh, et al. "Optimality and approximation with policy gradient methods in markov decision processes." Conference on Learning Theory. PMLR, 2020. [2] Qiu, Shuang, et al. "On finite-time convergence of actor-critic algorithm." IEEE Journal on Selected Areas in Information Theory 2.2 (2021): 652-664. I think the authors do a reasonable job of discussing and addressing limitations of the work. One other limitation that I see is in designing of the set Ω such that ω ∗ ∈ Ω , and also to have ω be a configurable parameter in the simulation environment. This is common practice in robust approaches, however. Also as noted by the authors, small dimensions (1-3) of uncertainty parameters were tested due to the exponentially growing size of the uncertainty parameter set with increasing dimensions. Two improvements in this area could be to design the set of parameters in a more directed manner (than a uniform cover), or to do the search in this space could more efficiently than exhaustive search.
==================================================

Focused review:

Weaknesses ---
[1 - Evaluation (major concern)] In few-shot image synthesis, image quality metrics (FID, KID) can be often improved at the cost of the synthesis diversity [1] (figure 4). For example, the model that perfectly reproduces the training set can achieve near-to-zero FID and KID. The diversity aspect is not evaluated in this work at all. This makes the whole evaluation of the paper unconvincing. Does FreGAN improve FID of FastGAN at the cost of diversity? Is FreGAN memorizing training images more than FastGAN? Are the interpolations in the latent space of FreGAN smooth? It is not possible to answer with the provided figures and tables.
In addition to FID or KID, I suggest adding a metric that explicitly measures the diversity of generated images, as done in some previous works. One can choose pairwise LPIPS as in [2] (tables G,H), or intra-cluster LPIPS as in [3] (table 2). Moreover, it would be good to measure the average LPIPS from generated images to the nearest example from the training set, which would help to assess the degree of memorization of the training set ([4], table 8). Lack of diversity is also partially supported by low recall across different datasets reported in the supplementary materials. On the qualitative side, it would be good to add examples of latent space interpolations as in [4] (figure 5) and the nearest real neighbors to some of generated images ([4], figure 12-13). Overall, without this analysis, given the size of the used datasets, only improving FID and KID is not a convincing result to me.
[2 - Claim (major concern)] The paper claims that the proposed method alleviates the unhealthy competition between G and D (lines 11, 47, 54) in the low data regime. The caption of figure 5 says that the FreGAN generator can better deceive the discriminator. The justification for the claim is claimed in the loss curves plot in figure 5(a).
In fact, what is seen in figure 5(a) seems to say the opposite. The discriminator loss of FreGAN is lower than the one of FastGAN (even though it has an additional term L^HF). This indicates that the discriminator can dissect real and fake images more confidently. It would be interesting to see the curves of the discriminator outputs during training (e.g., as in [5], figure 1(b-c)), where this effect should be more visible. Consequently, based on the plot 5(a), the FreGAN generator struggles to fool the discriminator more than FastGAN. This is confirmed by the larger G loss for FreGAN. It is thus not clear how the better GAN equilibrium is quantified by the authors, so one of the main claims is not supported.
[3.1 – Clarity (major concern)] Generally, it is not clear why the technical proposals of the paper are introduced as techniques for training GANs in low data regimes. The frequency bias for GANs exists not only when the training data size is limited [6]. In principle, any GAN model should benefit from generating images with better spectral properties (e.g., the motivation in lines 33-39 also fits to training GANs on large datasets). The explanation on mitigating unhealthy G and D competition is, in my opinion, unsupported (see Weaknesses-2).
Why are the introduced modifications tested only in low-data regimes? Do the proposed techniques help to improve the quality of images or spectral properties of GANs trained on large datasets?
[3.2 – Related work (major concern)] The proposed method is by design aimed to improve the quality of images in the frequency domain. There are other works with similar motivation (e.g., [6] as an overview). These methods can be potentially placed instead of the techniques introduced in section 3.2-3.4. It is not clear whether the effect of the proposed techniques could not be achieved with already existing techniques mitigating spectral biases of GANs. A proper comparison to previous works is expected.
[3.3 – Evaluation (major concern)] The proposed method is aimed to improve the quality of images in the frequency domain, but this aspect is not evaluated quantitatively. The only provided comparisons of high/low frequencies are visual, which makes it subjective, especially since ground truth spectrums to compare to are not available. Quantitative assessment of GAN images spectrums was approached before, for example by measuring the accuracy of a binary classifier trained on spectrum features of real and fake images [7]. A proper quantitative comparison of spectral properties of images for different models is expected.
[4 – Clarity (moderate concern)] The motivation of the frequency alignment loss (lines 151-161) is not clear. Firstly, what is meant by “G can only synthesize arbitrary frequency signals”? The synthesis of G is guided with the discriminator loss, which provides supervision on the image realism, which includes the intensity of details of different frequencies. So they should not be arbitrary. Secondly, why is HDA called a “regularizer for D”, while it is included only to the objective of G?
Finally, eq. (4) is a form of a reconstruction loss, but it is computed between D’s features of a real image and G’s features of a randomly sampled fake image. These images are not necessarily aligned spatially, so it is not clear why imposing a pixel-wise L1 reconstruction loss is meaningful. Please explain.
--- References ---
[1] Few-Shot Adaptation of Generative Adversarial Networks. Robb et al. Arxiv, 2020.
[2] Generating Novel Scene Compositions from Single Images and Videos. Sushko et al. Arxiv, 2021.
[3] Few-shot Image Generation via Cross-domain Correspondence. Ojha et al. CVPR 2021.
[4] Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis. Liu et al. ICLR 2021.
[5] Training Generative Adversarial Networks with Limited Data. Karras et al. NeurIPS 2020.
[6] On the Frequency Bias of Generative Models. Schwarz et al. NeurIPS 2021.
[7] Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral Distributions. Durall et al. CVPR 2020.
The authors have adequately addressed the limitations and potential negative societal impact of their work.


Review Point: --- [1 - Evaluation (major concern)] In few-shot image synthesis, image quality metrics (FID, KID) can be often improved at the cost of the synthesis diversity [1] (figure 4). For example, the model that perfectly reproduces the training set can achieve near-to-zero FID and KID. The diversity aspect is not evaluated in this work at all. This makes the whole evaluation of the paper unconvincing. Does FreGAN improve FID of FastGAN at the cost of diversity? Is FreGAN memorizing training images more than FastGAN? Are the interpolations in the latent space of FreGAN smooth? It is not possible to answer with the provided figures and tables. In addition to FID or KID, I suggest adding a metric that explicitly measures the diversity of generated images, as done in some previous works. One can choose pairwise LPIPS as in [2] (tables G,H), or intra-cluster LPIPS as in [3] (table 2). Moreover, it would be good to measure the average LPIPS from generated images to the nearest example from the training set, which would help to assess the degree of memorization of the training set ([4], table 8). Lack of diversity is also partially supported by low recall across different datasets reported in the supplementary materials. On the qualitative side, it would be good to add examples of latent space interpolations as in [4] (figure 5) and the nearest real neighbors to some of generated images ([4], figure 12-13). Overall, without this analysis, given the size of the used datasets, only improving FID and KID is not a convincing result to me. [2 - Claim (major concern)] The paper claims that the proposed method alleviates the unhealthy competition between G and D (lines 11, 47, 54) in the low data regime. The caption of figure 5 says that the FreGAN generator can better deceive the discriminator. The justification for the claim is claimed in the loss curves plot in figure 5(a). In fact, what is seen in figure 5(a) seems to say the opposite. The discriminator loss of FreGAN is lower than the one of FastGAN (even though it has an additional term L^HF). This indicates that the discriminator can dissect real and fake images more confidently. It would be interesting to see the curves of the discriminator outputs during training (e.g., as in [5], figure 1(b-c)), where this effect should be more visible. Consequently, based on the plot 5(a), the FreGAN generator struggles to fool the discriminator more than FastGAN. This is confirmed by the larger G loss for FreGAN. It is thus not clear how the better GAN equilibrium is quantified by the authors, so one of the main claims is not supported.
Review Point: [3.1 – Clarity (major concern)] Generally, it is not clear why the technical proposals of the paper are introduced as techniques for training GANs in low data regimes. The frequency bias for GANs exists not only when the training data size is limited [6]. In principle, any GAN model should benefit from generating images with better spectral properties (e.g., the motivation in lines 33-39 also fits to training GANs on large datasets). The explanation on mitigating unhealthy G and D competition is, in my opinion, unsupported (see Weaknesses-2). Why are the introduced modifications tested only in low-data regimes? Do the proposed techniques help to improve the quality of images or spectral properties of GANs trained on large datasets?
Review Point: [3.2 – Related work (major concern)] The proposed method is by design aimed to improve the quality of images in the frequency domain. There are other works with similar motivation (e.g., [6] as an overview). These methods can be potentially placed instead of the techniques introduced in section 3.2-3.4. It is not clear whether the effect of the proposed techniques could not be achieved with already existing techniques mitigating spectral biases of GANs. A proper comparison to previous works is expected.
Review Point: [3.3 – Evaluation (major concern)] The proposed method is aimed to improve the quality of images in the frequency domain, but this aspect is not evaluated quantitatively. The only provided comparisons of high/low frequencies are visual, which makes it subjective, especially since ground truth spectrums to compare to are not available. Quantitative assessment of GAN images spectrums was approached before, for example by measuring the accuracy of a binary classifier trained on spectrum features of real and fake images [7]. A proper quantitative comparison of spectral properties of images for different models is expected. [4 – Clarity (moderate concern)] The motivation of the frequency alignment loss (lines 151-161) is not clear. Firstly, what is meant by “G can only synthesize arbitrary frequency signals”? The synthesis of G is guided with the discriminator loss, which provides supervision on the image realism, which includes the intensity of details of different frequencies. So they should not be arbitrary. Secondly, why is HDA called a “regularizer for D”, while it is included only to the objective of G? Finally, eq. (4) is a form of a reconstruction loss, but it is computed between D’s features of a real image and G’s features of a randomly sampled fake image. These images are not necessarily aligned spatially, so it is not clear why imposing a pixel-wise L1 reconstruction loss is meaningful. Please explain. --- References --- [1] Few-Shot Adaptation of Generative Adversarial Networks. Robb et al. Arxiv, 2020. [2] Generating Novel Scene Compositions from Single Images and Videos. Sushko et al. Arxiv, 2021. [3] Few-shot Image Generation via Cross-domain Correspondence. Ojha et al. CVPR 2021. [4] Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis. Liu et al. ICLR 2021. [5] Training Generative Adversarial Networks with Limited Data. Karras et al. NeurIPS 2020. [6] On the Frequency Bias of Generative Models. Schwarz et al. NeurIPS 2021. [7] Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral Distributions. Durall et al. CVPR 2020. The authors have adequately addressed the limitations and potential negative societal impact of their work.
==================================================

Focused review:

* The proposed approach is heuristic. The main downside of heuristic defenses (unlike certified defenses) is that they are often easily broken by an adaptive attacker. With this in mind a proper evaluation of a heuristic defense warrants a strong attempt to break it (by the authors) by proposing an attack that's tailored specifically for it. Without such evidence it is not clear whether this defense would be useful in practice. For example, it is relatively straightforward to add an additional term in Mettack's loss that encourages adversarial edges between nodes with similar representations. * Since their approach relies on the similarity of hidden representations which is correlated with the similarity of the node features, an adaptive attacker can take this into account and modify the features to avoid detection. This limitation should be discussed. * As another adaptive strategy the authors can easily modify Mettack, adding the importance scores and computing the meta-gradients for the modified (rather than the standard) GCN. * The comparison with the baselines is for a single setting of the hyperparameters. Table 3 (right) shows the performance of GNNGuard for different budgets but there is no comparison with other baselines. Since it's not clear what is a reasonable threat model in practice comparison for e.g. different budgets would be insightful. * The defense is limited to networks with homophily. This is however, clearly stated by the authors and a reasonable assumption in practice.

Review Point: * The proposed approach is heuristic. The main downside of heuristic defenses (unlike certified defenses) is that they are often easily broken by an adaptive attacker. With this in mind a proper evaluation of a heuristic defense warrants a strong attempt to break it (by the authors) by proposing an attack that's tailored specifically for it. Without such evidence it is not clear whether this defense would be useful in practice. For example, it is relatively straightforward to add an additional term in Mettack's loss that encourages adversarial edges between nodes with similar representations.
Review Point: * Since their approach relies on the similarity of hidden representations which is correlated with the similarity of the node features, an adaptive attacker can take this into account and modify the features to avoid detection. This limitation should be discussed.
Review Point: * As another adaptive strategy the authors can easily modify Mettack, adding the importance scores and computing the meta-gradients for the modified (rather than the standard) GCN.
Review Point: * The comparison with the baselines is for a single setting of the hyperparameters. Table 3 (right) shows the performance of GNNGuard for different budgets but there is no comparison with other baselines. Since it's not clear what is a reasonable threat model in practice comparison for e.g. different budgets would be insightful.
Review Point: * The defense is limited to networks with homophily. This is however, clearly stated by the authors and a reasonable assumption in practice.
==================================================

Focused review:

- On including external forces that are not part of the original humanoid design: I would first like to acknowledge that the idea of having additional forces help the humanoid agent match the dynamics of a more complex human body is very nice and the results look amazing. However, I am having difficulty extrapolating this concept to a more general setup. The first example that comes in mind is an agent that **should** have a difficult time following certain motions. For example, an agent A which may be able to generate less torque than agent B should not be able to do certain moves / tasks, but in this setup, the additional forces will help agent A do what only agent B is able to do. This will take away from the realism component of the motions which agent A can perform. The way I understand this method is to be a type of training wheel agents are able to have to achieve the more complex human motion, however, these training wheels should be removed eventually. Another example is of extending this method to help agents interact with the world around them where the residual forces might intervene while the agent is, say, pushing a block to be able to push it as the imitation source (human or another more complex agent). I would appreciate it if the authors can comment on this, and please point out if I am missing something. - Kinematic policy naming: I feel the naming of “kinematic policy” is not completely accurate. As far as I know, we refer to policies as functions that map states to actions. In this case, the kinematic policy looks more like a conditional generative model of human motion (i.e., Equation 5). In addition, the overall method proposed in this paper is more like a model based reinforcement learning approach for imitation where the model is the human motion synthesis model previously mentioned. I would recommend the authors to rename the kinematic policy to something like “kinematic model” to prevent any confusions where readers may expect an actual policy. - Additional evaluations: Since there is a generative model of human motion, it would be good to present an evaluation of the diversity of generation for the models trained with CMU Mocap and Human 3.6M. I can see multiple possible futures in the supplementary video, but numbers would be good to have too.

Review Point: - On including external forces that are not part of the original humanoid design: I would first like to acknowledge that the idea of having additional forces help the humanoid agent match the dynamics of a more complex human body is very nice and the results look amazing. However, I am having difficulty extrapolating this concept to a more general setup. The first example that comes in mind is an agent that **should** have a difficult time following certain motions. For example, an agent A which may be able to generate less torque than agent B should not be able to do certain moves / tasks, but in this setup, the additional forces will help agent A do what only agent B is able to do. This will take away from the realism component of the motions which agent A can perform. The way I understand this method is to be a type of training wheel agents are able to have to achieve the more complex human motion, however, these training wheels should be removed eventually. Another example is of extending this method to help agents interact with the world around them where the residual forces might intervene while the agent is, say, pushing a block to be able to push it as the imitation source (human or another more complex agent). I would appreciate it if the authors can comment on this, and please point out if I am missing something.
Review Point: - Kinematic policy naming: I feel the naming of “kinematic policy” is not completely accurate. As far as I know, we refer to policies as functions that map states to actions. In this case, the kinematic policy looks more like a conditional generative model of human motion (i.e., Equation 5). In addition, the overall method proposed in this paper is more like a model based reinforcement learning approach for imitation where the model is the human motion synthesis model previously mentioned. I would recommend the authors to rename the kinematic policy to something like “kinematic model” to prevent any confusions where readers may expect an actual policy.
Review Point: - Additional evaluations: Since there is a generative model of human motion, it would be good to present an evaluation of the diversity of generation for the models trained with CMU Mocap and Human 3.6M. I can see multiple possible futures in the supplementary video, but numbers would be good to have too.
==================================================

Focused review:

Weaknesses: - The paper can be quite hard to follow, see the clarity section below. - Overall, the framework seems incremental. There does not seem to be a big distinction from [1], aside from parameterizing the dirichlet energy with learnable weights. The theoretical contributions appear minor and mirror those previously established in [2].
Clarity: The paper was hard to read and contained a number of typos. I have noted some here, but the manuscript contains others:
page 1 abstract: “requires a regularization” -> “requires regularization”
page 1: “over-parametric” -> “overparameterized”
page 2: “The low-rank” -> “Low-rankness”
page 2: “Furthermore, We” -> “Furthermore, we”
page 3: The functionals
R
W
i
are missing the trace
tr
(
⋅
)
.
page 3: “Another expected” -> “Another unexpected”
page 5: “Specialy, we sigh two of
L
c
(
t
=
4000
)
out”
page 6: “Air-Net Adaptive to Both Varies Data” should “varies” be “varying”?
page 6 Figure 1: “the
t
-th row” -> “the
i
-th row”
page 7: “reminding missing” -> “remaining missing”
Novelty and significance: The work seems to be an extension of [1], which also employed a deep matrix factorization approach to solve matrix completion problems and incorporated the dirichlet energy as an explicit regularizer. In that case, however, the laplacian was not learned, which seems to be the crucial difference. Some components of the theory is also borrowed from Arora et al [2].
General comments:
At the top of page 3, it says that the structure of the network parameterizing the Laplacian matrix is discussed in A.4, but it appears to actually be in A.3. There is also no discussion on why the network is chosen in this particular fashion. It would be good to talk about why this network structure is used, e.g. the use of a softmax-type parameterization. Also, under this parameterization, is it even possible for the learned
L
i
=
0
, the trivial solution?
It feels misleading to say in Remark 1 that Theorem 2 requires no restriction on
T
i
or
X
. The first assumption is that all columns of
T
i
(
X
)
are unit normed and the entries are positive. If, for example,
T
i
(
X
)
:=
X
, then this means that for the result to hold, the columns of
X
are unit normed and all of its entries are non-negative.
The matrix
C
k
,
l
should be defined in Theorem 2, since it arises in the definition of
γ
.
I am a bit puzzled by Figure 1. This seems to indicate that as time progresses, the network learns that there should be less and less similarity between rows/columns, as indicated by the lack of dark regions as
t
increases. What is the intuition for this? Shouldn't some notion of self-similarity be learned instead?
[1] Amit Boyarski, Sanketh Vedula, and Alex Bronstein. Spectral geometric matrix completion. MSML 2021
[2] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. NeurIPS 2019


Review Point: - The paper can be quite hard to follow, see the clarity section below.
Review Point: - Overall, the framework seems incremental. There does not seem to be a big distinction from [1], aside from parameterizing the dirichlet energy with learnable weights. The theoretical contributions appear minor and mirror those previously established in [2]. Clarity: The paper was hard to read and contained a number of typos. I have noted some here, but the manuscript contains others: page 1 abstract: “requires a regularization” -> “requires regularization” page 1: “over-parametric” -> “overparameterized” page 2: “The low-rank” -> “Low-rankness” page 2: “Furthermore, We” -> “Furthermore, we” page 3: The functionals R W i are missing the trace tr ( ⋅ ) . page 3: “Another expected” -> “Another unexpected” page 5: “Specialy, we sigh two of L c ( t = 4000 ) out” page 6: “Air-Net Adaptive to Both Varies Data” should “varies” be “varying”? page 6 Figure 1: “the t -th row” -> “the i -th row” page 7: “reminding missing” -> “remaining missing” Novelty and significance: The work seems to be an extension of [1], which also employed a deep matrix factorization approach to solve matrix completion problems and incorporated the dirichlet energy as an explicit regularizer. In that case, however, the laplacian was not learned, which seems to be the crucial difference. Some components of the theory is also borrowed from Arora et al [2]. General comments: At the top of page 3, it says that the structure of the network parameterizing the Laplacian matrix is discussed in A.4, but it appears to actually be in A.3. There is also no discussion on why the network is chosen in this particular fashion. It would be good to talk about why this network structure is used, e.g. the use of a softmax-type parameterization. Also, under this parameterization, is it even possible for the learned L i = 0 , the trivial solution? It feels misleading to say in Remark 1 that Theorem 2 requires no restriction on T i or X . The first assumption is that all columns of T i ( X ) are unit normed and the entries are positive. If, for example, T i ( X ) := X , then this means that for the result to hold, the columns of X are unit normed and all of its entries are non-negative. The matrix C k , l should be defined in Theorem 2, since it arises in the definition of γ . I am a bit puzzled by Figure 1. This seems to indicate that as time progresses, the network learns that there should be less and less similarity between rows/columns, as indicated by the lack of dark regions as t increases. What is the intuition for this? Shouldn't some notion of self-similarity be learned instead? [1] Amit Boyarski, Sanketh Vedula, and Alex Bronstein. Spectral geometric matrix completion. MSML 2021 [2] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. NeurIPS 2019
==================================================

Focused review:

Weakness:     My main concern is about the problem itself. The proposed method uses ANN to accelerate NN query, and claims achieving some "constant approximation". The "approximation" here means that in each step the two merged clusters differ only by constant times of the minimum "dissimilarity". However, with such relaxation, the output hierarchical tree can be completely different from the exact solution. It is not clear from the paper how to compare the quality of these two hierarchical trees. Obviously, if the two trees are not close,  the acceleration would not  make much sense.  Although  the proposed algorithm runs faster, it may not achieve the desired goal.         The paper does give some empirical evidence (Table 1) to suggest that the approximate solution is of "good quality". But the experiment is conducted only on a few small datasets and thus not very convincing. There are a few papers discussing how to define a suitable objective function for the HC problem (e.g. [1] and [2]). I   would like to see some discussions and comparisons with those approaches.     - [1] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. STOC'16    - [2] Vincent Cohen-Addad et al. Hierarchical Clustering: Objective Functions and Algorithms. SODA'17 

Review Point: My main concern is about the problem itself. The proposed method uses ANN to accelerate NN query, and claims achieving some "constant approximation". The "approximation" here means that in each step the two merged clusters differ only by constant times of the minimum "dissimilarity". However, with such relaxation, the output hierarchical tree can be completely different from the exact solution. It is not clear from the paper how to compare the quality of these two hierarchical trees. Obviously, if the two trees are not close, the acceleration would not make much sense. Although the proposed algorithm runs faster, it may not achieve the desired goal. The paper does give some empirical evidence (Table 1) to suggest that the approximate solution is of "good quality". But the experiment is conducted only on a few small datasets and thus not very convincing. There are a few papers discussing how to define a suitable objective function for the HC problem (e.g. [1] and [2]). I would like to see some discussions and comparisons with those approaches.
Review Point: - [1] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. STOC'16 - [2] Vincent Cohen-Addad et al. Hierarchical Clustering: Objective Functions and Algorithms. SODA'17
==================================================

Focused review:

Weaknesses:
I doubt the novelty of this paper. Injecting noises to the input or to the learned representation is not a new thing. There are already several papers covering similar topics, such as Ref1 and Ref2.
All experiments are carried out on very simple and easy datasets such as MNIST and FMNIST. Thus, it is not quite sure how reliable these results are and whether it is a safe way to draw conclusions from them.
Also for the comparisons, the authors only compare models with Dropout and L2 regularization. However, as the paper claims to learn a robust representation, the authors should consider comparing the proposed method with other robust models and other more complicated datasets such as MNIST-C and ImageNet-C for example.
Even on the simple dataset, the proposed method cannot consistently outperform models trained with Dropout. Thus, I do worry about the benefits of this proposed method can bring us.
There are some typos in the writing.
Ref1: Liu, Aishan, et al. "Training robust deep neural networks via adversarial noise propagation." IEEE Transactions on Image Processing 30 (2021): 5769-5781.
Ref2: Rusak, Evgenia, et al. "A simple way to make neural networks robust against diverse image corruptions." European Conference on Computer Vision. Springer, Cham, 2020.


Review Point: I doubt the novelty of this paper. Injecting noises to the input or to the learned representation is not a new thing. There are already several papers covering similar topics, such as Ref1 and Ref2. All experiments are carried out on very simple and easy datasets such as MNIST and FMNIST. Thus, it is not quite sure how reliable these results are and whether it is a safe way to draw conclusions from them. Also for the comparisons, the authors only compare models with Dropout and L2 regularization. However, as the paper claims to learn a robust representation, the authors should consider comparing the proposed method with other robust models and other more complicated datasets such as MNIST-C and ImageNet-C for example. Even on the simple dataset, the proposed method cannot consistently outperform models trained with Dropout. Thus, I do worry about the benefits of this proposed method can bring us. There are some typos in the writing.
Review Point: Ref1: Liu, Aishan, et al. "Training robust deep neural networks via adversarial noise propagation." IEEE Transactions on Image Processing 30 (2021): 5769-5781.
Review Point: Ref2: Rusak, Evgenia, et al. "A simple way to make neural networks robust against diverse image corruptions." European Conference on Computer Vision. Springer, Cham, 2020.
==================================================

Focused review:

Weakness main comments: • what is the advantage of using a differentiable LP layer (GNN and a LP solver) as a high-level policy, shown in Eq. 10?
– compare it to [1] that considers the LP optimization layer as a meta-environment?
– compare it to an explicit task assignment protocol (e.g. not implicit).
E.g. a high-level policy that directly outputs task weightings instead of the intermediary C matrix?
• How does this method address sparse reward problems in a better way? From the experiments, this does not support well. in practice, the proposed method requires sub-task-specific rewards to be specified, which would be similar to providing a dense reward signal that includes rewards for reaching sub-goals. If given the sum of low-level reward as the global reward, will the other methods (Qmix) solve the sparse-reward tasks as well?
minor comments: • It is hard to determine whether the solution to the matching problem (learned agent-task score matrix C) optimized by LP is achieving global perspective over the learning process.
• When the lower-level policies are also trained online, the learning could be unstable. Details on how to solve the instability in hierarchical learning are missing.
• What is the effect of the use of hand-defined tasks on performance? what is the effect of the algorithm itself? maybe do an ablation study.
• Section 5.2 ”training low-level actor-critic” should be put in the main text.
[1] Carion N, Usunier N, Synnaeve G, et al. A structured prediction approach for generalization in cooperative multi-agent reinforcement learning[J]. Advances in neural information processing systems, 2019, 32: 8130-8140.


Review Point: main comments:• what is the advantage of using a differentiable LP layer (GNN and a LP solver) as a high-level policy, shown in Eq. 10? – compare it to [1] that considers the LP optimization layer as a meta-environment? – compare it to an explicit task assignment protocol (e.g. not implicit). E.g. a high-level policy that directly outputs task weightings instead of the intermediary C matrix?
Review Point: • How does this method address sparse reward problems in a better way? From the experiments, this does not support well. in practice, the proposed method requires sub-task-specific rewards to be specified, which would be similar to providing a dense reward signal that includes rewards for reaching sub-goals. If given the sum of low-level reward as the global reward, will the other methods (Qmix) solve the sparse-reward tasks as well? minor comments:
Review Point: • It is hard to determine whether the solution to the matching problem (learned agent-task score matrix C) optimized by LP is achieving global perspective over the learning process.
Review Point: • When the lower-level policies are also trained online, the learning could be unstable. Details on how to solve the instability in hierarchical learning are missing.
Review Point: • What is the effect of the use of hand-defined tasks on performance? what is the effect of the algorithm itself? maybe do an ablation study.
Review Point: • Section 5.2 ”training low-level actor-critic” should be put in the main text. [1] Carion N, Usunier N, Synnaeve G, et al. A structured prediction approach for generalization in cooperative multi-agent reinforcement learning[J]. Advances in neural information processing systems, 2019, 32: 8130-8140.
==================================================

Focused review:

The novelty of the method seems to be limited, the author should compare to other similar works. 1. The worst case optimization framework is similar to DRO [1*]. The difference is that the author tries to optimize towards the worst case exposure strategy rather than group performance in DRO. 2. [2*] also proposes a dual learning algorithm to learn simultaneously the unbiased exposure distribution and the user preference. Some more experiments may be conducted against this type of work. In the experiments part, the paper's method shows minor improvement over POP as propensity weighting function. And I would like to see some explanations why in Table 1 g cannot be oracle in ACL, and why MLP/oracle is worse than MLP/Pop. [1*] Fairness Without Demographics in Repeated Loss Minimization, https://arxiv.org/abs/1806.08010 [2*] Unbiased Learning to Rank with Unbiased Propensity Estimation, sigir 2018

Review Point: The novelty of the method seems to be limited, the author should compare to other similar works.
Review Point: 1. The worst case optimization framework is similar to DRO [1*]. The difference is that the author tries to optimize towards the worst case exposure strategy rather than group performance in DRO.
Review Point: 2. [2*] also proposes a dual learning algorithm to learn simultaneously the unbiased exposure distribution and the user preference. Some more experiments may be conducted against this type of work. In the experiments part, the paper's method shows minor improvement over POP as propensity weighting function. And I would like to see some explanations why in Table 1 g cannot be oracle in ACL, and why MLP/oracle is worse than MLP/Pop. [1*] Fairness Without Demographics in Repeated Loss Minimization, https://arxiv.org/abs/1806.08010 [2*] Unbiased Learning to Rank with Unbiased Propensity Estimation, sigir 2018
==================================================

Focused review:

WEAKNESSES:
• The strong results come with a major caveat: the ViT-small and Swin-tiny architectures have 22M and 29M parameters respectively, while the compared baselines are almost entirely based on ResNet12, which by my recollection has only 12M parameters. While this is still less than the widely-used WRN-28-10 backbone (36.5M params), I worry that the comparisons presented in the paper are apples-to-oranges. The difference in model size should be discussed and addressed.
• Use of vision transformers for few shot classification deserves an empirical study all on its own. Understandably this is not provided here, but because of this it is unclear to what degree improvement is coming from the token reweighting scheme (in theory compatible with existing convolutional architectures) vs the vision transformer backbone (in theory compatible with existing few-shot classifiers). For example, how does the token reweighting scheme compare to simply training a linear classifier head on the support features from a vision transformer? Admittedly, a full comparison along both these axes would be clearly out of scope here.
• Similarly, there is no ablation study provided for the impact of token reweighting vs the logsumexp aggregation scheme. How much better is logsumexp aggregation than direct addition, for example, which would correspond to basic prototype comparison with reweighted averages on each prototype? More broadly, it appears that the token reweighting scheme is broadly compatible with many existing token-to-token classifiers such as CTX and FRN, and it is not clear how the logsumexp aggregator compares.
• More generally, the approach, while straightforward and sensible, does contain a few design choices that are not fully explained or empirically justified (for example, in addition to above, the choice of token similarity metric).
• A slightly relevant omitted citation: the masked inner token reweighting scheme might possibly owe some conceptual debt to Batch Folding from [Few Shot Learning with Localization in Realistic Settings, CVPR2019], which also models a support-to-support classification task with an identical image-masked leave-one-out scheme (though admittedly implemented quite differently).
The analysis of 1-shot effectiveness and discussion of smaller training datasets is insightful. The entanglement of vision transformer benefits with token reweighting benefits in presented results is not discussed. Societal impacts are not discussed, though do not extend beyond those of few-shot learning in general.


Review Point: WEAKNESSES:• The strong results come with a major caveat: the ViT-small and Swin-tiny architectures have 22M and 29M parameters respectively, while the compared baselines are almost entirely based on ResNet12, which by my recollection has only 12M parameters. While this is still less than the widely-used WRN-28-10 backbone (36.5M params), I worry that the comparisons presented in the paper are apples-to-oranges. The difference in model size should be discussed and addressed.
Review Point: • Use of vision transformers for few shot classification deserves an empirical study all on its own. Understandably this is not provided here, but because of this it is unclear to what degree improvement is coming from the token reweighting scheme (in theory compatible with existing convolutional architectures) vs the vision transformer backbone (in theory compatible with existing few-shot classifiers). For example, how does the token reweighting scheme compare to simply training a linear classifier head on the support features from a vision transformer? Admittedly, a full comparison along both these axes would be clearly out of scope here.
Review Point: • Similarly, there is no ablation study provided for the impact of token reweighting vs the logsumexp aggregation scheme. How much better is logsumexp aggregation than direct addition, for example, which would correspond to basic prototype comparison with reweighted averages on each prototype? More broadly, it appears that the token reweighting scheme is broadly compatible with many existing token-to-token classifiers such as CTX and FRN, and it is not clear how the logsumexp aggregator compares.
Review Point: • More generally, the approach, while straightforward and sensible, does contain a few design choices that are not fully explained or empirically justified (for example, in addition to above, the choice of token similarity metric).
Review Point: • A slightly relevant omitted citation: the masked inner token reweighting scheme might possibly owe some conceptual debt to Batch Folding from [Few Shot Learning with Localization in Realistic Settings, CVPR2019], which also models a support-to-support classification task with an identical image-masked leave-one-out scheme (though admittedly implemented quite differently). The analysis of 1-shot effectiveness and discussion of smaller training datasets is insightful. The entanglement of vision transformer benefits with token reweighting benefits in presented results is not discussed. Societal impacts are not discussed, though do not extend beyond those of few-shot learning in general.
==================================================

Focused review:

Weaknesses:
1.) Originality -- while the authors claim to propose a new notion of sub-optimality gap, this notion already exists in prior work up to a normalization factor of horizon. The novel contributions of the paper seem to mostly be in terms of removing spurious factors of horizon in upper and lower bounds from prior work. It is worth mentioning that the lower bound stated in Theorem 1 can not be directly compared to regret minimization lower bounds in prior work as the regret minimization lower bounds are asymptotic and regret minimization and PAC RL are not really aligned in their objectives as pointed out by Wagenmaker et al. 2022.
2.) Clarity and significance -- I do not find the claims of matching upper and lower bounds true. It is important to note that Theorem 1 holds in a different setting from Theorem 3, that is in Theorem 1 there are no assumptions made about boundedness of the rewards, and in fact this is a very important part of the proof, making the result similar in spirit to the lower bound in Theorem 4.5 (Dann et al. 2021). The results in Theorem 3, in my understanding, hold for the more standard setting of bounded rewards. It is important to note that there could be a large gap between the two settings, making the claims of matching upper and lower bounds inaccurate. In particular, for the bounded rewards setting, there exist MDPs in which state-action pairs need not be visited even once for to be eliminated. This is due to the fact, that it is possible to eliminate all actions on paths to such state-action pairs before ever visiting such state-action pairs and hence there is no alternative MDP as in Lemma 8 of the current paper.
Given that the paper does not really establish lower bounds in the more interesting setting of bounded rewards and the upper bounds mostly seem to remove horizon dependent factors, I am unsure of how strong the contributions of this work are.
References:
Wagenmaker, Andrew J., Max Simchowitz, and Kevin Jamieson. "Beyond no regret: Instance-dependent pac reinforcement learning." Conference on Learning Theory. PMLR, 2022.
Dann, Christoph, et al. "Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning." Advances in Neural Information Processing Systems 34 (2021): 1-12.
This work is theoretical in nature and future societal impacts are hard to judge.


Review Point: 1.) Originality -- while the authors claim to propose a new notion of sub-optimality gap, this notion already exists in prior work up to a normalization factor of horizon. The novel contributions of the paper seem to mostly be in terms of removing spurious factors of horizon in upper and lower bounds from prior work. It is worth mentioning that the lower bound stated in Theorem 1 can not be directly compared to regret minimization lower bounds in prior work as the regret minimization lower bounds are asymptotic and regret minimization and PAC RL are not really aligned in their objectives as pointed out by Wagenmaker et al. 2022.
Review Point: 2.) Clarity and significance -- I do not find the claims of matching upper and lower bounds true. It is important to note that Theorem 1 holds in a different setting from Theorem 3, that is in Theorem 1 there are no assumptions made about boundedness of the rewards, and in fact this is a very important part of the proof, making the result similar in spirit to the lower bound in Theorem 4.5 (Dann et al. 2021). The results in Theorem 3, in my understanding, hold for the more standard setting of bounded rewards. It is important to note that there could be a large gap between the two settings, making the claims of matching upper and lower bounds inaccurate. In particular, for the bounded rewards setting, there exist MDPs in which state-action pairs need not be visited even once for to be eliminated. This is due to the fact, that it is possible to eliminate all actions on paths to such state-action pairs before ever visiting such state-action pairs and hence there is no alternative MDP as in Lemma 8 of the current paper. Given that the paper does not really establish lower bounds in the more interesting setting of bounded rewards and the upper bounds mostly seem to remove horizon dependent factors, I am unsure of how strong the contributions of this work are. References: Wagenmaker, Andrew J., Max Simchowitz, and Kevin Jamieson. "Beyond no regret: Instance-dependent pac reinforcement learning." Conference on Learning Theory. PMLR, 2022. Dann, Christoph, et al. "Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning." Advances in Neural Information Processing Systems 34 (2021): 1-12. This work is theoretical in nature and future societal impacts are hard to judge.
==================================================

Focused review:

- Some aspects of the proposed approach require more explanation (please see the "Clarity" section for details). - The accuracy of BNNs is worse than the real-valued networks (please see the "Correctness" section for details).

Review Point: - Some aspects of the proposed approach require more explanation (please see the "Clarity" section for details).
Review Point: - The accuracy of BNNs is worse than the real-valued networks (please see the "Correctness" section for details).
==================================================

Focused review:

Weaknesses: • The novelty of this paper might be slightly insufficient. My understanding is that the proposed model can be considered as an extension of previous GraphCL. Despite the introduction of cross-view reconstruction, it seems that cross-view reconstruction exists in previous graph representation learning studies. • The performance might be still somewhat limited. The authors compared many methods under different settings with GraphCV on multiple datasets. However, it can be found that GraphCV is comparable to or even underperforms the best baseline in many cases. While GraphCV has better performance than baselines in some cases, the advantages are not significant enough. Other questions: • Why are two data augmentation graphs needed? • The proposed model is involved in multiple losses. How to determine the weight values of different losses? Could the fixed weight values be generalized to different datasets?


Review Point: • The novelty of this paper might be slightly insufficient. My understanding is that the proposed model can be considered as an extension of previous GraphCL. Despite the introduction of cross-view reconstruction, it seems that cross-view reconstruction exists in previous graph representation learning studies.
Review Point: • The performance might be still somewhat limited. The authors compared many methods under different settings with GraphCV on multiple datasets. However, it can be found that GraphCV is comparable to or even underperforms the best baseline in many cases. While GraphCV has better performance than baselines in some cases, the advantages are not significant enough. Other questions:
Review Point: • The proposed model is involved in multiple losses. How to determine the weight values of different losses? Could the fixed weight values be generalized to different datasets?
==================================================

Focused review:

Weakness
An important weakness of the paper is the evaluation (See "limitations" for more details).
The paper lacks clarity as well as many important details to make it self-contained and reproducible (See "limitations" for more details).
Phase and binaural rendering
The main limitation of the paper is the evaluation. Indeed, the only two objective metrics used to evaluate generate RIRs are insufficient to validate the central claim made by the authors in the abstract, namely: "continuous nature of NAFs enables us to render spatial acoustics for a listener at arbitrary locations".
The first metric is the RT60 error, which is a single scalar parameters that globally captures the reverberation time but ignores fine properties of the reverberation which typically vary across frequency. The second metric used is the mean squared error on the log-magnitude spectrogram of the RIR. This metric completely ignores the phase reconstruction. Though, correctly reconstructing the phase is known to be of crucial importance for rendering reverberation, and even more so for rendering spatial effects, in binaural settings. Indeed, the primary cue for sound source localization are phase differences across frequencies, which need to be very accurately rendered. The authors claim that their model is able to render multiple orientations of a binaural listener. However, the main notable effect of rotating a listener on the spot for a fixed source in a fixed environment will be on the phase differences between the two channels. The authors to propose to predict the phase via an instantaneous frequency representation, but the reconstructed phases and phase differences are never evaluation.
In short, the evaluation is currently insufficient to support the implicit strong claim that the proposed method is able to "render spatial acoustics" in the binaural settings. They merely convince that the method is able to accurately render monaural reverberation, which is a significantly easier (though highly non-trivial) task. This strong limitation is not properly addressed anywhere in the manuscript.
Clarity and precision
The writing is unprecise and handwavy at a number of places. It gives the general impression that the main background of the authors is in NeRF rather than in acoustics/audio signal processing.
L44: "an intractable number of rays are necessary" The term "intractable" is not precise enough here. Ray tracing is a widely used technique for acoustic simulation, for instance it was used to generate the SoundScape dataset [25] used by the authors themselves for training their method. In that paper, 200 rays are emitted from each source location. Is that considered an intractable number?
L47 "NAFs encode and represent an impulse-response in the Fourier frequency domain." -> actually, in the time-frequency domain, which is quite different.
L60: "The first approach encodes the sound field at a user-centric location by capturing the sound from spatially distributed sources [8,9,10,11]". It is not clear what the authors mean here by "capturing the sound from spatially distributed sources": capturing in what sense? How do those methods specifically differ from the proposed approach? Curiously, the 4 references given seem completely unrelated to each other and span 50 years of literature. The most recent one [11], from 2021, actually seems highly related to what the authors propose, and some more in-depth comparison with this work should be made in the paper.
L113 "Directly outputting the impulse-response v magnitude" -> this seems incorrect, the term "magnitude" should probably be removed.
L119 "due to the smooth nature of the frequency space" -> the authors probably mean "time-frequency space".
L122 "For phase, we use the instantaneous frequency (IF) representation proposed in [29]." : more details on the phase representation used should be provided to make the paper self-contained / reproducible. In particular, one cannot directly reconstruct the phase from the instantaneous frequency, since it is by definition the time derivative of the phase. Hence, a global phase is missing at each frequency. It is not clear how this is handled by the authors.
L147-150 on local geometric features: It is hard to understand what the authors are talking about here without reading in details the supplementary material. Would be good to make this part more self-explanatory.
The authors compare an approach using a "shared" feature grid vs. a "dual" feature grid, the former outperforming the latter. However, in the supplementary material (fig. A4), only the network architecture for a dual grid is presented, which is confusing.
Section 4.1: when presenting the two datasets, the authors omit to discuss their limitations, although both are in fact quite limited and do not allow testing the proposed approach in its full generality. The first one only employs 2D grids while the second one is monaural.
L217: "The bitrates are chosen to approach the storage costs for our NAF": This is not precise enough to understand how the baselines were implemented exactly. The authors should report results as a function of bitrates for the different methods to get a clearer picture of the benefit of the approach compared to standard techniques. Moreover, Table 3 seems to contradict this statement: here we see that the space consumption of NAF is vastly inferior to that of AAC and Opus.
Typos
L44: "an intractable number of rays are necessary to" -> is necessary.
L179 "on 6 representative scenes. Where" -> remove full stop
L212 "as well as using have" -> incorrect phrasing.


Review Point: An important weakness of the paper is the evaluation (See "limitations" for more details). The paper lacks clarity as well as many important details to make it self-contained and reproducible (See "limitations" for more details). Phase and binaural rendering The main limitation of the paper is the evaluation. Indeed, the only two objective metrics used to evaluate generate RIRs are insufficient to validate the central claim made by the authors in the abstract, namely: "continuous nature of NAFs enables us to render spatial acoustics for a listener at arbitrary locations". The first metric is the RT60 error, which is a single scalar parameters that globally captures the reverberation time but ignores fine properties of the reverberation which typically vary across frequency. The second metric used is the mean squared error on the log-magnitude spectrogram of the RIR. This metric completely ignores the phase reconstruction. Though, correctly reconstructing the phase is known to be of crucial importance for rendering reverberation, and even more so for rendering spatial effects, in binaural settings. Indeed, the primary cue for sound source localization are phase differences across frequencies, which need to be very accurately rendered. The authors claim that their model is able to render multiple orientations of a binaural listener. However, the main notable effect of rotating a listener on the spot for a fixed source in a fixed environment will be on the phase differences between the two channels. The authors to propose to predict the phase via an instantaneous frequency representation, but the reconstructed phases and phase differences are never evaluation. In short, the evaluation is currently insufficient to support the implicit strong claim that the proposed method is able to "render spatial acoustics" in the binaural settings. They merely convince that the method is able to accurately render monaural reverberation, which is a significantly easier (though highly non-trivial) task. This strong limitation is not properly addressed anywhere in the manuscript. Clarity and precision The writing is unprecise and handwavy at a number of places. It gives the general impression that the main background of the authors is in NeRF rather than in acoustics/audio signal processing.
Review Point: L44: "an intractable number of rays are necessary" The term "intractable" is not precise enough here. Ray tracing is a widely used technique for acoustic simulation, for instance it was used to generate the SoundScape dataset [25] used by the authors themselves for training their method. In that paper, 200 rays are emitted from each source location. Is that considered an intractable number? L47 "NAFs encode and represent an impulse-response in the Fourier frequency domain." -> actually, in the time-frequency domain, which is quite different.
Review Point: L60: "The first approach encodes the sound field at a user-centric location by capturing the sound from spatially distributed sources [8,9,10,11]". It is not clear what the authors mean here by "capturing the sound from spatially distributed sources": capturing in what sense? How do those methods specifically differ from the proposed approach? Curiously, the 4 references given seem completely unrelated to each other and span 50 years of literature. The most recent one [11], from 2021, actually seems highly related to what the authors propose, and some more in-depth comparison with this work should be made in the paper. L113 "Directly outputting the impulse-response v magnitude" -> this seems incorrect, the term "magnitude" should probably be removed. L119 "due to the smooth nature of the frequency space" -> the authors probably mean "time-frequency space". L122 "For phase, we use the instantaneous frequency (IF) representation proposed in [29]." : more details on the phase representation used should be provided to make the paper self-contained / reproducible. In particular, one cannot directly reconstruct the phase from the instantaneous frequency, since it is by definition the time derivative of the phase. Hence, a global phase is missing at each frequency. It is not clear how this is handled by the authors. L147-150 on local geometric features: It is hard to understand what the authors are talking about here without reading in details the supplementary material. Would be good to make this part more self-explanatory. The authors compare an approach using a "shared" feature grid vs. a "dual" feature grid, the former outperforming the latter. However, in the supplementary material (fig. A4), only the network architecture for a dual grid is presented, which is confusing. Section 4.1: when presenting the two datasets, the authors omit to discuss their limitations, although both are in fact quite limited and do not allow testing the proposed approach in its full generality. The first one only employs 2D grids while the second one is monaural.
Review Point: L217: "The bitrates are chosen to approach the storage costs for our NAF": This is not precise enough to understand how the baselines were implemented exactly. The authors should report results as a function of bitrates for the different methods to get a clearer picture of the benefit of the approach compared to standard techniques. Moreover, Table 3 seems to contradict this statement: here we see that the space consumption of NAF is vastly inferior to that of AAC and Opus. Typos L44: "an intractable number of rays are necessary to" -> is necessary. L179 "on 6 representative scenes. Where" -> remove full stop L212 "as well as using have" -> incorrect phrasing.
==================================================

Focused review:

Weaknesses: There are some experiments results I find not consistent with authors’ claims. 1. In table 7, combining TL-Align with Mixup, CutMix is worse than TL-Align+CutMix, does this mean TL-Align will not work well with Mixup? Can authors show separate Mixup w v.s. w/o TL-Align results? 2. In Table 2, TransMix result is shown as 80.1, the actual results from TransMix paper is 80.7, which is actually higher than this paper’ results 80.6. TransMix only uses last layer’s attention map where this paper proposes to use all layers, however the comparison results are kind of confusing here, can authors comment on that?


Review Point: There are some experiments results I find not consistent with authors’ claims.
Review Point: 1. In table 7, combining TL-Align with Mixup, CutMix is worse than TL-Align+CutMix, does this mean TL-Align will not work well with Mixup? Can authors show separate Mixup w v.s. w/o TL-Align results?
Review Point: 2. In Table 2, TransMix result is shown as 80.1, the actual results from TransMix paper is 80.7, which is actually higher than this paper’ results 80.6. TransMix only uses last layer’s attention map where this paper proposes to use all layers, however the comparison results are kind of confusing here, can authors comment on that?
==================================================

Focused review:

This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets.  At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this "data quality audit" work, because this work sets a precedent/"standard" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations. 
- It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
- Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - "Limitations" section in the Appendix could be moved to the "Impact Statement & Ethics" section. 

Review Point: This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets. At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this "data quality audit" work, because this work sets a precedent/"standard" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations.
Review Point: - It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
Review Point: - Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - "Limitations" section in the Appendix could be moved to the "Impact Statement & Ethics" section.
==================================================

Focused review:

1. It's not clear how important the details of the batch construction (Section 2.4) are to the eventual performance of the proposed approach. For example, when pre-training on Wikipedia, documents on the same topic in different languages are put in the same shard, imposing an indirect supervision towards the translation task. Similarly, when pre-training on News, only the first chunk of each document can be treated as target (Section 3, Data Pre-processing), again imposing a preference towards summarization. The consequences of these choices are not clear due to a lack of ablation studies. 2. The choice of some pre-training hyper-parameters (described in Section 3 pre-training) is not very well motivated. Does training twice on CC-News with varying # workers and a cyclical rate perform better than single-stage pre-training? 3. While the pre-training objective might play a role in the performance improvements observed here, MARGE is a relatively larger model with a different architecture from mBART / XLM-R. Ablating the effect of these architectural choices might convey more information regarding the effectiveness of the pre-training objective.

Review Point: 1. It's not clear how important the details of the batch construction (Section 2.4) are to the eventual performance of the proposed approach. For example, when pre-training on Wikipedia, documents on the same topic in different languages are put in the same shard, imposing an indirect supervision towards the translation task. Similarly, when pre-training on News, only the first chunk of each document can be treated as target (Section 3, Data Pre-processing), again imposing a preference towards summarization. The consequences of these choices are not clear due to a lack of ablation studies.
Review Point: 2. The choice of some pre-training hyper-parameters (described in Section 3 pre-training) is not very well motivated. Does training twice on CC-News with varying # workers and a cyclical rate perform better than single-stage pre-training?
Review Point: 3. While the pre-training objective might play a role in the performance improvements observed here, MARGE is a relatively larger model with a different architecture from mBART / XLM-R. Ablating the effect of these architectural choices might convey more information regarding the effectiveness of the pre-training objective.
==================================================

Focused review:

Weaknesses
Originality. This paper would be more substantial if the main algorithmic contribution weren't so tight to computational tricks to compute Knowledge Gradient or if the problem under investigation wasn't similar to REVI's [22] formulation. If I understood correctly, the main contribution is a speedup over REVI based on how KG is calculated. The theoretical analysis adds substance to the paper, but none of these results are ground-breaking.
Clarity. The presentation of section 2 could be a little bit more straightforward. There is unnecessary confusion about how information is captured by the inference procedure (the Gaussian process) and how decision-making is handled (construct an acquisition function).
Minors
115 - "integrating over tasks multiplies the computational burden" multiples -> increases
130 - "non-trivial modification to be able to account for how a sample affects similar tasks." Wouldn't a kernel that captures the correlation between similar tasks be enough to accomplish this?
133 - location of the peak P(x | ...) -> mutual information between both distributions (or RV), you can remove the word peak here.
149 - I don't think you meant to say that squared exponential or Matern is a kernel that factorizes here.
l89 - n_s? Shouldn't it be n_z?
After rebuttal I appreciate the author's detail response and I would strongly recommend the authors to add this level of clarity and detail to the original manuscript. Thanks


Review Point: Originality. This paper would be more substantial if the main algorithmic contribution weren't so tight to computational tricks to compute Knowledge Gradient or if the problem under investigation wasn't similar to REVI's [22] formulation. If I understood correctly, the main contribution is a speedup over REVI based on how KG is calculated. The theoretical analysis adds substance to the paper, but none of these results are ground-breaking. Clarity. The presentation of section 2 could be a little bit more straightforward. There is unnecessary confusion about how information is captured by the inference procedure (the Gaussian process) and how decision-making is handled (construct an acquisition function). Minors 115 - "integrating over tasks multiplies the computational burden" multiples -> increases 130 - "non-trivial modification to be able to account for how a sample affects similar tasks." Wouldn't a kernel that captures the correlation between similar tasks be enough to accomplish this?
Review Point: 133 - location of the peak P(x | ...) -> mutual information between both distributions (or RV), you can remove the word peak here.
Review Point: 149 - I don't think you meant to say that squared exponential or Matern is a kernel that factorizes here. l89 - n_s? Shouldn't it be n_z? After rebuttal I appreciate the author's detail response and I would strongly recommend the authors to add this level of clarity and detail to the original manuscript. Thanks
==================================================

Focused review:

Weaknesses: 1. The weakness of this paper lies in that although the proposed method is efficient compared to the directly unrolling method. The time complexity of both the baselines and this method should also be discussed so that readers can better balance these methods. 2. Although the module choice effect is shown in the experiments, there needs more analysis and instructions for readers to decide when to conduct finer-grained module split or must we conduct parameter-level module split with collecting more data in the developing set?
The authors point that the time complexity is the limitation of this method, but it needs more discussing as above.


Review Point: 1. The weakness of this paper lies in that although the proposed method is efficient compared to the directly unrolling method. The time complexity of both the baselines and this method should also be discussed so that readers can better balance these methods.
Review Point: 2. Although the module choice effect is shown in the experiments, there needs more analysis and instructions for readers to decide when to conduct finer-grained module split or must we conduct parameter-level module split with collecting more data in the developing set? The authors point that the time complexity is the limitation of this method, but it needs more discussing as above.
==================================================

Focused review:

Weakness: 1. It seems the method needs to update of the
c
i
,
σ
i
and
q
(
c
)
iteratively. How do you update these parameters in an end to end training manner? Does this increase your training complexity, how is the complexity compared to SwAV and DINO (e.g., in hours)? I think you could definitely hide most of your derivations into the appendix, whereas the space shall be left for the update rules in a succinct manner. This might significantly help improve the clarity and readability of your paper.
Weakness: 2. Training batchsize of the proposed method is not specified in the Table 1. This might cause unfair comparisons between the proposal and SwAV, DINO (which I think essentially is very close to this SVIB method) due to the sensitivity of these SOTA method regarding batchsize.
Weakness: 3. In my personal opinion, I suggest that the authors reconsider the claim: "In this paper, we make the first attempt to remove the redundancy between augmentations with the introduction of the information bottleneck", since SOTA method Barlow Twins [b] has explicitly discussed the impact of feature redundancy for the self-supervised learning tasks.
Weakness: 4. The presentation quality needs improvement.
If possible, please clarify my concerns in your rebuttal.
[a] Mitrovic et al., “Representation Learning via Invariant Causal Mechanisms” ICLR 2021.
[b] Zbontar et al., " Barlow Twins: Self-Supervised Learning via Redundancy Reduction", ICML 2021.


Review Point: 1. It seems the method needs to update of the c i , σ i and q ( c ) iteratively. How do you update these parameters in an end to end training manner? Does this increase your training complexity, how is the complexity compared to SwAV and DINO (e.g., in hours)? I think you could definitely hide most of your derivations into the appendix, whereas the space shall be left for the update rules in a succinct manner. This might significantly help improve the clarity and readability of your paper. Weakness:
Review Point: 2. Training batchsize of the proposed method is not specified in the Table 1. This might cause unfair comparisons between the proposal and SwAV, DINO (which I think essentially is very close to this SVIB method) due to the sensitivity of these SOTA method regarding batchsize. Weakness:
Review Point: 3. In my personal opinion, I suggest that the authors reconsider the claim: "In this paper, we make the first attempt to remove the redundancy between augmentations with the introduction of the information bottleneck", since SOTA method Barlow Twins [b] has explicitly discussed the impact of feature redundancy for the self-supervised learning tasks. Weakness:
Review Point: 4. The presentation quality needs improvement. If possible, please clarify my concerns in your rebuttal. [a] Mitrovic et al., “Representation Learning via Invariant Causal Mechanisms” ICLR 2021. [b] Zbontar et al., " Barlow Twins: Self-Supervised Learning via Redundancy Reduction", ICML 2021.
==================================================

Focused review:

Weaknesses
The novelty of this paper is not very exciting. Also, only a single subtask is given (i.e., binary similarity check), which can hardly prove the generalization of the model architecture over the assembly code.
Some important implementation details are missing.
The paper writing is quite confusing.
The evaluation is not very interesting.
Questions and concerns:
The idea of encoding the instructions together was used across multiple assembly applications (ASM2Vec, https://proceedings.neurips.cc/paper/2019/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html ). This paper adopts this idea into transformers and masked the entire instruction for pretraining. The idea is not very exciting.
There is a lot of typos in Sec 2.2 and Sec 3.3. I can hardly understand your derivation. For example, 1) I guessed out the meaning EM/TE/H in eq.2 and eq.3. But it never appears anywhere else and also in Figure 2. 2) I am confused about how you obtained eq.10 from eq.9. Also, what is K? What do you mean by `using an arbitrary function'? Is that a trainable matrix? These notations are broken.
Many details are missing, e.g., your training hyperparameters for baseline MLM. This is important because pretraining a transformer using assembly code can easily run out of GPU memory. According to your maximum sequence length ( which is 512 given in Sec.4), I don’t think this is enough for a moderate-size assembly code. Can you plot the histogram of the token sizes using your pretraining dataset?
Also, your baseline explanation is confusing (Sec 4.1, also a lot of typos and a lot of notations that are never explained). What is IDF? You should at least explain how TFIDF works. The paper constantly mentions `MLM outperforms all previous methods and machine learning baselines’, but no experiments are given. You can try out some naive machine learning baselines (e.g, a simple bag-of-words method given in https://arxiv.org/pdf/2006.05265v6.pdf).
Regarding your experiment results, I don’t know why O2-O3 matching is easier than O0-O0 matching. Intuitively, O0 with disabled optimization contains more information regarding the code and may be easier to identify its cross-compilation counterparts.


Review Point: The novelty of this paper is not very exciting. Also, only a single subtask is given (i.e., binary similarity check), which can hardly prove the generalization of the model architecture over the assembly code. Some important implementation details are missing. The paper writing is quite confusing. The evaluation is not very interesting. Questions and concerns: The idea of encoding the instructions together was used across multiple assembly applications (ASM2Vec, https://proceedings.neurips.cc/paper/2019/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html ). This paper adopts this idea into transformers and masked the entire instruction for pretraining. The idea is not very exciting. There is a lot of typos in Sec 2.2 and Sec 3.3. I can hardly understand your derivation. For example, 1) I guessed out the meaning EM/TE/H in eq.2 and eq.3. But it never appears anywhere else and also in Figure 2.
Review Point: 2) I am confused about how you obtained eq.10 from eq.9. Also, what is K? What do you mean by `using an arbitrary function'? Is that a trainable matrix? These notations are broken. Many details are missing, e.g., your training hyperparameters for baseline MLM. This is important because pretraining a transformer using assembly code can easily run out of GPU memory. According to your maximum sequence length ( which is 512 given in Sec.4), I don’t think this is enough for a moderate-size assembly code. Can you plot the histogram of the token sizes using your pretraining dataset? Also, your baseline explanation is confusing (Sec 4.1, also a lot of typos and a lot of notations that are never explained). What is IDF? You should at least explain how TFIDF works. The paper constantly mentions `MLM outperforms all previous methods and machine learning baselines’, but no experiments are given. You can try out some naive machine learning baselines (e.g, a simple bag-of-words method given in https://arxiv.org/pdf/2006.05265v6.pdf). Regarding your experiment results, I don’t know why O2-O3 matching is easier than O0-O0 matching. Intuitively, O0 with disabled optimization contains more information regarding the code and may be easier to identify its cross-compilation counterparts.
==================================================

Focused review:

- My main objection to this work is the lack of baselines. In the structural comparison results they include a random baseline in the appendix which is great but I would be very interested to know how well meta-learning compares to, e.g. one or a few flavors of SGD. If nothing else so to get a better sense of the scale in the results. Maybe there is a good reason as to why such a baseline would not make any sense but if you agree I think that would strengthen the paper. - In section 4.3 you state that you use the meta-learner to generate the input streams that you use to compute the results. How does this impact the validity of the results? And does the results differ much if you instead use the "optimal" agent to generate the stream?

Review Point: - My main objection to this work is the lack of baselines. In the structural comparison results they include a random baseline in the appendix which is great but I would be very interested to know how well meta-learning compares to, e.g. one or a few flavors of SGD. If nothing else so to get a better sense of the scale in the results. Maybe there is a good reason as to why such a baseline would not make any sense but if you agree I think that would strengthen the paper.
Review Point: - In section 4.3 you state that you use the meta-learner to generate the input streams that you use to compute the results. How does this impact the validity of the results? And does the results differ much if you instead use the "optimal" agent to generate the stream?
==================================================

Focused review:

Weaknesses: There are three main comments below, which need to consider carefully. First, the relaxation of the softmax function is the fundamental assumption, i.e., the reformulation in Eq.(3). The relaxed linear operation does not depend on X_i, which may be not an appropriate approximation. Also, the interpretability of the approach does not explain clearly. Moreover, the paper does not verify the effectiveness of the relaxation. For example, it is better to compare with the regularized training problem in (1) using SGD. Second, the shapes of the input and output of a Transformer block are usually the same. However, the paper only demonstrates the self-attention-only networks with the scalar and vector output. What about the matrix output with the same shape of X_i? Third, the paper tests with deep settings by stack the convex transformer layers in the experiments. It is better to give more detailed explanations on this issue. Is there any difficulty to extend to deep Transformers? Other minor comments: 1. There are two “of “ letters at the second line of the first paragraph in page 2. 2. In (2), “X” should be “X_i”. 3. At the first line of the second paragraph in page 4, “varios”should be “various”.


Review Point: There are three main comments below, which need to consider carefully. First, the relaxation of the softmax function is the fundamental assumption, i.e., the reformulation in Eq.(3). The relaxed linear operation does not depend on X_i, which may be not an appropriate approximation. Also, the interpretability of the approach does not explain clearly. Moreover, the paper does not verify the effectiveness of the relaxation. For example, it is better to compare with the regularized training problem in (1) using SGD. Second, the shapes of the input and output of a Transformer block are usually the same. However, the paper only demonstrates the self-attention-only networks with the scalar and vector output. What about the matrix output with the same shape of X_i? Third, the paper tests with deep settings by stack the convex transformer layers in the experiments. It is better to give more detailed explanations on this issue. Is there any difficulty to extend to deep Transformers? Other minor comments:
Review Point: 1. There are two “of “ letters at the second line of the first paragraph in page 2.
Review Point: 3. At the first line of the second paragraph in page 4, “varios”should be “various”.
==================================================

Focused review:

Weaknesses:
The quantitative evaluation on real images is missing. In the paper, only visual examples of different image editing methods are provided, which I think could be biased and subjective given that the proposed method is not visually much better than the baselines (TediGAN and StyleCLIP). I would recommend using some metrics to measure the image editing quality (such as disentanglement, FID and controllability) or doing some human study evaluations.
For the proposed disentanglement model, I do not see much novelty on how to deal with partially labeled attributes (Eq. 3-5), since as far as I know, previous works have already used this similar idea, such as [1].
I am just curious about why Locatello et al. [29] has such low MIG scores on these benchmarks (say, 0.01 on dSprites). Is this really because of permuted-labels configuration or the hyperparameters in the baseline have not been well selected?
It looks like the highest resolution of real images in this paper is only 256x256, which seems to limit the generation quality of the proposed method. I wonder if the method can be scaled up to a larger resolution, such as 1024x1024 in the StyleGANs. Is there any constraint in doing so?
[1] Kingma et al., Semi-Supervised Learning with Deep Generative Models, NeurIPS 2014.
=================
Post-rebuttal update:
I thank the authors for taking the time to respond to my reviews. But my major concerns still remain: 1) The qauntitative results of real images actually show the worse disentanglement performance measured by AD (though I agree that it only refects one aspect of disentanglement quality). 2) Although [1] only studies the single-attribute case, Eq. (3) and (4) look exactly the same with the second term of Eq. (7) and the second term of Eq. (9) in [1]. Thus, I still think the novelty of dealing with partially labeled attributes is limited. 3) The visual quality on real images (up to res 256x256) is not high, so I'm a little doubtful that the proposed method can be scaled to more complex high-res image. Therefore, I keep the initial rating unchanged and tend to reject.
=================
Post-rebuttal update (2nd round):
I thank the reviewer for the further clarifications, in particular about the difference between this work and the reference [1]. I agree that disentanglement evaluation on real images is more challenging but I think solely reporting AD results (i.e., measuring how the modification affects other attributes) is problematic. I suggest the authors also consider other metrics to measure the "editing strength" (i.e., how large the modification changes the considered attribute). One implementation could be that we first train image attribute classifiers on CelebA and use them as oracle models to measure the predicted score differences of generated images before and after editing. Overall, I'm convinced by the authors regarding the novelty and happy to raise my initial score by 1.
The authors have adequately addressed the limitations and potential negative societal impact of their work


Review Point: The quantitative evaluation on real images is missing. In the paper, only visual examples of different image editing methods are provided, which I think could be biased and subjective given that the proposed method is not visually much better than the baselines (TediGAN and StyleCLIP). I would recommend using some metrics to measure the image editing quality (such as disentanglement, FID and controllability) or doing some human study evaluations. For the proposed disentanglement model, I do not see much novelty on how to deal with partially labeled attributes (Eq. 3-5), since as far as I know, previous works have already used this similar idea, such as [1]. I am just curious about why Locatello et al. [29] has such low MIG scores on these benchmarks (say, 0.01 on dSprites). Is this really because of permuted-labels configuration or the hyperparameters in the baseline have not been well selected? It looks like the highest resolution of real images in this paper is only 256x256, which seems to limit the generation quality of the proposed method. I wonder if the method can be scaled up to a larger resolution, such as 1024x1024 in the StyleGANs. Is there any constraint in doing so? [1] Kingma et al., Semi-Supervised Learning with Deep Generative Models, NeurIPS 2014. ================= Post-rebuttal update: I thank the authors for taking the time to respond to my reviews. But my major concerns still remain:
Review Point: 1) The qauntitative results of real images actually show the worse disentanglement performance measured by AD (though I agree that it only refects one aspect of disentanglement quality).
Review Point: 2) Although [1] only studies the single-attribute case, Eq. (3) and (4) look exactly the same with the second term of Eq. (7) and the second term of Eq. (9) in [1]. Thus, I still think the novelty of dealing with partially labeled attributes is limited.
Review Point: 3) The visual quality on real images (up to res 256x256) is not high, so I'm a little doubtful that the proposed method can be scaled to more complex high-res image. Therefore, I keep the initial rating unchanged and tend to reject. ================= Post-rebuttal update (2nd round): I thank the reviewer for the further clarifications, in particular about the difference between this work and the reference [1]. I agree that disentanglement evaluation on real images is more challenging but I think solely reporting AD results (i.e., measuring how the modification affects other attributes) is problematic. I suggest the authors also consider other metrics to measure the "editing strength" (i.e., how large the modification changes the considered attribute). One implementation could be that we first train image attribute classifiers on CelebA and use them as oracle models to measure the predicted score differences of generated images before and after editing. Overall, I'm convinced by the authors regarding the novelty and happy to raise my initial score by 1. The authors have adequately addressed the limitations and potential negative societal impact of their work
==================================================

Focused review:

Weaknesses
Scope & Generalization: Vocabulary generation is crucial for many NLP tasks (as described in the motivation), but only NMT is studied. Therefore, the scope of the paper appears more limited than it alludes to. Furthermore, it should be clearly stated that the approach is built on top of the BPE segmentation algorithm, and thereby restricted to whitespace-tokenized languages. In addition, only joint source-target vocabularies are studied, so it is not clear how to apply the approach to distinct vocabularies nor whether it would generalize.
Related Work: Relevant related work is not cited, and general claims about the state of tuning BPE sizes ("rarely tried to search for [it]") disregard the existance of previous studies (see below).
Interpretation of Results: The relation between AMD and BLEU is not as simple as stated. In fact, one could take a look at the same plots (Fig. 1) and reason that 2k BPE is the magic solution that in most cases yields the best quality and therefore should be generally the best. There is no clear correlation between AMD and BLEU, and arguing for AMD just because a high value usually yields high BLEU is not sufficiently supported by the results, since large vocabulary sizes in Fig 1 show almost no reduction in AMD but a steep decrease in BLEU. As shown in the rightmost plot in Fig 1, AMD is not necessary for high BLEU either (1k vocabulary size). In Table 1, Info-VOT outperforms the baseline in roughly half of the cases and cannot therefore not be claimed to be "much better" (Sec. 6). It is also unclear if differences in BLEU are significant.
Clarity: It is not clear from the Introduction that the proposed solution is tied to BPE segmentation (only appears in Sec. 3), and not a self-standing solution. The dependence on heuristic post-processing is only mentioned in the end and is not clear from the description of the algorithm. Some open questions regarding the math (see below). There is a lack of precision in the math (see details below).
Reasoning and Assumptions: It is assumed that a high BPC, and thereby maximum entropy in the vocabulary distribution, is desirable, but this assumption is not well supported nor motivated. To relate it to MacKay (2003) that is cited to ground the work in information theory, the optimal input distribution for compression is dependent on the type of the channel, and the input with maximum entropy is therefore not always the best, see Chapter 9 in MacKay (2003). Furthermore, source and target language distributions are merged into one, and there is no consideration for interference or conditional dependence in a parallel corpus.
Evaluation: Confounding factors like architecture depth (which has been shown to be highly relevant in related work (see below)), vocabulary overlap for multiple languages (which affects multilingual NMT quality), regularization (which might be more needed for large models with large vocabularies), corpus size (shown to matter in related work) are neither discussed nor empirically studied. In addition, linear search rather than random search should be employed as a baseline since relations between vocabulary size and translation quality appear to be mostly monotonic.
Reproducibility: The code is not published (nor promised), and implementation details for OT are missing to replicate the results.
Strengths
Novelty and originality of the approach. So far, there has been no work relating segmentation to optimal transport.
Potential impact: Eliminating hyperparameter search for optimal BPE vocabularies could save the field a lot of wasted compute. The reported speed gains over random search are promising.
Multilingual: The experiments are conducted on multiple language pairs and a multilingual setting as well.
Missing Related Work
Cherry et al. 2018 (https://www.aclweb.org/anthology/D18-1461.pdf) already found indicators that segmentation close to character-level are advantageous for deep translation models, and proposed an algorithm to optimize segmentation granularity for downstream quality. The also show that the advantage of character-level modeling diminishes with increased training data.
Similary, Kreutzer & Sokolov 2018 (https://arxiv.org/abs/1810.01480) proposed to learn the input segmentation end-to-end, and also found a qualitative advantage of character-level segmentation with deeper models.
Details
Please provide a pseudo-code to illustrate the interplay of OT and BPE segmentation and the hyperparameters involved in the process.
The length normalization disappears from Eq 8 to Eq 9 on to the final objective (not numbered, p6). Please explain or correct.
Figure 1: The plots are too small to be able to read the axes' labels and titles.
With AMD being a dynamic feature defined on differences between merge steps, how can it be reported for a static BPE size like in Fig. 1?
"AMD-BPC, short for AMD" (p3) assumes a prior definition of the acronym, and probably should be reversed since "AMD" is shorter than "AMD-BPC".
Duplicated "can" on p4.
Eq. 8 should not have the min as far as I understand.
Eq. 8 should hold only under certain assumptions of l (that the average length decreases if tokens get more frequent), and therefore does not hold generally. For the case of BPE it is reasonable to assume that it holds.
What's the influence of the post-processing hyperparameter of 10% for comparing char and token frequencies? An ablation would be helpful to understand the implications of the post-processing.
The multilingual TED talk data set needs a citation because there are different versions on the web, I'd assume it's http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/ (Duh 2018)?
Table 1 boldfaces results that are not the best in each column, which is confusing for the reader and misleading for the interpretation.
Table 1 misses BPE-2k which seemed the consitently strongest setting for BPE in Fig 1. Is there any gain of Info-VOT over BPE-2k (vocabulary sizes are similar)?
How does it compare against character-level models? Those have been reported to perform well in related work, and need no tuning.

Update addressing author response:
I believe the quality of the paper has notably improved, in particular the clarity of the proposed method, and the wider space of experiments generally supports the method. However, some critical questions remain.
Q1: "For simplification, we use BPE segmentation to initialize the token candidate distribution. It does not mean that our approach can only be built on top of BPE." (also Q5) It remains unclear how important the initial choice is (why not pure characters or a random selection of merges?) and if it requires hyperparameter tuning (making the proposed solution less "green" than advertised). Same holds for the impact of post-processing heuristics.
BPE as proposed by Sennrich et al. 2016 assumes the notion of white-space separated words or tokens. SentencePiece, however, does not. This blogpost illustrates the differences quite well: https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html
Q7: The multilingual experiments confirm the effectiveness in some cases (significance tests would be great), but the effect of overlapping vocabularies is still not taken into consideration or analyzed.
Q12: Several of these gains might not be significant due to very small absolute BLEU differences.
Detail in the added references: the newly added reference Kreutzer & Sokolov was published at IWSLT 2018.


Review Point: Scope & Generalization: Vocabulary generation is crucial for many NLP tasks (as described in the motivation), but only NMT is studied. Therefore, the scope of the paper appears more limited than it alludes to. Furthermore, it should be clearly stated that the approach is built on top of the BPE segmentation algorithm, and thereby restricted to whitespace-tokenized languages. In addition, only joint source-target vocabularies are studied, so it is not clear how to apply the approach to distinct vocabularies nor whether it would generalize. Related Work: Relevant related work is not cited, and general claims about the state of tuning BPE sizes ("rarely tried to search for [it]") disregard the existance of previous studies (see below). Interpretation of Results: The relation between AMD and BLEU is not as simple as stated. In fact, one could take a look at the same plots (Fig. 1) and reason that 2k BPE is the magic solution that in most cases yields the best quality and therefore should be generally the best. There is no clear correlation between AMD and BLEU, and arguing for AMD just because a high value usually yields high BLEU is not sufficiently supported by the results, since large vocabulary sizes in Fig 1 show almost no reduction in AMD but a steep decrease in BLEU. As shown in the rightmost plot in Fig 1, AMD is not necessary for high BLEU either (1k vocabulary size). In Table 1, Info-VOT outperforms the baseline in roughly half of the cases and cannot therefore not be claimed to be "much better" (Sec. 6). It is also unclear if differences in BLEU are significant. Clarity: It is not clear from the Introduction that the proposed solution is tied to BPE segmentation (only appears in Sec. 3), and not a self-standing solution. The dependence on heuristic post-processing is only mentioned in the end and is not clear from the description of the algorithm. Some open questions regarding the math (see below). There is a lack of precision in the math (see details below). Reasoning and Assumptions: It is assumed that a high BPC, and thereby maximum entropy in the vocabulary distribution, is desirable, but this assumption is not well supported nor motivated. To relate it to MacKay (2003) that is cited to ground the work in information theory, the optimal input distribution for compression is dependent on the type of the channel, and the input with maximum entropy is therefore not always the best, see Chapter 9 in MacKay (2003). Furthermore, source and target language distributions are merged into one, and there is no consideration for interference or conditional dependence in a parallel corpus. Evaluation: Confounding factors like architecture depth (which has been shown to be highly relevant in related work (see below)), vocabulary overlap for multiple languages (which affects multilingual NMT quality), regularization (which might be more needed for large models with large vocabularies), corpus size (shown to matter in related work) are neither discussed nor empirically studied. In addition, linear search rather than random search should be employed as a baseline since relations between vocabulary size and translation quality appear to be mostly monotonic. Reproducibility: The code is not published (nor promised), and implementation details for OT are missing to replicate the results. Strengths Novelty and originality of the approach. So far, there has been no work relating segmentation to optimal transport. Potential impact: Eliminating hyperparameter search for optimal BPE vocabularies could save the field a lot of wasted compute. The reported speed gains over random search are promising. Multilingual: The experiments are conducted on multiple language pairs and a multilingual setting as well. Missing Related Work Cherry et al. 2018 (https://www.aclweb.org/anthology/D18-1461.pdf) already found indicators that segmentation close to character-level are advantageous for deep translation models, and proposed an algorithm to optimize segmentation granularity for downstream quality. The also show that the advantage of character-level modeling diminishes with increased training data. Similary, Kreutzer & Sokolov 2018 (https://arxiv.org/abs/1810.01480) proposed to learn the input segmentation end-to-end, and also found a qualitative advantage of character-level segmentation with deeper models. Details Please provide a pseudo-code to illustrate the interplay of OT and BPE segmentation and the hyperparameters involved in the process. The length normalization disappears from Eq 8 to Eq 9 on to the final objective (not numbered, p6). Please explain or correct. Figure 1: The plots are too small to be able to read the axes' labels and titles. With AMD being a dynamic feature defined on differences between merge steps, how can it be reported for a static BPE size like in Fig. 1? "AMD-BPC, short for AMD" (p3) assumes a prior definition of the acronym, and probably should be reversed since "AMD" is shorter than "AMD-BPC". Duplicated "can" on p4. Eq.
Review Point: 8 should not have the min as far as I understand. Eq.
Review Point: 8 should hold only under certain assumptions of l (that the average length decreases if tokens get more frequent), and therefore does not hold generally. For the case of BPE it is reasonable to assume that it holds. What's the influence of the post-processing hyperparameter of 10% for comparing char and token frequencies? An ablation would be helpful to understand the implications of the post-processing. The multilingual TED talk data set needs a citation because there are different versions on the web, I'd assume it's http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/ (Duh 2018)? Table 1 boldfaces results that are not the best in each column, which is confusing for the reader and misleading for the interpretation. Table 1 misses BPE-2k which seemed the consitently strongest setting for BPE in Fig 1. Is there any gain of Info-VOT over BPE-2k (vocabulary sizes are similar)? How does it compare against character-level models? Those have been reported to perform well in related work, and need no tuning. Update addressing author response: I believe the quality of the paper has notably improved, in particular the clarity of the proposed method, and the wider space of experiments generally supports the method. However, some critical questions remain.
Review Point: Q1: "For simplification, we use BPE segmentation to initialize the token candidate distribution. It does not mean that our approach can only be built on top of BPE." (also Q5) It remains unclear how important the initial choice is (why not pure characters or a random selection of merges?) and if it requires hyperparameter tuning (making the proposed solution less "green" than advertised). Same holds for the impact of post-processing heuristics. BPE as proposed by Sennrich et al. 2016 assumes the notion of white-space separated words or tokens. SentencePiece, however, does not. This blogpost illustrates the differences quite well: https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html Q7: The multilingual experiments confirm the effectiveness in some cases (significance tests would be great), but the effect of overlapping vocabularies is still not taken into consideration or analyzed.
Review Point: Q12: Several of these gains might not be significant due to very small absolute BLEU differences. Detail in the added references: the newly added reference Kreutzer & Sokolov was published at IWSLT 2018.
==================================================

Focused review:

1. I am concerned about the importance of this result. Since [15] says that perturbed gradient descent is able to find second-order stationary with almost-dimension free (with polylog factors of dimension) polynomial iteration complexity, it is not surprising to me that the decentralized algorithm with occasionally added noise is able to escape saddle point in polynomial time. In addition, the iteration complexity is no longer dimension-free anymore in Theorem 3 (there is a $d$ dependency instead of $log d$). 2. There is no empirical study in this paper. The authors should have constructed some synthetic examples where we know the exact location of saddle points and tried to verify the theoretical claims of the proposed algorithm. Furthermore, PGD [15] should also be compared. I know this is a theory paper, but given the presence of [15], the theoretical contribution is not strong enough from my perspective.

Review Point: 1. I am concerned about the importance of this result. Since [15] says that perturbed gradient descent is able to find second-order stationary with almost-dimension free (with polylog factors of dimension) polynomial iteration complexity, it is not surprising to me that the decentralized algorithm with occasionally added noise is able to escape saddle point in polynomial time. In addition, the iteration complexity is no longer dimension-free anymore in Theorem 3 (there is a $d$ dependency instead of $log d$).
Review Point: 2. There is no empirical study in this paper. The authors should have constructed some synthetic examples where we know the exact location of saddle points and tried to verify the theoretical claims of the proposed algorithm. Furthermore, PGD [15] should also be compared. I know this is a theory paper, but given the presence of [15], the theoretical contribution is not strong enough from my perspective.
==================================================

Focused review:

Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer. 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense? 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion. 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this. 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the "desired" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues: - "philosophical note" paragraph seems a bit digressive. - Line 109 failed to use superscript? - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.

Review Point: Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer.
Review Point: 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense?
Review Point: 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion.
Review Point: 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this.
Review Point: 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the "desired" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues:
Review Point: - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.
==================================================

Focused review:

Weakness:
W1. [Missing related work] There are some related works this paper omitted. For example, Adversarial AutoAugment [1] also uses an adversarial objective to learn an augmentation strategy formed by a set of label-preserving transformations for the target dataset. The paper can compare LP-A3 with other adversarial data augmentation strategies in terms of their idea and performance, so we will know which approach is better in which scenarios.
W2. [Insufficient experiment]
The experiments are not extensive enough. Only the results on several tiny datasets are provided. The results on the large-scale ImageNet dataset will make this paper stronger. In supervised image classification tasks, the paper compares LP-A3 with RandAugment on MedMNIST. It would be better to compare LP-A3 with other Automated Data Augmentation methods, like AutoAugment, Adversarial AutoAugment, and sample-mixing augmentations, like Mixup, Cutmix, on the full dataset of popular classification benchmarks, e.g. CIFAR10 and CIFAR100. This helps to verify the performance gains compared with the state-of-the-art data augmentation schemes.
Prior works such as RandAugment run multiple times for the model and present the mean and standard deviation. Did the authors run multiple times for each model and what about presenting the mean and standard deviation?
W3. [Adversarial training] According to lines 252-255 and Appendix B, it seems that LP-A3 uses an adversarial-attack-like procedure to perturb the input images. Adversarial attack perturbs an image to alter the class label; oppositely, LP-A3 aims to preserve the class label. With the opposing objectives, can LP-A3 be integrated with adversarial training? Does LP-A3 affect the use of perturbation-based adversarial attacks to create adversarial samples in adversarial defense?
[1] Zhang et al., Adversarial AutoAugment. ICLR, 2020.


Review Point: W1. [Missing related work] There are some related works this paper omitted. For example, Adversarial AutoAugment [1] also uses an adversarial objective to learn an augmentation strategy formed by a set of label-preserving transformations for the target dataset. The paper can compare LP-A3 with other adversarial data augmentation strategies in terms of their idea and performance, so we will know which approach is better in which scenarios.
Review Point: W2. [Insufficient experiment] The experiments are not extensive enough. Only the results on several tiny datasets are provided. The results on the large-scale ImageNet dataset will make this paper stronger. In supervised image classification tasks, the paper compares LP-A3 with RandAugment on MedMNIST. It would be better to compare LP-A3 with other Automated Data Augmentation methods, like AutoAugment, Adversarial AutoAugment, and sample-mixing augmentations, like Mixup, Cutmix, on the full dataset of popular classification benchmarks, e.g. CIFAR10 and CIFAR100. This helps to verify the performance gains compared with the state-of-the-art data augmentation schemes. Prior works such as RandAugment run multiple times for the model and present the mean and standard deviation. Did the authors run multiple times for each model and what about presenting the mean and standard deviation?
Review Point: W3. [Adversarial training] According to lines 252-255 and Appendix B, it seems that LP-A3 uses an adversarial-attack-like procedure to perturb the input images. Adversarial attack perturbs an image to alter the class label; oppositely, LP-A3 aims to preserve the class label. With the opposing objectives, can LP-A3 be integrated with adversarial training? Does LP-A3 affect the use of perturbation-based adversarial attacks to create adversarial samples in adversarial defense? [1] Zhang et al., Adversarial AutoAugment. ICLR, 2020.
==================================================

Focused review:

Weaknesses
1. Efficiency of the proposed model
As far as I understand, the proposed model is computationally much more expensive than existing transformer-based NTPP approaches. This aspect is not discussed in the paper and isn't analyzed in the experiments, even though it can be important in practice. To explain this, let's look at how the conditional density
p
(
t
i
 
H
t
i
)
=
λ
∗
(
t
i
)
−
∫
t
i
−
1
t
i
λ
∗
(
t
)
d
t
is evaluated for existing transformer-based NTPPs and the proposed model.
AMDN (Karishma et al., 2021):
we need to perform a single forward pass of the transformer to embed the history
H
t
i
into a vector
h
i
we evaluate
p
(
t
i
 
h
i
)
in closed form
In transformer Hawkes process (THP) (Zuo et al., 2020)
we need to perform a single forward pass of the transformer to embed the history
H
t
i
into a vector
h
i
we evaluate the integral
∫
t
i
−
1
t
i
λ
∗
(
t
)
d
t
with Monte Carlo integration, which is more expensive than the previous approach
In FATPP (this work)
we need to perform multiple forward passes of the transformer to evaluate the conditional intensity
λ
∗
(
t
)
at points randomly sampled in the interval
(
t
i
−
1
,
t
i
)
to approximate the integral
∫
t
i
−
1
t
i
λ
∗
(
t
)
d
t
. Of course, these can be done in parallel. However, even a small number of MC samples is used (e.g., 20), this increases the memory footprint of the model by a factor of 20.
Moreover, sampling cannot be done exactly in the FATPP model. In AMDN, we can sample the next inter-event time analytically. In THP we can use thinning since the conditional hazard decays over time (so an upper bound on the hazard is easy to obtain), which still leads to exact sampling. In contrast, FATPP would require inverse transform sampling with multiple forward passes and 2 approximations: (1) approximating the compensator with Monte Carlo and (2) inverting the compensator with numerical root-finding to obtain a sample.
At the very least, the above limitations should be mentioned in the paper. It would be even better to demonstrate the runtime / memory-consumption plots for different models on different dataset sizes.
Please let me know if I misunderstood any aspects of the proposed model in my description above and should update my assessment.
2. Choice of baselines during experimental evaluation
The main motivation for the proposed FATPP model is its flexible parametrization of the intensity function. However, experiments in Section 5.1 do not compare to existing flexible NTPP models, such as FullyNN (Omi et al., 2019), LogNormMix (Shchur et al., 2020), or the transformer-based AMDN model (Karishma et al., 2021). The implementation of all these 3 models is publically available, so the comparison should be rather straightforward.
Minor comments
Another simple baseline for sequence clustering with NTPPs was proposed in Section 5.5 of (Shchur et al., 2020). The learned sequence embeddings could be clustered with K-means to assign sequences to clusters. This approach could be added to Section 5.2.
Equation 2:
∫
0
T
instead of
∫
0
t
L
since we're talking about a TPP on
[
0
,
T
)
.
Figure 3 is rather hard to read because of the juxtaposition of all intensity curves. It might be helpful to provide plots in the appendix, where the intensity is plotted separately for each model.
Table 3: please elaborate the meaning of "crash" in "complex time intervals ... make RMTPP-MIX crash".


Review Point: 1. Efficiency of the proposed model As far as I understand, the proposed model is computationally much more expensive than existing transformer-based NTPP approaches. This aspect is not discussed in the paper and isn't analyzed in the experiments, even though it can be important in practice. To explain this, let's look at how the conditional density p ( t i H t i ) = λ ∗ ( t i ) − ∫ t i − 1 t i λ ∗ ( t ) d t is evaluated for existing transformer-based NTPPs and the proposed model. AMDN (Karishma et al., 2021): we need to perform a single forward pass of the transformer to embed the history H t i into a vector h i we evaluate p ( t i h i ) in closed form In transformer Hawkes process (THP) (Zuo et al., 2020) we need to perform a single forward pass of the transformer to embed the history H t i into a vector h i we evaluate the integral ∫ t i − 1 t i λ ∗ ( t ) d t with Monte Carlo integration, which is more expensive than the previous approach In FATPP (this work) we need to perform multiple forward passes of the transformer to evaluate the conditional intensity λ ∗ ( t ) at points randomly sampled in the interval ( t i − 1 , t i ) to approximate the integral ∫ t i − 1 t i λ ∗ ( t ) d t . Of course, these can be done in parallel. However, even a small number of MC samples is used (e.g., 20), this increases the memory footprint of the model by a factor of 20. Moreover, sampling cannot be done exactly in the FATPP model. In AMDN, we can sample the next inter-event time analytically. In THP we can use thinning since the conditional hazard decays over time (so an upper bound on the hazard is easy to obtain), which still leads to exact sampling. In contrast, FATPP would require inverse transform sampling with multiple forward passes and 2 approximations: (1) approximating the compensator with Monte Carlo and (2) inverting the compensator with numerical root-finding to obtain a sample. At the very least, the above limitations should be mentioned in the paper. It would be even better to demonstrate the runtime / memory-consumption plots for different models on different dataset sizes. Please let me know if I misunderstood any aspects of the proposed model in my description above and should update my assessment.
Review Point: 2. Choice of baselines during experimental evaluation The main motivation for the proposed FATPP model is its flexible parametrization of the intensity function. However, experiments in Section 5.1 do not compare to existing flexible NTPP models, such as FullyNN (Omi et al., 2019), LogNormMix (Shchur et al., 2020), or the transformer-based AMDN model (Karishma et al., 2021). The implementation of all these 3 models is publically available, so the comparison should be rather straightforward. Minor comments Another simple baseline for sequence clustering with NTPPs was proposed in Section 5.5 of (Shchur et al., 2020). The learned sequence embeddings could be clustered with K-means to assign sequences to clusters. This approach could be added to Section 5.2. Equation 2: ∫ 0 T instead of ∫ 0 t L since we're talking about a TPP on [ 0 , T ) . Figure 3 is rather hard to read because of the juxtaposition of all intensity curves. It might be helpful to provide plots in the appendix, where the intensity is plotted separately for each model. Table 3: please elaborate the meaning of "crash" in "complex time intervals ... make RMTPP-MIX crash".
==================================================

Focused review:

Some concerns remain over the empirical evaluation, as detailed in the comments. 
1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to "randomly stor[e] a few examples of old classes" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases.
Basically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples?
In summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.
3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that "The performance drops with removing Lco ("w/o CO"). It demonstrates that it is helpful to handle medical rare words" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed.
Minor grammatical/spelling concerns: (Line 97) "Medica intent" (Line 124) "Contrastive Reply Networks" (reply or replay?) 
(Line 161) "...top n example(s)" etc. 

Review Point: Some concerns remain over the empirical evaluation, as detailed in the comments.
Review Point: 1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to "randomly stor[e] a few examples of old classes" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases. Basically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples? In summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.
Review Point: 3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that "The performance drops with removing Lco ("w/o CO"). It demonstrates that it is helpful to handle medical rare words" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed. Minor grammatical/spelling concerns: (Line 97) "Medica intent" (Line 124) "Contrastive Reply Networks" (reply or replay?) (Line 161) "...top n example(s)" etc.
==================================================

Focused review:

Weakness:
The design of the model is not well-motivated.
the domain gap between target and source is usually the main point to decrease the generalization of the model. However, both the proposed IPNet and FSAT do not directly address the domain shift or use any domain loss. I feel like fusing two domain features brutally will still suffer from domain shift when using the prototyping. Only the task of the few-shot challenge is addressed here.
The idea of using FSAT for separating foreground and background is not well-motivated. Does this module aim to address the issue of missing detection or learn a better representation for feature encoder? Looks like this module is only to regularize the training of feature extractor like ResNet101 or VGG16 with prototypes from IPNet? It would be also good to explain why RPN is not trained in both of the two modules if this is the case.
FSAT is using adversarial learning for foreground and background instead of source or target. While this learning process may help to address the domain gap without aligning the distribution between source and target, would the inherent domain shift affect the performance of the foreground/background separation?
Since we only have few labeled target images and many source images, the issue of data imbalance will be taking place in this model. Will the imbalance of source and target images affect the model? I see only the focal loss applied for boxes imbalance issue.
Lack of analysis:
The idea of prototyping in IPM is not new and does not have further revision or customized for cross-domain tasks. Obviously, directly fusing two data would suffer from the domain gap. For example, does the improvement come from seeing few labeled data from the target or the model is really addressing the domain gap? 2)How would separating foreground and background be effective for domain shift is missing.
Missing context
The setting of few-shot cross-domain object detection is not strictly defined well. For example, CDOD assumes the labels of target domains are unavailable. Here FSCDOD can access the labels from the target domain.
Many recent DAOD related works in CVPR are missing. For example:
[a] He, Mengzhe, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan, Wei Wu, and Yu Qiao. "Cross Domain Object Detection by Target-Perceived Dual Branch Distillation." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 9570-9580. 2022.
[b]Zhao, Liang, and Limin Wang. "Task-specific Inconsistency Alignment for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14217-14226. 2022
[c] Li, Yu-Jhe, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu, Kan Chen, Bichen Wu, Zijian He, Kris Kitani, and Peter Vajda. "Cross-Domain Adaptive Teacher for Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 7581-7590. 2022
[d] Wu, Jiaxi, Jiaxin Chen, Mengzhe He, Yiru Wang, Bo Li, Bingqi Ma, Weihao Gan, Wei Wu, Yali Wang, and Di Huang. "Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 5301-5310. 2022.
[e] Li, Wuyang, Xinyu Liu, and Yixuan Yuan. "SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022.
[f] Xu, Yunqiu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi Yang. "H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., 2022.
Minor comments
Figure 2 is not clear for IPM. Does the class-wise average only apply to the support set? If so, what is the process for the query set (red features). This is missing in the figure.
In figure 2, since global average pooling is also performed aligned with class, is it the same as class-wise average? Why use two different names?


Review Point: The design of the model is not well-motivated. the domain gap between target and source is usually the main point to decrease the generalization of the model. However, both the proposed IPNet and FSAT do not directly address the domain shift or use any domain loss. I feel like fusing two domain features brutally will still suffer from domain shift when using the prototyping. Only the task of the few-shot challenge is addressed here. The idea of using FSAT for separating foreground and background is not well-motivated. Does this module aim to address the issue of missing detection or learn a better representation for feature encoder? Looks like this module is only to regularize the training of feature extractor like ResNet101 or VGG16 with prototypes from IPNet? It would be also good to explain why RPN is not trained in both of the two modules if this is the case. FSAT is using adversarial learning for foreground and background instead of source or target. While this learning process may help to address the domain gap without aligning the distribution between source and target, would the inherent domain shift affect the performance of the foreground/background separation? Since we only have few labeled target images and many source images, the issue of data imbalance will be taking place in this model. Will the imbalance of source and target images affect the model? I see only the focal loss applied for boxes imbalance issue. Lack of analysis: The idea of prototyping in IPM is not new and does not have further revision or customized for cross-domain tasks. Obviously, directly fusing two data would suffer from the domain gap. For example, does the improvement come from seeing few labeled data from the target or the model is really addressing the domain gap?
Review Point: 2)How would separating foreground and background be effective for domain shift is missing. Missing context The setting of few-shot cross-domain object detection is not strictly defined well. For example, CDOD assumes the labels of target domains are unavailable. Here FSCDOD can access the labels from the target domain. Many recent DAOD related works in CVPR are missing. For example: [a] He, Mengzhe, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan, Wei Wu, and Yu Qiao. "Cross Domain Object Detection by Target-Perceived Dual Branch Distillation." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 9570-9580. 2022. [b]Zhao, Liang, and Limin Wang. "Task-specific Inconsistency Alignment for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14217-14226.
Review Point: 2022 [c] Li, Yu-Jhe, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu, Kan Chen, Bichen Wu, Zijian He, Kris Kitani, and Peter Vajda. "Cross-Domain Adaptive Teacher for Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 7581-7590.
Review Point: 2022 [d] Wu, Jiaxi, Jiaxin Chen, Mengzhe He, Yiru Wang, Bo Li, Bingqi Ma, Weihao Gan, Wei Wu, Yali Wang, and Di Huang. "Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 5301-5310. 2022. [e] Li, Wuyang, Xinyu Liu, and Yixuan Yuan. "SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. [f] Xu, Yunqiu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi Yang. "H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., 2022. Minor comments Figure 2 is not clear for IPM. Does the class-wise average only apply to the support set? If so, what is the process for the query set (red features). This is missing in the figure. In figure 2, since global average pooling is also performed aligned with class, is it the same as class-wise average? Why use two different names?
==================================================

Focused review:

Despite being well written with adequate insights, the manuscript would make for a stronger submission by addressing some of the concerns below, 1. The work demonstrates high quality results however, the presented framework is an incremental extension of [5]. Particularly, the approach augments the loss in [5] with an adversarial component and feeds the conditioning at each layer similar to StyleGan. 2. The experimental evaluation presented in significantly rigorous as compared to the most prominent prior art. Particularly, the authors provide results only on 2 texture classes. 3. Lines 174: The authors make a design choice to split the frequencies into different sized bins and feed it to different layers. The authors claim that this is beneficial since this reduces the number of parameters. Is this beneficial in terms of performance? or training speed? . Where exactly are the benefits of this choice manifested? An experiment to support this design choice would be instructive. Particularly, demonstrating an ablation with feeding the full vector at each layer with A being (m x n) instead of (m x n/4). 4. The authors provide quantitative evaluation highlighting that the generated patches have adequate similarity to input exemplar. However, as in the claims of lines 51-55, the authors present no evaluation regarding the diversity of the synthesized patches. Particularly, how do the authors make sure that the quality is not improved at the cost of diversity? 5. Comparison is provided only to [5] and no other previous methods. Although [5] demonstrates state of the art performance over several previous method already, it is important to highlight the performance of the presented framework in the context of other similar texture synthesis methods like [a,b,c]. 6. Lines 193: A more detailed treatment of "sufficiently similar" texture would provide better insights regarding the generalizability of the framework. Particularly, how does one measure the similarity of the textures. A simple study would be, to demonstrate the generalization performance as a function of Gram Matrix distance of a test texture to the nearest neighbor training texture. This will also lend credence to the fact that the proposed framework beats prior art in terms of quality. [a] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In CVPR, 2017 [b] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. In ICML,2016 [c] Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, and Michal Lukac. Texture Mixer: A network for controllable synthesis and interpolation of texture. In CVPR, 2019

Review Point: Despite being well written with adequate insights, the manuscript would make for a stronger submission by addressing some of the concerns below, 1. The work demonstrates high quality results however, the presented framework is an incremental extension of [5]. Particularly, the approach augments the loss in [5] with an adversarial component and feeds the conditioning at each layer similar to StyleGan.
Review Point: 2. The experimental evaluation presented in significantly rigorous as compared to the most prominent prior art. Particularly, the authors provide results only on 2 texture classes.
Review Point: 3. Lines 174: The authors make a design choice to split the frequencies into different sized bins and feed it to different layers. The authors claim that this is beneficial since this reduces the number of parameters. Is this beneficial in terms of performance? or training speed? . Where exactly are the benefits of this choice manifested? An experiment to support this design choice would be instructive. Particularly, demonstrating an ablation with feeding the full vector at each layer with A being (m x n) instead of (m x n/4).
Review Point: 4. The authors provide quantitative evaluation highlighting that the generated patches have adequate similarity to input exemplar. However, as in the claims of lines 51-55, the authors present no evaluation regarding the diversity of the synthesized patches. Particularly, how do the authors make sure that the quality is not improved at the cost of diversity?
Review Point: 5. Comparison is provided only to [5] and no other previous methods. Although [5] demonstrates state of the art performance over several previous method already, it is important to highlight the performance of the presented framework in the context of other similar texture synthesis methods like [a,b,c].
Review Point: 6. Lines 193: A more detailed treatment of "sufficiently similar" texture would provide better insights regarding the generalizability of the framework. Particularly, how does one measure the similarity of the textures. A simple study would be, to demonstrate the generalization performance as a function of Gram Matrix distance of a test texture to the nearest neighbor training texture. This will also lend credence to the fact that the proposed framework beats prior art in terms of quality. [a] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In CVPR, 2017 [b] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. In ICML,2016 [c] Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, and Michal Lukac. Texture Mixer: A network for controllable synthesis and interpolation of texture. In CVPR, 2019
==================================================

Focused review:

Weaknesses:
Novelty: The abstract makes big claims (in bold) that may not be true. There are existing defense techniques that do not require clean data. Some papers that should be cited and methods to compare with the proposed method (others may exist):
Steinhardt, Jacob, Pang Wei W. Koh, and Percy S. Liang. "Certified defenses for data poisoning attacks." Advances in neural information processing systems 30 (2017). (https://arxiv.org/pdf/1706.03691.pdf)
Paudice, Andrea, et al. "Detection of adversarial training examples in poisoning attacks through anomaly detection." arXiv preprint arXiv:1802.03041 (2018). (https://arxiv.org/pdf/1802.03041.pdf)
Peri, Neehar, et al. "Deep k-nn defense against clean-label data poisoning attacks." European Conference on Computer Vision. Springer, Cham, 2020. (https://arxiv.org/pdf/1909.13374.pdf)
This work considers relatively outdated architectures and weak attacks. Some stronger poisoning and backdoor attacks (more exist):
Geiping, Jonas, et al. "Witches' brew: Industrial scale data poisoning via gradient matching." arXiv preprint arXiv:2009.02276 (2020).(https://arxiv.org/pdf/2009.02276.pdf)
Zhu, Chen, et al. "Transferable clean-label poisoning attacks on deep neural nets." International Conference on Machine Learning. PMLR, 2019. (http://proceedings.mlr.press/v97/zhu19a/zhu19a.pdf)
Aghakhani, Hojjat, et al. "Bullseye polytope: A scalable clean-label poisoning attack with improved transferability." 2021 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2021.
Saha, Aniruddha, Akshayvarun Subramanya, and Hamed Pirsiavash. "Hidden trigger backdoor attacks." Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020.
Missing related work. Some of the papers above have not even been cited. Some other papers probably should be cited. I'll point to a couple survey papers here:
Ahmed, Ibrahim M., and Manar Younis Kashmoola. "Threats on Machine Learning Technique by Data Poisoning Attack: A Survey." International Conference on Advances in Cyber Security. Springer, Singapore, 2021.
Goldblum, Micah, et al. "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses." IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
The writing is often unclear with many grammatical errors. Some examples:
Line 67 says that the paper will focus on backdoor attacks. Presumably, this is as opposed to poisoning attacks without a trigger. However, one of the three attacks is CLA, which has no trigger in the original method.
Line 30 says "poinson."
Line 97: "Specifically, we retrain the model using the channel shuffling operation, and then recover trigger based on the shuffle model..." Perhaps this should be "... recover the trigger..." or "... recover triggers ...".
Line 52 in the appendix says "We will future extend to this case."
Overall, I think there are interesting experiments, but there are problematic claims and missing relevant works/benchmarks. In my opinion, this paper is well below the threshold of acceptance in the state it is in. Short of showing that the proposed method does, in fact, beat the state of the art in poisoning detection/defense and better contextualized this method in the existing work, I recommend rejection.

After reading the authors' response and the updated submission (August 7):
My major concerned have been largely addressed. Thanks to much improved clarity around the setting, better contextualization with existing work, and additional experimental results, I can say this paper has much improved. I have increased my score accordingly and followed up with the comments below.
As a minor note, the discussion on limitations is still lacking.
The limitations are described in Section A4. There is very little description of ways in which this work on data security and model vulnerability is applicable (or not applicable) to modern architectures and real world data. In other words, a paper that purports to defend against any kind of attack should probably explain when and where this defense will work in practice. In my opinion, discussing limitations is critical for work on security and privacy.


Review Point: Novelty: The abstract makes big claims (in bold) that may not be true. There are existing defense techniques that do not require clean data. Some papers that should be cited and methods to compare with the proposed method (others may exist): Steinhardt, Jacob, Pang Wei W. Koh, and Percy S. Liang. "Certified defenses for data poisoning attacks." Advances in neural information processing systems 30 (2017). (https://arxiv.org/pdf/1706.03691.pdf) Paudice, Andrea, et al. "Detection of adversarial training examples in poisoning attacks through anomaly detection." arXiv preprint arXiv:1802.03041 (2018). (https://arxiv.org/pdf/1802.03041.pdf) Peri, Neehar, et al. "Deep k-nn defense against clean-label data poisoning attacks." European Conference on Computer Vision. Springer, Cham, 2020. (https://arxiv.org/pdf/1909.13374.pdf) This work considers relatively outdated architectures and weak attacks. Some stronger poisoning and backdoor attacks (more exist): Geiping, Jonas, et al. "Witches' brew: Industrial scale data poisoning via gradient matching." arXiv preprint arXiv:2009.02276 (2020).(https://arxiv.org/pdf/2009.02276.pdf) Zhu, Chen, et al. "Transferable clean-label poisoning attacks on deep neural nets." International Conference on Machine Learning. PMLR, 2019. (http://proceedings.mlr.press/v97/zhu19a/zhu19a.pdf) Aghakhani, Hojjat, et al. "Bullseye polytope: A scalable clean-label poisoning attack with improved transferability." 2021 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2021. Saha, Aniruddha, Akshayvarun Subramanya, and Hamed Pirsiavash. "Hidden trigger backdoor attacks." Proceedings of the AAAI conference on artificial intelligence. Vol.
Review Point: 34. No.07. 2020. Missing related work. Some of the papers above have not even been cited. Some other papers probably should be cited. I'll point to a couple survey papers here: Ahmed, Ibrahim M., and Manar Younis Kashmoola. "Threats on Machine Learning Technique by Data Poisoning Attack: A Survey." International Conference on Advances in Cyber Security. Springer, Singapore, 2021. Goldblum, Micah, et al. "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses." IEEE Transactions on Pattern Analysis and Machine Intelligence (2022). The writing is often unclear with many grammatical errors. Some examples: Line 67 says that the paper will focus on backdoor attacks. Presumably, this is as opposed to poisoning attacks without a trigger. However, one of the three attacks is CLA, which has no trigger in the original method. Line 30 says "poinson." Line 97: "Specifically, we retrain the model using the channel shuffling operation, and then recover trigger based on the shuffle model..." Perhaps this should be "... recover the trigger..." or "... recover triggers ...". Line 52 in the appendix says "We will future extend to this case." Overall, I think there are interesting experiments, but there are problematic claims and missing relevant works/benchmarks. In my opinion, this paper is well below the threshold of acceptance in the state it is in. Short of showing that the proposed method does, in fact, beat the state of the art in poisoning detection/defense and better contextualized this method in the existing work, I recommend rejection. After reading the authors' response and the updated submission (August 7): My major concerned have been largely addressed. Thanks to much improved clarity around the setting, better contextualization with existing work, and additional experimental results, I can say this paper has much improved. I have increased my score accordingly and followed up with the comments below. As a minor note, the discussion on limitations is still lacking. The limitations are described in Section A4. There is very little description of ways in which this work on data security and model vulnerability is applicable (or not applicable) to modern architectures and real world data. In other words, a paper that purports to defend against any kind of attack should probably explain when and where this defense will work in practice. In my opinion, discussing limitations is critical for work on security and privacy.
==================================================

Focused review:

Weaknesses:
General points:
-Measuring privacy loss in terms of privacy loss per iteration is not meaningful. Privacy loss accumulates, so it is very unusual and not informative to measure the loss of each step; instead epsilon should capture the privacy loss of the full algorithm (T rounds) to quantify the risk of inference attacks on the model
-The significance of the VN condition is not adequately explained. How exactly is this condition relevant to BR?
-The convergence results are not very interpretable, seem to have incorrect units
-Writing needs a lot of work
Specifics: 0. Abstract
-"...rendered invalid when workers enforce DP" is not accurate for two reasons: you only show this for a particular DP protocol, namely the Gaussian mechanism; showing it fundamentally for any DP mechanism would be much more interesting. Also, what you show is in terms of the VN condition, which is not explicitly connected to BR.
Introduction
-"trusted server" and "honest but curious server" are both used to describe the set up, which is confusing. Usually "trusted server" means honest and not curious (i.e. not a privacy threat).
-"Byzantine" should be informally defined or briefly described early on before it is used many times
-Dwork et al (2014) is a strange choice for citation for a sentence about private ML. There are many other more relevant works from 2014 to present. Also, "especially when considering neural networks" does not seem to add anything and I'm not sure I agree that neural nets are special in this sense.
-Abadi et al (2016) is the wrong/incomplete citation for DP SGD: Bassily et al (2014) and Song et al (2013) considered DP SGD earlier.
-Theorem (Informal): should state conditions on loss; need to define (or informally explain, at the very least) "approximated VN" before using it in a theorem; paragraph following Theorem is too much detail for not having defined VN
-"parameters have very little impact in most settings when considering DP or BR separately" is not clear
-Figure 1. Is the number of iterations fixed?
Problem Settign
-"common dataset" what does this mean? Are the m points divided/partitioned among n workers or all workers have access to D?
-"By far, the most widely used approach....is the differentially private version...": strong claim made without any evidence; also, there are many ways to provide DP besides Gaussian noise, so there is no single "the" DP version of SGD.
-"we are mainly interested in...per-step...privacy": Why?! This is not nearly as meaningful. If T goes to infinity then essentially the algorithm provides no privacy at all but your epsilon might still be small, which is very misleading.
-Def 2: explain intuition; provide an example of a BR GAR
Section 3:
-Algorithm 1: clarify the presentation. When you loop through "honest" and "Byzantine" workers, it seems as if the analyst/curator who is implementing the algorithm knows which workers are honest and not, which is clearly not the case
-Contextualize Theorem 1. the privacy properties of Gaussian mechanism and subsampling are well-known, so this theorem is not at all novel; this should be stated. Also the log term in the denominator can be tightened; can take
s
2
≈
C
2
l
o
g
(
1
/
δ
)
/
ϵ
2
m
2
-What's the significance of VN condition? How does it relate to BR?
-"when
ϵ
and
δ
are non-zero...": strange sentence because
s
increases as
ϵ
and
δ
decrease.
-Theorem 2: why doesn't
κ
appear? Units appear to be wrong. Should provide comparison to Aliastarh, Allen-Zhu, Li (2018) and the references therein. Also should compare to DP optimization rates
-Corollary 1: misleading because
ϵ
is not the actual privacy budget of full T-round algorithm, so first term should also scale with T. This remark applies to experiments too.


Review Point: General points: -Measuring privacy loss in terms of privacy loss per iteration is not meaningful. Privacy loss accumulates, so it is very unusual and not informative to measure the loss of each step; instead epsilon should capture the privacy loss of the full algorithm (T rounds) to quantify the risk of inference attacks on the model -The significance of the VN condition is not adequately explained. How exactly is this condition relevant to BR? -The convergence results are not very interpretable, seem to have incorrect units -Writing needs a lot of work Specifics:
Review Point: 0. Abstract -"...rendered invalid when workers enforce DP" is not accurate for two reasons: you only show this for a particular DP protocol, namely the Gaussian mechanism; showing it fundamentally for any DP mechanism would be much more interesting. Also, what you show is in terms of the VN condition, which is not explicitly connected to BR. Introduction -"trusted server" and "honest but curious server" are both used to describe the set up, which is confusing. Usually "trusted server" means honest and not curious (i.e. not a privacy threat). -"Byzantine" should be informally defined or briefly described early on before it is used many times -Dwork et al (2014) is a strange choice for citation for a sentence about private ML. There are many other more relevant works from 2014 to present. Also, "especially when considering neural networks" does not seem to add anything and I'm not sure I agree that neural nets are special in this sense. -Abadi et al (2016) is the wrong/incomplete citation for DP SGD: Bassily et al (2014) and Song et al (2013) considered DP SGD earlier. -Theorem (Informal): should state conditions on loss; need to define (or informally explain, at the very least) "approximated VN" before using it in a theorem; paragraph following Theorem is too much detail for not having defined VN -"parameters have very little impact in most settings when considering DP or BR separately" is not clear -Figure 1. Is the number of iterations fixed? Problem Settign -"common dataset" what does this mean? Are the m points divided/partitioned among n workers or all workers have access to D? -"By far, the most widely used approach....is the differentially private version...": strong claim made without any evidence; also, there are many ways to provide DP besides Gaussian noise, so there is no single "the" DP version of SGD. -"we are mainly interested in...per-step...privacy": Why?! This is not nearly as meaningful. If T goes to infinity then essentially the algorithm provides no privacy at all but your epsilon might still be small, which is very misleading. -Def 2: explain intuition; provide an example of a BR GAR Section 3: -Algorithm 1: clarify the presentation. When you loop through "honest" and "Byzantine" workers, it seems as if the analyst/curator who is implementing the algorithm knows which workers are honest and not, which is clearly not the case -Contextualize Theorem 1. the privacy properties of Gaussian mechanism and subsampling are well-known, so this theorem is not at all novel; this should be stated. Also the log term in the denominator can be tightened; can take s 2 ≈ C 2 l o g ( 1 / δ ) / ϵ 2 m 2 -What's the significance of VN condition? How does it relate to BR? -"when ϵ and δ are non-zero...": strange sentence because s increases as ϵ and δ decrease. -Theorem 2: why doesn't κ appear? Units appear to be wrong. Should provide comparison to Aliastarh, Allen-Zhu, Li (2018) and the references therein. Also should compare to DP optimization rates -Corollary 1: misleading because ϵ is not the actual privacy budget of full T-round algorithm, so first term should also scale with T. This remark applies to experiments too.
==================================================

Focused review:

- One of the biggest weaknesses is the clear discussion/motivation on why other methods are problematic. This is important because the use of continuous latent space in this kind of problem has been rigorously discussed in the ML literature and without clearly differentiating with prior works, the contribution is weakened. - The experiments for ablation is missing. For example, the effect of bias of importance sampling (or benefit of the beam search) is not demonstrated empirically.

Review Point: - One of the biggest weaknesses is the clear discussion/motivation on why other methods are problematic. This is important because the use of continuous latent space in this kind of problem has been rigorously discussed in the ML literature and without clearly differentiating with prior works, the contribution is weakened.
Review Point: - The experiments for ablation is missing. For example, the effect of bias of importance sampling (or benefit of the beam search) is not demonstrated empirically.
==================================================

Focused review:

1. Authors claim the proposed method outperforms OPDOP (Ding et al.,2020) given the fact that OPDOP’s result depends on |S, |A|, and L. However, I do not see that from your theoretical results. The regret and constraint violation bounds of the proposed algorithm are proportional to |S| sqrt(|A|) and L. 2. Even though Equ. (6) can be solved by an LP solver, there are still |S||A||S| decision variables. How do you solve such a large scale optimization in an efficient way? Could you present more details in terms of solving this optimization problem?

Review Point: 1. Authors claim the proposed method outperforms OPDOP (Ding et al.,2020) given the fact that OPDOP’s result depends on |S, |A|, and L. However, I do not see that from your theoretical results. The regret and constraint violation bounds of the proposed algorithm are proportional to |S| sqrt(|A|) and L.
Review Point: 2. Even though Equ. (6) can be solved by an LP solver, there are still |S||A||S| decision variables. How do you solve such a large scale optimization in an efficient way? Could you present more details in terms of solving this optimization problem?
==================================================

Focused review:

- similar to the other pre-training method like BERT, this requires massive computational resources. It is not easy to reproduce the result in this computational perspectives. - survey of the paper is not enough (or at least less structured). - frankly, the paper is difficult to read. We need to check the references to understand detailed descriptions, and it is not very self-sufficient.

Review Point: - similar to the other pre-training method like BERT, this requires massive computational resources. It is not easy to reproduce the result in this computational perspectives.
Review Point: - survey of the paper is not enough (or at least less structured).
Review Point: - frankly, the paper is difficult to read. We need to check the references to understand detailed descriptions, and it is not very self-sufficient.
==================================================

Focused review:

Weaknesses: • The overall idea is not novel, while the specific of the methods suggested here are. The idea of reweighting the training data according to their anomalousness degree has been done before (e.g. recent references [2,3,4]). • To construct the memory bank (i.e., the reference of the normality), the authors propose to reject “the top τ% patches with the highest outlier scores”. Do they implicitly assume that the ratio of the noise in the training data is known in advance? This assumption is strong in a real-world environment where the ratio of the contamination may vary according to the collected training data. • Experiments on other challenging state-of-the-art datasets are desirable. • Limitations are not thoroughly discussed (e.g., the complexity).
Clarity: • The paper is not particularly well written. But, the main points are comprehensible. • The notations of the formulas are sometimes confusing, which makes it difficult to follow. For example, in line 193, “w” denotes the anomaly weight. However, w denotes the batch position at line 140.
Relation to prior work: The authors state that “we are the first one to study the abnormal detection with noisy data”. However, many studies have been proposed in this research field: e.g., [5,6,7]
Reproducibility: • The conducted experiments are well described. • It would be helpful if the entire code would be made accessible in case the paper is accepted.


Review Point: • The overall idea is not novel, while the specific of the methods suggested here are. The idea of reweighting the training data according to their anomalousness degree has been done before (e.g. recent references [2,3,4]).
Review Point: • To construct the memory bank (i.e., the reference of the normality), the authors propose to reject “the top τ% patches with the highest outlier scores”. Do they implicitly assume that the ratio of the noise in the training data is known in advance? This assumption is strong in a real-world environment where the ratio of the contamination may vary according to the collected training data.
Review Point: • Limitations are not thoroughly discussed (e.g., the complexity). Clarity:
Review Point: • The paper is not particularly well written. But, the main points are comprehensible.
Review Point: • The notations of the formulas are sometimes confusing, which makes it difficult to follow. For example, in line 193, “w” denotes the anomaly weight. However, w denotes the batch position at line 140. Relation to prior work: The authors state that “we are the first one to study the abnormal detection with noisy data”. However, many studies have been proposed in this research field: e.g., [5,6,7] Reproducibility:
Review Point: • It would be helpful if the entire code would be made accessible in case the paper is accepted.
==================================================

Focused review:

Weaknesses: The paper can be improved based on the following points:
In the introduction section, it mentions that consumers/customers can use other's AI based IPs for prediction/classification purposes. This is now called AI as a Service (AIaaS). It would be good to mention this along with proper citation.
In the text, it is not evident what is the computational complexity of confusion neurons? One of the motivations mentioned in section 1 is that decrypting traditional encrypted networks are computationally expensive. Comparatively how inexpensive are including confusion neurone to achieve IP verification of neural networks? Please specify this. Please also justify the design choices.
Experimental section (section 4) is lacking comparative study with existing traditional works in the domain such as using watermark and encryption based neural networks. Please provide results on this matter to show the efficacy of the proposed method. 4)) There are several grammatical errors. Please rectify them. For example, "θ is the parameters" should be "θ can be denoted as the parameters."


Review Point: The paper can be improved based on the following points: In the introduction section, it mentions that consumers/customers can use other's AI based IPs for prediction/classification purposes. This is now called AI as a Service (AIaaS). It would be good to mention this along with proper citation. In the text, it is not evident what is the computational complexity of confusion neurons? One of the motivations mentioned in section 1 is that decrypting traditional encrypted networks are computationally expensive. Comparatively how inexpensive are including confusion neurone to achieve IP verification of neural networks? Please specify this. Please also justify the design choices. Experimental section (section 4) is lacking comparative study with existing traditional works in the domain such as using watermark and encryption based neural networks. Please provide results on this matter to show the efficacy of the proposed method.
Review Point: 4)) There are several grammatical errors. Please rectify them. For example, "θ is the parameters" should be "θ can be denoted as the parameters."
==================================================

Focused review:

Weaknesses: 1. Performance gains on downstream tasks of detection and instance segmentation are much lower -- how would the authors propose to improve these? 2. If the primary goal is to improve SSL performance on small models, I would have liked to see more analysis on how different design choices of setting up contrastive learning affect model performance and if these could aid performance improvement, in addition to knowledge distillation.
Questions and suggestions: 1. Adding fully-supervised baselines for small models in table 1 will be useful in understanding the gap between full supervision and SSL for these models. 2. In figure 3, does 100% (green line) represent the student network trained with 100% of labeled imagenet supervised data? It is hard to interpret what these numbers represent. 3. Minor point: Some citations, which should not be in parentheses, are in parentheses (e.g., Romero et al. page 8). Please fix this in the revision.


Review Point: 1. Performance gains on downstream tasks of detection and instance segmentation are much lower -- how would the authors propose to improve these?
Review Point: 2. If the primary goal is to improve SSL performance on small models, I would have liked to see more analysis on how different design choices of setting up contrastive learning affect model performance and if these could aid performance improvement, in addition to knowledge distillation. Questions and suggestions:
Review Point: 1. Adding fully-supervised baselines for small models in table 1 will be useful in understanding the gap between full supervision and SSL for these models.
Review Point: 2. In figure 3, does 100% (green line) represent the student network trained with 100% of labeled imagenet supervised data? It is hard to interpret what these numbers represent.
Review Point: 3. Minor point: Some citations, which should not be in parentheses, are in parentheses (e.g., Romero et al. page 8). Please fix this in the revision.
==================================================

Focused review:

1. Missing citations: It is similar to a prior work [ref1] which computes the variance of the past predictions; this idea was further developed into a general framework in [ref2]. These works should be discussed in the introduction. [ref4] should be mentioned in Section 3.2 because, to the best of my knowledge, it is the first work introducing random sampling in curriculum learning (called example dropout). 2. The statement may be revised that "updating instantaneous hardness typically requires extra inference steps of a model over all the samples" (Line 37). It may be true in the past but recent curriculum learning methods integrate this step inside mini-batch training and show a comparable convergence rate. See the SPADE algorithm in [ref2]. Besides, the authors did not talk about the extra GPU memory needed which appears to be the main reason that allows them to cut short in computation. 3. Is this metric specific to cyclic learning rate schedule, or it is general and can be used in other learning rate schedules, say piecewise exponential decay? If so, what was the performance compared to the cyclic learning rate. The reason to ask this question is because [ref3] shows that cyclic learning rates may lead to different learning patterns. [ref1] Chang, Haw-Shiuan, et al. "Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples." In NIPS 2017. [ref2] Jiang, Lu, et al. "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels." International Conference on Machine Learning. 2018. [ref3] Huang, Jinchi, et al. "O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks." Proceedings of the IEEE International Conference on Computer Vision. 2019. [ref4] Liang, Junwei, et al. "Learning to Detect Concepts from Webly-Labeled Video Data." IJCAI. 2016.

Review Point: 1. Missing citations: It is similar to a prior work [ref1] which computes the variance of the past predictions; this idea was further developed into a general framework in [ref2]. These works should be discussed in the introduction. [ref4] should be mentioned in Section 3.2 because, to the best of my knowledge, it is the first work introducing random sampling in curriculum learning (called example dropout).
Review Point: 2. The statement may be revised that "updating instantaneous hardness typically requires extra inference steps of a model over all the samples" (Line 37). It may be true in the past but recent curriculum learning methods integrate this step inside mini-batch training and show a comparable convergence rate. See the SPADE algorithm in [ref2]. Besides, the authors did not talk about the extra GPU memory needed which appears to be the main reason that allows them to cut short in computation.
Review Point: 3. Is this metric specific to cyclic learning rate schedule, or it is general and can be used in other learning rate schedules, say piecewise exponential decay? If so, what was the performance compared to the cyclic learning rate. The reason to ask this question is because [ref3] shows that cyclic learning rates may lead to different learning patterns. [ref1] Chang, Haw-Shiuan, et al. "Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples." In NIPS 2017. [ref2] Jiang, Lu, et al. "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels." International Conference on Machine Learning. 2018. [ref3] Huang, Jinchi, et al. "O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks." Proceedings of the IEEE International Conference on Computer Vision. 2019. [ref4] Liang, Junwei, et al. "Learning to Detect Concepts from Webly-Labeled Video Data." IJCAI. 2016.
==================================================

Focused review:

Weaknesses: - The authors claim that a volume-preserving mixing function is a natural restriction and is easily satisfied. I would like to see a stronger argument why this is true, as it seems easy to think of non-volume-preserving mixing functions. Such an argument should include why the triangle dataset and MNIST would be generated by volume-preserving mixing functions. - The experiments find that none of the assumptions in the main theorem are necessary for identifiability to hold. More discussion about what necessary conditions would look like would improve the paper. - The experiments are not very strong. Only quantitively analyzes the simple triangle dataset, so does not include a non-trivial dataset with known sources, so that identifiability can be quantified. Also, the experiments show large overlap with the experiments of [Sorrenson et al 2020]. An additional experiment with more complicated images with known sources would improve the paper. - It is unclear why the model does not fully succeed in identifying the true sources in the triangle dataset. Is one of the assumptions not satisfied? Are there learning difficulties?
Further comments: - It seems that there is a constraint that q(z u) must be equal to the push-forward of p(s u) through
g
∘
f
, in other words, that Figure 1 can be interpreted as a commuting diagram. However, this is never explicitly stated in the definition of the estimating model. Could the authors please clarify this? - I believe the appendix should be separately provides as supplementary material.
Typos: - In (18) in the appendix, z_0 should be s_0? - Above (23) in the appendix, should positive-defined be positive definite? Or positive semi-definite?


Review Point: - The authors claim that a volume-preserving mixing function is a natural restriction and is easily satisfied. I would like to see a stronger argument why this is true, as it seems easy to think of non-volume-preserving mixing functions. Such an argument should include why the triangle dataset and MNIST would be generated by volume-preserving mixing functions.
Review Point: - The experiments find that none of the assumptions in the main theorem are necessary for identifiability to hold. More discussion about what necessary conditions would look like would improve the paper.
Review Point: - The experiments are not very strong. Only quantitively analyzes the simple triangle dataset, so does not include a non-trivial dataset with known sources, so that identifiability can be quantified. Also, the experiments show large overlap with the experiments of [Sorrenson et al 2020]. An additional experiment with more complicated images with known sources would improve the paper.
Review Point: - It is unclear why the model does not fully succeed in identifying the true sources in the triangle dataset. Is one of the assumptions not satisfied? Are there learning difficulties? Further comments:
Review Point: - It seems that there is a constraint that q(z u) must be equal to the push-forward of p(s u) through g ∘ f , in other words, that Figure 1 can be interpreted as a commuting diagram. However, this is never explicitly stated in the definition of the estimating model. Could the authors please clarify this?
Review Point: - I believe the appendix should be separately provides as supplementary material. Typos:
Review Point: - In (18) in the appendix, z_0 should be s_0?
Review Point: - Above (23) in the appendix, should positive-defined be positive definite? Or positive semi-definite?
==================================================

Focused review:

Weaknesses
Clarity:
In the related works section, first paragraph, many italicized terms are used but not explained. I was familiar with all terms except primacy bias, which I had to look up separately while reading. But in general, these terms should be quickly defined to make the paper easier to read.
The first paragraph of 5.1.2 is written poorly and hard to read, please fix it.
Experiments:
IQMs are listed (which is great) but the authors should probably explain IQM and the “Fraction of runs with score >
τ
” statistics for readers unfamiliar with the benchmarking paper that introduced these metrics.
While the provided analysis on different algorithm design choices in light of high replay ratios is useful, I’m not sure that what the authors analyzed is the most sensible thing to analyze. The paper is about replay ratio scaling through resetting network parameters. Therefore, there should be analysis on two things: 1) different replay ratios, and 2) how/when to reset network parameters. The authors provide analysis of 1) through trying different replay ratios, but insufficient analysis of 2) as in Section 4 they define their reset strategies and just stick with it. I think the paper really should have this analysis to be more useful as an empirical study to inform design choices for other researchers.
In combination with the experiments in Section 5, it would be useful to have an experiment with an offline RL algorithm (perhaps IQL or CQL). Offline RL is increasingly popular and also being deployed on real robots, so the authors should study their replay ratio increases with an offline algorithm to make the paper more useful for researchers.
Minor details:
“The number of agent updates per environment step is usually called replay ratio, and most standard algorithms were designed to have values around or below 1” → not sure if they were designed to have values around or below 1, I think this statement would be more accurate if it was something like “most standard algorithms are trained with a replay ratio around 1.”
“under tasks switches” → “under task switches”
“with a same algorithm” → “with the same algorithm”
“for evaluation and comparisons, we follow the protocol suggested by Agarwal et a. (2021)” missing a period


Review Point: Clarity: In the related works section, first paragraph, many italicized terms are used but not explained. I was familiar with all terms except primacy bias, which I had to look up separately while reading. But in general, these terms should be quickly defined to make the paper easier to read. The first paragraph of 5.1.2 is written poorly and hard to read, please fix it. Experiments: IQMs are listed (which is great) but the authors should probably explain IQM and the “Fraction of runs with score > τ ” statistics for readers unfamiliar with the benchmarking paper that introduced these metrics. While the provided analysis on different algorithm design choices in light of high replay ratios is useful, I’m not sure that what the authors analyzed is the most sensible thing to analyze. The paper is about replay ratio scaling through resetting network parameters. Therefore, there should be analysis on two things:
Review Point: 1) different replay ratios, and 2) how/when to reset network parameters. The authors provide analysis of 1) through trying different replay ratios, but insufficient analysis of 2) as in Section 4 they define their reset strategies and just stick with it. I think the paper really should have this analysis to be more useful as an empirical study to inform design choices for other researchers. In combination with the experiments in Section 5, it would be useful to have an experiment with an offline RL algorithm (perhaps IQL or CQL). Offline RL is increasingly popular and also being deployed on real robots, so the authors should study their replay ratio increases with an offline algorithm to make the paper more useful for researchers. Minor details: “The number of agent updates per environment step is usually called replay ratio, and most standard algorithms were designed to have values around or below 1” → not sure if they were designed to have values around or below 1, I think this statement would be more accurate if it was something like “most standard algorithms are trained with a replay ratio around 1.” “under tasks switches” → “under task switches” “with a same algorithm” → “with the same algorithm” “for evaluation and comparisons, we follow the protocol suggested by Agarwal et a. (2021)” missing a period
==================================================

Focused review:

Weaknesses:

1.	The approach mentions attention over 3 modalities â image, question and answer. However, it is not clear what attention over answers mean because most of the answers are single words and even if they are multiword, they are treated as single word. The paper does not present any visualizations for attention over answers. So, I would like the authors to clarify this.

2.	From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.

3.	Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).

4.	The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.

5.	Other errors/typos:
a.	L38: mechanism -> mechanisms
b.	L237 mentions that the evaluation is on validation set. However Table 1 reports numbers on the test-dev and test-std sets?

Post-rebuttal comments:

Although the authors' response to the concern of "Proposed model not outperforming existing models for 2 modalities" does not sound satisfactory to me due to lack of quantitative evidence, I would like to recommend acceptance because of the generic attention framework for multiple modalities being proposed in the paper and quantitative results of 3-modality attention outperforming SOTA. The quantitative evaluation of the proposed model's attention maps against human attention maps (reported in the rebuttal) also looks good and suggests that the attention maps are more correlation with human maps' than existing models. Although, we don't know what this correlation value is for SOTA models such as MCB, I think it is still significantly better than that for HieCoAtt.

I have a question about one of the responses from the authors --

> Authors' response -- âMCB vs. MCTâ: MCT is a generic extension of MCB for n-modalities. Specifically for VQA the 3-MCT setup yields 68.4% on test-dev where 2-layer MCB yields 69.4%. We tested other combinations of more than 2-modalities MCB and found them to yield inferior results.

Are the numbers swapped here? 69.4% should be for 3-MCT, right? Also, the MCB overall performance in table 1 is 68.6%. So, not sure which number the authors are referring to when they report 68.4%.

Review Point: 1. The approach mentions attention over 3 modalities â image, question and answer. However, it is not clear what attention over answers mean because most of the answers are single words and even if they are multiword, they are treated as single word. The paper does not present any visualizations for attention over answers. So, I would like the authors to clarify this.
Review Point: 2. From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.
Review Point: 3. Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).
Review Point: 4. The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.
Review Point: 5. Other errors/typos: a.L38: mechanism -> mechanisms b. L237 mentions that the evaluation is on validation set. However Table 1 reports numbers on the test-dev and test-std sets? Post-rebuttal comments: Although the authors' response to the concern of "Proposed model not outperforming existing models for 2 modalities" does not sound satisfactory to me due to lack of quantitative evidence, I would like to recommend acceptance because of the generic attention framework for multiple modalities being proposed in the paper and quantitative results of 3-modality attention outperforming SOTA. The quantitative evaluation of the proposed model's attention maps against human attention maps (reported in the rebuttal) also looks good and suggests that the attention maps are more correlation with human maps' than existing models. Although, we don't know what this correlation value is for SOTA models such as MCB, I think it is still significantly better than that for HieCoAtt. I have a question about one of the responses from the authors -- > Authors' response -- âMCB vs. MCTâ: MCT is a generic extension of MCB for n-modalities. Specifically for VQA the 3-MCT setup yields 68.4% on test-dev where 2-layer MCB yields 69.4%. We tested other combinations of more than 2-modalities MCB and found them to yield inferior results. Are the numbers swapped here? 69.4% should be for 3-MCT, right? Also, the MCB overall performance in table 1 is 68.6%. So, not sure which number the authors are referring to when they report 68.4%.
==================================================

Focused review:

1. It is nice that there is an experimental section, but it refers to a very simple setting. It would have been more convincing if the authors included results with more than two groups (even in the appendix). 2. The feedback model of Section 4 seems to be too strong. In the online case, if we choose one specific group for task n, how can we expect to have delayed information over all groups for task n? Of course, by making this assumption, the authors can get good empirical estimates, so such an assumption seems to be convenient to complete the proof. But despite the motivating examples behind it, I do not find it very realistic.

Review Point: 1. It is nice that there is an experimental section, but it refers to a very simple setting. It would have been more convincing if the authors included results with more than two groups (even in the appendix).
Review Point: 2. The feedback model of Section 4 seems to be too strong. In the online case, if we choose one specific group for task n, how can we expect to have delayed information over all groups for task n? Of course, by making this assumption, the authors can get good empirical estimates, so such an assumption seems to be convenient to complete the proof. But despite the motivating examples behind it, I do not find it very realistic.
==================================================

Focused review:

Weaknesses:
However, explanations of experiment results are lacking, for example why DPMAC with privacy constraints gains better performance on Predator-Prey than that without privacy constraints? What is the main conclusion on the ablation study of hyperparameters \epsilon? The results on three environments seem not consistent, the hyperparameter \epsilon has a significant impact only on predator-prey when training steps up to 1e6.
It would be better to verify the privacy-preserving property in a more rational scenario that naturally possesses private requirements; agents in an MPE environment seem not to have the necessity of privacy-preserving.
I wonder if it is necessary to do experiments on whether real private information (such as personal living routines) can be protected.
Why the Nash existence is important in a cooperative game, the Nash does not always correspond to the solution with the greatest payoff in a cooperative game. And the first two lines in section 6 clarify that the “Many cooperative multi-agent games enjoy the existence of a unique NE”, but this paper proves the existence of NE instead of unique NE. It would be better to unify these statements. 5.The theoretical analysis on Nash existence is given, but it would be better to conduct experiments about NE existence for completeness.


Review Point: However, explanations of experiment results are lacking, for example why DPMAC with privacy constraints gains better performance on Predator-Prey than that without privacy constraints? What is the main conclusion on the ablation study of hyperparameters \epsilon? The results on three environments seem not consistent, the hyperparameter \epsilon has a significant impact only on predator-prey when training steps up to 1e6. It would be better to verify the privacy-preserving property in a more rational scenario that naturally possesses private requirements; agents in an MPE environment seem not to have the necessity of privacy-preserving. I wonder if it is necessary to do experiments on whether real private information (such as personal living routines) can be protected. Why the Nash existence is important in a cooperative game, the Nash does not always correspond to the solution with the greatest payoff in a cooperative game. And the first two lines in section 6 clarify that the “Many cooperative multi-agent games enjoy the existence of a unique NE”, but this paper proves the existence of NE instead of unique NE. It would be better to unify these statements.
Review Point: 5.The theoretical analysis on Nash existence is given, but it would be better to conduct experiments about NE existence for completeness.
==================================================

Focused review:

1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' 

Review Point: 1. I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this!
Review Point: 2. More baselines should be contained such XLM, XLMR, mBART.
Review Point: 1. The teacher model should also be evaluated to verify whether the distillation process is necessary.
==================================================

Focused review:

Weaknesses
However, some key architectural details can be clarified further for full reproducibility and analysis. Specifically: 1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed. 2. Can each node attend to its own lower-level representation? From equation 2, it seems to be that only neighbouring nodes are attended to, based on the description of N_l^(s). 3. Do the authors have any guidelines on how to select S/A/C (and consequently N) for a given receptive field L?
In addition, while the ablation analysis tests the impact of changing CSCM architectures, it would be good to evaluate the base performance without the PAM to determine the value added by attention. This would also provide a simple comparison vs dilated CNNs which have been used successfully in time series forecasting applications (e.g. WaveNet).
Finally, could I double check which dataset was used for the ablation analysis as well? I seem to be having some difficulty lining the numbers in Tables 4-6 up with Table 3.


Review Point: However, some key architectural details can be clarified further for full reproducibility and analysis. Specifically:
Review Point: 1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed.
Review Point: 2. Can each node attend to its own lower-level representation? From equation 2, it seems to be that only neighbouring nodes are attended to, based on the description of N_l^(s).
Review Point: 3. Do the authors have any guidelines on how to select S/A/C (and consequently N) for a given receptive field L? In addition, while the ablation analysis tests the impact of changing CSCM architectures, it would be good to evaluate the base performance without the PAM to determine the value added by attention. This would also provide a simple comparison vs dilated CNNs which have been used successfully in time series forecasting applications (e.g. WaveNet). Finally, could I double check which dataset was used for the ablation analysis as well? I seem to be having some difficulty lining the numbers in Tables 4-6 up with Table 3.
==================================================

Focused review:

weaknesses: 1. The paper in general does not read well, and more careful proofreading is needed. 2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.


Review Point: 1. The paper in general does not read well, and more careful proofreading is needed.
Review Point: 2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.
==================================================

Focused review:

Weaknesses:
1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.
2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.
3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.


Review Point: 1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.
Review Point: 2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.
Review Point: 3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.
==================================================

Focused review:

1. In AdaptNAS, a domain discriminator is used to approximate the domain discrepancy, which might introduce a certain amount of computation overhead. 2. The results only show marginal improvement compared to previous state-of-the-art, especially P-DARTS[6] and MdeNAS[23]. In particular, [23] performs consistently better than the proposed method while only searching in CIFAR-10. 3. More importantly, it seems like there is no ablation study between using L_d (domain adaptation loss) or not. This makes it difficult to identify whether the performance is caused by using training data from both domains (L_S, L_T) or by the domain adaptation loss (L_d), which is the main contribution. 4. Also, in Tab 1, when alpha=0, there is indeed no L_d. However, it performs even better than most of the other settings where L_d presents (alpha > 0). This indicates the proposed L_d is less effective than directly utilizing data from the target domain. 5. What causes the inconsistency between Rot-4 and Rot-1? For the AdaptNAS-S, the Rot-4 version performs worse, while for the AdaptNAS-C, Rot-4 version performs better with even smaller network capacity. ------------------------------------------------- After rebuttal: The author convince me with a detailed explanation of my concerns. I encourage the author add these details to the final version of the paper.

Review Point: 1. In AdaptNAS, a domain discriminator is used to approximate the domain discrepancy, which might introduce a certain amount of computation overhead.
Review Point: 2. The results only show marginal improvement compared to previous state-of-the-art, especially P-DARTS[6] and MdeNAS[23]. In particular, [23] performs consistently better than the proposed method while only searching in CIFAR-10.
Review Point: 3. More importantly, it seems like there is no ablation study between using L_d (domain adaptation loss) or not. This makes it difficult to identify whether the performance is caused by using training data from both domains (L_S, L_T) or by the domain adaptation loss (L_d), which is the main contribution.
Review Point: 4. Also, in Tab 1, when alpha=0, there is indeed no L_d. However, it performs even better than most of the other settings where L_d presents (alpha > 0). This indicates the proposed L_d is less effective than directly utilizing data from the target domain.
Review Point: 5. What causes the inconsistency between Rot-4 and Rot-1? For the AdaptNAS-S, the Rot-4 version performs worse, while for the AdaptNAS-C, Rot-4 version performs better with even smaller network capacity. ------------------------------------------------- After rebuttal: The author convince me with a detailed explanation of my concerns. I encourage the author add these details to the final version of the paper.
==================================================

Focused review:

Weaknesses of the paper:
a. The paper does not discuss the relationship between faithfulness and variational perturbation. How does the proposed method helps to improve faithfulness?
b. The author can show analysis as mentioned on the robustness of the explanation improves using the proposed method.
c. What is the local perturbation method in this paper? In the literature, many explanation methods discussed perturbing input by adding variants of noises. How is this perturbation different from the previous.
c. The author should discuss the “Top k operator” in detail.
d. How did you select “biggest k% attributions”? Is there any analysis? Can you please mention the list of attributes for an image and provide a few such sample examples? The author also shows the distribution of each feature attribute for each image, as discussed in the abstract. Also, the author shows how this distribution of feature attributes helps to improve faithfulness and robustness.
e. How do you get these “unselected regions” in an image? The author can provide details about this.
f. “ The classes with predictive probability to examine the classifier’s behavior itself”. How do you know about predictive probability and classifier behavior? It depends on the complete model and type of perturbation
g. The method section is not written carefully. The terms are not discussed in more detail. The loss function and other details about the model are not discussed in the paper. The author should put more focus on structured writing in this section.
h. In the abstract, the author uses an uncertainty-aware explanation method, how it is different from Uncertainty Class Activation Map (U-CAM)[4] in terms of robustness and faithfulness of output. In this paper, the author proposed a Bayesian framework to obtain an explanation map, which contradicts the claims in the abstract.
i. The literature survey section in this paper is not so interesting to continue the flow. There should be a detailed literature survey on the perturbation-based methods[1,2,3,4], uncertainty/Bayesian-based methods for explanation schemes as discussed in [4]. Many more famous gradient-based methods, such as Grad-CAM, Grad-CAM++, U-CAM, Guided Grad-CAM, etc., are missing in the papers. As per the understanding, a blurred image is not a masking technique or a suitable perturbation method. Can you please elaborate on this work (blurred image)?
j. Petsiuk, et al. proposed a RISE-based method to perturb the image. Can you show attributions using the RISE-based local perturbation method? How good the proposed method is in terms of faithfulness and robustness from the RISE method.
k. The visual feature attribution-based method is not discussed in the paper. How does the proposed method help to find visual attributes? Please discuss more details on it.
l. The author could motivate the problem statements, that is, how does variational perturbation help improve visual attributes.
m. It is not clear that KL divergence in equation -4 helps to improve visual attribution.
n. The author should compare results with the LIME [1], SHAPE[2], LOO[3], and Occlusion based method for input perturbation, and U-CAM[4] method for logit perturbation.
o. In table-3 of the experiments section, The author has evaluated both Probability difference vs. logit difference scores. Are both of them required?
p. Zhang et al. has discussed a Pointing Game [5] method to evaluate the Localization ability of an explanation map as discussed in the SCORE-CAM[6].
q. The pseudo-code is not so informative to get the proposed algorithm. Few components(soft_topk_approximator, explainer_vp) are not discussed in the pseudo-code.
Ref:
Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ” Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,1135–1144.
Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems, 4765–4774.
Li, J.; Monroe, W.; and Jurafsky, D. 2016. Understanding neural networks through representation erasure. arXivpreprint arXiv:1612.08220.
Patro, Badri N., Mayank Lunayach, Shivansh Patel, and Vinay P. Namboodiri. "U-cam: Visual explanation using uncertainty based class activation maps." In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7444-7453. 2019.
Zhang, Jianming, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. "Top-down neural attention by excitation backprop." International Journal of Computer Vision 126, no. 10 (2018): 1084-1102.
Wang, Haofan, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. "Score-CAM: Score-weighted visual explanations for convolutional neural networks." In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 24-25. 2020.


Review Point: of the paper: a. The paper does not discuss the relationship between faithfulness and variational perturbation. How does the proposed method helps to improve faithfulness? b. The author can show analysis as mentioned on the robustness of the explanation improves using the proposed method. c. What is the local perturbation method in this paper? In the literature, many explanation methods discussed perturbing input by adding variants of noises. How is this perturbation different from the previous. c. The author should discuss the “Top k operator” in detail. d. How did you select “biggest k% attributions”? Is there any analysis? Can you please mention the list of attributes for an image and provide a few such sample examples? The author also shows the distribution of each feature attribute for each image, as discussed in the abstract. Also, the author shows how this distribution of feature attributes helps to improve faithfulness and robustness. e. How do you get these “unselected regions” in an image? The author can provide details about this. f. “ The classes with predictive probability to examine the classifier’s behavior itself”. How do you know about predictive probability and classifier behavior? It depends on the complete model and type of perturbation g. The method section is not written carefully. The terms are not discussed in more detail. The loss function and other details about the model are not discussed in the paper. The author should put more focus on structured writing in this section. h. In the abstract, the author uses an uncertainty-aware explanation method, how it is different from Uncertainty Class Activation Map (U-CAM)[4] in terms of robustness and faithfulness of output. In this paper, the author proposed a Bayesian framework to obtain an explanation map, which contradicts the claims in the abstract. i. The literature survey section in this paper is not so interesting to continue the flow. There should be a detailed literature survey on the perturbation-based methods[1,2,3,4], uncertainty/Bayesian-based methods for explanation schemes as discussed in [4]. Many more famous gradient-based methods, such as Grad-CAM, Grad-CAM++, U-CAM, Guided Grad-CAM, etc., are missing in the papers. As per the understanding, a blurred image is not a masking technique or a suitable perturbation method. Can you please elaborate on this work (blurred image)? j. Petsiuk, et al. proposed a RISE-based method to perturb the image. Can you show attributions using the RISE-based local perturbation method? How good the proposed method is in terms of faithfulness and robustness from the RISE method. k. The visual feature attribution-based method is not discussed in the paper. How does the proposed method help to find visual attributes? Please discuss more details on it. l. The author could motivate the problem statements, that is, how does variational perturbation help improve visual attributes. m. It is not clear that KL divergence in equation -4 helps to improve visual attribution. n. The author should compare results with the LIME [1], SHAPE[2], LOO[3], and Occlusion based method for input perturbation, and U-CAM[4] method for logit perturbation. o. In table-3 of the experiments section, The author has evaluated both Probability difference vs. logit difference scores. Are both of them required? p. Zhang et al. has discussed a Pointing Game [5] method to evaluate the Localization ability of an explanation map as discussed in the SCORE-CAM[6]. q. The pseudo-code is not so informative to get the proposed algorithm. Few components(soft_topk_approximator, explainer_vp) are not discussed in the pseudo-code. Ref: Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ” Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,1135–1144. Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems, 4765–4774. Li, J.; Monroe, W.; and Jurafsky, D. 2016. Understanding neural networks through representation erasure. arXivpreprint arXiv:1612.08220. Patro, Badri N., Mayank Lunayach, Shivansh Patel, and Vinay P. Namboodiri. "U-cam: Visual explanation using uncertainty based class activation maps." In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7444-7453. 2019. Zhang, Jianming, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. "Top-down neural attention by excitation backprop." International Journal of Computer Vision 126, no.
Review Point: 10 (2018): 1084-1102. Wang, Haofan, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. "Score-CAM: Score-weighted visual explanations for convolutional neural networks." In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 24-25. 2020.
==================================================

Focused review:

Weaknesses
1. Exposition and Clarity
Theory
The exposition of the theoretical results needs to be improved. Right now, the authors do not provide any intuition or proof sketches in the main text, while the proofs are complicated (in some cases based on intricate graph constructions along the lines of the CFI gadgets), which will make it very hard for future readers to obtain an actual understanding of the presented theory. For example:
Proposition 2 and Theorem 3 are important results because they settle an open question regarding the comparison of k-WL with subgraph-enhanced GNNs, but unfortunately are poorly explained, while the proofs in the supplementary material are confusing in certain parts. Moreover, there are some small inconsistencies between the main paper and the proofs, e.g. Theorem 3 says that k+1-OSANs can distinguish non-isomorphic graphs that k-OSANs deem isomorphic, while the proof in the supplementary (Theorem 10) claims a strict inclusion, which is a stronger result. Also, the proof is very brief and points to a theorem in a different paper without adding sufficient explanations. Could the authors provide a more extensive explanation in the rebuttal so as to be able to validate this claim? Also, the chain of arguments to prove these two claims in the supplementary is tangled. Could the authors clearly explain the chain of arguments in the rebuttal and in the main paper?
I am a bit confused by section E.3. supplementary material. How are these results different from section D? In the title the authors mention a comparison between the vertex-subgraph k-OSWL variant and k-WL, yet in the proofs below they mention k-OSWL. Moreover, I think I am missing something in Lemma 23 (which perhaps I am also missing in section D): you mention that k-OSWL is strictly less expressive than (k+1)-WL and that (k+1)-OSWL distinguishes graphs that (k+1)-WL deems isomorphic. However, I don’t see why this immediately implies that k-OSWL is strictly less expressive than (k+1)-OSWL, since there might be graphs that k-OSWL distinguishes, therefore also (k+1)-WL, but are deemed isomoprhic by (k+1)-OSWL.
Gradient computation
The presentation of the I-MLE is also quite elusive. I believe the authors should provide clarifications and improve a lot the exposition in this part, first because this is probably going to be knew to the interested readers from the GNN community, and second because data-driven subgraph sampling is a novel contribution that can be potentially impactful, but currently doesn’t do its complete justice.
Furthermore, it is not immediately clear to me why the authors chose this specific parametrisation for the subgraph distribution. Another potential and simpler choice could have been to iteratively sample without replacement k vertices M times (one for each subgraph) and then estimate the gradients with a variant of the REINFORCE algorithm (as in Bouritsas et al., NeurIPS'21). Another idea that comes to my mind is the Plackett-Luce distribution (M times, one for each subgraph), as in the NeuralSort (Grover et al., ICLR'19) or more recent ones. Note that this can be made deterministic (essentialy sampling the top-k vertices with the highest scores) and therefore the overall pipeline can become permutation invariant, contrary to the current version, which is permutation sensitive due to sampling. Is there any particular advantage of the algorithm chosen?
2. Experiments:
I believe that the experimental section should be extended and improved in some respects:
First: it would have been useful to include in your tables more baselines, and especially baselines falling under the k-OSAN framework (e.g., as with ESAN). The results in Table 2b raise some concerns that perhaps the data-driven subgraph sampling is not yet at a level that can be reliably used in practice, so more extensive comparisons and/or an explanatory discussion with the potential reasons for this and remedies, would help. Also, note that the performance of the current baseline is not immediately visible in the tables.
Second: even though subgraph sampling is learnable, the authors still have to make several design choices that seem arbitrary, i.e., the number of subgraphs to sample and the number of vertices per subgraph. Can the authors discuss any potential strategies to circumvent ad-hoc selections? An ablation study in a controlled setup, showing how the performance changes when increasing the number of subgraphs and the number of vertices would have been very helpful.
Another potential interesting experiment would be to select a more informative baseline, than the random one, for subgraph sampling, e.g., according to certain subgraph properties that either (1) are potentially related to the task at hand, or (2) can break graph symmetries and make the vertices more identifiable.
I found it a bit puzzling that the authors experimentally used unordered subgraphs, although all the theoretical analysis refers to ordered subgraphs. If I am not mistaken, unordered subgraphs are sufficient to simulate the architectures from previous works, but I am wondering if the theoretical results comparing k-OSANs to k-WL also hold for unordered subgraphs.
Subgraph sampling is not permutation equivariant w.r.t. the subgraphs. The authors mention in appendix C that they use an auxiliary loss to prevent degenerate solutions, but I believe more emphasis should be given to this and this part should be ablated.
The tables are not quite self-explanatory. What are the # and #Subg. Columns? Also, I think it is going to be helpful to explain how the operation used is implemented within the k-OSAN framework (I understand that vertex deletion means that the anchor subgraph has n-1 vertices, while vertex selection means a 1-vertex subgraph, is that correct?).
A more elaborate discussion on the limitations of the data-driven subgraph sampling and on the choice of the subgraph parameters would be helpful (see above).
No immediate negative societal impact


Review Point: 1. Exposition and Clarity Theory The exposition of the theoretical results needs to be improved. Right now, the authors do not provide any intuition or proof sketches in the main text, while the proofs are complicated (in some cases based on intricate graph constructions along the lines of the CFI gadgets), which will make it very hard for future readers to obtain an actual understanding of the presented theory. For example: Proposition 2 and Theorem 3 are important results because they settle an open question regarding the comparison of k-WL with subgraph-enhanced GNNs, but unfortunately are poorly explained, while the proofs in the supplementary material are confusing in certain parts. Moreover, there are some small inconsistencies between the main paper and the proofs, e.g. Theorem 3 says that k+1-OSANs can distinguish non-isomorphic graphs that k-OSANs deem isomorphic, while the proof in the supplementary (Theorem 10) claims a strict inclusion, which is a stronger result. Also, the proof is very brief and points to a theorem in a different paper without adding sufficient explanations. Could the authors provide a more extensive explanation in the rebuttal so as to be able to validate this claim? Also, the chain of arguments to prove these two claims in the supplementary is tangled. Could the authors clearly explain the chain of arguments in the rebuttal and in the main paper? I am a bit confused by section E.3. supplementary material. How are these results different from section D? In the title the authors mention a comparison between the vertex-subgraph k-OSWL variant and k-WL, yet in the proofs below they mention k-OSWL. Moreover, I think I am missing something in Lemma 23 (which perhaps I am also missing in section D): you mention that k-OSWL is strictly less expressive than (k+1)-WL and that (k+1)-OSWL distinguishes graphs that (k+1)-WL deems isomorphic. However, I don’t see why this immediately implies that k-OSWL is strictly less expressive than (k+1)-OSWL, since there might be graphs that k-OSWL distinguishes, therefore also (k+1)-WL, but are deemed isomoprhic by (k+1)-OSWL. Gradient computation The presentation of the I-MLE is also quite elusive. I believe the authors should provide clarifications and improve a lot the exposition in this part, first because this is probably going to be knew to the interested readers from the GNN community, and second because data-driven subgraph sampling is a novel contribution that can be potentially impactful, but currently doesn’t do its complete justice. Furthermore, it is not immediately clear to me why the authors chose this specific parametrisation for the subgraph distribution. Another potential and simpler choice could have been to iteratively sample without replacement k vertices M times (one for each subgraph) and then estimate the gradients with a variant of the REINFORCE algorithm (as in Bouritsas et al., NeurIPS'21). Another idea that comes to my mind is the Plackett-Luce distribution (M times, one for each subgraph), as in the NeuralSort (Grover et al., ICLR'19) or more recent ones. Note that this can be made deterministic (essentialy sampling the top-k vertices with the highest scores) and therefore the overall pipeline can become permutation invariant, contrary to the current version, which is permutation sensitive due to sampling. Is there any particular advantage of the algorithm chosen?
Review Point: 2. Experiments: I believe that the experimental section should be extended and improved in some respects: First: it would have been useful to include in your tables more baselines, and especially baselines falling under the k-OSAN framework (e.g., as with ESAN). The results in Table 2b raise some concerns that perhaps the data-driven subgraph sampling is not yet at a level that can be reliably used in practice, so more extensive comparisons and/or an explanatory discussion with the potential reasons for this and remedies, would help. Also, note that the performance of the current baseline is not immediately visible in the tables. Second: even though subgraph sampling is learnable, the authors still have to make several design choices that seem arbitrary, i.e., the number of subgraphs to sample and the number of vertices per subgraph. Can the authors discuss any potential strategies to circumvent ad-hoc selections? An ablation study in a controlled setup, showing how the performance changes when increasing the number of subgraphs and the number of vertices would have been very helpful. Another potential interesting experiment would be to select a more informative baseline, than the random one, for subgraph sampling, e.g., according to certain subgraph properties that either (1) are potentially related to the task at hand, or (2) can break graph symmetries and make the vertices more identifiable. I found it a bit puzzling that the authors experimentally used unordered subgraphs, although all the theoretical analysis refers to ordered subgraphs. If I am not mistaken, unordered subgraphs are sufficient to simulate the architectures from previous works, but I am wondering if the theoretical results comparing k-OSANs to k-WL also hold for unordered subgraphs. Subgraph sampling is not permutation equivariant w.r.t. the subgraphs. The authors mention in appendix C that they use an auxiliary loss to prevent degenerate solutions, but I believe more emphasis should be given to this and this part should be ablated. The tables are not quite self-explanatory. What are the # and #Subg. Columns? Also, I think it is going to be helpful to explain how the operation used is implemented within the k-OSAN framework (I understand that vertex deletion means that the anchor subgraph has n-1 vertices, while vertex selection means a 1-vertex subgraph, is that correct?). A more elaborate discussion on the limitations of the data-driven subgraph sampling and on the choice of the subgraph parameters would be helpful (see above). No immediate negative societal impact
==================================================

Focused review:

Since I found the exposition of this paper is not very clear in some parts, I would ask questions on the settings / designs, and then raise concerns. Please confirm/explain if anything. 1. Are the transferring functions between categories (e.g. human->cat) precomputed on two specific template mesh of human and cat, or two sets of mesh instances? If it’s from two specific template meshes, then I doubt the transformation estimated from functional maps would be general enough for transforming different types of cats. Further, I recall the LB basis doesn’t stay static (in corresponding location on surface) if the object mesh is deforming. Therefore the proposed method to get transformation would also not fit for cats in different poses (rigid and non-rigid). In this sense, the design of the algorithm in this part is already not suitable for the problem that aims to cover deformable objects in various appearances and poses. 2. The transfer function from “universal” descriptor to target descriptor is designed to be linear. Do you think the linear transfer is sufficient? Since the appearance of different animals are quite different, I doubt so. (regardless, this would be an interesting experiment) 3. Would you show the solved correspondence transfer in visual results? The difficult parts would be those parts for which humans are not significant (tails, ear, nose->elephant trunk). Those parts might be definable “anatomically” (humans have a tail bone corresponding to long tails of many animals), but the mapping will result in quite extreme deformation in correspondence (not isometric). 4. On results: I found the improvement from baseline (e.g. in table 2) is quite marginal. Table 2: the improvement of best cases of CSE to corresponding setting of baseline are mostly <1.0 in all metrics. The visual results also presents potential mismatches: Figure 4: Row 1 example 1, the feet of cats are mistaken. Similar issue in Row 1 example 6, Row 2 example 5, giraffe feet in Row 3 example 3 The elephant trunk looks very similar to the color mapping of feet. I see artifacts that look “blocky”, such as row 2 example 3, row 4 example 4. 5. Improve visualization: might be good add the visualization that you would randomly sample some point on source image / scan and draw a line to the matched point by the proposed method. One more (slightly ambitious) visualization is to use the predicted descriptors to drive the template mesh (similar to the one in DensePose video results). This is actually a good application case, in that you could track animal motion from a single view image / video. 6. Improve writing: To me I feel the explanation on Laplacian-Beltrami operators and functional maps can be more concise or moved to supplement materials. Would be good to reorganize the procedure of compute the transfer and add more results.

Review Point: Since I found the exposition of this paper is not very clear in some parts, I would ask questions on the settings / designs, and then raise concerns. Please confirm/explain if anything.
Review Point: 1. Are the transferring functions between categories (e.g. human->cat) precomputed on two specific template mesh of human and cat, or two sets of mesh instances? If it’s from two specific template meshes, then I doubt the transformation estimated from functional maps would be general enough for transforming different types of cats. Further, I recall the LB basis doesn’t stay static (in corresponding location on surface) if the object mesh is deforming. Therefore the proposed method to get transformation would also not fit for cats in different poses (rigid and non-rigid). In this sense, the design of the algorithm in this part is already not suitable for the problem that aims to cover deformable objects in various appearances and poses.
Review Point: 2. The transfer function from “universal” descriptor to target descriptor is designed to be linear. Do you think the linear transfer is sufficient? Since the appearance of different animals are quite different, I doubt so. (regardless, this would be an interesting experiment) 3. Would you show the solved correspondence transfer in visual results? The difficult parts would be those parts for which humans are not significant (tails, ear, nose->elephant trunk). Those parts might be definable “anatomically” (humans have a tail bone corresponding to long tails of many animals), but the mapping will result in quite extreme deformation in correspondence (not isometric).
Review Point: 4. On results: I found the improvement from baseline (e.g. in table 2) is quite marginal. Table 2: the improvement of best cases of CSE to corresponding setting of baseline are mostly <1.0 in all metrics. The visual results also presents potential mismatches: Figure 4: Row 1 example 1, the feet of cats are mistaken. Similar issue in Row 1 example 6, Row 2 example 5, giraffe feet in Row 3 example 3 The elephant trunk looks very similar to the color mapping of feet. I see artifacts that look “blocky”, such as row 2 example 3, row 4 example 4.
Review Point: 5. Improve visualization: might be good add the visualization that you would randomly sample some point on source image / scan and draw a line to the matched point by the proposed method. One more (slightly ambitious) visualization is to use the predicted descriptors to drive the template mesh (similar to the one in DensePose video results). This is actually a good application case, in that you could track animal motion from a single view image / video.
Review Point: 6. Improve writing: To me I feel the explanation on Laplacian-Beltrami operators and functional maps can be more concise or moved to supplement materials. Would be good to reorganize the procedure of compute the transfer and add more results.
==================================================

Focused review:

Weaknesses:
-1. If I understand correctly, the proposed threshold scheme is not dynamically changed during inference. But, it is just learnable parameters during a training process. If so, I do not think it is a "dynamic" threshold scheme, which is just a learnable threshold scheme.
-2. The validation mainly relies on DesneNet. But the authors used 8-layer FC SNN for MNIST. How does the DesneNet work with the MNIST dataset? Does the proposed method work with other architectures well, e.g., ResNet, VGG?
-3. lines 282: "much fewer time steps needed" does not mean "suitable for processing real-time data." It depends on how much time spend on each time step. The logic does not hold here. The authors should report the processing speed for each time step.
-4. There are so many "-" in table 2. The authors did not explain why which I think is very necessary.
-5. The authors claimed "less memory space" but without providing corresponding proofs.
In summary, I think the validation is not convincing. If the proposed learnable threshold scheme only works with DenseNet, it is just an application. To make it more compelling, the authors should show the generalization of the proposed method. In addition, the authors should also show the experimental results of other tasks besides classification tasks. However, based on the current manuscript, I cannot see any generality of the proposed approach.
The proposed approach is not a dynamic threshold scheme. I suggest the authors call it a learnable threshold scheme.
The authors should compare other existing dynamic threshold schemes to the proposed learnable threshold, e.g., r1 and r2.
The validation only based on DesneNet is very limited. The authors should provide more experimental results to show the effectiveness of the proposed approach.
r1: Yunzhe Hao, Xuhui Huang, Meng Dong, and Bo Xu. A biologically plausible supervised learning method for spiking neural networks using the symmetric stdp rule. Neural Networks, 121:387–395, 2020. r2: TaeyoonKim,SumanHu,JaewookKim,JoonYoungKwak,JongkilPark,SuyounLee,InhoKim,Jong- Keuk Park, and YeonJoo Jeong. Spiking neural network (snn) with memristor synapses having non-linear weight update. Frontiers in computational neuroscience, 15:22, 2021.


Review Point: -1. If I understand correctly, the proposed threshold scheme is not dynamically changed during inference. But, it is just learnable parameters during a training process. If so, I do not think it is a "dynamic" threshold scheme, which is just a learnable threshold scheme. -2. The validation mainly relies on DesneNet. But the authors used 8-layer FC SNN for MNIST. How does the DesneNet work with the MNIST dataset? Does the proposed method work with other architectures well, e.g., ResNet, VGG? -3. lines 282: "much fewer time steps needed" does not mean "suitable for processing real-time data." It depends on how much time spend on each time step. The logic does not hold here. The authors should report the processing speed for each time step. -4. There are so many "-" in table 2. The authors did not explain why which I think is very necessary. -5. The authors claimed "less memory space" but without providing corresponding proofs. In summary, I think the validation is not convincing. If the proposed learnable threshold scheme only works with DenseNet, it is just an application. To make it more compelling, the authors should show the generalization of the proposed method. In addition, the authors should also show the experimental results of other tasks besides classification tasks. However, based on the current manuscript, I cannot see any generality of the proposed approach. The proposed approach is not a dynamic threshold scheme. I suggest the authors call it a learnable threshold scheme. The authors should compare other existing dynamic threshold schemes to the proposed learnable threshold, e.g., r1 and r2. The validation only based on DesneNet is very limited. The authors should provide more experimental results to show the effectiveness of the proposed approach.
Review Point: r1: Yunzhe Hao, Xuhui Huang, Meng Dong, and Bo Xu. A biologically plausible supervised learning method for spiking neural networks using the symmetric stdp rule. Neural Networks, 121:387–395, 2020.
Review Point: r2: TaeyoonKim,SumanHu,JaewookKim,JoonYoungKwak,JongkilPark,SuyounLee,InhoKim,Jong- Keuk Park, and YeonJoo Jeong. Spiking neural network (snn) with memristor synapses having non-linear weight update. Frontiers in computational neuroscience, 15:22, 2021.
==================================================

Focused review:

- Equivalence of classifiers and rates: this is somewhat questionable from the perspective of fairness, as the classifier encodes significantly more information about subgroups. For example, suppose the classifier is determining whom to hire for a job. The "rate" of positive classification for qualified candidates from group A can be the same as group B, but individuals from group B are only classified positively if they live far away from the job location, and individuals from group A are only classified positively if they live close to the job location. This exactly allows the classifier to discriminate, and would be caught by an expert looking at the classifier, but not someone who only has access to the rate information for groups A and B (without location breakdown). It would be helpful to have some description of how this equivalence is acceptable from a fairness perspective, e.g., if there is an assumption we can make about the information in the rates for a specific setting. - Unclear if the method uncovers true fairness preferences: one question with this method is how well this can capture the user's understanding of fairness. It seems that one of the main inputs an expert user can provide is which groups are important to consider, but this method seems to require that the groups are specified up front, and that the expert cannot ask to change them. It's also not clear how intersections of groups are handled, e.g. gender and race. - Realistic model? It's not clear how realistic the model is, in particular the linear preferences, and how well the model adapts in cases in which the underlying metric or preference does not conform to the assumptions. (The authors may have a sense that it is realistic, in which case it would be helpful to give an example setting to help the reader better understand the assumptions.) It's also unclear if the set of classifiers allowed can only be those which achieve the same rates for each group. In a medical context, it may not be acceptable from a fairness perspective to select a classifier which behaves sub-optimally on one group in order to equalize rates across groups, and it was unclear from the stated results whether classifiers which achieve rates on group A that cannot be achieved on group B would be allowed. A more clear, complete statement of the assumptions of the model would be helpful in assessing in which settings this elicitation procedure may be relevant. - Handling multiple users' preferences? It would be interesting to see how the method can be adapted in the case of multiple users with potentially conflicting preferences. - It's hard to reconcile the lack of flexibility in the framework in taking expert feedback (e.g., specifying different subgroups, non-linear preferences, inconsistent preferences) with the statements in the "Broader Impact" setting.

Review Point: - Equivalence of classifiers and rates: this is somewhat questionable from the perspective of fairness, as the classifier encodes significantly more information about subgroups. For example, suppose the classifier is determining whom to hire for a job. The "rate" of positive classification for qualified candidates from group A can be the same as group B, but individuals from group B are only classified positively if they live far away from the job location, and individuals from group A are only classified positively if they live close to the job location. This exactly allows the classifier to discriminate, and would be caught by an expert looking at the classifier, but not someone who only has access to the rate information for groups A and B (without location breakdown). It would be helpful to have some description of how this equivalence is acceptable from a fairness perspective, e.g., if there is an assumption we can make about the information in the rates for a specific setting.
Review Point: - Unclear if the method uncovers true fairness preferences: one question with this method is how well this can capture the user's understanding of fairness. It seems that one of the main inputs an expert user can provide is which groups are important to consider, but this method seems to require that the groups are specified up front, and that the expert cannot ask to change them. It's also not clear how intersections of groups are handled, e.g. gender and race.
Review Point: - Realistic model? It's not clear how realistic the model is, in particular the linear preferences, and how well the model adapts in cases in which the underlying metric or preference does not conform to the assumptions. (The authors may have a sense that it is realistic, in which case it would be helpful to give an example setting to help the reader better understand the assumptions.) It's also unclear if the set of classifiers allowed can only be those which achieve the same rates for each group. In a medical context, it may not be acceptable from a fairness perspective to select a classifier which behaves sub-optimally on one group in order to equalize rates across groups, and it was unclear from the stated results whether classifiers which achieve rates on group A that cannot be achieved on group B would be allowed. A more clear, complete statement of the assumptions of the model would be helpful in assessing in which settings this elicitation procedure may be relevant.
Review Point: - Handling multiple users' preferences? It would be interesting to see how the method can be adapted in the case of multiple users with potentially conflicting preferences.
Review Point: - It's hard to reconcile the lack of flexibility in the framework in taking expert feedback (e.g., specifying different subgroups, non-linear preferences, inconsistent preferences) with the statements in the "Broader Impact" setting.
==================================================

Focused review:

- While the authors have now edited the paper to motivate the syntactic paraphrasing task much better, my comments on a lack of motivation for architectural and modeling choices as well in deriving insights from the results still hold.  - The newly added results for the SGCP baseline look very different from the numbers reported in Kumar et al., 2020. The metrics reported in this paper for SGCP are significantly worse than even the copy baselines which makes me a bit doubtful about validity of the results reproduced by the authors.
Overall, through the revision the authors seem to have partially addressed my major issues with the first version but a significant portion is still unaddressed as pointed out above. I request the authors to try to rectify these in their final submission, especially the second point on the performance metrics for the SGCP baseline. 
1. In section 4.1 how is template encoder different from parse-tree encoder? Templates are also constituency parse trees right? 
2. It would have been useful if in Table 5, examples of the templates would have also been given. 

Review Point: - While the authors have now edited the paper to motivate the syntactic paraphrasing task much better, my comments on a lack of motivation for architectural and modeling choices as well in deriving insights from the results still hold.
Review Point: - The newly added results for the SGCP baseline look very different from the numbers reported in Kumar et al., 2020. The metrics reported in this paper for SGCP are significantly worse than even the copy baselines which makes me a bit doubtful about validity of the results reproduced by the authors. Overall, through the revision the authors seem to have partially addressed my major issues with the first version but a significant portion is still unaddressed as pointed out above. I request the authors to try to rectify these in their final submission, especially the second point on the performance metrics for the SGCP baseline.
Review Point: 1. In section 4.1 how is template encoder different from parse-tree encoder? Templates are also constituency parse trees right?
Review Point: 2. It would have been useful if in Table 5, examples of the templates would have also been given.
==================================================

Focused review:

1. It would be clearer if the accuracy rates of the methods were provided in a table. 2. The authors claimed that the method introduced in the paper “Mitigating Unwanted Biases with Adversarial Learning” is an in-processing algorithm. While this is not true as this method is a model-agnostic post-processing method that only changes the weights of the adversary. So, this model should be studied in the experiments. 3. The weight of the original model in all three methods should be known beforehand, while most of the cases we are dealing with black boxes. 4. Similar papers in the area are not mentioned, such as “Multiaccuracy: Black-Box Post-Processing for Fairness in Classification”, “Towards Debiasing Sentence Representations”, and “Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings”. ******************************************* After going through the rebuttal, even though the authers have presented more rexperimental results, I am still not clear about the motivation behind proposing algorithm 2 and 3 especially since the results show not much gain compared to the first algorithm. I also believe the authors still need to run more experiments and compare with more exiting works. Therefore, I will keep my score unchanged.

Review Point: 1. It would be clearer if the accuracy rates of the methods were provided in a table.
Review Point: 2. The authors claimed that the method introduced in the paper “Mitigating Unwanted Biases with Adversarial Learning” is an in-processing algorithm. While this is not true as this method is a model-agnostic post-processing method that only changes the weights of the adversary. So, this model should be studied in the experiments.
Review Point: 3. The weight of the original model in all three methods should be known beforehand, while most of the cases we are dealing with black boxes.
Review Point: 4. Similar papers in the area are not mentioned, such as “Multiaccuracy: Black-Box Post-Processing for Fairness in Classification”, “Towards Debiasing Sentence Representations”, and “Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings”. ******************************************* After going through the rebuttal, even though the authers have presented more rexperimental results, I am still not clear about the motivation behind proposing algorithm 2 and 3 especially since the results show not much gain compared to the first algorithm. I also believe the authors still need to run more experiments and compare with more exiting works. Therefore, I will keep my score unchanged.
==================================================

Focused review:

1. The convergence result seems to be more 'qualitative', showing a general effect of different error sources, rather than 'quantitative'. Most of the terms have a complicated dependence on the parameters of the problem class. I think, they can hardly be computed in practice. For example, they did not provide a direct answer to the question: how to choose 's' in practice? It somewhat expected that for 'big enough 's', the stochastic approximation would be reasonably good. However, I still believe that the given theoretical answers may help to advance in developing distributed methods. 2. Numerical experiments. It is not clear what is the 'Accuracy' on the graphs (is it the functional residual?). It looks like all the methods stuck at some level (85-95). I think, it is important to demonstrate that in *some regimes* the method (for example, full Newton method) may achieve near 100 accuracy.

Review Point: 1. The convergence result seems to be more 'qualitative', showing a general effect of different error sources, rather than 'quantitative'. Most of the terms have a complicated dependence on the parameters of the problem class. I think, they can hardly be computed in practice. For example, they did not provide a direct answer to the question: how to choose 's' in practice? It somewhat expected that for 'big enough 's', the stochastic approximation would be reasonably good. However, I still believe that the given theoretical answers may help to advance in developing distributed methods.
Review Point: 2. Numerical experiments. It is not clear what is the 'Accuracy' on the graphs (is it the functional residual?). It looks like all the methods stuck at some level (85-95). I think, it is important to demonstrate that in *some regimes* the method (for example, full Newton method) may achieve near 100 accuracy.
==================================================

Focused review:

Weaknesses: I was a bit puzzled by the fact that using larger contexts, beyond the sentences with blanks in them, did not help the models. After all, you were in a way using additional context in the HierEnc model, which accumulates knowledge from other contexts. There are two possible explanations: Either the sentences with blanks in them are across the board more informative for the task than the sentences without. This is the explanation suggested in the paper, but it seems a bit unintuitive that this should be the case. Another possible explanation is that the way that you were using additional context in HierEnc, using the temporal network, is much more useful than by enlarging individual contexts C and feeding that larger C into the recurrent network.  Do you think that that could be what is going on?
- General Discussion: I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution. 

Review Point: I was a bit puzzled by the fact that using larger contexts, beyond the sentences with blanks in them, did not help the models. After all, you were in a way using additional context in the HierEnc model, which accumulates knowledge from other contexts. There are two possible explanations: Either the sentences with blanks in them are across the board more informative for the task than the sentences without. This is the explanation suggested in the paper, but it seems a bit unintuitive that this should be the case. Another possible explanation is that the way that you were using additional context in HierEnc, using the temporal network, is much more useful than by enlarging individual contexts C and feeding that larger C into the recurrent network. Do you think that that could be what is going on?
Review Point: - General Discussion: I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution.
==================================================

Focused review:

1) Experiments are limited to image datasets. There are GANs for sequence generation, could the proposed method help improving this models as well? In this context is there a relation with e.g. MaskGAN [1] which uses a masking as augmentation. 2) Augmentation functions have to differentiable such that the error signal can backpropagate to the generator. Which augmentation functions are differentiable? 3) The main concern about this paper is the lack of theoretical elaboration, not sure if this paper should be published at a computer vision conference. [1] MaskGAN: Better Text Generation via Filling in the ____, William Fedus and Ian Goodfellow and Andrew Dai, 2018, URL= https://openreview.net/pdf?id=ByOExmWAb, ICLR 2018

Review Point: 1) Experiments are limited to image datasets. There are GANs for sequence generation, could the proposed method help improving this models as well? In this context is there a relation with e.g. MaskGAN [1] which uses a masking as augmentation.
Review Point: 2) Augmentation functions have to differentiable such that the error signal can backpropagate to the generator. Which augmentation functions are differentiable?
Review Point: 3) The main concern about this paper is the lack of theoretical elaboration, not sure if this paper should be published at a computer vision conference. [1] MaskGAN: Better Text Generation via Filling in the ____, William Fedus and Ian Goodfellow and Andrew Dai, 2018, URL= https://openreview.net/pdf?id=ByOExmWAb, ICLR 2018
==================================================

Focused review:

weaknesses  This paper is well written and clearly identifies the core technical ideas required to arrive at the main results. The build-up of the ideas going from squint/boa on a discretized set to averaging accelerability to learning the discretized set gradually guides the reader through the ideas used to derive the results in this paper. However, the proofs of the  main results can be quite laborious.   I quite liked the new averaging accelerability pseudo-metric and how it ties in with the analysis. It appears to be a useful new notion to measure the distance between the discretized set and the oracle for sparse oracles.   The fact that SABOA and BOA+ achieve fast rates for sparse oracles while maintaining a O(dT) runtime is interesting as thus far only non-efficient procedures were able to achieve this.      Minor comments  - footnote 3 has disappeared.  - there dot at the end of algorithm 1 falls on next line - proposition 1 appears to be missing the probability statement - line 150: evaluated thanks to a loss function -> evaluated by a loss function - line 173: I can not make sense of "one has to add in the grid of experts some points of the l_1-ball to the 2d corners - line 267: "Bernstein condition: it exists \alpha' > 0" it exists should be changed so that this makes sense - line 488: V_k/a -> V_k/a_k - line 526: I believe one still needs to apply Jensen's inequality to conclude the proof   

Review Point: This paper is well written and clearly identifies the core technical ideas required to arrive at the main results. The build-up of the ideas going from squint/boa on a discretized set to averaging accelerability to learning the discretized set gradually guides the reader through the ideas used to derive the results in this paper. However, the proofs of the main results can be quite laborious. I quite liked the new averaging accelerability pseudo-metric and how it ties in with the analysis. It appears to be a useful new notion to measure the distance between the discretized set and the oracle for sparse oracles. The fact that SABOA and BOA+ achieve fast rates for sparse oracles while maintaining a O(dT) runtime is interesting as thus far only non-efficient procedures were able to achieve this. Minor comments - footnote 3 has disappeared.
Review Point: - there dot at the end of algorithm 1 falls on next line - proposition 1 appears to be missing the probability statement - line 150: evaluated thanks to a loss function -> evaluated by a loss function - line 173: I can not make sense of "one has to add in the grid of experts some points of the l_1-ball to the 2d corners - line 267: "Bernstein condition: it exists \alpha' > 0" it exists should be changed so that this makes sense - line 488: V_k/a -> V_k/a_k - line 526: I believe one still needs to apply Jensen's inequality to conclude the proof
==================================================

Focused review:

1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.  2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better. 
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability.  - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column).  - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of "53", but I believe the value "52.9" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: "1/" and "2/" >> Maybe better use "(1)" and "(2)"; confused me first. 

Review Point: 1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.
Review Point: 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
Review Point: 3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
Review Point: 4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
Review Point: - 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
Review Point: - 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
Review Point: - 241: It would also be good to state the maximum number of tasks done by any annotator.
Review Point: - Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability.
Review Point: - Table 4 (2): The table exceeds the page width; that needs to be fixed.
Review Point: - Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column).
Review Point: - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
Review Point: - 305/310: Marie/Mary >> I think these should be written the same.
Review Point: - 357: The text speaks of "53", but I believe the value "52.9" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
Review Point: - 575/577: "1/" and "2/" >> Maybe better use "(1)" and "(2)"; confused me first.
==================================================

Focused review:

While I think the paper makes a good contribution, there are some limitation at the present stage: - [Remark 3.1] While it has been done in previous works, I think that a deeper understanding of those cases where modelling the pushforward P in (8) as a composition of perturbation in an RKHS does not introduce an error, would increase the quality of the work. Alternatively, trying to undestand the kind of error that this parametrization introduces would be valuable too. - The analysis does not cover explicitly what happens when the input measures \beta_i are absolutely continuous and one has to rely on samples. How does the sampling part impact the bound? - The experiments are limited to toy data. There is a range of problems with real data where barycenters can be used and it would be interesting to show performance of the method in those settings too.

Review Point: While I think the paper makes a good contribution, there are some limitation at the present stage:
Review Point: - [Remark 3.1] While it has been done in previous works, I think that a deeper understanding of those cases where modelling the pushforward P in (8) as a composition of perturbation in an RKHS does not introduce an error, would increase the quality of the work. Alternatively, trying to undestand the kind of error that this parametrization introduces would be valuable too.
Review Point: - The analysis does not cover explicitly what happens when the input measures \beta_i are absolutely continuous and one has to rely on samples. How does the sampling part impact the bound?
Review Point: - The experiments are limited to toy data. There is a range of problems with real data where barycenters can be used and it would be interesting to show performance of the method in those settings too.
==================================================

Focused review:

Weaknesses: i.          The conclusion is biased by the selected languages. 
ii.           The experiments do not cover the claim of this paper completely.
- General Discussion: This paper issues a simple but fundamental question about word representation: what subunit of a word is suitable to represent morphologies and how to compose the units. To answer this question, this paper applied word representations with various subunits (characters, character-trigram, and morphs) and composition functions (LSTM, CNN, and a simple addition) to the language modeling task to find the best combination. In addition, this paper evaluated the task for more than 10 languages. This is because languages are typologically diverse and the results can be different according to the word representation and composition function. From their experimental results, this paper concluded that character-level representations are more effective, but they are still imperfective in comparing them with a model with explicit knowledge of morphology. Another conclusion is that character-trigrams show reliable perplexity in the majority of the languages.  However, this paper leaves some issues behind.
-         First of all, there could be some selection bias of the experimental languages. This paper chose ten languages in four categories (up to three languages per a category). But, one basic question with the languages is “how can it be claimed that the languages are representatives of each category?” 
All the languages in the same category have the same tendency of word representation and composition function? How can it be proved? For instance, even in this paper, two languages belonging to the same typology (agglutinative) show different results. Therefore, at least to me, it seems to be better to focus on the languages tested in this paper instead of drawing a general conclusions about all languages.  -         There is some gap between the claim and the experiments. Is the language modeling the best task to prove the claim of this paper? Isn’t there any chance that the claim of this paper breaks in other tasks? Further explanation on this issue is needed.
-         In Section 5.2, this paper evaluated the proposed method only for Arabic. Is there any reason why the experiment is performed only for Arabic? 
There are plenty of languages with automatic morphological analyzers such as Japanese and Turkish.
-         This paper considers only character-trigram among various n-grams. Is there any good reason to choose only character-trigram? Is it always better than character-bigram or character-fourgram? In general, language modeling with n-grams is affected by corpus size and some other factors.  Minor typos:  - There is a missing reference in Introduction. ( 88 line in Page 1) - root-and-patter -> root-and-pattern (524 line in Page 6) 

Review Point: i. The conclusion is biased by the selected languages. ii. The experiments do not cover the claim of this paper completely.
Review Point: - General Discussion: This paper issues a simple but fundamental question about word representation: what subunit of a word is suitable to represent morphologies and how to compose the units. To answer this question, this paper applied word representations with various subunits (characters, character-trigram, and morphs) and composition functions (LSTM, CNN, and a simple addition) to the language modeling task to find the best combination. In addition, this paper evaluated the task for more than 10 languages. This is because languages are typologically diverse and the results can be different according to the word representation and composition function. From their experimental results, this paper concluded that character-level representations are more effective, but they are still imperfective in comparing them with a model with explicit knowledge of morphology. Another conclusion is that character-trigrams show reliable perplexity in the majority of the languages. However, this paper leaves some issues behind.
Review Point: - First of all, there could be some selection bias of the experimental languages. This paper chose ten languages in four categories (up to three languages per a category). But, one basic question with the languages is “how can it be claimed that the languages are representatives of each category?” All the languages in the same category have the same tendency of word representation and composition function? How can it be proved? For instance, even in this paper, two languages belonging to the same typology (agglutinative) show different results. Therefore, at least to me, it seems to be better to focus on the languages tested in this paper instead of drawing a general conclusions about all languages.
Review Point: - There is some gap between the claim and the experiments. Is the language modeling the best task to prove the claim of this paper? Isn’t there any chance that the claim of this paper breaks in other tasks? Further explanation on this issue is needed.
Review Point: - In Section 5.2, this paper evaluated the proposed method only for Arabic. Is there any reason why the experiment is performed only for Arabic? There are plenty of languages with automatic morphological analyzers such as Japanese and Turkish.
Review Point: - This paper considers only character-trigram among various n-grams. Is there any good reason to choose only character-trigram? Is it always better than character-bigram or character-fourgram? In general, language modeling with n-grams is affected by corpus size and some other factors. Minor typos:
Review Point: - There is a missing reference in Introduction. ( 88 line in Page 1) - root-and-patter -> root-and-pattern (524 line in Page 6)
==================================================

Focused review:

Weakness: 1. I found the paper hard to follow. Unfamiliar with local differential privacy, I found it hard to comprehend. The definition is in Section 2. I would urge the authors to present it in Section 1 2. The accuracy estimates  provided in the paper are probabilistic. Without proper experiments it is impossible to judge the tradeoff between privacy and accuracy. This paper does not provide any expt results 3. Since this is an iterative system, how scalable is the method? This is very important to understand this, since the authors guarantee diff privacy  after each epoch. There is a cost to pay for this in terms of the "delay" 4. From the simple problem of average of bits, how can we do go more complex data at each user? 5. No conclusion is provided  Updated after Author response:  I am still not happy that the authors did not do any expts. While theoretical results only provide a bound, the usefulness can only be found by thorough evaluation. I would also urge the authors to add a conclusion section since the takeaways become more informative after reading the whole paper.

Review Point: 1. I found the paper hard to follow. Unfamiliar with local differential privacy, I found it hard to comprehend. The definition is in Section 2. I would urge the authors to present it in Section 1 2. The accuracy estimates provided in the paper are probabilistic. Without proper experiments it is impossible to judge the tradeoff between privacy and accuracy. This paper does not provide any expt results 3. Since this is an iterative system, how scalable is the method? This is very important to understand this, since the authors guarantee diff privacy after each epoch. There is a cost to pay for this in terms of the "delay" 4. From the simple problem of average of bits, how can we do go more complex data at each user?
Review Point: 5. No conclusion is provided Updated after Author response: I am still not happy that the authors did not do any expts. While theoretical results only provide a bound, the usefulness can only be found by thorough evaluation. I would also urge the authors to add a conclusion section since the takeaways become more informative after reading the whole paper.
==================================================

Focused review:

- My major concern is that this paper misses important comparisons with the uniform channel scaling baseline. As shown in [1] that the final architecture plays a huge role in the effectiveness of channel/filter pruning, I think pruning methods should compare to a naive way of model scaling (width-multiplier). More specifically, width-multiplier uniformly increases or decreases the channel counts across all layers with the same proportion (e.g., 50% of the original channel counts). Such a method is very simple (near zero computational cost) and can match any desired FLOPs requirement. If after all the pruning efforts, one still fail to compare with uniform channel scaling, what is the point? To perform a fair comparison, I'd suggest training such a baseline using the epoch number of training+fine-tuning, which is 248 for ResNet on ImageNet, 512 for MobileNetv2 on ImageNet, and 400 for CIFAR according to this paper's setup. - I think [7] is also very relevant to this work that is not cited. [7] proposed a regularizer that improves upon L1 to avoid shrinkage for all parameters, which is of the same motivation for this work. I think it is necessary to compare with this work. - Another concern is regarding hyperparameter tuning. While this paper makes a good argument regarding the property of polarization regularizer compared to the L1 regularizer, it is has two hyperparameters and make pruning laborious. Most notably, since both hyperparameters affect FLOPs and accuracy, there should be multiple solutions that hit the same FLOPs, which makes the tuning process hard as one cannot navigate on the simple objective (FLOPs) to decide. According to current presentation, it is not clear how the grid search is done and I'd appreciate detailed discussions regarding this issue for reproducibility. - The empirical results only focus on pruning less than ~50% FLOPs away. It seems to me that going lower might not favor the proposed method due to the huge regularization imposed in the training loss. This seems to be a problem for L1 regularizer as well. However, there are many methods for filter pruning and it is not clear why one wants to go for regularization-based methods. It would be great if the paper provides a wider spectrum of FLOPs (this at least can be done by comparing the proposed method to uniform pruning and L1 pruning). - Since I can find better pruning results (available prior to the submission deadline) [2-6], I do not think it is appropriate to claim state-of-the-art. I agree the results of this paper look good, but without proper comparisons with these work (taking into account the differences in training settings), it seems to me the presented results are "comparable" to the current state-of-the-art. Also, it is informative to discuss these competitive methods [2-6] in the paper. [1] Liu, Zhuang, et al. "Rethinking the value of network pruning." ICLR 2019. [2] Guo, Shaopeng, et al. "DMCP: Differentiable Markov Channel Pruning for Neural Networks." arXiv preprint arXiv:2005.03354 (2020). [3] Chin, Ting-Wu, et al. "Towards Efficient Model Compression via Learned Global Ranking." arXiv preprint arXiv:1904.12368 (2019). [4] You, Zhonghui, et al. "Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks." Advances in Neural Information Processing Systems. 2019. [5] Dong, Xuanyi, and Yi Yang. "Network pruning via transformable architecture search." Advances in Neural Information Processing Systems. 2019. [6] Yu, Jiahui, and Thomas Huang. "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers." arXiv preprint arXiv:1903.11728 (2019). [7] Yang, Huanrui, Wei Wen, and Hai Li. "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures." ICLR 2020.

Review Point: - My major concern is that this paper misses important comparisons with the uniform channel scaling baseline. As shown in [1] that the final architecture plays a huge role in the effectiveness of channel/filter pruning, I think pruning methods should compare to a naive way of model scaling (width-multiplier). More specifically, width-multiplier uniformly increases or decreases the channel counts across all layers with the same proportion (e.g., 50% of the original channel counts). Such a method is very simple (near zero computational cost) and can match any desired FLOPs requirement. If after all the pruning efforts, one still fail to compare with uniform channel scaling, what is the point? To perform a fair comparison, I'd suggest training such a baseline using the epoch number of training+fine-tuning, which is 248 for ResNet on ImageNet, 512 for MobileNetv2 on ImageNet, and 400 for CIFAR according to this paper's setup.
Review Point: - I think [7] is also very relevant to this work that is not cited. [7] proposed a regularizer that improves upon L1 to avoid shrinkage for all parameters, which is of the same motivation for this work. I think it is necessary to compare with this work.
Review Point: - Another concern is regarding hyperparameter tuning. While this paper makes a good argument regarding the property of polarization regularizer compared to the L1 regularizer, it is has two hyperparameters and make pruning laborious. Most notably, since both hyperparameters affect FLOPs and accuracy, there should be multiple solutions that hit the same FLOPs, which makes the tuning process hard as one cannot navigate on the simple objective (FLOPs) to decide. According to current presentation, it is not clear how the grid search is done and I'd appreciate detailed discussions regarding this issue for reproducibility.
Review Point: - The empirical results only focus on pruning less than ~50% FLOPs away. It seems to me that going lower might not favor the proposed method due to the huge regularization imposed in the training loss. This seems to be a problem for L1 regularizer as well. However, there are many methods for filter pruning and it is not clear why one wants to go for regularization-based methods. It would be great if the paper provides a wider spectrum of FLOPs (this at least can be done by comparing the proposed method to uniform pruning and L1 pruning).
Review Point: - Since I can find better pruning results (available prior to the submission deadline) [2-6], I do not think it is appropriate to claim state-of-the-art. I agree the results of this paper look good, but without proper comparisons with these work (taking into account the differences in training settings), it seems to me the presented results are "comparable" to the current state-of-the-art. Also, it is informative to discuss these competitive methods [2-6] in the paper. [1] Liu, Zhuang, et al. "Rethinking the value of network pruning." ICLR 2019. [2] Guo, Shaopeng, et al. "DMCP: Differentiable Markov Channel Pruning for Neural Networks." arXiv preprint arXiv:2005.03354 (2020). [3] Chin, Ting-Wu, et al. "Towards Efficient Model Compression via Learned Global Ranking." arXiv preprint arXiv:1904.12368 (2019). [4] You, Zhonghui, et al. "Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks." Advances in Neural Information Processing Systems. 2019. [5] Dong, Xuanyi, and Yi Yang. "Network pruning via transformable architecture search." Advances in Neural Information Processing Systems. 2019. [6] Yu, Jiahui, and Thomas Huang. "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers." arXiv preprint arXiv:1903.11728 (2019). [7] Yang, Huanrui, Wei Wen, and Hai Li. "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures." ICLR 2020.
==================================================

Focused review:

Weakness:
1). The proposed method seems have several important hyper-parameters, that control the balance of bias and variance term, as well as the confidence bound. The methodology part does not give a clear guideline about how to choosing them. From empirical studies, it seems to be selected from the unbiased validation set? If I understand it correctly, it might be hard to get this validation set in real-world scenarios. Could the authors comment more on how these are selected for different experiments?
2). The uncertainty estimate is crucial for the uncertainty weight in Theorem 2. The NTK theory provides a nice quantification about the uncertainty for deep neural networks. However, I am concerned about the dimensionality of the gradient vector
g
, as well as the inversion stability of the matrix M. What are their corresponding dimensionality for the experiments? [1] uses the inverse of the diagonal of the matrix M as approximations. Could the authors comment more on how the quality of the uncertainty estimate affects both the evaluation and learning? It should be reflected in the MSE of the estimator as well as the policy optimization regret bound.
3). All the results are average results, without standard deviation. It would be great if the std is provided, and this could provide us more information about whether the gain over previous method is significant.
4). Could the authors comment more on the convergence criteria? For the policy optimization step, is it performed until full convergence? This part seems missing from the paper, and it is worth commenting more about the computational cost as well.
Reference: [1]. Neural Contextual Bandits with UCB-based Exploration


Review Point: 1). The proposed method seems have several important hyper-parameters, that control the balance of bias and variance term, as well as the confidence bound. The methodology part does not give a clear guideline about how to choosing them. From empirical studies, it seems to be selected from the unbiased validation set? If I understand it correctly, it might be hard to get this validation set in real-world scenarios. Could the authors comment more on how these are selected for different experiments?
Review Point: 2). The uncertainty estimate is crucial for the uncertainty weight in Theorem 2. The NTK theory provides a nice quantification about the uncertainty for deep neural networks. However, I am concerned about the dimensionality of the gradient vector g , as well as the inversion stability of the matrix M. What are their corresponding dimensionality for the experiments? [1] uses the inverse of the diagonal of the matrix M as approximations. Could the authors comment more on how the quality of the uncertainty estimate affects both the evaluation and learning? It should be reflected in the MSE of the estimator as well as the policy optimization regret bound.
Review Point: 3). All the results are average results, without standard deviation. It would be great if the std is provided, and this could provide us more information about whether the gain over previous method is significant.
Review Point: 4). Could the authors comment more on the convergence criteria? For the policy optimization step, is it performed until full convergence? This part seems missing from the paper, and it is worth commenting more about the computational cost as well. Reference: [1]. Neural Contextual Bandits with UCB-based Exploration
==================================================

Focused review:

1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space). 2. In a similar vein, I was envisioning a simple extension of SG-MCMC methods where we just do occasional random walks/Brownian motion in the original space (using Gaussian noise and ignoring the energies entirely) using some relatively cheap heuristic to detect if the fancier iterate sequence is stuck in local optima. Something like that would help explore the entire space (triggering some iterations of random walk if heuristic flags a local optima) without changing the underlying energy used in the real Sg-MCMC at each iteration. Wonder how that would compare to a method like the one in the paper. 3. Following from these, it would have been nice if the authors included synthetic experiments with asymmetric modes (one small mode to one side of a 2d space and one taller one on the other side) where the real mean would not be close to the 'center' and thus would not be easily approximable by a flatter version of the energy. Comparisons with something like the approach in 2. above would help understand the main insight of the paper better. 4. Runtime plots in the experiment sections on real data would be nice to have, for example plots of energy/error vs wall clock time / iteration count as provided for the synthetics. 5. Some insight on how to choose \delta_{u} and partition count for real datasets (like the values mentioned on line 173 for synthetic) would be nice to have.

Review Point: 1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space).
Review Point: 2. In a similar vein, I was envisioning a simple extension of SG-MCMC methods where we just do occasional random walks/Brownian motion in the original space (using Gaussian noise and ignoring the energies entirely) using some relatively cheap heuristic to detect if the fancier iterate sequence is stuck in local optima. Something like that would help explore the entire space (triggering some iterations of random walk if heuristic flags a local optima) without changing the underlying energy used in the real Sg-MCMC at each iteration. Wonder how that would compare to a method like the one in the paper.
Review Point: 3. Following from these, it would have been nice if the authors included synthetic experiments with asymmetric modes (one small mode to one side of a 2d space and one taller one on the other side) where the real mean would not be close to the 'center' and thus would not be easily approximable by a flatter version of the energy. Comparisons with something like the approach in 2. above would help understand the main insight of the paper better.
Review Point: 4. Runtime plots in the experiment sections on real data would be nice to have, for example plots of energy/error vs wall clock time / iteration count as provided for the synthetics.
Review Point: 5. Some insight on how to choose \delta_{u} and partition count for real datasets (like the values mentioned on line 173 for synthetic) would be nice to have.
==================================================

Focused review:

Weakness: 1. The authors should compare their methods to competing baselines. For example, the f-CMI bound from paper [1]. Moreover, the author should provide the results for more complex datasets, such as ImageNet. 2. In Figure3, the authors should plot training errors+ proposed generalization bounds, do they dominate the test error? In addition, the stability analysis is missing. And the authors should repeat the experiments and report the error bar to show the variance of the bounds in Figure 2&3. 3. The authors need to demonstrate their methods actually works in empirical settings. More concretely, the author should apply their methods on a set of different models trained on the same real-word dataset and show the predicted generalization bounds are consistent with the actual test error.
[1] Harutyunyan, Hrayr, et al. "Information-theoretic generalization bounds for black-box learning algorithms." Advances in Neural Information Processing Systems 34 (2021): 24670-24682.
My major concern is the utility of the methods, since the authors do not show any actual applications of their bounds to facilitate the real-world machine learning practices.


Review Point: 1. The authors should compare their methods to competing baselines. For example, the f-CMI bound from paper [1]. Moreover, the author should provide the results for more complex datasets, such as ImageNet.
Review Point: 2. In Figure3, the authors should plot training errors+ proposed generalization bounds, do they dominate the test error? In addition, the stability analysis is missing. And the authors should repeat the experiments and report the error bar to show the variance of the bounds in Figure 2&3.
Review Point: 3. The authors need to demonstrate their methods actually works in empirical settings. More concretely, the author should apply their methods on a set of different models trained on the same real-word dataset and show the predicted generalization bounds are consistent with the actual test error. [1] Harutyunyan, Hrayr, et al. "Information-theoretic generalization bounds for black-box learning algorithms." Advances in Neural Information Processing Systems 34 (2021): 24670-24682. My major concern is the utility of the methods, since the authors do not show any actual applications of their bounds to facilitate the real-world machine learning practices.
==================================================

Focused review:

Weakness:
One major issue with this paper is clarity of text and definitions. Examples:
Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature
Equation 3,
P
(
<
s
>
|
x
)
=
∑
y
∈
A
P
(
y
|
x
)
, I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)?
text issues:page 5, "is is useful think ..."
Many issues in Table1: 4.a row starting with "TRANSFORMER[36]": there is some number under LM (2.5/5.9), what does this mean? 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo?
The definition of the randomly split the words in bottom of page 6 is not clear to me.
The notation pDrop, how about p_drop ?
Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant.
Finally, there is a major concern about baseline comparisons:
All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3.
STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?


Review Point: One major issue with this paper is clarity of text and definitions. Examples: Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature Equation 3, P ( < s > | x ) = ∑ y ∈ A P ( y | x ) , I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)? text issues:page 5, "is is useful think ..." Many issues in Table1:
Review Point: 4.a row starting with "TRANSFORMER[36]": there is some number under LM (2.5/5.9), what does this mean?
Review Point: 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo? The definition of the randomly split the words in bottom of page 6 is not clear to me. The notation pDrop, how about p_drop ? Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant. Finally, there is a major concern about baseline comparisons: All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3. STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?
==================================================

Focused review:

Weaknesses
W1: Fig 1 is not clear. As the method seems having the order of operations, the figure without the order of operations is not clear. In addition, it is not clear how the pseudo-label generator is used to label the noisy labeled data.
W2: Empirical gain in small noise regime (SYM 20%-50% in CIFAR-10/100) are small.
W3: No empirical gain against SOTA in large dataset (Clothing1M)
W4: There are large gains in large noise regime (more than 80%). But this set-up may not be very realistic.


Review Point: W1: Fig 1 is not clear. As the method seems having the order of operations, the figure without the order of operations is not clear. In addition, it is not clear how the pseudo-label generator is used to label the noisy labeled data.
Review Point: W2: Empirical gain in small noise regime (SYM 20%-50% in CIFAR-10/100) are small.
Review Point: W3: No empirical gain against SOTA in large dataset (Clothing1M) W4: There are large gains in large noise regime (more than 80%). But this set-up may not be very realistic.
==================================================

Focused review:

Weaknesses:
I view the paper as trying to solve two independent problems, but the paper makes it seem they are intrinsically tied together. The two problems are: 1) learning a feature encoder and 2) exploration. While the paper does include an ablation study showing how important both these parts are, it would be nice to get a better understanding of these two independent components. For example, how does using the learned encoder improve other existing count-based methods, or how does using off-the-shelf feature encoders affect the performance of the elliptical exploration algorithm?


Review Point: I view the paper as trying to solve two independent problems, but the paper makes it seem they are intrinsically tied together. The two problems are:
Review Point: 1) learning a feature encoder and 2) exploration. While the paper does include an ablation study showing how important both these parts are, it would be nice to get a better understanding of these two independent components. For example, how does using the learned encoder improve other existing count-based methods, or how does using off-the-shelf feature encoders affect the performance of the elliptical exploration algorithm?
==================================================

Focused review:

I see no major weaknesses. But there are some minor issues that could be improved.
-  The description of the dataset is incomplete. It will be helpful to describe some demographic information of the singers in the dataset, such as gender and age range. What is the percetange of male and female singers? Are English songs recorded by native speakers? Male and female singers may pose slightly different challenges, so it will be good to know this. 
 - Have the forced alignment results been manually checked? Montreal Forced Aligner works well with modal speech but may not work well on singing. Will this affect the evaluation results? 
- Figure 4 is not easy to interpret. All pitch contours are mixed together. Is it possible to hightly the specific areas where DTW and CTW make errors? In this way it will be easier to spot the errors and validate the alignment result.  - The same for Figure 3. Since this is a new distance measure, it might not be familiar to readers. It will be immensely helpful if a numerical example could be provided. For example, this could be "if a points falls into region n with an angle of 30 degree, what is the numerical distance between this point and the anchor point?" 

Review Point: I see no major weaknesses. But there are some minor issues that could be improved.
Review Point: - The description of the dataset is incomplete. It will be helpful to describe some demographic information of the singers in the dataset, such as gender and age range. What is the percetange of male and female singers? Are English songs recorded by native speakers? Male and female singers may pose slightly different challenges, so it will be good to know this.
Review Point: - Have the forced alignment results been manually checked? Montreal Forced Aligner works well with modal speech but may not work well on singing. Will this affect the evaluation results?
Review Point: - Figure 4 is not easy to interpret. All pitch contours are mixed together. Is it possible to hightly the specific areas where DTW and CTW make errors? In this way it will be easier to spot the errors and validate the alignment result.
Review Point: - The same for Figure 3. Since this is a new distance measure, it might not be familiar to readers. It will be immensely helpful if a numerical example could be provided. For example, this could be "if a points falls into region n with an angle of 30 degree, what is the numerical distance between this point and the anchor point?"
==================================================

Focused review:

So far, I have many concerns about the current version. - I do not understand theorem 3.1, what is the length of the gradient flow? What does it mean to fool the network with a gradient flow starting at x_0? I admit the result is not clear at all. - In all the paper, I am a bit disturbed by d_k+1=o(d_k), it is not clear for me what it means in this context. - The footnote making equivalence between lower bound on singular values and c-surjectivity does not appear immediate or even false - The proofs are hardly readable: I suggest the authors to write them in a clearer way, it is too difficult to assess the correctness. - There are no experiments backing the theoretical claims: it would be good to illustrate them.

Review Point: So far, I have many concerns about the current version.
Review Point: - I do not understand theorem 3.1, what is the length of the gradient flow? What does it mean to fool the network with a gradient flow starting at x_0? I admit the result is not clear at all.
Review Point: - In all the paper, I am a bit disturbed by d_k+1=o(d_k), it is not clear for me what it means in this context.
Review Point: - The footnote making equivalence between lower bound on singular values and c-surjectivity does not appear immediate or even false - The proofs are hardly readable: I suggest the authors to write them in a clearer way, it is too difficult to assess the correctness.
Review Point: - There are no experiments backing the theoretical claims: it would be good to illustrate them.
==================================================

Focused review:

weaknesses.
• I feel that the technical novelty in the paper is quite limited. The parametrized random walk operator
P
(
ν
)
follows directly from the “generalized random walk Laplacian” operator
L
R
W
,
(
ν
)
of (Sevi et al.,2022) since
P
(
ν
)
=
I
−
L
R
W
,
(
ν
)
. This relation is currently stated in the form of Proposition 3.1, but this is a bit strange to me as there is nothing to prove here. The similarity kernel operator
K
t
,
ν
is obtained by taking the power of
P
(
ν
)
and subsequently normalizing it. However, this is quite similar to clustering approaches for undirected graphs which perform spectral clustering on the “the power of the adjacency matrix followed by an entry-wise truncation” (See for e.g. [1]). In essence, it is well-known that pre-processing a graph matrix (such as the adjacency) by taking a suitably high power of it (the exponent can be thought of as a regularizer term) leads to more robust clustering performance. So, I do not see new insight with regards to the proposed approach.
• The literature review is incomplete as there are many missing references for clustering directed graphs, see for e.g. [2,3,4,5] and reference therein. Therefore, I am not sure if the experiments are actually comparing with the state of the art for this problem. Additionally, I think it would have been beneficial to have more extensive experiments for synthetic examples – for e.g., on random generative models such as the directed Stochastic Block Model (SBM) (see [2]). Since the ground-truth is known, this would provide a clean framework for comparing the performance of the algorithms across different types of inputs (varying community sizes, noise levels, number of clusters etc.). The experiment results at the moment are incomplete in this sense.
• There also exist theoretical results for clustering directed graphs under a directed SBM generative model (e,g, [2]). In that sense, I believe it would have been interesting to provide some theoretical justification for the proposed approach on such a generative model.
Further comments:
• In Proposition 4.1, the notation
D
λ
,
D
d
has not been defined, I think.
• As mentioned above, Proposition 3.1 should really be a Definition since there is nothing to prove here.
• The choice of the vertex measure
ν
and the diffusion time
t
∗
are based on heuristics at the moment. I think it is natural to run some experiments on synthetic examples generated by a directed SBM, for different choices of the tuning parameters (e.g., through a grid search), and then to check whether the best "global" value is close to that returned by the heuristic (or maybe just the corresponding NMI values could compared).
References:
[1] Abbe et al., Graph Powering and Spectral Robustness, SIAM J. MATH. DATA SCI, Vol. 2, No. 1, pp. 132–157.
[2] Cucuringu et al., Hermitian matrices for clustering directed graphs: insights and applications, AISTATS, PMLR 108:983-992, 2020.
[3] Laenen and Sun, Higher-order spectral clustering of directed graphs. In NIPS'20, 941–951.
[4] Gong et al., Directed network Laplacians and random graph models, R. Soc. open sci., 2021.
[5] Hayashi et al., Skew-Symmetric Adjacency Matrices for Clustering Directed Graphs, arxiv: 2203.01388, 2022


Review Point: • I feel that the technical novelty in the paper is quite limited. The parametrized random walk operator P ( ν ) follows directly from the “generalized random walk Laplacian” operator L R W , ( ν ) of (Sevi et al.,2022) since P ( ν ) = I − L R W , ( ν ) . This relation is currently stated in the form of Proposition 3.1, but this is a bit strange to me as there is nothing to prove here. The similarity kernel operator K t , ν is obtained by taking the power of P ( ν ) and subsequently normalizing it. However, this is quite similar to clustering approaches for undirected graphs which perform spectral clustering on the “the power of the adjacency matrix followed by an entry-wise truncation” (See for e.g. [1]). In essence, it is well-known that pre-processing a graph matrix (such as the adjacency) by taking a suitably high power of it (the exponent can be thought of as a regularizer term) leads to more robust clustering performance. So, I do not see new insight with regards to the proposed approach.
Review Point: • The literature review is incomplete as there are many missing references for clustering directed graphs, see for e.g. [2,3,4,5] and reference therein. Therefore, I am not sure if the experiments are actually comparing with the state of the art for this problem. Additionally, I think it would have been beneficial to have more extensive experiments for synthetic examples – for e.g., on random generative models such as the directed Stochastic Block Model (SBM) (see [2]). Since the ground-truth is known, this would provide a clean framework for comparing the performance of the algorithms across different types of inputs (varying community sizes, noise levels, number of clusters etc.). The experiment results at the moment are incomplete in this sense.
Review Point: • There also exist theoretical results for clustering directed graphs under a directed SBM generative model (e,g, [2]). In that sense, I believe it would have been interesting to provide some theoretical justification for the proposed approach on such a generative model. Further comments:
Review Point: • In Proposition 4.1, the notation D λ , D d has not been defined, I think.
Review Point: • As mentioned above, Proposition 3.1 should really be a Definition since there is nothing to prove here.
Review Point: • The choice of the vertex measure ν and the diffusion time t ∗ are based on heuristics at the moment. I think it is natural to run some experiments on synthetic examples generated by a directed SBM, for different choices of the tuning parameters (e.g., through a grid search), and then to check whether the best "global" value is close to that returned by the heuristic (or maybe just the corresponding NMI values could compared). References: [1] Abbe et al., Graph Powering and Spectral Robustness, SIAM J. MATH. DATA SCI, Vol. 2, No. 1, pp. 132–157. [2] Cucuringu et al., Hermitian matrices for clustering directed graphs: insights and applications, AISTATS, PMLR 108:983-992, 2020. [3] Laenen and Sun, Higher-order spectral clustering of directed graphs. In NIPS'20, 941–951. [4] Gong et al., Directed network Laplacians and random graph models, R. Soc. open sci., 2021. [5] Hayashi et al., Skew-Symmetric Adjacency Matrices for Clustering Directed Graphs, arxiv: 2203.01388, 2022
==================================================

Focused review:

The work does not seem to have any major flaws, but there's a bunch of small to medium-sized weaknesses regarding the novelty, significance and relevance of the work. 1) the actual new objective is not of high novelty: switching from average to max is a well-known way of aiming for better worst-case performance 2) the motivation/discussion why to optimize for worst-case task loss is insufficient (see below) 3) the contribution on the side of optimization is not made clear enough. The proposed algorithm and its convergence analysis are based on prior work for min-max problems (of course), and it is not explained in how far the proposed steps are simply an application of that. 4) the established rates for the non -convex case ( max(1/eps^5,, 1/delta^5) ) are far from practical 5) the the generalization bounds are not useful as given, because they rely on a task-specific notion of Rademacher complexity, that is not explained, and not quantified. 6) the manuscript has no conclusion section, page 8 ends on experiments (the "Broader Impact" Section on page 9 is written as an Conclusion, but I disregard this, as that is not that section is meant to be, and it is beyond the page limit) 7) experiments are mostly unsurprising: switching from average to max loss, the average-case performance generally gets worse, the worst-case performance gets better Details: 2) The manuscript's arguments in favor of minimizing the max-loss across tasks is not fully convincing. The proposed objective is not "robust" as suggested in the manuscript's title, but it is brittle. A single "outlier" or "too hard" task would render the setting pointless. The manuscript mentions that adversarial tasks would be a problem, but an adversary is not required, already tasks of different difficulty (e.g. different Bayes error rates) should pose problems. Prior work (which is cited, e.g. [32],[9]) states explicitly that avg-loss has a lot of advantage, but that there is situations in which minimizing the max-loss can make sense. That, however, is across samples which come from the same distribution and emphasis is on the realizable setting, i.e. even the max can be 0, and the different to average loss is mainly of the optimization side. For multiple tasks, the differences and problem seem far bigger. I would have hoped to see a discussion of this, and potentially a justification. 5) The generalization bounds rely on standard arguments, which use the task-specific Rademacher complexity as a black box. For the reader to understand the implications of the bounds, the reader has to understand the behavior of the Rademacher complexity. Does it even converge to 0 for m->infty? Is the amount of test or the train data crucial for that? What's the dependence on \mathcal{W}? Maybe it could be expressed in terms of other existing, better understood, Rademacher complixity measures?

Review Point: The work does not seem to have any major flaws, but there's a bunch of small to medium-sized weaknesses regarding the novelty, significance and relevance of the work.
Review Point: 1) the actual new objective is not of high novelty: switching from average to max is a well-known way of aiming for better worst-case performance 2) the motivation/discussion why to optimize for worst-case task loss is insufficient (see below) 3) the contribution on the side of optimization is not made clear enough. The proposed algorithm and its convergence analysis are based on prior work for min-max problems (of course), and it is not explained in how far the proposed steps are simply an application of that.
Review Point: 4) the established rates for the non -convex case ( max(1/eps^5,, 1/delta^5) ) are far from practical 5) the the generalization bounds are not useful as given, because they rely on a task-specific notion of Rademacher complexity, that is not explained, and not quantified.
Review Point: 6) the manuscript has no conclusion section, page 8 ends on experiments (the "Broader Impact" Section on page 9 is written as an Conclusion, but I disregard this, as that is not that section is meant to be, and it is beyond the page limit) 7) experiments are mostly unsurprising: switching from average to max loss, the average-case performance generally gets worse, the worst-case performance gets better Details:
Review Point: 2) The manuscript's arguments in favor of minimizing the max-loss across tasks is not fully convincing. The proposed objective is not "robust" as suggested in the manuscript's title, but it is brittle. A single "outlier" or "too hard" task would render the setting pointless. The manuscript mentions that adversarial tasks would be a problem, but an adversary is not required, already tasks of different difficulty (e.g. different Bayes error rates) should pose problems. Prior work (which is cited, e.g. [32],[9]) states explicitly that avg-loss has a lot of advantage, but that there is situations in which minimizing the max-loss can make sense. That, however, is across samples which come from the same distribution and emphasis is on the realizable setting, i.e. even the max can be 0, and the different to average loss is mainly of the optimization side. For multiple tasks, the differences and problem seem far bigger. I would have hoped to see a discussion of this, and potentially a justification.
Review Point: 5) The generalization bounds rely on standard arguments, which use the task-specific Rademacher complexity as a black box. For the reader to understand the implications of the bounds, the reader has to understand the behavior of the Rademacher complexity. Does it even converge to 0 for m->infty? Is the amount of test or the train data crucial for that? What's the dependence on \mathcal{W}? Maybe it could be expressed in terms of other existing, better understood, Rademacher complixity measures?
==================================================

Focused review:

Weakness: 1.The novelty of the paper is limited. The two proposed acquisition functions are simple and straightforward extensions of NMI.
Section 4 which introduces the core methodology is very brief and it is not very clear how the proposed method addresses the clustering evaluation challenges. 3.The proposed approach is purely intuitive. It is hard to understand how and why it would work in general.


Review Point: 1.The novelty of the paper is limited. The two proposed acquisition functions are simple and straightforward extensions of NMI. Section 4 which introduces the core methodology is very brief and it is not very clear how the proposed method addresses the clustering evaluation challenges.
Review Point: 3.The proposed approach is purely intuitive. It is hard to understand how and why it would work in general.
==================================================

Focused review:

weaknesses of different chain-of-thought prompting methods: including zero-shot-CoT, few-shot-CoT, manual-CoT, and Auto-CoT. The paper conducts case study experiments to look into the limitations of existing methods and proposes improvement directions. Finally, the paper proposes an improved method for Auto-CoT that could achieve a competitive advantage over Manual-CoT.
2. The experiment part is very detailed and comprehensive.
3. The paper is well-organized.
The writing is good and most of the content is very clear to me.
Weaknesses/Feedback
1. The writing could be improved.
It would be helpful to draw a table to compare different CoT prompting methods across different dimensions.
How and why shall we make an assumption that “questions of all the wrong demonstrations fall into the same frequent-error cluster”?
Is the selection criteria in section 4.2 reasonable? Namely, why do we not choose questions with more than 60 tokens and rationales with more than 5 reasoning steps?
2. Some experimental details are missing.
The experimental details for Table 1 are not very clear and lack some details. For example, how the manual-CoT examples are built. What is the number of demonstration examples?
The experimental details for the Codex baseline are missing. I am curious about the instructions you used to prompt the Codex model.
3. It would be better to discuss more recent related work.
I understand some work has been released very recently. Since this work is closely related to the paper, it would be nice to include it in the revised paper. Recent work includes but is not limited to: [1] Calibrate Before Use: Improving Few-Shot Performance of Language Models, 2021 [2] Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning, 2022 [3] Complexity-Based Prompting for Multi-Step Reasoning, 2022 [4] Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering, 2022 [5] What Makes Pre-trained Language Models Better Zero/Few-shot Learners?, 2022
4. Is it possible to apply Auto-CoT to sample the examples with manually designed CoT?


Review Point: of different chain-of-thought prompting methods: including zero-shot-CoT, few-shot-CoT, manual-CoT, and Auto-CoT. The paper conducts case study experiments to look into the limitations of existing methods and proposes improvement directions. Finally, the paper proposes an improved method for Auto-CoT that could achieve a competitive advantage over Manual-CoT.
Review Point: 3. The paper is well-organized. The writing is good and most of the content is very clear to me. Weaknesses/Feedback 1. The writing could be improved. It would be helpful to draw a table to compare different CoT prompting methods across different dimensions. How and why shall we make an assumption that “questions of all the wrong demonstrations fall into the same frequent-error cluster”? Is the selection criteria in section 4.2 reasonable? Namely, why do we not choose questions with more than 60 tokens and rationales with more than 5 reasoning steps?
Review Point: 2. Some experimental details are missing. The experimental details for Table 1 are not very clear and lack some details. For example, how the manual-CoT examples are built. What is the number of demonstration examples? The experimental details for the Codex baseline are missing. I am curious about the instructions you used to prompt the Codex model.
Review Point: 3. It would be better to discuss more recent related work. I understand some work has been released very recently. Since this work is closely related to the paper, it would be nice to include it in the revised paper. Recent work includes but is not limited to: [1] Calibrate Before Use: Improving Few-Shot Performance of Language Models, 2021 [2] Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning, 2022 [3] Complexity-Based Prompting for Multi-Step Reasoning, 2022 [4] Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering, 2022 [5] What Makes Pre-trained Language Models Better Zero/Few-shot Learners?, 2022 4. Is it possible to apply Auto-CoT to sample the examples with manually designed CoT?
==================================================

Focused review:

Weaknesses:
The approach is connected to a large number of literatures, most of which were not reviewed. Given space, it would be hard to do so. However, current discussion of related work omits several literatures mentioned by the authors. Other possible connections include soft supervised approaches and probabilistic versions of teaching / cooperation.
The exposition is frustratingly vague at many points. Examples are supplied below. This is the largest weakness of the paper, which is that it is nowhere near self-encapsulated or even clear at many points. I would strongly implore the authors to adjust the exposition as the paper could be much more approachable for the broad audience who might find it interesting.
Specific questions and comments:
"Then the student keeps learn22 ing from this batch dataset for the target concept." What does this mean?
"[29] connects sequential teaching to optimal control and gains interesting insights, but it can not produce a practical teaching policy." This is not an adequate discussion of prior work. What constitutes interesting insights? What does "practical teaching policy" mean?
There are several related literatures that are not discussed. Soft supervised learning is one that comes to mind. A second that is more closely related is the probabilistic literature on teaching and cooperation, e.g. Sequential cooperative bayesian inference.
"Machine teaching is shown useful in reinforcement learning [53, 24, 45, 17], human-in the-loop learning [23, 6, 40], crowd sourcing [50, 67, 68] and cyber security [42, 2, 66, 64, 65]. [10, 14, 74, 47, 6, 26] study machine teaching from a more theoretical aspect." Please re-read this sentence.
"We first consider the cleanest teaching protocol following" What does this mean? Why is it the cleanest? One should provide an explanation for a strong statement.
"All these methods can be viewed as using a customized label synthesis policy in the black-box teaching scenario" It would be nice to have a more precise statement of this claim.
Given that the approach unifies knowledge distillation, label smoothing, and self-training, it would be good to review at least some work in those literatures too.
" This result validates our argument that it is not always optimal to feed the learner with ground truth label" I am not sure validates is the word you want here. Validation seems best provided by empirical demonstrations that it works on real data. "Is consistent with" may be a more suitable choice.
The move from predicted to ground truth labels is interesting. I would be interested to hear the author's thoughts on probabilistic interpretations.
" The predicted label is usually easy to learn for the current learner and the optimal label is more difficult to learn, implying that the learner should be taught in an easy-to-hard fashion." It would be helpful to explain why the predicted label is usually easy.
"This implication nicely matches the conclusion in curriculum learning [5] and IM" What conclusion?
" Moreover, greedy LAST also has the property of teaching monotonicity [33] if the learner loss satisfies certain conditions (Appendix C)." The conditions should be stated in the main text.
Figure 3 didn't help me much. It would require more explaining in the caption than simply "example".
" This nice property implies that LAST always converges to w∗ faster than a random teacher (i.e., SGD) if both of them sample teaching examples uniformly from the pool. An illustration of how LAST works and why LAST yields better convergence to w∗ is given in Fig. 3. If the learner loss is properly designed, LAST can converge faster than SGD" These passages are repetitive and do not effectively explain.
" The teaching monotonicity comes from the first iteration where the gradient update in LAST is guaranteed to better minimize the discrepancy between w1 and w∗." A more precise derivation/comparison would be helpful.
" we propose a simple yet effective heuristic to teach MLP learners." How do we know this is an effective heuristic? A more detailed exposition here would be nice.
" This shares similar spirits with back-propagation through time in recurrent networks and meta-learning" What does this mean? A more precise statement would be desirable.
" The greedy policy is in fact the optimal solution to Eq. (5) for T = 1. For larger T, unrolling builds a larger computational graph for back-propagation and is more costly". It is helpful for the reader to provide a clear statement of why optimality is obtained. Also, given that computational considerations are one of the arguments in favor of LAST, an analysis of computational complexity here would clarify the argument.
"Optimizing the unrolled teaching policy is conceptually similar to optimizing recurrent neural networks with back-propagation through time, as illustrated in Fig. 4. Unrolling T steps is equivalent to seeking a teaching policy that best minimizes the weight discrepancy after T-step gradient descent. Broadly speaking, our method also has intrinsic connections to learning an optimizer [4, 30] and teaching a loss function [61] in the sense that both aim to learn better gradients." Again, more precise statements would be helpful.
"We can simply define rt =−kwt −w∗ k as the reward and directly apply policy gradient [59] to solve it." Please explain more.
Theorem 1 contains several terms that are not defined.
"Theorem 1 shows that g(y) plays a critical role, similar in spirit to the way that in line search, the step size is adaptively adjusted." Please re-read the sentence.
"We connect Theorem 1 to Armijo linear search" A more precise term than connect would be desirable.
Figure 5 did not help me. The caption is inadequate.
"Based on [33], it is easy to verify that mixed teaching can achieve ET" Please explain.
"is able to achieve faster teaching empirically" please point to where this is demonstrated.
The discussion of the data in figure 11 is interesting! (but not convincing) I would be interested to hear more about this.


Review Point: The approach is connected to a large number of literatures, most of which were not reviewed. Given space, it would be hard to do so. However, current discussion of related work omits several literatures mentioned by the authors. Other possible connections include soft supervised approaches and probabilistic versions of teaching / cooperation. The exposition is frustratingly vague at many points. Examples are supplied below. This is the largest weakness of the paper, which is that it is nowhere near self-encapsulated or even clear at many points. I would strongly implore the authors to adjust the exposition as the paper could be much more approachable for the broad audience who might find it interesting. Specific questions and comments: "Then the student keeps learn22 ing from this batch dataset for the target concept." What does this mean? "[29] connects sequential teaching to optimal control and gains interesting insights, but it can not produce a practical teaching policy." This is not an adequate discussion of prior work. What constitutes interesting insights? What does "practical teaching policy" mean? There are several related literatures that are not discussed. Soft supervised learning is one that comes to mind. A second that is more closely related is the probabilistic literature on teaching and cooperation, e.g. Sequential cooperative bayesian inference. "Machine teaching is shown useful in reinforcement learning [53, 24, 45, 17], human-in the-loop learning [23, 6, 40], crowd sourcing [50, 67, 68] and cyber security [42, 2, 66, 64, 65]. [10, 14, 74, 47, 6, 26] study machine teaching from a more theoretical aspect." Please re-read this sentence. "We first consider the cleanest teaching protocol following" What does this mean? Why is it the cleanest? One should provide an explanation for a strong statement. "All these methods can be viewed as using a customized label synthesis policy in the black-box teaching scenario" It would be nice to have a more precise statement of this claim. Given that the approach unifies knowledge distillation, label smoothing, and self-training, it would be good to review at least some work in those literatures too. " This result validates our argument that it is not always optimal to feed the learner with ground truth label" I am not sure validates is the word you want here. Validation seems best provided by empirical demonstrations that it works on real data. "Is consistent with" may be a more suitable choice. The move from predicted to ground truth labels is interesting. I would be interested to hear the author's thoughts on probabilistic interpretations. " The predicted label is usually easy to learn for the current learner and the optimal label is more difficult to learn, implying that the learner should be taught in an easy-to-hard fashion." It would be helpful to explain why the predicted label is usually easy. "This implication nicely matches the conclusion in curriculum learning [5] and IM" What conclusion? " Moreover, greedy LAST also has the property of teaching monotonicity [33] if the learner loss satisfies certain conditions (Appendix C)." The conditions should be stated in the main text. Figure 3 didn't help me much. It would require more explaining in the caption than simply "example". " This nice property implies that LAST always converges to w∗ faster than a random teacher (i.e., SGD) if both of them sample teaching examples uniformly from the pool. An illustration of how LAST works and why LAST yields better convergence to w∗ is given in Fig.
Review Point: 3. If the learner loss is properly designed, LAST can converge faster than SGD" These passages are repetitive and do not effectively explain. " The teaching monotonicity comes from the first iteration where the gradient update in LAST is guaranteed to better minimize the discrepancy between w1 and w∗." A more precise derivation/comparison would be helpful. " we propose a simple yet effective heuristic to teach MLP learners." How do we know this is an effective heuristic? A more detailed exposition here would be nice. " This shares similar spirits with back-propagation through time in recurrent networks and meta-learning" What does this mean? A more precise statement would be desirable. " The greedy policy is in fact the optimal solution to Eq. (5) for T = 1. For larger T, unrolling builds a larger computational graph for back-propagation and is more costly". It is helpful for the reader to provide a clear statement of why optimality is obtained. Also, given that computational considerations are one of the arguments in favor of LAST, an analysis of computational complexity here would clarify the argument. "Optimizing the unrolled teaching policy is conceptually similar to optimizing recurrent neural networks with back-propagation through time, as illustrated in Fig.
Review Point: 4. Unrolling T steps is equivalent to seeking a teaching policy that best minimizes the weight discrepancy after T-step gradient descent. Broadly speaking, our method also has intrinsic connections to learning an optimizer [4, 30] and teaching a loss function [61] in the sense that both aim to learn better gradients." Again, more precise statements would be helpful. "We can simply define rt =−kwt −w∗ k as the reward and directly apply policy gradient [59] to solve it." Please explain more. Theorem 1 contains several terms that are not defined. "Theorem 1 shows that g(y) plays a critical role, similar in spirit to the way that in line search, the step size is adaptively adjusted." Please re-read the sentence. "We connect Theorem 1 to Armijo linear search" A more precise term than connect would be desirable. Figure 5 did not help me. The caption is inadequate. "Based on [33], it is easy to verify that mixed teaching can achieve ET" Please explain. "is able to achieve faster teaching empirically" please point to where this is demonstrated. The discussion of the data in figure 11 is interesting! (but not convincing) I would be interested to hear more about this.
==================================================

Focused review:

Weaknesses: - The results require spectral initialization, although the initial error bound in the present work is less stringent in previous works. Experimentally, was the spectral initialization scheme necessary for subGD with the proposed step sizes to converge? The case
k
=
d
used random initialization with small norm, but the over-specified and exact rank case all used spectral initialization. - The overall sample complexity of the convergence results appears off by a large factor of
k
2
. Could the authors comment on where this bottleneck arises and where the difficulty lies? This may be helpful for the reader to know where potential improvements could be made. It appears this factor is due to the sample complexity required for the RDPP, as it scales like
δ
−
4
, and the theory requires
δ
⩽
1
/
k
.
Clarity
I found the paper easy to follow, and gained a good amount of intuition from the proof sketches and discussion of results.
Typos: pg 1 line 13 “adversary sparse” -> “adversarial sparse”, pg 3 line 112 “facterized” -> “factorized”, pg 3 line 117 “dynamic” -> “dynamics”, pg 5 line 164 is missing a period
Other comments:
I believe that the proof of Proposition 2.2 is in Appendix C instead of D.
Eq (2.1) should have
y
i
instead of
s
i
.
The results shown for DIP are certainly interesting, but the proposed method wasn’t used in this setting (namely the adaptive step sizes). Thus the sentence on pg 2 line 57-58 may be misleading (instead one could simply note robustness to overfitting for DIP is shown with decaying step sizes). Along these lines, is there an analogous adaptive step-size rule that could be used for denoising with sparse corruptions for the DIP (say, simply taking the median of the measurements)?
The authors commented on how analyzing other measurement matrices would be an interesting avenue for future work. Along similar lines, could the RDPP (or a variant thereof) potentially have applications to other inverse problems as well?
Significance
The present work adds an interesting contribution to providing rigorous guarantees for the over-specified matrix sensing setting. This setting is important as, in practice, one would rarely know the exact rank of the ground-truth matrix.
As the work is theoretical in nature, there are no foreseeable societal impacts. Some limitations on the theory are discussed near the end, and one I would add here is the sub-optimal sample complexity scaling.


Review Point: - The results require spectral initialization, although the initial error bound in the present work is less stringent in previous works. Experimentally, was the spectral initialization scheme necessary for subGD with the proposed step sizes to converge? The case k = d used random initialization with small norm, but the over-specified and exact rank case all used spectral initialization.
Review Point: - The overall sample complexity of the convergence results appears off by a large factor of k 2 . Could the authors comment on where this bottleneck arises and where the difficulty lies? This may be helpful for the reader to know where potential improvements could be made. It appears this factor is due to the sample complexity required for the RDPP, as it scales like δ − 4 , and the theory requires δ ⩽ 1 / k . Clarity I found the paper easy to follow, and gained a good amount of intuition from the proof sketches and discussion of results. Typos: pg 1 line 13 “adversary sparse” -> “adversarial sparse”, pg 3 line 112 “facterized” -> “factorized”, pg 3 line 117 “dynamic” -> “dynamics”, pg 5 line 164 is missing a period Other comments: I believe that the proof of Proposition 2.2 is in Appendix C instead of D. Eq (2.1) should have y i instead of s i . The results shown for DIP are certainly interesting, but the proposed method wasn’t used in this setting (namely the adaptive step sizes). Thus the sentence on pg 2 line 57-58 may be misleading (instead one could simply note robustness to overfitting for DIP is shown with decaying step sizes). Along these lines, is there an analogous adaptive step-size rule that could be used for denoising with sparse corruptions for the DIP (say, simply taking the median of the measurements)? The authors commented on how analyzing other measurement matrices would be an interesting avenue for future work. Along similar lines, could the RDPP (or a variant thereof) potentially have applications to other inverse problems as well? Significance The present work adds an interesting contribution to providing rigorous guarantees for the over-specified matrix sensing setting. This setting is important as, in practice, one would rarely know the exact rank of the ground-truth matrix. As the work is theoretical in nature, there are no foreseeable societal impacts. Some limitations on the theory are discussed near the end, and one I would add here is the sub-optimal sample complexity scaling.
==================================================

Focused review:

Weakness: 1. The authors rarely discuss the insight of the theorems and the construction in proofs. 2. It seems that the theory in this paper can hardly help us foresee whether we can train a model that is robust to adversarial perturbations.


Review Point: 1. The authors rarely discuss the insight of the theorems and the construction in proofs.
Review Point: 2. It seems that the theory in this paper can hardly help us foresee whether we can train a model that is robust to adversarial perturbations.
==================================================

Focused review:

Weaknesses: I am not sure if the proposed algorithm for decoys generation is effective, which as a consequence puts the paper on questions.
For each target caption, the algorithm basically picks out those with similar representation and surface form but do not belong to the same image. But a fundamentally issue with this approach is: not belonging to the image-A does not mean not appropriate to describe image-A, especially when the representation and surface form are close. So the ground-truth labels might not be valid. As we can see in Figure-1, the generated decoys are either too far from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair substitutes for the target (*small boy playing kites* vs *boy flies a kite*).
Thus, I am afraid that the dataset generated with this algorithm can not train a model to really *go beyond key word recognition*, which was claimed as contribution in this paper. As shown in Figure-1, most decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word match*, they look very tempting to be a correct option.
Furthermore, it is interesting that humans only do correctly on 82.8% on a sampled test set. Does it mean that those examples are really too hard even for human to correctly classify? Or are some of the *decoys* in fact good enough to be the target's substitute (or even better) so that human choose them over ground-truth targets?
- General Discussion: I think this is a well-written paper with clear motivation and substantial experiments. 
The major issue is that the data-generating algorithm and the generated dataset do not seem helpful for the motivation. This in turn makes the experimental conclusions less convincing. So I tend to reject this paper unless my concerns can be fully addressed in rebuttal. 

Review Point: I am not sure if the proposed algorithm for decoys generation is effective, which as a consequence puts the paper on questions. For each target caption, the algorithm basically picks out those with similar representation and surface form but do not belong to the same image. But a fundamentally issue with this approach is: not belonging to the image-A does not mean not appropriate to describe image-A, especially when the representation and surface form are close. So the ground-truth labels might not be valid. As we can see in Figure-1, the generated decoys are either too far from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair substitutes for the target (*small boy playing kites* vs *boy flies a kite*). Thus, I am afraid that the dataset generated with this algorithm can not train a model to really *go beyond key word recognition*, which was claimed as contribution in this paper. As shown in Figure-1, most decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word match*, they look very tempting to be a correct option. Furthermore, it is interesting that humans only do correctly on 82.8% on a sampled test set. Does it mean that those examples are really too hard even for human to correctly classify? Or are some of the *decoys* in fact good enough to be the target's substitute (or even better) so that human choose them over ground-truth targets?
Review Point: - General Discussion: I think this is a well-written paper with clear motivation and substantial experiments. The major issue is that the data-generating algorithm and the generated dataset do not seem helpful for the motivation. This in turn makes the experimental conclusions less convincing. So I tend to reject this paper unless my concerns can be fully addressed in rebuttal.
==================================================

Focused review:

Weakness]
W1: Although the paper aims to improve intersectional fairness, the proposed algorithm seems not specialized to solve the intersectional fairness issues, including the limited access to protected attributes and the scalability issues. It is unclear how the proposed algorithm is especially effective to those issues.
W2: The paper uses a proxy for measuring the mutual information between the protected and label attributes, but the the justification on the proxy is not enough.
One related work [1], which is cited in the paper, theoretically shows how the adversarial training framework can capture the mutual information between the protected attributes and the predicted labels. Several other works also provide theoretical discussions on fair training with mutual information [2].
Compared to these works, this work’s proxy is not based on enough theoretical evidence. It would be much better if the paper can give clear discussion on how exact the proxy measure the mutual information.
[1] Cho et al., A fair classifier using mutual information, ISIT 2020 [2] Zhang et al., Mitigating unwanted biases with adversarial learning, AIES 2018
W3: Empirical results are not sufficient to demonstrate the advantages of the proposed algorithm.
For example, it seems the algorithm may not achieve high enough fairness in several scenarios. In Figure 1, the first and third graphs in the second row show that the proposed algorithm (blue) does not have high fairness points compared to other two methods. Also, the paper observes that the accuracy-fairness tradeoffs become similar with other methods in the high fairness regimes. However, we usually aim to achieve high fairness (say less than 0.1), then the proposed algorithm has no benefit in this practical scenario.
Moreover, in Figure 2, the proposed algorithm shows comparable fairness results (especially in DP cases) with upsampled baseline, which is very simple. Thus, it is quite questionable why we use the proposed method.
Relatively minor) Most experimental results are reported without the error ranges.
W4: There is no detailed discussion on when the algorithm performs when equalized odds (EO) or demographic parity (DP). The paper argues that one benefit of their method is not targeting a specific fairness metric. However, simply debiasing the data may not enough high fairness in terms of EO and DP, thus it is unclear why only reducing the mutual information between the protected attribute and the true label (not predicted label) can be an attractive advantage in terms of group fairness. It would be better if the paper can clarify their method’s characteristics based on the previous insights in the literature, e.g., the impossibility theorem between EO and DP [1].
[1] Barocas et al., Fairness and machine learning, 2019


Review Point: W1: Although the paper aims to improve intersectional fairness, the proposed algorithm seems not specialized to solve the intersectional fairness issues, including the limited access to protected attributes and the scalability issues. It is unclear how the proposed algorithm is especially effective to those issues.
Review Point: W2: The paper uses a proxy for measuring the mutual information between the protected and label attributes, but the the justification on the proxy is not enough. One related work [1], which is cited in the paper, theoretically shows how the adversarial training framework can capture the mutual information between the protected attributes and the predicted labels. Several other works also provide theoretical discussions on fair training with mutual information [2]. Compared to these works, this work’s proxy is not based on enough theoretical evidence. It would be much better if the paper can give clear discussion on how exact the proxy measure the mutual information. [1] Cho et al., A fair classifier using mutual information, ISIT 2020 [2] Zhang et al., Mitigating unwanted biases with adversarial learning, AIES 2018 W3: Empirical results are not sufficient to demonstrate the advantages of the proposed algorithm. For example, it seems the algorithm may not achieve high enough fairness in several scenarios. In Figure 1, the first and third graphs in the second row show that the proposed algorithm (blue) does not have high fairness points compared to other two methods. Also, the paper observes that the accuracy-fairness tradeoffs become similar with other methods in the high fairness regimes. However, we usually aim to achieve high fairness (say less than 0.1), then the proposed algorithm has no benefit in this practical scenario. Moreover, in Figure 2, the proposed algorithm shows comparable fairness results (especially in DP cases) with upsampled baseline, which is very simple. Thus, it is quite questionable why we use the proposed method. Relatively minor) Most experimental results are reported without the error ranges. W4: There is no detailed discussion on when the algorithm performs when equalized odds (EO) or demographic parity (DP). The paper argues that one benefit of their method is not targeting a specific fairness metric. However, simply debiasing the data may not enough high fairness in terms of EO and DP, thus it is unclear why only reducing the mutual information between the protected attribute and the true label (not predicted label) can be an attractive advantage in terms of group fairness. It would be better if the paper can clarify their method’s characteristics based on the previous insights in the literature, e.g., the impossibility theorem between EO and DP [1]. [1] Barocas et al., Fairness and machine learning, 2019
==================================================

Focused review:

Weakness and concerns
It is not clear whether the design of the proposed method really leads to the improvement of the performance or not. Since the authors use SimCLR to pretrain the model, the pretrained model has already had a substantially good feature representation. Consequently, if we know the number of classes in the training data, unsupervised learning methods like [R1] (or simple clustering methods) might work well to discriminate all classes. Although the authors adopt a new idea to the supervised loss, it seems unclear whether the supervised loss itself really contributes to good performance. Are there any experimental result on performance sensitivity to \eta?
[R1] "Learning Discrete Representations via Information Maximizing Self-Augmented Training," ICML 2017.
The experimental setup is not convincing. Due to the motivation of SSL, the number of unlabeled data should be much larger than that of labeled data, and it is often tens or hundreds times larger in the literature. However, in this paper, it is basically 3~7 times larger in the experiments. Is there any reason why the authors did not much reduce the number of labeled data?
Two concerns on uncertainty based adaptive margin.
How did the authors estimate the class posterior probability in Eq. (4)? Is it just the output of the softmax function?
Since this adaptive margin is adopted to improve the accuracy of pseudo-labels, how much it is improved should be reported in Fig. 3.
How did the authors conduct validation? Due to the existence of unseen classes in unlabeled data, how to conduct validation is not trivial.
Is it reasonable to call L_BCE unsupervised loss? Since Z_l' is obtained using ground-truth labels, L_BCE cannot be computed in an unsupervised manner.
Minor concerns
I could not get the reason why the proposed method is called ORCA.
--After receiving authors' response--
I would like to thank the authors for providing additional experimental results and giving clarifications. Since my concerns are almost solved, and I updated my score from 5 to 6. This study would be a great first step to tackle the challenging problem, open-world semi-supervised learning, though there are several remaining issues (e.g., validation is hard to conduct, the number of unknown classes should be known, etc.). I vote for "weak accept."


Review Point: and concerns It is not clear whether the design of the proposed method really leads to the improvement of the performance or not. Since the authors use SimCLR to pretrain the model, the pretrained model has already had a substantially good feature representation. Consequently, if we know the number of classes in the training data, unsupervised learning methods like [R1] (or simple clustering methods) might work well to discriminate all classes. Although the authors adopt a new idea to the supervised loss, it seems unclear whether the supervised loss itself really contributes to good performance. Are there any experimental result on performance sensitivity to \eta? [R1] "Learning Discrete Representations via Information Maximizing Self-Augmented Training," ICML 2017. The experimental setup is not convincing. Due to the motivation of SSL, the number of unlabeled data should be much larger than that of labeled data, and it is often tens or hundreds times larger in the literature. However, in this paper, it is basically 3~7 times larger in the experiments. Is there any reason why the authors did not much reduce the number of labeled data? Two concerns on uncertainty based adaptive margin. How did the authors estimate the class posterior probability in Eq. (4)? Is it just the output of the softmax function? Since this adaptive margin is adopted to improve the accuracy of pseudo-labels, how much it is improved should be reported in Fig.
Review Point: 3. How did the authors conduct validation? Due to the existence of unseen classes in unlabeled data, how to conduct validation is not trivial. Is it reasonable to call L_BCE unsupervised loss? Since Z_l' is obtained using ground-truth labels, L_BCE cannot be computed in an unsupervised manner. Minor concerns I could not get the reason why the proposed method is called ORCA. --After receiving authors' response-- I would like to thank the authors for providing additional experimental results and giving clarifications. Since my concerns are almost solved, and I updated my score from 5 to 6. This study would be a great first step to tackle the challenging problem, open-world semi-supervised learning, though there are several remaining issues (e.g., validation is hard to conduct, the number of unknown classes should be known, etc.). I vote for "weak accept."
==================================================

Focused review:

Weakness: 1. The experiments are all single-dimensional. I'm not sure whether the proposed method works for high-dimensional x. If it works, it is better to include some high-dimensional experiments. If it doesn't work, why? 2. A new training point is added to the training set only if the variance of prediction is smaller than some pre-defined threshold (Line 14 in Algorithm 1). I wonder is there any case that no data points satisfy the criteria? If so, is there any possibility that the algorithm will stuck when sigma are too small? If not, is there any theoretical guarantee? Thanks!


Review Point: 1. The experiments are all single-dimensional. I'm not sure whether the proposed method works for high-dimensional x. If it works, it is better to include some high-dimensional experiments. If it doesn't work, why?
Review Point: 2. A new training point is added to the training set only if the variance of prediction is smaller than some pre-defined threshold (Line 14 in Algorithm 1). I wonder is there any case that no data points satisfy the criteria? If so, is there any possibility that the algorithm will stuck when sigma are too small? If not, is there any theoretical guarantee? Thanks!
==================================================

Focused review:

Weakness:
1.I think the work is lack of novelty as the work GPN[1] has already proposed to add node importance score in the calculation of class prototype and the paper only give a theoretical analysis on it.
2.The experiment part is not sufficient enough. (1) For few-shot graph node classification problem to predict nodes with novel labels, there are some methods that the paper does not compare with. For example, G-Meta is mentioned in the related works but not compared in the experiments. A recent work TENT[2] is not mentioned in related works. As far as I know, the above two approaches can be applied in the problem setting in the paper. (2) For the approach proposed in the paper, there is no detailed ablation study for the functionalities of each part designed. (3) It is better to add a case study part to show the strength of the proposed method by an example.
Concerns:
1.The paper consider the node importance among nodes with same label in support set. In 1-shot scenario, how node importance can be used? I also find that the experiment part in the paper does not include the 1-shot paper setting, but related works such as RALE have 1-shot setting, why?
2.The paper says that the theory of node importance can be applied to other domains. I think there should be an example to verify that conclusion.
3.In section 5.3, ‘we get access to abundant nodes belonging to each class’. I do not think this is always true as there might be a class in the training set that only has few samples given the long-tailed distribution of samples in most graph datasets.
[1] Ding et al. Graph Prototypical Networks for Few-shot Learning on Attributed Networks
[2] Wang et al. Task-Adaptive Few-shot Node Classification


Review Point: 1.I think the work is lack of novelty as the work GPN[1] has already proposed to add node importance score in the calculation of class prototype and the paper only give a theoretical analysis on it.
Review Point: 2.The experiment part is not sufficient enough. (1) For few-shot graph node classification problem to predict nodes with novel labels, there are some methods that the paper does not compare with. For example, G-Meta is mentioned in the related works but not compared in the experiments. A recent work TENT[2] is not mentioned in related works. As far as I know, the above two approaches can be applied in the problem setting in the paper. (2) For the approach proposed in the paper, there is no detailed ablation study for the functionalities of each part designed. (3) It is better to add a case study part to show the strength of the proposed method by an example. Concerns:
Review Point: 1.The paper consider the node importance among nodes with same label in support set. In 1-shot scenario, how node importance can be used? I also find that the experiment part in the paper does not include the 1-shot paper setting, but related works such as RALE have 1-shot setting, why?
Review Point: 2.The paper says that the theory of node importance can be applied to other domains. I think there should be an example to verify that conclusion.
Review Point: 3.In section 5.3, ‘we get access to abundant nodes belonging to each class’. I do not think this is always true as there might be a class in the training set that only has few samples given the long-tailed distribution of samples in most graph datasets. [1] Ding et al. Graph Prototypical Networks for Few-shot Learning on Attributed Networks [2] Wang et al. Task-Adaptive Few-shot Node Classification
==================================================

Focused review:

Weaknesses:
1.I feel confused on the novelty of the proposed method. From my viewpoint, this paper presents a policy learning method with distributional shift. However, offline policy learning methods with distributional shift is already a popular topic. Moreover, it is not rare to use some DA tricks to improve the transferability of the learned policy across different environments.
2.Related work should distinguish this paper from previous policy learning methods. Their motivations are settings are very similar.
They have adequately addressed the limitations and potential negative societal impact of their work.


Review Point: 1.I feel confused on the novelty of the proposed method. From my viewpoint, this paper presents a policy learning method with distributional shift. However, offline policy learning methods with distributional shift is already a popular topic. Moreover, it is not rare to use some DA tricks to improve the transferability of the learned policy across different environments.
Review Point: 2.Related work should distinguish this paper from previous policy learning methods. Their motivations are settings are very similar. They have adequately addressed the limitations and potential negative societal impact of their work.
==================================================

Focused review:

1) The task of enhancing the target coverage in Directional Sensor Networks (DSNs) is important and challenging. However, as far as I am concerned, it is not a standard benchmark environment for studying multi-agent reinforcement learning. The proposed method/model design targets at a specific problem, limiting its significance. There already exist some popular environments for multi-agent cooperation. If experiments are conducted on these standard benchmarks, the significance of this work for the machine learning (ML) or reinforcement learning (RL) community can be improved. 2) The design choice is reasonable, however, the technical novelty seems a little bit low. Attention-based encoders have been widely used in literature to solve the problem with the variable length. And modeling the marginal contribution seems straightforward. 3) As shown in Table.1, the average gain of the proposed method is not as good as COMA or ILP. Are there any comments?

Review Point: 1) The task of enhancing the target coverage in Directional Sensor Networks (DSNs) is important and challenging. However, as far as I am concerned, it is not a standard benchmark environment for studying multi-agent reinforcement learning. The proposed method/model design targets at a specific problem, limiting its significance. There already exist some popular environments for multi-agent cooperation. If experiments are conducted on these standard benchmarks, the significance of this work for the machine learning (ML) or reinforcement learning (RL) community can be improved.
Review Point: 2) The design choice is reasonable, however, the technical novelty seems a little bit low. Attention-based encoders have been widely used in literature to solve the problem with the variable length. And modeling the marginal contribution seems straightforward.
Review Point: 3) As shown in Table.1, the average gain of the proposed method is not as good as COMA or ILP. Are there any comments?
==================================================

Focused review:

1. It is not immediately clear to me what is the main technical challenge to extend the dual-free analysis in [33] and [16] into the decentralized setting. The authors may need to explain this clearly. 2. When is it the case that the evaluating the gradient of conjugate function is expensive or infeasible? It is better to include more examples in machine learning to motivate this dual-free approach.

Review Point: 1. It is not immediately clear to me what is the main technical challenge to extend the dual-free analysis in [33] and [16] into the decentralized setting. The authors may need to explain this clearly.
Review Point: 2. When is it the case that the evaluating the gradient of conjugate function is expensive or infeasible? It is better to include more examples in machine learning to motivate this dual-free approach.
==================================================

Focused review:

- This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.  - Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.  - The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. 
Putting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work.  Some minor suggestions & questions:  - Line 516: "We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth." Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.  - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:    - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020. 
  - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020. 
  - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020. 

Review Point: - This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.
Review Point: - Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.
Review Point: - The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. Putting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work. Some minor suggestions & questions:
Review Point: - Line 516: "We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth." Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.
Review Point: - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:
Review Point: - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020.
Review Point: - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020.
Review Point: - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020.
==================================================

Focused review:

Weaknesses section below, I do think a better comparison of the results, recommendations and methods in this work to previous work would be beneficial to clarify the novelty of the contribution.
Clarity: The paper is very well-written, and explains it's points clearly in all parts. The visualisations are well explained and provide good intuition for the variety of statistical ideas presented in the text of the paper, in a way that is rigorous. The paper's structure is well-organised the motivates the main messages of the paper well.
Quality: The technical quality of the experiments and visualisations is very high, and the techniques used and choices made are well-motivated. The claims made are well supported by the experiments.
Significance: The results presented in the paper of very high significance to the reinforcement learning community. The demonstration of spurious or misleading reporting of previous results is very important and should inform the field as to which methods are actually improvements on previous methods, and in what way. The improved approaches for reporting results, including representing the full distribution of results in multiple ways, should be taken up by current researchers in RL to improve the robustness of their results and determining whether real progress is being made.
Weaknesses
It would be useful to have a (small) related work section (possibly in the appendix, given the space constraints) on previous work and what it demonstrated, to clarify the contributions in this paper. For example, the papers listed in L318 could be explained in more detail.
Suggestions for improvements
L186: I think it would be more fair to say that the increase in the number of runs which would be required to address the statistical uncertainty issues is infeasible for computationally demanding deep RL benchmarks.
L233: I think the "(1)" should go after "... is also" for this list to read correctly.
From my understanding of dopamine, using the Rainbow implementation from dopamine isn't the full Rainbow algorithm (it misses out on three components - dueling DQN, double DQN and NoisyNets) - this should be made clear in the paper, as it changes the results on the Atari@200M benchmark. For example, see the Rainbow results in table 2 of Muesli: Combining Improvements in Policy Optimization, which has a much higher median than reported in your paper (and in the DreamerV2 paper, which also uses the dopamine implementation of Rainbow).
Edit
Upon reading the response and other reviews, I have raised my confidence from 3 to 4, maintaining my score of 9.
I believe limitations and societal impacts are adaquately addressed in the paper.


Review Point: section below, I do think a better comparison of the results, recommendations and methods in this work to previous work would be beneficial to clarify the novelty of the contribution. Clarity: The paper is very well-written, and explains it's points clearly in all parts. The visualisations are well explained and provide good intuition for the variety of statistical ideas presented in the text of the paper, in a way that is rigorous. The paper's structure is well-organised the motivates the main messages of the paper well. Quality: The technical quality of the experiments and visualisations is very high, and the techniques used and choices made are well-motivated. The claims made are well supported by the experiments. Significance: The results presented in the paper of very high significance to the reinforcement learning community. The demonstration of spurious or misleading reporting of previous results is very important and should inform the field as to which methods are actually improvements on previous methods, and in what way. The improved approaches for reporting results, including representing the full distribution of results in multiple ways, should be taken up by current researchers in RL to improve the robustness of their results and determining whether real progress is being made. Weaknesses It would be useful to have a (small) related work section (possibly in the appendix, given the space constraints) on previous work and what it demonstrated, to clarify the contributions in this paper. For example, the papers listed in L318 could be explained in more detail. Suggestions for improvements L186: I think it would be more fair to say that the increase in the number of runs which would be required to address the statistical uncertainty issues is infeasible for computationally demanding deep RL benchmarks.
Review Point: L233: I think the "(1)" should go after "... is also" for this list to read correctly. From my understanding of dopamine, using the Rainbow implementation from dopamine isn't the full Rainbow algorithm (it misses out on three components - dueling DQN, double DQN and NoisyNets) - this should be made clear in the paper, as it changes the results on the Atari@200M benchmark. For example, see the Rainbow results in table 2 of Muesli: Combining Improvements in Policy Optimization, which has a much higher median than reported in your paper (and in the DreamerV2 paper, which also uses the dopamine implementation of Rainbow). Edit Upon reading the response and other reviews, I have raised my confidence from 3 to 4, maintaining my score of 9. I believe limitations and societal impacts are adaquately addressed in the paper.
==================================================

Focused review:

1. Natural images especially non-texture type of images may not have intrinsic low-rank structures would limit the application of the proposed method on image denoising. 2. Since only the L1-regularization is imposed on the noise component, the proposed method can handle the salt-and-pepper noise well but the denoising performance is unknown for other types of noise. 3. The paper still needs more practical guidance on the learning rate selection case-by-case and a brief discussion of the impact of sampling rate on the performance.

Review Point: 1. Natural images especially non-texture type of images may not have intrinsic low-rank structures would limit the application of the proposed method on image denoising.
Review Point: 2. Since only the L1-regularization is imposed on the noise component, the proposed method can handle the salt-and-pepper noise well but the denoising performance is unknown for other types of noise.
Review Point: 3. The paper still needs more practical guidance on the learning rate selection case-by-case and a brief discussion of the impact of sampling rate on the performance.
==================================================

Focused review:

Weaknesses ： 1. The disadvantage is that although this paper finds the impact of the training objective on the performance of rain removal generalization, it doesn’t propose a better way to achieve the balance between restoring the image background and removing rain streaks. 2. It doesn’t show a significant improvement in generalization performance compared to the existing rain removal methods. 3. I think this paper needs to do more experiments to prove the effectiveness of this finding.


Review Point: ： 1. The disadvantage is that although this paper finds the impact of the training objective on the performance of rain removal generalization, it doesn’t propose a better way to achieve the balance between restoring the image background and removing rain streaks.
Review Point: 2. It doesn’t show a significant improvement in generalization performance compared to the existing rain removal methods.
Review Point: 3. I think this paper needs to do more experiments to prove the effectiveness of this finding.
==================================================

Focused review:

+ Counterfactual Transformation. The generation of counterfactual negative results is reasonable; however, the generation of counterfactual positive results from the inessential proposal set is confusing. How can authors guarantee the positive results have higher alignment scores with the original results than the negative results by using the proposed counterfactual transformation. + The distribution of original, counterfactual negative and counterfactual positive results. In fact, we don’t know what counterfactual positive/negative results are generated in the process of counterfactual transformation. Could you provide some visualization or analysis about the distribution of original, counterfactual negative and positive results?

Review Point: + Counterfactual Transformation. The generation of counterfactual negative results is reasonable; however, the generation of counterfactual positive results from the inessential proposal set is confusing. How can authors guarantee the positive results have higher alignment scores with the original results than the negative results by using the proposed counterfactual transformation.
Review Point: + The distribution of original, counterfactual negative and counterfactual positive results. In fact, we don’t know what counterfactual positive/negative results are generated in the process of counterfactual transformation. Could you provide some visualization or analysis about the distribution of original, counterfactual negative and positive results?
==================================================

Focused review:

- Although the papers’ focus is on the theory, the experimental evaluation could be stronger by comparing to established, as well as state-of-the-art baseline methods on outlier filtering, e.g. RANSAC, M-estimation methods or specialized global solvers like branch-and-bound-based consensus maximization. - The paper is very densely written and difficult to follow.

Review Point: - Although the papers’ focus is on the theory, the experimental evaluation could be stronger by comparing to established, as well as state-of-the-art baseline methods on outlier filtering, e.g. RANSAC, M-estimation methods or specialized global solvers like branch-and-bound-based consensus maximization.
Review Point: - The paper is very densely written and difficult to follow.
==================================================

Focused review:

1. It would be useful to have an in-depth explanation of when, (if at all), this kind of regularization does not represent a useful inductive bias. For example, if one modality is a duplicate signal of a subset of another, is the model forced to make use of both redundant signals? If one modality signal is purely random, to what extent is the model performance harmed? 2. As the authors note, a number of regularizers have been developed (such as the l2-norm regularizer on the weights) which may be less appropriate for multi-modal data. It may make the work more convincing if the experiments included a wide array of popular regularizers (e.g. different kinds of weight norms etc.) to validate their claim that directly searching for the "simplest" model is not the best course of action for multimodal inputs.

Review Point: 1. It would be useful to have an in-depth explanation of when, (if at all), this kind of regularization does not represent a useful inductive bias. For example, if one modality is a duplicate signal of a subset of another, is the model forced to make use of both redundant signals? If one modality signal is purely random, to what extent is the model performance harmed?
Review Point: 2. As the authors note, a number of regularizers have been developed (such as the l2-norm regularizer on the weights) which may be less appropriate for multi-modal data. It may make the work more convincing if the experiments included a wide array of popular regularizers (e.g. different kinds of weight norms etc.) to validate their claim that directly searching for the "simplest" model is not the best course of action for multimodal inputs.
==================================================

Focused review:

- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.
- For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.
[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 
Please consider adding new human evaluation results and baselines as mentioned in weaknesses. 

Review Point: - Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.
Review Point: - For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines. [1] CTRL: A conditional transformer language model for controllable generation. [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 Please consider adding new human evaluation results and baselines as mentioned in weaknesses.
==================================================

Focused review:

1. Not clear how this method can be applied outside of fully cooperative settings, as the authors claim. The authors should justify this claim theoretically or empirically, or else remove it. 2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019). 3. The analysis is somewhat "circumstantial", need more detailed experiments to be a convincing argument in this section. For example the claim in lines 235 - 236 seems to require further evidence to be completely convincing. 4. The link with self-play could be more clearly drawn out. As far as I can tell, the advantage of this over self-play is precisely the different initialization of the separate agents. It is surprising and important that this has such a significant effect, and could potentially spur a meta-learning investigation into optimal initialization for SEAC in future work.

Review Point: 1. Not clear how this method can be applied outside of fully cooperative settings, as the authors claim. The authors should justify this claim theoretically or empirically, or else remove it.
Review Point: 2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019).
Review Point: 3. The analysis is somewhat "circumstantial", need more detailed experiments to be a convincing argument in this section. For example the claim in lines 235 - 236 seems to require further evidence to be completely convincing.
Review Point: 4. The link with self-play could be more clearly drawn out. As far as I can tell, the advantage of this over self-play is precisely the different initialization of the separate agents. It is surprising and important that this has such a significant effect, and could potentially spur a meta-learning investigation into optimal initialization for SEAC in future work.
==================================================

Focused review:

Weakness:
The statement of introduction, related works, and problem statements should narrow down the ordinal regression to the vision-language or CV ordinal regression task cuz there are some pure language ordinal regression tasks.
2.The two loss (image-to-text loss and a text-to-image loss ) should be introduced in detailed, and the reason for using KL.
we choose to maintain the order of rank embeddings to preserve the order of the language prototypes. This statement is unclear. How to maintain the order of the languag?


Review Point: The statement of introduction, related works, and problem statements should narrow down the ordinal regression to the vision-language or CV ordinal regression task cuz there are some pure language ordinal regression tasks.
Review Point: 2.The two loss (image-to-text loss and a text-to-image loss ) should be introduced in detailed, and the reason for using KL. we choose to maintain the order of rank embeddings to preserve the order of the language prototypes. This statement is unclear. How to maintain the order of the languag?
==================================================

Focused review:

Weakness:
The necessity and complexity of using PLMs are not clear, as 1) the total amount of data generated is small, in the range of hundreds. 2) significant human review is still needed up and down the pipeline.
The size of DR SPIDER (The Reviewer's impression is 1k) is much smaller than SPIDER (10K questions).
Detailed suggestions:
Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics?
Section 3.2 paragraph 2: better explain what is "naturally occuring tables and columns".
Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting?
Section 4. Upper-case of common model names, "Bert-large" -> "BERT-large" etc.


Review Point: The necessity and complexity of using PLMs are not clear, as 1) the total amount of data generated is small, in the range of hundreds.
Review Point: 2) significant human review is still needed up and down the pipeline. The size of DR SPIDER (The Reviewer's impression is 1k) is much smaller than SPIDER (10K questions). Detailed suggestions: Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics? Section 3.2 paragraph 2: better explain what is "naturally occuring tables and columns". Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting? Section 4. Upper-case of common model names, "Bert-large" -> "BERT-large" etc.
==================================================

Focused review:

Weaknesses
[W1] Very unclear about how the ID and OOD teachers are trained/modeled (aka Section 3.1 + supplementary): I am assuming the paper simply followed the implementation of [23]. If that is the case, then there are no problems and I am familiar with [23]. However, it is super unclear what is happening with regards to ID and OOD teachers, even after reading the supplemental. E.g., the notations used for TIE and NIE are not consistent with the paper -- e.g., compare paper's presentation with Figure 3 from [23] which shows that counterfactual VQA used only Q to predict the answer, whereas the current paper shows links from inputs X which contains both V and Q to answer Y, which is not correct. Please clarify the exact formulation of the ID and OOD teachers in the revised version.
[W2] Some concerns about OOD/ID testing setup :
W2.1 Use of soft/hard weighting for strong vs weak teachers is not very well motivated. Why is Rubi (or by extension, NIE methods) a weak teacher? Why does using hard weighing help "weak models"?
W2.2 Are ID and OOD performance measured for the same model/data choices? As mentioned in [30], retraining before reporting ID performance can inflate the results on ID evaluation and effectively create two different versions of the model. Upon reading the paper, I don't think that this has been addressed.
Overall: Overall, I think this is a good paper and I currently recommend a weak acceptance. However, it could be slightly higher if the clarity issues were resolved.


Review Point: [W1] Very unclear about how the ID and OOD teachers are trained/modeled (aka Section 3.1 + supplementary): I am assuming the paper simply followed the implementation of [23]. If that is the case, then there are no problems and I am familiar with [23]. However, it is super unclear what is happening with regards to ID and OOD teachers, even after reading the supplemental. E.g., the notations used for TIE and NIE are not consistent with the paper -- e.g., compare paper's presentation with Figure 3 from [23] which shows that counterfactual VQA used only Q to predict the answer, whereas the current paper shows links from inputs X which contains both V and Q to answer Y, which is not correct. Please clarify the exact formulation of the ID and OOD teachers in the revised version. [W2] Some concerns about OOD/ID testing setup :
Review Point: W2.1 Use of soft/hard weighting for strong vs weak teachers is not very well motivated. Why is Rubi (or by extension, NIE methods) a weak teacher? Why does using hard weighing help "weak models"?
Review Point: W2.2 Are ID and OOD performance measured for the same model/data choices? As mentioned in [30], retraining before reporting ID performance can inflate the results on ID evaluation and effectively create two different versions of the model. Upon reading the paper, I don't think that this has been addressed. Overall: Overall, I think this is a good paper and I currently recommend a weak acceptance. However, it could be slightly higher if the clarity issues were resolved.
==================================================

Focused review:

Weaknesses.
W1: The paper is unclear on how the pretrained model was used, and when it was used, especially for the baseline attacks. Since this is a very important factor based on the appendix, this needs to be clarified.
W2: The baselines do not include Extraction-AT which is seemingly the best alternative, and this approach might be tuned to perform better. But instead, this is only shown to motivate the problem with a limited setting.
W3: The main varying parameter for each configuration is epochs, not budget. This limits the understanding of alternative approaches and overall utility as the attacker is more limited by the budget than the number of epochs. Also, the maximum epoch tested here looks rather a bit too high as ResNet-18 on CIFAR can train much faster. Does the attack augment the data somehow with more epochs despite the limited budget?


Review Point: W1: The paper is unclear on how the pretrained model was used, and when it was used, especially for the baseline attacks. Since this is a very important factor based on the appendix, this needs to be clarified.
Review Point: W2: The baselines do not include Extraction-AT which is seemingly the best alternative, and this approach might be tuned to perform better. But instead, this is only shown to motivate the problem with a limited setting.
Review Point: W3: The main varying parameter for each configuration is epochs, not budget. This limits the understanding of alternative approaches and overall utility as the attacker is more limited by the budget than the number of epochs. Also, the maximum epoch tested here looks rather a bit too high as ResNet-18 on CIFAR can train much faster. Does the attack augment the data somehow with more epochs despite the limited budget?
==================================================

Focused review:

The main weaknesses of this paper are: - Lack of comparison and reference to a very similar 2019 ICML paper "Actor-Attention-Critic for Multi-Agent Reinforcement Learning" by Iqbal et al. In that paper the authors also use attention to learn what observations are relevant to the agents. - Comparing RL algorithms can be difficult as RL is sensitive to hyperparameter choice and network design. As such using the same network and hyperparameters might disadvantage the baselines / compared method. Perhaps a better methodology would be to optimize baselines (in terms of network design, hyperparameters and training) independent given a rough budget of computation. - Figure 3 is very hard to decipher and gives little insight. Bottom right has very large overlaps between methods making it unclear how large the difference is. Bottom left has several series though it is unclear to me what the dashed lines represent. I'm not sure what the top row is supposed to present. Correlations between dashed lines and areas seems relatively low, maybe this plot should be represented some other way.

Review Point: - Lack of comparison and reference to a very similar 2019 ICML paper "Actor-Attention-Critic for Multi-Agent Reinforcement Learning" by Iqbal et al. In that paper the authors also use attention to learn what observations are relevant to the agents.
Review Point: - Comparing RL algorithms can be difficult as RL is sensitive to hyperparameter choice and network design. As such using the same network and hyperparameters might disadvantage the baselines / compared method. Perhaps a better methodology would be to optimize baselines (in terms of network design, hyperparameters and training) independent given a rough budget of computation.
Review Point: - Figure 3 is very hard to decipher and gives little insight. Bottom right has very large overlaps between methods making it unclear how large the difference is. Bottom left has several series though it is unclear to me what the dashed lines represent. I'm not sure what the top row is supposed to present. Correlations between dashed lines and areas seems relatively low, maybe this plot should be represented some other way.
==================================================

Focused review:

Weaknesses
While the paper title is phrased as targeting the predictive uncertainty, the paper's discussion and experiments solely focus on the epistemic/aleatoric decomposed setup (apart from some in-distribution calibration results). While the distinction between reducible and irreducible uncertainty is nice within a model, to better understand its performance or as a guiding signal for an active learning task, the paper explores no direction where decomposition of the predictive uncertainty actually is relevant.
Related to the last point, the experimental comparisons are also all against methods that allow for a clean distinction instead of focusing on a broader class of models that target a predictive uncertainty independent of whether they allow for a theoretical/practical decomposition of such a term (e.g. a simple deterministic net with temperature scaling for calibration).
The exponential family offers an extension away from the standard classification case. The paper has experiments with regression and count data, but as the authors acknowledge, the Poisson likelihood seems to perform poorly in the chosen data set. Having an experiment where the explicit modeling of the counts as counts was necessary would be a lot stronger.
The paper is closely related to prior work by Charpentier et al. (2020), down to using the same input examples in Figure 2.
The related work discusses BNNs in the sampling-based paragraph but lacks a similar discussion of BNNs in the sampling-free methods Edit: Wrong claim from my side. I overlooked some references in the sampling-free paragraph as the keyword BNN was missing.
Questions and minor comments
Q1: In the contribution, the authors mention (point (3)) that the density is added "to the last predictor layer", a term that is never mentioned again. Can the authors comment on what they mean by that?
Q2: As mentioned above on the epistemic/aleatoric point, a stated goal is to aim "at accurately modelling both aleatoric and epistemic uncertainty". Can the authors comment on that goal and why they target it as the paper lacks discussions/explorations on the usefulness of the distinction between the two?
The experiments compare against Amini et al. (2020) in the regression case but lack a comparison in the classification against Sensoy et al. (2018) Amini et al.'s closest relation and predecessor. That work also has the benefit of not OOD data during training as the PriorNets do.
The BNN discussion comments on the huge computational budget of Izmailov et al. (2021) as a downside. This comment ignores that this cost was due to their goal of inferring the true posterior as optimally as possible via a costly HMC approach. However, that work includes many cheaper methods (be they variational inference-based or MCMC-based, such as SGLD, SGHMC) that demonstrate good performance for a much cheaper computational cost.
Table 1 contains (1),(2) whose meaning is only resolved several pages later. At the same time, the page includes three different sets of (1),(2) (twice in two separate lists, and once each as equation numbers). A restructuring might be helpful to guide the reader towards the desired meaning in table 1.
While the term is only taken from prior work, I would strongly encourage the authors to rename their objective to something other than "the Bayesian loss". It is an objective whose optimum is given by the posterior, which is nice, but apart from that, there is no such thing as THE Bayesian loss or even "the principled Bayesian loss"..
NatPN Ensembles are discussed, but the NatPE abbreviation is missing from the paragraph and never formally introduced
In equation (6), as soon as
λ
≠
1
, L is not actually proportional to the right-hand side anymore.
Minor Appendix comments
App B: The ELBO loss refers again to the appendix
App B: There appears a discussion on a prior over
y
, while talking about distributions over
θ
In the appendix
P
(
θ
)
and
P
(
y
 
θ
)
seem to be used interchangeably
The appendix states that the entropy of the posterior is used to estimate the predictive uncertainty, but the posterior predictive uncertainty should actually be computed after marginalization over the posterior.
Typos
Throughout the paper, please follow the ICLR style guide properly. E.g. captions belong above tables and below figures.
Table 1 refers to eq (7) which is in the appendix. It should probably mean (5), i.e. the same loss just in the main paper
End of page 3: Bishop (2006) -> (Bishop, 2006).
page 4: predictive posterior distribution -> posterior predictive distribution
page 5: using each NatPN member all separately
The abbreviation OOD is introduced, but ID is not and needs to be guessed from the context

Sensoy et al., Evidential Deep Learning to Quantify Classification Uncertainty, NeurIPS 2018


Review Point: While the paper title is phrased as targeting the predictive uncertainty, the paper's discussion and experiments solely focus on the epistemic/aleatoric decomposed setup (apart from some in-distribution calibration results). While the distinction between reducible and irreducible uncertainty is nice within a model, to better understand its performance or as a guiding signal for an active learning task, the paper explores no direction where decomposition of the predictive uncertainty actually is relevant. Related to the last point, the experimental comparisons are also all against methods that allow for a clean distinction instead of focusing on a broader class of models that target a predictive uncertainty independent of whether they allow for a theoretical/practical decomposition of such a term (e.g. a simple deterministic net with temperature scaling for calibration). The exponential family offers an extension away from the standard classification case. The paper has experiments with regression and count data, but as the authors acknowledge, the Poisson likelihood seems to perform poorly in the chosen data set. Having an experiment where the explicit modeling of the counts as counts was necessary would be a lot stronger. The paper is closely related to prior work by Charpentier et al. (2020), down to using the same input examples in Figure 2. The related work discusses BNNs in the sampling-based paragraph but lacks a similar discussion of BNNs in the sampling-free methods Edit: Wrong claim from my side. I overlooked some references in the sampling-free paragraph as the keyword BNN was missing. Questions and minor comments Q1: In the contribution, the authors mention (point (3)) that the density is added "to the last predictor layer", a term that is never mentioned again. Can the authors comment on what they mean by that?
Review Point: Q2: As mentioned above on the epistemic/aleatoric point, a stated goal is to aim "at accurately modelling both aleatoric and epistemic uncertainty". Can the authors comment on that goal and why they target it as the paper lacks discussions/explorations on the usefulness of the distinction between the two? The experiments compare against Amini et al. (2020) in the regression case but lack a comparison in the classification against Sensoy et al. (2018) Amini et al.'s closest relation and predecessor. That work also has the benefit of not OOD data during training as the PriorNets do. The BNN discussion comments on the huge computational budget of Izmailov et al. (2021) as a downside. This comment ignores that this cost was due to their goal of inferring the true posterior as optimally as possible via a costly HMC approach. However, that work includes many cheaper methods (be they variational inference-based or MCMC-based, such as SGLD, SGHMC) that demonstrate good performance for a much cheaper computational cost. Table 1 contains (1),(2) whose meaning is only resolved several pages later. At the same time, the page includes three different sets of (1),(2) (twice in two separate lists, and once each as equation numbers). A restructuring might be helpful to guide the reader towards the desired meaning in table 1. While the term is only taken from prior work, I would strongly encourage the authors to rename their objective to something other than "the Bayesian loss". It is an objective whose optimum is given by the posterior, which is nice, but apart from that, there is no such thing as THE Bayesian loss or even "the principled Bayesian loss".. NatPN Ensembles are discussed, but the NatPE abbreviation is missing from the paragraph and never formally introduced In equation (6), as soon as λ ≠ 1 , L is not actually proportional to the right-hand side anymore. Minor Appendix comments App B: The ELBO loss refers again to the appendix App B: There appears a discussion on a prior over y , while talking about distributions over θ In the appendix P ( θ ) and P ( y θ ) seem to be used interchangeably The appendix states that the entropy of the posterior is used to estimate the predictive uncertainty, but the posterior predictive uncertainty should actually be computed after marginalization over the posterior. Typos Throughout the paper, please follow the ICLR style guide properly. E.g. captions belong above tables and below figures. Table 1 refers to eq (7) which is in the appendix. It should probably mean (5), i.e. the same loss just in the main paper End of page 3: Bishop (2006) -> (Bishop, 2006). page 4: predictive posterior distribution -> posterior predictive distribution page 5: using each NatPN member all separately The abbreviation OOD is introduced, but ID is not and needs to be guessed from the context Sensoy et al., Evidential Deep Learning to Quantify Classification Uncertainty, NeurIPS 2018
==================================================

Focused review:

Weaknesses:
The setting borrows from Rosenfeld et al, which shows the reason why IRM may fail in some simple cases. However, the goal of the current paper is to show an DG algorithm that works in practice. As much as I appreciate the theoretical contributions about the positive results of IFM, in practice (e.g. on MNIST), the algorithm does not work. Moreover, as mentioned in Appendix C, there is even a
O
(
1
)
environmental complexity algorithm based on Assumption 3.2, which further adds doubts to the applicability of this assumption. If IFM does provide intuitions into algorithmic design, the authors need to at least show some more practical examples.
The lower bounds of ERM/IRM do not seem novel compared to Rosenfeld et al. The only difference is the assumption 3.2 vs. assumption 3.1. In Line 149-150, the authors claim that the analysis applies to all but a measure-zero set of covariances, but I'm not quite convinced since the Gram matrix is built from standard normal and the
Σ
¯
is bounded.
A practical algorithm for finding the maximum dimension
r
t
in Alg 1 is needed. This is very important for practitioners to apply this algorithm.
In the experimental section, the main contribution part is 1) addition of orthonomality of the featurizer in the Gaussian model; 2) matching the sufficient statistics of each layer vs. only matching the last layer. This experiment may be too far away from the linear featurizer and the data model presented in the theoretical analysis. If matching the representation of each layer works for CORAL/DANN/MMD in practice, could the authors try some experiments or prove some theoretical results?
In line 203-205, the goal is to provide a theoretical justification of the feature matching algorithm, rather than to solve a specific data model, but the main theoretical results are based on this model.
The IFM algorithm resembles Independent Component Analysis (ICA). Discussions and comparisons of the similarities and differences are needed.
As the authors acknowledge in Appendix D, the main limitation is the applicability of IFM in realistic datasets.


Review Point: The setting borrows from Rosenfeld et al, which shows the reason why IRM may fail in some simple cases. However, the goal of the current paper is to show an DG algorithm that works in practice. As much as I appreciate the theoretical contributions about the positive results of IFM, in practice (e.g. on MNIST), the algorithm does not work. Moreover, as mentioned in Appendix C, there is even a O ( 1 ) environmental complexity algorithm based on Assumption 3.2, which further adds doubts to the applicability of this assumption. If IFM does provide intuitions into algorithmic design, the authors need to at least show some more practical examples. The lower bounds of ERM/IRM do not seem novel compared to Rosenfeld et al. The only difference is the assumption 3.2 vs. assumption 3.1. In Line 149-150, the authors claim that the analysis applies to all but a measure-zero set of covariances, but I'm not quite convinced since the Gram matrix is built from standard normal and the Σ ¯ is bounded. A practical algorithm for finding the maximum dimension r t in Alg 1 is needed. This is very important for practitioners to apply this algorithm. In the experimental section, the main contribution part is 1) addition of orthonomality of the featurizer in the Gaussian model;
Review Point: 2) matching the sufficient statistics of each layer vs. only matching the last layer. This experiment may be too far away from the linear featurizer and the data model presented in the theoretical analysis. If matching the representation of each layer works for CORAL/DANN/MMD in practice, could the authors try some experiments or prove some theoretical results? In line 203-205, the goal is to provide a theoretical justification of the feature matching algorithm, rather than to solve a specific data model, but the main theoretical results are based on this model. The IFM algorithm resembles Independent Component Analysis (ICA). Discussions and comparisons of the similarities and differences are needed. As the authors acknowledge in Appendix D, the main limitation is the applicability of IFM in realistic datasets.
==================================================

Focused review:

Weakness:  1. There are many limitations of the proposed method. The proposed method assumes that the causal graphical is given. Also, the values must be discrete.   2. It would be good to show how to use the proposed method to achieve fair policy learning without "severely damaging the performance of predictive model".   3. It would be great to discuss why the fairness bound achieved by the proposed method is tighter compared with previous methods.  Minor issues: line 17 irrespective their -> irrespective of their line 240 to find -> to finding  Should the the numbers in Table 3 CE #of o 4 be bold?  The bound of the proposed method is tighter than previous methods.  

Review Point: 1. There are many limitations of the proposed method. The proposed method assumes that the causal graphical is given. Also, the values must be discrete.
Review Point: 2. It would be good to show how to use the proposed method to achieve fair policy learning without "severely damaging the performance of predictive model".
Review Point: 3. It would be great to discuss why the fairness bound achieved by the proposed method is tighter compared with previous methods. Minor issues: line 17 irrespective their -> irrespective of their line 240 to find -> to finding Should the the numbers in Table 3 CE #of o 4 be bold? The bound of the proposed method is tighter than previous methods.
==================================================

Focused review:

Weakness
The main question I have is how the algorithm/theory analysis in this paper can handle the case when the multiple tasks are kind of different from the others, given that the authors did not have data selection step but instead transfer all of them.
On the empirical side, I'm feeling if the other tasks are different from the current task, by directly using Eq. 8, the data from other tasks will significant bias the learned Q function from the true Q* in current task. I'm not sure if it is because the tasks are similar in the experiments part that this problem hasn't been observed in experiments. I would be appreciate it if the authors can explain more about it.
On the theory side, I'm feeling there are some mistakes when the authors adapting the theory from [3] to justify their methods and there are also some clarity issue in their proof. Different from [3] which considered single task learning in linear MDP, the authors in this paper considers multi-task learning, and it's unclear what assumptions they made about the similarity about the feature map
ϕ
i
and the linear transition kernel
P
i
=
⟨
ϕ
i
,
μ
i
⟩
between different tasks (
i
is the task index here), especially the authors used the same feature map
ϕ
in this paper. Besides, in Page 18, the proof of Eq. (47), the authors directly refer [3] for detailed proof, while [3] only considers the single task setting. I do not understand why their proof can directly transfer here, and I feel there should be some bias term (maybe something captures the distance between tasks) occurs on the RHS.
As a summary, the method in this paper does not make sense to me and I would like the authors explain more details.
Besides, there are also some clarity issue:
(1) I do not understand Eq. (6) very well. According to Eq. (7), the term inside the expectation of Eq. (6) would just be the uncertainty term
β
Γ
, which does not make sense. Or maybe you ``stop gradient'' of
T
^
o
o
d
Q
i
during the training? But it is also weird since it always asks
Q
i
to approximate something that is lower than itself, which will drag it to approach zero.
Besides, why it takes square after taking the expectation instead of before expectation?
(2) The author claims that
T
^
U
T
D
S
Q
is a contraction mapping but just refer it to a book [1]. I would like the authors to clarify which chapter in [1] such result comes from, because intuitively speaking, when computing
T
^
U
T
D
S
, it requires to subtract uncertainty term, which is different from the normal Bellman operator.
Besides, no matter whether it is a contraction operator, I would suggest the authors to make it clear what it converges to if applying
T
^
U
T
D
S
for multiple times (and whether it converges if it is not a contraction operator). I think it is important to justify the algorithm is doing something reasonable.
(3) The Sec. 4 is also hard to follow. I think the authors should first clarify that the linear MDP setting in [2-3] is episodic MDP (with finite horizon H), instead of the discounted MDP in this paper. This is crucial because in discounted setting Q/V function is stationary for all time step, while in episodic setting the Q/V function is usually time-dependent (since the horizon is finite). As a result, the authors should also specify the step index
h
when they discussing the value function (and the uncertainty terms) in Sec. 4. Moreover, I think the authors should also discuss whether the gap/difference between discounted and episodic setting will weaken their explanation.
Besides, what is T in Thm. 2?
[1] Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms, 2022.
[2] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143. PMLR, 2020.
[3] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pp. 5084–5096. PMLR, 2021.


Review Point: The main question I have is how the algorithm/theory analysis in this paper can handle the case when the multiple tasks are kind of different from the others, given that the authors did not have data selection step but instead transfer all of them. On the empirical side, I'm feeling if the other tasks are different from the current task, by directly using Eq. 8, the data from other tasks will significant bias the learned Q function from the true Q* in current task. I'm not sure if it is because the tasks are similar in the experiments part that this problem hasn't been observed in experiments. I would be appreciate it if the authors can explain more about it. On the theory side, I'm feeling there are some mistakes when the authors adapting the theory from [3] to justify their methods and there are also some clarity issue in their proof. Different from [3] which considered single task learning in linear MDP, the authors in this paper considers multi-task learning, and it's unclear what assumptions they made about the similarity about the feature map ϕ i and the linear transition kernel P i = ⟨ ϕ i , μ i ⟩ between different tasks ( i is the task index here), especially the authors used the same feature map ϕ in this paper. Besides, in Page 18, the proof of Eq. (47), the authors directly refer [3] for detailed proof, while [3] only considers the single task setting. I do not understand why their proof can directly transfer here, and I feel there should be some bias term (maybe something captures the distance between tasks) occurs on the RHS. As a summary, the method in this paper does not make sense to me and I would like the authors explain more details. Besides, there are also some clarity issue: (1) I do not understand Eq. (6) very well. According to Eq. (7), the term inside the expectation of Eq. (6) would just be the uncertainty term β Γ , which does not make sense. Or maybe you ``stop gradient'' of T ^ o o d Q i during the training? But it is also weird since it always asks Q i to approximate something that is lower than itself, which will drag it to approach zero. Besides, why it takes square after taking the expectation instead of before expectation? (2) The author claims that T ^ U T D S Q is a contraction mapping but just refer it to a book [1]. I would like the authors to clarify which chapter in [1] such result comes from, because intuitively speaking, when computing T ^ U T D S , it requires to subtract uncertainty term, which is different from the normal Bellman operator. Besides, no matter whether it is a contraction operator, I would suggest the authors to make it clear what it converges to if applying T ^ U T D S for multiple times (and whether it converges if it is not a contraction operator). I think it is important to justify the algorithm is doing something reasonable. (3) The Sec. 4 is also hard to follow. I think the authors should first clarify that the linear MDP setting in [2-3] is episodic MDP (with finite horizon H), instead of the discounted MDP in this paper. This is crucial because in discounted setting Q/V function is stationary for all time step, while in episodic setting the Q/V function is usually time-dependent (since the horizon is finite). As a result, the authors should also specify the step index h when they discussing the value function (and the uncertainty terms) in Sec.
Review Point: 4. Moreover, I think the authors should also discuss whether the gap/difference between discounted and episodic setting will weaken their explanation. Besides, what is T in Thm. 2? [1] Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms, 2022. [2] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143. PMLR, 2020. [3] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pp. 5084–5096. PMLR, 2021.
==================================================

Focused review:

Weaknesses
Regarding the background: the authors should consider adding a preliminary section to introduce the background knowledge on the nonparametric kernel regression, kernel density estimation, and the generalized Fourier Integral theorem, which could help the readers easily follow the derivation of Section 2 and understand the motivation to use the Fourier Integral theorem as a guide to developing a new self-attention mechanism.
Regarding the experimental evaluation: the issues are three-fold. 1) since the authors provide an analysis of the approximation error between estimators and true functions (Theorem 1 and 2), it is informative to provide an empirical evaluation of these quantities on real data as further verification. 2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive. 3) Since the FourierFormer need customized operators for implementation, the authors should also provide the memory/time cost profiling compared to popular Transformer architectures. Based on these issues, the efficiency and effectiveness of the FourierFormer are doubtful.
-------After Rebuttal------- Thank authors for the detailed response. Most of my concerns have been addressed. I have updated my scores to 6.


Review Point: Regarding the background: the authors should consider adding a preliminary section to introduce the background knowledge on the nonparametric kernel regression, kernel density estimation, and the generalized Fourier Integral theorem, which could help the readers easily follow the derivation of Section 2 and understand the motivation to use the Fourier Integral theorem as a guide to developing a new self-attention mechanism. Regarding the experimental evaluation: the issues are three-fold.
Review Point: 1) since the authors provide an analysis of the approximation error between estimators and true functions (Theorem 1 and 2), it is informative to provide an empirical evaluation of these quantities on real data as further verification.
Review Point: 2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive.
Review Point: 3) Since the FourierFormer need customized operators for implementation, the authors should also provide the memory/time cost profiling compared to popular Transformer architectures. Based on these issues, the efficiency and effectiveness of the FourierFormer are doubtful. -------After Rebuttal------- Thank authors for the detailed response. Most of my concerns have been addressed. I have updated my scores to 6.
==================================================

Focused review:

. The currently proposed theory is quite limited and provides limited actionable insights for developing new techniques. Here are a few important questions left largely unanswered -- (a) Why do certain views work better than others? For example, why does L + ab work worse than the image + patch view? Is there a way to quantify this. (b) While the authors empirically verify the schematic in Figure 1(c), it does little to inform us how to design the views without training models. Since I_NCE is computed using the model trained on the views, this measure still requires one to train models. Since the model is already trained, one may also directly compute the transfer performance. 2. I_NCE is essentially -L_NCE which means that Figures 3, 4, 5 really show a correlation between the transfer performance and the training loss. InstDisc [57] also shows such correlations (although only in the limited context of instance discrimination and not studying variables like patch distance) and it would be good to note this. 3. The proposed InfoMin Aug is not explored in great detail. This result in mentioned in the abstract, but the paper spend one paragraph (L209) to explain everything about this method - the model, the data, the training, the transfer result. The authors mention that this method uses augmentation from prior work, but it is not clear how these are combined. Also, since each of these augmentations provides a different type of view, it would be good to quantify how much each augmentation matters. Questions q1 - How is g parametrized in 4.2.1? q2 - Isn't Figure 2 also a schematic? Might be good to clarify that as a hypothesis/schematic, rather than an observation.

Review Point: . The currently proposed theory is quite limited and provides limited actionable insights for developing new techniques. Here are a few important questions left largely unanswered -- (a) Why do certain views work better than others? For example, why does L + ab work worse than the image + patch view? Is there a way to quantify this. (b) While the authors empirically verify the schematic in Figure 1(c), it does little to inform us how to design the views without training models. Since I_NCE is computed using the model trained on the views, this measure still requires one to train models. Since the model is already trained, one may also directly compute the transfer performance.
Review Point: 2. I_NCE is essentially -L_NCE which means that Figures 3, 4, 5 really show a correlation between the transfer performance and the training loss. InstDisc [57] also shows such correlations (although only in the limited context of instance discrimination and not studying variables like patch distance) and it would be good to note this.
Review Point: 3. The proposed InfoMin Aug is not explored in great detail. This result in mentioned in the abstract, but the paper spend one paragraph (L209) to explain everything about this method - the model, the data, the training, the transfer result. The authors mention that this method uses augmentation from prior work, but it is not clear how these are combined. Also, since each of these augmentations provides a different type of view, it would be good to quantify how much each augmentation matters. Questions q1 - How is g parametrized in 4.2.1? q2 - Isn't Figure 2 also a schematic? Might be good to clarify that as a hypothesis/schematic, rather than an observation.
==================================================

Focused review:

Weaknesses: The novelty of the paper is very limited. There are several messages from section 3:
1. ImageNet performance is not indicative of downstream performance for SSL.
2. Larger networks do not always perform better in contrastive SSL.
3. There is no winner in the battle of top vs. bottom heavy networks in SSL.
4. It is necessary to allocate the right portion of parameters to different layers of a given network topology.
All the messages are trivial, not mathematically rigorous, and are generally observed in different machine-learning tasks. Although I do agree that the empirical study is sufficient as a motivation and can support the general key takeaway: ``we need to move beyond handcrafted architectures in SSL'', but section 3 doesn't convey any new insights about SSL.
The second part of the paper (section 4) applies NAS in SSL, this combination is also a trivial extension and the resulting improvement is not surprising.
I suggest the author treat the empirical study as the first step towards designing a new NAS algorithm to improve the SSL results or provide a more theoretical understanding of the described phenomenon.
Others:
The title of table 1 is misleading, ``NAS vs handcraft architecture '' will be more clear.


Review Point: The novelty of the paper is very limited. There are several messages from section 3:
Review Point: 1. ImageNet performance is not indicative of downstream performance for SSL.
Review Point: 2. Larger networks do not always perform better in contrastive SSL.
Review Point: 3. There is no winner in the battle of top vs. bottom heavy networks in SSL.
Review Point: 4. It is necessary to allocate the right portion of parameters to different layers of a given network topology. All the messages are trivial, not mathematically rigorous, and are generally observed in different machine-learning tasks. Although I do agree that the empirical study is sufficient as a motivation and can support the general key takeaway: ``we need to move beyond handcrafted architectures in SSL'', but section 3 doesn't convey any new insights about SSL. The second part of the paper (section 4) applies NAS in SSL, this combination is also a trivial extension and the resulting improvement is not surprising. I suggest the author treat the empirical study as the first step towards designing a new NAS algorithm to improve the SSL results or provide a more theoretical understanding of the described phenomenon. Others: The title of table 1 is misleading, ``NAS vs handcraft architecture '' will be more clear.
==================================================

Focused review:

1. SVR-ADA does not outperform the baseline methods; in particular, for CIFAR10 (see supplementary). It is completely unacceptable to report a few selected results on simple applications. From this viewpoint, there is no advantage of the proposed SVR-ADA over Katyusha. 2. This paper builds on top of the well-known control-variate approach to reduce the variance of the stochastic gradient and proposed the SVR-ADA algorithm. The control variate approach has been widely studied in many different scenarios, but the application is quite limited, which in practice cannot outperform the baseline SGD. The novelty of this work is very limited, and the contribution is very incremental. 3. More discussion is required to explain the theoretical advantage of SVR-ADA over the existing work. I did not see the real advantage of the theoretical results listed in Table 1 compared with the existing variance reduction methods. 4. In modern machine learning, most of the objective functions are nonconvex, and deep neural networks are used. Whether SVR-ADA applicable to training deep neural networks should be studied, and the generalization performance should be addressed.

Review Point: 1. SVR-ADA does not outperform the baseline methods; in particular, for CIFAR10 (see supplementary). It is completely unacceptable to report a few selected results on simple applications. From this viewpoint, there is no advantage of the proposed SVR-ADA over Katyusha.
Review Point: 2. This paper builds on top of the well-known control-variate approach to reduce the variance of the stochastic gradient and proposed the SVR-ADA algorithm. The control variate approach has been widely studied in many different scenarios, but the application is quite limited, which in practice cannot outperform the baseline SGD. The novelty of this work is very limited, and the contribution is very incremental.
Review Point: 3. More discussion is required to explain the theoretical advantage of SVR-ADA over the existing work. I did not see the real advantage of the theoretical results listed in Table 1 compared with the existing variance reduction methods.
Review Point: 4. In modern machine learning, most of the objective functions are nonconvex, and deep neural networks are used. Whether SVR-ADA applicable to training deep neural networks should be studied, and the generalization performance should be addressed.
==================================================

Focused review:

Weaknesses: The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).
- General Discussion: The current presented approach, in comparison to existing approaches, is promising. 

Review Point: The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).
Review Point: - General Discussion: The current presented approach, in comparison to existing approaches, is promising.
==================================================

Focused review:

Weakness:
While the authors' motivation for the need to embed 3D information into learning visual representation is clear, the proposed approach is not convincing for multiple reasons,
Authors have proposed a pseudo depth estimator and pseudo camera parameter estimator to lift 2D token locations to 3D. While I can understand 2D image information being used to learn depth, in a free-view image analysis scenario, I don’t think camera parameter estimation makes sense. I.e. We don't even know the reference, and how the R,t of one image is related to the R, t of the other image in CIFAR-10? In 3D multi-view datasets there are approaches that directly regress camera poses and scene coordinates ([1], [2]). However, in this case, the authors are trying to embed 3D scene information in model weights, while in free-viewpoint images that don't have a common reference frame, the proposed approach is most likely not valid.
The transformers are effective with large amounts of data, and the minor improvement of 3DTRL on the ImageNet-1K dataset shows that transformers already become effective without 3DTRL. I understand that authors have shown a validation set on view-point perturbed data with the perspective transformation of the validation-set of ImageNet-1K, but this is not a valid viewpoint change, and we cannot trust that this will really work in multi-view scenarios.
While I can understand this approach being suitable for multi-view video alignment, since there we are fine-tuning it for each task and 3DTRL can build implicit representation, I cannot see this as a generic module for learning viewpoint-agnostic representation.
Also, from the depth-map visualization (Fig. 6), I think it is just learning to pay attention to the primary parts of the class (e.g. for each of the dog images, the eyes, ears, and noses have very low value). I cannot see the depth being learned here.
Authors mention that this is a generic plug-and-play (L14, L48), but all the experiments are only with DeiT. More experiments with different architecture are needed to support the claim.
The notations and math are also unclear at many places:
E.g. in eq (2) authors represent (u,v) as center pixel coordinates. While in eq (3), (u,v) are token 2D locations.
The camera model presented in eq (2) is not a standard pinhole camera model. In standard pinhole camera if dn is a depth, then xn = dnun/c , yn = dnvn/c , zn=d (considering camera frame and image frame matches at (0,0))
Some typos/writing mistakes
L55: (u, v) are pixel coordinates: (u, v) are not any pixel coordinates, they are coordinates of the center pixel of the image. L104: which is beneficial for later representation learning → which is beneficial later for representation learning (or did you mean, which is beneficial for better representation learning) L133: raw → yaw Fig4: 3DTPL → 3DTRL
References: [1] Li, Xiaotian, Juha Ylioinas, and Juho Kannala. "Full-Frame Scene Coordinate Regression for Image-Based Localization." Robotics: Science and Systems Conference. University of Queensland, 2018.
[2] Brachmann, Eric, and Carsten Rother. "Learning less is more-6d camera localization via 3d surface regression." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[3] Liu, Ze, et al. "Swin transformer: Hierarchical vision transformer using shifted windows." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
The authors have not mentioned any limitations or negative social impact of the work. The clear limitation of the 3DTRL module I see is it is not generic module, and mostly applicable for data with multiple views of the same scene. (i.e. multiview video alignment)


Review Point: While the authors' motivation for the need to embed 3D information into learning visual representation is clear, the proposed approach is not convincing for multiple reasons, Authors have proposed a pseudo depth estimator and pseudo camera parameter estimator to lift 2D token locations to 3D. While I can understand 2D image information being used to learn depth, in a free-view image analysis scenario, I don’t think camera parameter estimation makes sense. I.e. We don't even know the reference, and how the R,t of one image is related to the R, t of the other image in CIFAR-10? In 3D multi-view datasets there are approaches that directly regress camera poses and scene coordinates ([1], [2]). However, in this case, the authors are trying to embed 3D scene information in model weights, while in free-viewpoint images that don't have a common reference frame, the proposed approach is most likely not valid. The transformers are effective with large amounts of data, and the minor improvement of 3DTRL on the ImageNet-1K dataset shows that transformers already become effective without 3DTRL. I understand that authors have shown a validation set on view-point perturbed data with the perspective transformation of the validation-set of ImageNet-1K, but this is not a valid viewpoint change, and we cannot trust that this will really work in multi-view scenarios. While I can understand this approach being suitable for multi-view video alignment, since there we are fine-tuning it for each task and 3DTRL can build implicit representation, I cannot see this as a generic module for learning viewpoint-agnostic representation. Also, from the depth-map visualization (Fig. 6), I think it is just learning to pay attention to the primary parts of the class (e.g. for each of the dog images, the eyes, ears, and noses have very low value). I cannot see the depth being learned here. Authors mention that this is a generic plug-and-play (L14, L48), but all the experiments are only with DeiT. More experiments with different architecture are needed to support the claim. The notations and math are also unclear at many places: E.g. in eq (2) authors represent (u,v) as center pixel coordinates. While in eq (3), (u,v) are token 2D locations. The camera model presented in eq (2) is not a standard pinhole camera model. In standard pinhole camera if dn is a depth, then xn = dnun/c , yn = dnvn/c , zn=d (considering camera frame and image frame matches at (0,0)) Some typos/writing mistakes L55: (u, v) are pixel coordinates: (u, v) are not any pixel coordinates, they are coordinates of the center pixel of the image.
Review Point: L104: which is beneficial for later representation learning → which is beneficial later for representation learning (or did you mean, which is beneficial for better representation learning) L133: raw → yaw Fig4: 3DTPL → 3DTRL References: [1] Li, Xiaotian, Juha Ylioinas, and Juho Kannala. "Full-Frame Scene Coordinate Regression for Image-Based Localization." Robotics: Science and Systems Conference. University of Queensland, 2018. [2] Brachmann, Eric, and Carsten Rother. "Learning less is more-6d camera localization via 3d surface regression." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. [3] Liu, Ze, et al. "Swin transformer: Hierarchical vision transformer using shifted windows." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. The authors have not mentioned any limitations or negative social impact of the work. The clear limitation of the 3DTRL module I see is it is not generic module, and mostly applicable for data with multiple views of the same scene. (i.e. multiview video alignment)
==================================================

Focused review:

Novelty: 1. One of the main contribution of the paper is the algorithm proposed in Section 5 which achieves the minimax optimal regret bound. However, the main technique used here is the mini-batch paradigm which has been considered before, e.g., in [Dekel et al., 2011; Arora et al., 2012], and it seems that the proof technique is rather straightforward. 2. It seems that the adversary’s strategy and the minimax analysis in Section 4.1 are largely based on those of [Abernethy et al., 2008]. It would be better if the authors could add more discussions on what are the difficulties to adapt the minimax analysis for classic OCO into the switch-constrained setting. Significance: 2. The fugal game proposed in this paper is novel and very interesting, but I am unsure about the significance of this contribution since it is considered in the 1-d situation and it only improves the minimax bound by constant factors. --------------------------------------Post Rebuttal--------------------------- The authors cleared my concerns in the rebuttal. I am happy to raise my score.

Review Point: Novelty:1. One of the main contribution of the paper is the algorithm proposed in Section 5 which achieves the minimax optimal regret bound. However, the main technique used here is the mini-batch paradigm which has been considered before, e.g., in [Dekel et al., 2011; Arora et al., 2012], and it seems that the proof technique is rather straightforward.
Review Point: 2. It seems that the adversary’s strategy and the minimax analysis in Section 4.1 are largely based on those of [Abernethy et al., 2008]. It would be better if the authors could add more discussions on what are the difficulties to adapt the minimax analysis for classic OCO into the switch-constrained setting. Significance:
Review Point: 2. The fugal game proposed in this paper is novel and very interesting, but I am unsure about the significance of this contribution since it is considered in the 1-d situation and it only improves the minimax bound by constant factors. --------------------------------------Post Rebuttal--------------------------- The authors cleared my concerns in the rebuttal. I am happy to raise my score.
==================================================

Focused review:

Although I find Sections 1-3 nicely written and easy to follow, I have many issues with the experimental section. There are way too much information (and way too less description). I found Table 1 almost impossible to parse, even going through it multiple times. Much of the experimental setup is not really described either. - How was the the proposed method compared to supervised adversarial training [9,2] on the linear evaluation setting? Do the supervised methods are trained, then the CNN features frozen, then a new linear layer trained on the top of it? - I find the actual task of linear evaluation for adversarial training not very well motivated. The fact of evaluating self-supervised/unsupervised features with a linear probing makes (a bit) of sense, since we want to see how good the learning of features are. However, in the case of feature robustness (which is about the performance of features in a particular task) does not make sense, I dont understand why one would be interested in linear evaluation. I feel like the finetuning scenario makes much more sense -- and unfortunately, only a small part of experiments deal with it. - There are way too much training details missing in the paper. For example, what kind of data augmentation were used on the contrastive learning part? How comes SimCLR works only assuming a batch size of 256 (and therefore a very small number of negative samples at each iteration)? SimCLR require a very large batch size (order of thousands) to make it work. - It is not clear to me why is it necessary to consider both t'(x) and t'(x)^{adv} as positive samples in RoCL. Why the two positive samples instead of just the the adversarial pertubation? Some ablation studies explaining some model choices would be also helpful.

Review Point: Although I find Sections 1-3 nicely written and easy to follow, I have many issues with the experimental section. There are way too much information (and way too less description). I found Table 1 almost impossible to parse, even going through it multiple times. Much of the experimental setup is not really described either.
Review Point: - How was the the proposed method compared to supervised adversarial training [9,2] on the linear evaluation setting? Do the supervised methods are trained, then the CNN features frozen, then a new linear layer trained on the top of it?
Review Point: - I find the actual task of linear evaluation for adversarial training not very well motivated. The fact of evaluating self-supervised/unsupervised features with a linear probing makes (a bit) of sense, since we want to see how good the learning of features are. However, in the case of feature robustness (which is about the performance of features in a particular task) does not make sense, I dont understand why one would be interested in linear evaluation. I feel like the finetuning scenario makes much more sense -- and unfortunately, only a small part of experiments deal with it.
Review Point: - There are way too much training details missing in the paper. For example, what kind of data augmentation were used on the contrastive learning part? How comes SimCLR works only assuming a batch size of 256 (and therefore a very small number of negative samples at each iteration)? SimCLR require a very large batch size (order of thousands) to make it work.
Review Point: - It is not clear to me why is it necessary to consider both t'(x) and t'(x)^{adv} as positive samples in RoCL. Why the two positive samples instead of just the the adversarial pertubation? Some ablation studies explaining some model choices would be also helpful.
==================================================

Focused review:

The main weaknesses of this paper are perhaps in missed opportunities to make it even more insightful. - The fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way). - This could perhaps be demonstrated by seeing if those detectors also suffered the same degradation in detection accuracy under compression. It would be even more interesting to see if the accuracy of these prior methods suffered after the proposed 'spectrum synthesis' step. - It would also be good to have more discussion, and preferably also analysis, of "why" these generative models produce images with different spectra. Is it something in the small-kernel convolution-transpose decoder architecture that have become so common ? Or is it because of where and how the random source (i.e., noise) is injected into these architectures ? One can imagine experiments to investigate this further: for example, how does fake detection accuracy change when one trains a GAN with spatially larger kernels; and how does it change if one introduces noise in later vs earlier layers.

Review Point: The main weaknesses of this paper are perhaps in missed opportunities to make it even more insightful.
Review Point: - The fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way).
Review Point: - This could perhaps be demonstrated by seeing if those detectors also suffered the same degradation in detection accuracy under compression. It would be even more interesting to see if the accuracy of these prior methods suffered after the proposed 'spectrum synthesis' step.
Review Point: - It would also be good to have more discussion, and preferably also analysis, of "why" these generative models produce images with different spectra. Is it something in the small-kernel convolution-transpose decoder architecture that have become so common ? Or is it because of where and how the random source (i.e., noise) is injected into these architectures ? One can imagine experiments to investigate this further: for example, how does fake detection accuracy change when one trains a GAN with spatially larger kernels; and how does it change if one introduces noise in later vs earlier layers.
==================================================

Focused review:

Weakness:
A stronger attack for stronger defense is not employed due to its harmful impacts on natural accuracy, but I cannot find any explanations about the observation from the proposed perspective. It is necessary to add the corresponding explanation using the proposed view because methods inspired by the proposed framework are dropped from the main page, making the framework less convincing.
The definition of Eq. (3) seems confusing. The trainer T is directly applied to update the model, i.e., changing the learned representations, thus, introducing the trainer T, given the representation, seems meaningless, but the trainer T is important for the rest of the work. Hence, the authors should clarify the point. Similarly, the utilized F_f^T is also confusing.
Suggestions: 1 The claimed theory seems to be a hypothesis, as merely experimental analysis is given in the current version.
2 According to the claimed perspective -- the main goal is to prevent the model trainer T from fitting non-robust features too quickly and too adequately --, a straightforward approach can be that the trainer decreases the learning rate progressively from the previous epoch decaying the learning rate by 0.1. Thus, the corresponding experiments are required. However, it is lacking in the current version.
3 Different from the section on the method, the section on explanation seems weak, I suggest employing more references to convince the claim, the related perspective is the spurious feature perspective, such as CausalADV and group DRO.
4 The ‘feature’ considered in Eq. (2) corresponds to the data space, while the ‘feature’ considered in this work Eq. (3) is more about the representation space, thus, I suggest the authors make these two features distinguishable as they are conceptually different.
[CausalADV] Adversarial robustness through the lens of causality. Zhang et al. 2021
[group DRO] Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. Sagawa et al., 2020


Review Point: A stronger attack for stronger defense is not employed due to its harmful impacts on natural accuracy, but I cannot find any explanations about the observation from the proposed perspective. It is necessary to add the corresponding explanation using the proposed view because methods inspired by the proposed framework are dropped from the main page, making the framework less convincing. The definition of Eq. (3) seems confusing. The trainer T is directly applied to update the model, i.e., changing the learned representations, thus, introducing the trainer T, given the representation, seems meaningless, but the trainer T is important for the rest of the work. Hence, the authors should clarify the point. Similarly, the utilized F_f^T is also confusing. Suggestions: 1 The claimed theory seems to be a hypothesis, as merely experimental analysis is given in the current version.
Review Point: 2 According to the claimed perspective -- the main goal is to prevent the model trainer T from fitting non-robust features too quickly and too adequately --, a straightforward approach can be that the trainer decreases the learning rate progressively from the previous epoch decaying the learning rate by 0.1. Thus, the corresponding experiments are required. However, it is lacking in the current version.
Review Point: 3 Different from the section on the method, the section on explanation seems weak, I suggest employing more references to convince the claim, the related perspective is the spurious feature perspective, such as CausalADV and group DRO.
Review Point: 4 The ‘feature’ considered in Eq. (2) corresponds to the data space, while the ‘feature’ considered in this work Eq. (3) is more about the representation space, thus, I suggest the authors make these two features distinguishable as they are conceptually different. [CausalADV] Adversarial robustness through the lens of causality. Zhang et al. 2021 [group DRO] Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. Sagawa et al., 2020
==================================================

Focused review:

1. The self-supervision part is over-claimed. In fact, this work uses supervised learning to train the correspondence predictor for warm-start good initialization. It is more like a semi-supervised method. 2. Line 33-34 is a claim without any real support. Why predict the latent parameters from 3D point cloud is difficult? 3. The comparison in Fig 3 is not fair, the use of 3D landmarks is for good initialization, the proposed method also needs a good initialization [Line: 218-219], if the purpose is to show the B results, it is better to show the LoopReg results without the supervision loss used for warm-start initialization.

Review Point: 1. The self-supervision part is over-claimed. In fact, this work uses supervised learning to train the correspondence predictor for warm-start good initialization. It is more like a semi-supervised method.
Review Point: 2. Line 33-34 is a claim without any real support. Why predict the latent parameters from 3D point cloud is difficult?
Review Point: 3. The comparison in Fig 3 is not fair, the use of 3D landmarks is for good initialization, the proposed method also needs a good initialization [Line: 218-219], if the purpose is to show the B results, it is better to show the LoopReg results without the supervision loss used for warm-start initialization.
==================================================

Focused review:

Weaknesses: The presentation of the paper could be improved, especially the methods section. For example, figure 4 which shows the full method overview should appear near the top, especially before figure 3 and 2. Seeing the system overview is important before diving into the specific pieces. Figure 4 should also include
F
A
so we can see where those weights are. It also was not clear how the weights for
F
A
are learned. Eq. 7 has two unknowns:
a
c
which is the attention score and
F
A
which is the weights of the attention module. Do we have access to the ground truth attention score during training in order to learn
F
A
?
The other main weakness is that the results shown in the video still contain considerable artifacts especially in the way the hand moves. Compared to other methods for related 3D reconstruction tasks, such as Nerf based methods, the artifacts are very noticeable. The results are better than the single channel baseline, but could still be better especially given the number of camera views.
Limitations are well addressed in the final section.
If the authors can address questions about the presentation and clarity, and include that in a revised version, then I would consider increasing my review score given that the contribution and method are interesting.


Review Point: The presentation of the paper could be improved, especially the methods section. For example, figure 4 which shows the full method overview should appear near the top, especially before figure 3 and 2. Seeing the system overview is important before diving into the specific pieces. Figure 4 should also include F A so we can see where those weights are. It also was not clear how the weights for F A are learned. Eq.
Review Point: 7 has two unknowns: a c which is the attention score and F A which is the weights of the attention module. Do we have access to the ground truth attention score during training in order to learn F A ? The other main weakness is that the results shown in the video still contain considerable artifacts especially in the way the hand moves. Compared to other methods for related 3D reconstruction tasks, such as Nerf based methods, the artifacts are very noticeable. The results are better than the single channel baseline, but could still be better especially given the number of camera views. Limitations are well addressed in the final section. If the authors can address questions about the presentation and clarity, and include that in a revised version, then I would consider increasing my review score given that the contribution and method are interesting.
==================================================

Focused review:

- 1) Even if the complexity of the algorithm presented in this paper gains a factor log(d), they seem that it might be slower in some cases because the epsilon term cannot really be treated as a "constant". For instance, if d is small and the required precision epsilon is large, and since the terms in the complexity C_n eq. (18) are all multiplied by log(1/epsilon), it might happen that it would be slower than classical competitors. - 2) Unfortunately, the optimality of IHS is only proved for Haar transform. We wish we could have a lower-bound for the error in the SRHT case, like in eq. (11) instead of just eq. (14) - 3) The SRHT used in this paper is not exactly the original method because it incorporates an additional permutation step with matrix P in line 235 - 4) No numerical comparisons against state-of-the-art randomized methods are given. This is sorely lack in the numerical experiments section. Could you please argue on this?

Review Point: - 1) Even if the complexity of the algorithm presented in this paper gains a factor log(d), they seem that it might be slower in some cases because the epsilon term cannot really be treated as a "constant". For instance, if d is small and the required precision epsilon is large, and since the terms in the complexity C_n eq. (18) are all multiplied by log(1/epsilon), it might happen that it would be slower than classical competitors.
Review Point: - 2) Unfortunately, the optimality of IHS is only proved for Haar transform. We wish we could have a lower-bound for the error in the SRHT case, like in eq. (11) instead of just eq. (14) - 3) The SRHT used in this paper is not exactly the original method because it incorporates an additional permutation step with matrix P in line 235 - 4) No numerical comparisons against state-of-the-art randomized methods are given. This is sorely lack in the numerical experiments section. Could you please argue on this?
==================================================

Focused review:

weakness is the assumption that the adversarial noise is bounded, which seems necessary for the proposed stability property to guarantee a high revenue. However, this assumption seems very strong, and it would be interesting to design variants of stability that are suitable for more realistic noise models.
Minor comments: - Parenthesis on last line of Def. 3.1 would help with clarity.
(*) Huang, Liu, and Wang. Learning Optimal Reserve Price against Non-myopic Bidders, 2018.
——- Update: Thanks to the authors for their detailed responses. Although investigating the stability of a richer class of mechanisms would provide a more comprehensive picture, the author response has convinced me that their results on multi-item auctions are sufficiently important for acceptance. I have thus decided to raised my score.


Review Point: is the assumption that the adversarial noise is bounded, which seems necessary for the proposed stability property to guarantee a high revenue. However, this assumption seems very strong, and it would be interesting to design variants of stability that are suitable for more realistic noise models. Minor comments:
Review Point: - Parenthesis on last line of Def. 3.1 would help with clarity. (*) Huang, Liu, and Wang. Learning Optimal Reserve Price against Non-myopic Bidders, 2018. ——- Update: Thanks to the authors for their detailed responses. Although investigating the stability of a richer class of mechanisms would provide a more comprehensive picture, the author response has convinced me that their results on multi-item auctions are sufficiently important for acceptance. I have thus decided to raised my score.
==================================================

Focused review:

Weakness
The main innovation of the paper is adopting the proposed pipeline MDOD[1] to 2D human pose estimation, and specifically on the RKG part where such an issue is not as severe in object detection. The existence of MDOD does undermine the novelty of this work somewhat, but not significantly.
[1] Yoo, J., Lee, H., Chung, I., Seo, G., & Kwak, N. (2021). Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 3417-3426.


Review Point: The main innovation of the paper is adopting the proposed pipeline MDOD[1] to 2D human pose estimation, and specifically on the RKG part where such an issue is not as severe in object detection. The existence of MDOD does undermine the novelty of this work somewhat, but not significantly. [1] Yoo, J., Lee, H., Chung, I., Seo, G., & Kwak, N. (2021). Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image.
Review Point: 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 3417-3426.
==================================================

Focused review:

weaknesses: W1: There is no problem formulation. It's hard to know what is the computational problem studied in this paper. W2: It's not clear how do the proposed measures answer the raised problem. W3: There is a lack of discussion of the rationale of the proposed measures. W4: There is a lack of empirical validation for the proposed measure.
I have some concerns about this paper. There is no problem formulation. It's hard to know what is the computational problem studied in this paper. It seems that the goal of this paper is only to reveal and explain preferential attachment. It's not clear how do the proposed measures answer the raised problem. What is the intuition of the proposed measure in terms of analyzing structure evolution? There is a lack of discussion of the rationale of the proposed measures. There is a lack of empirical validation for the proposed measure.


Review Point: W1: There is no problem formulation. It's hard to know what is the computational problem studied in this paper.
Review Point: W2: It's not clear how do the proposed measures answer the raised problem.
Review Point: W3: There is a lack of discussion of the rationale of the proposed measures.
Review Point: W4: There is a lack of empirical validation for the proposed measure. I have some concerns about this paper. There is no problem formulation. It's hard to know what is the computational problem studied in this paper. It seems that the goal of this paper is only to reveal and explain preferential attachment. It's not clear how do the proposed measures answer the raised problem. What is the intuition of the proposed measure in terms of analyzing structure evolution? There is a lack of discussion of the rationale of the proposed measures. There is a lack of empirical validation for the proposed measure.
==================================================

Focused review:

Weaknesses:
Though I can understand that the large domain gap between base dataset and target domain is quite challenging to be tackled, but the assumption of having access to a portion of target domain data (even it is unlabeled) seems to be quite impractical and hard to envision its usage in real-world scenarios. Moreover, even there exists two prior works (i.e. STARTUP and Dynamic-Distillation) addressing the same problem setting as the proposed method, the authors should still provide sufficient motivations as well as the description on the potential usage of such problem setting in their paper to make the whole story self-contained and persuasive. (Besides, in this paper they set to have 20% of target domain data available during training, in which such amount is not "a small portion" in my perspective).
In addition to the teacher model trained on base dataset and kept fixed during knowledge distillation, the authors introduce another model, named as "old student" (which in implementation is the student model in the previous learning iteration), in order to keep the knowledge of historical students during the learning of student model. Though such design seems to provide benefit to the overall model performance, having such additional model actually increase the computational cost for the overall training procedure. Moreover, the proposed method adopts the linear combination of such old student model and the teacher model as the actual source of knowledge to execute the knowledge distillation, in which such old student model acts similarly to the moving average of historical students (i.e. an ensemble) if we expand the overall iterative procedure (thus being closely related to Dynamic-Distillation@NeuraIPS2021), hence the linear combination of the old student and the teacher becomes an "ensemble of ensemble" or analogously a straightforward integration over the distillation strategies of STARTUP and Dynamic-Distillation. Overall, such the distillation strategy of the proposed method seems to be incremental from the methodology perspective.
The cross-level distillation mechanism is closely related to the well-known BYOT@ICCV2019 work (where the shallow layers try to learn the knowledge from the deeper layers), in which the only difference exists on that BYOT is doing self-distillation while the proposed method applies the same idea across two networks. The cross-level distillation proposed in this paper thus seems to be incremental as well.
Though the experimental results show that the proposed method provide superior performance, there are some concerns as detailed below: 1) we see that the proposed method using BYOL as its self-supervised loss (denoted as BYOL+CLD+FD (ours) in Table.1) does not present significant performance improvement with respect to Dynamic-Distillation (especially the loss function in Dynamic-Distillation is closely related to BYOL); 2) the baseline SimCLR (self-supervised learning applied on unlabeled target domain data) already provides strong performance (e.g. better than BYOL+CLD+FD (ours) and even competitive with SimCLR+CLD+FD (ours) which also adopts SimCLR in its loss functions). From these two observations, the contribution of the proposed method becomes skeptical (in which the proposed method adopts many different designs into its framework but in results only achieves sightly better, comparable, or even worse performance with respect to the baselines).


Review Point: Though I can understand that the large domain gap between base dataset and target domain is quite challenging to be tackled, but the assumption of having access to a portion of target domain data (even it is unlabeled) seems to be quite impractical and hard to envision its usage in real-world scenarios. Moreover, even there exists two prior works (i.e. STARTUP and Dynamic-Distillation) addressing the same problem setting as the proposed method, the authors should still provide sufficient motivations as well as the description on the potential usage of such problem setting in their paper to make the whole story self-contained and persuasive. (Besides, in this paper they set to have 20% of target domain data available during training, in which such amount is not "a small portion" in my perspective). In addition to the teacher model trained on base dataset and kept fixed during knowledge distillation, the authors introduce another model, named as "old student" (which in implementation is the student model in the previous learning iteration), in order to keep the knowledge of historical students during the learning of student model. Though such design seems to provide benefit to the overall model performance, having such additional model actually increase the computational cost for the overall training procedure. Moreover, the proposed method adopts the linear combination of such old student model and the teacher model as the actual source of knowledge to execute the knowledge distillation, in which such old student model acts similarly to the moving average of historical students (i.e. an ensemble) if we expand the overall iterative procedure (thus being closely related to Dynamic-Distillation@NeuraIPS2021), hence the linear combination of the old student and the teacher becomes an "ensemble of ensemble" or analogously a straightforward integration over the distillation strategies of STARTUP and Dynamic-Distillation. Overall, such the distillation strategy of the proposed method seems to be incremental from the methodology perspective. The cross-level distillation mechanism is closely related to the well-known BYOT@ICCV2019 work (where the shallow layers try to learn the knowledge from the deeper layers), in which the only difference exists on that BYOT is doing self-distillation while the proposed method applies the same idea across two networks. The cross-level distillation proposed in this paper thus seems to be incremental as well. Though the experimental results show that the proposed method provide superior performance, there are some concerns as detailed below:
Review Point: 1) we see that the proposed method using BYOL as its self-supervised loss (denoted as BYOL+CLD+FD (ours) in Table.1) does not present significant performance improvement with respect to Dynamic-Distillation (especially the loss function in Dynamic-Distillation is closely related to BYOL);
Review Point: 2) the baseline SimCLR (self-supervised learning applied on unlabeled target domain data) already provides strong performance (e.g. better than BYOL+CLD+FD (ours) and even competitive with SimCLR+CLD+FD (ours) which also adopts SimCLR in its loss functions). From these two observations, the contribution of the proposed method becomes skeptical (in which the proposed method adopts many different designs into its framework but in results only achieves sightly better, comparable, or even worse performance with respect to the baselines).
==================================================

Focused review:

- The solution is only useful when a distance between data samples is computed - The Implementation is very weakly described and seems to be more related to a general description of ideas and programming principles. A detailed description of how solution is implement would have been highly appreciated. - The solution foresees that a specific binary is compiled for every new formula-reduction pair. How can this be implemented? Does this significantly affect the performance? - The improvements brought by the proposed solution are visibile only in a limited range of in the number of samples and dimension (hence not being general)

Review Point: - The solution is only useful when a distance between data samples is computed - The Implementation is very weakly described and seems to be more related to a general description of ideas and programming principles. A detailed description of how solution is implement would have been highly appreciated.
Review Point: - The solution foresees that a specific binary is compiled for every new formula-reduction pair. How can this be implemented? Does this significantly affect the performance?
Review Point: - The improvements brought by the proposed solution are visibile only in a limited range of in the number of samples and dimension (hence not being general)
==================================================

Focused review:

* The authors formulate the problem that the explanation of latent factors remain unclear in the context GGNs and VAEs. To overcome these limitations, they proposed DGVAE however it is unclear to me in what sense the Dirichlet approach leads to a better explanation compared to other clustering approaches. * It would be interesting to compare your approach to more recent state-of-the-art clustering approach instead of plain k-means such as [4] * The three proposed contributions seems to be only loosely connected. For example, in the experimental evaluation one gets the impression that only Heatts is required to obtain the good results (e.g. l. 214). Therefore, I sometimes got the impression that the other contributions are nice to have however not necessary for the whole approach. It would be nice if the authors could try embed their contributions better in their story such that they are also reflected in the experiments.

Review Point: * The authors formulate the problem that the explanation of latent factors remain unclear in the context GGNs and VAEs. To overcome these limitations, they proposed DGVAE however it is unclear to me in what sense the Dirichlet approach leads to a better explanation compared to other clustering approaches.
Review Point: * It would be interesting to compare your approach to more recent state-of-the-art clustering approach instead of plain k-means such as [4] * The three proposed contributions seems to be only loosely connected. For example, in the experimental evaluation one gets the impression that only Heatts is required to obtain the good results (e.g. l. 214). Therefore, I sometimes got the impression that the other contributions are nice to have however not necessary for the whole approach. It would be nice if the authors could try embed their contributions better in their story such that they are also reflected in the experiments.
==================================================

Focused review:

Weaknesses
W1. The idea of partitioning a scene and doing estimation within each part independently from others is not novel. In relation to INRs see, e.g., (Tretschk et al. ECCV 2020), (Tancik et al. CVPR 2022) and (Turki et al. CVPR 2022).
W2. Hypothesis 1 (namely that the complexity is exponential in the number of boundaries) is simply stated but no insight into its validity for natural images is provided. The rest of the theory merely shows that when hypothesis 1 holds, partitioning benefits efficiency.
References
Tretschk et al. PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations. ECCV 2020.
Tancik et al. Block-NeRF: Scalable Large Scene Neural View Synthesis. CVPR 2022.
Turki et al. Mega-NERF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs. CVPR 2022.


Review Point: W1. The idea of partitioning a scene and doing estimation within each part independently from others is not novel. In relation to INRs see, e.g., (Tretschk et al. ECCV 2020), (Tancik et al. CVPR 2022) and (Turki et al. CVPR 2022).
Review Point: W2. Hypothesis 1 (namely that the complexity is exponential in the number of boundaries) is simply stated but no insight into its validity for natural images is provided. The rest of the theory merely shows that when hypothesis 1 holds, partitioning benefits efficiency. References Tretschk et al. PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations. ECCV 2020. Tancik et al. Block-NeRF: Scalable Large Scene Neural View Synthesis. CVPR 2022. Turki et al. Mega-NERF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs. CVPR 2022.
==================================================

Focused review:

1. In Table 2, why gamma = 0.2 performs the worst and introduces huge variance? Should the performance change smoothly with gamma? 2. What is the definition of the complementary/orthogonal architecture? Should the union of alpha_{i-1} and alpha_i^c be the whole search space, or it just needs the union of alpha_{i-1} and alpha_i^c includes alpha_i? 3. Please consider redrawing Fig. 1 as the current version possess unnatural skewing. 4. It should be minus in Eqs. (2) and (3) for gradient descend.

Review Point: 1. In Table 2, why gamma = 0.2 performs the worst and introduces huge variance? Should the performance change smoothly with gamma?
Review Point: 2. What is the definition of the complementary/orthogonal architecture? Should the union of alpha_{i-1} and alpha_i^c be the whole search space, or it just needs the union of alpha_{i-1} and alpha_i^c includes alpha_i?
Review Point: 3. Please consider redrawing Fig. 1 as the current version possess unnatural skewing.
Review Point: 4. It should be minus in Eqs. (2) and (3) for gradient descend.
==================================================

Focused review:

- It is harsh to really call this a weakness, but: by the authors own admission (line 73) the optimality results for communication and local gradient calls do not hold simultaneously. So there is a gap, and still something for future work. - The results depend on Assumption 3.1, which other related works do not require.

Review Point: - It is harsh to really call this a weakness, but: by the authors own admission (line 73) the optimality results for communication and local gradient calls do not hold simultaneously. So there is a gap, and still something for future work.
Review Point: - The results depend on Assumption 3.1, which other related works do not require.
==================================================

Focused review:

Weaknesses: - It's unfortunate that Sokoban is the _only_ domain used for evaluation, and also unfortunate that there is only a single, domain-specific strategy used to generate training tasks (specifically, generating new Sokoban instances from predefined ones using a subset of boxes/locations). Thus it is possible that some of the proposed strategies do not work as well in other domains. For example, the interaction between GN and MIX in Table 1 is not something I would have predicted beforehand, and may be an artifact of the domain. Indeed, the motivation for MIX in Section 3.5 is relatively weak: this seems like a strategy that only makes sense with the particular baseline strategy that the paper used for generating a curriculum. The paper still makes a novel & interesting contribution, although perhaps are more accurate title would be "A Novel Automated Curriculum Strategy to Solve Hard Sokoban Instances". - There's no indication of inter-run variance in any of the results. Given how high the variance of RL algorithms can be, it would be good to include this in the camera-ready (or any future re-submissions), or at least provide some qualitative indication of how much results vary between runs. - The comparison in Section 4.2 is somewhat misleading: while it gives an accurate picture of the difficulty of solving these instances, it does not give an accurate impression of the capabilities of non-learning approaches. I'm not sure about Sokolution, but FF is definitely constrained to a single CPU core. In contrast, the evaluated method has access to five GPUs. Assuming 250W/GPU (GTX-1080Ti) and 12 hours of traiing, the proposed method gets 15kWh of compute, whereas if the baselines are run on one core of an eight-core CPU with 150W TDP (made-up numbers, but fairly typical), then they receive only 150/8*12=225Wh of compute, which is 1.5% of the learning solver. (I expect similar results with other metrics like hardware cost.) This disparity should be acknowledged somewhere in 4.2. - The only baseline is an analogue to the method of Feng et al. [11] (referred to as BL). While it seems unlikely that domain-specific heuristic search planners could be competitive in this setting, it would at least be nice to compare to Groshev et al.'s supervised leapfrogging strategy [13], and any other methods that belong in a similar class to this one.

Review Point: - It's unfortunate that Sokoban is the _only_ domain used for evaluation, and also unfortunate that there is only a single, domain-specific strategy used to generate training tasks (specifically, generating new Sokoban instances from predefined ones using a subset of boxes/locations). Thus it is possible that some of the proposed strategies do not work as well in other domains. For example, the interaction between GN and MIX in Table 1 is not something I would have predicted beforehand, and may be an artifact of the domain. Indeed, the motivation for MIX in Section 3.5 is relatively weak: this seems like a strategy that only makes sense with the particular baseline strategy that the paper used for generating a curriculum. The paper still makes a novel & interesting contribution, although perhaps are more accurate title would be "A Novel Automated Curriculum Strategy to Solve Hard Sokoban Instances".
Review Point: - There's no indication of inter-run variance in any of the results. Given how high the variance of RL algorithms can be, it would be good to include this in the camera-ready (or any future re-submissions), or at least provide some qualitative indication of how much results vary between runs.
Review Point: - The comparison in Section 4.2 is somewhat misleading: while it gives an accurate picture of the difficulty of solving these instances, it does not give an accurate impression of the capabilities of non-learning approaches. I'm not sure about Sokolution, but FF is definitely constrained to a single CPU core. In contrast, the evaluated method has access to five GPUs. Assuming 250W/GPU (GTX-1080Ti) and 12 hours of traiing, the proposed method gets 15kWh of compute, whereas if the baselines are run on one core of an eight-core CPU with 150W TDP (made-up numbers, but fairly typical), then they receive only 150/8*12=225Wh of compute, which is 1.5% of the learning solver. (I expect similar results with other metrics like hardware cost.) This disparity should be acknowledged somewhere in 4.2.
Review Point: - The only baseline is an analogue to the method of Feng et al. [11] (referred to as BL). While it seems unlikely that domain-specific heuristic search planners could be competitive in this setting, it would at least be nice to compare to Groshev et al.'s supervised leapfrogging strategy [13], and any other methods that belong in a similar class to this one.
==================================================

Focused review:

Weaknesses:
w1) In Fig.1(b), the figure of temporal cropping seems incorrect. According to the description of algorithm 1, the red box should be within the blue box. In the second line, the first image in the red box is outside the blue box.
w2) In Sec 3. “Preference-based reinforcement learning,” “Then, we model a preference predictor using the reward function
r
^
ψ
following the Bradley-Terry model.” It’s not clear to the reviewer how the reward function is learned. “Specifically, given a dataset of preferences D, the reward function is updated by minimizing the binary cross-entropy loss.” Does the reward function have extra constraints? Assume
ψ
is the reward function that satisfies the Eqn (1). It seems that there are infinite equivalent reward functions (
ψ
+ arbitrary constant number) if the length of segments (H) is the same.
w3) In Sec 4.1, the same question as the above question 2) w.r.t. the learning process of the reward function through the loss of Eqn(3). If multiple seeds are used, do these reward functions generate similar results on the samples?
w4) In Sec 4.2, “The intuition behind the augmentation is that for a given pair of behavior clips, the human teacher may keep their relative preferences for slightly shifted or resized versions of them.” The assumption is too strong and does not generally hold. It will actually strengthen the paper if more analyses are provided. What would happen in situations when the assumption does not hold?


Review Point: w1) In Fig.1(b), the figure of temporal cropping seems incorrect. According to the description of algorithm 1, the red box should be within the blue box. In the second line, the first image in the red box is outside the blue box.
Review Point: w2) In Sec 3. “Preference-based reinforcement learning,” “Then, we model a preference predictor using the reward function r ^ ψ following the Bradley-Terry model.” It’s not clear to the reviewer how the reward function is learned. “Specifically, given a dataset of preferences D, the reward function is updated by minimizing the binary cross-entropy loss.” Does the reward function have extra constraints? Assume ψ is the reward function that satisfies the Eqn (1). It seems that there are infinite equivalent reward functions ( ψ + arbitrary constant number) if the length of segments (H) is the same.
Review Point: w3) In Sec 4.1, the same question as the above question 2) w.r.t. the learning process of the reward function through the loss of Eqn(3). If multiple seeds are used, do these reward functions generate similar results on the samples?
Review Point: w4) In Sec 4.2, “The intuition behind the augmentation is that for a given pair of behavior clips, the human teacher may keep their relative preferences for slightly shifted or resized versions of them.” The assumption is too strong and does not generally hold. It will actually strengthen the paper if more analyses are provided. What would happen in situations when the assumption does not hold?
==================================================

Focused review:

weakness of the paper: it only provides one experimental evaluation. In particular, I would have liked to see a discussion of the solver for linear problems for which the implicit likelihood (6) turns into a non-central Gaussian. This would have been particularly interesting, as classical implicit methods are highly effective for stiff problems. It could have been investigated whether this holds for their probabilistic counterparts as well.  Secondly, I had trouble following the description of Sect. 2.4. In particular: did I understand correctly, that you do _not_ sample from (6) in practice, but from the non-centered Gaussian described in Sect. 2.4? How does this not introduce a bias into the method? As a side note: if I determine a valid Lipschitz constant for f, I should be able to generate a rejection sampler zeta_c,h, correct?  I think this work could be an important piece in the puzzle of covering all variations of classical numerical integrators. As the authors rightly point out: many previous publications on the subject have thoroughly, but narrowly, explored the space of explicit step-by-step methods. I have the impression that the path followed by the authors here could lead to a whole new class of probabilistic numerical integrators that might have properties orthogonal to the existing class of solvers. Thus, I strongly recommend this work for acceptance. As some open questions, in particular the analysis for linear and stiff problems, remain completely unanswered, I refrain from giving a higher score.  Minor points for improvement: - In Sect. 1.1, the authors could consider also discussing https://arxiv.org/abs/1803.04303 - The last sentence of Sect. 1.2 mentions the connection to predictor-corrector methods. Schober et al. (2018) point out that their model corresponds to an implicit method, although they only evaluate it in PEC fashion. This connection could have been discussed in your work. - In Sect. 2.3, the authors could consider refering to https://arxiv.org/abs/1702.03673 - In the discussion of the scaling matrix H below Eq. (10), the authors could discuss the resemblance of their idea to the Milne device of predictor-corrector methods - I have a mixed impression of the comments on the "de-biasing effect" (starting on line 331). I appreciate that they highlight the observation and I agree with their rationale on the method by Conrad et al., but I think it's a bit too early to attribute a de-biasing effect to the novel method based on one experiment. In this respect, the authors might also want to discuss the connection to probabilistic symplectic integrators of https://arxiv.org/abs/1801.01340  Some stylistic feedback which is orthogonal to the scientific discussion. a) the authors often apply dashes in their punctuation. I suggest using an em-dash and dropping the neighboring white-space like---so. https://en.wikipedia.org/wiki/Dash#Em_dash b) Figure 2: the authors could mark the true value of theta_3 = 3.0 in the figure and also put it in the caption.  --- Post-rebuttal update: I have read the rebuttal and agree with the authors' points. My overall (already good) impression from the paper was unfortunately not further improved, but I appreciate the clarifications.

Review Point: of the paper: it only provides one experimental evaluation. In particular, I would have liked to see a discussion of the solver for linear problems for which the implicit likelihood (6) turns into a non-central Gaussian. This would have been particularly interesting, as classical implicit methods are highly effective for stiff problems. It could have been investigated whether this holds for their probabilistic counterparts as well. Secondly, I had trouble following the description of Sect. 2.4. In particular: did I understand correctly, that you do _not_ sample from (6) in practice, but from the non-centered Gaussian described in Sect. 2.4? How does this not introduce a bias into the method? As a side note: if I determine a valid Lipschitz constant for f, I should be able to generate a rejection sampler zeta_c,h, correct? I think this work could be an important piece in the puzzle of covering all variations of classical numerical integrators. As the authors rightly point out: many previous publications on the subject have thoroughly, but narrowly, explored the space of explicit step-by-step methods. I have the impression that the path followed by the authors here could lead to a whole new class of probabilistic numerical integrators that might have properties orthogonal to the existing class of solvers. Thus, I strongly recommend this work for acceptance. As some open questions, in particular the analysis for linear and stiff problems, remain completely unanswered, I refrain from giving a higher score. Minor points for improvement:
Review Point: - In Sect. 1.1, the authors could consider also discussing https://arxiv.org/abs/1803.04303 - The last sentence of Sect. 1.2 mentions the connection to predictor-corrector methods. Schober et al. (2018) point out that their model corresponds to an implicit method, although they only evaluate it in PEC fashion. This connection could have been discussed in your work.
Review Point: - In Sect. 2.3, the authors could consider refering to https://arxiv.org/abs/1702.03673 - In the discussion of the scaling matrix H below Eq. (10), the authors could discuss the resemblance of their idea to the Milne device of predictor-corrector methods - I have a mixed impression of the comments on the "de-biasing effect" (starting on line 331). I appreciate that they highlight the observation and I agree with their rationale on the method by Conrad et al., but I think it's a bit too early to attribute a de-biasing effect to the novel method based on one experiment. In this respect, the authors might also want to discuss the connection to probabilistic symplectic integrators of https://arxiv.org/abs/1801.01340 Some stylistic feedback which is orthogonal to the scientific discussion. a) the authors often apply dashes in their punctuation. I suggest using an em-dash and dropping the neighboring white-space like---so. https://en.wikipedia.org/wiki/Dash#Em_dash b) Figure 2: the authors could mark the true value of theta_3 = 3.0 in the figure and also put it in the caption. --- Post-rebuttal update: I have read the rebuttal and agree with the authors' points. My overall (already good) impression from the paper was unfortunately not further improved, but I appreciate the clarifications.
==================================================

Focused review:

(1) The experimental results did not quite convince me that their proposed approach outperforms prior work. The authors claim in the abstract that PAM is a current state-of-the-art technique, but it is from 1990 and there have been several improvements since then (in fact, all of the algorithms that the authors compared to except for FastPAM1 are at least a decade old). Specifically, the authors compared to FastPAM1 in [1], but in most of the experiments in [1], FastPAM1 was outperformed by FastPAM2, FastCLARA, and FastCLARANS. Also, how does this algorithm compare to [2], which won a best paper award at AISTATS 2017 and runs in O(n^{3/2}) time? (2) (minor) It seems the runtime of their algorithm is actually O(nk log n), and the way the title and intro are written seem like false advertising, because most clustering research does not treat k as a constant. The authors do clarify this later in the intro, when they say that they treat k as a constant for the remainder of the paper. (3) (minor) Can theorems 1 and 2 be proven for an arbitrary delta < 1/n^2? And then the probability of success and the #_distance_computations would be in terms of delta. Small comments: - The phrase “theoretically prove” is redundant (in the abstract and intro) - Eq. 4, line 133, and again in other places. Shouldn’t the carat be min(., .) ? - Pick a variable to represent “batchsize” in theorem 2? Even though I would like to see these weaknesses addressed, I think the pros marginally outweigh the cons. [1] Erich Schubert, Peter Rousseeuw. Faster k-medoids clustering: improving the PAM, CLARA, and CLARANS Algorithms. [2] James Newling, Francois Fleuret. A Sub-Quadratic Exact Medoid Algorithm. 2017.

Review Point: (1) The experimental results did not quite convince me that their proposed approach outperforms prior work. The authors claim in the abstract that PAM is a current state-of-the-art technique, but it is from 1990 and there have been several improvements since then (in fact, all of the algorithms that the authors compared to except for FastPAM1 are at least a decade old). Specifically, the authors compared to FastPAM1 in [1], but in most of the experiments in [1], FastPAM1 was outperformed by FastPAM2, FastCLARA, and FastCLARANS. Also, how does this algorithm compare to [2], which won a best paper award at AISTATS 2017 and runs in O(n^{3/2}) time? (2) (minor) It seems the runtime of their algorithm is actually O(nk log n), and the way the title and intro are written seem like false advertising, because most clustering research does not treat k as a constant. The authors do clarify this later in the intro, when they say that they treat k as a constant for the remainder of the paper. (3) (minor) Can theorems 1 and 2 be proven for an arbitrary delta < 1/n^2? And then the probability of success and the #_distance_computations would be in terms of delta. Small comments:
Review Point: - The phrase “theoretically prove” is redundant (in the abstract and intro) - Eq. 4, line 133, and again in other places. Shouldn’t the carat be min(., .) ?
Review Point: - Pick a variable to represent “batchsize” in theorem 2? Even though I would like to see these weaknesses addressed, I think the pros marginally outweigh the cons. [1] Erich Schubert, Peter Rousseeuw. Faster k-medoids clustering: improving the PAM, CLARA, and CLARANS Algorithms. [2] James Newling, Francois Fleuret. A Sub-Quadratic Exact Medoid Algorithm. 2017.
==================================================

Focused review:

Weaknesses:
• There should be a clearer explanation of the distinction between ChebBase and ChebNet.
• It would be interesting to showcase some of the filters that ChebNetII has learned. What distinguishes them from the ones learned by BernNet?
Minor points:
• The best results should also be highlighted in Table 3.


Review Point: • There should be a clearer explanation of the distinction between ChebBase and ChebNet.
Review Point: • It would be interesting to showcase some of the filters that ChebNetII has learned. What distinguishes them from the ones learned by BernNet? Minor points:
Review Point: • The best results should also be highlighted in Table 3.
==================================================

Focused review:

- The writing of this paper is very misleading. First of all, it claims that it can be trained only using a single viewpoint of the object. In fact, all previous diffrentiable rendering techniques can be trained using a single view of object at training time. However, the reason why multi-view images are used for training in prior works is that single-view images usually lead to ambiguity in the depth direction. The proposed method also suffers from this problem -- it cannot resolve the ambiguity of depth using a single image either. The distrance-transformed silhouette can only provide information on the xy plane - the shape perpendicular to the viewing direction. - I doubt the proposed method can be trained without using any camera information (Line 223, the so called "knowledge of CAD model correspondences"). Without knowing the viewpoint, how is it possible to perform ray marching? How do you know where the ray comes from? - The experiments are not comprehensive and convincing. 1) The comparisons do not seem fair. The performance of DVR is far worse than that in the original DVR paper. Is DVR trained and tested on the same data? What is the code used for evaluation? Is it from the original authors or reimplementation? 2) Though it could be interesting to see how SoftRas performs, it is not very fair to compare SoftRas here as it use a different 3D representation -- mesh. It is well known that mesh representation cannot model arbitrary topology. Thus it is not surprising to see it is outperformed. Since this paper works on implicit surface, it would be more interesting to compare with more state-of-the-art differentiable renderers for implicit surface, i.e. [26],[27],[14], or at least the baseline approach [38]. However, no direct comparisons with these approaches are provided, making it difficult to verify the effectivenss of the proposed approach.

Review Point: - The writing of this paper is very misleading. First of all, it claims that it can be trained only using a single viewpoint of the object. In fact, all previous diffrentiable rendering techniques can be trained using a single view of object at training time. However, the reason why multi-view images are used for training in prior works is that single-view images usually lead to ambiguity in the depth direction. The proposed method also suffers from this problem -- it cannot resolve the ambiguity of depth using a single image either. The distrance-transformed silhouette can only provide information on the xy plane - the shape perpendicular to the viewing direction.
Review Point: - I doubt the proposed method can be trained without using any camera information (Line 223, the so called "knowledge of CAD model correspondences"). Without knowing the viewpoint, how is it possible to perform ray marching? How do you know where the ray comes from?
Review Point: 1) The comparisons do not seem fair. The performance of DVR is far worse than that in the original DVR paper. Is DVR trained and tested on the same data? What is the code used for evaluation? Is it from the original authors or reimplementation?
Review Point: 2) Though it could be interesting to see how SoftRas performs, it is not very fair to compare SoftRas here as it use a different 3D representation -- mesh. It is well known that mesh representation cannot model arbitrary topology. Thus it is not surprising to see it is outperformed. Since this paper works on implicit surface, it would be more interesting to compare with more state-of-the-art differentiable renderers for implicit surface, i.e. [26],[27],[14], or at least the baseline approach [38]. However, no direct comparisons with these approaches are provided, making it difficult to verify the effectivenss of the proposed approach.
==================================================

Focused review:

L107: It would be nice to introduce the notation of
λ
,
μ
,
ν
in Eq 1 for completeness.
L118:
h
ϕ
l
is called a 'local projector' whereas it is the equivalent of the VICREG expander but at the scale of the feature map. Since 120 uses the terminology 'global expander', it would seem more consistent to use the word 'local expander' rather than projector. This would also be more consistent with VICREG on which the paper relies on.
L127, L135, 181: 'only the top-
γ
pairs are kept'. It is not clear whether for each feature position, only the top-
γ
are kept to derive the loss, or if the top-
γ
among all pairs in the batch are kept (assuming top-
γ
means the pairs with the smallest distance). This would help the reader if the definition were introduced L127.
L127, L135, 181: Is there a motivation / intuition on why it is better to keep only the top-
γ
pairs? Together with the l2-distance loss, this could reinforce the intuition that the l2-distance loss act as a regularizer/booster as it would be applied only to feature vectors that are already close to each other, which one can assume to be ground-truth positives.
L306: a feature maps -> a feature map


Review Point: L107: It would be nice to introduce the notation of λ , μ , ν in Eq 1 for completeness.
Review Point: L118: h ϕ l is called a 'local projector' whereas it is the equivalent of the VICREG expander but at the scale of the feature map. Since 120 uses the terminology 'global expander', it would seem more consistent to use the word 'local expander' rather than projector. This would also be more consistent with VICREG on which the paper relies on. L127, L135, 181: 'only the top- γ pairs are kept'. It is not clear whether for each feature position, only the top- γ are kept to derive the loss, or if the top- γ among all pairs in the batch are kept (assuming top- γ means the pairs with the smallest distance). This would help the reader if the definition were introduced L127. L127, L135, 181: Is there a motivation / intuition on why it is better to keep only the top- γ pairs? Together with the l2-distance loss, this could reinforce the intuition that the l2-distance loss act as a regularizer/booster as it would be applied only to feature vectors that are already close to each other, which one can assume to be ground-truth positives.
==================================================

Focused review:

Weaknesses:  - The paper is about is-a relation extraction but the majority of literature about taxonomization is not referenced in the paper, inter alia: Flati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto. 
2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.
Soren Auer, Christian Bizer, Georgi Kobilarov, Jens ¨ Lehmann, Richard Cyganiak, and Zachary Ive. 
2007. DBpedia: A nucleus for a web of open data.
Gerard de Melo and Gerhard Weikum. 2010. MENTA: Inducing Multilingual Taxonomies from Wikipedia.
Zornitsa Kozareva and Eduard H. Hovy. 2010. A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web.  Vivi Nastase, Michael Strube, Benjamin Boerschinger, Caecilia Zirn, and Anas Elghafari. 2010. WikiNet: A Very Large Scale Multi-Lingual Concept Network.
Simone Paolo Ponzetto and Michael Strube. 2007. 
Deriving a large scale taxonomy from Wikipedia.
Simone Paolo Ponzetto and Michael Strube. 2011. 
Taxonomy induction based on a collaboratively built knowledge repository.  Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from Wikipedia and WordNet.  Paola Velardi, Stefano Faralli, and Roberto Navigli. 
2013. OntoLearn Reloaded: A graph-based algorithm for taxonomy induction.   - Experiments are poor, they only compare against "Hearst patterns" without taking into account the works previously cited.
- General Discussion:  The paper is easy to follow and the supplementary material is also well written and useful, however the paper lack of references of is a relation extraction and taxonomization literature. The same apply for the experiments. 
In fact no meaningful comparison is performed and the authors not even take into account the existence of other systems (more recent than hearst patterns).
I read authors answers but still i'm not convinced that they couldn't perform more evaluations. I understand that they have a solid theoretical motivation but still, i think that comparison are very important to asses if the theoretical intuitions of the authors are confirmed also in practice. While it's true that all the works i suggested as comparison build taxonomies, is also true that a comparison is possible considering the edges of a taxonomy.
Anyway, considering the detailed author answer and the discussion with the other reviewer i can rise my score to 3 even if i still think that this paper is poor of experiments and does not frame correctly in the is-a relation extraction / taxonomy building literature. 

Review Point: - The paper is about is-a relation extraction but the majority of literature about taxonomization is not referenced in the paper, inter alia: Flati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto. 2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project. Soren Auer, Christian Bizer, Georgi Kobilarov, Jens ¨ Lehmann, Richard Cyganiak, and Zachary Ive. 2007. DBpedia: A nucleus for a web of open data. Gerard de Melo and Gerhard Weikum. 2010. MENTA: Inducing Multilingual Taxonomies from Wikipedia. Zornitsa Kozareva and Eduard H. Hovy. 2010. A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web. Vivi Nastase, Michael Strube, Benjamin Boerschinger, Caecilia Zirn, and Anas Elghafari. 2010. WikiNet: A Very Large Scale Multi-Lingual Concept Network. Simone Paolo Ponzetto and Michael Strube. 2007. Deriving a large scale taxonomy from Wikipedia. Simone Paolo Ponzetto and Michael Strube. 2011. Taxonomy induction based on a collaboratively built knowledge repository. Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from Wikipedia and WordNet. Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013. OntoLearn Reloaded: A graph-based algorithm for taxonomy induction.
Review Point: - Experiments are poor, they only compare against "Hearst patterns" without taking into account the works previously cited.
Review Point: - General Discussion: The paper is easy to follow and the supplementary material is also well written and useful, however the paper lack of references of is a relation extraction and taxonomization literature. The same apply for the experiments. In fact no meaningful comparison is performed and the authors not even take into account the existence of other systems (more recent than hearst patterns). I read authors answers but still i'm not convinced that they couldn't perform more evaluations. I understand that they have a solid theoretical motivation but still, i think that comparison are very important to asses if the theoretical intuitions of the authors are confirmed also in practice. While it's true that all the works i suggested as comparison build taxonomies, is also true that a comparison is possible considering the edges of a taxonomy. Anyway, considering the detailed author answer and the discussion with the other reviewer i can rise my score to 3 even if i still think that this paper is poor of experiments and does not frame correctly in the is-a relation extraction / taxonomy building literature.
==================================================

Focused review:

The contributions unique to this paper appear to be mostly incremental. In the introduction, the authors describe their algorithm as a generalization of one previously presented and that their main contribution is to prove non-asymptotic convergence rates for that algorithm, doing so in section 4.4, in which they find that covergence in Wasserstein space is similar to that of proximal gradient methods in Hilbert space. 2 The experiments are mostly trivial and are barely analyzed. The authors provide the code to generate all experiments and visualizations in their supplementary material (and it works!), but as in the paper, there's minimal documentation supporting the experiments. As it exists now, there's very little added value in the experiments section. 3 There appears to be very limited direct application for this work in practice, due to the massive computational complexity of these measures.

Review Point: The contributions unique to this paper appear to be mostly incremental. In the introduction, the authors describe their algorithm as a generalization of one previously presented and that their main contribution is to prove non-asymptotic convergence rates for that algorithm, doing so in section 4.4, in which they find that covergence in Wasserstein space is similar to that of proximal gradient methods in Hilbert space.
Review Point: 2 The experiments are mostly trivial and are barely analyzed. The authors provide the code to generate all experiments and visualizations in their supplementary material (and it works!), but as in the paper, there's minimal documentation supporting the experiments. As it exists now, there's very little added value in the experiments section. 3 There appears to be very limited direct application for this work in practice, due to the massive computational complexity of these measures.
==================================================

Focused review:

Weaknesses: There are some issues that need to be improved: 1) The introduction and related work section lack a detailed discussion of the recent method CoCoOp. 2) A comparison with the SOTA method CoCoOp is lacking in the experimental section. For example, in Tab1, Tab2, Tab3, Tab4 and Fig2.


Review Point: 1) The introduction and related work section lack a detailed discussion of the recent method CoCoOp.
Review Point: 2) A comparison with the SOTA method CoCoOp is lacking in the experimental section. For example, in Tab1, Tab2, Tab3, Tab4 and Fig2.
==================================================

Focused review:

1. The motivation of AE-KD is to encourage the optimization direction of the student guided equally by all the teachers. However, considering there are some weak teachers (low generalization accuracy) in the ensemble teacher pool, why are these weak teachers treated equally with other strong teachers in the gradient space? Intuitively, the guidance of student should favor those strong teachers, but keep away from the weak teachers. 2. The proposed AE-KD seems similar with the previous work OKDDip [1], where the dynamic attentions are learned by gradients to adaptively weight the teachers’ logits. What is the difference between them? 3. How to optimize the weights \alpha_m in Eq. (11)? Is it end-to-end optimized together with the student? If yes, how to ensure \alpha_m less than C during optimization? 4. The teacher resnet56 is trained for 240 epochs. Why is the student resnet20 trained for 350 epochs? 5. In ensemble learning, it is valuable that the ensemble networks have various architectures with different accuracies. How does AE-KD perform in this situation? [1] Online Knowledge Distillation with Diverse Peers. AAAI, 2020.

Review Point: 1. The motivation of AE-KD is to encourage the optimization direction of the student guided equally by all the teachers. However, considering there are some weak teachers (low generalization accuracy) in the ensemble teacher pool, why are these weak teachers treated equally with other strong teachers in the gradient space? Intuitively, the guidance of student should favor those strong teachers, but keep away from the weak teachers.
Review Point: 2. The proposed AE-KD seems similar with the previous work OKDDip [1], where the dynamic attentions are learned by gradients to adaptively weight the teachers’ logits. What is the difference between them?
Review Point: 3. How to optimize the weights \alpha_m in Eq. (11)? Is it end-to-end optimized together with the student? If yes, how to ensure \alpha_m less than C during optimization?
Review Point: 4. The teacher resnet56 is trained for 240 epochs. Why is the student resnet20 trained for 350 epochs?
Review Point: 5. In ensemble learning, it is valuable that the ensemble networks have various architectures with different accuracies. How does AE-KD perform in this situation? [1] Online Knowledge Distillation with Diverse Peers. AAAI, 2020.
==================================================

Focused review:

Weaknesses/Questions: 1. The proposed method is memory intensive. Is this the reason that only up to 7 consecutive frames are used in the experimental setting section? 2. If the foreground object only appears in part of the video(we don’t have supervisory signal so we don’t have information on this), would the method still work? What would the alpha value for foreground be in those frames where foreground doesn’t exist? 3. Since optical flow is the training signal, how would the OF quality/failure impact the result? It might be good to evaluate on a synthetic dataset and compare the training with both ground truth OF and estimated/flawed OF.


Review Point: 1. The proposed method is memory intensive. Is this the reason that only up to 7 consecutive frames are used in the experimental setting section?
Review Point: 2. If the foreground object only appears in part of the video(we don’t have supervisory signal so we don’t have information on this), would the method still work? What would the alpha value for foreground be in those frames where foreground doesn’t exist?
Review Point: 3. Since optical flow is the training signal, how would the OF quality/failure impact the result? It might be good to evaluate on a synthetic dataset and compare the training with both ground truth OF and estimated/flawed OF.
==================================================

Focused review:

- The theoretical foundation builds off a number of assumptions, specifically assumption 5, that may limit its applicability in real world scenarios. - The work claims the attack is limited by the adversaries limited access to the attack set (the vulnerable nodes of the graph). But there is no discussion on how this attack set is generated, its properties, or its effect on the proposed methodology. - The three baseline attacks used in the experimental results are not sufficiently introduced. - Further, it is not clear why the chosen baselines are more sufficient comparisons than recent works like [1] and [3]. - While the results indicate that in many cases the methodology decreases the performance of the model, in many of the scenarios there is only a minor improvement over the baseline comparisons. - Lambda, the magnitude of perturbation on each node, is used as a measure of the strength of the attack, but intuitively it seems like J, the number of node perturbed, would be a more accurate judge of the attack strength.

Review Point: - The theoretical foundation builds off a number of assumptions, specifically assumption 5, that may limit its applicability in real world scenarios.
Review Point: - The work claims the attack is limited by the adversaries limited access to the attack set (the vulnerable nodes of the graph). But there is no discussion on how this attack set is generated, its properties, or its effect on the proposed methodology.
Review Point: - The three baseline attacks used in the experimental results are not sufficiently introduced.
Review Point: - Further, it is not clear why the chosen baselines are more sufficient comparisons than recent works like [1] and [3].
Review Point: - While the results indicate that in many cases the methodology decreases the performance of the model, in many of the scenarios there is only a minor improvement over the baseline comparisons.
Review Point: - Lambda, the magnitude of perturbation on each node, is used as a measure of the strength of the attack, but intuitively it seems like J, the number of node perturbed, would be a more accurate judge of the attack strength.
==================================================

Focused review:

weaknesses spread throughout the paper
W1) Overall setup and achieved performance. I am very surprised by the rate-distortion performance, and I cannot see any explanation for the low rates we achieve. After all, the encoder transform "WZ Encoder Net" does not see anything except the current frame. By design, the only way to leverage temporal redundancies is on the decoder side via the side information loop. However, this means that the encoder never knows what the decoder will have access to. I thus do not see any way it could in general learn to reduce bitrate by any significant margin.
How could the encoder know which information is novel and needs to be transmitted?
When looking at Fig 1, top row, we see the representation \hat y contains a lot of information about the image. How could the rate of the model then be lower than just an image compression model? You also mention you compare to Minnen et al's I-frame model. Are you fine-tuning them for your task? Otherwise, the "Minnen2022" Results in Fig. 6 seem really off. FWIW, an image compression model will have about 3x the rate shown. Please enlighten me how the proposed scheme could achieve these results, without a temporal entropy model.
W2) Speed Results. While the results in Table 2 are promising, various important comparisons are missing. First and foremost I am missing "ELF-VC" (Rippel et al, ICCV 2021). Despite using flow on the encoder side, they achieve 10 FPS encoding and 18 FPS decoding on a Titan V, while reporting 44.3% BD-rate reduction over H.264 on UVG. In contrast, the proposed method achieves only either 32% BD-rate reduction at about 2.9FPS encoding, or 24.3% BD-rate reduction at 5.5 FPS, thus being both slower and worse in R-D. (It's unfortunately unclear what the authors used to measure GPU speed, possibly the NVIDIA RTX A5000 they used for training, which is a better GPU than the Titan V). Further note that the proposed methods uses future frames for frame interpolation, whereas ELF-VC is only operating in the P-frame mode, which means the ELF-VC numbers are even more impressive.
W3) (minor) H.264, H.265 configs: The authors use ffmpeg using both veryfast and -tune zerolatency as well as various additional parameters (C.1), which puts the codecs at a disadvantage.


Review Point: spread throughout the paper W1) Overall setup and achieved performance. I am very surprised by the rate-distortion performance, and I cannot see any explanation for the low rates we achieve. After all, the encoder transform "WZ Encoder Net" does not see anything except the current frame. By design, the only way to leverage temporal redundancies is on the decoder side via the side information loop. However, this means that the encoder never knows what the decoder will have access to. I thus do not see any way it could in general learn to reduce bitrate by any significant margin. How could the encoder know which information is novel and needs to be transmitted? When looking at Fig 1, top row, we see the representation \hat y contains a lot of information about the image. How could the rate of the model then be lower than just an image compression model? You also mention you compare to Minnen et al's I-frame model. Are you fine-tuning them for your task? Otherwise, the "Minnen2022" Results in Fig. 6 seem really off. FWIW, an image compression model will have about 3x the rate shown. Please enlighten me how the proposed scheme could achieve these results, without a temporal entropy model.
Review Point: W2) Speed Results. While the results in Table 2 are promising, various important comparisons are missing. First and foremost I am missing "ELF-VC" (Rippel et al, ICCV 2021). Despite using flow on the encoder side, they achieve 10 FPS encoding and 18 FPS decoding on a Titan V, while reporting 44.3% BD-rate reduction over H.264 on UVG. In contrast, the proposed method achieves only either 32% BD-rate reduction at about 2.9FPS encoding, or 24.3% BD-rate reduction at 5.5 FPS, thus being both slower and worse in R-D. (It's unfortunately unclear what the authors used to measure GPU speed, possibly the NVIDIA RTX A5000 they used for training, which is a better GPU than the Titan V). Further note that the proposed methods uses future frames for frame interpolation, whereas ELF-VC is only operating in the P-frame mode, which means the ELF-VC numbers are even more impressive.
Review Point: W3) (minor) H.264, H.265 configs: The authors use ffmpeg using both veryfast and -tune zerolatency as well as various additional parameters (C.1), which puts the codecs at a disadvantage.
==================================================

Focused review:

- It is unclear how well the regularized objectives approximate the true objectives. - The experiment of section 5.2 is unclear from the text, as most details are left to supplementary material. Based on the text itself, I don't know how to interpret this table. - The marginal constraints in the problem will generally be violated. - No quantitative rates of convergence for the regularized problems are given.

Review Point: - It is unclear how well the regularized objectives approximate the true objectives.
Review Point: - The experiment of section 5.2 is unclear from the text, as most details are left to supplementary material. Based on the text itself, I don't know how to interpret this table.
Review Point: - The marginal constraints in the problem will generally be violated.
Review Point: - No quantitative rates of convergence for the regularized problems are given.
==================================================

Focused review:

Weaknesses:   No significant theoretical contribution.   Possibly in response, the manuscript is a little vague in its positioning relative to prior work. While relevant prior work is cited, the reader is left with some ambiguity and, if not familiar with this prior work, might be misled to think that there is methodological innovation beyond the specifics of architecture and application.   Based on its solid and nontrivial experimental contribution I advocate acceptance; but the manuscript would profit from a clearer enunciation of the fact that / what prior work is being built on.   Comments:   50: "treat pixels independently"; this is unclear (or faulty?), the quoted papers also use an encoder/decoder structure  As a consequence, contribution 1 (line 74) is dubious or needs clarification.   Contribution (2) is fine.   Contribution (3): The statement is true, but the contribution is unclear. If the claim refers to the fact that latent z is concatenated only at the fully connected layers, then this has been done before (e.g. in DISCO Nets by Bouchacourt et al., (NIPS 2016)).  Contribution (4): The claim is vague. If we take generative models in their generality, then it encompasses e.g. most of Bayesian Statistics and the claim of only qualitative evaluation is obviously wrong. If we only consider generative models in the deep learning world, the statement is correct insofar as many papers only contain qualitative evaluation of "My pictures are prettier than yours"; but there are nevertheless quantitative metrics, such as the inception score, or the metrics used in the Wasserstein AE.   Section 2:  It is not made very obvious to the reader which part of this VAE structure is novel and which parts are not. The paper does follow [4] and especially [5] closely (the latter should also be cited in line 82). So the only really novel part here is the U-net structure of P(y|z,x). Concatenating z after the U-net in (2) is new in this formulation, but not in general (e.g. as already mentioned by DISCO Nets (Bouchacourt at al., NIPS 2016)).   Finally, there is no justification for the appearance of $\beta$ in (4), but it is up to the parameter name identical to what Higgins et al., (ICLR 2017) do with their \beta-VAE. Especially since authors choose $\beta >= 1$, which follows the Higgins et al. disentangling argument, and not the usual $\beta_t <= 1$, in which case it would be a time dependent downscaling of the KL term to avoid too much regularization in the beginning of the training (but then again the references to earlier work would be missing).   

Review Point: No significant theoretical contribution. Possibly in response, the manuscript is a little vague in its positioning relative to prior work. While relevant prior work is cited, the reader is left with some ambiguity and, if not familiar with this prior work, might be misled to think that there is methodological innovation beyond the specifics of architecture and application. Based on its solid and nontrivial experimental contribution I advocate acceptance; but the manuscript would profit from a clearer enunciation of the fact that / what prior work is being built on. Comments:
Review Point: 50: "treat pixels independently"; this is unclear (or faulty?), the quoted papers also use an encoder/decoder structure As a consequence, contribution 1 (line 74) is dubious or needs clarification. Contribution (2) is fine. Contribution (3): The statement is true, but the contribution is unclear. If the claim refers to the fact that latent z is concatenated only at the fully connected layers, then this has been done before (e.g. in DISCO Nets by Bouchacourt et al., (NIPS 2016)). Contribution (4): The claim is vague. If we take generative models in their generality, then it encompasses e.g. most of Bayesian Statistics and the claim of only qualitative evaluation is obviously wrong. If we only consider generative models in the deep learning world, the statement is correct insofar as many papers only contain qualitative evaluation of "My pictures are prettier than yours"; but there are nevertheless quantitative metrics, such as the inception score, or the metrics used in the Wasserstein AE. Section 2: It is not made very obvious to the reader which part of this VAE structure is novel and which parts are not. The paper does follow [4] and especially [5] closely (the latter should also be cited in line 82). So the only really novel part here is the U-net structure of P(y|z,x). Concatenating z after the U-net in (2) is new in this formulation, but not in general (e.g. as already mentioned by DISCO Nets (Bouchacourt at al., NIPS 2016)). Finally, there is no justification for the appearance of $\beta$ in (4), but it is up to the parameter name identical to what Higgins et al., (ICLR 2017) do with their \beta-VAE. Especially since authors choose $\beta >= 1$, which follows the Higgins et al. disentangling argument, and not the usual $\beta_t <= 1$, in which case it would be a time dependent downscaling of the KL term to avoid too much regularization in the beginning of the training (but then again the references to earlier work would be missing).
==================================================

Focused review:

- The setting is restrictive, as it requires a setting where the barycenter support must be pre-specified. In general, the ideas for solving the fixed support problem do not seem to generalize to the general Wasserstein barycenter problem. - The algorithm they propose does not achieve the best complexity bound in all regimes. There seems to be a tradeoff between approximation factor and size of the empirical distributions, since it is obviously faster than IBP (it is an accelerated version), but it does not achieve a better dependence on approximation factor when compared with accelerated IBP and APDAGD. However, it does have a better dependence on the size of the empirical distributions and barycenter, n, than these two methods. This tradeoff is not explored in the paper. Not hints as to interesting theory from the analysis of FastIBP are given. -The explanation of the algorithm is short, and instead of giving heuristic ideas they instead just give the steps of the algorithm. Can anything be learned from this method or is it actually just taking some past work and applying it to this problem? Why is it able to achieve the better dependencies over other methods? - The authors don't compare with some mentioned methods, accelerated IBP and APDAGD, in the experiments. - It is unclear whether the assumption that all measures are supported on the same number of points necessary for these results - is the extension easy based on these results? Does the algorithmic complexity result extend?

Review Point: - The setting is restrictive, as it requires a setting where the barycenter support must be pre-specified. In general, the ideas for solving the fixed support problem do not seem to generalize to the general Wasserstein barycenter problem.
Review Point: - The algorithm they propose does not achieve the best complexity bound in all regimes. There seems to be a tradeoff between approximation factor and size of the empirical distributions, since it is obviously faster than IBP (it is an accelerated version), but it does not achieve a better dependence on approximation factor when compared with accelerated IBP and APDAGD. However, it does have a better dependence on the size of the empirical distributions and barycenter, n, than these two methods. This tradeoff is not explored in the paper. Not hints as to interesting theory from the analysis of FastIBP are given. -The explanation of the algorithm is short, and instead of giving heuristic ideas they instead just give the steps of the algorithm. Can anything be learned from this method or is it actually just taking some past work and applying it to this problem? Why is it able to achieve the better dependencies over other methods?
Review Point: - The authors don't compare with some mentioned methods, accelerated IBP and APDAGD, in the experiments.
Review Point: - It is unclear whether the assumption that all measures are supported on the same number of points necessary for these results - is the extension easy based on these results? Does the algorithmic complexity result extend?
==================================================

Focused review:

Weakness
The weakness of the paper results from the lack of clarity of the definition and configuration, as well as the worry about the overall significance of the results.
1. w.r.t. Definition 2: Selection diagram
Selection diagram is the core definition with respect to which major theoretical results in the paper are presented. While I agree with authors that we need some structural assumptions on domain generalization so that we can perform analyses, I am hesitant to agree with the way assumptions are introduced with "selection diagram". In my opinion, the definition is confusing. If the
S
variable is an additional cause for the observed variable (e.g., Figure 1b - 1d), this is essentially changing the underlying causal semantics among the observed variables, which is not the setting described in the paper. In the medical study example in Section 2, what is the additional cause for
W
(Figure 1b) in this context? Shouldn't it be
W
→
S
W
since it is a (general) kind of selection? Figure 1c - 1d share the same problem.
2. the modeling of domain generalization
As a follow-up to 1., a very relevant notion in the medical study context is selection bias (e.g., "A structural approach to selection bias" by Hernán et al., 2004). While I understand the fact that in this paper the domain generalization may not be limited to selection bias, the practical example presented (e.g., observational study vs. randomized trial) seems to be more aligned with the selection bias analysis. Further clarifications will be very helpful.
3. the limited significance of theoretical results
While it is nice to see a formalized connection between (certain types of) domain generalization to transportability theory, there is a worry about the significance of the theoretical results. For instance, Theorem 1 is directly applying Definition 3. Theorem 2 and Theorem 3 are, to a certain extent, direct outcomes of observational and interventional equivalences between different SCMs. The analysis on relative robustness is essentially just hierarchical relations between equivalence, observational equivalence, and interventional equivalence, which are established results in causal inference literature (e.g., "Elements of causal inference" by Peters et al., 2017).
Some additional minor questions/comments:
Is
Y
limited to binary?
In introduction, what is the reason behind the "informative purpose" served by
0
<
l
<
u
<
1
?
Before Section 2, "share an unobserved confounder", maybe "at least one" is more rigorous (just a minor thing, though).
Typo: Section 2.1, "instead optimizing over" -> "instead of optimizing over"
Typo: Section 2.1, "a chosen uncertainty set" -> "a chosen uncertain set"


Review Point: The weakness of the paper results from the lack of clarity of the definition and configuration, as well as the worry about the overall significance of the results.
Review Point: 1. w.r.t. Definition 2: Selection diagram Selection diagram is the core definition with respect to which major theoretical results in the paper are presented. While I agree with authors that we need some structural assumptions on domain generalization so that we can perform analyses, I am hesitant to agree with the way assumptions are introduced with "selection diagram". In my opinion, the definition is confusing. If the S variable is an additional cause for the observed variable (e.g., Figure 1b - 1d), this is essentially changing the underlying causal semantics among the observed variables, which is not the setting described in the paper. In the medical study example in Section 2, what is the additional cause for W (Figure 1b) in this context? Shouldn't it be W → S W since it is a (general) kind of selection? Figure 1c - 1d share the same problem.
Review Point: 2. the modeling of domain generalization As a follow-up to 1., a very relevant notion in the medical study context is selection bias (e.g., "A structural approach to selection bias" by Hernán et al., 2004). While I understand the fact that in this paper the domain generalization may not be limited to selection bias, the practical example presented (e.g., observational study vs. randomized trial) seems to be more aligned with the selection bias analysis. Further clarifications will be very helpful.
Review Point: 3. the limited significance of theoretical results While it is nice to see a formalized connection between (certain types of) domain generalization to transportability theory, there is a worry about the significance of the theoretical results. For instance, Theorem 1 is directly applying Definition 3. Theorem 2 and Theorem 3 are, to a certain extent, direct outcomes of observational and interventional equivalences between different SCMs. The analysis on relative robustness is essentially just hierarchical relations between equivalence, observational equivalence, and interventional equivalence, which are established results in causal inference literature (e.g., "Elements of causal inference" by Peters et al., 2017). Some additional minor questions/comments: Is Y limited to binary? In introduction, what is the reason behind the "informative purpose" served by 0 < l < u < 1 ? Before Section 2, "share an unobserved confounder", maybe "at least one" is more rigorous (just a minor thing, though). Typo: Section 2.1, "instead optimizing over" -> "instead of optimizing over" Typo: Section 2.1, "a chosen uncertainty set" -> "a chosen uncertain set"
==================================================

Focused review:

1. The improvements are marginal, inparticular the ConLL.    2. The method can be regarded as data reweighting, where the important data point is with large weight.  Thus, the paper should compare with this line of work. 
1. It seems that the four proposed resampling methods behave differently in different settings.  The paper might be stronger if there exists one method better than the others.       2. Significance test might be needed.      3. See the weakness 2. 

Review Point: 2. The method can be regarded as data reweighting, where the important data point is with large weight. Thus, the paper should compare with this line of work.
Review Point: 1. It seems that the four proposed resampling methods behave differently in different settings. The paper might be stronger if there exists one method better than the others.
==================================================

Focused review:

Weaknesses

Over-simplification of the field of DG in the intro. One minor point: The introduction reads as if the standard assumption to make in DG is that the distributions of labels given features should be environment dependent (lines 21-22). Furthermore, there is a claim that algorithms that rely on these assumptions "dominate" DG benchmarks (lines 23-24). I think that both of these claims are in doubt. There are many other works that make different assumptions on the set of stable or invariant features (see e.g. [Zhang et al., 2021] and [Robey et al., 2021], which assume domain/measurement shift). I think this points to the fact that this paper could benefit from a related work section, which is notably absent in this submission. WRT the point about feature-matching algorithms "dominating" the leaderboards, this seems untrue based on recent surveys: [Gulrajani & Lopez-Paz, 2020] and [Koh & Sagawa et al., 2021]. Both of these works show that ERM is a rather strong baseline relative to all other algorithms, especially relative to feature-matching algorithms.
The term "feature matching." One point of confusion throughout the work was referring to "matching." I think it would help if the authors could formally define what it means to "match" features. The reason I bring this up is that it gets rather confusing in the non-linear case how features are being "matched." The paper seems to assume that the reader is a priori going to know what this means, but I imagine that many readers may become confused at this point.
Algorithm description. The description of the algorithm that is actually run in practice is confusing, and crucial details are missing. One question is: How are the subsets selected? Is
T
a hyperparameter? If so, how is it tuned? Furthermore, which Alg. 1 shows us the basic steps of the algorithms, it's not clear how one would actually go about implementing this method. In particular, the step in line 5 seems challenging. In the end, the authors appear to tweak the implementation of CORAL by removing the ERM term. I think it would be reasonable to expect a more robust discussion of the implementation of these methods -- which also seems to be missing from the appendices. I am listing this as a weakness because if a contribution of this paper is to propose this algorithm as a new method, which seems to be the case based on this:
"This work presents the first domain generalization algorithm which provably recovers an invariant predictor with a number of environments that scales sub-linearly with the spurious feature dimension."
then the paper should be responsible for connecting the pseudocode in Alg. 1 to an implementable optimization problem.
Why doesn't CORAL work? Related to the previous point, after reading the paper I am still unsure of why CORAL fails in the experiments whereas IFM succeeds. Is it the fact that IFM optimizes for each
U
t
separately in rounds, or is it the dropping of the ERM term that helps? Perhaps this is already answered somewhere, but it was not obvious to me after reading.
Experiments. The experiments feel like a weak point of this paper, with the caveat that this paper seems to be more of a theoretical paper than an experimental one. Even still, I think it wouldn't be unreasonable to expect a stronger set of experiments here. Some comments:
The Noisy MNIST dataset doesn't seem to satisfy the assumption concerning
Z
2
below line 136.
The authors claim that
"Our results in Section 7 suggest that practitioners may benefit from feature matching algorithms when the distinguishing property of the signal feature is indeed conditional distributional invariance"
However, the authors do not run their proposed algorithm on any standard benchmarks, making it difficult to discern whether or not IFM would be useful for practitioners. Indeed, it looks like IFM isn't even run on the Noised MNIST dataset, which raises concerns about whether IFM on its own works (without the ERM loss term).
The authors refer to
U
t
as a "layer" (line 325), which is confusing to me. Is this supposed to be related to the layers of a DNN?
It's unclear how the algorithm works in the nonlinear case, i.e. when you need to do feature matching for different layers. As I mentioned above, a better understanding of how the algorithms are implemented would lead to stronger insights from the experiments.
Overall evaluation

Overall, I liked this paper. I think that it makes a strong theoretical contribution, and that it studies an important and understudied aspect of DG. I believe that the paper is generally well written, and I appreciate that although this is more of a theoretical paper, the authors ran some experiments. This being said, there are a number of drawbacks that I would like to see addressed. Firstly, the algorithmic description and experimental evaluation are weak compared to the theory, but I think both of these points could be addressed (see above). Another weakness is the lack of a related work section, which ideally would more broadly discuss recent developments and algorithms proposed for the DG setting. All of this being said, I'm leaning toward accepting this paper.
Citations

[Zhang et al., 2021] Zhang, Marvin, et al. "Adaptive risk minimization: Learning to adapt to domain shift." Advances in Neural Information Processing Systems 34 (2021): 23664-23678.
[Robey et al., 2021] Robey, Alexander, George J. Pappas, and Hamed Hassani. "Model-based domain generalization." Advances in Neural Information Processing Systems 34 (2021): 20210-20229.
[Gulrajani & Lopez-Paz, 2020] Gulrajani, Ishaan, and David Lopez-Paz. "In search of lost domain generalization." arXiv preprint arXiv:2007.01434 (2020).
[Koh & Sagawa et al., 2021] Koh, Pang Wei, et al. "Wilds: A benchmark of in-the-wild distribution shifts." International Conference on Machine Learning. PMLR, 2021.


Review Point: Over-simplification of the field of DG in the intro. One minor point: The introduction reads as if the standard assumption to make in DG is that the distributions of labels given features should be environment dependent (lines 21-22). Furthermore, there is a claim that algorithms that rely on these assumptions "dominate" DG benchmarks (lines 23-24). I think that both of these claims are in doubt. There are many other works that make different assumptions on the set of stable or invariant features (see e.g. [Zhang et al., 2021] and [Robey et al., 2021], which assume domain/measurement shift). I think this points to the fact that this paper could benefit from a related work section, which is notably absent in this submission. WRT the point about feature-matching algorithms "dominating" the leaderboards, this seems untrue based on recent surveys: [Gulrajani & Lopez-Paz, 2020] and [Koh & Sagawa et al., 2021]. Both of these works show that ERM is a rather strong baseline relative to all other algorithms, especially relative to feature-matching algorithms. The term "feature matching." One point of confusion throughout the work was referring to "matching." I think it would help if the authors could formally define what it means to "match" features. The reason I bring this up is that it gets rather confusing in the non-linear case how features are being "matched." The paper seems to assume that the reader is a priori going to know what this means, but I imagine that many readers may become confused at this point. Algorithm description. The description of the algorithm that is actually run in practice is confusing, and crucial details are missing. One question is: How are the subsets selected? Is T a hyperparameter? If so, how is it tuned? Furthermore, which Alg.
Review Point: 1 shows us the basic steps of the algorithms, it's not clear how one would actually go about implementing this method. In particular, the step in line 5 seems challenging. In the end, the authors appear to tweak the implementation of CORAL by removing the ERM term. I think it would be reasonable to expect a more robust discussion of the implementation of these methods -- which also seems to be missing from the appendices. I am listing this as a weakness because if a contribution of this paper is to propose this algorithm as a new method, which seems to be the case based on this: "This work presents the first domain generalization algorithm which provably recovers an invariant predictor with a number of environments that scales sub-linearly with the spurious feature dimension." then the paper should be responsible for connecting the pseudocode in Alg.
Review Point: 1 to an implementable optimization problem. Why doesn't CORAL work? Related to the previous point, after reading the paper I am still unsure of why CORAL fails in the experiments whereas IFM succeeds. Is it the fact that IFM optimizes for each U t separately in rounds, or is it the dropping of the ERM term that helps? Perhaps this is already answered somewhere, but it was not obvious to me after reading. Experiments. The experiments feel like a weak point of this paper, with the caveat that this paper seems to be more of a theoretical paper than an experimental one. Even still, I think it wouldn't be unreasonable to expect a stronger set of experiments here. Some comments: The Noisy MNIST dataset doesn't seem to satisfy the assumption concerning Z 2 below line 136. The authors claim that "Our results in Section 7 suggest that practitioners may benefit from feature matching algorithms when the distinguishing property of the signal feature is indeed conditional distributional invariance" However, the authors do not run their proposed algorithm on any standard benchmarks, making it difficult to discern whether or not IFM would be useful for practitioners. Indeed, it looks like IFM isn't even run on the Noised MNIST dataset, which raises concerns about whether IFM on its own works (without the ERM loss term). The authors refer to U t as a "layer" (line 325), which is confusing to me. Is this supposed to be related to the layers of a DNN? It's unclear how the algorithm works in the nonlinear case, i.e. when you need to do feature matching for different layers. As I mentioned above, a better understanding of how the algorithms are implemented would lead to stronger insights from the experiments. Overall evaluation Overall, I liked this paper. I think that it makes a strong theoretical contribution, and that it studies an important and understudied aspect of DG. I believe that the paper is generally well written, and I appreciate that although this is more of a theoretical paper, the authors ran some experiments. This being said, there are a number of drawbacks that I would like to see addressed. Firstly, the algorithmic description and experimental evaluation are weak compared to the theory, but I think both of these points could be addressed (see above). Another weakness is the lack of a related work section, which ideally would more broadly discuss recent developments and algorithms proposed for the DG setting. All of this being said, I'm leaning toward accepting this paper. Citations [Zhang et al., 2021] Zhang, Marvin, et al. "Adaptive risk minimization: Learning to adapt to domain shift." Advances in Neural Information Processing Systems 34 (2021): 23664-23678. [Robey et al., 2021] Robey, Alexander, George J. Pappas, and Hamed Hassani. "Model-based domain generalization." Advances in Neural Information Processing Systems 34 (2021): 20210-20229. [Gulrajani & Lopez-Paz, 2020] Gulrajani, Ishaan, and David Lopez-Paz. "In search of lost domain generalization." arXiv preprint arXiv:2007.01434 (2020). [Koh & Sagawa et al., 2021] Koh, Pang Wei, et al. "Wilds: A benchmark of in-the-wild distribution shifts." International Conference on Machine Learning. PMLR, 2021.
==================================================

Focused review:

Idea: while the idea and results/implementation are clearly good, I was lacking some interpretation or intuition as to why the sinusoidal activation function is a good choice beyond the derivatives being well behaved. I think there were some interesting discussions in related work about inspiration from Fourier series and DCT for other work. I think it would be helpful in the intro to have some discussion of the authors' inspiration. Experiments: - the authors operate on a 5 layer network. While this allows for a range of realistic tasks, I wonder if training becomes more challenging as the layer increases in depth? It would be interesting then to explore layer depth vs training ease vs performance for other researchers. - Following up on the above point, how difficult were these models to train? Also did the authors consider different architectures -- one of the benefits of the ReLU setup is it works well for a variety of architectures.

Review Point: Idea: while the idea and results/implementation are clearly good, I was lacking some interpretation or intuition as to why the sinusoidal activation function is a good choice beyond the derivatives being well behaved. I think there were some interesting discussions in related work about inspiration from Fourier series and DCT for other work. I think it would be helpful in the intro to have some discussion of the authors' inspiration. Experiments:
Review Point: - the authors operate on a 5 layer network. While this allows for a range of realistic tasks, I wonder if training becomes more challenging as the layer increases in depth? It would be interesting then to explore layer depth vs training ease vs performance for other researchers.
Review Point: - Following up on the above point, how difficult were these models to train? Also did the authors consider different architectures -- one of the benefits of the ReLU setup is it works well for a variety of architectures.
==================================================

Focused review:

Weakness:  1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world. 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling.   Detailed Comments: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.   

Review Point: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world.
Review Point: 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling. Detailed Comments:
Review Point: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.
==================================================

Focused review:

- The authors propose to create a "single unified method can explain a variety of illusions" (line 70), but this is an overstatement. Only 3 types of illusions are studied (simultaneous contrast, White's, Hermann grid), which are actually quite similar within the overall very broad realm of visual illusions (which includes other dimensions like size constancy, collinearity/offset illusions, illusory contours/shapes, various 3D illusions like the Penrose stairs, dynamic illusions like the barber pole, breathing square, and many others, etc). - Thus, while the approach is interesting, its scope is quite narrow. It would help rather than hurt if the authors made that clearer (e.g., in the title, abstract, and contributions).

Review Point: - The authors propose to create a "single unified method can explain a variety of illusions" (line 70), but this is an overstatement. Only 3 types of illusions are studied (simultaneous contrast, White's, Hermann grid), which are actually quite similar within the overall very broad realm of visual illusions (which includes other dimensions like size constancy, collinearity/offset illusions, illusory contours/shapes, various 3D illusions like the Penrose stairs, dynamic illusions like the barber pole, breathing square, and many others, etc).
Review Point: - Thus, while the approach is interesting, its scope is quite narrow. It would help rather than hurt if the authors made that clearer (e.g., in the title, abstract, and contributions).
==================================================

Focused review:

Weaknesses
W1. Missing some related work. I consider the masking techniques used in this paper as related to BERT-style masking that has been previously explored in the context of videos. Some relevant references to consider are Lu et al., 2019 (https://arxiv.org/abs/1908.02265), Sun et al., 2019 (https://arxiv.org/abs/1904.01766), and Zellers et al., 2021 (https://arxiv.org/abs/2106.02636). I recommend a more comprehensive review of recent related work to better contextualize the paper.
W2. It is not clear early in the paper why "iterative" is necessarily better than "auto-regressive." Auto-regression may also be considered an iterative process. I suggest being more precise here to give better intuition on why the proposed method is faster than existing auto-regressive methods.
W3. Different models (e.g., VQ-GANs) are trained for different datasets, which each represent fairly narrow domains. I would be interested to see if the technique could be extended to training a single model on a large corpus of video data or at least the union of the datasets considered. How well does this model perform compared to the single models? I feel this experiment is important to elucidate the scalability of the method.
W4. The intuition for why keeping the most confident frames leads to static videos is not clear to me. Given motion in the video ground truth, won't such predictions lead to very high loss?
W5. Why is the evaluation protocol different for BAIR than the other video datasets (L194-196)?
W6. Why train VQ-GAN from scratch, why not use the encoder trained on internet scale data mentioned in L291-292. Would using this tokenizer improve performance? I do not consider this to be outside the scope of the paper.
Minor
M1. I suggest changing the title as ViT usually refers to the Dosovitskiy et al., 2021 work (https://arxiv.org/abs/2010.11929), where "Vi" does not mean video. Hence the title could be a little misleading.
M2. The paper claims that "there is an inconsistency between the video prediction task and autoregressive masked visual pretraining – while the training process assumes partial knowledge of the ground truth future frames, at test time the model has to predict a complete sequence of future frames from scratch." However, this explanation might not be clear for someone without a lot of background knowledge on autoregressive techniques in video. I suggest giving an example or providing a reference here to make things more clear.
M3. I feel a major strength of the method is to be able to condition prediction on arbitrary frames in a video. This naturally allows for goal conditioning for robot manipulation, which seems like a major plus. I recommend emphasizing this feature more in the writing earlier on, otherwise it may get lost.


Review Point: W1. Missing some related work. I consider the masking techniques used in this paper as related to BERT-style masking that has been previously explored in the context of videos. Some relevant references to consider are Lu et al., 2019 (https://arxiv.org/abs/1908.02265), Sun et al., 2019 (https://arxiv.org/abs/1904.01766), and Zellers et al., 2021 (https://arxiv.org/abs/2106.02636). I recommend a more comprehensive review of recent related work to better contextualize the paper.
Review Point: W2. It is not clear early in the paper why "iterative" is necessarily better than "auto-regressive." Auto-regression may also be considered an iterative process. I suggest being more precise here to give better intuition on why the proposed method is faster than existing auto-regressive methods.
Review Point: W3. Different models (e.g., VQ-GANs) are trained for different datasets, which each represent fairly narrow domains. I would be interested to see if the technique could be extended to training a single model on a large corpus of video data or at least the union of the datasets considered. How well does this model perform compared to the single models? I feel this experiment is important to elucidate the scalability of the method.
Review Point: W4. The intuition for why keeping the most confident frames leads to static videos is not clear to me. Given motion in the video ground truth, won't such predictions lead to very high loss?
Review Point: W5. Why is the evaluation protocol different for BAIR than the other video datasets (L194-196)?
Review Point: W6. Why train VQ-GAN from scratch, why not use the encoder trained on internet scale data mentioned in L291-292. Would using this tokenizer improve performance? I do not consider this to be outside the scope of the paper. Minor M1. I suggest changing the title as ViT usually refers to the Dosovitskiy et al., 2021 work (https://arxiv.org/abs/2010.11929), where "Vi" does not mean video. Hence the title could be a little misleading.
Review Point: M2. The paper claims that "there is an inconsistency between the video prediction task and autoregressive masked visual pretraining – while the training process assumes partial knowledge of the ground truth future frames, at test time the model has to predict a complete sequence of future frames from scratch." However, this explanation might not be clear for someone without a lot of background knowledge on autoregressive techniques in video. I suggest giving an example or providing a reference here to make things more clear.
Review Point: M3. I feel a major strength of the method is to be able to condition prediction on arbitrary frames in a video. This naturally allows for goal conditioning for robot manipulation, which seems like a major plus. I recommend emphasizing this feature more in the writing earlier on, otherwise it may get lost.
==================================================

Focused review:

Weaknesses: 1. The Cycle FC align features at different spatial locations to the same channel, but analysis is slightly insufficient. There could be many different designs of it. For example, the experiments or analysis with different sampling intervals and sample size. 2. CycleMLP is slightly insufficient in discussion of the design and ablation studies.


Review Point: 1. The Cycle FC align features at different spatial locations to the same channel, but analysis is slightly insufficient. There could be many different designs of it. For example, the experiments or analysis with different sampling intervals and sample size.
Review Point: 2. CycleMLP is slightly insufficient in discussion of the design and ablation studies.
==================================================

Focused review:

+ The main problem with the paper is the game design. In visual dialogue, i.e GuessWhich game[2], does not have access to the image. It has to build up the visual representation based on the caption and dialogue. That is why having a caption is important for the GuessWhich game (L69). While in the proposed game, since Q-Bot has constant access to the images. It just needs to ask questions such that it distinguished the one image from the other. Which mean that ask questions such that narrow down the type of image (bird from car) and then try to differentiate among different similar type image (say bird). This could be observed from the questions generated from Figure 3. + Since the model is trained on VQA data to distinguish images, it only learns to ask the discriminative type of questions. However, the dialogue is not always discriminative; for example, the dialogue could also involve clarification, follow-up question etc. That means that model is not learning these types of skills only focusing on one type of skill. + The way distractor images are selected is not systematic. Because of random selection, some of the games might be very easy to perform, only need to distinguish b/w contrasting VQA images. For VQA (i.e., MS-COCO) images, object category, super-categories, etc, could be used to create systematic and challenging distractor images. Look at "Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog" for some suggestions. + As we could see from Table 1, B3 vs. C3. Having random distractor significantly improves performance. So having challenging distractors will have a greater effect on the performance. + Dipper analysis on the distractor, for eg in the case of CUB does having bird image as target images make tasks more challenging. + Results are not clearly explained in the text. For example, L266 "Longer dialogs (more rounds) achieve better accuracy". However, it is not supported clearly. It would have been better to see results for 9 distractors with a different number of rounds of dialogue and all measures how it performs say as a graph. + Make it clear how the target image is selected for A-Bot (L62-63) + Missing dataset splits is paper using the same split as GuessWhich. Please provide details. + For language analysis looks at "The Devil is in the Details: A Magnifying Glass for the GuessWhich Visual Dialogue Game," which measures Lexical Diversity (similar to language-diversity), Question diversity etc. + Table 1 caption "Our method strikes a balance between guessing game performance and interpretability." Please clarify how interpretability is improved. + Fig 3 provide Q4 answer and guessed image by models

Review Point: + The main problem with the paper is the game design. In visual dialogue, i.e GuessWhich game[2], does not have access to the image. It has to build up the visual representation based on the caption and dialogue. That is why having a caption is important for the GuessWhich game (L69). While in the proposed game, since Q-Bot has constant access to the images. It just needs to ask questions such that it distinguished the one image from the other. Which mean that ask questions such that narrow down the type of image (bird from car) and then try to differentiate among different similar type image (say bird). This could be observed from the questions generated from Figure 3.
Review Point: + Since the model is trained on VQA data to distinguish images, it only learns to ask the discriminative type of questions. However, the dialogue is not always discriminative; for example, the dialogue could also involve clarification, follow-up question etc. That means that model is not learning these types of skills only focusing on one type of skill.
Review Point: + The way distractor images are selected is not systematic. Because of random selection, some of the games might be very easy to perform, only need to distinguish b/w contrasting VQA images. For VQA (i.e., MS-COCO) images, object category, super-categories, etc, could be used to create systematic and challenging distractor images. Look at "Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog" for some suggestions.
Review Point: + As we could see from Table 1, B3 vs.
Review Point: C3. Having random distractor significantly improves performance. So having challenging distractors will have a greater effect on the performance.
Review Point: + Dipper analysis on the distractor, for eg in the case of CUB does having bird image as target images make tasks more challenging.
Review Point: + Results are not clearly explained in the text. For example, L266 "Longer dialogs (more rounds) achieve better accuracy". However, it is not supported clearly. It would have been better to see results for 9 distractors with a different number of rounds of dialogue and all measures how it performs say as a graph.
Review Point: + Make it clear how the target image is selected for A-Bot (L62-63) + Missing dataset splits is paper using the same split as GuessWhich. Please provide details.
Review Point: + For language analysis looks at "The Devil is in the Details: A Magnifying Glass for the GuessWhich Visual Dialogue Game," which measures Lexical Diversity (similar to language-diversity), Question diversity etc.
Review Point: + Table 1 caption "Our method strikes a balance between guessing game performance and interpretability." Please clarify how interpretability is improved.
Review Point: + Fig 3 provide Q4 answer and guessed image by models
==================================================

Focused review:

Weakness
--- The overall technical contribution is limited. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2].
-- The use of the sine activation function is not well supported by the experiments. In table 1, sine function shows marginal improvement over tanh, and in table 5 of the appendix, using tanh shows much better performance over sine. Furthermore, tanh tends to become saturated during backpropagation of a long sequence. I would suggest the author also compares with ReLU.
-- Presentation in the paper can be improved:
The modified Adam optimizer is listed as one of the technical contributions. However, it is not stated clearly what is the modification and how does it compare to the original Adam optimizer.
The name of the robots are presented in Fig. 7 of Sec. 4.5 but are referenced multiple times in previous sections and table 1. It would be helpful to clarify the name of the agents early on.
Reference of Fig. 5 at the end of Sec. 4.2 seems to be incorrect, as no comparison of SGD and Adam is shown in Fig. 5.
[1] Lee, Joonho, et al. "Learning quadrupedal locomotion over challenging terrain." Science robotics 5.47 (2020). [2] Zhao, Allan, et al. "RoboGrammar: graph grammar for terrain-optimized robot design." ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.


Review Point: --- The overall technical contribution is limited. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2]. -- The use of the sine activation function is not well supported by the experiments. In table 1, sine function shows marginal improvement over tanh, and in table 5 of the appendix, using tanh shows much better performance over sine. Furthermore, tanh tends to become saturated during backpropagation of a long sequence. I would suggest the author also compares with ReLU. -- Presentation in the paper can be improved: The modified Adam optimizer is listed as one of the technical contributions. However, it is not stated clearly what is the modification and how does it compare to the original Adam optimizer. The name of the robots are presented in Fig. 7 of Sec. 4.5 but are referenced multiple times in previous sections and table 1. It would be helpful to clarify the name of the agents early on. Reference of Fig. 5 at the end of Sec. 4.2 seems to be incorrect, as no comparison of SGD and Adam is shown in Fig.
Review Point: 5. [1] Lee, Joonho, et al. "Learning quadrupedal locomotion over challenging terrain." Science robotics 5.47 (2020). [2] Zhao, Allan, et al. "RoboGrammar: graph grammar for terrain-optimized robot design." ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.
==================================================

Focused review:

I believe the results proposed in this paper are related to existing work. The techniques used are close to existing methods - at the very least a detailed comparison is in order. The paper fails to acknowledge lots of literature on representing coalitional games in a restricted manner. In fact, many techniques have been proposed for concisely representing coalitional games, and approximately solving them. This issue is covered in depth in (e.g): Chalkiadakis, Georgios, Edith Elkind, and Michael Wooldridge. "Computational aspects of cooperative game theory." Synthesis Lectures on Artificial Intelligence and Machine Learning 5.6 (2011): 1-168. Michalak, Tomasz P., et al. "Efficient computation of the Shapley value for game-theoretic network centrality." Journal of Artificial Intelligence Research 46 (2013): 607-650. Conitzer, Vincent, and Tuomas Sandholm. "Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains." AAAI. Vol. 4. 2004. Conitzer, Vincent, and Tuomas Sandholm. "Complexity of constructing solutions in the core based on synergies among coalitions." Artificial Intelligence 170.6-7 (2006): 607-619. There are many good approximations for the Shapley value, including: Fatima, Shaheen S., Michael Wooldridge, and Nicholas R. Jennings. "A linear approximation method for the Shapley value." Artificial Intelligence 172.14 (2008): 1673-1699. The issue of proving a lower bound on the number of queries required to learn a characteristic function has been also investigate earlier (some of the techniques in these are similar to what is proposed in this paper): Maleki, Sasan, et al. "Bounding the estimation error of sampling-based Shapley value approximation." arXiv preprint arXiv:1306.4265 (2013). Bachrach, Yoram, et al. "Approximating power indices: theoretical and empirical analysis." Autonomous Agents and Multi-Agent Systems 20.2 (2010): 105-122. Liben-Nowell, David, et al. "Computing shapley value in supermodular coalitional games." International Computing and Combinatorics Conference. Springer, Berlin, Heidelberg, 2012. I think the topic is very interesting, but given the above work, I think the innovation is limited. Also, the empirical evaluation is carried on only one dataset, and a more thorugh coverage is warrented. Additionally, this looks to me like a game theory paper with some learning aspects, not a machine learning paper.

Review Point: I believe the results proposed in this paper are related to existing work. The techniques used are close to existing methods - at the very least a detailed comparison is in order. The paper fails to acknowledge lots of literature on representing coalitional games in a restricted manner. In fact, many techniques have been proposed for concisely representing coalitional games, and approximately solving them. This issue is covered in depth in (e.g): Chalkiadakis, Georgios, Edith Elkind, and Michael Wooldridge. "Computational aspects of cooperative game theory." Synthesis Lectures on Artificial Intelligence and Machine Learning 5.6 (2011): 1-168. Michalak, Tomasz P., et al. "Efficient computation of the Shapley value for game-theoretic network centrality." Journal of Artificial Intelligence Research 46 (2013): 607-650. Conitzer, Vincent, and Tuomas Sandholm. "Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains." AAAI. Vol.
Review Point: 4. 2004. Conitzer, Vincent, and Tuomas Sandholm. "Complexity of constructing solutions in the core based on synergies among coalitions." Artificial Intelligence 170.6-7 (2006): 607-619. There are many good approximations for the Shapley value, including: Fatima, Shaheen S., Michael Wooldridge, and Nicholas R. Jennings. "A linear approximation method for the Shapley value." Artificial Intelligence 172.14 (2008): 1673-1699. The issue of proving a lower bound on the number of queries required to learn a characteristic function has been also investigate earlier (some of the techniques in these are similar to what is proposed in this paper): Maleki, Sasan, et al. "Bounding the estimation error of sampling-based Shapley value approximation." arXiv preprint arXiv:1306.4265 (2013). Bachrach, Yoram, et al. "Approximating power indices: theoretical and empirical analysis." Autonomous Agents and Multi-Agent Systems 20.2 (2010): 105-122. Liben-Nowell, David, et al. "Computing shapley value in supermodular coalitional games." International Computing and Combinatorics Conference. Springer, Berlin, Heidelberg, 2012. I think the topic is very interesting, but given the above work, I think the innovation is limited. Also, the empirical evaluation is carried on only one dataset, and a more thorugh coverage is warrented. Additionally, this looks to me like a game theory paper with some learning aspects, not a machine learning paper.
==================================================

Focused review:

1. Unclear connection between the theory of orthogonal Monte Carlo (section 3) and the proposed method (near-orthogonal Monte Carlo, section 4-5). To me, the proposed method seems unrelated to the theory. The experiment in section 5 doesn't seem to connect to the theory either. Perhaps the authors can expand on the connection between the two parts of the paper. 2. Lack of details/reproducibility. The algorithm alg-NOMC in Section 4.2 (which is also the method used in the experiments in section 5) is not described in details. The hyperparameter tuning procedure (e.g. delta in the opt-NOMC method) is not described. Without code, this might make it hard to reproduce the results. In particular, it's not clear how to construct s random omegas vectors in section 4.2, which is needed to implement the algorithm. 3. Lack of experiment on the downstream applications. It would be interesting to see if NOMC improves in the downstream applications mentioend, such as in kernel regression and generative models.

Review Point: 1. Unclear connection between the theory of orthogonal Monte Carlo (section 3) and the proposed method (near-orthogonal Monte Carlo, section 4-5). To me, the proposed method seems unrelated to the theory. The experiment in section 5 doesn't seem to connect to the theory either. Perhaps the authors can expand on the connection between the two parts of the paper.
Review Point: 2. Lack of details/reproducibility. The algorithm alg-NOMC in Section 4.2 (which is also the method used in the experiments in section 5) is not described in details. The hyperparameter tuning procedure (e.g. delta in the opt-NOMC method) is not described. Without code, this might make it hard to reproduce the results. In particular, it's not clear how to construct s random omegas vectors in section 4.2, which is needed to implement the algorithm.
Review Point: 3. Lack of experiment on the downstream applications. It would be interesting to see if NOMC improves in the downstream applications mentioend, such as in kernel regression and generative models.
==================================================

Focused review:

1. The computation of H_i in deep lazy map could be time-consuming and with large variacne. It is not sure how to work well in real applications. 2. According to Proposition 3, we can see that there is a trade-off between coverage rate and optimal: when t = 1, the optimal do not alow faster converage. However, suboptimal choice without loss the convergence property.

Review Point: 1. The computation of H_i in deep lazy map could be time-consuming and with large variacne. It is not sure how to work well in real applications.
Review Point: 2. According to Proposition 3, we can see that there is a trade-off between coverage rate and optimal: when t = 1, the optimal do not alow faster converage. However, suboptimal choice without loss the convergence property.
==================================================

Focused review:

Weakness1: The main concern is that this work mainly relies on empirical results on showing effectiveness. For example, it is fully unclear why masking a subset of the input pixels can stabilize FGSM-AT. Why does it help prevent catastrophic overfitting? Why does catastrophic overfitting happen to ViT not CNN? Why does larger stride help? To my understanding, the core issue in FGSM AT its how to prevent catastrophic overfitting, a recent work [1] shows random noise augmentation is sufficient for preventing catastrophic overfitting and the reason of its success is analyzed in another work [2]. Can Non-robust feature perspective in [2] be used to explain to the success of the proposed techniques in this work? Or do the authors have their own new perspective? From my understanding, to provide explanations for why the techniques can be more important than the tricks themselves, especially in the adversarial ML community. Otherwise, the takeaway from this work can be limited.
Weakness2: The technical contributions might also be limited.
[1] Fast Adversarial Training with Noise Augmentation: A Unified Perspective on RandStart and GradAlign, https://arxiv.org/pdf/2202.05488.pdf
[2] Understanding Catastrophic Overfitting in Fast Adversarial Training From a Non-robust Feature Perspective, https://openreview.net/forum?id=UO8UP_xDMwD


Review Point: The main concern is that this work mainly relies on empirical results on showing effectiveness. For example, it is fully unclear why masking a subset of the input pixels can stabilize FGSM-AT. Why does it help prevent catastrophic overfitting? Why does catastrophic overfitting happen to ViT not CNN? Why does larger stride help? To my understanding, the core issue in FGSM AT its how to prevent catastrophic overfitting, a recent work [1] shows random noise augmentation is sufficient for preventing catastrophic overfitting and the reason of its success is analyzed in another work [2]. Can Non-robust feature perspective in [2] be used to explain to the success of the proposed techniques in this work? Or do the authors have their own new perspective? From my understanding, to provide explanations for why the techniques can be more important than the tricks themselves, especially in the adversarial ML community. Otherwise, the takeaway from this work can be limited.
Review Point: Weakness2: The technical contributions might also be limited. [1] Fast Adversarial Training with Noise Augmentation: A Unified Perspective on RandStart and GradAlign, https://arxiv.org/pdf/2202.05488.pdf [2] Understanding Catastrophic Overfitting in Fast Adversarial Training From a Non-robust Feature Perspective, https://openreview.net/forum?id=UO8UP_xDMwD
==================================================

Focused review:

Weaknesses:
The boolean task algebra is quite restrictive. I don't see many use cases for it. If the authors introduce a new problem setting, they should motivate it.
assuming the agent can interact with the task as long as it needs is quite an unrealistic assumption in LRL.
all experiments on toy tasks. I think a more realistic benchmark should be introduced.
there's no LRL baselines.
in SOGPOL,
the new skill is initialize randomly. This will be pretty inefficient is more realistic settings.
IIUC, when learning, to enable transfer from previous tasks, goals need to be reached. But in realistic settings, goals will be hard to reach. So essentially lots time and compute will be wasted relearning over and over same things.
Minor details:
def 3:
r
M
A
X
is the same as
r
M
I
N
theorem 1: what is
γ
?
what's the difference between prop. 1 and prop. 2 ?
theorem 2: can you provide some intuition/explanation on those bounds?


Review Point: The boolean task algebra is quite restrictive. I don't see many use cases for it. If the authors introduce a new problem setting, they should motivate it. assuming the agent can interact with the task as long as it needs is quite an unrealistic assumption in LRL. all experiments on toy tasks. I think a more realistic benchmark should be introduced. there's no LRL baselines. in SOGPOL, the new skill is initialize randomly. This will be pretty inefficient is more realistic settings. IIUC, when learning, to enable transfer from previous tasks, goals need to be reached. But in realistic settings, goals will be hard to reach. So essentially lots time and compute will be wasted relearning over and over same things. Minor details: def 3: r M A X is the same as r M I N theorem 1: what is γ ? what's the difference between prop.
Review Point: 1 and prop.2 ? theorem 2: can you provide some intuition/explanation on those bounds?
==================================================

Focused review:

* A big concern for me is that this paper was hard to read. Since it is very applications specific, I am not familiar with a lot of the theory or the inverse problem(s) considered here. As a result, I am unable to appreciate the key aspects of the paper. For example, the introduction directly gets into the details of wave based imaging without sufficient detail or context with more commonly considered inverse problems. This makes it unapproachable for someone not familiar with this exact application. There is quite a bit of detail left in the supplement, but I believe this should be in the main paper for the contribution to be fully appreciated. * A second point about it being so applications specific is that the paper lacks context to existing methods, for e.g. how can FIONets be useful for someone outside of wave-based imaging? * Another issue is that there are no quantitative comparisons in the main paper (but in the supplement), leaving only qualitative comparisons. * There are no comparisons to any other method other than a U-Net (which essentially serves as an ablation of whether or not including the physics based network helps). Considering this is a linear inverse problem, what are other existing solutions to this problem? It is imperative to compare the proposed FIONet to iterative or classical solutions to the problem to place them in context. * Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods. * I am not very sure how to read or interpret figure 7 describing the diffeomorphisms. * A minor comment, there is already a model called “routing networks” (Rosenbaum et al, ICLR 2018) which are different from those described in the paper. In the interest of mitigating confusion for the reader it maybe better to clarify or re-name the model.

Review Point: * A big concern for me is that this paper was hard to read. Since it is very applications specific, I am not familiar with a lot of the theory or the inverse problem(s) considered here. As a result, I am unable to appreciate the key aspects of the paper. For example, the introduction directly gets into the details of wave based imaging without sufficient detail or context with more commonly considered inverse problems. This makes it unapproachable for someone not familiar with this exact application. There is quite a bit of detail left in the supplement, but I believe this should be in the main paper for the contribution to be fully appreciated.
Review Point: * A second point about it being so applications specific is that the paper lacks context to existing methods, for e.g. how can FIONets be useful for someone outside of wave-based imaging?
Review Point: * Another issue is that there are no quantitative comparisons in the main paper (but in the supplement), leaving only qualitative comparisons.
Review Point: * There are no comparisons to any other method other than a U-Net (which essentially serves as an ablation of whether or not including the physics based network helps). Considering this is a linear inverse problem, what are other existing solutions to this problem? It is imperative to compare the proposed FIONet to iterative or classical solutions to the problem to place them in context.
Review Point: * Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods.
Review Point: * I am not very sure how to read or interpret figure 7 describing the diffeomorphisms.
Review Point: * A minor comment, there is already a model called “routing networks” (Rosenbaum et al, ICLR 2018) which are different from those described in the paper. In the interest of mitigating confusion for the reader it maybe better to clarify or re-name the model.
==================================================

Focused review:

Weaknesses
I do not have any major issues with the paper. Some minor comments to be addresses
W1: It wasn't clear to me how the gating embedding U is obtained. The authors should clarify this.
W2: It would be interesting to see the performance of LSTT with decoupling features (i.e. an extra entry in Table 3a for completeness).
W3: Are both the encoded visual features as well as the ID features used to obtain the target mask, or only the ID features?
W4: The current approach performs a strict separation of object-specific and object-agnostic features, and mainly uses the object-agnostic features to compute the attention masks. However I imagine that certain object-specific information could also be helpful when computing attention masks. Thus would it make sense to instead have two branches, one which is used to compute the attention masks, and the second used to obtain the final mask. Both of these branch can take both the image and object masks as input, and determine whether to use the ID information or not during the training process.


Review Point: I do not have any major issues with the paper. Some minor comments to be addresses W1: It wasn't clear to me how the gating embedding U is obtained. The authors should clarify this.
Review Point: W2: It would be interesting to see the performance of LSTT with decoupling features (i.e. an extra entry in Table 3a for completeness).
Review Point: W3: Are both the encoded visual features as well as the ID features used to obtain the target mask, or only the ID features?
Review Point: W4: The current approach performs a strict separation of object-specific and object-agnostic features, and mainly uses the object-agnostic features to compute the attention masks. However I imagine that certain object-specific information could also be helpful when computing attention masks. Thus would it make sense to instead have two branches, one which is used to compute the attention masks, and the second used to obtain the final mask. Both of these branch can take both the image and object masks as input, and determine whether to use the ID information or not during the training process.
==================================================

Focused review:

- There needs to be a discussion of how this work relates to earlier work by Chang et al. on crowdsourcing synthetic 3D scenes to investigate language grounding (including spatial relation grounding) -- see the detailed comments on related work. - A few details are missing from the exposition and should be clarified: 1) how were specific object instances selected and were they pre-specified for a given crowdworker? 2) Rationale for subsampling only 1/4 of all relations and what relations are excluded due to that? 3) To my understanding, a crowdworker's judgment that a relation is not valid in a presented image is deemed to imply that the relation is not using an intrinsic frame of reference -- is this a reasonable assumption (i.e. are there other cases when a crowdworker might say the relation does not hold)?

Review Point: - There needs to be a discussion of how this work relates to earlier work by Chang et al. on crowdsourcing synthetic 3D scenes to investigate language grounding (including spatial relation grounding) -- see the detailed comments on related work.
Review Point: - A few details are missing from the exposition and should be clarified:
Review Point: 1) how were specific object instances selected and were they pre-specified for a given crowdworker?
Review Point: 2) Rationale for subsampling only 1/4 of all relations and what relations are excluded due to that?
Review Point: 3) To my understanding, a crowdworker's judgment that a relation is not valid in a presented image is deemed to imply that the relation is not using an intrinsic frame of reference -- is this a reasonable assumption (i.e. are there other cases when a crowdworker might say the relation does not hold)?
==================================================

Focused review:

- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results. 
- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained. 

Review Point: - The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
Review Point: - Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results.
Review Point: - Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
Review Point: - The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained.
==================================================

Focused review:

Weakness:
Size of the dataset (number of nodes) is very small here. Is it because of the limitation of the datasets or the proposed framework is very computationally expensive? 
It would be beneficial if the authors add a ground truth subfigure in Fig. 2.
The proposed method is only applied on simulated or synthetic datasets. I strongly recommend examining the framework on at least two real-world datasets to show the priority of the proposed method to reconstruct the graph structure. Simulated or synthetic data are usually more "predictable" since the data generation process is known and with less uncertainty, while high complexity and unknowns is more likely to be involved in real-world datasets.
I suggest the authors to look into the following related paper on reconstructing network
https://openreview.net/forum?id=rJgTciR9tm
https://www.nature.com/articles/s41467-019-09774-x


Review Point: Size of the dataset (number of nodes) is very small here. Is it because of the limitation of the datasets or the proposed framework is very computationally expensive? It would be beneficial if the authors add a ground truth subfigure in Fig.
Review Point: 2. The proposed method is only applied on simulated or synthetic datasets. I strongly recommend examining the framework on at least two real-world datasets to show the priority of the proposed method to reconstruct the graph structure. Simulated or synthetic data are usually more "predictable" since the data generation process is known and with less uncertainty, while high complexity and unknowns is more likely to be involved in real-world datasets. I suggest the authors to look into the following related paper on reconstructing network https://openreview.net/forum?id=rJgTciR9tm https://www.nature.com/articles/s41467-019-09774-x
==================================================

Focused review:

Weakness]
There is a strong assumption that the mapping function during the few-shot task is one of the functions during training. Is this practical?
The contrast learning only encourages those function representations to be different and it might not be able to learn the underlying function characteristics. Why can it help during down-stream few-shot tasks? In other words, I could use one-hot vector to represent each function. I could give the function index to the down-stream task. Is it helpful?
Although contrast learning is hot, why must use contrast learning to learn function representation? You know the function labels, so you could also use triplet loss during the representation learning.
Although the authors consider quite a few downstream tasks, all the comparisons are quite preliminary. It would be more convincing to show that in at least one task your approach can outperform SOTA or your function representation can improve SOTA.
Figure 3(a) shows a better result with 50 shots, but with 200 shots. Why?
There is no explanation for Fig. 2(b). It seems you use N samples to identify the function and the rest N-M samples to train the decoder. Is it correct? What's a good N here? What is the "S" operation?
Eq. (4) does not look correct.
Is Section 3.1 complete or not ? The organization is a bit wired.

Update:
I think the idea of this paper is very interesting. The authors' responses also address most of my concerns. The remaining issues are: 1) the writing needs to be improved to make the paper easy to read; 2) I am still not so convinced with the experiments. It would be more convincing if the authors can demonstrate this on few-shot image classification to improve SOTA, for which the joint (x,y) distribution might be too big at the image level. On the other hand, I am not so familiar with this topic. If such experiments are quite normal in this area, then I am fine to increase the rating.


Review Point: There is a strong assumption that the mapping function during the few-shot task is one of the functions during training. Is this practical? The contrast learning only encourages those function representations to be different and it might not be able to learn the underlying function characteristics. Why can it help during down-stream few-shot tasks? In other words, I could use one-hot vector to represent each function. I could give the function index to the down-stream task. Is it helpful? Although contrast learning is hot, why must use contrast learning to learn function representation? You know the function labels, so you could also use triplet loss during the representation learning. Although the authors consider quite a few downstream tasks, all the comparisons are quite preliminary. It would be more convincing to show that in at least one task your approach can outperform SOTA or your function representation can improve SOTA. Figure 3(a) shows a better result with 50 shots, but with 200 shots. Why? There is no explanation for Fig. 2(b). It seems you use N samples to identify the function and the rest N-M samples to train the decoder. Is it correct? What's a good N here? What is the "S" operation? Eq. (4) does not look correct. Is Section 3.1 complete or not ? The organization is a bit wired. Update: I think the idea of this paper is very interesting. The authors' responses also address most of my concerns. The remaining issues are:
Review Point: 1) the writing needs to be improved to make the paper easy to read;
Review Point: 2) I am still not so convinced with the experiments. It would be more convincing if the authors can demonstrate this on few-shot image classification to improve SOTA, for which the joint (x,y) distribution might be too big at the image level. On the other hand, I am not so familiar with this topic. If such experiments are quite normal in this area, then I am fine to increase the rating.
==================================================

Focused review:

Weaknesses: 1. The introduction of the knowledge coefficient matrix may raise the concern of privacy leaks and the communication cost. 2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting. 3. To my best knowledge, there are some other personalized FL works (Hierarchical Personalized Federated Learning for User Modeling, WWW'21; Exploiting Shared Representations for Personalized Federated, arXiv:2102.07078), which need to be listed as baselines.


Review Point: 1. The introduction of the knowledge coefficient matrix may raise the concern of privacy leaks and the communication cost.
Review Point: 2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting.
Review Point: 3. To my best knowledge, there are some other personalized FL works (Hierarchical Personalized Federated Learning for User Modeling, WWW'21; Exploiting Shared Representations for Personalized Federated, arXiv:2102.07078), which need to be listed as baselines.
==================================================

Focused review:

weaknesses of their work?
The experiments are well designed and clearly described, and the authors discuss limitations/strengths/weaknesses/relation to prior work throughout the text. The claims are well supported experimentally, with two caveats:
Results are limited to a classification setting: As acknowledged by the authors, nearly all results are on image classification problems. While classification is a fair and reasonable problem setting, it is not likely to be the primary setting in which the method would find real world application. And there may be non-trivial issues with extending this work to other problems, e.g. object detection. (a) While patches may increase the saliency of an object, it may not help (and could even harm) prediction of bounding boxes or instance segmentations. (b) Multiple patches in the same image could conflict with each other or overwhelm other prediction signals. (The results of Fig. 5 are encouraging in that patches are shown to be effective improving classification scores without dominating the visual features of the object during classification, but further experiments are needed.)
Results are limited to a white-box network/dataset setting: Again this limitation is acknowledged and is not unreasonable (though some cited hypothetical examples, e.g. line 159, would be hard to fit in this setting). The results would be stronger with an examination of the effectiveness of patches learned on Network A when seen by Network B. Also of interest here is whether the dataset is important, e.g. can patches learned with a network trained on KITTI perform well when tested on a network trained for Cityscapes?
Clarity: Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)
The paper is well written and organized (including supplementary), figures are polished and clear, and it seems adequately informative for reproduction.
Significance: Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?
While there is little methodologically novel here, the task and experiments are quite interesting and raise practical questions for further research. For instance, QR tags (and similar) are quite common in robotics settings; could learning "unadversarial" patches be preferable in some settings? If these results hold for black-box network settings, what impact could this have on safety and robustness with autonomous agents? The experiments in this work are a useful start.
Bottom line I find the paper to be very well written, methodologically straightforward, but experimentally good, and potentially impactful both practically and toward further research. That said, the conclusions would be much more informative with two further experimental settings (object detection and transfer from network A to network B or dataset A to dataset B), and it is difficult to comprehensively judge the importance of this work without them.
Notes
Line 159: Designing road signs would likely require the "black box" network setting, so it doesn't seem to be a good example for the "fixed network" setting. Similar in line 309.
Fig. 4, Fig. 5, Fig. 6, and Fig. 9: Does "Accuracy" refer to "Top-1 Accuracy" in each of these cases? Only Fig. 7 specifically names "Top-1 Accuracy".
Line 181, "we construct unadversarial patches of varying size": Is there a single learned patch that is scaled to various sizes when pasted on test images?
Fig 5: "(red bards)" should be "(red bars)"
Supplementary, Line 732: "we fix the -a- set of patches to [a] predefined pattern"
The primary limitation addressed by the authors is that the method requires differentiability w.r.t. the patch or texture of interest. Implicit in this the need to access to the weights of a fixed, pre-trained network that is later used (unmodified) at test time. This latter implication is not always clearly considered, particularly when hypothesizing about applications of the work, as listed in the previous section. Additional care should be taken here.
As noted above, further limitations include demonstrating few results outside of classification problems. In particular, no results are shown with multiple visible patches or where size or pose of the object are predicted.
Regarding societal impact, all methods applied in this paper were first explored in the context of creating adversarial examples, designed to trick a classifier. This paper leverages such methods to improve classification when subjects can be modified, e.g. by applying a sticker to an object. The paper mentions the potential for misuse in this type of work and also notes some potential benefits when end users have the ability to increase or decrease classifier performance even without being able to control the classifier itself.


Review Point: of their work? The experiments are well designed and clearly described, and the authors discuss limitations/strengths/weaknesses/relation to prior work throughout the text. The claims are well supported experimentally, with two caveats: Results are limited to a classification setting: As acknowledged by the authors, nearly all results are on image classification problems. While classification is a fair and reasonable problem setting, it is not likely to be the primary setting in which the method would find real world application. And there may be non-trivial issues with extending this work to other problems, e.g. object detection. (a) While patches may increase the saliency of an object, it may not help (and could even harm) prediction of bounding boxes or instance segmentations. (b) Multiple patches in the same image could conflict with each other or overwhelm other prediction signals. (The results of Fig. 5 are encouraging in that patches are shown to be effective improving classification scores without dominating the visual features of the object during classification, but further experiments are needed.) Results are limited to a white-box network/dataset setting: Again this limitation is acknowledged and is not unreasonable (though some cited hypothetical examples, e.g. line 159, would be hard to fit in this setting). The results would be stronger with an examination of the effectiveness of patches learned on Network A when seen by Network B. Also of interest here is whether the dataset is important, e.g. can patches learned with a network trained on KITTI perform well when tested on a network trained for Cityscapes? Clarity: Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note that a superbly written paper provides enough information for an expert reader to reproduce its results.) The paper is well written and organized (including supplementary), figures are polished and clear, and it seems adequately informative for reproduction. Significance: Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach? While there is little methodologically novel here, the task and experiments are quite interesting and raise practical questions for further research. For instance, QR tags (and similar) are quite common in robotics settings; could learning "unadversarial" patches be preferable in some settings? If these results hold for black-box network settings, what impact could this have on safety and robustness with autonomous agents? The experiments in this work are a useful start. Bottom line I find the paper to be very well written, methodologically straightforward, but experimentally good, and potentially impactful both practically and toward further research. That said, the conclusions would be much more informative with two further experimental settings (object detection and transfer from network A to network B or dataset A to dataset B), and it is difficult to comprehensively judge the importance of this work without them. Notes Line 159: Designing road signs would likely require the "black box" network setting, so it doesn't seem to be a good example for the "fixed network" setting. Similar in line 309. Fig. 4, Fig. 5, Fig. 6, and Fig.
Review Point: 9: Does "Accuracy" refer to "Top-1 Accuracy" in each of these cases? Only Fig. 7 specifically names "Top-1 Accuracy". Line 181, "we construct unadversarial patches of varying size": Is there a single learned patch that is scaled to various sizes when pasted on test images? Fig 5: "(red bards)" should be "(red bars)" Supplementary, Line 732: "we fix the -a- set of patches to [a] predefined pattern" The primary limitation addressed by the authors is that the method requires differentiability w.r.t. the patch or texture of interest. Implicit in this the need to access to the weights of a fixed, pre-trained network that is later used (unmodified) at test time. This latter implication is not always clearly considered, particularly when hypothesizing about applications of the work, as listed in the previous section. Additional care should be taken here. As noted above, further limitations include demonstrating few results outside of classification problems. In particular, no results are shown with multiple visible patches or where size or pose of the object are predicted. Regarding societal impact, all methods applied in this paper were first explored in the context of creating adversarial examples, designed to trick a classifier. This paper leverages such methods to improve classification when subjects can be modified, e.g. by applying a sticker to an object. The paper mentions the potential for misuse in this type of work and also notes some potential benefits when end users have the ability to increase or decrease classifier performance even without being able to control the classifier itself.
==================================================

Focused review:

1. Some part of this paper is not clearly explained. For example, I guess people (at least for me ) in RL community are not familiar with formal verification. Can you give some simple example to intuitively explain how and why it works? 2. In section 3.1. the shield has the form of the piecewise linear policy. Does it work for complicated scenarios where the dimension of the state space is high? In that case the number of region is quite large. 3. In section 2, one assumption is that P\sharp is available in a closed form to the learner. Can you explain why it is reasonable? or give an example of P\sharp in your experiment.

Review Point: 1. Some part of this paper is not clearly explained. For example, I guess people (at least for me ) in RL community are not familiar with formal verification. Can you give some simple example to intuitively explain how and why it works?
Review Point: 2. In section 3.1. the shield has the form of the piecewise linear policy. Does it work for complicated scenarios where the dimension of the state space is high? In that case the number of region is quite large.
Review Point: 3. In section 2, one assumption is that P\sharp is available in a closed form to the learner. Can you explain why it is reasonable? or give an example of P\sharp in your experiment.
==================================================

Focused review:

Weaknesses:
Increasing diagonality of the attention maps has been observed in prior work as well (as the authors have cited and discussed). While the present work improves this understanding (mainly the phonetic information of the lower layers part), the analysis is not entirely novel.
There are several approaches to improving inference speed by directly addressing the attention map computation. For example, for ASR, prior work has looked at using masked attention where-in each frame only attends to L frames to left and R frames to the right [1, 2, 3]. Therefore, each frame is computing attention only using at (L + 1+ R) frames. This addresses the quadratic increase in computation.
Alternatively, Performers [4] also proposes linear time and space complexity for attention computation. It would be useful to have some of these comparisons to the presented method.
The authors’ analysis suggests that the attention map in lower layers is reusable, but those in the upper layers is not (Fig. 5(b)). But their best strategy ties maps for both lower and upper layers. This feels counterintuitive. Is there a reason why that is so?
How much of the analysis is tied to the CTC-ASR model used? What about LAS [5] or RNN-T [6] models, which are widely used as well?
The results on Librispeech are much worse than other state-of-the-art (SOTA) end-to-end models (e.g., [7]), most likely because the authors use CTC. It would be useful to see if the techniques result in SOTA performance when using LAS or RNN-T.
Other remarks:
Figure 1 has too much information and is a little hard to follow. Consider either improving the figure description, or splitting into multiple parts.
Sec. 3.2: The diagonality analysis is not that different from Zhang et al., and arrives at very similar conclusions. For the models authors consider, are the plots different when using the metric in Zhang et al.?
Sec. 3.2: Furthermore, it’s not very clear from the description what r(T - 1) is.
For the plots, is PAR and CAD summarized per utterance or per corpora?
Figure 6 and the description is hard to follow. What does it mean to set averaged PAR of the baseline to 1.0? Consider moving some of the details from the Appendix to the Sec. 3.4 since PAR is one of the main proposals of the paper.
Figure 6: The authors claim that a coverage of 0.974 is bad. It’s unclear why. 0.974 seems quite high.
Does adding more heads to the baseline improve performance? It seems like using 8 heads works the best for the shared attention map model, but the baseline only uses 4.
Minor:
Abstract: “We propose a novel metric …” The sentence is too long and a little hard to follow, especially with so little context since it is appearing in the abstract.
Abstract: standardizes the phonetic variance -> reduces the phonetic variance?
Introduction: 30 seconds … 750 frames: Will be worth adding what is the processing window size (40 msec?).
Sec. 2.1: (d_h)^2: Consider using special chars like \dag or \ddag for footnotes to avoid confusion. As of now, it looks like d_h is squared.
Sec. 3.2: What does h stand for in CAD_h?
Sec 3.3: (1) First, … (2) Second, .. : First and Second are redundant.
Consider using just 1 decimal point for results. 2 decimal points does not really add a lot of value, comparison-wise.
References:
[1] Zhang, Qian, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar. "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss." In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7829-7833. IEEE, 2020.
[2] Audhkhasi, Kartik, Tongzhou Chen, Bhuvana Ramabhadran, and Pedro J. Moreno. "Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition." Proc. Interspeech 2021 (2021): 1812-1816.
[3] Tripathi, Anshuman, Jaeyoung Kim, Qian Zhang, Han Lu, and Hasim Sak. "Transformer transducer: One model unifying streaming and non-streaming speech recognition." arXiv preprint arXiv:2010.03192 (2020).
[4] Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins et al. "Rethinking attention with performers." arXiv preprint arXiv:2009.14794 (2020).
[5] Chan, William, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition." In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4960-4964. IEEE, 2016.
[6] Graves, Alex. "Sequence transduction with recurrent neural networks." arXiv preprint arXiv:1211.3711 (2012).
[7] Gulati, Anmol, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han et al. "Conformer: Convolution-augmented transformer for speech recognition." arXiv preprint arXiv:2005.08100 (2020).


Review Point: Increasing diagonality of the attention maps has been observed in prior work as well (as the authors have cited and discussed). While the present work improves this understanding (mainly the phonetic information of the lower layers part), the analysis is not entirely novel. There are several approaches to improving inference speed by directly addressing the attention map computation. For example, for ASR, prior work has looked at using masked attention where-in each frame only attends to L frames to left and R frames to the right [1, 2, 3]. Therefore, each frame is computing attention only using at (L + 1+ R) frames. This addresses the quadratic increase in computation. Alternatively, Performers [4] also proposes linear time and space complexity for attention computation. It would be useful to have some of these comparisons to the presented method. The authors’ analysis suggests that the attention map in lower layers is reusable, but those in the upper layers is not (Fig. 5(b)). But their best strategy ties maps for both lower and upper layers. This feels counterintuitive. Is there a reason why that is so? How much of the analysis is tied to the CTC-ASR model used? What about LAS [5] or RNN-T [6] models, which are widely used as well? The results on Librispeech are much worse than other state-of-the-art (SOTA) end-to-end models (e.g., [7]), most likely because the authors use CTC. It would be useful to see if the techniques result in SOTA performance when using LAS or RNN-T. Other remarks: Figure 1 has too much information and is a little hard to follow. Consider either improving the figure description, or splitting into multiple parts. Sec. 3.2: The diagonality analysis is not that different from Zhang et al., and arrives at very similar conclusions. For the models authors consider, are the plots different when using the metric in Zhang et al.? Sec. 3.2: Furthermore, it’s not very clear from the description what r(T - 1) is. For the plots, is PAR and CAD summarized per utterance or per corpora? Figure 6 and the description is hard to follow. What does it mean to set averaged PAR of the baseline to 1.0? Consider moving some of the details from the Appendix to the Sec. 3.4 since PAR is one of the main proposals of the paper. Figure 6: The authors claim that a coverage of 0.974 is bad. It’s unclear why. 0.974 seems quite high. Does adding more heads to the baseline improve performance? It seems like using 8 heads works the best for the shared attention map model, but the baseline only uses 4. Minor: Abstract: “We propose a novel metric …” The sentence is too long and a little hard to follow, especially with so little context since it is appearing in the abstract. Abstract: standardizes the phonetic variance -> reduces the phonetic variance? Introduction: 30 seconds … 750 frames: Will be worth adding what is the processing window size (40 msec?). Sec. 2.1: (d_h)^2: Consider using special chars like \dag or \ddag for footnotes to avoid confusion. As of now, it looks like d_h is squared. Sec. 3.2: What does h stand for in CAD_h? Sec 3.3: (1) First, … (2) Second, .. : First and Second are redundant. Consider using just 1 decimal point for results.
Review Point: 2 decimal points does not really add a lot of value, comparison-wise. References: [1] Zhang, Qian, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar. "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss." In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7829-7833. IEEE, 2020. [2] Audhkhasi, Kartik, Tongzhou Chen, Bhuvana Ramabhadran, and Pedro J. Moreno. "Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition." Proc. Interspeech 2021 (2021): 1812-1816. [3] Tripathi, Anshuman, Jaeyoung Kim, Qian Zhang, Han Lu, and Hasim Sak. "Transformer transducer: One model unifying streaming and non-streaming speech recognition." arXiv preprint arXiv:2010.03192 (2020). [4] Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins et al. "Rethinking attention with performers." arXiv preprint arXiv:2009.14794 (2020). [5] Chan, William, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition." In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4960-4964. IEEE, 2016. [6] Graves, Alex. "Sequence transduction with recurrent neural networks." arXiv preprint arXiv:1211.3711 (2012). [7] Gulati, Anmol, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han et al. "Conformer: Convolution-augmented transformer for speech recognition." arXiv preprint arXiv:2005.08100 (2020).
==================================================

Focused review:

It is unclear why we need to define $g_k(x)$ as in Algorithm 1. The authors may want to clarify the motivation behind it. All $g(x)$ in the experiments are MCP penalty. The authors may want to conduct more experiments with other penalties, such as SCAD and Exp in Table 2. Though the paper is well written overall, some sentences and formulations are confusing. 1. In the abstract, the sentence “We also establish new convergence complexities to achieve an approximate KKT solution when the objective can be smooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity that is on a par with gradient descent when applied to nonconvex regularized problem” is confusing. 2. The authors may want to change the notation $\psi’(x)$ in the inequality (6) to $\partial \psi(x), as they have assumed that $\psi(x)$ can be nonsmooth.

Review Point: It is unclear why we need to define $g_k(x)$ as in Algorithm 1. The authors may want to clarify the motivation behind it. All $g(x)$ in the experiments are MCP penalty. The authors may want to conduct more experiments with other penalties, such as SCAD and Exp in Table 2. Though the paper is well written overall, some sentences and formulations are confusing.
Review Point: 1. In the abstract, the sentence “We also establish new convergence complexities to achieve an approximate KKT solution when the objective can be smooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity that is on a par with gradient descent when applied to nonconvex regularized problem” is confusing.
Review Point: 2. The authors may want to change the notation $\psi’(x)$ in the inequality (6) to $\partial \psi(x), as they have assumed that $\psi(x)$ can be nonsmooth.
==================================================

Focused review:

1. The mathematical rigorousness renders the reading much more difficult which I don't think it's necessary for a ML paper. 2. The connection between the discretized versions and Laplace regularization for semi-supervised learning should be made more clear. It was not clear for me why it is relevant to consider the discretized versions. 3. Since the loss minimization problems are wrt functions, the problems are convex, but in practice, we don't optimize high-dimensional functions directly; instead, a parameterization of the function is introduced and the loss minimization wrt the parameters is not necessarily convex. Can the results of this paper be extended to those cases? Are these results applicable for neural networks (since the main applications of Lip constraints are there)?

Review Point: 1. The mathematical rigorousness renders the reading much more difficult which I don't think it's necessary for a ML paper.
Review Point: 2. The connection between the discretized versions and Laplace regularization for semi-supervised learning should be made more clear. It was not clear for me why it is relevant to consider the discretized versions.
Review Point: 3. Since the loss minimization problems are wrt functions, the problems are convex, but in practice, we don't optimize high-dimensional functions directly; instead, a parameterization of the function is introduced and the loss minimization wrt the parameters is not necessarily convex. Can the results of this paper be extended to those cases? Are these results applicable for neural networks (since the main applications of Lip constraints are there)?
==================================================

Focused review:

This paper does not have any glaring concerns in my opinion.
Weaknesses - It is not clear if the defense success is depending on the neural architecture specifically the solutions the network is capable of representing. Are alternative networks, capable of representing higher order functions, still capable of exploiting the defensed embedding? In other words, is the robustness of your approach dependent on the representational capacity of attacker?
- The approach was applied to a sequence level representation defined in the end of text token in GPT-2. Is it conceivable that the attacker could still attack the individual token level representations? A defense of these token representations would necessitate larger changes to the underlying DialogGPT model. 
Missing reference for "First there is no existing data that can be used to quantify how much private information is revealed by a LM". Maybe rephrase as, "no standalone dataset to evaluate privacy of training data in a LM"? 
Carlini, Nicholas, et al. "Extracting training data from large language models." 30th USENIX Security Symposium (USENIX Security 21). 2021.
Explain Distinct-1 and Distinct-2. It is a key column in your evaluation but is not explained at all. Just say Distinct-1 and Distinct-2 are the number of distinct unigrams and bigrams divided by total number of words.
Is this an error? 
"Here the defender owns 6,654 conversations with personal labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7." 
Should this be "the adversary holds 3,753 dialogues with persona labels ranging from 0 to 3."?
KL loss equation The dropped constant C in the log is different from the mean reduction 1/C sum 0-(C-1). Due to the notation, the reduction where one C term (the constant in KL) is removed is unclear. Maybe change the mean reduction to another letter such as M or N. 

Review Point: This paper does not have any glaring concerns in my opinion. Weaknesses - It is not clear if the defense success is depending on the neural architecture specifically the solutions the network is capable of representing. Are alternative networks, capable of representing higher order functions, still capable of exploiting the defensed embedding? In other words, is the robustness of your approach dependent on the representational capacity of attacker?
Review Point: - The approach was applied to a sequence level representation defined in the end of text token in GPT-2. Is it conceivable that the attacker could still attack the individual token level representations? A defense of these token representations would necessitate larger changes to the underlying DialogGPT model. Missing reference for "First there is no existing data that can be used to quantify how much private information is revealed by a LM". Maybe rephrase as, "no standalone dataset to evaluate privacy of training data in a LM"? Carlini, Nicholas, et al. "Extracting training data from large language models." 30th USENIX Security Symposium (USENIX Security 21). 2021. Explain Distinct-1 and Distinct-2. It is a key column in your evaluation but is not explained at all. Just say Distinct-1 and Distinct-2 are the number of distinct unigrams and bigrams divided by total number of words. Is this an error? "Here the defender owns 6,654 conversations with personal labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7." Should this be "the adversary holds 3,753 dialogues with persona labels ranging from 0 to 3."? KL loss equation The dropped constant C in the log is different from the mean reduction 1/C sum 0-(C-1). Due to the notation, the reduction where one C term (the constant in KL) is removed is unclear. Maybe change the mean reduction to another letter such as M or N.
==================================================

Focused review:

1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers). 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim. 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score. 4. Were other architectures explored besides WideResNet? 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well. 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?

Review Point: 1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers).
Review Point: 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim.
Review Point: 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score.
Review Point: 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well.
Review Point: 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?
==================================================

Focused review:

Weakness:
W1. I’ve found the structure of the paper not completely clear. In particular I don’t completely understand the order of the subsections in Sect 3: for example, what section 3.2 refers to? Why putting “consistency” before the algorithm and not in the section “Properties of the algorithm” (last subsection)? In Section 3.1 I would also add the description of the SRNN for classification: the proposed approach is an extension of it, thus it would be beneficial to summarize it.
W2. The section 2 “Related Work” contains a general introduction to the regression problem. I think this is too general, and not relevant for the paper. On the other hand, it does not contain the description of works which are more strictly related to the presented method, such as the general nearest neighbour strategies and especially the method in Huang (2017): these have been only cited at the end of the section. In particular I think it is fundamental to define the differences between the proposed approach and that of Huang (2017) – authors only reported a general sentence (“However, the proposed algorithm does not have guarantee of convergence or achieving some sort of optimum solution.”), without describing the algorithm and the idea. Finally, I suggest authors to clearly discuss the differences with the work of Tavallali et al 2020b, which has been cited in many different places inside the manuscript.
W3. The authors use a K-means like strategy. Many methods in the literature used such generalized strategies, I think authors should at least acknowledge some of them. Moreover, it is clear from such methods that the initialization is always an issue. How did authors solve it? With the procedure described in Section 3.2? If so, how did they initialize the k-means inside it? How many repetitions? Which is the impact of this procedure (also empirically)? Please see also the discussion in:
P Fränti, S Sieranoja: How much can k-means be improved by using better initialization and repeats? Pattern Recognition, 2019
W4. How did author face the problem of determining the number of centroids K? How relevant is this choice? I suggest the authors to at least discuss this crucial issue
W5. I think the experimental part can be largely improved. Here are some comments:
Many important details are missing. For example:
Details on datasets: number and (at least) names of the datasets, characteristics in terms of number of features, number of objects, dimensions of training and testing sets (if any) and so on
Details on protocols: evaluation metric, details on Cross Val (e.g. how many repetitions of 80%/20%?)
Details on the proposed approach: initialization, number of iterations, different options investigated, impact on the performances and so on. For example, authors used K=4 for one dataset and K=2 for the others: what if we change this value? How critical is this choice?
What is the meaning of “We used various setups for each dataset to achieve the smallest possible train error.”? Which setups? Is this behaviour fair? Please add a justification
For what concerns the chosen competitors: instead of using these general methods, I suggest the authors to add a comparison with some of the original NN regression methods and with the method of Huang, which represents the most similar approach in the literature.
Why authors consider the different clusters in their approach as “base models” (like trees in forests?)
Plots reproduced in fig 1 are very difficult to read. Even zooming the pdf their presentation remains obscure (e.g. the colors of the different competitors are too similar)
Comments on the results are missing: there is only one very general sentence (“As can be observed from figures 1, and 2, the SRNN-Reg was able to achieve better or comparable train and test errors to other models.”). Example of aspects which can be discussed are:
how does the method work in the different datasets? Any relation with the different characteristics?
which are the best competitors?
why in some plots we only have 6 or 7 lines?
why in some plots we have horizontal lines?


Review Point: W1. I’ve found the structure of the paper not completely clear. In particular I don’t completely understand the order of the subsections in Sect 3: for example, what section 3.2 refers to? Why putting “consistency” before the algorithm and not in the section “Properties of the algorithm” (last subsection)? In Section 3.1 I would also add the description of the SRNN for classification: the proposed approach is an extension of it, thus it would be beneficial to summarize it.
Review Point: W2. The section 2 “Related Work” contains a general introduction to the regression problem. I think this is too general, and not relevant for the paper. On the other hand, it does not contain the description of works which are more strictly related to the presented method, such as the general nearest neighbour strategies and especially the method in Huang (2017): these have been only cited at the end of the section. In particular I think it is fundamental to define the differences between the proposed approach and that of Huang (2017) – authors only reported a general sentence (“However, the proposed algorithm does not have guarantee of convergence or achieving some sort of optimum solution.”), without describing the algorithm and the idea. Finally, I suggest authors to clearly discuss the differences with the work of Tavallali et al 2020b, which has been cited in many different places inside the manuscript.
Review Point: W3. The authors use a K-means like strategy. Many methods in the literature used such generalized strategies, I think authors should at least acknowledge some of them. Moreover, it is clear from such methods that the initialization is always an issue. How did authors solve it? With the procedure described in Section 3.2? If so, how did they initialize the k-means inside it? How many repetitions? Which is the impact of this procedure (also empirically)? Please see also the discussion in: P Fränti, S Sieranoja: How much can k-means be improved by using better initialization and repeats? Pattern Recognition, 2019 W4. How did author face the problem of determining the number of centroids K? How relevant is this choice? I suggest the authors to at least discuss this crucial issue W5. I think the experimental part can be largely improved. Here are some comments: Many important details are missing. For example: Details on datasets: number and (at least) names of the datasets, characteristics in terms of number of features, number of objects, dimensions of training and testing sets (if any) and so on Details on protocols: evaluation metric, details on Cross Val (e.g. how many repetitions of 80%/20%?) Details on the proposed approach: initialization, number of iterations, different options investigated, impact on the performances and so on. For example, authors used K=4 for one dataset and K=2 for the others: what if we change this value? How critical is this choice? What is the meaning of “We used various setups for each dataset to achieve the smallest possible train error.”? Which setups? Is this behaviour fair? Please add a justification For what concerns the chosen competitors: instead of using these general methods, I suggest the authors to add a comparison with some of the original NN regression methods and with the method of Huang, which represents the most similar approach in the literature. Why authors consider the different clusters in their approach as “base models” (like trees in forests?) Plots reproduced in fig 1 are very difficult to read. Even zooming the pdf their presentation remains obscure (e.g. the colors of the different competitors are too similar) Comments on the results are missing: there is only one very general sentence (“As can be observed from figures 1, and 2, the SRNN-Reg was able to achieve better or comparable train and test errors to other models.”). Example of aspects which can be discussed are: how does the method work in the different datasets? Any relation with the different characteristics? which are the best competitors? why in some plots we only have 6 or 7 lines? why in some plots we have horizontal lines?
==================================================

Focused review:

Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as "undecided", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting. - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the "raw" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening? - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have "training the neural representation" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization? - More a comment: I found the "Algorithm 1" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.

Review Point: Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as "undecided", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting.
Review Point: - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the "raw" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening?
Review Point: - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have "training the neural representation" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization?
Review Point: - More a comment: I found the "Algorithm 1" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.
==================================================

Focused review:

I only have one major comment, about the GRE assumption: This assumption, in Theorem 1, actually depends on the error of the initial estimate (since the set \mathcal B depends on \Detla), and hence, it seems pretty artificial to me. Could the authors elaborate a bit? For instance, if the entries of the matrix X are iid Gaussian and independent of the initial estimate, does the Assumption hold with high probability? A few minor comments: 1. In theorem 2, could you be a little more precise with the asymptotics (what quantities grow to infinity among n, p, s, and how?) 2. Line 164, replace \alpha with n in the subscript of \nu. 3. Line 179, there is a typo in the word "unchanging" 4. Right after Theorem 4, could you elaborate a little more about why it is useful?

Review Point: I only have one major comment, about the GRE assumption: This assumption, in Theorem 1, actually depends on the error of the initial estimate (since the set \mathcal B depends on \Detla), and hence, it seems pretty artificial to me. Could the authors elaborate a bit? For instance, if the entries of the matrix X are iid Gaussian and independent of the initial estimate, does the Assumption hold with high probability? A few minor comments:
Review Point: 1. In theorem 2, could you be a little more precise with the asymptotics (what quantities grow to infinity among n, p, s, and how?) 2. Line 164, replace \alpha with n in the subscript of \nu.
Review Point: 3. Line 179, there is a typo in the word "unchanging" 4. Right after Theorem 4, could you elaborate a little more about why it is useful?
==================================================

Focused review:

Weakness:
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
- Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4.

Comments:
The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained.  



Review Point: - The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
Review Point: - Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4. Comments: The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained.
==================================================

Focused review:

Weaknesses:
Authors compare with only ARPL and OpenHybrid approaches because they say that the other approaches perform lower on open-set benchmarks. I think it is still important to consider also the other methods especially because the authors propose new benchmarks and on these benchmarks the ranking between the methods changes. So it could be for the other methods. I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3).
In Fig.2 I would expect to see also results for OpenHybrid. Also, in the figure, for the simplest datasets (with performance close to 100) there is not much correlation between open-set can close-set performance as ARPL does not improve over cross-entropy on the close-set scenario. It is something that should be mentioned.
Results in Table 1 for the cross-entropy are presented with a ranking based on the classifier logits, which in my opinion makes more sense. However, as previous works use the softmax scores, it is important to compare the two ways. I see a comparison in Appendix B (Fig. 6c), but this is only for a model. I would like to see this comparison at least for the models in tab. 1.
Additional questions/comments:
Fig.1a presents ARPL in the legend, without really having yet introduced the method.
The authors propose a clear distinction between open-set recognition and out-of-distribution. However, is this distinction really necessary. Could not the two fields be unified? It would be more interesting to know if the same conclusions of this paper are valid also for out-of-distribution problems.
The "more theoretical" justification of the results at bottom of page 4 does not add much, but I do not have suggestions on how to improve it.
In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open-set scenario on ImageNet. Is it due to the fact that it has been trained on more data or it is the reduced inductive bias (no convolutions)? It would be interesting to see if with different sizes of ViT, the correlation between open and close-set scenario still holds.


Review Point: Authors compare with only ARPL and OpenHybrid approaches because they say that the other approaches perform lower on open-set benchmarks. I think it is still important to consider also the other methods especially because the authors propose new benchmarks and on these benchmarks the ranking between the methods changes. So it could be for the other methods. I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3). In Fig.2 I would expect to see also results for OpenHybrid. Also, in the figure, for the simplest datasets (with performance close to 100) there is not much correlation between open-set can close-set performance as ARPL does not improve over cross-entropy on the close-set scenario. It is something that should be mentioned. Results in Table 1 for the cross-entropy are presented with a ranking based on the classifier logits, which in my opinion makes more sense. However, as previous works use the softmax scores, it is important to compare the two ways. I see a comparison in Appendix B (Fig. 6c), but this is only for a model. I would like to see this comparison at least for the models in tab.
Review Point: 1. Additional questions/comments: Fig.1a presents ARPL in the legend, without really having yet introduced the method. The authors propose a clear distinction between open-set recognition and out-of-distribution. However, is this distinction really necessary. Could not the two fields be unified? It would be more interesting to know if the same conclusions of this paper are valid also for out-of-distribution problems. The "more theoretical" justification of the results at bottom of page 4 does not add much, but I do not have suggestions on how to improve it. In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open-set scenario on ImageNet. Is it due to the fact that it has been trained on more data or it is the reduced inductive bias (no convolutions)? It would be interesting to see if with different sizes of ViT, the correlation between open and close-set scenario still holds.
==================================================

Focused review:

1. In experiments of learning with noisy labels, the baseline algorithms are not latest. 2. Furthermore, the datasets are not enough. Should have a non-image dataset. 3. The distribution shift is synthetic. Should have a real-world dataset.

Review Point: 1. In experiments of learning with noisy labels, the baseline algorithms are not latest.
Review Point: 2. Furthermore, the datasets are not enough. Should have a non-image dataset.
Review Point: 3. The distribution shift is synthetic. Should have a real-world dataset.
==================================================

Focused review:

The biggest limitations of this work are that: 1) I feel that their assumption 1 (essentially, that every classifier uses at leats one unique feature) seems overly restrictive. It is not obvious to me that this is strictly necessary. For example, consider 3 2-sparse classifiers that use features (1,2), (2,3), and (1,3). Is it really impossible to learn to distinguish these? 2) There are no simulations provided, so it is hard to say how this would work in practice.

Review Point: 1) I feel that their assumption 1 (essentially, that every classifier uses at leats one unique feature) seems overly restrictive. It is not obvious to me that this is strictly necessary. For example, consider 3 2-sparse classifiers that use features (1,2), (2,3), and (1,3). Is it really impossible to learn to distinguish these?
Review Point: 2) There are no simulations provided, so it is hard to say how this would work in practice.
==================================================

Focused review:

1. This paper only compares the running times, F1 scores, and conductance values of the proposed SLQ algorithm with ACL and CRD, more comparisons with other existing methods are needed for better evaluations. 2. The performance of SLQ (quantified by F1 score and conductance values) appears to have limited competitiveness compared with previous CRD method. In Figure 3, SLQ shows worse performance than CRD; while in Figure 4, the improvement of SLQ over ACL appear to be very small. 3. The notations are used inconsistently: e.g., $T$ in Theorem 3.1 refers to the # of iterations; however, on lines 195-196, Assumptions 1 and 2, and Theorem 4.1, $T$ is used to refer to as a cluster. 4. The number of datasets tested in this paper is also very limited. ** UPDATE ** I have read the responses of the authors and all reviews. I would like to thank the authors for responding. I think that my evaluation is proper for this paper, considering the main idea of a simple generalization of an existing optimization formulation, the scope of experimental validation, and the presentation. Also, it is not clear about how useful the theorem is for real datasets.

Review Point: 1. This paper only compares the running times, F1 scores, and conductance values of the proposed SLQ algorithm with ACL and CRD, more comparisons with other existing methods are needed for better evaluations.
Review Point: 2. The performance of SLQ (quantified by F1 score and conductance values) appears to have limited competitiveness compared with previous CRD method. In Figure 3, SLQ shows worse performance than CRD; while in Figure 4, the improvement of SLQ over ACL appear to be very small.
Review Point: 3. The notations are used inconsistently: e.g., $T$ in Theorem 3.1 refers to the # of iterations; however, on lines 195-196, Assumptions 1 and 2, and Theorem 4.1, $T$ is used to refer to as a cluster.
Review Point: 4. The number of datasets tested in this paper is also very limited. ** UPDATE ** I have read the responses of the authors and all reviews. I would like to thank the authors for responding. I think that my evaluation is proper for this paper, considering the main idea of a simple generalization of an existing optimization formulation, the scope of experimental validation, and the presentation. Also, it is not clear about how useful the theorem is for real datasets.
==================================================

Focused review:

Weakness: despite the rigorous mathematical derivation, I am not sure how the presented PAC analysis result helps the community. One can directly apply the sample complexity analysis on the adversarial loss (say via Rademacher complexity). Does the presented result provide more insights or a better rate?
Weakness: the paper doesn't actually provide an executable algorithm, but only an existence results, i.e., the existence of measure mu, the existence of sample compression scheme. Whether the rate results pair with a computational feasible algorithm is unclear, and there is no simulation to demonstrate empirical performance/comparison. 4 Weakness: in conclusion, the paper neither provide a theoretical information limit result, nor an executable adversarial training algorithm.


Review Point: despite the rigorous mathematical derivation, I am not sure how the presented PAC analysis result helps the community. One can directly apply the sample complexity analysis on the adversarial loss (say via Rademacher complexity). Does the presented result provide more insights or a better rate? Weakness: the paper doesn't actually provide an executable algorithm, but only an existence results, i.e., the existence of measure mu, the existence of sample compression scheme. Whether the rate results pair with a computational feasible algorithm is unclear, and there is no simulation to demonstrate empirical performance/comparison.
Review Point: 4 Weakness: in conclusion, the paper neither provide a theoretical information limit result, nor an executable adversarial training algorithm.
==================================================

Focused review:

Weakness - One issue I find with the paper writing is this paper is not self-contained. In Table 1, a minimal introduction to iCaRL and CCIL is not missing, which makes it difficult to get the idea in section 3.1 before reviewing prior papers. Similarly, L169, minimal introduction to LwF-MC is not included. - The proposed SemanticAug seems to be an incremental contribution as it resembles ISDA [24] a lot. While the discussions at L255 are valid, the adaptation of ISDA idea in incremental classification seems to be straight-forward. - Some technical exposition is incomplete. o Fig 2 (b), when data augmentation is used, such as mixup, cutmix, it often requires more training epochs to learn a better model. Can you clarify whether results with data augmentation in Fig 2 (b) are obtained with more training epochs? o L217, how many augmented classes (i.e. m) are added? Any data sampling technique is used to balance original new classes and augmented new classes? how many images are generated for each augmented class? o L239, for each old class, do you fix the M deep features once they are generated from a normal distribution with fixed mean/covariance? Since backbone is also updated as more new tasks are added, the mean/covariance will be outdated. o L292, what is “herd” selection technique?
The chance of having negative social impact is low. So it is ok authors do not address them.


Review Point: - One issue I find with the paper writing is this paper is not self-contained. In Table 1, a minimal introduction to iCaRL and CCIL is not missing, which makes it difficult to get the idea in section 3.1 before reviewing prior papers. Similarly, L169, minimal introduction to LwF-MC is not included.
Review Point: - The proposed SemanticAug seems to be an incremental contribution as it resembles ISDA [24] a lot. While the discussions at L255 are valid, the adaptation of ISDA idea in incremental classification seems to be straight-forward.
Review Point: - Some technical exposition is incomplete. o Fig 2 (b), when data augmentation is used, such as mixup, cutmix, it often requires more training epochs to learn a better model. Can you clarify whether results with data augmentation in Fig 2 (b) are obtained with more training epochs? o L217, how many augmented classes (i.e. m) are added? Any data sampling technique is used to balance original new classes and augmented new classes? how many images are generated for each augmented class? o L239, for each old class, do you fix the M deep features once they are generated from a normal distribution with fixed mean/covariance? Since backbone is also updated as more new tasks are added, the mean/covariance will be outdated. o L292, what is “herd” selection technique? The chance of having negative social impact is low. So it is ok authors do not address them.
==================================================

Focused review:

Weaknesses
W1: Technical Depth: The main weakness of the paper (in my opinion) is the lack of technical depth. The idea is natural, but the technical content is light and technical novelty is low. Here are some examples of increasing the technical depth:
What is convergence behavior of the resulting optimization algorithm (even in simple special cases)?
What are the privacy implications of this approach? For instance, sending projected parameters seems bad for privacy. Is the privacy-utility tradeoff of the proposed method better or worse than the usual approach under differential privacy?
What is the generalization behavior of this approach?
How does the algorithm behave when only some clients are sampled in each round?
The expectation is not that a paper solves all these questions. Rather, exploring some question of this sort can more richly convey the benefits and limitations of the idea along with deeper insights.
W2: Possible flaw (please correct me if I'm wrong): the target of the linear regression problem are higher in dimension than the inputs:
The target
Y
is of size
d
×
k
×
N
, while the input
V
is
o
×
k
×
N
. In MNIST, for instance,
d
=
784
while
o
=
10
for a single layer model.
Thus, there could be an infinite number of possible solutions that exactly interpolate the data. Why is the proposed algorithm reasonable in this case?
W3: Some choices are puzzling: Some of the design choices taken in the paper are not well explained and seem puzzling. Here are some examples:
Why is compressing the output dimension the right kind of compression (section 4.2.2)? Why not compress the inputs? Why not flatten all the weights into a vector and compress them? It would be helpful to explore these options numerically (preferable) or at least a discussion of why the first approach is the right one is in order.
Multi-layer setting: same as before, why not concatenate all of the layers into one vector and compress this? Why is your layer-wise approach better?
Sparse models (Section 5.3): why is this kind of sparsity the right one? I would argue that sparsity of the updates uploaded to the server is more important then the sparsity of the weight vector broadcast from the server (which is what seems to me to be provided by Section 5.3). In mobile phones, for instance, the upload speed is usually the bottleneck while the download speed is usually much higher.
Sparsity: the paper denotes weights smaller than
10
−
6
as sparse. How did you arrive at this threshold? What does the performance look like when you actually zero out these small entries?
W4: issues with experiments:
(major issue): the data appears to be split i.i.d. across clients for most of the experiments (I'm guessing this from Appendix A.4.2, correct me if I'm mistaken). This makes the experiments less relevant to federated learning in practice. I would recommend using a non-i.i.d. split by default.
In general, the details of the per-client partitioning must be specified more prominently.
baselines are missing. For instance, [1, 2] for robust aggregation and [3] for compression
References:
[1] Yin et al, ICML (2018). Byzantine-robust distributed learning: Towards optimal statistical rates.
[2] Pillutla et al, IEEE Transactions on Signal Processing (2022). Robust Aggregation for Federated Learning.
[3] Konecny et al (2016). Federated learning: Strategies for improving communication efficiency.


Review Point: W1: Technical Depth: The main weakness of the paper (in my opinion) is the lack of technical depth. The idea is natural, but the technical content is light and technical novelty is low. Here are some examples of increasing the technical depth: What is convergence behavior of the resulting optimization algorithm (even in simple special cases)? What are the privacy implications of this approach? For instance, sending projected parameters seems bad for privacy. Is the privacy-utility tradeoff of the proposed method better or worse than the usual approach under differential privacy? What is the generalization behavior of this approach? How does the algorithm behave when only some clients are sampled in each round? The expectation is not that a paper solves all these questions. Rather, exploring some question of this sort can more richly convey the benefits and limitations of the idea along with deeper insights.
Review Point: W2: Possible flaw (please correct me if I'm wrong): the target of the linear regression problem are higher in dimension than the inputs: The target Y is of size d × k × N , while the input V is o × k × N . In MNIST, for instance, d = 784 while o = 10 for a single layer model. Thus, there could be an infinite number of possible solutions that exactly interpolate the data. Why is the proposed algorithm reasonable in this case?
Review Point: W3: Some choices are puzzling: Some of the design choices taken in the paper are not well explained and seem puzzling. Here are some examples: Why is compressing the output dimension the right kind of compression (section 4.2.2)? Why not compress the inputs? Why not flatten all the weights into a vector and compress them? It would be helpful to explore these options numerically (preferable) or at least a discussion of why the first approach is the right one is in order. Multi-layer setting: same as before, why not concatenate all of the layers into one vector and compress this? Why is your layer-wise approach better? Sparse models (Section 5.3): why is this kind of sparsity the right one? I would argue that sparsity of the updates uploaded to the server is more important then the sparsity of the weight vector broadcast from the server (which is what seems to me to be provided by Section 5.3). In mobile phones, for instance, the upload speed is usually the bottleneck while the download speed is usually much higher. Sparsity: the paper denotes weights smaller than 10 − 6 as sparse. How did you arrive at this threshold? What does the performance look like when you actually zero out these small entries?
Review Point: W4: issues with experiments: (major issue): the data appears to be split i.i.d. across clients for most of the experiments (I'm guessing this from Appendix A.4.2, correct me if I'm mistaken). This makes the experiments less relevant to federated learning in practice. I would recommend using a non-i.i.d. split by default. In general, the details of the per-client partitioning must be specified more prominently. baselines are missing. For instance, [1, 2] for robust aggregation and [3] for compression References: [1] Yin et al, ICML (2018). Byzantine-robust distributed learning: Towards optimal statistical rates. [2] Pillutla et al, IEEE Transactions on Signal Processing (2022). Robust Aggregation for Federated Learning. [3] Konecny et al (2016). Federated learning: Strategies for improving communication efficiency.
==================================================

Focused review:

Weakness: 1. The conclusion seems to be only for GCN. I wonder GAT[1] may exhibit a smaller degree bias, even smaller than graph contrastive learning methods. 2. From Figure 6 in Appendix A, the advantage of graph contrastive learning methods over GCN on Photo dataset is not obvious. The numerical values of their slopes are close. 3. There is a small gap between degree bias and theoretical analysis of clear community structure. 4. The improvement of the proposed method in Table 1 does not seem statistically significant because of high variance. 5. There are some related works designed for degree bias, such as SL-DSGCN[2]. But these methods are not set as baselines in the experimental comparison.
[1] Veličković P, Cucurull G, Casanova A, et al. Graph Attention Networks[C]//International Conference on Learning Representations. 2018. [2] Tang X, Yao H, Sun Y, et al. Investigating and mitigating degree-related biases in graph convoltuional networks[C]//Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020: 1435-1444.
In addition to the limitations mentioned in the paper, the generalization of the conclusion should be taken into consideration.


Review Point: 1. The conclusion seems to be only for GCN. I wonder GAT[1] may exhibit a smaller degree bias, even smaller than graph contrastive learning methods.
Review Point: 2. From Figure 6 in Appendix A, the advantage of graph contrastive learning methods over GCN on Photo dataset is not obvious. The numerical values of their slopes are close.
Review Point: 3. There is a small gap between degree bias and theoretical analysis of clear community structure.
Review Point: 4. The improvement of the proposed method in Table 1 does not seem statistically significant because of high variance.
Review Point: 5. There are some related works designed for degree bias, such as SL-DSGCN[2]. But these methods are not set as baselines in the experimental comparison. [1] Veličković P, Cucurull G, Casanova A, et al. Graph Attention Networks[C]//International Conference on Learning Representations. 2018. [2] Tang X, Yao H, Sun Y, et al. Investigating and mitigating degree-related biases in graph convoltuional networks[C]//Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020: 1435-1444. In addition to the limitations mentioned in the paper, the generalization of the conclusion should be taken into consideration.
==================================================

Focused review:

weaknesses and the limitation of the work are not discussed.
Clarity: The submission is clearly written and it is organized OK.
Significance: The results are important and probably the results will be used by other researchers or practitioners. It seems that it does advance the state of the art based on numerical experiments. It provides an environment based on a real-world dataset.
I enjoyed reading the paper. I have few comments:
Q1- In several places of the paper the term "bi-level reinforcement learning (RL) based optimization framework" is used. This is misleading since bi-level optimization is a known type of problem in mathematical programming in which the solution of an optimization problem is a constraint in the main problem (see the problem definition and the formulation in [1] as a recent paper in this field). This is not the case in the problem that you have defined. The algorithm that is defined, has two levels, which are commonly named hierarchical RL. I suggest revising the paper accordingly to avoid any confusion in the future.
Q2- Several recent papers are not missed in the literature review section. I would add and review the contributions of [3, 4, 5, 6]. For older papers see [2].
Minor comments: "Although these methods mainly focus on TSPs or VRPs, of which all orders’ information is known in advance and much fewer constraints are considered comparing with DPDP, learning-based methods have shown great potential to help solve large-scale DPDPs and reach superior performance." This is not correct. There are several works on RL for VRP which consider stochastic VRP problems in which the demands and their locations are not known a priori.
[1] Tahernejad, S., Ralphs, T.K. & DeNegre, S.T. A branch-and-cut algorithm for mixed integer bilevel linear optimization problems and its implementation. Math. Prog. Comp. 12, 529–568 (2020). https://doi.org/10.1007/s12532-020-00183-6
[2] Berbeglia, Gerardo, Jean-François Cordeau, and Gilbert Laporte. "Dynamic pickup and delivery problems." European journal of operational research 202, no. 1 (2010): 8-15.
[3] Karami, Farzaneh, Wim Vancroonenburg, and Greet Vanden Berghe. "A periodic optimization approach to dynamic pickup and delivery problems with time windows." Journal of Scheduling 23, no. 6 (2020): 711-731.
[4] Ulmer, Marlin W., Barrett W. Thomas, Ann Melissa Campbell, and Nicholas Woyak. "The restaurant meal delivery problem: Dynamic pickup and delivery with deadlines and random ready times." Transportation Science 55, no. 1 (2021): 75-100.
[5] Su, Zhiyuan, Wantao Li, Jicao Li, and Bin Cheng. "Heterogeneous fleet vehicle scheduling problems for dynamic pickup and delivery problem with time windows in shared logistics platform: formulation, instances and algorithms." International Journal of Systems Science: Operations & Logistics (2021): 1-25.
[6] Györgyi, Péter, and Tamás Kis. "A probabilistic approach to pickup and delivery problems with time window uncertainty." European Journal of Operational Research 274, no. 3 (2019): 909-923.
There is no social impact and the limitations of the work are not discussed.


Review Point: and the limitation of the work are not discussed. Clarity: The submission is clearly written and it is organized OK. Significance: The results are important and probably the results will be used by other researchers or practitioners. It seems that it does advance the state of the art based on numerical experiments. It provides an environment based on a real-world dataset. I enjoyed reading the paper. I have few comments: Q1- In several places of the paper the term "bi-level reinforcement learning (RL) based optimization framework" is used. This is misleading since bi-level optimization is a known type of problem in mathematical programming in which the solution of an optimization problem is a constraint in the main problem (see the problem definition and the formulation in [1] as a recent paper in this field). This is not the case in the problem that you have defined. The algorithm that is defined, has two levels, which are commonly named hierarchical RL. I suggest revising the paper accordingly to avoid any confusion in the future. Q2- Several recent papers are not missed in the literature review section. I would add and review the contributions of [3, 4, 5, 6]. For older papers see [2]. Minor comments: "Although these methods mainly focus on TSPs or VRPs, of which all orders’ information is known in advance and much fewer constraints are considered comparing with DPDP, learning-based methods have shown great potential to help solve large-scale DPDPs and reach superior performance." This is not correct. There are several works on RL for VRP which consider stochastic VRP problems in which the demands and their locations are not known a priori. [1] Tahernejad, S., Ralphs, T.K. & DeNegre, S.T. A branch-and-cut algorithm for mixed integer bilevel linear optimization problems and its implementation. Math. Prog. Comp. 12, 529–568 (2020). https://doi.org/10.1007/s12532-020-00183-6 [2] Berbeglia, Gerardo, Jean-François Cordeau, and Gilbert Laporte. "Dynamic pickup and delivery problems." European journal of operational research 202, no.
Review Point: 1 (2010): 8-15. [3] Karami, Farzaneh, Wim Vancroonenburg, and Greet Vanden Berghe. "A periodic optimization approach to dynamic pickup and delivery problems with time windows." Journal of Scheduling 23, no.
Review Point: 6 (2020): 711-731. [4] Ulmer, Marlin W., Barrett W. Thomas, Ann Melissa Campbell, and Nicholas Woyak. "The restaurant meal delivery problem: Dynamic pickup and delivery with deadlines and random ready times." Transportation Science 55, no.
Review Point: 1 (2021): 75-100. [5] Su, Zhiyuan, Wantao Li, Jicao Li, and Bin Cheng. "Heterogeneous fleet vehicle scheduling problems for dynamic pickup and delivery problem with time windows in shared logistics platform: formulation, instances and algorithms." International Journal of Systems Science: Operations & Logistics (2021): 1-25. [6] Györgyi, Péter, and Tamás Kis. "A probabilistic approach to pickup and delivery problems with time window uncertainty." European Journal of Operational Research 274, no.
Review Point: 3 (2019): 909-923. There is no social impact and the limitations of the work are not discussed.
==================================================

Focused review:

I have the following critical concerns about this paper: 1. The used datasets are too simple. More complex datasets such as SculptFaces in NeRV paper are required to evaluate the performance of the proposed algorithm. 2. The contribution of this paper is incremental and is not sufficient to be published at NeurIPS. As detailed above, ClassNeRV could be seen as a variation of NeRV through penalizing within-class missed neighbors and between-class false neighbors with class information. Therefore, in my opinion, it is not a significant contribution. 3. According to Sec 3.2, they derived the ClassNeRV Stress Function from NeRV Stress Function by splitting Eq. 2 into within-class and between-class relations. Therefore, there are two additional distinct class-based trade-off parameters $\tau^{\in}$ and $\tau^{\notin}$, and they claimed that when $\tau^{\in}=\tau^{\notin}$, ClassNeRV is an unsupervised method and reduces to the original NeRV. However, Eq. 3 has already utilized class information and thus I disagree that it is unsupervised. Besides, please clarify that it does not reduce to the original NeRV since ClassNeRV uses Bregman divergence. 4. Please elaborate more on the importance of this issue, \ie, preserving the class structures in DR. In detail, the supervised DR methods could be applied to classification by separating classes while the unsupervised methods could be used to visualize high-dimensional data. I'm confused about what cases ClassNeRV makes sense since it does not have not a clear impact as other methods. 5. The authors claimed that they mainly focused on avoiding the distortions of the neighborhood structure that is harmful to the class structure by NeRV. However, according to appendix Table 2, the proposed methods do not have a great improvement to performance. Besides, considered that the proposed methods have two hyper-parameters to control the balance for within classes and between classes, what's the unique advantage of ClassNeRV? Once more hyper-parameters are introduced, the proposed method is limited. In other words, what case do we need to preserve the class structure rather than directly applying the sota unsupervised methods since in many cases we do not have the labeled dataset? 6. An ablation study about different $\tau^{\in}$ and $\tau^{\notin}$ could be considered. It will make the claim of balancing within classes and between classes more convincing.

Review Point: 1. The used datasets are too simple. More complex datasets such as SculptFaces in NeRV paper are required to evaluate the performance of the proposed algorithm.
Review Point: 2. The contribution of this paper is incremental and is not sufficient to be published at NeurIPS. As detailed above, ClassNeRV could be seen as a variation of NeRV through penalizing within-class missed neighbors and between-class false neighbors with class information. Therefore, in my opinion, it is not a significant contribution.
Review Point: 3. According to Sec 3.2, they derived the ClassNeRV Stress Function from NeRV Stress Function by splitting Eq.
Review Point: 2 into within-class and between-class relations. Therefore, there are two additional distinct class-based trade-off parameters $\tau^{\in}$ and $\tau^{\notin}$, and they claimed that when $\tau^{\in}=\tau^{\notin}$, ClassNeRV is an unsupervised method and reduces to the original NeRV. However, Eq.
Review Point: 3 has already utilized class information and thus I disagree that it is unsupervised. Besides, please clarify that it does not reduce to the original NeRV since ClassNeRV uses Bregman divergence.
Review Point: 4. Please elaborate more on the importance of this issue, \ie, preserving the class structures in DR. In detail, the supervised DR methods could be applied to classification by separating classes while the unsupervised methods could be used to visualize high-dimensional data. I'm confused about what cases ClassNeRV makes sense since it does not have not a clear impact as other methods.
Review Point: 5. The authors claimed that they mainly focused on avoiding the distortions of the neighborhood structure that is harmful to the class structure by NeRV. However, according to appendix Table 2, the proposed methods do not have a great improvement to performance. Besides, considered that the proposed methods have two hyper-parameters to control the balance for within classes and between classes, what's the unique advantage of ClassNeRV? Once more hyper-parameters are introduced, the proposed method is limited. In other words, what case do we need to preserve the class structure rather than directly applying the sota unsupervised methods since in many cases we do not have the labeled dataset?
Review Point: 6. An ablation study about different $\tau^{\in}$ and $\tau^{\notin}$ could be considered. It will make the claim of balancing within classes and between classes more convincing.
==================================================

Focused review:

Weaknesses
There is no evaluation on the overfitting which often occurs in neural optical operators.
Evaluation is limited on a device scale as mentioned by the authors.
The comparison of the proposed method running on a GPU is done against FDFD method running on CPUs as described in Fig1.
L104: It would be helpful to write detailed forms A,x, and b in the main paper


Review Point: There is no evaluation on the overfitting which often occurs in neural optical operators. Evaluation is limited on a device scale as mentioned by the authors. The comparison of the proposed method running on a GPU is done against FDFD method running on CPUs as described in Fig1.
Review Point: L104: It would be helpful to write detailed forms A,x, and b in the main paper
==================================================

Focused review:

Weaknesses: Some of the analyses and ablations could be more complete, in order to truly investigate whether the conclusions are correct.
More specific feedback:
It is unclear to me what the definition of the pseudo-labels is. Does it mean, if we have, e.g., 10k data points, that the pseudo label is a one-hot encoding with 10k elements? So each data points has its own 'class' label? A proper definition is lacking.
p4: The authors mention that direct optimization of Eq. 2 is inefficient and unscalable. But I don't really understand why, as it is in fact just a classification model like any other supervised classifier. So why is it impossible to use the label for each datapoint? And then second, how is it justified that the label of the full batch y_B does contain information about all data windows in the batch? What is it's definition? This addition to the model feels a bit strange, as it is also not further mentioned in the rest of the paper.
eq. 10: Are the p_i probabilities trainable parameters? And how do they relate to the importance scores mentioned in appendix D.3? Are the importance scores just unnormalized probabilities? How is the temperature set during training?
3.1: This paragraph is a summation of a lot of models, but it does actually not explain the models that are finally used as benchmark models. If the reader is unfamiliar with any of the benchmark models, the paper does not provide any information to the reader about their difference compared to the proposed model. So I would extend this section with a (semi-technical) explanation of the benchmark models and how they differ in their main technique. For example, which of the proposed benchmarks is actually also a contrastive learning model, and which of the benchmarks is supervised? This information is useful to better interpret the provided results.
Contrastive Predictive Coding (van Oord et al., 2018) is another well-known contrastive learning method that is often used for time-series, but the authors did not mention it anywhere in the paper. Besides mentioning it in related work, it would be a very useful benchmark as well, possibly combined with the local and global loss as the authors proposed.
p7, last line: The authors mention that TS2Vec achies 2nd best perofrmance because it adopts subsequence by random cropping. How do the authors know this is the reason? No causal conclusions can be drawn from the presented Tabels.
Same remark for p8, where the authors argue that InfoTS can adaptively select the most suitable augmentations. At this point of reading, there is no proof of that yet. Later in the ablation studies the authors do however check for this, but the conclusion from Table 3 is mainly that all factors contributed (at least a bit), where the Fidelity and Variety criterion seemed to have contributed the least. So it's not super fair to state that the model's superior performance is (only) thanks to adaptive augmentation selection. In fact, from Fig. 7 it even seems that the model just learns to select one best augmentation, rather than a best combination of all of them.
Fig. 3: Can the authors also provide the plot for the ablation models that are not trained with the variety and fidelity criteria? Now it's unsure whether these criteria contributed to this positive relation, or whether this relation already exists by default in such models. Also, how does this relation look like in the forecasting task?
Minor things/ typos:
end p3: "parameterized" instead of paramterized
I would refer to Fig. 2 already earlier, for example already both in the High fidelity and high variety sections. It helps the understanding of the reader.
p4: The authors mention that the nr of labels is equal to the number of instances in dataset X in the unsupervised case, but this is actually for supervised training right? In the unsupervised case there are no labels. Or do the authors refer to the pseudolabels then? But even then, in the supervised case this remark also holds.
p5: Is the batch X_b the same as the mini-batch used during training and mentioned later in the paper? Or are batch and mini-batch two different entities in this work?
p5: "non-neighboring samples", is a sample here a subsequence, just like s?
eq. 10: The work from Jang et al., ("Categorical reparameterization with gumbel-softmax") concurrently invented the concrete/Gumbel-softmax distribution as the work of Madisson et al., so these works are typically cited together.
LSTnet is compared in table 1, but not cited in section 4.1, and StemGNN is cited but not added to the table.
4.4: "The advantage of adaptive selection": So if I understand correctly with adaptive selection the authors refer to the concrete sample from a trained categorical over possible augmentations?
Appendix C.1: What's the difference between the cutout and the subsequence augmentation?
Appendix C.1: If time warping is applied, the number of samples per window is different right? So how do the authors deal with this? Is resampling applied maybe?
Appendix D.1: To which experiment does figure 5 relate? And are all runs run with the same randomized seed?
Appendix D.3: The MAE of Subsequence for L_y = 168 and 336 in Table 5 are missing a leading zero.


Review Point: Some of the analyses and ablations could be more complete, in order to truly investigate whether the conclusions are correct. More specific feedback: It is unclear to me what the definition of the pseudo-labels is. Does it mean, if we have, e.g., 10k data points, that the pseudo label is a one-hot encoding with 10k elements? So each data points has its own 'class' label? A proper definition is lacking.
Review Point: 2 is inefficient and unscalable. But I don't really understand why, as it is in fact just a classification model like any other supervised classifier. So why is it impossible to use the label for each datapoint? And then second, how is it justified that the label of the full batch y_B does contain information about all data windows in the batch? What is it's definition? This addition to the model feels a bit strange, as it is also not further mentioned in the rest of the paper. eq.
Review Point: 10: Are the p_i probabilities trainable parameters? And how do they relate to the importance scores mentioned in appendix D.3? Are the importance scores just unnormalized probabilities? How is the temperature set during training? 3.1: This paragraph is a summation of a lot of models, but it does actually not explain the models that are finally used as benchmark models. If the reader is unfamiliar with any of the benchmark models, the paper does not provide any information to the reader about their difference compared to the proposed model. So I would extend this section with a (semi-technical) explanation of the benchmark models and how they differ in their main technique. For example, which of the proposed benchmarks is actually also a contrastive learning model, and which of the benchmarks is supervised? This information is useful to better interpret the provided results. Contrastive Predictive Coding (van Oord et al., 2018) is another well-known contrastive learning method that is often used for time-series, but the authors did not mention it anywhere in the paper. Besides mentioning it in related work, it would be a very useful benchmark as well, possibly combined with the local and global loss as the authors proposed. p7, last line: The authors mention that TS2Vec achies 2nd best perofrmance because it adopts subsequence by random cropping. How do the authors know this is the reason? No causal conclusions can be drawn from the presented Tabels. Same remark for p8, where the authors argue that InfoTS can adaptively select the most suitable augmentations. At this point of reading, there is no proof of that yet. Later in the ablation studies the authors do however check for this, but the conclusion from Table 3 is mainly that all factors contributed (at least a bit), where the Fidelity and Variety criterion seemed to have contributed the least. So it's not super fair to state that the model's superior performance is (only) thanks to adaptive augmentation selection. In fact, from Fig. 7 it even seems that the model just learns to select one best augmentation, rather than a best combination of all of them. Fig.
Review Point: 3: Can the authors also provide the plot for the ablation models that are not trained with the variety and fidelity criteria? Now it's unsure whether these criteria contributed to this positive relation, or whether this relation already exists by default in such models. Also, how does this relation look like in the forecasting task? Minor things/ typos: end p3: "parameterized" instead of paramterized I would refer to Fig. 2 already earlier, for example already both in the High fidelity and high variety sections. It helps the understanding of the reader.
Review Point: p4: The authors mention that the nr of labels is equal to the number of instances in dataset X in the unsupervised case, but this is actually for supervised training right? In the unsupervised case there are no labels. Or do the authors refer to the pseudolabels then? But even then, in the supervised case this remark also holds.
Review Point: p5: Is the batch X_b the same as the mini-batch used during training and mentioned later in the paper? Or are batch and mini-batch two different entities in this work?
Review Point: p5: "non-neighboring samples", is a sample here a subsequence, just like s? eq.
Review Point: 10: The work from Jang et al., ("Categorical reparameterization with gumbel-softmax") concurrently invented the concrete/Gumbel-softmax distribution as the work of Madisson et al., so these works are typically cited together. LSTnet is compared in table 1, but not cited in section 4.1, and StemGNN is cited but not added to the table. 4.4: "The advantage of adaptive selection": So if I understand correctly with adaptive selection the authors refer to the concrete sample from a trained categorical over possible augmentations? Appendix C.1: What's the difference between the cutout and the subsequence augmentation? Appendix C.1: If time warping is applied, the number of samples per window is different right? So how do the authors deal with this? Is resampling applied maybe? Appendix D.1: To which experiment does figure 5 relate? And are all runs run with the same randomized seed? Appendix D.3: The MAE of Subsequence for L_y = 168 and 336 in Table 5 are missing a leading zero.
==================================================

Focused review:

weakness in He et al., and proposes a more invisible watermarking algorithm, making their method more appealing to the community. 2. Instead of using a heuristic search, the authors elegantly cast the watermark search issue into an optimization problem and provide rigorous proof. 3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation. 4. This work theoretically proves that CATER is resilient to statistical reverse-engineering, which is also verified by their experiments. In addition, they show that CATER can defend against ONION, an effective approach for backdoor removal.
Weakness: 1. The authors assume that all training data are from the API response, but what if the adversary only uses the part of the API response? 2. Figure 5 is hard to comprehend. I would like to see more details about the two baselines presented in Figure 5.
The authors only study CATER for the English-centric datasets. However, as we know, the widespread text generation APIs are for translation, which supports multiple languages. Probably, the authors could extend CATER to other languages in the future.


Review Point: in He et al., and proposes a more invisible watermarking algorithm, making their method more appealing to the community.
Review Point: 2. Instead of using a heuristic search, the authors elegantly cast the watermark search issue into an optimization problem and provide rigorous proof.
Review Point: 3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation.
Review Point: 4. This work theoretically proves that CATER is resilient to statistical reverse-engineering, which is also verified by their experiments. In addition, they show that CATER can defend against ONION, an effective approach for backdoor removal. Weakness:
Review Point: 1. The authors assume that all training data are from the API response, but what if the adversary only uses the part of the API response?
Review Point: 2. Figure 5 is hard to comprehend. I would like to see more details about the two baselines presented in Figure 5. The authors only study CATER for the English-centric datasets. However, as we know, the widespread text generation APIs are for translation, which supports multiple languages. Probably, the authors could extend CATER to other languages in the future.
==================================================

Focused review:

weakness:
1 The theoretical parts (Section 3.2 and 3.3) are a bit hard to follow.
1-1. too many symbols are used. It would be more clear if the table list of all symbols is provided.
1-2. I am not sure if the following assumption is really valid:
1-2-1. lines 143-144: "Besides, under mild assumptions, if (E) ! 0 144 then % ! 0".
1-2-2. lines 148--149: "Though it is hard to conduct task-agnostic analysis on (t) term, we believe that perfect alignment is still an adequate criteria in minimizing (t) term".
For the assumption made at lines 148--149, I'd like to know (t) is really reduced by using a perfect alignment encoder (PA-SF) in the robotic arm control benchmarks.
also, lines 145--146 (and lines 631--636 in appendix): "Theoretically speaking, when # is a perfect alignment encoder, an goal-conditioned RL policy trained 146 over the encoded space {z(s)}s2S will minimize (E) to 0."
I think this statement is not valid if multiple optimal policies \pi_G exist. (say that there are two optimal policies \pi_{G,1} and \pi_{G,2} and that \pi is converged to \pi_{G,1}, \epsilon^{e_i}(\pi || \pi_{G, 2}) is still can be greater than zero.)
Minor comments:
Typos?:
line 145: an goal-conditioned -> a goal-conditioned
line 149: criteria -> criterion
line 330: is formally formulated -> is formulated
line 369: mdps -> {MDP}s
Reference style is not consistent (e.g., the abbreviated style of conference name is used in some references, and not in the others).
---Edit after reading the author response and the other reviews:------
I would like to improve my score, as the author has basically adequately addressed my concerns: WR->WA
I still have some concerns about the assumptions made in the theoretical analysis. For example, I saw the author's response regarding the term (t), but was not sufficiently convinced.
I also think that the presentation of the theoretical analysis section needs to be improved (e.g., as suggested by Reviewer S2fL).
Yes. # Discussion about societal impact is not contained in the paper, but I think it is not really necessary for this research.


Review Point: 1 The theoretical parts (Section 3.2 and 3.3) are a bit hard to follow. 1-1. too many symbols are used. It would be more clear if the table list of all symbols is provided. 1-2. I am not sure if the following assumption is really valid: 1-2-1. lines 143-144: "Besides, under mild assumptions, if (E) !
Review Point: 0 144 then % ! 0". 1-2-2. lines 148--149: "Though it is hard to conduct task-agnostic analysis on (t) term, we believe that perfect alignment is still an adequate criteria in minimizing (t) term". For the assumption made at lines 148--149, I'd like to know (t) is really reduced by using a perfect alignment encoder (PA-SF) in the robotic arm control benchmarks. also, lines 145--146 (and lines 631--636 in appendix): "Theoretically speaking, when # is a perfect alignment encoder, an goal-conditioned RL policy trained 146 over the encoded space {z(s)}s2S will minimize (E) to 0." I think this statement is not valid if multiple optimal policies \pi_G exist. (say that there are two optimal policies \pi_{G,1} and \pi_{G,2} and that \pi is converged to \pi_{G,1}, \epsilon^{e_i}(\pi || \pi_{G, 2}) is still can be greater than zero.) Minor comments: Typos?: line 145: an goal-conditioned -> a goal-conditioned line 149: criteria -> criterion line 330: is formally formulated -> is formulated line 369: mdps -> {MDP}s Reference style is not consistent (e.g., the abbreviated style of conference name is used in some references, and not in the others). ---Edit after reading the author response and the other reviews:------ I would like to improve my score, as the author has basically adequately addressed my concerns: WR->WA I still have some concerns about the assumptions made in the theoretical analysis. For example, I saw the author's response regarding the term (t), but was not sufficiently convinced. I also think that the presentation of the theoretical analysis section needs to be improved (e.g., as suggested by Reviewer S2fL). Yes. # Discussion about societal impact is not contained in the paper, but I think it is not really necessary for this research.
==================================================

Focused review:

Weaknesses:
My biggest concern in this work is the low performance of the main experiments, shown in Table 1. Compared to the transfer learning or linear evaluation results from ALIGN and CLIP, the numbers in Table 1 aren’t convincing enough to argue that this kind of data expansion policy is practically useful than the standard protocol, i.e. training an encoder by SSL objectives then fine-tuning the encoder on small-scale datasets.
I summarize the performance gap between this method and CLIP and ALIGN below:
Method Caltech101 Cars Flowers Pets
GIF-MAE (proposed one) 58.4 44.5 84.4 52.4
GIF-DALLE2 (proposed one) 63.0 53.1 88.2 66.4
CLIP (ViT-B/32) - linear eval. 93.0 81.8 96.9 90.0
CLIP (ViT-L/14, 334px) - linear eval. 96.0 91.5 99.2 95.1
ALIGN - transfer learning - 96.1 99.7 96.2
ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision, https://arxiv.org/abs/2102.05918
More questions
Q1. In my understanding, the roles of the entropy maximization and diversity promotion seem to be very similar to each other. Could you elaborate a little bit more on the reason why both conditions are required instead of using the diversity promotion?
Q2. In table 1, GIF-DALLE consistently performs better than GIF-MAE, though the resolution of the synthetic samples based on GIF-MAE is much higher than the ones on GIF-DALLE (224px vs. 64px.), as described in Appendix C.1. It would be helpful to describe the reason why GIF-DALLE performs better than the MAE in the revised manuscript.


Review Point: My biggest concern in this work is the low performance of the main experiments, shown in Table 1. Compared to the transfer learning or linear evaluation results from ALIGN and CLIP, the numbers in Table 1 aren’t convincing enough to argue that this kind of data expansion policy is practically useful than the standard protocol, i.e. training an encoder by SSL objectives then fine-tuning the encoder on small-scale datasets. I summarize the performance gap between this method and CLIP and ALIGN below: Method Caltech101 Cars Flowers Pets GIF-MAE (proposed one) 58.4 44.5 84.4 52.4 GIF-DALLE2 (proposed one) 63.0 53.1 88.2 66.4 CLIP (ViT-B/32) - linear eval. 93.0 81.8 96.9 90.0 CLIP (ViT-L/14, 334px) - linear eval. 96.0 91.5 99.2 95.1 ALIGN - transfer learning - 96.1 99.7 96.2 ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision, https://arxiv.org/abs/2102.05918 More questions Q1. In my understanding, the roles of the entropy maximization and diversity promotion seem to be very similar to each other. Could you elaborate a little bit more on the reason why both conditions are required instead of using the diversity promotion?
Review Point: Q2. In table 1, GIF-DALLE consistently performs better than GIF-MAE, though the resolution of the synthetic samples based on GIF-MAE is much higher than the ones on GIF-DALLE (224px vs. 64px.), as described in Appendix C.1. It would be helpful to describe the reason why GIF-DALLE performs better than the MAE in the revised manuscript.
==================================================

Focused review:

Weakness: 1) Although each part of the proposed method is effective, the overall algorithm is still cumbersome. It has multiple stages. In contrast, many of existing pruning methods do not need fine-tuning. 2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty. 3) The experimental results are not convincing. The compared methods are few. Although few authors have attempted to prune EfficientNet, other networks can be compressed in experiments such as ResNet. In addition, the performance gains compared with SOTAs are also marginal, which are also within 1%. 4) The paper is poorly written. There are many typos and some are listed as follows: --In caption of Figure 2, “An subset of a network” should be “A subset of a network”. --In Line157 of Page4, “The output output vector” should be “The output vector”. --In Line283 of Page7, “B0V2 as,” should be “B0V2 as teacher,”. --In Line301 of Page7, “due the inconsistencies” should be “due to the inconsistencies”.


Review Point: 1) Although each part of the proposed method is effective, the overall algorithm is still cumbersome. It has multiple stages. In contrast, many of existing pruning methods do not need fine-tuning.
Review Point: 2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty.
Review Point: 3) The experimental results are not convincing. The compared methods are few. Although few authors have attempted to prune EfficientNet, other networks can be compressed in experiments such as ResNet. In addition, the performance gains compared with SOTAs are also marginal, which are also within 1%.
Review Point: 4) The paper is poorly written. There are many typos and some are listed as follows: --In caption of Figure 2, “An subset of a network” should be “A subset of a network”. --In Line157 of Page4, “The output output vector” should be “The output vector”. --In Line283 of Page7, “B0V2 as,” should be “B0V2 as teacher,”. --In Line301 of Page7, “due the inconsistencies” should be “due to the inconsistencies”.
==================================================

Focused review:

- Table 1 gives some theorical ananlysis of the parameters and memory requirement of different methods. If some real resource usages can be reported, it will be more intersting. - The title of the paper is not very good. "Optical flow by learning matching cost" is not new, in my understanding. Many methods estimate the flow by learning matching cost. Furthermore, the core idea "decoupling" is not reflected in the title at all.

Review Point: - Table 1 gives some theorical ananlysis of the parameters and memory requirement of different methods. If some real resource usages can be reported, it will be more intersting.
Review Point: - The title of the paper is not very good. "Optical flow by learning matching cost" is not new, in my understanding. Many methods estimate the flow by learning matching cost. Furthermore, the core idea "decoupling" is not reflected in the title at all.
==================================================

Focused review:

- By training a classifier on the original data in the white-box setting, the authors break the promise of avoiding any re-training that is given in the paper. - The main aim of this work is to be of practical use in the cloud services. I think certified robustness is generally believed to provide weaker robust accuracies than the Madry's adversarial training. This questions the applicability of this approach. - Limited novelty: The idea of using the input transformation in adversarial robustness is not novel (Lecuyer et al. (2018) as mentioned in the paper uses a similar idea). The only difference is that the authors are using this idea in the context of certified robustness and also making the transformation adaptive to the original classifier.

Review Point: - By training a classifier on the original data in the white-box setting, the authors break the promise of avoiding any re-training that is given in the paper.
Review Point: - The main aim of this work is to be of practical use in the cloud services. I think certified robustness is generally believed to provide weaker robust accuracies than the Madry's adversarial training. This questions the applicability of this approach.
Review Point: - Limited novelty: The idea of using the input transformation in adversarial robustness is not novel (Lecuyer et al. (2018) as mentioned in the paper uses a similar idea). The only difference is that the authors are using this idea in the context of certified robustness and also making the transformation adaptive to the original classifier.
==================================================

Focused review:

Weaknesses: 1. The novelty of the proposed method is weak. It seems to be a modified version of previous work: PatchCore, by adding a denoising process to the memory bank. 2. The setting of ‘Overlap’ experiment is somewhat confusing and meaningless. It is unfair for methods to train and test on the same (overlap) data. 3. In the experiments on ‘No overlap’ setting, little performance drops were seen on state-of-the-art methods, and the comparison between the proposed methods (98.6) and PatchCore (98.4) can not show the benefits of the new approach.


Review Point: 1. The novelty of the proposed method is weak. It seems to be a modified version of previous work: PatchCore, by adding a denoising process to the memory bank.
Review Point: 2. The setting of ‘Overlap’ experiment is somewhat confusing and meaningless. It is unfair for methods to train and test on the same (overlap) data.
Review Point: 3. In the experiments on ‘No overlap’ setting, little performance drops were seen on state-of-the-art methods, and the comparison between the proposed methods (98.6) and PatchCore (98.4) can not show the benefits of the new approach.
==================================================

Focused review:

1. The authors propose to train two different networks to learn the embedding and transformation separately with two PointNets (as stated in Supp Mat) whereas previous works directly learn the transformation to align the two point clouds. Can the authors elaborate on model and time complexity compared to previous works? 2. If the transformation A_{x, y} can be obtained directly using closed form from the correspondence matrix \Pi_{x, y}, why do we need to learn it in Sec. 4.2? 3. Based on the description of the framework, do the embedding network need to be trained first? Then it is used to train the network to learn the transformation? If so, is it possible that biases or even errors learned in the first will propagate to the second network? 4. The approach is only evaluated on one dataset for human poses. How well does this generalize to other unseen shapes? Will biases of human (generally standing upright) shapes cause problems if one is given point clouds of more irregular shapes like dogs, and cats? 5. Missing hyper parameters (learning rate, optimization method) to replicate results

Review Point: 1. The authors propose to train two different networks to learn the embedding and transformation separately with two PointNets (as stated in Supp Mat) whereas previous works directly learn the transformation to align the two point clouds. Can the authors elaborate on model and time complexity compared to previous works?
Review Point: 2. If the transformation A_{x, y} can be obtained directly using closed form from the correspondence matrix \Pi_{x, y}, why do we need to learn it in Sec. 4.2?
Review Point: 3. Based on the description of the framework, do the embedding network need to be trained first? Then it is used to train the network to learn the transformation? If so, is it possible that biases or even errors learned in the first will propagate to the second network?
Review Point: 4. The approach is only evaluated on one dataset for human poses. How well does this generalize to other unseen shapes? Will biases of human (generally standing upright) shapes cause problems if one is given point clouds of more irregular shapes like dogs, and cats?
Review Point: 5. Missing hyper parameters (learning rate, optimization method) to replicate results
==================================================

Focused review:

- both the algorithm and the framework are small modifications of previous works. The different assumptions made by OSAKA are not completely novel (e.g. the authors acknowledge MOCA, and other “task-free” / “task agnostic” frameworks), but the authors justify why putting all of them together could be useful for evaluating continual learning algorithms in some domains. - several continual learning papers measured the cumulative performance in task-agnostic setups, maybe without formalising the methodology; therefore in terms of originality the paper doesn't bring much

Review Point: - both the algorithm and the framework are small modifications of previous works. The different assumptions made by OSAKA are not completely novel (e.g. the authors acknowledge MOCA, and other “task-free” / “task agnostic” frameworks), but the authors justify why putting all of them together could be useful for evaluating continual learning algorithms in some domains.
Review Point: - several continual learning papers measured the cumulative performance in task-agnostic setups, maybe without formalising the methodology; therefore in terms of originality the paper doesn't bring much
==================================================

Focused review:

Weaknesses
Needs more clarity on the evaluation metrics used in the experiments
The metrics in Table 1 are presented in 5.2.1, and several more metrics in 5.2.2. Are these actually the same metrics with slightly different names? Some specificity on this would be appreciated.
Further questions
In Eq. 17 and in Algorithm 1, line 6, the layers are
l
∈
S
. However, in section 4.1, Figure 2, and the beginning of 4.3, the critical layers are denoted
l
∈
R
. Is this difference a mistake, or is there a difference between
S
and
R
? If there is, please define
S
in 4.3, or somewhere near Eq. 17, to make this clear.


Review Point: Needs more clarity on the evaluation metrics used in the experiments The metrics in Table 1 are presented in 5.2.1, and several more metrics in 5.2.2. Are these actually the same metrics with slightly different names? Some specificity on this would be appreciated. Further questions In Eq.
Review Point: 17 and in Algorithm 1, line 6, the layers are l ∈ S . However, in section 4.1, Figure 2, and the beginning of 4.3, the critical layers are denoted l ∈ R . Is this difference a mistake, or is there a difference between S and R ? If there is, please define S in 4.3, or somewhere near Eq. 17, to make this clear.
==================================================

Focused review:

1. There is some imprecision in the discussion of the results in 4.2.2: “In contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.” Is this conclusion really justified? Visually, it is not clear that the red line is “above” the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.
2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.
3. “ First, we found that language models redundantly encoded syntactic information in their embeddings” — Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn’t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. 
It seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy?
417: why is “as” included along with “were” and “are”?
295: What do you mean by “conservative but tight estimate of mutual information”? This is pretty unclear to me 188: typo at “represented by within” 

Review Point: 1. There is some imprecision in the discussion of the results in 4.2.2: “In contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.” Is this conclusion really justified? Visually, it is not clear that the red line is “above” the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.
Review Point: 2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.
Review Point: 3. “ First, we found that language models redundantly encoded syntactic information in their embeddings” — Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn’t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. It seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy? 417: why is “as” included along with “were” and “are”? 295: What do you mean by “conservative but tight estimate of mutual information”? This is pretty unclear to me 188: typo at “represented by within”
==================================================

Focused review:

- Simplicity bias is attributed to "standard training procedures such as SGD". However, Jacobsen et al. (reference [20]) showed that cross-entropy may be to blame: a modified loss function encourages neural networks to learn more than the most simple solution. This issue should be discussed and ideally one would like to investigate the invertible network from [20] on the proposed datasets. - The paper could benefit from investigating the role of initialization: clearly, a model that happened to be initialized such that a complex feature is already "learned" would make use of that feature. But at which point would it switch to learn the simpler one? I.e., one could design an experiment where a network is trained on a dataset where only the complex feature is predictive. This network is then used as the initialization for a dataset where both the complex and simpler feature are predictive. Linearly interpolating between the weights of this network and a random initialization could enable one to investigate how much the weights can deviate from the "complex feature solution" such that the complex solution is still learned, or whether the simple solution will always be preferred irrespective of initialization. - I appreciate that code was submitted alongside the paper. That being said, it would be good to mention if/how the dataset will be made available to others (which would be very helpful).

Review Point: - Simplicity bias is attributed to "standard training procedures such as SGD". However, Jacobsen et al. (reference [20]) showed that cross-entropy may be to blame: a modified loss function encourages neural networks to learn more than the most simple solution. This issue should be discussed and ideally one would like to investigate the invertible network from [20] on the proposed datasets.
Review Point: - The paper could benefit from investigating the role of initialization: clearly, a model that happened to be initialized such that a complex feature is already "learned" would make use of that feature. But at which point would it switch to learn the simpler one? I.e., one could design an experiment where a network is trained on a dataset where only the complex feature is predictive. This network is then used as the initialization for a dataset where both the complex and simpler feature are predictive. Linearly interpolating between the weights of this network and a random initialization could enable one to investigate how much the weights can deviate from the "complex feature solution" such that the complex solution is still learned, or whether the simple solution will always be preferred irrespective of initialization.
Review Point: - I appreciate that code was submitted alongside the paper. That being said, it would be good to mention if/how the dataset will be made available to others (which would be very helpful).
==================================================

Focused review:

Weaknesses
W1. The contributions of the submission are very incremental. I see it as an extension of the work of (Venital et al. 2020) in the two directions outlined in the summary. Further, the theoretical contributions are obfuscated by the rough presentation (more on this below) and the validation is, in part, designed for the proposed method (i.e., some of the evaluation sequences are designed to not be handled properly by prior work).
W2. The presentation has many issues:
often symbols are introduced without definition and sometimes never get defined, e.g., Z in eq. (10), d in eq. (10) does not match the d previously defined; A at the end of page 4;
evaluation metrics like those in Table 1 are never defined; etc.
some explanations do not make much sense, e.g., “gives preference to modules which helped achieve a higher accuracy, on the problems which they have been trained on,” “minimum number of transfer which need to be transferred,” “can be explored by applying this search strategy to each sequentially,” etc.
the probabilistic models proposed are not explained in detail, e.g., eq. (5) is simply stated with no explanation and the Gaussian Process in section 5.1 is not even specified.
W3. The paper disregards other benchmarks in CL like (Lin et al. NeurIPS 2021.) Such a benchmark should be discussed, and results on it should either be included or there should be a discussion explaining why those would not be relevant.
References
Lin et al. The CLEAR Benchmark: Continual LEArning on Real-World Imagery. NeurIPS 2021.


Review Point: W1. The contributions of the submission are very incremental. I see it as an extension of the work of (Venital et al. 2020) in the two directions outlined in the summary. Further, the theoretical contributions are obfuscated by the rough presentation (more on this below) and the validation is, in part, designed for the proposed method (i.e., some of the evaluation sequences are designed to not be handled properly by prior work).
Review Point: W2. The presentation has many issues: often symbols are introduced without definition and sometimes never get defined, e.g., Z in eq. (10), d in eq. (10) does not match the d previously defined; A at the end of page 4; evaluation metrics like those in Table 1 are never defined; etc. some explanations do not make much sense, e.g., “gives preference to modules which helped achieve a higher accuracy, on the problems which they have been trained on,” “minimum number of transfer which need to be transferred,” “can be explored by applying this search strategy to each sequentially,” etc. the probabilistic models proposed are not explained in detail, e.g., eq. (5) is simply stated with no explanation and the Gaussian Process in section 5.1 is not even specified.
Review Point: W3. The paper disregards other benchmarks in CL like (Lin et al. NeurIPS 2021.) Such a benchmark should be discussed, and results on it should either be included or there should be a discussion explaining why those would not be relevant. References Lin et al. The CLEAR Benchmark: Continual LEArning on Real-World Imagery. NeurIPS 2021.
==================================================

Focused review:

Weaknesses:
It is not clear how the proposed formulation may change the derived combinatorial solutions besides the tightened gap for objective function values. This is related to the computation of objective function values in Algorithms 2 and 3 (in appendix), which basically took the best derived objective function values among the sampled transport maps for the GK-TopK formulation. Are the reported objective function values in Tables 1 and 2 based on these values? To access how this changes the solution quality, should the authors also provide the actual objective values with the derived combinatorial solutions? For now, it is difficult to tell whether the proposed method will have meaningful improvement in derived solution in practice.
It is not clear to me how significant is this proposed improvement over Soft-TopK. Based on the presentation, in particular, Theorem 1 and Remarks on page 6, by introducing Gumble noise, the derived bounds now have two terms. It may address the diverging gap issue when the sorted probabilities at the boundary are the same (
x
K
=
x
K
+
1
). But why when GS-TopK will always have tighter bounds than Soft-TopK even when
x
K
≠
x
K
+
1
? Why adding Gumble noise to the original optimal transport formulation of Soft-TopK can always improve the bounds? In Figure 1, I assume that the actual optimal objective value is 2.4 while Soft-TopK has the gap at 2.0. What is the actual objective value for the derived solution by Soft-TopK? What about GS-TopK? Also, it seems the choice of
τ
does not affect the gap by Soft-TopK in this simple illustrative example but it does change GS-TopK gaps. But
τ
appears in the derived bounds for both formulations. Why?
For real applications, will different
τ
and
σ
change the performances significantly?
Mathematical notations and the proofs have numerous issues. Here are a few examples: 1) In equation (6),
x
k
and
x
k
+
1
have not been defined yet. They were explained until the beginning of page 5. 2) Below equation (13) and the text after that, should "the optimal solution to Eq. (12)"
T
~
be
T
~
∗
instead? 3) In equations (17) and (18), is
x
i
a vector? Please make sure about the meaning of bold and regular fonts in these equations. Also, regular font
x
's have been used for sorted probabilities. Different notations may need to be used to avoid confusion here. 4) In Appendix A.1 above equation (30), "By Lemma 1..." but this part is to prove Lemma 1. Do the authors mean by "Lemma 3" instead? Even that is the case, there appear to lack some steps to connect Lemma 3 with equation (30). 5) On page 19 above "Condition 1", "... except for the following condition:
x
i
=
x
k
,
x
i
=
x
k
." There must be typos for these two same equations. 6) Figure 4 and the proof for Theorem 2, do we need to worry about the cases with
x
i
≤
x
j
for Conditions 3 and 4?
There are missing information. For example, it is not clear what sample size #G was used in the experiments. It is not clear how exactly shrinking procedure was scheduled for homotopy GS-TopK as there is no detail provided in Algorithm 2. It would be also nice to discuss more why in Figure 3, the approximate formulations always have lower bounds. The lemmas, theorems or their proofs do not seems to indicate that will always happen?
Presentation can be improved significantly. For example, Figure 2 appears to be a general illustration instead of being the "overview of our pipeline" for max covering as indicated on page 6. There are also numerous typos, for example, "... important hcardinality constrained ..." in the last line of the paper on page 9.


Review Point: It is not clear how the proposed formulation may change the derived combinatorial solutions besides the tightened gap for objective function values. This is related to the computation of objective function values in Algorithms 2 and 3 (in appendix), which basically took the best derived objective function values among the sampled transport maps for the GK-TopK formulation. Are the reported objective function values in Tables 1 and 2 based on these values? To access how this changes the solution quality, should the authors also provide the actual objective values with the derived combinatorial solutions? For now, it is difficult to tell whether the proposed method will have meaningful improvement in derived solution in practice. It is not clear to me how significant is this proposed improvement over Soft-TopK. Based on the presentation, in particular, Theorem 1 and Remarks on page 6, by introducing Gumble noise, the derived bounds now have two terms. It may address the diverging gap issue when the sorted probabilities at the boundary are the same ( x K = x K + 1 ). But why when GS-TopK will always have tighter bounds than Soft-TopK even when x K ≠ x K + 1 ? Why adding Gumble noise to the original optimal transport formulation of Soft-TopK can always improve the bounds? In Figure 1, I assume that the actual optimal objective value is 2.4 while Soft-TopK has the gap at 2.0. What is the actual objective value for the derived solution by Soft-TopK? What about GS-TopK? Also, it seems the choice of τ does not affect the gap by Soft-TopK in this simple illustrative example but it does change GS-TopK gaps. But τ appears in the derived bounds for both formulations. Why? For real applications, will different τ and σ change the performances significantly? Mathematical notations and the proofs have numerous issues. Here are a few examples:
Review Point: 1) In equation (6), x k and x k + 1 have not been defined yet. They were explained until the beginning of page 5.
Review Point: 2) Below equation (13) and the text after that, should "the optimal solution to Eq. (12)" T ~ be T ~ ∗ instead?
Review Point: 3) In equations (17) and (18), is x i a vector? Please make sure about the meaning of bold and regular fonts in these equations. Also, regular font x 's have been used for sorted probabilities. Different notations may need to be used to avoid confusion here.
Review Point: 4) In Appendix A.1 above equation (30), "By Lemma 1..." but this part is to prove Lemma 1. Do the authors mean by "Lemma 3" instead? Even that is the case, there appear to lack some steps to connect Lemma 3 with equation (30).
Review Point: 5) On page 19 above "Condition 1", "... except for the following condition: x i = x k , x i = x k ." There must be typos for these two same equations.
Review Point: 6) Figure 4 and the proof for Theorem 2, do we need to worry about the cases with x i ≤ x j for Conditions 3 and 4? There are missing information. For example, it is not clear what sample size #G was used in the experiments. It is not clear how exactly shrinking procedure was scheduled for homotopy GS-TopK as there is no detail provided in Algorithm 2. It would be also nice to discuss more why in Figure 3, the approximate formulations always have lower bounds. The lemmas, theorems or their proofs do not seems to indicate that will always happen? Presentation can be improved significantly. For example, Figure 2 appears to be a general illustration instead of being the "overview of our pipeline" for max covering as indicated on page 6. There are also numerous typos, for example, "... important hcardinality constrained ..." in the last line of the paper on page 9.
==================================================

Focused review:

Weaknesses:
(W1) Interpretability: The biggest weakness for me is the lack of interpretability of the results, especially in Section 4.
The unparticipating generalization bound must show some measure of the spread of
P
as a bound on the heterogeniety. In the degenerate case that every client is completely different from each other, it should not be possible to bound the unparticipating gap. Likewise, if also the clients are identical, this gap should be 0.
My best guess is that these details are hidden in the
κ
m
and
ω
m
objects (or in the previous sections, in the bound on the loss). It would be extremely helpful if the authors could elaborate on this dependence.
More details on this below.
(W2) Lack of clarity: it is nearly impossible to understand this paper without a deep familiarity with a fairly advanced and niche area in learning theory. To make the paper more accessible to the federated learning community, it would be helpful to precisely define all of the mathematical objects used (at least in the appendix), point to textbooks for a condensed background (if applicable), and discuss several simple special cases such as linear regression and binary logistic regression.


Review Point: (W1) Interpretability: The biggest weakness for me is the lack of interpretability of the results, especially in Section 4. The unparticipating generalization bound must show some measure of the spread of P as a bound on the heterogeniety. In the degenerate case that every client is completely different from each other, it should not be possible to bound the unparticipating gap. Likewise, if also the clients are identical, this gap should be 0. My best guess is that these details are hidden in the κ m and ω m objects (or in the previous sections, in the bound on the loss). It would be extremely helpful if the authors could elaborate on this dependence. More details on this below.
Review Point: (W2) Lack of clarity: it is nearly impossible to understand this paper without a deep familiarity with a fairly advanced and niche area in learning theory. To make the paper more accessible to the federated learning community, it would be helpful to precisely define all of the mathematical objects used (at least in the appendix), point to textbooks for a condensed background (if applicable), and discuss several simple special cases such as linear regression and binary logistic regression.
==================================================

Focused review:

Weaknesses/Questions:
I only have minor suggestions:
1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying
Q
^
t
and
S
t
, as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of
α
t
and also on the average lengths of the predictive intervals when we let
Q
^
t
and
S
t
vary with time?
2.) I found the definition of the quantile a little confusing, an extra pair of brackets around the term
(
1
|
D
|
∑
(
X
r
,
Y
r
)
∈
D
1
S
(
X
r
,
Y
r
)
≤
s
)
might help, or maybe defining the bracketed term separately if space allows.
3.) I think there are typos in Lines 93, 136, 181 (and maybe in the Appendix too): should it be
Q
^
t
(
1
−
α
t
)
instead?
#####################################################################
Overall:
This is a very interesting extension to conformal prediction that no longer relies on exchangeability but is still general, which will hopefully lead to future work that guarantees coverage under weak assumptions. I believe the generality also makes this method useful in practice.
The authors have described the limitations of their theory, e.g. having a fixed
Q
^
with time.


Review Point: 1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying Q ^ t and S t , as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of α t and also on the average lengths of the predictive intervals when we let Q ^ t and S t vary with time?
Review Point: 2.) I found the definition of the quantile a little confusing, an extra pair of brackets around the term ( 1 | D | ∑ ( X r , Y r ) ∈ D 1 S ( X r , Y r ) ≤ s ) might help, or maybe defining the bracketed term separately if space allows.
Review Point: 3.) I think there are typos in Lines 93, 136, 181 (and maybe in the Appendix too): should it be Q ^ t ( 1 − α t ) instead? ##################################################################### Overall: This is a very interesting extension to conformal prediction that no longer relies on exchangeability but is still general, which will hopefully lead to future work that guarantees coverage under weak assumptions. I believe the generality also makes this method useful in practice. The authors have described the limitations of their theory, e.g. having a fixed Q ^ with time.
==================================================

Focused review:

Weaknesses:
Technical novelty seems limited, as the garment modelling is handled by TIE, Shao et al. 2020. Can the author clarify the differences between the approaches.
In eq. 3 the garment is modelled as a function of body and previous state of the garment. Since the paper primarily deals with multi-layered garments, should't the garment also be a function of other garments on the body? eg: let's say a person is wearing a loose skirt and a long jacket on top, such that the bottom of the jacket draper around the top of the skirt. Isn't the jacket deformation more dependent on the skirt than the body?
Eq. 14 only handles collisions between the body and the garments, but since authors deal with multi-layered garments, what about the collisions between different layers of garments?
Related work on multi-layered garment is completely missing. This important as multi-layered garments are the main contribution of the work. Authors should add a paragraph about it as currently it appears that the authors are the first one to address multi-layered garments which is not true. --> SMPLicit, CVPR'21 (not cited) has shown generation of multi-layered garments. --> SimulCap, CVPR'19 (not cited) Also use multi-layered garments + physics based simulation for motion capture. --> Another concurrent work: Layered-Garment Net, ACCV'22 (not cited) also generates multi-layered clothing using simulated data.
Clarifications:
Eq. 12: Shouldn't this be a double summation. One over multi-layered garments and other over vertices of the garment.
What is f(.) in the line just above eq. 3?
There are existing works like TailorNet, Patel et al. 2021 that also use PBS to drape garments on human body. Can the authors also provide details on how simulating multiple layers is more challenging than simulating single layered clothing?


Review Point: Technical novelty seems limited, as the garment modelling is handled by TIE, Shao et al. 2020. Can the author clarify the differences between the approaches. In eq.
Review Point: 3 the garment is modelled as a function of body and previous state of the garment. Since the paper primarily deals with multi-layered garments, should't the garment also be a function of other garments on the body? eg: let's say a person is wearing a loose skirt and a long jacket on top, such that the bottom of the jacket draper around the top of the skirt. Isn't the jacket deformation more dependent on the skirt than the body? Eq.
Review Point: 14 only handles collisions between the body and the garments, but since authors deal with multi-layered garments, what about the collisions between different layers of garments? Related work on multi-layered garment is completely missing. This important as multi-layered garments are the main contribution of the work. Authors should add a paragraph about it as currently it appears that the authors are the first one to address multi-layered garments which is not true. --> SMPLicit, CVPR'21 (not cited) has shown generation of multi-layered garments. --> SimulCap, CVPR'19 (not cited) Also use multi-layered garments + physics based simulation for motion capture. --> Another concurrent work: Layered-Garment Net, ACCV'22 (not cited) also generates multi-layered clothing using simulated data. Clarifications: Eq.
Review Point: 12: Shouldn't this be a double summation. One over multi-layered garments and other over vertices of the garment. What is f(.) in the line just above eq. 3? There are existing works like TailorNet, Patel et al. 2021 that also use PBS to drape garments on human body. Can the authors also provide details on how simulating multiple layers is more challenging than simulating single layered clothing?
==================================================

Focused review:

Weaknesses:  - While the proposed method outperforms certain restricted variants of DeepLab V2 [7], it performs significantly worse than the full DeepLab V2 on both of the two datasets used for comparisons.  - The experimental results show that the proposed method outperforms a restricted variant of DeepLab V2 that does not include the MSC, COCO, and CRF improvements (that are also not used with the proposed method).  However, the proposed method has worse accuracy than the full DeepLab V2 approach, with a particularly large accuracy gap on the Pascal VOC 2012 dataset.  The authors should give results for the proposed method in combination with the MSC, COCO, and CRF improvements.  - The proposed method is very similar to two prior works cited by the authors [3, 5].  The paper should more clearly compare the proposed method to these prior works, and include them for comparison in the tables of experimental results.  - The proposed method seems to have lower accuracy than the two very similar prior works [3] and [5], and also seems to offer no advantage in terms of computational efficiency over [5].  - The paper would benefit from proofreading of the grammar.  Response to author rebuttal: I thank the authors for clarifying the computational complexity of the diffusion step, specifically the fact that it is done with the features downsampled to 64x64; this addresses my confusion regarding the reported computation times.  I appreciate the difficulty of reproducing and comparing against [3] and [5] without source code, and that the that they may also be more difficult to train than the proposed method.  While a more robust method is a valuable contribution even if it does not provider better accuracy or speed, I think a more thorough investigation than is described in the paper would be needed to show that.  Regarding the lack of evaluation with the DeepLab MSC, COCO and CRF steps, while the authors make the valid point that the MSC step and especially the CRF step increase the computational complexity, and that the COCO pretraining adds training data and therefore it may not be fair to compare COCO-pretrained models to those without it, evaluations including these steps are nonetheless important for properly comparing this method to other methods, given the existing similar work.

Review Point: - While the proposed method outperforms certain restricted variants of DeepLab V2 [7], it performs significantly worse than the full DeepLab V2 on both of the two datasets used for comparisons.
Review Point: - The experimental results show that the proposed method outperforms a restricted variant of DeepLab V2 that does not include the MSC, COCO, and CRF improvements (that are also not used with the proposed method). However, the proposed method has worse accuracy than the full DeepLab V2 approach, with a particularly large accuracy gap on the Pascal VOC 2012 dataset. The authors should give results for the proposed method in combination with the MSC, COCO, and CRF improvements.
Review Point: - The proposed method is very similar to two prior works cited by the authors [3, 5]. The paper should more clearly compare the proposed method to these prior works, and include them for comparison in the tables of experimental results.
Review Point: - The proposed method seems to have lower accuracy than the two very similar prior works [3] and [5], and also seems to offer no advantage in terms of computational efficiency over [5].
Review Point: - The paper would benefit from proofreading of the grammar. Response to author rebuttal: I thank the authors for clarifying the computational complexity of the diffusion step, specifically the fact that it is done with the features downsampled to 64x64; this addresses my confusion regarding the reported computation times. I appreciate the difficulty of reproducing and comparing against [3] and [5] without source code, and that the that they may also be more difficult to train than the proposed method. While a more robust method is a valuable contribution even if it does not provider better accuracy or speed, I think a more thorough investigation than is described in the paper would be needed to show that. Regarding the lack of evaluation with the DeepLab MSC, COCO and CRF steps, while the authors make the valid point that the MSC step and especially the CRF step increase the computational complexity, and that the COCO pretraining adds training data and therefore it may not be fair to compare COCO-pretrained models to those without it, evaluations including these steps are nonetheless important for properly comparing this method to other methods, given the existing similar work.
==================================================

Focused review:

Weaknesses: 
 1. The proposed technique does not seem to predict uncertainty. To the best
    of my understanding, it predicts where the original model is making
    errors. An easy way to see this would be to consider a point where the
    model is confident but inaccurate, i.e., a bias. The predicted mask
    should cover this point, even though it is not an uncertain prediction
    of the model. The problem is actually in the definition of
    ``uncertainty''. Equation 1 (or 3) do not necessarily only correspond to
    uncertainty. It corresponds to expected error for a sample. It also
    includes the contribution of bias. In this regard, I think the
    uncertainty positioning may not be very accurate. Consequently, the
    comparison with a method that computes confidence intervals may not be
    appropriate. Here, I should note that predicting expected error or the error is not a
    bad target. However, the difference between this and uncertainty
    estimation should probably be made clear. 

 2. I am not sure about the contribution of the corollary 1. The result is
    not very surprising in my opinion. The mask that aims to minimize the
    ``uncertainty'' is bound to be related to the ``uncertainty''. 

 3. It is unclear how authors estimate the expectation in Equations 4
    or 5. This is over y x variable. However, in Equation 6, they drop the
    expectation and simply take only one sample to compute this
    expectation. Using this I am assuming the model learns to predict the
    error and not the expected error. As a result, the model may not be able
    to identify an uncertain prediction at a pixel for a given sample since
    for that pixel, there may be only 1 output that happens to be close to
    the ground truth. 

 4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0,
    m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} =
    \infty. It is unclear how authors deal with this. Furthermore, the
    intuition of this specific calibration form is unclear. 


Review Point: 1. The proposed technique does not seem to predict uncertainty. To the best of my understanding, it predicts where the original model is making errors. An easy way to see this would be to consider a point where the model is confident but inaccurate, i.e., a bias. The predicted mask should cover this point, even though it is not an uncertain prediction of the model. The problem is actually in the definition of ``uncertainty''. Equation 1 (or 3) do not necessarily only correspond to uncertainty. It corresponds to expected error for a sample. It also includes the contribution of bias. In this regard, I think the uncertainty positioning may not be very accurate. Consequently, the comparison with a method that computes confidence intervals may not be appropriate. Here, I should note that predicting expected error or the error is not a bad target. However, the difference between this and uncertainty estimation should probably be made clear.
Review Point: 2. I am not sure about the contribution of the corollary 1. The result is not very surprising in my opinion. The mask that aims to minimize the ``uncertainty'' is bound to be related to the ``uncertainty''.
Review Point: 3. It is unclear how authors estimate the expectation in Equations 4 or 5. This is over y x variable. However, in Equation 6, they drop the expectation and simply take only one sample to compute this expectation. Using this I am assuming the model learns to predict the error and not the expected error. As a result, the model may not be able to identify an uncertain prediction at a pixel for a given sample since for that pixel, there may be only 1 output that happens to be close to the ground truth.
Review Point: 4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0, m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} = \infty. It is unclear how authors deal with this. Furthermore, the intuition of this specific calibration form is unclear.
==================================================

Focused review:

Weaknesses: • Unprofessional writing. - Most starkly, “policies” is misspelled in the title. • At times, information is not given in an easy-to-understand way. - E.G. lines 147 - 152, 284 - 289. • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding. • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes. • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective. • Blank spaces in table 1 are unclear.
Opportunities:
• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
• More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well.
The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication.
Overall, the paper and its presentation have several problems, but the idea seems elegant and useful.
Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work


Review Point: • Unprofessional writing.- Most starkly, “policies” is misspelled in the title.
Review Point: • At times, information is not given in an easy-to-understand way.
Review Point: • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding.
Review Point: • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes.
Review Point: • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective.
Review Point: • It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
Review Point: • More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well. The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication. Overall, the paper and its presentation have several problems, but the idea seems elegant and useful. Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work
==================================================

Focused review:

The main weakness of this work is the lack of direct comparisons with previous works. The authors claim that they outperform the state-of-the-art on UCF, HMDB, and ESC, but fail to compare with other approaches under the same backbone architecture and the same pertaining dataset. For example, the following concerns arise from table 1: - Is MMV with TSM-50 better than ELo because MMV has a better architecture and was pretrained on a larger dataset? What is the performance of ELo pretrained on AudioSet+HowTo100M using TSM-50? Or What is the performance of MMV using R(2+1)D-50 and pretrained on YouTube8M? - XDC outperformed its R(2+1)D-18 fully-supervised pertaining baseline, but MMV did not exceed its fully-supervised pertaining baseline. On the other hand, MMV shows better performance than XDC. Can the authors explain why this is the case? Is MMV better than XDC because it uses a better backbone architecture? - MIL-NCE uses the same S3D backbone as MMV, but MIL-NCE is only pretrained on HowTo100M. Could the additional AudioSet pertaining be the reason why MMV outperforms MIL-NCE by 1.3% on UCF? What is the performance of MMV trained only on HowTo100M? - AVTS uses MC3 architecture, which has an inferior performance to TSM-50 on action recognition tasks. Could this be a factor why AVTS shows worse performance on UCF compared to MMV? What is the performance of AVTS using TSM-50? Or what is the performance of MMV using MC3? Overall, the lack of a fair direct comparison with any of the previous approaches undermines the claim that MMV is a better representation-learning model. The authors should compare with at least one method under the same settings (the same architecture and the same pretraining dataset).

Review Point: The main weakness of this work is the lack of direct comparisons with previous works. The authors claim that they outperform the state-of-the-art on UCF, HMDB, and ESC, but fail to compare with other approaches under the same backbone architecture and the same pertaining dataset. For example, the following concerns arise from table 1:
Review Point: - Is MMV with TSM-50 better than ELo because MMV has a better architecture and was pretrained on a larger dataset? What is the performance of ELo pretrained on AudioSet+HowTo100M using TSM-50? Or What is the performance of MMV using R(2+1)D-50 and pretrained on YouTube8M?
Review Point: - XDC outperformed its R(2+1)D-18 fully-supervised pertaining baseline, but MMV did not exceed its fully-supervised pertaining baseline. On the other hand, MMV shows better performance than XDC. Can the authors explain why this is the case? Is MMV better than XDC because it uses a better backbone architecture?
Review Point: - MIL-NCE uses the same S3D backbone as MMV, but MIL-NCE is only pretrained on HowTo100M. Could the additional AudioSet pertaining be the reason why MMV outperforms MIL-NCE by 1.3% on UCF? What is the performance of MMV trained only on HowTo100M?
Review Point: - AVTS uses MC3 architecture, which has an inferior performance to TSM-50 on action recognition tasks. Could this be a factor why AVTS shows worse performance on UCF compared to MMV? What is the performance of AVTS using TSM-50? Or what is the performance of MMV using MC3? Overall, the lack of a fair direct comparison with any of the previous approaches undermines the claim that MMV is a better representation-learning model. The authors should compare with at least one method under the same settings (the same architecture and the same pretraining dataset).
==================================================

Focused review:

Weakness
Flaws in the proposed formulation
The paper claims that MIM is a special case of the siamese approach using patch masking as data augmentation. However, it is not. MIM objective just needs to reconstruct the masked patches and has no explicit reason to make masked images invariant.
This flaw is hindered in the derivation. Specifically, the paper substitutes the original image (target for reconstruction)
x
with some network
d
ϕ
′
that satisfies
d
ϕ
′
(
f
θ
(
T
(
x
)
)
)
≈
T
(
x
)
. Thus, the training objective in Eq. 7 becomes not only a function of target encoder and decoder
d
ϕ
, but also a (fixed) network
d
ϕ
′
. Since
ϕ
and
ϕ
′
are different, the objective should not be called the siamese form. However, the paper simply approximates
ϕ
=
ϕ
′
, which is not true. Actually, this crude substitution makes the final objective in Eq. 8 becomes MIM + something, which is claimed to be
≈
MIM.
Flaws in the empirical evaluation
CKA analysis in Figure 1 does not suggest that MAE is learning occlusion invariant features. The similarity between MAE features of the original and masked images decreases as the layer increases. In contrast, the proposed R-MAE and C-MAE (which explicitly learns invariance) maintain the similarity. It indicates that they are indeed different from MAE.
Fine-tuning accuracy of R-MAE is lower than MAE. The paper explains that it is due to the relaxation of the constrained optimization in Eq. 7 to the regularized objective with weight
λ
in Eq. 8. It can be partly true since the constraint part of Eq. 7 is just MAE. However, then what is the contribution of the siamese part of Eq. 7? To check this, one may show the performance over varying
λ
. If the performance improves as one increases
λ
, one can conclude that the MIM part is the key to success and the siamese part may not be necessary.
The evaluation should include lower-level vision tasks such as object detection (COCO) and semantic segmentation (ADE20K). The reconstruction loss better for learning lower-level features, while the siamese approach more fits higher-level semantics (e.g., classification). I suspect that the proposed siamese variant would be much worse than MAE for those tasks.
C-MAE in Section 4 suggests that the siamese approach is different from MAE. The paper says that only using a masking augmentation suffered from the convergence issue and needed additional color augmentations. It supports that the MIM part rather than the siamese part was the key to the success for R-MAE.
Additional comments
The relation between reconstruction vs. contrastive objective was already discussed in the classic CPC [1] paper. Reconstruction is just a different (variational) way to approximate mutual information [2] from InfoNCE, and the connection between reconstruction and contrastive objectives in the mutual information perspective is well-known.
[1] van den Oord et al. Representation Learning with Contrastive Predictive Coding. arXiv 2018.
[2] Barber et al. The IM Algorithm: A Variational Approach to Information Maximization. NeurIPS 2003.


Review Point: Flaws in the proposed formulation The paper claims that MIM is a special case of the siamese approach using patch masking as data augmentation. However, it is not. MIM objective just needs to reconstruct the masked patches and has no explicit reason to make masked images invariant. This flaw is hindered in the derivation. Specifically, the paper substitutes the original image (target for reconstruction) x with some network d ϕ ′ that satisfies d ϕ ′ ( f θ ( T ( x ) ) ) ≈ T ( x ) . Thus, the training objective in Eq.
Review Point: 7 becomes not only a function of target encoder and decoder d ϕ , but also a (fixed) network d ϕ ′ . Since ϕ and ϕ ′ are different, the objective should not be called the siamese form. However, the paper simply approximates ϕ = ϕ ′ , which is not true. Actually, this crude substitution makes the final objective in Eq.
Review Point: 8 becomes MIM + something, which is claimed to be ≈ MIM. Flaws in the empirical evaluation CKA analysis in Figure 1 does not suggest that MAE is learning occlusion invariant features. The similarity between MAE features of the original and masked images decreases as the layer increases. In contrast, the proposed R-MAE and C-MAE (which explicitly learns invariance) maintain the similarity. It indicates that they are indeed different from MAE. Fine-tuning accuracy of R-MAE is lower than MAE. The paper explains that it is due to the relaxation of the constrained optimization in Eq.
Review Point: 7 to the regularized objective with weight λ in Eq.
Review Point: 8. It can be partly true since the constraint part of Eq.
Review Point: 7 is just MAE. However, then what is the contribution of the siamese part of Eq. 7? To check this, one may show the performance over varying λ . If the performance improves as one increases λ , one can conclude that the MIM part is the key to success and the siamese part may not be necessary. The evaluation should include lower-level vision tasks such as object detection (COCO) and semantic segmentation (ADE20K). The reconstruction loss better for learning lower-level features, while the siamese approach more fits higher-level semantics (e.g., classification). I suspect that the proposed siamese variant would be much worse than MAE for those tasks. C-MAE in Section 4 suggests that the siamese approach is different from MAE. The paper says that only using a masking augmentation suffered from the convergence issue and needed additional color augmentations. It supports that the MIM part rather than the siamese part was the key to the success for R-MAE. Additional comments The relation between reconstruction vs. contrastive objective was already discussed in the classic CPC [1] paper. Reconstruction is just a different (variational) way to approximate mutual information [2] from InfoNCE, and the connection between reconstruction and contrastive objectives in the mutual information perspective is well-known. [1] van den Oord et al. Representation Learning with Contrastive Predictive Coding. arXiv 2018. [2] Barber et al. The IM Algorithm: A Variational Approach to Information Maximization. NeurIPS 2003.
==================================================

Focused review:

Weaknesses:
1.Lock of novelty.
2.The reviewer thought the result in this paper was not very satisfactory. Not SOTA performance and VQAv2 SOTA is 84+ for now.
3.This article is very terribly written, perhaps with the intention of fooling the reviewers? For example, the authors claim that they propose a method to reduce flops, but in Table 2(a) of ablation, we can see that Method-Add and Method-Ours_gated have the same flops.


Review Point: 1.Lock of novelty.2.The reviewer thought the result in this paper was not very satisfactory. Not SOTA performance and VQAv2 SOTA is 84+ for now.
Review Point: 3.This article is very terribly written, perhaps with the intention of fooling the reviewers? For example, the authors claim that they propose a method to reduce flops, but in Table 2(a) of ablation, we can see that Method-Add and Method-Ours_gated have the same flops.
==================================================

Focused review:

My issues with the paper are twofold: 1) There exist no real world data where the classes are well-separated. Most real-world data are not curated; images suffer from different lighting and lens conditions; real images of objects to not occur in isolation, i.e. objects are always obscured by other objects. Indeed, the authors make statements to this effect under Broader Impact. 2) The classifier proposed in the existence proof essentially computes the distance of a test point to every point in the training data. Thus all of the training data must be available at test time. In practice, this would be infeasible, or contrary to policy.

Review Point: 1) There exist no real world data where the classes are well-separated. Most real-world data are not curated; images suffer from different lighting and lens conditions; real images of objects to not occur in isolation, i.e. objects are always obscured by other objects. Indeed, the authors make statements to this effect under Broader Impact.
Review Point: 2) The classifier proposed in the existence proof essentially computes the distance of a test point to every point in the training data. Thus all of the training data must be available at test time. In practice, this would be infeasible, or contrary to policy.
==================================================

Focused review:

weaknesses.)
Weaknesses:
W2: The method is mostly constructed on top of previous methods; there are no network changes or losses. There is a contribution in the signed distance function and a pipeline for transferable implicit displacement fields. Why are we using two SIRENs for f and d? Shouldn't the d be a simpler network?
W3: Considering the experimental result. I feel that the method will fail with noise because of things like the need for normal to the points. This is a very relevant fact, and it is not described in the document. There are small tests with noise in the appendices, but the level of noise added is almost 0.


Review Point: W2: The method is mostly constructed on top of previous methods; there are no network changes or losses. There is a contribution in the signed distance function and a pipeline for transferable implicit displacement fields. Why are we using two SIRENs for f and d? Shouldn't the d be a simpler network?
Review Point: W3: Considering the experimental result. I feel that the method will fail with noise because of things like the need for normal to the points. This is a very relevant fact, and it is not described in the document. There are small tests with noise in the appendices, but the level of noise added is almost 0.
==================================================

Focused review:

1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. 
Suggestions: 1. Move the Appendix header to page 11 Questions: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ? 

Review Point: 1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. Suggestions:
Review Point: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ?
==================================================

Focused review:

1. The kth-order Proximity is similar to prior work with no significant improvement in this part. Given such, the impact becomes smaller. 2. The performance for its major competitor, DCGN, is significantly lower than what it have reported in the original DCGN paper,. If the original experiment results are used, then the proposed model does not really outperform DCGN. I wouldn't mind upgrading my grade if there is a convincing explanation on this part.

Review Point: 1. The kth-order Proximity is similar to prior work with no significant improvement in this part. Given such, the impact becomes smaller.
Review Point: 2. The performance for its major competitor, DCGN, is significantly lower than what it have reported in the original DCGN paper,. If the original experiment results are used, then the proposed model does not really outperform DCGN. I wouldn't mind upgrading my grade if there is a convincing explanation on this part.
==================================================

Focused review:

Weaknesses
The phase of feature initialization (through the linear two-time pass algorithm) is independent of the later graph-based representation learning method. What would be the impact of an initialization method that is dependent on the downstream task?
The experimental evaluation both for the probability estimation and the VBPI tasks is performed more as an ablation study over various graph neural network encoders. To my knowledge, there are some deep learning approaches that deal with the problem of phylogenetic inference [Solis-Lemus 2022, Jiang 2022, Fioravanti 2017]. Could the authors discuss how their method is compared with theirs or explain why these alternative approaches may not be applicable?
[Thomas 1949] Elliptic problems in linear difference equations over a network. 1949
[Solis-Lemus 2022] Accurate Phylogenetic Inference with a Symmetry-preserving Neural Network Model. 2022
[Jiang 2022] Learning Hyperbolic Embedding for Phylogenetic Tree Placement and Updates. 2022
[Fioravanti 2017] Phylogenetic convolutional neural networks in metagenomics. 2017


Review Point: The phase of feature initialization (through the linear two-time pass algorithm) is independent of the later graph-based representation learning method. What would be the impact of an initialization method that is dependent on the downstream task? The experimental evaluation both for the probability estimation and the VBPI tasks is performed more as an ablation study over various graph neural network encoders. To my knowledge, there are some deep learning approaches that deal with the problem of phylogenetic inference [Solis-Lemus 2022, Jiang 2022, Fioravanti 2017]. Could the authors discuss how their method is compared with theirs or explain why these alternative approaches may not be applicable? [Thomas 1949] Elliptic problems in linear difference equations over a network.
Review Point: 1949 [Solis-Lemus 2022] Accurate Phylogenetic Inference with a Symmetry-preserving Neural Network Model.
Review Point: 2022 [Jiang 2022] Learning Hyperbolic Embedding for Phylogenetic Tree Placement and Updates.
==================================================

Focused review:

1) Missing Citations and prior work: [1] The MOCO paper is missing: Kaiming He, et al., Momentum Contrast for Unsupervised Visual Representation Learning, CVPR 2020. [2] Herzig, et al., Learning Canonical Representations for Scene Graph to Image Generation, ECCV 2020. Recently, the authors showed in [2] that by using a canonicalization process for graphs, the information is propagated in the graph better than deeper networks, and thus they can generate complex visual scenes. Maybe it could be relevant as a new data augmentation (see next bullet) 2) The data augmentations are very straightforward. I wonder if more sophisticated augmentations could be used, such as capturing invariance to logical equivalences (see [2]), permutation invariant for the nodes, and more. 3) Although the graph domain is new, the graph contrastive learning framework seems to be employed from past works (SimCLR and MOCO).

Review Point: 1) Missing Citations and prior work: [1] The MOCO paper is missing: Kaiming He, et al., Momentum Contrast for Unsupervised Visual Representation Learning, CVPR 2020. [2] Herzig, et al., Learning Canonical Representations for Scene Graph to Image Generation, ECCV 2020. Recently, the authors showed in [2] that by using a canonicalization process for graphs, the information is propagated in the graph better than deeper networks, and thus they can generate complex visual scenes. Maybe it could be relevant as a new data augmentation (see next bullet) 2) The data augmentations are very straightforward. I wonder if more sophisticated augmentations could be used, such as capturing invariance to logical equivalences (see [2]), permutation invariant for the nodes, and more.
Review Point: 3) Although the graph domain is new, the graph contrastive learning framework seems to be employed from past works (SimCLR and MOCO).
==================================================

Focused review:

- Regarding the comparisons in Table 1., I would have liked to see comparisons with [9], [10] and/or [11]. SMPL and ASAP are good, but they are blended with an initialization from the authors' method and a comparison with DHBC alone is not completely convincing. - The method requires the ground truth correspondences as supervision which is a strong assumption. Many related methods ([11, 13, 40] in the paper) investigated the unsupervised setting at least to some degree and therefore allow for training on non-synthetic data. - The authors decided to limit themselves to human shapes in this work, although there are no modelling assumption that clearly require this. While I can to some extent understand this decision, it would still be interesting to see some qualitative generalization results to other classes of shapes. I understand that your method requires the human template for training, but similar methods [11] also show at least some examples that are not human or only close to human shape.

Review Point: - Regarding the comparisons in Table 1., I would have liked to see comparisons with [9], [10] and/or [11]. SMPL and ASAP are good, but they are blended with an initialization from the authors' method and a comparison with DHBC alone is not completely convincing.
Review Point: - The method requires the ground truth correspondences as supervision which is a strong assumption. Many related methods ([11, 13, 40] in the paper) investigated the unsupervised setting at least to some degree and therefore allow for training on non-synthetic data.
Review Point: - The authors decided to limit themselves to human shapes in this work, although there are no modelling assumption that clearly require this. While I can to some extent understand this decision, it would still be interesting to see some qualitative generalization results to other classes of shapes. I understand that your method requires the human template for training, but similar methods [11] also show at least some examples that are not human or only close to human shape.
==================================================

Focused review:

- The improvement is not a large margin as shown in Table 2. Although the average score of LaCon-vanilla is better than others mostly but the difference ranges from -0.6 to 3.6, which is a small number. Moreover, the improvements on YelpRev, DBPedia, QNLI, and QQP are always less than 1. These four datasets are also not considered in the ablation study.
- Code is unavailable. It's hard to reproduce the results.
- Writing should be improved and double-checked. 
Please refer to the comments above.
Grammar errors:  - Line 90, “constrastive learning” -> “contrastive learning” - Figure 1 (a), there are two ‘ICL loss 2’s?
- line 237, “an clipped” -> “a clipped” - line 499-500 “a more strict experiments” ? 

Review Point: - The improvement is not a large margin as shown in Table 2. Although the average score of LaCon-vanilla is better than others mostly but the difference ranges from -0.6 to 3.6, which is a small number. Moreover, the improvements on YelpRev, DBPedia, QNLI, and QQP are always less than 1. These four datasets are also not considered in the ablation study.
Review Point: - Code is unavailable. It's hard to reproduce the results.
Review Point: - Writing should be improved and double-checked. Please refer to the comments above. Grammar errors:
Review Point: - Line 90, “constrastive learning” -> “contrastive learning” - Figure 1 (a), there are two ‘ICL loss 2’s?
Review Point: - line 237, “an clipped” -> “a clipped” - line 499-500 “a more strict experiments” ?
==================================================

Focused review:


- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
- For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
- It would be better to define some notations and give a clear definition of the "information axis", "word concreteness" and also "Markov chain information content".
- Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE. 

Review Point: - The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
Review Point: - For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
Review Point: - It would be better to define some notations and give a clear definition of the "information axis", "word concreteness" and also "Markov chain information content".
Review Point: - Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE.
==================================================

Focused review:

Weaknesses:
1. They did not compare the whole system with other SOTA open domain dialog models like RetGen (the successor of DialoGPT), meena (Adiwardana et al., 2020), know-edg (Li et al., 2020a)
2. They did not use the latest version of VHRED with linear Gaussian prior (Zhao and Kawahara 2020)  for evaluating their primitive language model. 
3. No back propagation of reward from dialogue management to language model.
4. They did not tackle other problems in open domain dialogue such as memory to register and personalize to user characterstics  , reasoning over common sense and facts.
    5.Most of the evaluation details are in the appendix and not in the main paper


Review Point: 1. They did not compare the whole system with other SOTA open domain dialog models like RetGen (the successor of DialoGPT), meena (Adiwardana et al., 2020), know-edg (Li et al., 2020a) 2. They did not use the latest version of VHRED with linear Gaussian prior (Zhao and Kawahara 2020) for evaluating their primitive language model.
Review Point: 3. No back propagation of reward from dialogue management to language model.
Review Point: 4. They did not tackle other problems in open domain dialogue such as memory to register and personalize to user characterstics , reasoning over common sense and facts.
Review Point: 5.Most of the evaluation details are in the appendix and not in the main paper
==================================================

Focused review:

- Some important analysis are missing: for example, what are the typical failure modes of the proposed method? What would happen if an object moves out of camera, or there are textureless patches (e.g. sky, water) that are confusing to learn from? Could the visual encoder learn to implicitly encode relative spatial location in each frame and use that as a shortcut? If so how to avoid it? - I found the interpretation of "contrastive learning with latent views" in section 2 a bit confusing: on one hand, the "views" presented in this paper are not latent, but well-defined visual patches on different frames; on the other, it's unclear what properties the representations of the latent views need to have. For example, when the two views have the same representations, \mathcal{L}_{contrast} can be effectively minimized without capturing the semantics. It would be great if the authors could further elaborate on this interpretation. - Why choose 7x7 as the training patch size? What would be the effect on performance and speed by increasing or decreasing this patch size? In Figure 4, the propagated masks seem quite high-resolution. How was this achieved?

Review Point: - Some important analysis are missing: for example, what are the typical failure modes of the proposed method? What would happen if an object moves out of camera, or there are textureless patches (e.g. sky, water) that are confusing to learn from? Could the visual encoder learn to implicitly encode relative spatial location in each frame and use that as a shortcut? If so how to avoid it?
Review Point: - I found the interpretation of "contrastive learning with latent views" in section 2 a bit confusing: on one hand, the "views" presented in this paper are not latent, but well-defined visual patches on different frames; on the other, it's unclear what properties the representations of the latent views need to have. For example, when the two views have the same representations, \mathcal{L}_{contrast} can be effectively minimized without capturing the semantics. It would be great if the authors could further elaborate on this interpretation.
Review Point: - Why choose 7x7 as the training patch size? What would be the effect on performance and speed by increasing or decreasing this patch size? In Figure 4, the propagated masks seem quite high-resolution. How was this achieved?
==================================================

Focused review:

- In Section 2, the authors mention that “To simplify the dataflow, we found it beneficial to symmetrise this matrix” (row 95). Is this actually a simplification of the method? In the target algorithm, it is crucial to follow the unidirectional recursion call (from child to parents). I think that by symmetrising the pointer matrix, the message passing benefits from a better exploration of the graph structure, leading to a more global representation for each node (not particularly necessary in this case). However, I don’t see why a node should send information both to children and to parents. (maybe by adding additional information on the edge, to differentiate between the two types of links child-parent or parent-child would make the process easier, while preserving the global view). - According to the paper, the processor applies only a single message-passing at each time step. Is it enough from the theoretical - algorithmic point of view? As long as the intermediate supervision urges the PGN to imitate the operations from the original algorithm, a number of iterations equal to the depth of the recursive call should be made (in order to reach the root of the current component). The height of each tree is probably very small, especially when using the compression trick. Is this the reason why a single step is enough? When increasing the number of nodes, eliminating the compression step or applying a more complicated algorithm, is it better to allow a variable number of iterations, computed as a function of size? - Running an additional ablative experiment, similar to the (Unrestricted) GNN but using the computed alpha coefficients as adjacency matrix (pi = alpha) would be great. It would indicate if a GAT architecture could learn sparser attention when it's necessary or it is mandatory to explicitly design that inside your network, using a pointer-network approach as in the current work.

Review Point: - In Section 2, the authors mention that “To simplify the dataflow, we found it beneficial to symmetrise this matrix” (row 95). Is this actually a simplification of the method? In the target algorithm, it is crucial to follow the unidirectional recursion call (from child to parents). I think that by symmetrising the pointer matrix, the message passing benefits from a better exploration of the graph structure, leading to a more global representation for each node (not particularly necessary in this case). However, I don’t see why a node should send information both to children and to parents. (maybe by adding additional information on the edge, to differentiate between the two types of links child-parent or parent-child would make the process easier, while preserving the global view).
Review Point: - According to the paper, the processor applies only a single message-passing at each time step. Is it enough from the theoretical - algorithmic point of view? As long as the intermediate supervision urges the PGN to imitate the operations from the original algorithm, a number of iterations equal to the depth of the recursive call should be made (in order to reach the root of the current component). The height of each tree is probably very small, especially when using the compression trick. Is this the reason why a single step is enough? When increasing the number of nodes, eliminating the compression step or applying a more complicated algorithm, is it better to allow a variable number of iterations, computed as a function of size?
Review Point: - Running an additional ablative experiment, similar to the (Unrestricted) GNN but using the computed alpha coefficients as adjacency matrix (pi = alpha) would be great. It would indicate if a GAT architecture could learn sparser attention when it's necessary or it is mandatory to explicitly design that inside your network, using a pointer-network approach as in the current work.
==================================================

Focused review:

- It is not clear whether this minimax rate is the best achievable within a more general family of hard instances. - It is also not clear what is the best achievable upper bound. That being said, I think this paper is making an interesting step towards the above goal(s).

Review Point: - It is not clear whether this minimax rate is the best achievable within a more general family of hard instances.
Review Point: - It is also not clear what is the best achievable upper bound. That being said, I think this paper is making an interesting step towards the above goal(s).
==================================================

Focused review:

Weakness - The authors specify they use pseudo-labels from both D_A and D_N in Eq 9 to train the network. Is this pseudo-label based self-training done together with noise aware training with raw labels? Or is it trained in alternating iterations with either of those losses? - Continuing on previous point, a large gain is seen with the use of pseudo-labels in training which suggests the model’s prediction after warm-up is better than the raw labels. In that case, the raw labels might differ from the pseudo-labels significantly. The small performance gap by removing negative learning in both pseudo and raw loss shows its more dependent on the positive learning using perceived labels. In this case, the network is being trained using both raw label and pseudo-label for the same sample at the same time, which could be different from each other. Is there a detailed explanation for this occurrence? - Have the authors tried not using raw labels after warm-up and only use the pseudo-labels for all loss formulations? This is interesting as biggest gain comes from pseudo-labels after the initial warm-up. - From appendix A.4, using noise robust loss on pseudo-labels performs lower than using positive learning from Eq 9. This finding is also interesting as the pseudo-label would also be assumed to have certain ambiguity to it that could benefit from the noise robust loss. Is the model underfit caused by pseudo-labels being very accurate that positive learning has better feedback than a toned-down robust loss? - Since the approach shows more impact in semi-supervised setting, perhaps including more study on that similar to FixMatch would benefit the paper and the readers more.


Review Point: - The authors specify they use pseudo-labels from both D_A and D_N in Eq 9 to train the network. Is this pseudo-label based self-training done together with noise aware training with raw labels? Or is it trained in alternating iterations with either of those losses?
Review Point: - Continuing on previous point, a large gain is seen with the use of pseudo-labels in training which suggests the model’s prediction after warm-up is better than the raw labels. In that case, the raw labels might differ from the pseudo-labels significantly. The small performance gap by removing negative learning in both pseudo and raw loss shows its more dependent on the positive learning using perceived labels. In this case, the network is being trained using both raw label and pseudo-label for the same sample at the same time, which could be different from each other. Is there a detailed explanation for this occurrence?
Review Point: - Have the authors tried not using raw labels after warm-up and only use the pseudo-labels for all loss formulations? This is interesting as biggest gain comes from pseudo-labels after the initial warm-up.
Review Point: - From appendix A.4, using noise robust loss on pseudo-labels performs lower than using positive learning from Eq 9. This finding is also interesting as the pseudo-label would also be assumed to have certain ambiguity to it that could benefit from the noise robust loss. Is the model underfit caused by pseudo-labels being very accurate that positive learning has better feedback than a toned-down robust loss?
Review Point: - Since the approach shows more impact in semi-supervised setting, perhaps including more study on that similar to FixMatch would benefit the paper and the readers more.
==================================================

Focused review:

Weaknesses: It is unclear whether the work reported in this paper represents a substantial advance over Perez-Beltrachini et al.'s (2016) method for selecting content. 
The authors do not directly compare the present paper to that one. It appears that the main novelty of this paper is the additional analysis, which is however rather superficial.
It is good that the authors report a comparison of how an NNLG baseline fares on this corpus in comparison to that of Wen et al. (2016).  However, the BLEU scores in Wen et al.'s paper appear to be much much higher, suggesting that this NNLG baseline is not sufficient for an informative comparison.
- General Discussion: The authors need to more clearly articulate why this paper should count as a substantial advance over what has been published already by Perez-Beltrachini et al, and why the NNLG baseline should be taken seriously.  In contrast to LREC, it is not so common for ACL to publish a main session paper on a corpus development methodology in the absence of some new results of a system making use of the corpus.
The paper would also be stronger if it included an analysis of the syntactic constructions in the two corpora, thereby more directly bolstering the case that the new corpus is more complex.  The exact details of how the number of different path shapes are determined should also be included, and ideally associated with the syntactic constructions.
Finally, the authors should note the limitation that their method does nothing to include richer discourse relations such as Contrast, Consequence, Background, etc., which have long been central to NLG. In this respect, the corpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more interesting and should be discussed in comparison to the method here.
References Marilyn Walker, Amanda Stent, François Mairesse, and Rashmi Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research (JAIR), 30:413–456.
Amy Isard, 2016. “ The Methodius Corpus of Rhetorical Discourse Structures and Generated Texts” , Proceedings of the Tenth Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia, May 2016.
--- Addendum following author response: Thank you for the informative response.  As the response offers crucial clarifications, I have raised my overall rating.  Re the comparison to Perez-Beltrachini et al.: While this is perhaps more important to the PC than to the eventual readers of the paper, it still seems to this reviewer that the advance over this paper could've been made much clearer.  While it is true that Perez-Beltrachini et al. "just" cover content selection, this is the key to how this dataset differs from that of Wen et al.  There doesn't really seem to be much to the "complete methodology" of constructing the data-to-text dataset beyond obvious crowd-sourcing steps; to the extent these steps are innovative or especially crucial, this should be highlighted.  Here it is interesting that 8.7% of the crowd-sourced texts were rejected during the verification step; related to Reviewer 1's concerns, it would be interesting to see some examples of what was rejected, and to what extent this indicates higher-quality texts than those in Wen et al.'s dataset.  Beyond that, the main point is really that collecting the crowd-sourced texts makes it possible to make the comparisons with the Wen et al. corpus at both the data and text levels (which this reviewer can see is crucial to the whole picture).
Re the NNLG baseline, the issue is that the relative difference between the performance of this baseline on the two corpora could disappear if Wen et al.'s substantially higher-scoring method were employed.  The assumption that this relative difference would remain even with fancier methods should be made explicit, e.g. by acknowledging the issue in a footnote.  Even with this limitation, the comparison does still strike this reviewer as a useful component of the overall comparison between the datasets.
Re whether a paper about dataset creation should be able to get into ACL without system results:  though this indeed not unprecedented, the key issue is perhaps how novel and important the dataset is likely to be, and here this reviewer acknowledges the importance of the dataset in comparison to existing ones (even if the key advance is in the already published content selection work).
Finally, this reviewer concurs with Reviewer 1 about the need to clarify the role of domain dependence and what it means to be "wide coverage" in the final version of the paper, if accepted. 

Review Point: It is unclear whether the work reported in this paper represents a substantial advance over Perez-Beltrachini et al.'s (2016) method for selecting content. The authors do not directly compare the present paper to that one. It appears that the main novelty of this paper is the additional analysis, which is however rather superficial. It is good that the authors report a comparison of how an NNLG baseline fares on this corpus in comparison to that of Wen et al. (2016). However, the BLEU scores in Wen et al.'s paper appear to be much much higher, suggesting that this NNLG baseline is not sufficient for an informative comparison.
Review Point: - General Discussion: The authors need to more clearly articulate why this paper should count as a substantial advance over what has been published already by Perez-Beltrachini et al, and why the NNLG baseline should be taken seriously. In contrast to LREC, it is not so common for ACL to publish a main session paper on a corpus development methodology in the absence of some new results of a system making use of the corpus. The paper would also be stronger if it included an analysis of the syntactic constructions in the two corpora, thereby more directly bolstering the case that the new corpus is more complex. The exact details of how the number of different path shapes are determined should also be included, and ideally associated with the syntactic constructions. Finally, the authors should note the limitation that their method does nothing to include richer discourse relations such as Contrast, Consequence, Background, etc., which have long been central to NLG. In this respect, the corpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more interesting and should be discussed in comparison to the method here. References Marilyn Walker, Amanda Stent, François Mairesse, and Rashmi Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research (JAIR), 30:413–456. Amy Isard, 2016. “ The Methodius Corpus of Rhetorical Discourse Structures and Generated Texts” , Proceedings of the Tenth Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia, May 2016. --- Addendum following author response: Thank you for the informative response. As the response offers crucial clarifications, I have raised my overall rating. Re the comparison to Perez-Beltrachini et al.: While this is perhaps more important to the PC than to the eventual readers of the paper, it still seems to this reviewer that the advance over this paper could've been made much clearer. While it is true that Perez-Beltrachini et al. "just" cover content selection, this is the key to how this dataset differs from that of Wen et al. There doesn't really seem to be much to the "complete methodology" of constructing the data-to-text dataset beyond obvious crowd-sourcing steps; to the extent these steps are innovative or especially crucial, this should be highlighted. Here it is interesting that 8.7% of the crowd-sourced texts were rejected during the verification step; related to Reviewer 1's concerns, it would be interesting to see some examples of what was rejected, and to what extent this indicates higher-quality texts than those in Wen et al.'s dataset. Beyond that, the main point is really that collecting the crowd-sourced texts makes it possible to make the comparisons with the Wen et al. corpus at both the data and text levels (which this reviewer can see is crucial to the whole picture). Re the NNLG baseline, the issue is that the relative difference between the performance of this baseline on the two corpora could disappear if Wen et al.'s substantially higher-scoring method were employed. The assumption that this relative difference would remain even with fancier methods should be made explicit, e.g. by acknowledging the issue in a footnote. Even with this limitation, the comparison does still strike this reviewer as a useful component of the overall comparison between the datasets. Re whether a paper about dataset creation should be able to get into ACL without system results: though this indeed not unprecedented, the key issue is perhaps how novel and important the dataset is likely to be, and here this reviewer acknowledges the importance of the dataset in comparison to existing ones (even if the key advance is in the already published content selection work). Finally, this reviewer concurs with Reviewer 1 about the need to clarify the role of domain dependence and what it means to be "wide coverage" in the final version of the paper, if accepted.
==================================================

Focused review:

- I am not a fan in general of introducing new CNN architectures (S-FC, S-CONV etc.), and the reason being that they remain an isolated academic exercise, with little impact to research in applied domains (computer vision or natural language processing). The reason for this is that the base neural network architectures are mostly fixed after years of refinement in the specific area of architecture selection, therefore it is very unlikely that any of these target communities will readily adapt to these new architectures, especially considering that while they perform reasonably well, they do not provide state-of-the-art results, and in fact, underperform compared to (more on this later). - The above point being said, I do agree that there is a lot of value in learning sparser variants of these architectures, and providing rigorous regularizers for deep neural network models. This paper does demonstrate benefits of \beta-LASSO over vanilla SGD on ResNet-18 for various kernel sizes, but that's about the only conventional architecture it has been tested on. The remainder of comparisons are done on the architectures introduced by the authors, which, while interesting, has limited applicability: there is no evidence to suggest that S-FC and S-CONV generalize to tougher problems (ImageNet, CUB-2011, etc.). - \beta-LASSO and Table 2: The authors suggest that \beta-LASSO is a variant of LASSO with thresholding. It is surprising then that comparisons with naive LASSO or other regularizers have been ommitted altogether (in Table 2), and a (somewhat arbitrary) set of benchmarks have been provided (and that too, only for CIFAR-10). A fair comparison would involve adjusting the regularizer (and not the optimization algorithm, which the authors have done in their benchmarks, with comparisons on Adam/RMSProp and SET). - In continuation with the above remark, a number of popular sparse NN training algorithms have been omitted altogether, e.g., https://homes.cs.washington.edu/~kusupati/pubs/kusupati20.pdf, https://arxiv.org/abs/1611.06694, https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf to name a few. Moreover, the central argument is to learn efficient architectures to replace fully-connected and convolutional networks, yet, there are no comparisons of actual model sparsity and FLOPs, something that is typical in the literature. - Finally, I believe that this research domain has moved on from toy experiments on CIFAR-10 and CIFAR-100, especially for heuristic-based regularization techniques, and the absence of experiments on ImageNet or other learning problems (only image classification is studied) raises questions on the scalability of the proposed algorithm itself.

Review Point: - I am not a fan in general of introducing new CNN architectures (S-FC, S-CONV etc.), and the reason being that they remain an isolated academic exercise, with little impact to research in applied domains (computer vision or natural language processing). The reason for this is that the base neural network architectures are mostly fixed after years of refinement in the specific area of architecture selection, therefore it is very unlikely that any of these target communities will readily adapt to these new architectures, especially considering that while they perform reasonably well, they do not provide state-of-the-art results, and in fact, underperform compared to (more on this later).
Review Point: - The above point being said, I do agree that there is a lot of value in learning sparser variants of these architectures, and providing rigorous regularizers for deep neural network models. This paper does demonstrate benefits of \beta-LASSO over vanilla SGD on ResNet-18 for various kernel sizes, but that's about the only conventional architecture it has been tested on. The remainder of comparisons are done on the architectures introduced by the authors, which, while interesting, has limited applicability: there is no evidence to suggest that S-FC and S-CONV generalize to tougher problems (ImageNet, CUB-2011, etc.).
Review Point: - \beta-LASSO and Table 2: The authors suggest that \beta-LASSO is a variant of LASSO with thresholding. It is surprising then that comparisons with naive LASSO or other regularizers have been ommitted altogether (in Table 2), and a (somewhat arbitrary) set of benchmarks have been provided (and that too, only for CIFAR-10). A fair comparison would involve adjusting the regularizer (and not the optimization algorithm, which the authors have done in their benchmarks, with comparisons on Adam/RMSProp and SET).
Review Point: - In continuation with the above remark, a number of popular sparse NN training algorithms have been omitted altogether, e.g., https://homes.cs.washington.edu/~kusupati/pubs/kusupati20.pdf, https://arxiv.org/abs/1611.06694, https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf to name a few. Moreover, the central argument is to learn efficient architectures to replace fully-connected and convolutional networks, yet, there are no comparisons of actual model sparsity and FLOPs, something that is typical in the literature.
Review Point: - Finally, I believe that this research domain has moved on from toy experiments on CIFAR-10 and CIFAR-100, especially for heuristic-based regularization techniques, and the absence of experiments on ImageNet or other learning problems (only image classification is studied) raises questions on the scalability of the proposed algorithm itself.
==================================================

Focused review:

1. Low clarity. Honestly speaking, I could not follow the discussion in the current manuscript at all, partly due to the lack of my knowledge for the previous methods presented by the same author group. 2. If my understanding is correct, the current manuscript does not contain any experimental comparisons with other previous methods related to statistical modelling of video signals.

Review Point: 1. Low clarity. Honestly speaking, I could not follow the discussion in the current manuscript at all, partly due to the lack of my knowledge for the previous methods presented by the same author group.
Review Point: 2. If my understanding is correct, the current manuscript does not contain any experimental comparisons with other previous methods related to statistical modelling of video signals.
==================================================

Focused review:

- It is unclear how difficult the generalization from algorithms for robust submodularity to robust sequence submodularity is. The authors do not explain this relationship, and it feels like a routine generalization, at least in parts. - Did not find any of the applications particularly well motivated from ML perspective. In particular, for movie recommendation, most movies are stand-alone, rather than dependent on a sequence. For movies that form explicit sequences, it seems unlikely that the user would adversarially skip some of the sequence. - Lack of empirical evaluation of the proposed approach. - The algorithms are very simple greedy approaches. The simplicity of the algorithms, combined with the lack of context on algorithms in the robust set case, is a drawback.

Review Point: - It is unclear how difficult the generalization from algorithms for robust submodularity to robust sequence submodularity is. The authors do not explain this relationship, and it feels like a routine generalization, at least in parts.
Review Point: - Did not find any of the applications particularly well motivated from ML perspective. In particular, for movie recommendation, most movies are stand-alone, rather than dependent on a sequence. For movies that form explicit sequences, it seems unlikely that the user would adversarially skip some of the sequence.
Review Point: - The algorithms are very simple greedy approaches. The simplicity of the algorithms, combined with the lack of context on algorithms in the robust set case, is a drawback.
==================================================

Focused review:

The relevance of this paper is entirely unclear, for multiple reasons: 1. The author themselves state "This work does not present any foreseeable societal consequence.", raising the question why we should we care about this work in the first place. 2. They don't make any detectable effort towards arguing for why their work is relevant in the paper either, rendering it a purely theoretical exercise. 3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.

Review Point: The relevance of this paper is entirely unclear, for multiple reasons:
Review Point: 1. The author themselves state "This work does not present any foreseeable societal consequence.", raising the question why we should we care about this work in the first place.
Review Point: 2. They don't make any detectable effort towards arguing for why their work is relevant in the paper either, rendering it a purely theoretical exercise.
Review Point: 3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.
==================================================

Focused review:

Weaknesses:
This paper appears to present surprising results (as indicated by the title -- The “Unreasonable” Effectiveness). However, the claims and contributions in this paper need to be rectified and re-evaluated.
First, a more rigorous way to treat the terminologies is needed. Pruning can be split into pruning a pretrained model (which is the traditional way), and pruning at initialization (i.e, pruning a randomly initialized network). What this paper really discussed and compared is pruning at initialization (PaI). PaI is only a relatively small track in pruning currently. And importantly, it has been shown PaI seriously underperforms the traditional pruning (pruning a pretraiend model), see [*1]. So the title should be “The Unreasonable Effectiveness of Random Pruning at Initialization”, not the most general pruning case.
Otherwise, if you want to claim random pruning can be on par with other pruning methods (let’s take the most simple magnitude pruning as example) under the traditional pruning case, please try to add this result: Prune a pretrained ResNet50 with 90% sparsity on ImageNet, compare random pruning with magnitude pruning (extensive previous methods actually have shown the latter is better). Then, if we are talking about pruning at initialization, the so-called “unreasonable effectiveness” is actually not that unreasonable, provided you’ve seen [*1], which is cited as Frankle et al. (2020b) in the paper but I don’t think it is well discussed. [*1] has quite a lot of overlap with this paper.
Specifically, in [*1], they propose several sanity-check rules to evaluate whether a kind of pruning criterion is really effective or not: Randomly shuffling, Reinitialization, Inversion, in their Sec. 5. Particularly, for the random shuffling, they shuffled the masks obtained by some PaI methods (like SNIP, SynFlow, GraSP) and found after mask random shuffling, these methods can achieve comparable accuracy (“All methods maintain accuracy or improve when randomly shuffled”). Randomly shuffling masks is essentially the same as random pruning. That is, in [*1], it has been shown that random pruning can perform on par with other PaI methods like SNIP. So the major point of this paper is actually reiterating what has been found in [*1].
Even similarly, in [*1], after discovering randomly shuffling masks does not hurt accuracy, they thus proposed “In other words, the useful information these techniques extract is not which individual weights to remove, but rather the layerwise proportions by which to prune the network”. Then look at the 2nd contribution claimed in this paper: “We further identify that appropriate layer-wise sparsity ratios can be an important booster for the performance of random pruning”. They are basically the same. Given above, this paper has non-trivial overlaps with [*1]. Although it presents more empirical evidence to show random pruning can be useful in some cases (out-of-distribution detection, uncertainty estimation, and adversarial robustness), yet the major claims and points are already established in [*1].
Also, in the experiments, some are questionable. E.g., they compare pruned WideResNet50 to ResNet50 (Sec. 4.2). Although they have similar #params, notably they are two different networks. The comparison is not an apple-to-apple comparison (not scientifically valid), no matter how surprising the result may look. To claim random pruning is effective, what should be done is to compare randomly pruned WideResNet50 to a dense WideResNet50, not ResNet50. If the authors can provide this result and still show they have similar accuracy, I will improve my rating.
[*1] Pruning Neural Networks at Initialization: Why are We Missing the Mark?. ICLR, 2021.


Review Point: This paper appears to present surprising results (as indicated by the title -- The “Unreasonable” Effectiveness). However, the claims and contributions in this paper need to be rectified and re-evaluated. First, a more rigorous way to treat the terminologies is needed. Pruning can be split into pruning a pretrained model (which is the traditional way), and pruning at initialization (i.e, pruning a randomly initialized network). What this paper really discussed and compared is pruning at initialization (PaI). PaI is only a relatively small track in pruning currently. And importantly, it has been shown PaI seriously underperforms the traditional pruning (pruning a pretraiend model), see [*1]. So the title should be “The Unreasonable Effectiveness of Random Pruning at Initialization”, not the most general pruning case. Otherwise, if you want to claim random pruning can be on par with other pruning methods (let’s take the most simple magnitude pruning as example) under the traditional pruning case, please try to add this result: Prune a pretrained ResNet50 with 90% sparsity on ImageNet, compare random pruning with magnitude pruning (extensive previous methods actually have shown the latter is better). Then, if we are talking about pruning at initialization, the so-called “unreasonable effectiveness” is actually not that unreasonable, provided you’ve seen [*1], which is cited as Frankle et al. (2020b) in the paper but I don’t think it is well discussed. [*1] has quite a lot of overlap with this paper. Specifically, in [*1], they propose several sanity-check rules to evaluate whether a kind of pruning criterion is really effective or not: Randomly shuffling, Reinitialization, Inversion, in their Sec.
Review Point: 5. Particularly, for the random shuffling, they shuffled the masks obtained by some PaI methods (like SNIP, SynFlow, GraSP) and found after mask random shuffling, these methods can achieve comparable accuracy (“All methods maintain accuracy or improve when randomly shuffled”). Randomly shuffling masks is essentially the same as random pruning. That is, in [*1], it has been shown that random pruning can perform on par with other PaI methods like SNIP. So the major point of this paper is actually reiterating what has been found in [*1]. Even similarly, in [*1], after discovering randomly shuffling masks does not hurt accuracy, they thus proposed “In other words, the useful information these techniques extract is not which individual weights to remove, but rather the layerwise proportions by which to prune the network”. Then look at the 2nd contribution claimed in this paper: “We further identify that appropriate layer-wise sparsity ratios can be an important booster for the performance of random pruning”. They are basically the same. Given above, this paper has non-trivial overlaps with [*1]. Although it presents more empirical evidence to show random pruning can be useful in some cases (out-of-distribution detection, uncertainty estimation, and adversarial robustness), yet the major claims and points are already established in [*1]. Also, in the experiments, some are questionable. E.g., they compare pruned WideResNet50 to ResNet50 (Sec. 4.2). Although they have similar #params, notably they are two different networks. The comparison is not an apple-to-apple comparison (not scientifically valid), no matter how surprising the result may look. To claim random pruning is effective, what should be done is to compare randomly pruned WideResNet50 to a dense WideResNet50, not ResNet50. If the authors can provide this result and still show they have similar accuracy, I will improve my rating. [*1] Pruning Neural Networks at Initialization: Why are We Missing the Mark?. ICLR, 2021.
==================================================

Focused review:

Weaknesses:  1. The central contribution of modeling weight evolution using ODEs hinges on the mentioned problem of neural ODEs exhibiting inaccuracy while recomputing activations. It appears a previous paper first reported this issue. The reviewer is not convinced about this problem. The current paper doesn't provide a convincing analytical argument or empirical evidence about this issue.   2. Leaving aside the claimed weakness of neuralODE, the idea of modeling weight evolution as ODE is itself very intellectually interesting and worthy of pursuit. But the empirical improvement reported in Table 1 over AlexNet, ResNet-4 and ResNet-10 is <= 1.75 % for both configurations. The improvement of decoupling weight evolution is in fact even small and not consistent - the improvement in ResNet for configuration 2 is smaller than keeping the evolution of parameters and activations aligned. The improvement for ablation study over neuralODE is also minimal. So, the empirical case for the proposed approach is not convincing.   3. The derivation of optimality conditions for the coupled formulation is interesting because of connections to a machine learning application (backpropagation) but a pretty standard textbook derivation from dynamical systems / controls point of view. 

Review Point: 1. The central contribution of modeling weight evolution using ODEs hinges on the mentioned problem of neural ODEs exhibiting inaccuracy while recomputing activations. It appears a previous paper first reported this issue. The reviewer is not convinced about this problem. The current paper doesn't provide a convincing analytical argument or empirical evidence about this issue.
Review Point: 2. Leaving aside the claimed weakness of neuralODE, the idea of modeling weight evolution as ODE is itself very intellectually interesting and worthy of pursuit. But the empirical improvement reported in Table 1 over AlexNet, ResNet-4 and ResNet-10 is <= 1.75 % for both configurations. The improvement of decoupling weight evolution is in fact even small and not consistent - the improvement in ResNet for configuration 2 is smaller than keeping the evolution of parameters and activations aligned. The improvement for ablation study over neuralODE is also minimal. So, the empirical case for the proposed approach is not convincing.
Review Point: 3. The derivation of optimality conditions for the coupled formulation is interesting because of connections to a machine learning application (backpropagation) but a pretty standard textbook derivation from dynamical systems / controls point of view.
==================================================

Focused review:

One of the main concerns is the size of the dataset used from training (4900 images). To train a deep architecture, this size is very small. It is well-known that deep models using smaller datasets often result in a lower test accuracy, perhaps because the training set is not sufficiently representative of the problem and the model might overfit. To address this, researchers have of the used transfer learning (pre-trained base CNNs that are trained over the large diverse datasets) and fine-tuned on the smaller dataset. Given the size of the AlexNet (61M parameters), I have a feeling that the model is overfitted for this particular experimental design and evaluation. In the untrained models, are model initialized with random weights or taken from the pre-trained model (e.g. ImageNet). This has not been clarified. AlexNet is one of the first CNN architectures. Since 2012 there has been significant advancement in CNN architectures (e.g. Inception, Xception, DenseNet, etc.) and thus, it would have been beneficial if the experimental evaluation has been carried out using one of these latest architectures. During the training of decoding experiments, the base models (ResNet and AlexNet) were frozen and these base models are trained on a different set of labels. This makes two doubts: 1) why base models trained on different set of labels when the goal is to measure the influence of the type of features (shape, color, and textures) and 2) if layers in base models are frozen during decoding than why not use the base model trained on natural images (e.g. ImageNet) The selection of upper layers in the base models. Is there any rationale behind the selection of pool3 layer? In pooling layers, there are no learnable parameters associated with them and thus, one would expect features extraction from other types layers (e.g. CONV and FC). It would have been appropriate to select features from last CONV layer. The reduction in number of units in fc6 and fc7 layers is not justified. In the original model, a significant contribution towards the performance is from these two layers. To my view, the reduction in units in these two layers has impacted the performance of the model. What are the target classes in both datasets (Navon: 23, Trifeature: 7)? Is it a unique combination of shape, color, and texture? Or their rotational and positional combinations as well. How the samples are generated using correlation? It is very important for predicting multiple features, but the explanation is unclear. For example, how do you generate image with conditional probability. For example, if one wishes to generate a sample shape to match color of 0.3. Is this implying that 30% of the given shape has the target color? If yes, then how that 30% of the shape is chosen? One chunk of 30% or randomly over entire shape? This is unclear. Moreover, it is also unclear what color is given to the rest 70% of the shape. The binary features datasets are unclear as well. What is 32-element binary vectors and how it is selected from the input dataset. How do you define easy feature and difficult features?

Review Point: One of the main concerns is the size of the dataset used from training (4900 images). To train a deep architecture, this size is very small. It is well-known that deep models using smaller datasets often result in a lower test accuracy, perhaps because the training set is not sufficiently representative of the problem and the model might overfit. To address this, researchers have of the used transfer learning (pre-trained base CNNs that are trained over the large diverse datasets) and fine-tuned on the smaller dataset. Given the size of the AlexNet (61M parameters), I have a feeling that the model is overfitted for this particular experimental design and evaluation. In the untrained models, are model initialized with random weights or taken from the pre-trained model (e.g. ImageNet). This has not been clarified. AlexNet is one of the first CNN architectures. Since 2012 there has been significant advancement in CNN architectures (e.g. Inception, Xception, DenseNet, etc.) and thus, it would have been beneficial if the experimental evaluation has been carried out using one of these latest architectures. During the training of decoding experiments, the base models (ResNet and AlexNet) were frozen and these base models are trained on a different set of labels. This makes two doubts:
Review Point: 1) why base models trained on different set of labels when the goal is to measure the influence of the type of features (shape, color, and textures) and 2) if layers in base models are frozen during decoding than why not use the base model trained on natural images (e.g. ImageNet) The selection of upper layers in the base models. Is there any rationale behind the selection of pool3 layer? In pooling layers, there are no learnable parameters associated with them and thus, one would expect features extraction from other types layers (e.g. CONV and FC). It would have been appropriate to select features from last CONV layer. The reduction in number of units in fc6 and fc7 layers is not justified. In the original model, a significant contribution towards the performance is from these two layers. To my view, the reduction in units in these two layers has impacted the performance of the model. What are the target classes in both datasets (Navon: 23, Trifeature: 7)? Is it a unique combination of shape, color, and texture? Or their rotational and positional combinations as well. How the samples are generated using correlation? It is very important for predicting multiple features, but the explanation is unclear. For example, how do you generate image with conditional probability. For example, if one wishes to generate a sample shape to match color of 0.3. Is this implying that 30% of the given shape has the target color? If yes, then how that 30% of the shape is chosen? One chunk of 30% or randomly over entire shape? This is unclear. Moreover, it is also unclear what color is given to the rest 70% of the shape. The binary features datasets are unclear as well. What is 32-element binary vectors and how it is selected from the input dataset. How do you define easy feature and difficult features?
==================================================

Focused review:

- The importance of the SGC condition remains unclear. In Line 129, the authors claimed that SGC condition is satisfied in some practical settings such as the training of deep neural networks, therefore the SGC condition should be regarded as an interesting special setting for nonconvex optimization. However, recent work [1,2] showed that the training of deep neural networks can be further regarded as a special task of convex optimization in the Neural tangent kernel (NTK) regime, which is a stronger condition than SGC. Therefore, the authors may want to clarify the importance of SGC by showing some more examples in machine learning. - The technique contribution of this work seems incremental. As the authors suggested, [VBS18] firstly studied the SGC condition under nonconvex setting and proposed that SGD costs O(1/\epsilon^2) gradient complexity to find first-order stationary points. Meanwhile, note that [AZL18] proposed a generic framework which could turn any algorithms for finding first-order stationary points into algorithms for finding approximate local minimizer, without hurting the convergence rate. Therefore, is it true that the convergence rate O(1/\epsilon^2) for SGD to find approximate local minimizer established in this paper can be directly deduced by combining existing results from [VBS18] and [AZL18]? The authors may want to discuss above issues to highlight their technique contribution in this paper. - The authors may want to add some discussion about how the SGC condition will affect the variance-reduction based algorithms such as [3,4,5,6]. [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. "A convergence theory for deep learning via over-parameterization." International Conference on Machine Learning. 2019. [2] Zou, Difan, et al. "Gradient descent optimizes over-parameterized deep ReLU networks." Machine Learning 109.3 (2020): 467-492. [3] Johnson, Rie, and Tong Zhang. "Accelerating stochastic gradient descent using predictive variance reduction." Advances in neural information processing systems. 2013. [4] Zhou, Dongruo, Pan Xu, and Quanquan Gu. "Stochastic nested variance reduction for nonconvex optimization." Advances in Neural Information Processing Systems. 2018. [5] Wang, Zhe, et al. "SpiderBoost and momentum: Faster variance reduction algorithms." Advances in Neural Information Processing Systems. 2019. [6] Nguyen, Lam M., et al. "Finite-sum smooth optimization with sarah." arXiv preprint arXiv:1901.07648 (2019).

Review Point: - The importance of the SGC condition remains unclear. In Line 129, the authors claimed that SGC condition is satisfied in some practical settings such as the training of deep neural networks, therefore the SGC condition should be regarded as an interesting special setting for nonconvex optimization. However, recent work [1,2] showed that the training of deep neural networks can be further regarded as a special task of convex optimization in the Neural tangent kernel (NTK) regime, which is a stronger condition than SGC. Therefore, the authors may want to clarify the importance of SGC by showing some more examples in machine learning.
Review Point: - The technique contribution of this work seems incremental. As the authors suggested, [VBS18] firstly studied the SGC condition under nonconvex setting and proposed that SGD costs O(1/\epsilon^2) gradient complexity to find first-order stationary points. Meanwhile, note that [AZL18] proposed a generic framework which could turn any algorithms for finding first-order stationary points into algorithms for finding approximate local minimizer, without hurting the convergence rate. Therefore, is it true that the convergence rate O(1/\epsilon^2) for SGD to find approximate local minimizer established in this paper can be directly deduced by combining existing results from [VBS18] and [AZL18]? The authors may want to discuss above issues to highlight their technique contribution in this paper.
Review Point: - The authors may want to add some discussion about how the SGC condition will affect the variance-reduction based algorithms such as [3,4,5,6]. [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. "A convergence theory for deep learning via over-parameterization." International Conference on Machine Learning. 2019. [2] Zou, Difan, et al. "Gradient descent optimizes over-parameterized deep ReLU networks." Machine Learning 109.3 (2020): 467-492. [3] Johnson, Rie, and Tong Zhang. "Accelerating stochastic gradient descent using predictive variance reduction." Advances in neural information processing systems. 2013. [4] Zhou, Dongruo, Pan Xu, and Quanquan Gu. "Stochastic nested variance reduction for nonconvex optimization." Advances in Neural Information Processing Systems. 2018. [5] Wang, Zhe, et al. "SpiderBoost and momentum: Faster variance reduction algorithms." Advances in Neural Information Processing Systems. 2019. [6] Nguyen, Lam M., et al. "Finite-sum smooth optimization with sarah." arXiv preprint arXiv:1901.07648 (2019).
==================================================

Focused review:

1.The author may need more experiments to show the effect of different transformations. Though in the supplementary the authors experimented with random rotation and the combination of random scale and rotation, more experiments on different models and larger datasets will make it more convincing. There might be space issue, but I think this experiment should also be put in the main paper rather than the supplementary. 2. The claim on model robustness to adversarial attack may be too strong. FGSM is just one type of adversarial attack approach. To claim the general model robustness, the authors may need more experiments on different adversarial attacks. While I can understand that the proposed method is not focused on adversarial attack, it still would be more precise to claim the robustness to FGSM attack based on the experiments in the paper. 3. Although it is stated that the training procedure of the proposed GradAug is similar to the regular network, the training time for each epoch might be longer due to sub-networks. It would be good to include such discussion and analysis. 4. In ImageNet classification experiment, the images are randomly resized to one of {224, 192, 160, 128}. It is not clear why these particular resolutions are selected. How the image resolution and the number of resolutions that the sub-networks can choose would affect the performance?

Review Point: 1.The author may need more experiments to show the effect of different transformations. Though in the supplementary the authors experimented with random rotation and the combination of random scale and rotation, more experiments on different models and larger datasets will make it more convincing. There might be space issue, but I think this experiment should also be put in the main paper rather than the supplementary.
Review Point: 2. The claim on model robustness to adversarial attack may be too strong. FGSM is just one type of adversarial attack approach. To claim the general model robustness, the authors may need more experiments on different adversarial attacks. While I can understand that the proposed method is not focused on adversarial attack, it still would be more precise to claim the robustness to FGSM attack based on the experiments in the paper.
Review Point: 3. Although it is stated that the training procedure of the proposed GradAug is similar to the regular network, the training time for each epoch might be longer due to sub-networks. It would be good to include such discussion and analysis.
Review Point: 4. In ImageNet classification experiment, the images are randomly resized to one of {224, 192, 160, 128}. It is not clear why these particular resolutions are selected. How the image resolution and the number of resolutions that the sub-networks can choose would affect the performance?
==================================================

Focused review:

1: Limited novelty: CLEARER uses multi-scale search space that consists of three types of modules: parallel module, transition module, and fusion module. All of these modules were originally proposed in [2, 1].The authors did not cite these works when mentioning the said modules throughout the paper. 2: The proposed method generates two different architectures: one for image denoising and the other for image deraining. It seems inconvenient, as for every new task we would have a different architecture. it would have been more desirable to aim for a universal network that can be trained for different image restoration tasks. 3: The authors claim several times that CLEARER is particularly designed for image restoration. However, they did not provide any analysis/insights of what makes it specific for image restoration. For instance, what makes it suitable for image denoising and image deraining, OR why it would not work for any other applications such as semantic segmentation? 4: Experiments aren’t comprehensive. Only one dataset is used for image deraining evaluation. The reviewer would recommend testing on other commonly used image deraining datasets, such as Rain100L, Rain100H, Rain1200,etc. And for image denoising, the reviewer would recommend testing on real image datasets, such as SIDD and DND. 5: It seems that the architecture generated by CLEARER is driven by the dataset on which it is trained on, rather than the particular image restoration task. The reviewer would suggest to search for an architecture for image deraining using CLEARER, but on a new dataset (not Rain800), and see if it yields the same architecture as in Fig. 4 of the supplementary material. [1] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5693–5703, 2019. [2] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, YangZhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 2020.

Review Point: 1: Limited novelty: CLEARER uses multi-scale search space that consists of three types of modules: parallel module, transition module, and fusion module. All of these modules were originally proposed in [2, 1].The authors did not cite these works when mentioning the said modules throughout the paper.
Review Point: 2: The proposed method generates two different architectures: one for image denoising and the other for image deraining. It seems inconvenient, as for every new task we would have a different architecture. it would have been more desirable to aim for a universal network that can be trained for different image restoration tasks.
Review Point: 3: The authors claim several times that CLEARER is particularly designed for image restoration. However, they did not provide any analysis/insights of what makes it specific for image restoration. For instance, what makes it suitable for image denoising and image deraining, OR why it would not work for any other applications such as semantic segmentation?
Review Point: 4: Experiments aren’t comprehensive. Only one dataset is used for image deraining evaluation. The reviewer would recommend testing on other commonly used image deraining datasets, such as Rain100L, Rain100H, Rain1200,etc. And for image denoising, the reviewer would recommend testing on real image datasets, such as SIDD and DND.
Review Point: 5: It seems that the architecture generated by CLEARER is driven by the dataset on which it is trained on, rather than the particular image restoration task. The reviewer would suggest to search for an architecture for image deraining using CLEARER, but on a new dataset (not Rain800), and see if it yields the same architecture as in Fig. 4 of the supplementary material. [1] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5693–5703, 2019. [2] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, YangZhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 2020.
==================================================

Focused review:

- Authors miss to cite and comment some very related work in network interpretability such as: Bau et al, CVPR 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. I think the work in Network Dissection is directly relevant to the topic discussed in the paper, particularly for its hierarchical analysis as well as focus on ImageNet. - It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics. - Why did authors restrict themselves to a smaller number of categories? I think it would be interesting to understand the behavior on a larger category set. - Have authors evaluated qualitatively some of the metrics? I think it would be interesting to see whether, additionally to the quantitative evaluation, the images have some features in common. After rebuttal: The authors have addressed most of my concerns and I will update my score to 7.

Review Point: - Authors miss to cite and comment some very related work in network interpretability such as: Bau et al, CVPR 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. I think the work in Network Dissection is directly relevant to the topic discussed in the paper, particularly for its hierarchical analysis as well as focus on ImageNet.
Review Point: - It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics.
Review Point: - Why did authors restrict themselves to a smaller number of categories? I think it would be interesting to understand the behavior on a larger category set.
Review Point: - Have authors evaluated qualitatively some of the metrics? I think it would be interesting to see whether, additionally to the quantitative evaluation, the images have some features in common. After rebuttal: The authors have addressed most of my concerns and I will update my score to 7.
==================================================

Focused review:

Weakness: 1. "The main limitation of our proposal is that the superiority over existing state-of-the-art algorithms may not be sufficiently significant." The authors themselves point this out. So what is the advantage of using Sinkhorn iterations? 2. Figure 3 (b) Why does the performance degrade with increase in sample size? Increasing the samples should better approximate the return distribution.


Review Point: 1. "The main limitation of our proposal is that the superiority over existing state-of-the-art algorithms may not be sufficiently significant." The authors themselves point this out. So what is the advantage of using Sinkhorn iterations?
Review Point: 2. Figure 3 (b) Why does the performance degrade with increase in sample size? Increasing the samples should better approximate the return distribution.
==================================================

Focused review:

Weaknesses:
The study is limited to only certain kinds of network models. Most notably, this work does not study artificial neural network models and tasks prevalent in machine learning, so some of the results might be misleading or confusing to the NeurIPS community. In particular, the only models used are a max-entropy style statistical model of recurrent dynamics and a simple dynamical RNN, both linear (in the max-ent model the sufficient statistics are linear) in the previous system state (in contrast to nonlinearities used for machine learning and present in real brains). Furthermore, the stimuli are assumed to be static vectors (eqn 1). The networks don't really have to solve any kind of task, as far as I can tell, except to reproduce certain distributions. These model choices are minimally motivated in the text, despite standing out to me as not standard compared to the work I know.
My confidence in the results is lower because it feels like a lot of details were relegated to the supplement, which I did not have time to read.
Small points by line
1: "implies" seems like wrong word
2: "number of possible architectures" this sentence struck me as imprecise, is this a new (minor) result? If so present as such.
89: "whose entries are all larger than zero" is not obvious, explain
100:
s
i
g
n
(
i
)
is incorrect notation, all indices
i
>
0
, what you mean is there is a label
s
i
∈
±
1
for each neuron
i
106: "100 ms" is arbitrary, you make no connection to a timescale in the text, the only mention of a step size I see is in Fig 1b, which was hard to find
181: Introducing this new model seems to require a new section. I had a hard time distinguishing between models 1 & 2 throughout the results, as the paper switches back and forth. Be explicit throughout.
183: add comma after "Here" in "Here neural activity"
207: "In learning and optimizing their function" is awkward
The models seem narrow to me, can the authors better motivate them and better discuss their limitations?
The last paragraph of section 3 discusses limitations much too briefly, expand.


Review Point: The study is limited to only certain kinds of network models. Most notably, this work does not study artificial neural network models and tasks prevalent in machine learning, so some of the results might be misleading or confusing to the NeurIPS community. In particular, the only models used are a max-entropy style statistical model of recurrent dynamics and a simple dynamical RNN, both linear (in the max-ent model the sufficient statistics are linear) in the previous system state (in contrast to nonlinearities used for machine learning and present in real brains). Furthermore, the stimuli are assumed to be static vectors (eqn 1). The networks don't really have to solve any kind of task, as far as I can tell, except to reproduce certain distributions. These model choices are minimally motivated in the text, despite standing out to me as not standard compared to the work I know. My confidence in the results is lower because it feels like a lot of details were relegated to the supplement, which I did not have time to read. Small points by line 1: "implies" seems like wrong word 2: "number of possible architectures" this sentence struck me as imprecise, is this a new (minor) result? If so present as such.
Review Point: 89: "whose entries are all larger than zero" is not obvious, explain 100: s i g n ( i ) is incorrect notation, all indices i > 0 , what you mean is there is a label s i ∈ ± 1 for each neuron i 106: "100 ms" is arbitrary, you make no connection to a timescale in the text, the only mention of a step size I see is in Fig 1b, which was hard to find 181: Introducing this new model seems to require a new section. I had a hard time distinguishing between models 1 & 2 throughout the results, as the paper switches back and forth. Be explicit throughout. 183: add comma after "Here" in "Here neural activity" 207: "In learning and optimizing their function" is awkward The models seem narrow to me, can the authors better motivate them and better discuss their limitations? The last paragraph of section 3 discusses limitations much too briefly, expand.
==================================================

Focused review:

- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. 
  * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. 
  * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
- The number of questions with multi-span answers in the proposed dataset is small. 
  * The sizes are Train: 6465, Val: 646, Test: 646. 
  * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. 
  * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. 
#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].
[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.
#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world 

Review Point: - How the proposed multiple-span answer setting is essential in real-world applications is unclear to me.
Review Point: * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting.
Review Point: * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
Review Point: - The number of questions with multi-span answers in the proposed dataset is small.
Review Point: * The sizes are Train: 6465, Val: 646, Test: 646.
Review Point: * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test.
Review Point: * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. #### Minor comments:
Review Point: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1]. [1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL. #### Typos:
Review Point: - L47: consistinga of -> consisting of - L348: real-word -> real-world
==================================================

Focused review:

weakness of the method. (I suppose you want to write Canonical TMNRE next…!)
5 - I do not understand these “consistency checks through local amortization”. I figure you are suggesting that through the use of synthetic data we can test the inference result by recovering the ground truth. If this is the case, this seems trivial/generally true to me and does not seem to be a real plus-point of your method in particular. The real issues with methods such as this arise from model-mismatch in the generative model, and hence there are no real tests for this. I think this claim is overstated, and unless I’ve gotten something wrong, I would rather see this removed. (It could be referenced in passing as a sanity check or for debugging -- but nothing more). I could not follow Section 3.3 as a result.
6 - According to Algorithm 1, the data points aren’t drawn from the prior, they are drawn from the truncated prior, and so is the ratio estimator targeting the correct quantity? This may just be a typo, i.e. Line 5 and Line 7 are identical (modulo a dash), whereas I believe Line 7 should drop the indicator term.
7 - Using the C2ST score seems a little biased to me. Since the other methods are computing full posteriors, and yours is only computing a single marginal, performing a two-sample test comparing yourself to the full posterior actually seems a little harsh on your own method -- even incorrect as a metric! I suppose this shows your method is working admirably, but also shows that maybe the solutions just happen to admit a factorized posterior (and hence are amenable to your method). Either way, I find using this as the only performance metric a bit of a head-scratcher. As mentioned above, I would like to see a more diverse suite of performance metrics examined.
8 - You recover N^2 marginal pairs, and N marginals. The N^2 + N marginals that you recover will all be dramatically different as a result of marginalizing over N-2 other variables. Practically speaking, how do you select which distribution to draw from for a particular distribution? Moreover, how do you actually go about generating a sample from the joint...?
Other Comments:
A - Are the authors able to comment on the difference between their method and that of Jeffrey & Wandelt? I believe that the work is sufficiently different to be easily classified as novel throughout, but I think a more substantiated explanation of any similarities and differences would be useful.
B - Does truncating the prior lead to sections of parameter space being permanently deleted? I don’t think the algorithm could recover if a section with mass were not covered. I realise this is true for most sampling algorithms, and Fig 4 suggests that the distribution starts broad and then narrows. But it strikes me as though it could be particularly dangerous in this setting if there is categorically no way to expand the distribution. Maybe consider adding some discussion of entropy regularization to the prior to the conclusion.
C - There is a relatively substantial limitations section in the supplementary materials. I would prefer it if some of these limitations were brought up and explored in the main text.
D - The cosmology experiment is by far the most interesting of the experiments. I do not understand why it was demoted to the supplementary materials.
E - There also seems to be a missed opportunity using the marginals recovered to establish independencies or conditional independencies in the posterior, where this method explicitly enforces the efficiency gains that these independences can provide.
F - An interesting point that is not strongly emphasised is that the same samples can be (re-)used to learn all of the ratio estimators.
G - I am also a little confused about the “local amortization”. This method is inherently sequential and conditioned on a single
x
o
, and so there is no real amortization going on. Around Line 190, you say that you train “locally amortized posteriors that are valid for parameters
θ
∈
Γ
(
m
)
”. Are suggesting that the function approximators are going to be someway accurate/informative over the entire domain of
Γ
(
m
)
? This should be explained more clearly if this is the case.
Minor/Typographical Comments:
Inline or wrapped figures should be avoided. They are bad for readability and flow. Figures should really be floated to the top (or bottom at a push) of the page.
I don’t understand the sentence beginning Line 30: “With existing…”.
Line 129: I don’t understand how a likelihood ratio estimator can be overconfident...!
Line 147: Incorrectly rendered characters (I only see boxes).
Some of the font sizes in the figures are TINY.
I don’t really understand what Figures 4(b-d) are showing.
Figure 6 does a much better job of showing the truncation compared to Figure 4a.
The discussion is very long and doesn’t add much (partly because the rest of the paper is pretty well written). This could be cut down and used for the cosmology example instead, or, for Figure 6.
The societal impact statement is too long. I think it is safe to say here: “There are no societal impacts.” (as is explicitly permitted in the submission instructions).


Review Point: of the method. (I suppose you want to write Canonical TMNRE next…!) 5 - I do not understand these “consistency checks through local amortization”. I figure you are suggesting that through the use of synthetic data we can test the inference result by recovering the ground truth. If this is the case, this seems trivial/generally true to me and does not seem to be a real plus-point of your method in particular. The real issues with methods such as this arise from model-mismatch in the generative model, and hence there are no real tests for this. I think this claim is overstated, and unless I’ve gotten something wrong, I would rather see this removed. (It could be referenced in passing as a sanity check or for debugging -- but nothing more). I could not follow Section 3.3 as a result.
Review Point: 6 - According to Algorithm 1, the data points aren’t drawn from the prior, they are drawn from the truncated prior, and so is the ratio estimator targeting the correct quantity? This may just be a typo, i.e. Line 5 and Line 7 are identical (modulo a dash), whereas I believe Line 7 should drop the indicator term.
Review Point: 7 - Using the C2ST score seems a little biased to me. Since the other methods are computing full posteriors, and yours is only computing a single marginal, performing a two-sample test comparing yourself to the full posterior actually seems a little harsh on your own method -- even incorrect as a metric! I suppose this shows your method is working admirably, but also shows that maybe the solutions just happen to admit a factorized posterior (and hence are amenable to your method). Either way, I find using this as the only performance metric a bit of a head-scratcher. As mentioned above, I would like to see a more diverse suite of performance metrics examined.
Review Point: 8 - You recover N^2 marginal pairs, and N marginals. The N^2 + N marginals that you recover will all be dramatically different as a result of marginalizing over N-2 other variables. Practically speaking, how do you select which distribution to draw from for a particular distribution? Moreover, how do you actually go about generating a sample from the joint...? Other Comments: A - Are the authors able to comment on the difference between their method and that of Jeffrey & Wandelt? I believe that the work is sufficiently different to be easily classified as novel throughout, but I think a more substantiated explanation of any similarities and differences would be useful. B - Does truncating the prior lead to sections of parameter space being permanently deleted? I don’t think the algorithm could recover if a section with mass were not covered. I realise this is true for most sampling algorithms, and Fig 4 suggests that the distribution starts broad and then narrows. But it strikes me as though it could be particularly dangerous in this setting if there is categorically no way to expand the distribution. Maybe consider adding some discussion of entropy regularization to the prior to the conclusion. C - There is a relatively substantial limitations section in the supplementary materials. I would prefer it if some of these limitations were brought up and explored in the main text. D - The cosmology experiment is by far the most interesting of the experiments. I do not understand why it was demoted to the supplementary materials. E - There also seems to be a missed opportunity using the marginals recovered to establish independencies or conditional independencies in the posterior, where this method explicitly enforces the efficiency gains that these independences can provide. F - An interesting point that is not strongly emphasised is that the same samples can be (re-)used to learn all of the ratio estimators. G - I am also a little confused about the “local amortization”. This method is inherently sequential and conditioned on a single x o , and so there is no real amortization going on. Around Line 190, you say that you train “locally amortized posteriors that are valid for parameters θ ∈ Γ ( m ) ”. Are suggesting that the function approximators are going to be someway accurate/informative over the entire domain of Γ ( m ) ? This should be explained more clearly if this is the case. Minor/Typographical Comments: Inline or wrapped figures should be avoided. They are bad for readability and flow. Figures should really be floated to the top (or bottom at a push) of the page. I don’t understand the sentence beginning Line 30: “With existing…”. Line 129: I don’t understand how a likelihood ratio estimator can be overconfident...! Line 147: Incorrectly rendered characters (I only see boxes). Some of the font sizes in the figures are TINY. I don’t really understand what Figures 4(b-d) are showing. Figure 6 does a much better job of showing the truncation compared to Figure 4a. The discussion is very long and doesn’t add much (partly because the rest of the paper is pretty well written). This could be cut down and used for the cosmology example instead, or, for Figure 6. The societal impact statement is too long. I think it is safe to say here: “There are no societal impacts.” (as is explicitly permitted in the submission instructions).
==================================================

Focused review:

1) the authors do not compare with the model in [15]: "Modeling long-and short-term temporal patterns with deep neural networks.". This restricts the potential impact of the model. 2) the model has many components whose hyper parameters are not fully provided (someone may have to trace them in the source code) 3) the paper doesn't propose a conceptual/computational novelty. it combines existing modules to achieves its results.

Review Point: 1) the authors do not compare with the model in [15]: "Modeling long-and short-term temporal patterns with deep neural networks.". This restricts the potential impact of the model.
Review Point: 2) the model has many components whose hyper parameters are not fully provided (someone may have to trace them in the source code) 3) the paper doesn't propose a conceptual/computational novelty. it combines existing modules to achieves its results.
==================================================

Focused review:

1. In Table 1, the convergence when c=0 is faster than the one when c>0. However, when c>0, the objective has an additional quadratic term than the case of c=0. From my experimence, adding a quadratic term always makes the algorithms faster. So Table 1 looks strange to me. It might better to give more intuitions and explanations. The sharpness condition may give a faster convergence than the QG condition. However, can the authors prove the convergence beyond the QG condition when c>0, for example, give a possible faster rate by exploiting the specification of the problem? 2. In Section 5.1, the authors claim that IPPA is slower than M-ISG. However, from my experience, the proximal point algorithm is always faster than the subgradient method in general. It might better to explain more. 3. Please confirm the Holderian growth condition is global or local.

Review Point: 1. In Table 1, the convergence when c=0 is faster than the one when c>0. However, when c>0, the objective has an additional quadratic term than the case of c=0. From my experimence, adding a quadratic term always makes the algorithms faster. So Table 1 looks strange to me. It might better to give more intuitions and explanations. The sharpness condition may give a faster convergence than the QG condition. However, can the authors prove the convergence beyond the QG condition when c>0, for example, give a possible faster rate by exploiting the specification of the problem?
Review Point: 2. In Section 5.1, the authors claim that IPPA is slower than M-ISG. However, from my experience, the proximal point algorithm is always faster than the subgradient method in general. It might better to explain more.
Review Point: 3. Please confirm the Holderian growth condition is global or local.
==================================================

Focused review:

- significance and novelty: is quite similar to closely related work [13,14] (and maybe PathNet), but does not compare to them. -- The discussion says "the major difference between [13,14] and ours is that they treat their network depends on the task, which might not be suitable for online continual learning setting." So the only difference is that they may underperform. This paper either has to compare to [13,14] (especially [13] is very similar and open-source), or show numbers in efficiency (FLOPs, test time, etc) to show that they cannot be used efficiently. - claims and evaluation -- fairness of number of parameters: As far as I can tell, HAT has 7.1M params, A-GEM uses ResNet18 which is 11M params. This paper uses 43M params which is not fair. It is also not fair to say all methods have similar efficiency at test time, since multiple blocks in the same layer can be on. -- The exploration trick improved performance quite a lot. It is great that this paper did an ablation study, but since it has a lot of hyperparameters and design choices (why H^1/2 not H, why sigmoid, how are gamma/kappa/epsilon chosen), it is unclear how the design choices are made and how the hyperparameter was tuned, as this would tweak the balance between learning and remembering. An analysis of sensitivity to hyperparameters is preferred. -- The paper is not *entirely* an online method, since the model is pre-trained on the first task (with multiple epochs) to make NAS training work. --- Was line 253 epochs referring to this? Or did this paper train anything else with more than 1 epoch?

Review Point: - significance and novelty: is quite similar to closely related work [13,14] (and maybe PathNet), but does not compare to them. -- The discussion says "the major difference between [13,14] and ours is that they treat their network depends on the task, which might not be suitable for online continual learning setting." So the only difference is that they may underperform. This paper either has to compare to [13,14] (especially [13] is very similar and open-source), or show numbers in efficiency (FLOPs, test time, etc) to show that they cannot be used efficiently.
Review Point: - claims and evaluation -- fairness of number of parameters: As far as I can tell, HAT has 7.1M params, A-GEM uses ResNet18 which is 11M params. This paper uses 43M params which is not fair. It is also not fair to say all methods have similar efficiency at test time, since multiple blocks in the same layer can be on. -- The exploration trick improved performance quite a lot. It is great that this paper did an ablation study, but since it has a lot of hyperparameters and design choices (why H^1/2 not H, why sigmoid, how are gamma/kappa/epsilon chosen), it is unclear how the design choices are made and how the hyperparameter was tuned, as this would tweak the balance between learning and remembering. An analysis of sensitivity to hyperparameters is preferred. -- The paper is not *entirely* an online method, since the model is pre-trained on the first task (with multiple epochs) to make NAS training work. --- Was line 253 epochs referring to this? Or did this paper train anything else with more than 1 epoch?
==================================================

Focused review:

Weaknesses>
Although we can learn many things from Table 1, I have a concern on the experiments in the aspect of the limited evaluation. Many researchers have given attention to the powerful generation capability of Transformer decoders. But, this paper only measured the perplexities, the probabilistic results of language models. I think it seems to be not sufficient to note that there is no performance degradation on generation tasks. How about zero/few-shot evaluation results? With various kinds of tasks, the contribution of this paper would be strengthened.
I have also another concern on the acceleration of this method, the vector-wise quantization and decomposition method. There is a short explanation about acceleration in Appendix A and Table 3. But I think it is not sufficient to prove there is no acceleration overhead. There is no description on the environmental setup of inference measurement. I think this paper should include some arguments about this issue in detail.
To support their assertions, I think this paper can be refined in two aspects: 1) additional experimental results on performance of generation models, and 2) additional arguments in the acceleration environment and implementation.


Review Point: Although we can learn many things from Table 1, I have a concern on the experiments in the aspect of the limited evaluation. Many researchers have given attention to the powerful generation capability of Transformer decoders. But, this paper only measured the perplexities, the probabilistic results of language models. I think it seems to be not sufficient to note that there is no performance degradation on generation tasks. How about zero/few-shot evaluation results? With various kinds of tasks, the contribution of this paper would be strengthened. I have also another concern on the acceleration of this method, the vector-wise quantization and decomposition method. There is a short explanation about acceleration in Appendix A and Table 3. But I think it is not sufficient to prove there is no acceleration overhead. There is no description on the environmental setup of inference measurement. I think this paper should include some arguments about this issue in detail. To support their assertions, I think this paper can be refined in two aspects:
Review Point: 1) additional experimental results on performance of generation models, and 2) additional arguments in the acceleration environment and implementation.
==================================================

Focused review:

Weakness: 1. It is better to discuss the relation with dimensional collapse in contrastive learning. Also, it does not compare with the methods addressed the limitation. For example, in the following paper, it uses the subspaces to represent global image manifold and develops new method to prevent the collapse in the representation space. Li Jing, Pascal Vincent, Yann LeCun, Yuandong Tian. Understanding Dimensional Collapse in Contrastive Self-supervised Learning. ICLR 2022. 2. It is better to detail the solution process of (2). For large image sets, how to solve problem (2) efficiently? 3. It is better to discuss the possibility to use sophisticated manifold geometries instead of elliptical.


Review Point: 1. It is better to discuss the relation with dimensional collapse in contrastive learning. Also, it does not compare with the methods addressed the limitation. For example, in the following paper, it uses the subspaces to represent global image manifold and develops new method to prevent the collapse in the representation space. Li Jing, Pascal Vincent, Yann LeCun, Yuandong Tian. Understanding Dimensional Collapse in Contrastive Self-supervised Learning. ICLR 2022.
Review Point: 2. It is better to detail the solution process of (2). For large image sets, how to solve problem (2) efficiently?
Review Point: 3. It is better to discuss the possibility to use sophisticated manifold geometries instead of elliptical.
==================================================

Focused review:

• The protocol the authors use looks a bit artificial. Why is this the order of play? What is the motivation for the agent observing the classifier? Why can Nature pick the feature vector arbitrarily (and not stochastically, from an unknown but fixed distribution)? Why can’t the learner commit to a distribution over classifiers and not a single classifier? I’m not claiming against the proposed model, but I wish the authors could better justify their modeling decisions. • As the authors themselves admit, Grinder is impractical due to runtime complexity blowup. It also means that the experimental part is extremely synthetic. On the positive side, the authors acknowledge this weakness and provide several options for reducing the algorithm’s runtime (approximation oracle and an ML-based approach with Logistic Regression). • In many ML applications, we use convex surrogates to non-convex problems (to illustrate, finding the best linear predictor in the offline problem w.r.t. binary loss takes exponential time in the dimension). These surrogates can be optimized efficiently AND turn to be robust w.r.t. binary loss (e.g., SVM or Logistic Regression). This raises two questions: 1) Are there any convex surrogate that could fit this setting? Even if their formal guarantees are weaker or they are trivial to implement, and 2) In the simulations part, the authors do not consider a convex surrogate-based algorithm as a baseline, which I think is misleading. Sure, we expect their algorithm to work better, but by how much? Quantifying this extent could help to assess the tradeoff between runtime and accuracy.

Review Point: • The protocol the authors use looks a bit artificial. Why is this the order of play? What is the motivation for the agent observing the classifier? Why can Nature pick the feature vector arbitrarily (and not stochastically, from an unknown but fixed distribution)? Why can’t the learner commit to a distribution over classifiers and not a single classifier? I’m not claiming against the proposed model, but I wish the authors could better justify their modeling decisions.
Review Point: • As the authors themselves admit, Grinder is impractical due to runtime complexity blowup. It also means that the experimental part is extremely synthetic. On the positive side, the authors acknowledge this weakness and provide several options for reducing the algorithm’s runtime (approximation oracle and an ML-based approach with Logistic Regression).
Review Point: • In many ML applications, we use convex surrogates to non-convex problems (to illustrate, finding the best linear predictor in the offline problem w.r.t. binary loss takes exponential time in the dimension). These surrogates can be optimized efficiently AND turn to be robust w.r.t. binary loss (e.g., SVM or Logistic Regression). This raises two questions:
Review Point: 1) Are there any convex surrogate that could fit this setting? Even if their formal guarantees are weaker or they are trivial to implement, and 2) In the simulations part, the authors do not consider a convex surrogate-based algorithm as a baseline, which I think is misleading. Sure, we expect their algorithm to work better, but by how much? Quantifying this extent could help to assess the tradeoff between runtime and accuracy.
==================================================

Focused review:

1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. 
2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. 
Here are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? 
2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? 
3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? 

Review Point: 1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison.
Review Point: 2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. Here are some questions:
Review Point: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue?
Review Point: 2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement?
Review Point: 3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues?
==================================================

Focused review:

Weakness:
1.The idea that uses replay buffer is not quite novel. It is common for rl methods trained with replay buffer. I think it is novel to use the learned critic in previous rounds, can this technique helps for single RL algorithms?
2.The performance curve of each algorithm during the training is missing.
3.For the experimental details, how is the preference vector decided in practice?
4.The critic function is related with the preference vector, can does the model generalize well on this variable?
5.How the performance change when the number of objectives increases?


Review Point: 1.The idea that uses replay buffer is not quite novel. It is common for rl methods trained with replay buffer. I think it is novel to use the learned critic in previous rounds, can this technique helps for single RL algorithms?
Review Point: 2.The performance curve of each algorithm during the training is missing.
Review Point: 3.For the experimental details, how is the preference vector decided in practice?
Review Point: 4.The critic function is related with the preference vector, can does the model generalize well on this variable?
Review Point: 5.How the performance change when the number of objectives increases?
==================================================

Focused review:

- More details on how exactly the topic related chit-chat turns would have strengthened the paper. What are prompts provided to the blender bot and the impact of different prompts on the quality of generated data?
- Also, Blenderbot details for TOD simulation can be expanded in section 2.3. For instance, what is the impact of using mergeSGD vs TOD simulation on the overall quality ?
- The paper seems to lack details on performance of the intent detector model and QA models and their impact on the quality of the dialogs generated. It would be nice to have an ablation study on the quality of dialogs using different intent detectors (including the data used to train). 
During the transition turn, did the process also check if the user is requesting for more information or a question before switching to TOD setting ? 

Review Point: - More details on how exactly the topic related chit-chat turns would have strengthened the paper. What are prompts provided to the blender bot and the impact of different prompts on the quality of generated data?
Review Point: - Also, Blenderbot details for TOD simulation can be expanded in section 2.3. For instance, what is the impact of using mergeSGD vs TOD simulation on the overall quality ?
Review Point: - The paper seems to lack details on performance of the intent detector model and QA models and their impact on the quality of the dialogs generated. It would be nice to have an ablation study on the quality of dialogs using different intent detectors (including the data used to train). During the transition turn, did the process also check if the user is requesting for more information or a question before switching to TOD setting ?
==================================================

Focused review:

Weaknesses:
The motivation for the particular approach is not extremely clear. The authors build the argument around “transportability of patterns”, but I feel the point is never fully explained. If an action is never taken under a certain context in the past, (meaning is in the non-overlapping set), the algorithm will have to extrapolate the outcome. In order for the extrapolation to hold some assumptions need to be made about the reward model, and I find the authors do not really spend time on this.
The resulting objective is closely resembling the counterfactual loss proposed in [1], eq. 3. I think the authors should expand on this approach in the related work, and highlight that unlike in the cited work, they can choose the counterfactual world in which they want to act. Furthermore, I think borrowing either the counterfactual notation or the policy learning notation and vocabulary would help quite a lot in explaining the approach.


Review Point: The motivation for the particular approach is not extremely clear. The authors build the argument around “transportability of patterns”, but I feel the point is never fully explained. If an action is never taken under a certain context in the past, (meaning is in the non-overlapping set), the algorithm will have to extrapolate the outcome. In order for the extrapolation to hold some assumptions need to be made about the reward model, and I find the authors do not really spend time on this. The resulting objective is closely resembling the counterfactual loss proposed in [1], eq.
Review Point: 3. I think the authors should expand on this approach in the related work, and highlight that unlike in the cited work, they can choose the counterfactual world in which they want to act. Furthermore, I think borrowing either the counterfactual notation or the policy learning notation and vocabulary would help quite a lot in explaining the approach.
==================================================

Focused review:

Weaknesses
(W1) Incremental improvement. Compared to the baseline (BiAF), the proposed method achieves very small improvements across all tasks even in the low training data setting.
(W2) Reliance on prior representations. Although representation learning is not the main goal of the paper, the disconnect with the representation learning model puts it in a position of disadvantage. In experiments, they first train the baseline model (Roberta+BiAF) on the training data and then use its representation (assuming - prefinal layer) as features for the proposed model. Compared to end-to-end models, the proposed method could perform worse due to the lack of proper features.
General Comment. Overall, I think it is an interesting paper. Despite weak practical gains, the work could be helpful to other researchers in the community working on such structured prediction problems.


Review Point: (W1) Incremental improvement. Compared to the baseline (BiAF), the proposed method achieves very small improvements across all tasks even in the low training data setting.
Review Point: (W2) Reliance on prior representations. Although representation learning is not the main goal of the paper, the disconnect with the representation learning model puts it in a position of disadvantage. In experiments, they first train the baseline model (Roberta+BiAF) on the training data and then use its representation (assuming - prefinal layer) as features for the proposed model. Compared to end-to-end models, the proposed method could perform worse due to the lack of proper features. General Comment. Overall, I think it is an interesting paper. Despite weak practical gains, the work could be helpful to other researchers in the community working on such structured prediction problems.
==================================================

Focused review:

1. Evaluation setup is not completely clear. Authors do not mention whether they re-trained the baselines (PIFu and DeepHuman) with their images or directly used the pre-trained models. This information is critical as the training data (number of samples and quality of scans) for PIFu was very different than the DeepHuman dataset. 2. It is not clear whether the authors kept the networks comparable for the ablation studies in Table 2. In line 290, authors write that they add several FC layers to the fusion network. It is not clear whether the number of parameters were kept the same for Exp-a,b. One would suspect that multi scale 3D features with more parameters should eventually also capture the features that the 2D branch captures. More details on this experiment would be useful. 3. Authors briefly mention 3D GAN losses in Sec. 4.3 and provide a qualitative example in Fig 5d, but this experiment is not sufficiently explored. It would be useful to add quantitative results in Table 2 (one qualitative example is hardly enough). 4. Authors briefly mention PIFuHD but do not compare against it as a baseline. Though original PIFuHD was trained with high res images, the approach can be meaningfully retrained with regular images as well. This comparison is useful because PIFuHD has significantly better results than PIFu. 5. When works such as ARCH or DeepHuman leverage the coarse 3D reconstruction, they use parametric models such as SMPL (ARCH uses a different model). This greatly constraints the coarse prediction to have a valid human shape (no missing parts, body proportions are respected etc.). In the proposed approach the predicted coarse shape need not have these constraints, hence the predicted coarse shape could still have the problems regarding missing parts (very common when a limb is fully occluded), foreshortening effects and other ambiguities. This makes the argument for using coarse 3D shape weaker. It would be useful to elaborate on this and concretely argue how is making an intermediate unconstrained 3D prediction still useful for the reconstruction (authors do show quantitative results in table 2 but adding some intuitive discussion would be useful, also see pt.2). 6. Sec 3.1.1: The proposed feature learning approach is very similar to IF-Net, Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion, CVPR'20. Chibane et. al. It would be good to cite the paper and highlight the differences in Sec 3.1.1 7. Line 146-147: Authors write that geometry aligned features are good at predicting coarse human shape but not so good for fine details (hence they need the image aligned features) but IF-Net shows that multi-scale grid aligned features are good at making local prediction and key for detailed reconstruction. PiFuHD makes similar argument for their two branch network. It would be useful to elaborate on this further. [MINOR] 1. Line 75: 'then' -> 'than'. 2. Additional references for parametric 3D reconstruction: 2.1 Learning to Reconstruct People in Clothing from a Single RGB Camera, CVPR'19. Alldieck et al. Reconstructs full 3D shape from images. 2.2 Multi-Garment Net: Learning to Dress 3D People from Images, ICCV'19. Bhatnagar et al. Reconstructs body shape + clothing as separate meshes from images. 2.3 3D People: Modeling the Geometry of Dressed Humans, ICCV'19. Pumarola et. al. Use geometry image to reconstruct detailed 3D avatars from images. (2.1, 2.2 can be cited to support the claims in line 106, 107) 3.Line 110: 'on the' -> 'on whether the' 4. Sec 3, In eq. 1 \sigma is used for predicted occupancy, in Eq. 4 it is used for GT occupancy. 5. Line 117, training data should be {I, \sigma(P|I)} not {I, \sigma(P)} as occupancy is a function of both point and the input. Also it is a bit confusing to use \sigma as both occupancy value and a function. Overall, I think this would be a stronger paper if authors address pt. 1,2 and add more discussion regarding the key insights and design choices. ** Post Rebuttal ** Thanks for the rebuttal. + I like the idea of using geometry aligned features and using global shape as a proxy input. + I understand the author's point on not comparing their method against PIFuHD since it was CVPR'20. - My main concern is the technical novelty. The idea of using geometry aligned features (primary contribution of the work) is same as Chibane et al. CVPR'20. Authors address this point in the rebuttal (Chibane et al. use 3D voxels as input where as proposed method works on images) but the formulation is essentially the same. Due to this issue I'm not very confident regarding acceptance.

Review Point: 1. Evaluation setup is not completely clear. Authors do not mention whether they re-trained the baselines (PIFu and DeepHuman) with their images or directly used the pre-trained models. This information is critical as the training data (number of samples and quality of scans) for PIFu was very different than the DeepHuman dataset.
Review Point: 2. It is not clear whether the authors kept the networks comparable for the ablation studies in Table 2. In line 290, authors write that they add several FC layers to the fusion network. It is not clear whether the number of parameters were kept the same for Exp-a,b. One would suspect that multi scale 3D features with more parameters should eventually also capture the features that the 2D branch captures. More details on this experiment would be useful.
Review Point: 3. Authors briefly mention 3D GAN losses in Sec. 4.3 and provide a qualitative example in Fig 5d, but this experiment is not sufficiently explored. It would be useful to add quantitative results in Table 2 (one qualitative example is hardly enough).
Review Point: 4. Authors briefly mention PIFuHD but do not compare against it as a baseline. Though original PIFuHD was trained with high res images, the approach can be meaningfully retrained with regular images as well. This comparison is useful because PIFuHD has significantly better results than PIFu.
Review Point: 5. When works such as ARCH or DeepHuman leverage the coarse 3D reconstruction, they use parametric models such as SMPL (ARCH uses a different model). This greatly constraints the coarse prediction to have a valid human shape (no missing parts, body proportions are respected etc.). In the proposed approach the predicted coarse shape need not have these constraints, hence the predicted coarse shape could still have the problems regarding missing parts (very common when a limb is fully occluded), foreshortening effects and other ambiguities. This makes the argument for using coarse 3D shape weaker. It would be useful to elaborate on this and concretely argue how is making an intermediate unconstrained 3D prediction still useful for the reconstruction (authors do show quantitative results in table 2 but adding some intuitive discussion would be useful, also see pt.2).
Review Point: 6. Sec 3.1.1: The proposed feature learning approach is very similar to IF-Net, Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion, CVPR'20. Chibane et. al. It would be good to cite the paper and highlight the differences in Sec 3.1.1 7. Line 146-147: Authors write that geometry aligned features are good at predicting coarse human shape but not so good for fine details (hence they need the image aligned features) but IF-Net shows that multi-scale grid aligned features are good at making local prediction and key for detailed reconstruction. PiFuHD makes similar argument for their two branch network. It would be useful to elaborate on this further. [MINOR] 1. Line 75: 'then' -> 'than'.
Review Point: 2. Additional references for parametric 3D reconstruction: 2.1 Learning to Reconstruct People in Clothing from a Single RGB Camera, CVPR'19. Alldieck et al. Reconstructs full 3D shape from images. 2.2 Multi-Garment Net: Learning to Dress 3D People from Images, ICCV'19. Bhatnagar et al. Reconstructs body shape + clothing as separate meshes from images. 2.3 3D People: Modeling the Geometry of Dressed Humans, ICCV'19. Pumarola et. al. Use geometry image to reconstruct detailed 3D avatars from images. (2.1, 2.2 can be cited to support the claims in line 106, 107) 3.Line 110: 'on the' -> 'on whether the' 4. Sec 3, In eq.
Review Point: 5. Line 117, training data should be {I, \sigma(P|I)} not {I, \sigma(P)} as occupancy is a function of both point and the input. Also it is a bit confusing to use \sigma as both occupancy value and a function. Overall, I think this would be a stronger paper if authors address pt. 1,2 and add more discussion regarding the key insights and design choices. ** Post Rebuttal ** Thanks for the rebuttal.
Review Point: + I like the idea of using geometry aligned features and using global shape as a proxy input.
Review Point: + I understand the author's point on not comparing their method against PIFuHD since it was CVPR'20.
Review Point: - My main concern is the technical novelty. The idea of using geometry aligned features (primary contribution of the work) is same as Chibane et al. CVPR'20. Authors address this point in the rebuttal (Chibane et al. use 3D voxels as input where as proposed method works on images) but the formulation is essentially the same. Due to this issue I'm not very confident regarding acceptance.
==================================================

Focused review:

- how does the CDN-SDN perform on KITTI stereo benchmark? The authors somehow only report its performance on KITTI 3D detection and Sceneflow and I wonder why? - Whats the rationale that "CDN-GANet Deep" performs worse than its original counterpart for "all"? If the foreground is better, then it's essentially implying that the background is doing way worse. Why is that? Can the authors provide more analysis on this? Also, I notice that the improvement on occluded regions is quite limited comparing to non-occluded regions. Does this imply that the newly proposed solution will harm the hallucination capability of the model?

Review Point: - how does the CDN-SDN perform on KITTI stereo benchmark? The authors somehow only report its performance on KITTI 3D detection and Sceneflow and I wonder why?
Review Point: - Whats the rationale that "CDN-GANet Deep" performs worse than its original counterpart for "all"? If the foreground is better, then it's essentially implying that the background is doing way worse. Why is that? Can the authors provide more analysis on this? Also, I notice that the improvement on occluded regions is quite limited comparing to non-occluded regions. Does this imply that the newly proposed solution will harm the hallucination capability of the model?
==================================================

Focused review:

+ I find the significance novelty of this work to be minimal; since the “latent treatment representation learning” part is trivial and the “variational sample weight learning” part is already published in [6, 23]. The central contribution of this work appears to be Theorem 1; however, the proof seems to be wrong. + In the supplementary material, the authors introduced an updated version of the objective function that they actually use in their experiments. This makes reproducibility impossible for people who only have access to the main paper. The main paper should include all the vital information on reproducing the work while the supplementary material should only be used for examples, or reminding the readers of already known results.

Review Point: + I find the significance novelty of this work to be minimal; since the “latent treatment representation learning” part is trivial and the “variational sample weight learning” part is already published in [6, 23]. The central contribution of this work appears to be Theorem 1; however, the proof seems to be wrong.
Review Point: + In the supplementary material, the authors introduced an updated version of the objective function that they actually use in their experiments. This makes reproducibility impossible for people who only have access to the main paper. The main paper should include all the vital information on reproducing the work while the supplementary material should only be used for examples, or reminding the readers of already known results.
==================================================

Focused review:

weaknesses to the approach and the way it is presented in the paper:
W1: The experimental evaluation doesn't seem to report performance on the training set. Specifically, Table 1 and Table 2 contain results for the compositional and novel splits, but not for the train split (which I assume is different for the first two). I think this is important for evaluating how well the baselines have actually fit the data; it could be that their poor performance is due to inadequate expressivity or optimisation, and not because of some generalisation failure (e.g. due to lack of compositional structure in the algorithm).
W2: Clarity issues. I could not understand some parts of the method and experimental analysis, as described in the "further requests for clarification" below. There were also some areas where I felt the paper could be reworded for clarity, described in "minor issues and suggestions".
W3: The algorithm must use blind search over tasks when planning without unknown task description
t
. The paper acknowledges this as a weakness.
W4: Parts of the algorithm feel like they could be more principled. e.g.
L
looks a lot like a trajectory log likelihood under a Boltzmann model, but isn't quite a log likelihood (it's not obvious how to fit the task-contrastive term and the
log
⁡
G
/
log
⁡
I
terms into a probabilistic generative model). This doesn't influence my score much because it isn't the core conceptual contribution of the paper, but it might be interesting to think about how this framework could be grounded as maximum likelihood (or similar) on a probabilistic model.
Further requests for clarification:
CL1: Section 3: Why is there an explicit initiation condition
I
t
for each task
t
?
I
t
could also be left implicit: you could say that any state from which
G
t
is attainable must satisfy
I
t
.
Indeed, It seems to me that this property is necessary to interleave skills. As an example, say you want to achieve
t
=
a
 then 
b
. In the course of achieving
G
a
=
u
∧
v
, the planner also achieves some prerequisites
p
∧
q
to satisfying
G
b
=
p
∧
q
∧
r
. However, this means that
I
b
needs to be invariant to whether
p
and
q
have been satisfied: if
I
b
is only true when
¬
p
or
¬
q
hold, then achieving
G
a
(and
p
∧
q
along the way) is going to make it impossible to initiate the skill for
b
.
If we extend this argument, then it seems that
I
b
needs to be true for any state along any trajectory that achieves
G
b
, so that the agent can start executing
b
midway through. But if this is the case, then why not go the whole way and explicitly define
I
b
(
s
)
as true iff
G
b
is attainable from
s
?
CL2: Section 4.1: "For planning, we use the built-in deep-Q-learning algorithm." The original maxent IRL does (tabular, exact) soft-value iteration in order to plan. Why is deep Q-learning necessary for these tasks? How does the planner implement the entropy penalty requireed by maxent IRL?
CL3: Section 4.1: "For inverse planning, we rank all candidates by the consistency between the observation and the task-conditioned policy." Why use the policy instead of directly using the return to compare trajectories, given that
p
(
s
―
,
a
―
)
∝
exp
⁡
R
(
s
―
,
a
―
)
in the (deterministic) maximum entropy model?
CL4: How does the BC-FSM baseline incorporate an FSM? This was not clear to me from the description in Section 4.1.
CL5: Do the experiments use NLM feature extractors for all baselines, or just for RatSkills? (I think the answer is "yes", just want to double-check.)
CL6: Section A.2: I was quite confused by this description of FSM-A*, since it does not seem to resemble the description in the appendix. What does it mean to "prune" in A*? Why is this pruning only applied after the first
b
layers? Psuedocode may be useful here.
Minor issues and suggestions:
M1: When I saw "abstract language descriptions" in the abstract, I assumed the paper was going to focus on natural language. It wasn't until I got to page 4 that I understood that it was referring to a structured, logical language. This could be clarified by referring to rephrasing slightly, such as by calling them "abstract logical descriptions", or by making the given example look more formal, which is what I tried to do above by writing, e.g.,
turn-on-music
instead of "turn on music" (the current underlining in the abstract helps but was not sufficient for me to intuit the structure of the language).
M2: In Section 3.3, the description of A* refers to the priority used to pop nodes from the priority queue as a "heuristic function" ("Each node is associated with a heuristic value which adds up the total cost of the agent reaching this state and an estimated cost-to-go"). This is a bit weird; in the planning community, a "heuristic" is generally something that estimates cost-to-go. I'd recommend stealing terms from Artificial Intelligence: A Modern Approach, which calls the combined function an "evaluation function"
f
(
n
)
=
g
(
n
)
+
h
(
n
)
, which is the sum of the "path cost"
g
(
n
)
and the "heuristic value"
h
(
n
)
.
M3: While reading the "task language" section, I assumed "primitive skills" were the same thing as "primitive actions". It seems like this isn't the case; atomic skills in the task language exist at a strictly more abstract level than primitive actions from the MDP. I recommend exclusively referring to these as "atomic skills" or similar, and reserving "primitive" only for actions in the base-level MDP (which I think is typical nomenclature in hierarchial RL).
M4: Section 4.2: the appendix notes that the Crafting World environment has been modified relative to that used in past work. The fact that the experiments use a modified environment should be mentioned in the body of the paper, and the changes should be explicitly justified in the appendix.
M5: Appendix A.2: the sub-section which discusses optimality of A* search should also note that the heuristic must be admissible in order to guarantee optimal solutions (this will typically not be the case for learned heuristics).


Review Point: to the approach and the way it is presented in the paper:
Review Point: W1: The experimental evaluation doesn't seem to report performance on the training set. Specifically, Table 1 and Table 2 contain results for the compositional and novel splits, but not for the train split (which I assume is different for the first two). I think this is important for evaluating how well the baselines have actually fit the data; it could be that their poor performance is due to inadequate expressivity or optimisation, and not because of some generalisation failure (e.g. due to lack of compositional structure in the algorithm).
Review Point: W2: Clarity issues. I could not understand some parts of the method and experimental analysis, as described in the "further requests for clarification" below. There were also some areas where I felt the paper could be reworded for clarity, described in "minor issues and suggestions".
Review Point: W3: The algorithm must use blind search over tasks when planning without unknown task description t . The paper acknowledges this as a weakness.
Review Point: W4: Parts of the algorithm feel like they could be more principled. e.g. L looks a lot like a trajectory log likelihood under a Boltzmann model, but isn't quite a log likelihood (it's not obvious how to fit the task-contrastive term and the log ⁡ G / log ⁡ I terms into a probabilistic generative model). This doesn't influence my score much because it isn't the core conceptual contribution of the paper, but it might be interesting to think about how this framework could be grounded as maximum likelihood (or similar) on a probabilistic model. Further requests for clarification:
Review Point: CL1: Section 3: Why is there an explicit initiation condition I t for each task t ? I t could also be left implicit: you could say that any state from which G t is attainable must satisfy I t . Indeed, It seems to me that this property is necessary to interleave skills. As an example, say you want to achieve t = a then b . In the course of achieving G a = u ∧ v , the planner also achieves some prerequisites p ∧ q to satisfying G b = p ∧ q ∧ r . However, this means that I b needs to be invariant to whether p and q have been satisfied: if I b is only true when ¬ p or ¬ q hold, then achieving G a (and p ∧ q along the way) is going to make it impossible to initiate the skill for b . If we extend this argument, then it seems that I b needs to be true for any state along any trajectory that achieves G b , so that the agent can start executing b midway through. But if this is the case, then why not go the whole way and explicitly define I b ( s ) as true iff G b is attainable from s ?
Review Point: CL2: Section 4.1: "For planning, we use the built-in deep-Q-learning algorithm." The original maxent IRL does (tabular, exact) soft-value iteration in order to plan. Why is deep Q-learning necessary for these tasks? How does the planner implement the entropy penalty requireed by maxent IRL?
Review Point: CL3: Section 4.1: "For inverse planning, we rank all candidates by the consistency between the observation and the task-conditioned policy." Why use the policy instead of directly using the return to compare trajectories, given that p ( s ― , a ― ) ∝ exp ⁡ R ( s ― , a ― ) in the (deterministic) maximum entropy model?
Review Point: CL4: How does the BC-FSM baseline incorporate an FSM? This was not clear to me from the description in Section 4.1.
Review Point: CL5: Do the experiments use NLM feature extractors for all baselines, or just for RatSkills? (I think the answer is "yes", just want to double-check.) CL6: Section A.2: I was quite confused by this description of FSM-A*, since it does not seem to resemble the description in the appendix. What does it mean to "prune" in A*? Why is this pruning only applied after the first b layers? Psuedocode may be useful here. Minor issues and suggestions:
Review Point: M1: When I saw "abstract language descriptions" in the abstract, I assumed the paper was going to focus on natural language. It wasn't until I got to page 4 that I understood that it was referring to a structured, logical language. This could be clarified by referring to rephrasing slightly, such as by calling them "abstract logical descriptions", or by making the given example look more formal, which is what I tried to do above by writing, e.g., turn-on-music instead of "turn on music" (the current underlining in the abstract helps but was not sufficient for me to intuit the structure of the language).
Review Point: M2: In Section 3.3, the description of A* refers to the priority used to pop nodes from the priority queue as a "heuristic function" ("Each node is associated with a heuristic value which adds up the total cost of the agent reaching this state and an estimated cost-to-go"). This is a bit weird; in the planning community, a "heuristic" is generally something that estimates cost-to-go. I'd recommend stealing terms from Artificial Intelligence: A Modern Approach, which calls the combined function an "evaluation function" f ( n ) = g ( n ) + h ( n ) , which is the sum of the "path cost" g ( n ) and the "heuristic value" h ( n ) .
Review Point: M3: While reading the "task language" section, I assumed "primitive skills" were the same thing as "primitive actions". It seems like this isn't the case; atomic skills in the task language exist at a strictly more abstract level than primitive actions from the MDP. I recommend exclusively referring to these as "atomic skills" or similar, and reserving "primitive" only for actions in the base-level MDP (which I think is typical nomenclature in hierarchial RL).
Review Point: M4: Section 4.2: the appendix notes that the Crafting World environment has been modified relative to that used in past work. The fact that the experiments use a modified environment should be mentioned in the body of the paper, and the changes should be explicitly justified in the appendix.
Review Point: M5: Appendix A.2: the sub-section which discusses optimality of A* search should also note that the heuristic must be admissible in order to guarantee optimal solutions (this will typically not be the case for learned heuristics).
==================================================


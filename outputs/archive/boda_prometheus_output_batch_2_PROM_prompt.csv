review_id,source,focused_review,review_point,human_actionability,human_specificity,human_verifiability,human_politeness,Notes,llm_actionability,llm_feedback_actionability,llm_politeness,llm_feedback_politeness,llm_verifiability,llm_feedback_verifiability,llm_specificity,llm_feedback_specificity
3412,NIPS_2020_1180,"Here are the following things that I think are missing from this work and should be addressed: 1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture. 2. Competitive Baselines: I carefully looked through the outputs of Self-Tuning [2] and the results are equally impressive. The quantitative analysis and the human studies also seemingly suggest that. Impressively, [2] runs on a CPU with 8 core and the proposed formulation requires a Tesla V100 GPU. I would also point to the quality of results synthesized using Texture CNN. One may, however, complain about the amount of time it takes to synthesize a new texture using this approach. 3. I have some reservation about the evaluations. Please see the next section for specific details.","1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture.",-1.0,-1.0,1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of Universal Texture Synthesis. It correctly identifies the issue with the paper's claim of universal texture synthesis. However, the feedback lacks actionable suggestions that the author can use to improve their paper. It merely states the problem without offering any concrete steps or recommendations that guide the author in addressing those issues. The feedback does not provide a clear path for revision, which is essential for the author to improve their paper. Therefore, the feedback is not actionable, and it does not meet the criteria of providing specific, clear, and practical suggestions. So the score is -1. 
[RESULT] -1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the paper's claim of universal texture synthesis and then proceeds to critique it by citing a relevant study that demonstrates the limitations of the paper's approach. The feedback is delivered in a manner that is both respectful and encouraging, as it provides a clear direction for the authors to improve their work. The reviewer's tone is positive and constructive, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and addresses the claim of universal texture synthesis made in the paper. The reviewer has supported their claim by referencing external work (Zhou et al. [61]) and by providing a logical argument that the paper's work is limited to regular texture patterns. This demonstrates a clear understanding of the paper's claims and a critical evaluation of its limitations. The feedback is well-structured and directly addresses the aspect in question, making it a strong and valid critique. Therefore, the feedback is in line with the score rubric's criteria for a score of 1, as it imposes a certain claim and then supports it using external references and logical reasoning. 
[RESULT] 1",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the claim of universal texture synthesis in the paper and provides a clear critique by referencing a specific study (Zhou et al. [61]) that demonstrates the limitations of the paper's approach. The feedback also identifies the fundamental regularity assumption as a key issue, which is a specific point in the paper. This specificity in the feedback ensures that the critique is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria of the score rubric by being specific and directly relevant to the content of the draft. 
[RESULT] 1"
4916,NIPS_2020_1560,"The paper has a number of issues which are beyond the authors control. Most are caused by the extreme publication speed in the field and the limitations of the conference publishing model. Indeed all of my major concerns are of this type: 1. The use of ImageNet-V2 as primary example: The main issue I find in the paper is the use of ImageNet-V2 as the primary example for “natural distribution shift”. There is indeed a large gap between model performance on ImageNet and ImageNet-V2 but almost all of this can be attributed to subtle effects that arise in the dataset replication process as described by Engstrom et. al. 2020. Using this to criticize the paper is however unfair as Engstrom et. al. 2020 was published on 19. May 2020 only a week before the abstract deadline. ### Post-rebuttal comments: I did not know Shankar et. al., ICML ’20. Thanks for pointing out that reference. I have to look more deeply into this but judging from a quick read their results do indeed change my perception on the performance gap in ImageNet-V2. Nevertheless I think ObjectNet is the more obvious example and should be front and center. As Djolonga et. al. 2020 show it's the least correlated with other benchmarks with a very different design making it (in my current view) the most interesting of the selected benchmarks. 2. The definition of “distribution shifts arising in real data”: While the distribution shift from ImageNet to ImageNet-V2 has mostly been explained by Engstrom et. al. 2020 those to ObjetNet, ImageNet-vid-robust and YTBB-vid-robust can reasonably be expected to be real and existent. They do however only cover a subset of distribution shifts arising in real-world images. As to what is missing take for example ImageNet-R which was recently introduced by Hendrycks et. al. 2020. ImageNet-R shows real-world images not of the original objects but of different artistic renditions like paintings or sculptures. In this case some robustness interventions do have an effect. So the discussion of what constitutes a “natural distribution shift” has to be more nuanced. But as before this information was not available to the authors at time of submission because Hendrycks et. a. 2020 was published 3 weeks after the submission deadline. What turns this situation absurd is the fact that Hendrycks et. al. 2020 heavily builds upon what can readily be assumed to be an earlier version of the present article (not cited here to keep the double-blind mechanism as much intact as possible). ### Post-rebuttal comments: Thanks for including ImageNet-R even though it doesn't make the story easier. The dedication to completing the testbed is really amazing. 3. Big Transfer (BiT) models are missing from the analysis: The recently published Big Transfer model family (Kolesnikov et. al. 2019) was shown to have stunning generalization properties. The most interesting model of that family, BiT trained on the JFT300 dataset, has however not yet been publicly released. After seeing the L2-NoisyStudent model perform so well it would have been interesting to see if BiT-JFT can live up to it’s hype. Instead the authors of BiT have released their own robustness study using partly similar methodology as in the paper presented here (Djolonga et. al. 2020). This should not imply that Djolonga et. al. 2020 is biased or does something wrong but simply illustrate how fast paced the field has become. ### Post-rebuttal comments: Thanks for including BiT-M and reaching out to the authors. As I said above the commitment to completeness is great! 4. Too much information for 8 pages: It is pretty obvious that the amount of content presented in this paper is more than fits 8 pages in the NeurIPS template. I think the authors did a good job presenting their work in that format but when reading the paper it is still noticeable that there was much more content than could fit. It becomes even more obvious when reading the appendix which is full of exciting experiments that provide valuable information but have a good chance of being overlooked there. I want to repeat here that these problems are beyond the authors control. Most of it is caused by the huge amount of related work that was done in parallel and the conference submission system makes it impossible to publish longer papers or significantly update them during the review process. As a result I think it would be unfair to judge the submission based on these flaws. I would still appreciated if the authors could adapt their interpretations and related work prior to submission. Thus the following suggestions contain points regarding the above mentioned issues: 1. Use ObjectNet instead of ImageNet-V2 as the go to example (especially in Figures 1 & 5 as well as in Section 4) 2. Discuss different possibilities to select distribution shifts in real data in the introduction. State and motivate your choice. If I was asked I’d call them distribution shifts to ImageNet like images, as opposed to sketches, renditions, images with specific environmental factors like nighttime scenes or images taken in bad weather etc. which have a specific and easy to point out distribution shift. I think the second to last paragraph of the broader impact statement does a good job in justifying and contextualizing this approach and could be used here. 3. Follows thereof: Try to be a bit more specific as the chosen “natural” distribution shifts are just a subset of what is possible. The paper sometimes reads as if it covered all natural distribution shifts while it doesn’t (especially in section 1). Smaller suggestions: 4. Place “Dataset shifts” before “Consistency shifts” in section 3.1.1 or mention the video datasets in the first section 5. Change the description of “Image corruptions” in 3.1.2 removing the statement that you used corruptions from Geirhos et. al. 2019 which according to the appendix were not used and either don’t mention the number of corruptions (38) or explain why it’s 38 and not 19. I’d probably just remove that number as the nuanced discussion of “in memory” and “on disk” corruptions is only mentioned in the appendix. 6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”",6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”,1.0,1.0,0.0,0.0,nan,1,"The feedback provided in the review is actionable and directly addresses the issue of specifying the dataset on which the computation of ""Dataset shifts"" is performed. The feedback is clear and practical, offering a specific direction for the author to follow in order to improve their paper. It directly points out the need for specifying the dataset, which is a crucial aspect of the paper's content. The feedback is also concise, making it easy for the author to understand and implement the suggested changes. Therefore, the feedback is highly relevant to the review and effectively guides the author in addressing the identified issue. So the overall score is 1. [RESULT] 1",1,"The feedback provided is direct and to the point, which is a positive aspect. It is clear and respectful, focusing on the specific aspect of the dataset used in the paper. However, the tone of the feedback could be perceived as slightly more demanding than encouraging, which might not be as effective in motivating the authors to improve their work. The feedback does not include any negative or aggressive language, which is a good point. It is also factual and does not contain any passive or neutral language. The feedback is constructive and provides a clear direction for the authors to follow. Therefore, the feedback is relevant to the review and is followed or respected in the review. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is relevant to the review and directly addresses the aspect of dataset shifts. It specifically asks for clarification on which dataset is used in section 41, which is a logical and necessary step in verifying the claims made in the paper. The feedback is not only relevant but also respects the aspect by seeking to understand the details of the dataset used, which is crucial for the validity of the paper's findings. The feedback is also verifiable, as it can be checked against the paper's content. Therefore, the feedback is well-aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It clearly identifies the section of the paper that needs to be addressed, which is section 41. The feedback also specifies the aspect that needs to be clarified, which is the dataset on which the computation of ρ is performed. This level of specificity ensures that the feedback is tailored to the content of the paper and not a general comment for any scientific document. Therefore, the feedback aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1629,ICLR_2023_840,"Weaknesses:
Some important related works are missing: [1] tackles temporal causal discovery with Neural ODEs that would be able to handle inconistent sampling intervals, [2] performs joint structure learning and data imputation, [3] performs temporal causal discovery using the NOTEARS framework for continuous DAG learning. Methods based on the same framework have been applied to static data for joint causal discovery and data imputation [4]. All these weaken the novelty of this paper.
The paper mentions that it is based on Granger causality. However, the current formulation also allows for an interpretation as an additive noise model: ie x i = f ( p a i ) + e i
. Could you please comment on this? This interpretation would also allow for the identification of the temporal causal graph A 0 , τ
rather than just the summary graph A ^ = m a x t A t .
Please add some comments about the difference between Granger causality and ANMs or PCMCI that also identify the temporal causal graph.
It is unclear how τ m a x
is chosen. Is this assumed to be known? What if this isn't known?
All experiments use missing data. It would be great to see a baseline comparing to datasets with full observability. Misc:
What's the intuition of using the moving average as a training signal for the imputation network?
For the graph discovery stage - do you calculate an expectation over multiple graph samples or is this amortised over different batches? Or do you use the same graph sample for optimising this loss?
Please explain ZOH earlier in the text.
Eq 3: what is e
? What's the assumption about it? This might make or break the use of the L2 loss.
Please pay attention to the use of \citep and \citet.
Eq 5: You use inconsistent τ = 0. . . and τ = 1 , . . . .
p5 just above eq 11: I believe n 2
should be n 3 .
p2: "" to conduct causal inference and .."" - should this be ""causal discovery""? Causal inference tackles the question of inferring causal estimates (e.g. ATEs).
[1] Bellot, Alexis, Kim Branson, and Mihaela van der Schaar. ""Neural graphical modelling in continuous-time: consistency guarantees and algorithms."" International Conference on Learning Representations. 2021. [2] Morales-Alvarez, Pablo, et al. ""VICause: Simultaneous Missing Value Imputation and Causal Discovery with Groups."" arXiv preprint arXiv:2110.08223 (2021). [3] Pamfil, Roxana, et al. ""Dynotears: Structure learning from time-series data."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020. [4] Geffner, Tomas, et al. ""Deep End-to-end Causal Inference."" arXiv preprint arXiv:2202.02195 (2022).",... p5 just above eq 11: I believe n 2 should be n 3 .,1.0,1.0,0.0,0.0,nan,0,"The feedback provided in the response is not actionable. It merely points out a discrepancy in the notation without offering any guidance on how to correct it or why it is incorrect. The feedback lacks specificity and does not provide any practical suggestions for the author to improve their paper. It does not offer any concrete steps or recommendations that would help the author address the issue. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided is quite direct and lacks the polite and respectful tone that is expected in a scientific review. The feedback is focused on a specific mathematical error, but it does not offer any constructive guidance or encouragement to the authors. The tone is more critical than helpful, which could potentially discourage the authors from improving their work. The feedback does not attempt to engage with the authors in a positive manner, nor does it provide any positive reinforcement. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not encourage the authors. So the overall score is -1. 
[RESULT] -1",-1,"The feedback provided in the response is not supported by any evidence or logical reasoning. The claim that ""n 2 should be n 3"" is made without any reference to the paper or external sources. This lack of support for the claim makes it difficult to verify the feedback, which is a critical aspect of the score rubric. The feedback does not provide any context or explanation as to why the claim is made, which further diminishes its relevance and usefulness in the review process. Therefore, based on the score rubric, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of verifiability. Instead, it falls into the category of a score of -1, as it imposes a claim without providing any evidence for it. 
[RESULT] -1",-1,"The feedback provided is not specific enough to be relevant to the review. It does not mention any specific parts or methods in the paper, nor does it provide any context or explanation as to why the change from n 2 to n 3 is suggested. This lack of specificity makes it difficult to identify which part of the document is being discussed in the feedback. The feedback does not follow the aspect of specificity, as it does not address a certain point or design decision in the paper. Therefore, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of specificity in the review. 
[RESULT] -1"
1073,ICLR_2023_642,"Weaknesses:
Unclear notations. The authors used the same notations to write vectors and scalars. Reading these notations would be challenging to follow for many readers. Please consider updating your notations and refer to the notation section in the Formatting Instructions template for ICLR 23.
The framework impact is unclear. The authors mentioned that the case of intrinsic but known bias and variance is often the case in computational neuroscience and neuromorphic engineering. This is the main motivation for their approach. However, the framework provided is limited to specific cases, namely, white noise and fixed bias. The authors argue that their assumptions are reasonable for most cases computational scientists and neuromorphic engineers face, but they don’t provide evidence for their claims. Clearly, this framework provides an important way for analyzing methods such as perturbed gradient descent methods with Gaussian noise, but it’s unclear how it can help analyze other cases. This suggests that the framework is quite limited.
The authors need to show that their choices and assumption are still useful for computational neuroscience and neuromorphic engineering. This can happen by referring to contributing and important works from these fields having known bias and variance with Gaussian noise.
In the experiments, the used bias is restricted to having the same magnitude for all weights ( b 1 →
). Can we reproduce the results if we use arbitrary biases? It would be better if the authors tried a number of arbitrary biases and averaged the results.
The paper is not well-placed in the literature. The authors didn’t describe the related works fully (e.g., stochastic gradient Langevin dynamics). This makes the work novelty unclear since the authors didn’t mention how analyzing the gradient estimator was done in earlier works and how their contribution is discernible from the earlier contributions. Mentioning earlier contributions increases the quality of your work and makes it distinguishable from other work. Please also refer to my comment in the novelty section.
Missing evidence of some claims and missing details. Here, I mention a few:
It’s not clear how increasing the width and/or depth can lower the trace of the Hessian (Section 2.1). If this comes from a known result/theory, please mention it. Otherwise, please show how it lowers the trace.
The authors mentioned that they use an analytical and empirical framework that is agnostic to the actual learning rule. However, the framework is built on top of a specific learning rule. It’s unclear what is meant by agnostic in this context (Section 1).
The authors mentioned in the abstract that the ideal amount of variance depends on the size and sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. However, the authors didn’t mention the sparsity dependence anywhere in the paper.
The authors mentioned in a note after the proof of Theorem A.5 that it is also valid for Tanh but not Sigmoid. However, the proof assumed that the second derivative is zero. It’s unclear whether a similar derivation can be developed without this assumption. However, the authors only mention the relationship with the gain of ϕ ( . ) .
More information is needed on how the empirical likelihood of descent is computed (Fig. 7).
The use of MSE should be mentioned in Theorem A.3 since it’s not proven for any loss function. In addition, the dataset notation is wrong. It should be D = { ( x 1 , y 1 ) , . . . , ( x M , y M ) }
, where M
is the number of examples since it’s a set containing input-output pairs, not just a single pair.
The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead.
A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1).
The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work.
It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary.
The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30.
It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance.
The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results.
Alternative hypotheses can be made with some experiments. The experiment in Fig. 3.A needs improvement. The authors mention that excessive amounts of variance and/or bias can hinder learning performance. In Fig. 3. A, they only show levels of variance that help decrease loss. An alternative explanation from their figure is that by increasing the variance, the performance improves. This is not the case, of course, so I think the authors need to add more variance curves that hinder performance to avoid alternative interpretations.
Minor issues that didn’t impact the score:
There are nine arXiv references. If they are published, please add this information instead of citing arXiv.
What is a norm N
vector? Can you please add the definition to the paper?
You mentioned that the step size has to be very small. However, in Fig. 1, the step size used is large (0.02). Can you please explain why? Can this be an additional reason why there is no smooth relationship between the values of the variance and performance?
No error bars are added in Fig. 4 or Fig. 7. Can you please add them?
In experiments shown in Fig. 3 and Fig. 5, the number of runs used to create the error bars is not mentioned in Appendix B.2.
A missing 2 D
in Eq. 27.
In Theorem A.3 proof, how the input x
has two indices? The input is a vector, not a matrix. Moreover, shouldn’t ∑ k ( W k ( 2 ) ) 2 = 1 / d
, not d ?",".. , ( x M , y M ) } , where M is the number of examples since it’s a set containing input-output pairs, not just a single pair. The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead. A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1). The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work. It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary. The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30. It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance. The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results. Alternative hypotheses can be made with some experiments. The experiment in Fig.",1.0,1.0,1.0,1.0,"This is tricky, cause some comments lack some aspects, but others doesn't",0,"The feedback provided in the review is not actionable. It identifies issues with the paper but fails to offer specific, clear, and practical suggestions that the author can use to improve their paper. The feedback is more of a critique without providing any guidance on how to address the issues raised. For example, the feedback mentions that the experiment results are not consistent with the rest of the paper but does not suggest how to make the results more consistent. Similarly, the feedback points out that the experiment in Fig. 1 is misplaced and that the number of seeds is insufficient, but it does not provide any concrete steps or recommendations for the author to follow. The feedback also lacks in providing a clear path for revision, which is essential for the author to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided is quite critical and lacks the necessary politeness and respect that is expected in a scientific review. The tone is negative and the feedback is delivered in a way that could be perceived as aggressive. The reviewer does not seem to be trying to encourage the authors, but rather to point out the shortcomings of the paper. The feedback is also quite detailed, which could be seen as an attempt to provide constructive criticism, but the manner in which it is presented is not conducive to a positive review. The feedback does not seem to be trying to help the authors improve their work, but rather to criticize it. This is not in line with the expected tone of a scientific review, which should be polite, helpful, and positive. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not follow the aspect of politeness and respect in the review. So the score is -1. 
[RESULT] -1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiable feedback. The reviewer's claims are not supported by references to external sources, references to parts of the paper, or logical reasoning that can be independently verified. The feedback is based on the reviewer's personal opinion and lacks any evidence or logical reasoning to back up the claims. The reviewer's suggestions are not substantiated with any references or logical reasoning, making it difficult to verify the feedback. The feedback also fails to provide any constructive criticism or suggestions that could be independently verified. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is not specific to the content of the draft. It fails to mention specific parts or methods in the paper, making it difficult to identify the parts that are being addressed. The feedback is general and does not provide clear guidance on how to improve the paper. It lacks the specificity required to effectively critique the paper. The feedback does not address the relevance of the aspect to the review, nor does it respect the aspect in the review. Instead, it provides a broad critique without focusing on the specific aspects of the paper. This lack of specificity and relevance to the content of the draft is why the feedback does not meet the criteria for a score of 1. 
[RESULT] 0"
4133,NIPS_2020_73,"1. While I find the combination of single-image 3D reconstruction and GAN interesting, I am concerned about the technical contribution of the paper. It seems that each component is similar to previous works. The single-image 3D reconstruction network is almost identical to [24], and the GAN network also are standard. It feels like the contribution of the paper is just a combination of these two tasks. 2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network. 3. Why using the sinusoidal encoding in the network? How does it compare to directly using the (u, v) coordinates? Overall, I like the results of the paper. However, I am not fully convinced about the choice of the framework, particularly for the questions discussed in point 2. The technical contributions of the proposed method is also not significant to me.","2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network.",1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the response is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. Instead, it merely suggests an alternative solution without providing any guidance on how to implement it or why it would be beneficial. The feedback also fails to address the original aspect of the review, which is to evaluate the proposed framework and its potential advantages over existing methods. It does not provide any direction or hints on what needs to be done to improve the paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer offers an alternative solution to the problem, which demonstrates an understanding of the task and a willingness to engage with the authors' work. The feedback also acknowledges the potential of the proposed method, while expressing skepticism about its superiority to the suggested baseline. This approach is both respectful and encouraging, as it invites the authors to consider alternative approaches and to provide a more detailed comparison with the baseline. The feedback is also factual and does not contain any negative or aggressive language. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiable feedback. The response suggests an alternative solution to the task, but it does not provide any evidence or references to support its claims. The feedback is based on the reviewer's personal opinion and lacks logical reasoning or references to external sources that could be independently verified. The response does not cite any work or reference any part of the paper, which is a key requirement for a score of 1. Therefore, the feedback does not meet the criteria for verifiable feedback and is not supported by any evidence or logical reasoning. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is not specific to the content of the draft. It does not mention any specific parts or methods in the paper, making it difficult to identify which sections, statements, or elements in the paper are being addressed. The feedback is more of a general comment on the topic of 2D GANs and single-image 3D reconstruction networks, rather than a tailored critique of the paper. It lacks the specificity required to effectively critique the paper. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
1433,ICLR_2023_2880,"Weaknesses: 1. The first question is that the evidence of the motivation is not direct. Since the problem to be solved is that “a predictor suffers from the accuracy decline due to long-term and continuous usage”, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim. 2. Another question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match. 3. Is there any related work about the mixed-batch method?",3. Is there any related work about the mixed-batch method?,0.0,1.0,0.0,0.0,nan,-1,"The feedback provided in the review is not actionable. It merely asks a question about related work without offering any guidance or suggestions for the author to improve their paper. The feedback does not provide any direction or hints on what needs to be done, which is crucial for the author to address the issues raised in the review. Therefore, the feedback does not meet the criteria of being actionable, and it lacks the necessary elements to enhance the usefulness of the review. So the score is -1. [RESULT] -1",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. The feedback is simply asking for related work, which is a factual inquiry. However, the feedback could be more constructive by providing a more detailed explanation of why the related work is important or how it could be relevant to the authors' work. This would have made the feedback more helpful and positive, which would have earned a higher score. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it asks for related work on the mixed-batch method. However, it does not provide any evidence or references to support the claim, which is a key aspect of the score rubric. The feedback is a logical claim, but it lacks the necessary verification or citation of external work. Therefore, while the feedback is relevant, it does not fully meet the criteria for a score of 1, as it does not support its claim with evidence or references. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It asks for related work on the mixed-batch method, which is a clear and direct reference to a specific aspect of the paper. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback is also easy to identify which part of the document is being discussed, as it directly addresses the method in question. Therefore, the feedback meets the criteria of the score rubric, as it is specific, directly relevant, and easy to identify the parts being addressed in the document. So the score is 1. 
[RESULT] 1"
531,ICLR_2022_1119,"weaknesses, starting from the most significant ones.
Assumptions and Threat Model? This is probably the only “true” problem of the paper, which should be absolutely rectified. I was not fully able to understand the assumptions made by Tesseract. Does it work “only” against the “directed deviation attack” proposed by Fang et al.? Or does it also protect against different attacks? In general, Section 2.2, Threat Model, is not very comprehensive. The authors should better expand this section by clearly pointing out all the assumptions and requirements of the proposed method. This is especially true because the Fang et al. attack was proposed in 2020, and some of its assumptions are not yet well-known. Specifically, this statement is suspicious: “We assume a full-knowledge (white-box) attack where the attackers have access to the current benign gradients.”. Does it mean that Tesseract only works under this assumption? I.e., the attacker knows, and exploits, the current benign gradients? This is a rather “unrealistic” assumption: I understand the willingness to work against “worst case” scenarios; yet, if such “worst case” scenarios are not realistic in the first place, then what is the purpose of the proposed mechanism? What benefit is there in protecting against an attack that will never happen in the first place? I invite the authors to restructure this section by using the common taxonomies adopted in adversarial ML papers [I].
Problem or Feature Space attacks? The authors perform their experiments on four well-known datasets: MNIST, CIFAR, Shakespeare, FEMNIST; for each dataset, a different (deep) ML model is targeted. Three of these datasets are of images, whereas Shakespeare contains text data. There are different ways to create “adversarial examples”, depending on the ‘space’ where the perturbation is applied. As far as I am aware, the adversarial examples considered in this paper to perform the poisoned updates are created in the feature space. It would be a lot more interesting if at least one evaluation included adversarial examples generated in the “problem” space [A]—or, at the very least, considered samples generated by “physically realizable” adversarial perturbations [B]. I acknowledge that the method should work even in these circumstances, as the proposed Tesseract defense is agnostic of the process used to apply the perturbation. However, considering the strong relationship with (real) security that permeates the paper, I believe that a more convincing use-case would dramatically improve the quality of the paper. This is also motivated by the current state-of-the-art: after almost a decade of adversarial attacks, more recent efforts are leaning towards evaluation that consider more realistic circumstances, where the attacker is constrained by the limitations of the real world; this is even more true in “distributed system” scenarios, such as Network Intrusion Detection Systems, which bear a strong relationship with federated learning (e.g., [C, D, E, F]). As such, I invite the authors to perform an additional “proof-of-concept” experiment where they consider adversaries with constrained capabilities. This is also motivated by the fact that some perturbations may yield different effects when created in the problem space (as shown in [A]).
Tradeoff? A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients. Even if the baseline performance does not decrease, what is the overhead of the proposed method? For instance, in Table 2 the authors report some results for “Attack=None”, which I assume represent the accuracy when no attack takes place. However, all the rows of these experiments (namely, FedSGD, Tesseract, Faba, FoolsGold, FLTrust) consider hardening FL techniques; for instance, on MNIST the proposed Tesseract has an accuracy of 92.52 when no attack takes place—the best among all other defences. Despite being appreciable, I am interested in knowing the performance when NO defense is applied. Surely, the test accuracy in a “fully trusted” FL setting should be superior than 92.52. Hence, I ask: what is the ‘cost’ of Tesseract?
Lack of a concrete use-case. I believe that the paper could be further improved with a concrete use-case, where the authors explain, step-by-step, how a (single, or multiple) attacker can compromise a federated learning system, and how the proposed method can help in solving such problem. Hence, I request the description of a concrete use-case explaining the abstract scenario reported in Figure 1. Such use-case can be at the basis of the “constrained” attack that I invite the authors to perform in my ""problem space perturbations"" suggestion.
Some additional issues:
• In the Introduction, the authors state: “To counter this threat, a set of approaches has been developed for countering Byzantine clients in FL…”. I believe that “Byzantine Clients” is a wrong term: what is countered by Tesseract are not byzantine clients, but ""unloyal"" clients, that are “against” the byzantine clients (at least by referring to the well-known problem of the byzantine generals, which should agree on a method to reach consensus in the presence of unloyal generals).
• The caption of Figure 1 has a typo “c out of m clients maybe be malicious”.
• In Figure, the gradient “LM_{c-1}” is out of place.
• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?
• Figure 1 appears before Figure 2, but in the text it is referenced after Figure 2.
• Putting Figure 2 so early on is very confusing. The “flip score” is a measure introduced for the first time in this paper. As such, any reader would be thrown off by such graphs before reading the paper, meaning that the findings of Figure 2 are difficult to interpret---during the Introduction---, as the flip score has not been defined yet. As such, such graphs are ultimately meaningless: I have to trust the authors that they correspond to “interesting” observations and “fair” experiments, which is not scientific.
• The presentation and notation in the “Flip-score” (page 5) is very ugly and difficult to follow.
• Section 5 should be merged in Section 6
• W.r.t. Table 2, the authors state “We see that TESSERACT is the winner or 2nd place finisher in 7 of the 12 cells (benign + two attacks * 4 datasets)”. This should be better highlighted. I only see three bold values for Tesseract in Table 2.
• W.r.t. Table 2, the authors state “We have not shown the test loss curve for Krum aggregation because of the large loss values.”. I invite the authors to report such values in Table 2, because the different “formats” of the three subtables (None, Full-Krum, Full-Trim) make this table very hard to interpret.
EXTERNAL REFERENCES
[A]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[B]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Conaml: Constrained adversarial machine learning for cyber-physical systems."" Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021.
[F]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.
[I]: ""Wild patterns: Ten years after the rise of adversarial machine learning."" Pattern Recognition 84 (2018): 317-331.","• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?",0.0,1.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely questions the minimum amount of ""m"" without offering any guidance or suggestions on how the authors could address this question. The feedback does not provide any direction or hints on what needs to be done, which is crucial for the authors to improve their paper. It lacks the specificity and clarity that would make it a useful tool for the authors to revise their work. Therefore, the feedback does not meet the criteria of being actionable, and it does not contribute to the usefulness of the review. So the score is 0. [RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on a specific aspect of the paper, asking a question about the minimum amount of ""m"". This question is relevant to the review, as it pertains to the simulation of federated learning and the number of clients involved. However, the feedback does not provide any additional context or suggestions for improvement, which could have made the feedback more helpful and constructive. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is relevant to the review and directly addresses the aspect of the paper. It questions the minimum amount of ""m"" in the simulation, which is a critical aspect of the paper's methodology. The feedback is supported by a direct reference to the paper's Section 2 and the specific claim made by the authors. This shows that the feedback is verifiable and can be independently verified against the paper's content. The feedback also respects the aspect by focusing on a specific detail of the paper, which is the number of clients in the simulation. Therefore, the feedback is both relevant and respectful to the aspect of the paper. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the number of clients in the federated learning simulation. The feedback is tailored to the content of the paper, and it is clear which part of the document is being discussed. The question about the minimum amount of ""m"" is a direct critique of the paper's content, and it shows that the reviewer is paying close attention to the details of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2329,ACL_2017_318_review.json,"Weaknesses: 1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted. 
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper. 
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings? 
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work. 
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs. 
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task. 
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"". 
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2? ","3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",1.0,1.0,-1.0,1.0,nan,-1,"The feedback provided in the review is somewhat vague and lacks specificity. While it does mention the need for more baselines based on related work, it fails to provide clear guidance on how to implement this suggestion. The feedback also does not offer any concrete steps or recommendations that the author can use to improve their paper. Instead, it merely points out a potential issue without offering a clear path for revision. This lack of actionable advice makes the feedback less useful for the author. Therefore, based on the score rubric, the feedback is not sufficiently actionable and does not provide the author with a clear path for revision. So the score is -1. 
[RESULT] -1",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is respectful and it encourages the authors to consider additional baselines for their models. The feedback also suggests that the paper would be stronger with the inclusion of more baselines, which is a positive and helpful suggestion. The feedback does not contain any negative or aggressive language, and it maintains a professional tone throughout. Therefore, based on the score rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback suggests that the proposed models should be compared to models that only consider different senses but not sememes, and it questions the relevance of the MST baseline. However, it fails to provide any evidence or logical reasoning to support these claims. The feedback does not cite any external sources or reference any parts of the paper that would allow for independent verification of the claims made. Instead, it makes general statements without substantiating them. This lack of verifiable evidence and logical reasoning is why the feedback does not meet the criteria for a score of 1. 

Therefore, the feedback does not respect the aspect of verifiability, as it does not provide any evidence or logical reasoning to support its claims. This is why the feedback is not relevant to the review and does not respect the aspect of verifiability. 

So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response is somewhat specific, but it lacks the depth and clarity that would make it fully relevant to the content of the draft. The reviewer mentions the MST baseline and the need for more baselines, but it does not clearly explain why these baselines are important or how they relate to the paper's focus on sememes. The feedback also fails to provide a clear connection between the proposed models and the baselines mentioned. This lack of specificity and direct relevance to the content of the draft makes it difficult to identify which parts of the document are being addressed in the feedback. Therefore, the feedback does not fully meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
5163,NIPS_2020_1524,"Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer. 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense? 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion. 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this. 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues: - ""philosophical note"" paragraph seems a bit digressive. - Line 109 failed to use superscript? - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.","3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues:",-1.0,1.0,-1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the simplification of the Hessian and the introduction of lambda, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback merely states the problems without providing any guidance on how to address them. This lack of actionable advice makes the feedback less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the complexity of the simplification and its impact on the optimizer, which demonstrates an understanding of the subject matter. The feedback also identifies the introduction of lambda as a significant deviation from the Bayesian filtering model, which is a relevant point in the context of the review. The minor issues mentioned are specific and do not detract from the overall quality of the feedback. The tone of the feedback is positive and encouraging, which is in line with the score rubric's criteria for a score of 1. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which justifies a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer makes assertions about the simplification of the Hessian and the introduction of lambda, but these claims are not substantiated with any references to the paper or external sources. The feedback also lacks logical reasoning that could be independently verified. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any evidence or logical reasoning to support the claims made. 

The feedback also does not meet the criteria for a score of 0, as it does not make general statements or observations that are not relevant to the review. Instead, it makes specific claims about the paper's content and the reviewer's expectations. 

In conclusion, the feedback provided in the response does not meet the criteria for a score of 1 or 0, as it lacks evidence and logical reasoning to support the claims made. Therefore, the feedback is not relevant to the review and does not respect the aspect described in the instruction. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the simplification of the Hessian by the squared gradient and the replacement of eta/2sigma^2 with lambda. These are clear and specific points that are directly related to the content of the paper. The feedback also mentions the ""desired"" RMS style optimizer, which is a specific aspect of the paper. The minor issues mentioned are also specific, indicating that the feedback is tailored to the content of the paper and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1904,ARR_2022_306_review,"1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. 
1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set. ","1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI?",0.0,1.0,0.0,0.0,nan,0,"The feedback provided in the response does not offer any actionable suggestions or recommendations. It merely questions the necessity of including examples in the BI, without providing any concrete steps or guidance on how to improve the paper. The feedback does not help the author in addressing the issues raised, nor does it provide a clear path for revision. Therefore, the feedback lacks the necessary actionability and does not meet the criteria set out in the score rubric. 

The feedback does not enhance the usefulness of the review, as it does not offer any practical advice or recommendations that the author can use to improve their paper. It fails to provide the author with a clear path for revision, which is essential for a constructive review. 

In conclusion, the feedback provided in the response is not actionable and does not meet the criteria set out in the score rubric. It lacks the necessary direction and guidance that would help the author in addressing the issues raised in the review. This approach is not conducive to a respectful and helpful review process. 
[RESULT] 0",-1,"The feedback provided in the response is not respectful or constructive. It questions the relevance of the examples in the BI, which could be perceived as dismissive of the author's approach. The tone of the feedback is critical and lacks the necessary politeness and encouragement that is expected in a scientific review. The feedback does not provide any constructive suggestions or acknowledge the potential merits of the author's method. Instead, it focuses on a perceived flaw without offering any alternative solutions or acknowledging the potential benefits of the author's approach. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not follow the aspect of providing a polite and helpful review. So the score is -1. 
[RESULT] -1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiability. The reviewer's claim that the BI should not contain examples because they are already included in the instances is not supported by any evidence or logical reasoning. There is no reference to external sources or parts of the paper that could validate this claim. The feedback is based on a general observation without any substantiation, which is not in line with the rubric's requirement for verifiable feedback. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the proposed method, which is the inclusion of examples in the BI. The feedback questions the necessity of these examples and suggests an alternative approach, which is to have instructions in the BI instead. This shows a clear understanding of the content and a direct critique of a specific element in the paper. The feedback is tailored to the content of the paper, and it is easy to identify which part of the document is being discussed in the feedback. Therefore, the feedback meets the criteria of specificity and relevance to the content of the paper. So the score is 1. [RESULT] 1"
1972,ARR_2022_357_review,"- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. 
- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion ","- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.",-1.0,0.0,-1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the methods used are basic and not novel, without offering any specific, clear, or practical suggestions for improvement. The feedback does not provide any direction or hints on what needs to be done to enhance the paper. It also fails to acknowledge the potential value of the annotated resource, which is a significant part of the paper's contribution. Therefore, the feedback does not meet the criteria of being actionable, as it does not guide the author in addressing the issues identified. 
[RESULT] 0",0,"The feedback provided is neutral and factual, focusing on the technical aspects of the paper. It does not contain any negative or disrespectful language, and it maintains a professional tone throughout. The feedback is also constructive, as it points out the limitations of the methods used and highlights the main contribution of the paper. However, it does not explicitly encourage the authors or express any positive sentiment. Therefore, based on the score rubric, the feedback is neither negative nor positive, but rather neutral. It respects the authors' work and provides a clear assessment of the paper's contributions. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response is not supported by any evidence or references. The claim that the methods used are basic and not novel is made without any supporting data or citations. The feedback also fails to provide any logical reasoning that could be independently verified. The feedback does not meet the criteria for verifiable feedback as it does not provide any evidence or references to support its claims. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. So the score is -1. [RESULT] -1",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the methods used in the paper, namely BERT, SVM, and the 1-baseline, and it clearly states that these methods are basic and not novel. This specificity aligns with the score rubric's requirement for detailed and directly relevant feedback. The feedback also correctly identifies the main contribution of the paper as the annotated resource, which is a direct critique of the paper's content. Therefore, the feedback is tailored to the content of the paper and not a general comment for any scientific document. It is clear that the feedback is specific and directly relevant to the content of the draft, which is why it should be scored as a 1. 
[RESULT] 1"
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.",- A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance?,0.0,1.0,0.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the scientific paper. It questions the frequency of mode selection and the relation re-prediction, which are critical components of the prediction model. However, the feedback lacks specificity and practical suggestions that the author can use to improve their paper. It does not offer concrete steps or recommendations that guide the author in addressing the issues raised. The feedback is more of a general inquiry rather than actionable advice. Therefore, it does not meet the criteria of providing a clear path for revision. So the overall score is -1. [RESULT] -1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback is focused on the aspect of mode selection and its impact on the final performance, which is relevant to the review. The feedback also encourages the authors to consider the frequency of mode selection and its relation to the re-prediction of the variables, which is a positive approach to the review. The feedback is not aggressive or disrespectful, and it maintains a neutral and factual tone throughout. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and it respects the aspect of the paper. The feedback questions the frequency of mode selection and the relation re-prediction, which are critical aspects of the paper's methodology. It also suggests a potential change in the prediction mode, which is a logical claim based on the paper's content. However, the feedback does not provide any external references or internal references from the paper to support its claims. It relies solely on logical reasoning, which is a valid approach but does not meet the highest standards of verifiable feedback. Therefore, the feedback is not fully supported by evidence, which is why it does not receive the highest score. 

So the overall score is 1. [RESULT] 1",1,"The feedback provided is quite specific and directly addresses the content of the draft. It questions the frequency of mode selection and the relation re-prediction, which are key aspects of the paper's methodology. The feedback is tailored to the content of the paper, and it doesn't make general comments about the paper but rather focuses on specific points. This specificity aligns with the score rubric's requirement for detailed and directly relevant feedback. Therefore, the feedback is relevant to the review and respects the aspect in question. So the score is 1. [RESULT] 1"
1232,ICLR_2023_3724,"Weaknesses:
The paper has low readability. A lot of the issues are certainly fixable but in its current form, it is confusing enough to distract from evaluating the technical contributions of the paper. Certain examples are:
“DeVAE surpasses 2% for β-TCVAE and 9% for β-VAE.” In what terms exactly?
Many instances in the introduction talking about spreading the conflict of disentanglement and reconstruction over time and space is not easy to follow and understand.
“However, in this work, we get rid of calculating TC by leveraging the narrow information bottleneck (Tishby et al., 1999; Burgess et al., 2018) to find efficient codes for representing the data, which promotes disentanglement.” This is pretty confusing right where it is in the introduction and only becomes somewhat clear after reading the method section.
The authors claim that DynamicVAE suffers from Information Diffusion problems. If that is the case, wouldn’t that result in low disentanglement scores or at least high variance across different seeds for DynamicVAE? But, that’s not the case in Figure 2.
The experiment for high-dimensional latent space is weak. 1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space?
A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set?
What are the layer embeddings? They seem to be an important component of the method but are never explained.
How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper?
Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.","1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space? A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set? What are the layer embeddings? They seem to be an important component of the method but are never explained. How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper? Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.",-1.0,1.0,-1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely points out the issues with the paper without offering any concrete steps or recommendations that the author can use to improve their paper. The feedback does not provide any direction or hints of what needs to be done. It is more of a critique rather than a constructive review. The feedback does not help the author in addressing the issues raised. It lacks the specificity and clarity that are necessary for actionable feedback. Therefore, the feedback does not meet the criteria of being actionable. 
[RESULT] 0",-1,"The feedback provided is highly critical and lacks the necessary respect and constructiveness. The tone is negative and aggressive, with phrases such as ""seems unrealistic"" and ""why is DeVAE worse for low dimensions?"" which can be perceived as disrespectful. The feedback also fails to provide any constructive criticism or suggestions for improvement, instead focusing on the perceived shortcomings of the paper. The lack of politeness and the absence of any positive or encouraging remarks further contribute to the negative tone of the feedback. Therefore, based on the score rubric, the feedback does not meet the criteria for a polite, helpful, positive, respectful, or encouraging tone. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiability. The feedback makes several claims about the DeVAE method, such as the unrealistic dimensionality of the latent space and the lack of quantitative numbers. However, it fails to provide any evidence or references to support these claims. The feedback also raises questions about the method, such as the selection of hyperparameters and the comparison with other methods, but again, it does not provide any evidence or references to support these questions. The feedback is based on general observations and questions, which do not require any external sources or logical validation. Therefore, the feedback does not meet the criteria for verifiability as outlined in the score rubric. So the overall score is 0. [RESULT] 0",1,"The feedback provided is highly specific and directly addresses the content of the draft. It critically examines the use of a 1024 dimensional latent space for dSprites, questioning its appropriateness for the dataset. The feedback also raises pertinent questions about the design decisions, hyperparameter choices, and the computational overhead of the DeVAE method, which are all directly relevant to the paper's content. Furthermore, the feedback points out the lack of comparison with other relevant methods, such as FactorVAE and Cascade-VAE, which is a significant oversight in the paper. The feedback is tailored to the specifics of the paper, making it clear which sections, statements, or elements in the paper are being addressed. Therefore, the feedback is highly specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2353,NIPS_2021_1788,"Weaknesses: - The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution. - Some parts of the paper need clearer writing (more below)
Comments and questions: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching. - More explanation is needed for Eq (5). For example, what is the meaning of the cost c 0 ( s , a )
? (e.g., to quantify out-of-distribution actions) - The use of s ′ and a ′
when defining $\hat{\pi}{\beta} a t l i n e 107 m i g h t c a u s e c o n f u s i o n a s \mathcal{D} c o n t a i n s
(s,a,r,s’)$. - This paper is about deriving a conservative estimate of the quantiles of the return from offline data where the conservativeness is for penalizing out-of-distribution actions. In the paper, they define OOD actions as those are not drawn from \hat{\pi}{\beta}(.|s) (line 109) but in Assumption 3.1. they assume that \hat{\pi}_{\beta}(a|s) > 0, i.e., there is no OOD actions. Thus, what is the merit of the theoretical result presented in the paper?
The authors have adequately addressed the limitations and social impact of their work.","- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution.",-1.0,1.0,1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the approach proposed is simple and straightforward without much technical innovation. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback does not guide the author in addressing the issues identified. It lacks direction or hints of what needs to be done to improve the paper. Therefore, the feedback does not meet the criteria of being actionable. 
[RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it simply states the nature of the proposed approach, which is straightforward and lacks technical innovation. The feedback does not show any disrespect or lack of respect for the authors' work, but it also does not provide any positive reinforcement or encouragement. Therefore, based on the score rubric, the feedback is neutral and does not meet the criteria for a score of 1 or 2. 
[RESULT] 0",0,"The feedback provided does not meet the criteria for a score of 1. It makes a claim about the simplicity and lack of technical innovation in the proposed approach, but it does not provide any evidence or references to support this claim. The feedback does not cite any external sources or reference any parts of the paper, nor does it use logical reasoning to validate the claim. Therefore, the feedback does not meet the requirements for verifiable feedback as outlined in the score rubric. It is also noteworthy that the feedback does not provide any constructive criticism or suggestions for improvement, which is a key aspect of a review. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the approach proposed for learning conservative quantiles of the return distribution. The feedback clearly identifies the method used, CODAC, and its components, CQL and QR-DQN. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback also respects the aspect by providing a critique of the simplicity and lack of technical innovation in the approach. Therefore, the feedback aligns well with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3805,NIPS_2020_1707,"There are some concerns: 1. In line 82, authors should provide more explanations why they assumed linear constrains. How does it compare with non-linear combination in terms of performance and optimization speed. 2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability. 3. How does the cheap upsampling method (bicubic in the paper) influence the result? What is the limitations of upscaling factor, say will it fail if the factor is 8? 4. More comparisons and results from RAISR should be presented. 5. Experiments on image denoising and deblocking is very limited, lacking quantitative comparisons on benchmarks and intuitive explanation of this generalization.",2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability.,0.0,1.0,-1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the pre-defined dictionary and the comparison of hand-crafted filters with learned filters, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback merely states the problems without providing any guidance on how to address them. This lack of actionable advice makes the feedback less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is critical and direct, which can be perceived as negative or disrespectful. The tone of the feedback is not constructive, as it does not offer any guidance or encouragement to the authors. Instead, it points out the limitations of the experiments without providing any suggestions for improvement. The feedback also lacks the politeness and respect that are expected in a scientific review. It does not attempt to encourage the authors or acknowledge their efforts. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not follow the aspect of politeness and respect. So the overall score is -1. 
[RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. It fails to provide any evidence or logical reasoning to support the claims made. The feedback simply states that the experiments on Set5 are limited in data size and generalization ability, but it does not provide any references to external sources or internal references from the paper to substantiate this claim. Furthermore, the feedback does not compare the hand-crafted filters with learned filters, which is a key aspect of the review. The lack of evidence and logical reasoning in the feedback makes it difficult to verify the reviewer's claims, which is a critical aspect of the score rubric. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claims made. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the aspect of proving the pre-defined dictionary is over-complete and comparing hand-crafted filters with learned filters. The feedback also critiques the experiments on Set5, pointing out its limitations in data size and generalization ability. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5651,NIPS_2018_195,"Weaknesses: - What is the time comparison of VCL relative to BatchNorm and having no normalization? - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently. - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed. - Line 177 typo: ""batchsized"" - It would aid a reader if the authors summarized the loss and how it is computed at the end of Section 2.2. - How sensitive is the framework to the choice of n? - How does \beta vary over time? Could the authors include a graph for this in the Appendix? Questions: - Will the authors be open-sourcing the code for the experiments? - Have you experimented with a constant \beta? - Have you experimented with having _both_ BN and VCL?  Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal. ",- Have you experimented with having _both_ BN and VCL? Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal.,0.0,0.0,0.0,0.0,nan,0,"The feedback provided in the response is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. Instead, it merely asks a question about the use of BN and VCL without providing any guidance or direction on how to address the issue. The feedback does not enhance the usefulness of the review for the author, as it does not provide a clear path for revision. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. So the score is 0. [RESULT] 0",1,"The response is polite and respectful, which aligns with the score rubric's requirement for a positive tone. The reviewer acknowledges the authors' efforts in clarifying their questions, which is a constructive approach. However, the response could be improved by providing more specific feedback on the authors' work, rather than just asking a question about their methodology. This would make the feedback more helpful and encouraging, which is a key aspect of the score rubric. Despite this, the response is still respectful and maintains a positive tone throughout. So the overall score is 1. [RESULT] 1",-1,"The feedback provided in the response is not aligned with the score rubric's criteria for verifiability. The reviewer asks a question about the use of both BN and VCL, but does not provide any evidence or references to support this claim. The feedback does not cite any external sources or use logical reasoning to validate the claim. It also does not reference any part of the paper, which is a key aspect of the score rubric. Therefore, the feedback does not meet the criteria for verifiability and is not supported by any evidence. This lack of evidence and support for the claim results in a score of -1. 
[RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the use of both BN and VCL methods. The feedback is tailored to the content of the paper, and it is clear that the reviewer is referring to specific parts of the document. The response also acknowledges the authors' clarification in the rebuttal, which shows that the aspect is respected and followed in the review. Therefore, based on the score rubric, the feedback is specific and directly relevant to the content of the draft, and it is easy to identify which part of the document is being discussed in the feedback. So the score is 1. 
[RESULT] 1"
5726,NIPS_2018_245,"Weakness] 1: Poor writing and annotations are a little hard to follow. 2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.  3: The motivation is to solve when the question doesn't focus on the most obvious visual concept when there are synonyms and homographs. However, from the experiment, it's hard to see whether this specific problem is solved or not. Although the number is better than the previous method, it will be great if the authors could product more experiments to show more about the question/motivation raised in the introduction.  4: Following 3, applying MLP after GCN is very common, and I'm not surprised that the performance will drop without MLP. The authors should show more ablation studies on performance when varying the number of facts retrieval, what happened if we different number of layer of GCN? ","2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.",-1.0,1.0,-1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the technical novelty of the paper is limited without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issues identified, nor does it provide any direction or hints on what needs to be done. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it offers a straightforward critique of the paper's technical novelty. The feedback is also concise, which is a positive aspect. However, it lacks the constructive element that would make it more helpful to the authors. The feedback could have been improved by providing more specific suggestions for improvement or by acknowledging the potential merits of the paper. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that the technical novelty of the paper is limited is not supported by any evidence or references. There is no mention of external sources or logical reasoning that could be used to verify the claim. The feedback is a general statement that does not engage with the specifics of the paper. It lacks the depth and detail that would be expected from a reviewer who is an expert critic. The feedback does not provide any insight into the paper's content or its relevance to the field. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support its claims. 
[RESULT] 0",-1,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. It mentions the application of GCN on FVQA but fails to address specific parts or methods in the paper. The feedback does not provide any concrete examples or detailed analysis of the paper's content, which makes it difficult to identify the parts that are being addressed in the document. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a score of 1 would require the feedback to be specific and directly relevant to the content of the draft. It should make it clear which sections, statements, or elements in the paper are addressed. Specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. 

Based on the score rubric, the feedback provided in the response does not meet the criteria for specificity, and therefore, it is not relevant to the content of the draft. So the overall score is -1. 
[RESULT] -1"
1656,ICLR_2023_1511,"Weakness_ - The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented). - Similarly, it is lacking a ""So What"" on the bounds provided, which are often just left there as final statements, without an analysis that explains whether 1) they are (likely to be) tight and 2) what this implies for practitioners. - Although well-written, the paper felt quite dense, even compared to other pure-math ML papers. More examples such as Figure 2 would help. - As far as I understood, the assumption on the non-linearities discards the sigmoid and the softmax, which are popular non-linearities. It would be good to acknowledge this directly by name.","- The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented).",1.0,-1.0,-1.0,-1.0,nan,-1,"The feedback provided in the review is relevant to the aspect of motivation in the paper. However, it falls short in offering specific, clear, and practical suggestions that the author can use to improve their paper. The feedback merely states that the paper could do better in motivating the ""Why"" without providing any concrete steps or recommendations. It lacks the actionability that is crucial for guiding the author in addressing the issues identified. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. So the overall score is -1. [RESULT] -1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The feedback is delivered in a respectful manner, offering a suggestion for improvement without being negative or aggressive. It encourages the authors to provide a clear motivation for their work, which is a positive and helpful approach. The feedback is also positive, as it acknowledges the potential for improvement, rather than criticizing the paper outright. This approach is likely to be well-received by the authors, as it provides a clear direction for them to follow. Therefore, based on the score rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it addresses the importance of motivation in the paper. However, it fails to provide any evidence or references to support the claim that the paper could do better in motivating the ""Why"". The feedback is based on a general observation and does not cite any external work or use logical reasoning to validate the claim. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. It is a general statement that lacks the necessary support to be considered a valid critique. So the overall score is 0. [RESULT] 0",-1,"The feedback provided is quite vague and does not offer specific details or examples from the paper. It merely suggests that the paper could improve in motivating the ""Why"" without providing any concrete examples or areas where this could be achieved. This lack of specificity makes it difficult to identify which parts of the paper are being addressed in the feedback. The feedback does not demonstrate a clear understanding of the content of the paper, nor does it provide actionable advice for the authors to improve their work. Therefore, based on the score rubric, the feedback does not meet the criteria for specificity and relevance to the content of the paper. So the overall score is -1. [RESULT] -1"
3604,NIPS_2020_356,"1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community. 2. Evaluation conducted using 102 scenes from the simulated dataset. There is no mention of why or how these scenes were selected, whether they are complex enough, or whether they pose a significant challenge. Will their proposed method scale to any environment? Some statistics about the selected scenes are necessary. How big are the selected scenes? How many rooms? How many of each target object in each scene? List the scenes used in the appendix for reproducibility. --POST-REBUTTAL--- I am happy with the author's response on this. 3. Though great to have reported real world transfer, the experiments run seem to be minimal and the evaluation premature. --POST-REBUTTAL--- I am happy with the author's response on this. 4. There is overall lack of a deeper discussion about the importance and implications of this work for the wider research effort in autonomous agents and how the authors expect the proposed method to scale. In addition, the paper is overly focused on results from the datasets and challenge, with little interpretation of these results. 5. No standard deviations are reported in the quantitative results. ---POST-REBUTTAL--- The authors have not addressed this concern. I strongly believe that results without any notion of deviation or error (without a good reason for their absence) are not reliable. 6. I am not sure if I missed it, but I could not find an explanation of the “Random” baseline. Is this random at the level of the low level action (local policy) or random at the level of the long-term goal? Also, the distance to success values reported in Table 1 are not that much larger than that achieved by the proposed model. This is of course difficult to judge because of the lack of more scene information or results interpretation. --POST-REBUTTAL--- I am happy with the author's response on this. I understand now that this is indeed a challenging task - looking at the very low success rate of baselines.","1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community.",-1.0,-1.0,-1.0,0.0,nan,0,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely expresses concern about the novelty of the work and questions its significance as a scientific contribution. While it does acknowledge the value of the specific approach for the embodied indoors navigation community, it does not provide any guidance on how the author can enhance the paper's relevance or impact. The feedback lacks specificity and fails to offer any practical steps or recommendations that would help the author address the issues raised. Therefore, based on the score rubric, the feedback is not actionable and does not meet the criteria for providing clear and practical suggestions for improvement. 
[RESULT] 0",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a positive tone. The reviewer expresses concern about the novelty of the work, which is a valid point of critique. However, the reviewer also acknowledges the value of the specific approach for the indoors navigation community, demonstrating a constructive and respectful approach to the feedback. The reviewer's tone remains neutral and factual throughout the feedback, which is in line with the score rubric's criteria for a score of 1. The reviewer's willingness to reconsider their initial concerns after reviewing the authors' response further exemplifies a respectful and helpful tone. Therefore, based on the score rubric, the feedback is relevant to the review and is delivered in a polite and respectful manner. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The reviewer's concern about the novelty of the work is not supported by any evidence or references. The claim that the algorithm is not a significant scientific contribution is made without any logical reasoning or references to external sources or parts of the paper. The reviewer's statement that the algorithm is well engineered for the CVPR 2020 Habitat Object Goal Navigation Challenge is also not supported by any evidence or references. The feedback does not provide any verifiable claims or suggestions, which is a key aspect of the score rubric. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",0,"The feedback provided in the response is somewhat specific, as it addresses the novelty of the work and its potential impact on the wider community. However, it lacks the detailed and direct relevance to the content of the draft that is expected from a score of 1. The feedback does not mention specific parts or methods in the paper, and it is not clear which sections, statements, or elements in the paper are being addressed. The reviewer's discussion and the authors' response are mentioned, but the feedback does not provide a clear connection between these elements and the specific aspects of the paper. Therefore, the feedback does not fully meet the criteria for specificity as outlined in the score rubric. 

So the overall score is 0. [RESULT] 0"
2313,ACL_2017_657_review.json,"Weaknesses: one of the main weaknesses of the paper lies in the fact that the goals are not clear enough. One overall, ambitious goal put forward by the authors is to use approaches from experimental psychology to interpret LSTMs. 
However, no clear methodology to do so is presented in the paper. On the other hand, if the goal is to validate sociological assumptions, then one should do so by studying the relationships between gender markers and the written justifications, independently on any model. The claim that ""expected gender differences (are) a function of theories of gendered self-construal"" is not proven in the study.
- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up.
As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed. ","- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up. As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed.",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is quite critical and points out several issues with the study. However, it falls short in providing actionable suggestions for the author to improve their paper. While the feedback identifies problems with the correlation between LSTM network probabilities and SVM coefficients, it does not offer any concrete steps or recommendations on how to address these issues. Similarly, the feedback mentions the lack of clarity in the authors' goals and the need for more details on the methodology, but it does not provide any specific guidance on how to achieve these improvements. The feedback also fails to suggest any practical steps or edits that the author could make to enhance the relevance and applicability of the study. Therefore, the feedback is not actionable and does not meet the criteria set out in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is critical and somewhat negative, which is not entirely respectful or constructive. The tone of the feedback is more of a critique rather than a supportive review. The feedback points are relevant to the review, but the manner in which they are presented lacks the necessary politeness and encouragement that would be expected from an expert critic. The feedback does not seem to be trying to encourage the authors, but rather to point out the shortcomings of the paper. The feedback also fails to provide any positive reinforcement or constructive suggestions for improvement. Therefore, based on the score rubric, the feedback does not meet the criteria for a score of 1, as it does not demonstrate a polite, helpful, positive, respectful, or encouraging tone. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for verifiable feedback as outlined in the score rubric. The reviewer makes several claims without providing any evidence to support these claims. For instance, the reviewer states that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". However, the reviewer does not provide any external sources or logical reasoning to back up this claim. Similarly, the reviewer criticizes the authors for hesitating between two goals and for not developing the multilevel bayesian models used to quantify the gender-based self-construal assumptions. Again, the reviewer does not provide any evidence or references to support these criticisms. The feedback is therefore not verifiable and does not meet the standards set by the score rubric. 

[RESULT] 0",0,"The feedback provided in the response is quite general and lacks specificity. It does not directly address any particular part of the paper, nor does it provide any concrete examples or criticisms. Instead, it offers a broad critique of the study's arguments and methodology without pinpointing specific issues or areas for improvement. This lack of specificity makes it difficult to identify which parts of the paper are being discussed in the feedback. The feedback also fails to mention any specific methods or concepts used in the paper, which is crucial for a review to be relevant and useful. Therefore, based on the score rubric, the feedback does not meet the criteria for specificity and relevance to the content of the paper. 
[RESULT] 0"
1569,ICLR_2023_303,"Weaknesses: 1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it? 2. Regarding the proposed hierarchical patch embedding, can authors give more explanation about the specifical design as 2 consecutive MLP with ratio 3, how authors reach this design. As this is a critical module proposed, can authors provide some ablation studies about it, e.g. why uses a different MLP ratio of 3 as latter stages uses 4 instead? Will allocate computations differently to 56x56, 28x28 influence the final performance? What computation percentage should be used in patch embedding v.s. latter stage (14x14) for better performance?","1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it?",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is not actionable. It identifies several issues with the paper, such as the lack of explanation for certain terms in the table caption, the missing stage definition, and the incomplete sentences. However, it does not offer any specific, clear, or practical suggestions for improvement. Instead, it simply points out the problems without providing any guidance on how to address them. This lack of actionable feedback makes it difficult for the author to know what steps to take in order to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer has identified specific areas where the authors could improve their paper, such as the lack of explanation for certain terms in the table caption and the missing stage definition. The reviewer also suggests additional information that could be included, like the parameters and flops in Table 2, and requests more details about the merging operation. These suggestions are presented in a manner that is both respectful and encouraging, aiming to help the authors improve their work. The feedback is also factual and does not contain any negative or aggressive language. Therefore, based on the score rubric, the feedback is relevant and respectful, and it is delivered in a positive and constructive manner. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it addresses specific aspects of the paper that need clarification or additional information. However, the feedback does not provide any evidence or references to support the claims made. For example, the feedback suggests that the authors should add more details to Table 2 and provide a clearer definition of the stages, but it does not cite any external sources or provide logical reasoning to back up these suggestions. The feedback also points out incomplete sentences and the lack of detail in the merging operation, but again, it does not offer any evidence or references to support these observations. Therefore, while the feedback is relevant, it does not meet the criteria for verifiable feedback as it lacks the necessary evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses several parts of the descriptions, such as the lack of explanation for RPE, Low Att, Mid Att in Table 2, the missing stage definition, the need for additional information in Table 2, incomplete sentences, and the lack of detail in the merging operation. Each of these points is clearly linked to specific sections of the paper, making it easy to identify which part of the document is being discussed in the feedback. The feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria for a score of 1 according to the score rubric. 
[RESULT] 1"
5633,NIPS_2018_197,"weakness of the paper: its clarity. From the presentation, it seems evident that the author is an expert in the field of computer algebra/algebraic geometry. It is my assumption that most members of the NIPS community will not have a strong background on this subject, me including. As a consequence, I found it very hard to follow Sect. 3. My impression was that the closer the manuscript comes to the core of algebraic geometry results, the less background was provided. In particular, I would have loved to see at least a proof idea or some more details/background on Thm. 3.1 and Cor. 3.2. Or maybe, the author could include one less example in the main text but show the entire derivation how to get from one concrete instance of A to right kernel B by manual computation? Also, for me the description in Sect. 2.4 was insufficient. As a constructive instruction, maybe drop one of the examples (R(del_t) / R[sigma_x]), but give some more background on the other? This problem of insufficient clarity cannot be explained by different backgrounds alone. In Sect. 3.2, the sentence ""They are implemented in various computer algebra systems, 174 e.g., Singular [8] and Macaulay2 [16] are two well-known open source systems."" appears twice (and also needs grammar checking). If the author could find a minimal non-trivial example (to me, this would be an example not including the previously considered linear differential operator examples) for which the author can show the entire computation in Sect. 3.2 or maybe show pseudo-code for some algorithms involving the Groebner basis, this would probably go a long way in the community. That being said, the paper's strengths are (to the best of this reviewer's knowledge) its originality and potential significance. The insight that Groebner bases can be used as a rich language to encode algebraic constraints and highlighting the connection to this vast background theory opens an entirely new approach in modelling capacities for Gaussian processes. I can easily imagine this work being the foundation for many physical/empirical-hybrid models in many engineering applications. I fully agree and applaud the rationale in lines 43-54! Crucially, the significance of this work will depend on whether this view will be adopted fast enough by the rest of the community which in turn depends on the clarity of the presentation. In conclusion: if I understood the paper correctly, I think the theory presented therein is highly original and significant, but in my opinion, the clarity should be improved significantly before acceptance, if this work should reach its full potential. However, if other reviewers have a different opinion on the level of necessary background material, I would even consider this work for oral presentation. Minor suggestions for improvements: - In line 75, the author writes that the ""mean function is used as regression model"" and this is how the author uses GPs throughout. However, in practice the (posterior) covariance is also considered as ""measure of uncertainty"". It would be insightful, if the author could find a way to visualize this for one or two of the examples the author considers, e.g., by drawing from the posterior process. - I am not familiar with the literature: all the considerations in this paper should also be applicable to kernel (ridge) regression, no? Maybe this could also be presented in the 'language of kernel interpolation/smoothing' as well? - I am uncertain about the author's reasoning on line 103. Does the author want to express that the mean is a sample from the GP? But the mean is not a sample from the GP with probability 1. Generally, there seems to be some inconsistency with the (algebraic) GP object and samples from said object. - The comment on line 158 ""This did not lead to practical problems, yet."" is very ominous. Would we even expect any problem? If not, I would argue you can drop it entirely. - I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.","- I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.",1.0,1.0,0.0,1.0,nan,-1,"The feedback provided in the review is not actionable. It lacks specific, clear, and practical suggestions that the author can use to improve their paper. The reviewer's questions and assumptions about the paper's content do not offer concrete steps or recommendations that guide the author in addressing the issues. Instead, the feedback is filled with uncertainties and assumptions, which do not contribute to the actionability of the feedback. The reviewer's suggestions, such as adding computer-algebra code or presenting the work in the context of kernel ridge regression, are not clearly articulated and do not provide a clear path for revision. The feedback does not respect the aspect of providing actionable comments, as it does not offer any specific guidance or recommendations. Therefore, the feedback is not useful for the author in improving their paper. So the score is -1. [RESULT] -1",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a positive tone. The reviewer expresses uncertainty and seeks clarification, which is a constructive approach to feedback. The reviewer also offers alternative suggestions, such as using computer-algebra code or presenting the work in the context of kernel ridge regression, demonstrating a willingness to help the authors improve their paper. Furthermore, the reviewer acknowledges the author's rationale for using differential equations and provides a constructive critique by suggesting a different approach. The feedback is also respectful, as it does not dismiss the author's work but instead offers alternative perspectives. Overall, the feedback is helpful, positive, and respectful, which is why it would receive a score of 1 according to the score rubric. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The reviewer's claims are not supported by any evidence or references to the paper. The reviewer's questions and assumptions about the paper's content are not substantiated with any logical reasoning or references to the paper's content. The reviewer's suggestions for improvements are not based on any analysis of the paper's content or any external sources. The feedback does not demonstrate an understanding of the paper's content or the ability to provide a verifiable critique. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the draft. The reviewer's confusion about the relationship between u(t) and x(t) in the context of the paper is not addressed with specificity. The reviewer's assumptions about the content of the paper are not substantiated with direct references to the paper. The suggestions for improvement, such as adding computer-algebra code or presenting the work in the context of kernel ridge regression, are not directly related to the specific aspects of the paper that the reviewer is questioning. The feedback does not clearly identify which parts of the document are being discussed, and it lacks the specificity required to provide a tailored critique. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
3433,NIPS_2020_1241,"1. The analysis on neural networks are direct results induced from the kernel method results. It may not be tight, and can even suffer from the curse of dimensionality. Hence, the different between two types of models showed in the theorems may not fully characterize the actual performance difference. Neural networks can possibly perform much better than the bound. 2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?","2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?",1.0,1.0,-1.0,1.0,nan,0,"The feedback provided in the review is somewhat actionable, as it suggests the use of a smooth activation function instead of ReLU. However, it falls short in providing a clear direction or specific steps for the author to follow. The feedback does not explain why a smooth activation function would be more appropriate or how it could be implemented. It also does not address the potential reasons for choosing ReLU over other activation functions. This lack of detail and guidance makes the feedback less useful for the author in terms of actionability. Therefore, the feedback does not fully meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is respectful and it encourages the authors to consider alternative activation functions. The feedback also maintains a positive and helpful tone by suggesting that smooth activation functions could be more illustrative. It does not come across as negative, rude, disrespectful, or aggressive, which is why it meets the criteria for a score of 1. The feedback is also relevant to the review, as it addresses the aspect of the activation function used in the numerical experiments. 

So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback suggests that the use of ReLU in the numerical experiments is not ideal and proposes the use of a smooth activation function. However, it fails to provide any evidence or logical reasoning to support this claim. The feedback does not reference any part of the paper or external sources, which is crucial for verifiable feedback. It also does not explain why a smooth activation function would be more illustrative or beneficial, leaving the claim unsupported. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support its claims. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the use of ReLU in the numerical experiments and questions the choice of activation function. The feedback is tailored to the content of the paper, as it specifically mentions the activation function and suggests alternatives like tanh or sigmoid. This shows that the reviewer has carefully considered the content of the paper and provided feedback that is directly related to the specific aspects of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
796,ICLR_2021_2043,"Weaknesses: - The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.
- Clarify the difference between S-Full and S-PGD from Experiments section. Since S-Full also uses T-step PGD, how it is different than S-PGD? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric, the performance gains are very minimal and not very significant than the baseline OE (Hendrycks et al., 2019b) under two metrics AUROC and AUPR.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on adversarial robustness is significant, there are critical points yet to be addressed. Therefore, I marginally accept this paper.
Post rebuttal: The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper.","- The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.",1.0,1.0,1.0,1.0,nan,1,"The feedback provided in the review is actionable and directly addresses the aspect of safe-spot aware adversarial training. It suggests that the authors should evaluate the proposed defense against safe-spot aware adversarial attacks and provide a rationale if such attacks are deemed infeasible. This feedback not only identifies the issue but also offers a clear direction for the authors to follow in order to improve their paper. It is specific, clear, and practical, guiding the authors on what needs to be done to address the concerns raised in the review. Therefore, the feedback is highly relevant to the review and is followed and respected in the review. So the score is 1. [RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the authors' efforts in proposing a safe-spot aware adversarial training and their evaluation of the proposed technique. However, the reviewer also points out a gap in the authors' evaluation, suggesting that the possibility of safe spot aware adversarial attack on the proposed defense should be addressed. This feedback is not only relevant but also encourages the authors to provide a rationale for their evaluation, which is a positive and respectful approach. The feedback is delivered in a manner that is both polite and helpful, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is well-aligned with the score rubric's requirements for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of the proposed technique. It critically evaluates the authors' approach to safe-spot aware adversarial training and the evaluation of the proposed defense against adversarial attacks. However, the feedback lacks the necessary evidence or references to support its claims. It does not cite external sources or provide logical reasoning that could be independently verified. The feedback is more of a general observation rather than a well-supported critique. Therefore, it does not meet the criteria for a score of 1, as it does not provide the necessary evidence or logical reasoning to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the evaluation of the proposed technique and the possibility of safe spot aware adversarial attack on the proposed defense. The feedback also asks for the rationale behind the claim that such an attack is infeasible. This shows that the reviewer has carefully considered the content of the paper and has provided feedback that is tailored to the specific aspects of the paper. The feedback is not a general comment for any scientific document, but rather a detailed critique of the paper's content. Therefore, the feedback is in line with the score rubric's requirement for specificity and relevance to the content of the paper. So the score is 1. [RESULT] 1"
3455,NIPS_2020_725,"- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks. - One particular concern is that in this setup, as shown Figure 4a, a strong Hebbian learning rule leads to a much faster convergence than gradient descent, presumably because Hebbian learning quickly memorizes the training set. It might very well be that Hebbian learning leads to more progressive differentiation in tasks where it results in equal or slower convergence than gradient descent.",- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks.,-1.0,-1.0,1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the paper's applicability to more complex tasks and non-linear networks. However, it falls short in offering actionable suggestions for improvement. The feedback merely states the limitation of the paper without providing any guidance on how to address this limitation. It does not offer any concrete steps or recommendations that the author can use to improve their paper. The feedback is more of a commentary rather than a constructive critique that could help the author in revising their work. Therefore, the feedback lacks the necessary actionability that is required for a higher score. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback acknowledges the simplicity of the task and the linearity of the network used, which is a valid point of critique. It also raises a relevant concern about the generalizability of the findings to more complex tasks and non-linear networks. This shows that the feedback is relevant to the review and is being followed or respected. The feedback is delivered in a neutral and factual manner, which is appropriate for a scientific review. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of the paper's limitations. It points out that the paper's approach to a hierarchical task may not be applicable to more complex tasks or non-linear networks. However, the feedback does not provide any evidence or references to support this claim. It is a logical claim based on the paper's description, but it lacks the verifiable aspect that would make it stronger. The feedback could have been improved by including references to external sources or by providing a more detailed analysis of the paper's limitations. Therefore, while the feedback is relevant and respects the aspect of the review, it does not fully meet the criteria for a score of 1 due to the lack of verifiable evidence. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a limitation of the paper, which is the simplicity of the hierarchical task and the linear network used. The feedback also hints at the potential for the lessons learned to generalize to more complex tasks and non-linear networks, which is a critical aspect of the paper's relevance and applicability. The specificity of the feedback is evident in the way it directly addresses the content of the paper, making it clear which sections or elements are being discussed. Therefore, the feedback is tailored to the content of the paper, and not a general comment for any scientific document. This aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3000,NIPS_2022_1200,"Weakness: Originality:
1.I want to know if this paper is the first time to study the problem of the robust collaborative inference, where there are both arbitrary agents and adversarial agents. The arbitrary agents are easy to identify. However, I’m afraid the proposed method achieves a similar performance to identify the adversarial agents compared with baselines.
From Eq.(5), the framework aims to find a combined feature l
which is on the manifold and is near h
. The manifold projection could get a similar results for the adversarial sub-features. Could the authors discuss more about it? Writtings:
1.After so many times of reading, I guess I understand this paper. The authors introduce their method in Section 2.3, which is very simple. However, it relies block-sparse structure which is detailed stated in Section4. This could cause confuse when understanding the proposed method.
2.The notations are confusing. For example, h and l
both denote the feature. Why not use a letter (or with its variants)?
the citation format may be ICLR rather NeurIPS.
Theoretical analysis:
1.This paper provides an extensive theoretical analysis. In fact, I suggest the authors discuss more what the analysis means. Compared with baselines, why CoPur could do better.
2.Could the authors give an intuitive explanation about the effect of the sparsity α
on CoPur? Experiments:
1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c
and different Ω a d v ?
2.More analysis is helpful, for example, The comparison on optimization efficiency.
The authors discuss the limitations in Appendix. I have no other suggestions.","1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c and different Ω a d v ?",0.0,1.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It raises a question about different Ω c and different Ω a d v, but it does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback does not guide the author in addressing the issues raised. It lacks the necessary direction or hints of what needs to be done. Therefore, the feedback does not meet the criteria of actionability as outlined in the score rubric. 
[RESULT] 0",0,"The response provided is a direct question that is relevant to the review. It shows an understanding of the topic and the ability to engage with the content of the paper. However, the tone of the feedback is neutral and lacks the constructive and encouraging elements that would be expected in a score of 1. The feedback does not offer any additional insights or suggestions that could help the authors improve their work. It simply poses a question without providing any guidance or support. Therefore, while the aspect is relevant, the feedback does not fully respect or follow the aspect as it lacks the positive and respectful tone that would be expected in a higher score. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. It introduces a hypothetical scenario without providing any evidence or logical reasoning to support the claim. The feedback does not reference any part of the paper or external sources, which is crucial for verifiable feedback. It also lacks any logical reasoning that could be independently verified. Therefore, the feedback does not respect the aspect of verifiability, as it fails to provide any evidence or logical reasoning to support its claims. This is why the feedback is not relevant to the review and does not meet the criteria for a score of 1. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the comparison of CoPur with the manifold projection. The feedback also introduces a hypothetical scenario involving different Ω c and different Ω a d v, which shows a deeper understanding of the topic and a more nuanced critique. This specificity in the feedback ensures that the review is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1170,ICLR_2023_3572,"Weakness: 1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body. 2. Figure 3 is the most important picture in the article ,but it is confusing. Although the style of the figure is nice, it does not help me understand PRG. There is no need to place a black box on (1,1).","1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body.",-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is relevant to the paper, as it identifies a specific issue with the symbol \xi in Section 3.1. However, the feedback lacks actionable suggestions that the author can use to improve their paper. It merely points out the problem without offering any concrete steps or recommendations to address the issue. The feedback does not provide the author with a clear path for revision, which is essential for enhancing the usefulness of a review. Therefore, the feedback is not fully aligned with the score rubric's criteria for actionability. 
[RESULT] 0",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The reviewer identifies a specific issue with the paper, pointing out the lack of clarity in the explanation of the symbol \xi and its inconsistent use in the paper. This feedback is respectful and aims to help the authors improve their work. The tone is positive and encouraging, which is in line with the rubric's criteria for a score of 1. The feedback is also factual and does not contain any negative or aggressive language. Therefore, based on the score rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper, which is the clarity of the symbol \xi in Section 3.1. However, the feedback does not provide any evidence or references to support the claim that the symbol is not clearly explained. The feedback also fails to mention any external sources or logical reasoning that could be used to verify the reviewer's claim. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. The feedback is also not supported by internal references from the paper, which would have been a more appropriate way to substantiate the claim. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It identifies a particular section, Section 3.1, and points out a specific issue with the symbol \xi. The feedback also notes that the symbol appears in the main body but disappears in the pseudo-code, which is a clear indication of the aspect being respected or followed in the review. The feedback is tailored to the content of the paper, and it is not a general comment for any scientific document. Therefore, the feedback meets the criteria of the score rubric, and it is easy to identify which part of the document is being discussed in the feedback. So the score is 1. [RESULT] 1"
5211,NIPS_2020_1428,"- Novelty: This work can appear as incrementally innovative since Sobolev descent and unbalanced optimal transport were already present in the literature. This work is essentially the merging of these two existing works, resulting however in a new contribution. - Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.",- Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.,-1.0,-1.0,-1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the experiments are ""toyish"" and do not outperform existing results, without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issues raised, nor does it provide any concrete steps or recommendations that could help the author improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is critical and somewhat dismissive of the research presented. The use of the term ""toyish"" to describe the experiments and the assertion that they do not outperform existing results can be perceived as disrespectful and discouraging. This tone is not conducive to a constructive review process, as it does not provide any constructive feedback or encouragement to the authors. The feedback does not respect the efforts of the authors and fails to acknowledge any potential merits of the research. Therefore, based on the score rubric, the feedback is not respectful or helpful, and it does not encourage the authors. So the overall score is -1. 
[RESULT] -1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that the experiments are ""toyish"" and do not outperform existing results is made without any supporting evidence. There are no references to external sources, nor are there any logical arguments presented to substantiate the claim. The feedback does not provide any verifiable information, which is a key aspect of the score rubric. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. The feedback is not supported by any evidence, and it does not follow the aspect of verifiable feedback. So the overall score is -1. 
[RESULT] -1",0,"The feedback provided in the response is not specific enough to be relevant to the review. It uses vague terms such as ""toyish"" and ""existing results"" without specifying which parts of the paper are being referred to. This lack of specificity makes it difficult to identify which sections, statements, or elements in the paper are being addressed. The feedback does not provide a clear connection to the content of the draft, which is crucial for a review to be effective. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
3081,NIPS_2017_330,"Weaknesses
- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.
- Issues of convergence, especially when applying gradient descent over a non-Euclidean space, is not addressed
In all, a rather thorough paper that derives an efficient way to compute gradients for optimization on LDSs modeled using extended subspaces and kernel-based similarity. At one hand, this leads to improvements over some competing methods. Yet, at its core, the paper avoids handling of the harder topics including convergence and any analysis of the proposed optimization scheme. None the less, the derivation of the gradient computations is interesting by itself. Hence, my recommendation. ",- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.,1.0,1.0,-1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of writing style and readability. However, it falls short in offering specific, clear, and practical suggestions that the author can use to improve their paper. The feedback merely states that the section is tersely written and could have benefitted from a slower development, but it does not provide any concrete steps or recommendations that guide the author in addressing these issues. The feedback lacks actionability, which is crucial for the author to understand how to improve their paper. Therefore, the feedback does not meet the criteria of providing actionable comments. So the score is -1. 
[RESULT] -1",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is respectful and it offers a helpful suggestion for improvement. The feedback does not come across as negative or disrespectful, and it maintains a positive and encouraging tone throughout. The feedback also directly addresses the aspect in question, which is the writing style of Section 4, and provides a specific recommendation for improvement. This shows that the feedback is relevant to the review and that the aspect is respected and followed. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that Section 4 is ""very tersely written"" is not supported by any evidence or references. There is no mention of how this claim was reached or any logical reasoning provided. The feedback also lacks any reference to the paper itself or any external sources that could be used to verify the claim. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any evidence or logical reasoning to support the claim made. 

In contrast, a score of 1 would require the feedback to include references to external sources, references to parts of the paper, or logical reasoning that can be independently verified. The feedback should be able to provide a clear and verifiable basis for the claims made. 

In conclusion, the feedback provided in the response does not meet the criteria for a score of 1, as it lacks any evidence or logical reasoning to support the claim made. The feedback is not relevant to the review and does not respect the aspect of the paper being reviewed. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the writing style of Section 4. The feedback is tailored to the content of the paper, as it provides a critique of the writing style and suggests an improvement. The feedback is also easy to identify, as it clearly states which section is being discussed. Therefore, the feedback meets the criteria of specificity and relevance to the content of the paper. So the score is 1. [RESULT] 1"
85,ICLR_2022_3099,"Weaknesses
W1: The setting seems to be limited and not well justified. 1) It only consider ONE truck and ONE drone. Would it be easy to extend to multiple trucks and drones? This seems to be a more interesting and practical setting. 2) What is the difference of this setting versus settings where there are multiple trucks? Are there methods solving this setting, and why are they not working in TSP-D? 3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.
W2: It is not clear why exactly an LSTM-decoder is better than an attention-based decoder. The paper justifies that ""AM loses its strong competency in routing multiple vehicles in coordination"". However, AM decoder still conditions ""on the current location of the vehicle and the current state of nodes"". Thus, I don't think it overlooks the interaction between different vehicles. It depends more on how you design the decoder. Compared to attention, an LSTM essentially adds to the historical decisions to the policy, not the interactions between vehicles. Therefore, it is not clear why exactly LSTM-decoder is better, and the justification is quite vague in the paper.
W3: Except for AM, NM by Nazari et al. (2018) has also been an important counterpart of the proposed HM. However, it is not compared as a baseline. Whereas I understand that not every baseline should be compared, but NM is mentioned a few times throughout. If historical information is important in decoding an action, why is it not important in encoding a state? Because of this, the empirical evaluation is not totally convincing to me.","3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.",0.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely points out a contradiction in the assumptions made in the paper without providing any guidance on how to resolve this contradiction or improve the paper. The feedback does not suggest any specific steps or edits that should be made, nor does it provide any direction or hints on what needs to be done. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 

In conclusion, the feedback is not actionable and does not contribute to the usefulness of the review for the author. It fails to provide a clear path for revision, which is essential for an effective review. 

So the score is 0. [RESULT] 0",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a positive tone. The reviewer acknowledges the contradiction in the assumptions and expresses a willingness to understand the reasoning behind them. This approach is constructive and encourages the authors to clarify their points. The feedback also maintains a neutral stance by not making any assumptions about the authors' intentions or the relevance of the topic to a larger audience. The tone is neither negative nor aggressive, and it is clear that the reviewer is trying to engage in a respectful dialogue with the authors. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the two assumptions are contradicting. The reviewer's question about the contradiction is not backed by any logical reasoning or references to the paper. The feedback also lacks any citation of external sources, which would have strengthened the claim. The feedback is more of a general observation rather than a well-supported critique. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or references to support the claim. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It identifies two assumptions made in the paper and questions their contradiction, which shows a clear understanding of the paper's content. The feedback also acknowledges the limited setting of the paper, which is a relevant point in the context of the paper's potential audience. This specificity and relevance to the paper's content align with the score rubric's criteria for a score of 1. Therefore, the feedback is well-tailored to the content of the paper, and it is easy to identify which parts of the document are being discussed in the feedback. 
[RESULT] 1"
587,ICLR_2022_1057,"Weakness:
Experiments: 1. Why experiments do not contained the same as RIS? Just for completeness to show that you do better than RIS on their experiments. 2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?",2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?,1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the response is not actionable. It merely questions the author's choice of algorithm without offering any specific guidance or suggestions on how to improve the paper. The feedback does not provide any direction or hints on what needs to be done, which is crucial for the author to make any meaningful revisions. The lack of actionable content in the feedback makes it less useful for the author. Therefore, based on the score rubric, the feedback does not meet the criteria for actionability. 
[RESULT] 0",-1,"The tone of the feedback is somewhat confrontational, which could be perceived as disrespectful. The question posed is direct and somewhat accusatory, which may not be well-received by the authors. The feedback does not provide any constructive criticism or suggestions for improvement, which is a key aspect of a review. Instead, it focuses on a specific aspect that the authors did not consider, without offering any insight into why this might be relevant or how it could be incorporated into their work. This lack of constructive feedback and the somewhat aggressive tone of the question make the feedback less effective in guiding the authors towards improving their work. Therefore, the feedback does not meet the criteria for a score of 1, as it does not demonstrate politeness, helpfulness, or respect. 
[RESULT] -1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the Skew-fit algorithm should be considered for comparison. It merely suggests that there is some similarity between the two without providing any logical reasoning or references to the paper. The feedback does not follow the aspect of verifiable feedback, as it does not allow for independent verification of the claims made. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. So the score is -1. [RESULT] -1",0,"The feedback provided in the response is not specific to the content of the draft. It does not mention any specific parts or methods in the paper, making it difficult to identify which sections or elements in the paper are being addressed. The feedback is a general comment that could apply to any scientific document, not just the one being reviewed. It lacks the specificity required to be a relevant and effective critique. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
1723,ICLR_2023_2202,"Weakness: 1. The results are not stunning. The multi-task training does not lead to consistently better performance on all graph-structure prediction tasks. The performance improvement on downstream tasks is not effective. 2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.","2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.",1.0,1.0,-1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of experimental design and results. However, it falls short in offering specific, clear, and practical suggestions that the author can use to improve their paper. The feedback mentions that the experimental settings are not rigorously designed and that the experimental results could be further discussed or explained. However, it does not provide concrete steps or recommendations that guide the author in addressing these issues. The feedback lacks actionability, which is crucial for the author to understand and implement the necessary changes. Therefore, the feedback does not meet the criteria of providing actionable comments that enhance the usefulness of the review. So the score is -1. [RESULT] -1",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback addresses the aspect of experimental design and the need for further discussion of experimental results. It does so in a manner that is not aggressive or disrespectful, but rather in a way that encourages the authors to improve their work. The feedback also maintains a positive tone by suggesting that the authors could further discuss or explain their results. This approach is likely to be well-received by the authors, as it provides them with a clear direction for improvement. Therefore, based on the score rubric, the feedback is relevant to the review and is delivered in a respectful and constructive manner. So the overall score is 1. [RESULT] 1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the experimental settings are not rigorously designed. It also fails to provide any logical reasoning that could be independently verified. The feedback simply states that the experimental results could be further discussed or explained, without any specific examples or references to the paper. This lack of evidence and logical reasoning makes the feedback less credible and does not meet the standards set by the score rubric. Therefore, the feedback does not respect the aspect of verifiability, and it is not relevant to the review. So the score is -1. 
[RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the experimental design, namely the lack of rigor in the experimental settings. The feedback also provides a concrete example, MTT testing on unseen tasks, which illustrates the point being made. Furthermore, the feedback suggests that the experimental results could be further discussed or explained, indicating a specific area where the paper could be improved. This level of detail and relevance to the content of the paper aligns with the score rubric's criteria for a score of 1. Therefore, the feedback is tailored to the content of the paper and not a general comment for any scientific document. 
[RESULT] 1"
2250,ACL_2017_104_review.json,"Weaknesses: - Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion: ",- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.,-1.0,1.0,-1.0,0.0,nan,0,"The feedback provided in the response is not actionable. It merely states a difficulty in determining the impact of a specific method without offering any concrete steps or recommendations for improvement. The feedback does not guide the author in addressing the issues raised, nor does it provide a clear path for revision. It lacks the specificity and clarity that would make it actionable. Therefore, based on the score rubric, the feedback does not meet the criteria for actionability. 
[RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it presents a critique of the method used in the paper, suggesting that a comparison to other methods would be necessary to assess its impact. This critique is delivered in a respectful manner, without any disrespect or aggression. The feedback is also constructive, as it points out a potential area for improvement in the paper. However, it does not go beyond this critique to offer any positive reinforcement or encouragement to the authors. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a score of 1 or 2. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. It fails to provide any evidence or logical reasoning to support the claim that the impact of sense disambiguation order cannot be determined without comparison to other unsupervised entity linking methods. The feedback does not reference any external sources or parts of the paper, which is a key aspect of the score rubric. It also does not provide any logical reasoning that could be independently verified. Therefore, the feedback does not meet the requirements for a score of 1, as it does not respect the aspect of verifiable feedback. 

In contrast, a score of 1 would require the feedback to provide evidence or logical reasoning to support the claim, which is not present in the response. The feedback in the response is more aligned with a score of 0, as it does not reference any work and is not relevant to the review. 

So the overall score is 0. [RESULT] 0",0,"The feedback provided is not specific enough to be relevant to the content of the draft. It fails to mention any specific parts or methods in the paper, making it difficult to identify which aspects of the document are being addressed. The feedback is too general and does not provide any concrete or actionable insights that would help the author improve their work. It lacks the specificity required to be a useful critique of the paper. Therefore, based on the score rubric, the feedback does not meet the criteria for specificity and relevance to the content of the draft. 
[RESULT] 0"
2803,NIPS_2022_1523,"Weakness:
1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data.
[1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.
2 Unclear model design: The model architecture and learning details are fragmented or missing. The authors could either provide a plot of model illustration, pseudo-code table, or code repository. Considering that Neurochaos Learning is not a well-known method, it is important to demonstrate integrated details to facilitate reproductivity.
3 Experimental design: The experiments regarding Coupled autoregressive (AR) processes and Coupled 1D chaotic maps etc. don't seem to be well-motivated. Could the authors reason why particularly using such setting to investigate cause-effect of time series. Lastly, the comparison to a five-layer neural network seems to be less convincing, given the rapid developments of deep learning architectures.
Yes, the authors fairly discussed the limitations of the method. The potential negative impact may not be applicable to this study.","1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data. [1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the discussion of causality but fails to offer specific, clear, and practical suggestions for improvement. The feedback points out the lack of explicit definition of causality and the confusion around the term 'classification of cause-effect', but it does not provide any concrete steps or recommendations for the author to address these issues. The feedback also lacks in guiding the author on how to improve their paper, which is crucial for a review. The feedback is more of a critique rather than a constructive review that provides actionable advice. Therefore, the feedback does not meet the criteria of being actionable, as it does not offer any direction or hints of what needs to be done. So the score is 0. [RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer has identified a key aspect of the manuscript, which is the discussion of causality, and has provided specific examples from the text to support their critique. The reviewer has also suggested improvements, such as including the exact definition of causality and clarifying the meaning of 'classification of cause-effect'. This approach is helpful and encourages the authors to refine their work. The feedback is also respectful, as it acknowledges the complexity of the topic and the potential unfamiliarity of the reviewers with the specific definitions used. The reviewer's tone is positive and constructive, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is well-aligned with the score rubric's requirements for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is not aligned with the score rubric. The feedback does not provide any evidence or references to support the claims made. The reviewer criticizes the lack of explicit definition of causality and the use of the term 'classification of cause-effect', but fails to provide any external sources or internal references to back up these criticisms. The feedback also lacks logical reasoning that could be independently verified. The reviewer's claims are not supported by any evidence, making the feedback unverifiable and not in line with the score rubric. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or references to support the claims made. 
[RESULT] 0",1,"The feedback provided is quite specific and directly addresses the content of the draft. It identifies a particular aspect of the manuscript, which is the discussion of causality, and provides a detailed critique of how this aspect is handled. The feedback points out the lack of explicit definition of causality and questions the use of the term 'classification of cause-effect'. It also references specific literature to support its critique, which adds to the specificity of the feedback. The feedback is tailored to the content of the paper and not a general comment for any scientific document. It is clear that the feedback is relevant to the review and that the aspect is followed or respected in the review. Therefore, the feedback meets the criteria for a score of 1. 
[RESULT] 1"
5224,NIPS_2020_1363,"- Even if MLE was better than DuetRL in objective evaluations, it is not a reason to leave DuetRL out of subjective evaluations. - The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references. - The quality and reusability of Chinese folk music transcriptions are not discussed. I think that the dataset is fine however, the usage of this dataset has to be justified by the authors, at least part of the supplementary. See the next three comments below: 1) Folk music is transmitted orally; scores are transcriptions into a single melody. These transcriptions do not typically include heterophony and other stylistic features like glissandi, ornaments, dynamics contrasts, etc. that characterize the folk song genre or style. The authors should discuss the limitations of the lack of performance elements in the transcriptions. 2) Another important aspect is the collection process and purpose of the transcriptions and if they are suitable for music generation? Given that the data is retrieved from Essen folk collection, is it possible that European scholars who prepared transcriptions with a Western classical music perspective and without adequate understanding of the characteristics of Chinese folk music? Are we sure that the transcription process retained the melodic characteristics? Such transcriptions are sometimes useful for descriptive reasons from a ""Western"" point of view, and they are not intended for prescriptive work, e.g., to be used in performance practice. (https://www.amherst.edu/system/files/media/1770/Seeger%252520-%252520Prescriptive%252520and%252520Descriptive%252520Music-Writing.pdf). There is no discussion if these transcriptions are re-usable for rendering music. To the authors' defense, I explored the dataset and Essen archives for more evidence (https://wiki.ccarh.org/wiki/EsAC) and consulted an ethnomusicologist specializing in Chinese music to validate the dataset. The response was that the transcriptions sound OK albeit that lack the performance elements as described above. Nevertheless, - since it is going to be the first paper on the topic - the authors need to include these discussions as part of the study (paper or the supplementary). 3) There are many different Chinese folk music traditions, which significantly differ in characteristics due to region, ethnicity, and era. This is not acknowledged by the authors, except in Section 4.1: ""It comprises 2250 traditional Chinese folk songs, from Han, Natmin, Shanxi, and Xinhua origins."" However, the origins are confusing: Han is an ethnicity; Shanxi is a province; Xinhua is a small county or the official state-run press agency, and the transcriptions listed under Xinhua are from all over China and modern-day Mongolia; I am not sure what Natmin is. This is a sloppy explanation from the authors' side.","- The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references.",1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the musical motivation and background but fails to offer specific, clear, and practical suggestions for improvement. The feedback does not provide any direction or hints on what needs to be done to address the issues. Instead, it poses questions and makes assumptions about the nature of the compositions and their relevance to Western classical music. This does not help the author in understanding how to improve their paper. The feedback does not guide the author in addressing the issues, which is a key aspect of actionability. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer has identified specific areas where the paper could be improved, such as the lack of musicological sources and the need for more detailed information about the composers and their works. The feedback is also encouraging, suggesting that the readers should be able to appreciate these details and that there might be a demand for more ""counterpointed"" folk music. This approach is likely to be well-received by the authors, as it provides clear guidance on how to enhance their paper. The feedback is also factual and does not contain any negative or aggressive language, which is in line with the score rubric's criteria for a score of 1. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which makes it a high-quality review. 
[RESULT] 1",0,"The feedback provided in the response is not aligned with the score rubric. The feedback does not provide any evidence or references to support the claims made. It questions the origin and style of the music, but it does not provide any logical reasoning or references to external sources that could be independently verified. The feedback also fails to cite any parts of the paper, which is a key aspect of the score rubric. The feedback is more of a speculation rather than a critique based on the provided paper. Therefore, the feedback does not meet the criteria for a score of 1, as it does not support its claims with any evidence or references. 

In conclusion, the feedback does not provide any evidence or references to support its claims, which is a key aspect of the score rubric. It also fails to cite any parts of the paper, which is another important aspect of the score rubric. The feedback is more of a speculation rather than a critique based on the provided paper. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",1,"The feedback provided is highly specific and directly addresses the content of the draft. It identifies specific sections of the paper, such as lines 15-17 and lines 18-20, and provides a detailed critique of the musical motivation and background. The feedback also questions the nature of the compositions and their relevance to the development of Western classical style in China, which shows a deep understanding of the subject matter. Furthermore, it suggests potential applications of the music, such as music education and film/game music generation, demonstrating a comprehensive approach to the review. The feedback is tailored to the content of the paper, and it is clear that the reviewer has taken the time to engage with the specific details of the draft. Therefore, based on the score rubric, the feedback is highly specific and directly relevant to the content of the draft, making it a score of 1. 
[RESULT] 1"
2656,NIPS_2019_346,"weakness of the paper is a lack of theoretical results on the proposed methodology. Most of the benefits of the new model have been demonstrated by simulations. It would be very helpful if the authors could provide some theoretical insights on the relation between the model parameters and the tail dependence measures, and on the performance (consistency, efficiency etc) of the parameter estimators. Itemized comments: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired. 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2. 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general. 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful. 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.","1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired.",1.0,1.0,0.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the new quantile function (3) compared to the existing function (2). However, the feedback does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely points out the lack of supporting data for the change in the function, without providing any guidance on how to address this issue. The feedback does not enhance the usefulness of the review by providing a clear path for revision. Therefore, the feedback is not actionable, and it does not respect the aspect of the review. So the score is -1. 
[RESULT] -1",0,"The feedback provided in the response is critical and points out a significant issue with the new quantile function. It is clear that the feedback is relevant to the review, as it addresses the aspect of the function's sensitivity to tail parameters. However, the tone of the feedback is somewhat negative, as it questions the justification for the change in the function. The feedback could have been more constructive by suggesting ways to address the issue or by acknowledging the potential benefits of the new function. Despite this, the feedback does not cross into being disrespectful or aggressive, and it maintains a level of professionalism. Therefore, the feedback is not entirely negative, but it could be improved by being more respectful and encouraging. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not support its claims with any evidence or references. It merely states that the new quantile function (3) seems unjustified and changes the multiplicative factors into an additive term, without providing any supporting data or logical reasoning. The feedback also fails to cite any external sources or reference any parts of the paper that could substantiate its claims. Therefore, the feedback does not meet the requirements for verifiable feedback as outlined in the score rubric. It is not supported by references to external sources, references to parts of the paper, or logical reasoning that can be independently verified. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the change from a multiplicative to an additive term in the quantile function. The feedback also critiques the lack of supporting data for this change, which is a clear and direct comment on the paper's content. The feedback is tailored to the specifics of the paper, making it easy to identify which part of the document is being discussed. Therefore, the feedback meets the criteria of specificity and relevance to the content of the paper. 
[RESULT] 1"
3138,NIPS_2017_15,"WEAKNESS AND CONCONERN
   	1) As already mentioned in previous section, the description lacks certain levels of details for complete reproduction, for example, how is the physics engine implemented, it's understandable that the authors left out some details with proper reference, however it is not very clear, in paricular as physics engine is an important component in the system, how the engine is set up and the set up affect the results.
   	2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.
   	3) The authors should also cite recent work on frame prediction. this is very related.","2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.",1.0,1.0,1.0,0.0,nan,-1,"The feedback provided in the review is not actionable. It raises questions about the evaluation protocols and the comparison of settings with previous results, but it does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback is more focused on the reviewer's concerns rather than providing guidance for the author to address the issues. It lacks the direction or hints of what needs to be done, which is crucial for actionable feedback. Therefore, the feedback does not meet the criteria of being actionable, and it does not enhance the usefulness of the review for the author. So the score is -1. [RESULT] -1",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer's concerns about the evaluation protocols and the comparison to previous results are presented in a manner that is both respectful and encouraging. The reviewer's questions are framed in a way that invites the authors to provide more information, which is a positive approach to feedback. The feedback also acknowledges the potential benefits of the authors' approach, which is a constructive way to provide criticism. The tone of the feedback is positive and respectful, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of the evaluation protocols for the billiard cases. However, it does not provide any evidence or references to support the claims made. The feedback questions the comparison of results to previous results and the comparability of settings, but it does not cite any external sources or provide logical reasoning to validate these concerns. The feedback also suggests that the authors can argue that full construction may be easier for rendering more articulated prediction results, but again, it lacks the necessary evidence or references to support this claim. Therefore, the feedback does not meet the criteria for verifiable feedback as it does not provide any evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the evaluation protocols and the comparison of the results to previous results, which are crucial aspects of the paper. The feedback also points out the importance of the settings being comparable to those in previous results, which is a direct critique of the paper's methodology. Furthermore, the feedback provides a clear argument about the potential benefits of the full reconstruction approach, which is a direct comment on the design decision made in the paper. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's requirement for specificity and relevance to the content of the draft. So the score is 1. [RESULT] 1"
3611,NIPS_2020_878,"In short, the paper misses some critical details and some explanations are not clear. After reading the paper and the supplementary materials, I still believe it is hard to reproduce their results. - The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem. - The meaning of the shaded regions of Figure 2 (right), Figure 5 (left), and Figure 6 (left) is never explained in the caption or main text. Morevoer, it neighor represents standard deviation or the range between the best and worst case because the line with the same color can go outside the shaded region. For example, the solid red line in Figure 5 below the red shaded region between 60 and 90 trained models. - In the equation after line 156: ""\sigma ( A H^l W H^l)"" should be ""\sigma( A H^l W )"". Is this a typo? Otherwise, this is not a graph convolution layer. I have low tolerance with a typo in the ""only one"" equation in a paper. - It is not clear how the adjacency matrix A is defined in line 155. A_ij is the edge of node i to node j or node j to node i? Although I can figure it out from Figure S1 in supplementary materials, it is not clear in the main paper. - Following the previously point, based on Figure S1, the graph convolution operation propogates the features ""backward"" in the graph (in the opposite direction of an edge). I wonder if the authors can explain why you made this decision instead of adding edges for each node to the ""global"" node and following the edge direction to do graph convolution? - The original GCN (Kipf and Welling) uses normalized adajancy matrix with renormalization trick. However, this paper uses the original adjacency matrix without explantation or ablation study. I wonder if there is a reason behind not using the conventional graph convoltuional operation.","- The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem.",-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is relevant to the aspect of the binary relation predictor model. It identifies a problem with the model's outputs not being symmetric, which is a critical issue in the context of the paper. However, the feedback lacks actionable suggestions or recommendations that the author can use to improve their paper. It merely points out the problem without offering any concrete steps or guidance on how to address it. This makes the feedback less useful for the author, as it does not provide a clear path for revision. Therefore, the feedback is not fully aligned with the score rubric's criteria for actionability. 
[RESULT] 0",0,"The feedback provided in the response is factual and does not contain any negative or disrespectful language. It is clear that the reviewer is pointing out a potential issue with the model, which is a valid aspect of the review. However, the tone of the feedback is somewhat passive and lacks the encouragement or positive reinforcement that would be expected in a score of 1. The reviewer does not attempt to engage the authors or provide any constructive suggestions for improvement. Therefore, based on the score rubric, the feedback is neutral and factual, but it does not fully meet the criteria for a score of 1 due to its lack of encouragement and constructive engagement. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper, which is the lack of symmetry in the outputs of the binary relation predictor model. The response correctly identifies the problem and provides a logical explanation of why the outputs are not symmetric. However, the feedback does not provide any evidence or references to support the claim made about the model's behavior. It is a logical claim, but it lacks the necessary verification to be considered fully supported. Therefore, the feedback is relevant but not fully supported by evidence or references, which is why it receives a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the lack of symmetry in the outputs of the binary relation predictor model. The feedback is tailored to the content of the paper, as it specifically mentions the model architecture and the process of concatenating two graph embedded vectors. It also questions how the authors handle this problem, which shows a deeper understanding of the paper's content. The feedback is not a general comment for any scientific document, but rather a focused critique on a specific aspect of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
225,ICLR_2022_2575,"Weakness:
1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?
2). 80.0% top-1 accuracy on ImageNet is not hard. Does the design principle can scale well across different (data size, model parameters, FLOPs)?
3). I don't know which of the proposed components contribute the most to the overall performance. What can be concluded from Appendix Figure 7 and Figure 8?","1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?",0.0,1.0,0.0,0.0,nan,0,"The feedback provided in the response is not actionable. It raises questions about the motivation of WGM and the refinement by SRM, but it does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback is more of a critique rather than a constructive review that guides the author in addressing the issues. It lacks the necessary direction or hints of what needs to be done, which is crucial for the author to make improvements. Therefore, the feedback does not meet the criteria of being actionable. 
[RESULT] 0",0,"The tone of the feedback is neutral and factual, which aligns with the score rubric's description for a score of 0. The feedback does not contain any negative, rude, disrespectful, or aggressive language, and it maintains a professional tone throughout. However, the feedback does not explicitly encourage the authors or provide positive reinforcement, which would have been indicative of a score of 1. The feedback is focused on the content of the paper and does not address the tone or manner of the feedback itself. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not encourage the authors or provide positive reinforcement. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the WGM is refined by SRM and encourages more randomized selection. It is a direct question without any supporting argument or evidence. The feedback does not use internal references from the paper, nor does it cite external work. It also lacks logical reasoning that could be independently verified. Therefore, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of verifiable feedback. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the motivation behind the use of WGM and its refinement by SRM. The feedback also questions the randomized selection process, which is a critical aspect of the paper. The response is tailored to the content of the paper, and it is clear that the feedback is focused on specific elements within the document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2468,NIPS_2021_783,"Weaknesses: 1. The introduction of the knowledge coefficient matrix may raise the concern of privacy leaks and the communication cost. 2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting. 3. To my best knowledge, there are some other personalized FL works (Hierarchical Personalized Federated Learning for User Modeling, WWW'21; Exploiting Shared Representations for Personalized Federated, arXiv:2102.07078), which need to be listed as baselines.",2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting.,-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely states a conflict between the use of public data and the general FL setting without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issue, nor does it provide any concrete steps or recommendations. It lacks the necessary detail and direction that would make it useful for the author to revise their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper. However, the tone of the feedback is not as respectful or constructive as it could be. The feedback points out a conflict between the general FL setting and the use of public data in step 2 of the paper. While this is a valid point, the feedback could be more encouraging and less critical to foster a positive and constructive dialogue with the authors. The feedback does not fully respect the authors' efforts, which is an important aspect of the score rubric. Therefore, the feedback is not as effective as it could be in terms of politeness and constructiveness. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper. However, it fails to provide any evidence or logical reasoning to support the claim made. The feedback simply states that the use of public data in step 2 of the paper conflicts with the general FL setting, but it does not elaborate on why this is the case or how it affects the paper's overall quality. This lack of supporting evidence or logical reasoning makes the feedback less convincing and does not meet the criteria for a score of 1. Therefore, the feedback is not fully respected in the review, as it does not provide the necessary evidence or logical reasoning to substantiate the claim. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It clearly identifies the aspect of the paper that is being addressed, which is the use of public data in step 2 of the figure. The feedback also points out the conflict with the general FL setting, which is a specific detail from the paper. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria of the score rubric, as it is specific and directly relevant to the content of the draft. So the score is 1. [RESULT] 1"
5707,NIPS_2018_845,"Weakness - Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks. - One of the main advantages from the simulated random-walks is parallelization (at least for sequence generation) and scalability. Section 3.5 is not enough for scalability argument.  * Detailed comments - Due to [28] and the attention usage in the language models, the attention is generally regarded per instance (or sentence/sequence) while here the attention is globally defined. The proposed method has its own value as the authors describe in the paper, but the terminology can lead to some confusion. - Some references such as [16] need to be polished. - Section 3.5 seems somewhat contradictory to the original purpose of trying the graph embedding because it will cut out the information that SVD cannot preserve. Also, whether Section 3.5 is just an idea or it is actually implemented to be used in experiments is not clear. It would be interesting to see the difference in performance between full-matrix context and SVD-approximate context. - Section 3.6 seems more promising to generalize the proposed idea beyond the simple random-walks. - While the authors sweep C for node2vec, the other hyperparameters are not mentioned where it is hard to believe that the default values of the other hyperparameters work best across the datasets. Mentioning the detail about C without mentioning the others naturally raise those questions, so the authors need to add some description.","- Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks.",-1.0,1.0,1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the paper, which is the comparison with node2vec. However, the feedback does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely points out the difference between the first-order and second-order Markov chains without providing any guidance on how to address this issue. The feedback lacks the specificity and clarity needed to be actionable, which is crucial for the author to understand and rectify the problem. Therefore, the feedback does not meet the criteria of being actionable, and it fails to provide a clear path for revision. So the overall score is -1. 
[RESULT] -1",0,"The feedback provided is factual and does not contain any negative or disrespectful language. It points out a discrepancy in the methodology used by the authors, which is a relevant aspect of the review. However, the tone of the feedback is neutral and does not attempt to encourage or respect the authors' work. It simply states the difference between the authors' method and the method used in node2vec. The feedback does not provide any constructive criticism or suggestions for improvement, which could have been helpful to the authors. Therefore, the feedback is not as helpful or positive as it could be. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it addresses a key difference between the proposed method and the node2vec algorithm. However, the feedback does not provide any evidence or references to support its claims. It simply states that the authors only deal with the first-order Markov chain and that this means D in node2vec is not obtainable by the power-series in the proposed method. The feedback also makes a general claim that all random-walks are regarded the same as simple random walks, but it does not provide any logical reasoning or references to back up this claim. Therefore, the feedback does not meet the criteria for verifiable feedback as it lacks the necessary evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the difference between the first-order and second-order Markov chains used in the paper and node2vec, which is a direct comparison made by the authors. The feedback also points out the misinterpretation of the random-walks in the paper, which is a critical aspect of the methodology. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5813,NIPS_2016_95,"Weaknesses 1. The time complexity of the learning algorithm should be explicitly estimated to proof the scalability properties. 2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained. ","2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained.",1.0,1.0,1.0,1.0,nan,-1,"The feedback provided in the review is relevant to the aspect of time complexity in the TRMF-AR algorithm. However, it lacks specificity and actionable guidance. The feedback merely states that the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same, without offering any direction or hints on what needs to be done to address this issue. It does not provide any concrete steps or recommendations that would guide the author in improving their paper. Therefore, the feedback is not actionable, and it does not enhance the usefulness of the review for the author. So the score is -1. 
[RESULT] -1",1,"The feedback provided in the response is polite and constructive. It identifies a potential issue in the paper and requests an explanation, which is a positive approach to reviewing. The tone is respectful and encourages the authors to provide more information. This aligns with the score rubric's criteria for a score of 1, as the feedback is relevant to the review and is delivered in a manner that is respectful and helpful. The feedback does not contain any negative or aggressive language, and it maintains a neutral and factual tone throughout. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",-1,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper, which is the time complexity of TRMF-AR. However, the feedback does not provide any evidence or logical reasoning to support the claim that the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. This lack of support or verification makes the feedback less reliable and does not meet the criteria for a score of 1. The feedback also does not cite any external sources or provide any internal references from the paper, which would have strengthened the claim. Therefore, the feedback is not fully respected in the review, as it does not provide the necessary evidence or logical reasoning to support the claim. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the time complexity of TRMF-AR for different sets of parameters. The feedback is tailored to the content of the paper, and it is clear which sections of the paper are being discussed. The feedback also respects the aspect of the review, as it asks for an explanation of why the time complexity is the same for the different sets of parameters. This shows that the feedback is not a general comment for any scientific document, but rather a specific critique of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4583,NIPS_2020_1678,"1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested. 2. I am not an expert in the are, but my impression is that the novelty of the work is somewhat limited. In particular the novelty is mainly to refine the framework of Dai et al. [4] on the particular problem, and to introduce the components of noise predictor and importance sampling for scalability.","1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested.",-1.0,-1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely states the nature of the method used in the paper without offering any specific, clear, or practical suggestions for improvement. The feedback does not provide any direction or hints on what needs to be done to enhance the quality of the method. It lacks the necessary detail and guidance that would help the author in addressing the issues raised. Therefore, the feedback does not meet the criteria of being actionable, as it does not offer concrete steps or recommendations that would guide the author in revising their paper. 
[RESULT] 0",0,"The feedback provided is neutral and factual, focusing on the methodology of the paper. It does not contain any negative or disrespectful language, and it maintains a professional tone throughout. The feedback is constructive, pointing out the lack of guarantee in the method and suggesting that the quality of the method can only be judged empirically. However, it does not explicitly encourage the authors or provide positive reinforcement. Therefore, while the feedback is relevant and respectful, it does not fully meet the criteria for a score of 1, as it lacks the positive and encouraging tone that would be expected in a score of 1 feedback. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. It does not offer any evidence or references to support the claim that the method is mainly heuristic and lacks empirical validation. The feedback is a general statement that does not engage with the specifics of the paper or provide any logical reasoning to support the claim. It fails to provide any form of verifiable feedback, which is a key aspect of the score rubric. Therefore, the feedback does not respect the aspect of verifiability, and it does not follow the aspect description. The feedback is not relevant to the review, and it does not respect the aspect of verifiability. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is quite specific and directly addresses the methodology of the paper. It points out the heuristic nature of the method and the lack of guarantee for its performance, which is a critical aspect of the paper. The feedback also suggests that the quality of the method can only be judged empirically, which is a direct critique of the methodology. This specificity in the feedback aligns with the score rubric's requirement for specificity in a review. The feedback is tailored to the content of the paper, and it is clear which sections of the paper are being addressed. Therefore, the feedback is relevant to the review and respects the aspect being discussed. So the overall score is 1. [RESULT] 1"
4000,NIPS_2020_1878,"- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors? - Authors DBSCAN++ show cases where DBSCAN++ is even providing better clustering partition than for DBSCAN. Could the authors perform a similar analysis here with SNG-DBSCAN? - DBSCAN++ is also applied to outlier detection. How is performing SNG-DBSCAN compared to DBSCAN++ in this context? - Which parameters are used with DBSCAN for time and space consumption comparison? - Would it be possible to have empirical results on datasets for which DBSCAN is actually working? It seems that DBSCAN is performing well only for Still. Hence DBSCAN and SNG-DBSCAN are equally performing bad on the remaining sets...","- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors?",1.0,1.0,1.0,1.0,nan,-1,"The feedback provided in the review is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. Instead, it merely points out a perceived weakness in the empirical evaluation without providing any guidance on how to address this issue. The feedback also fails to respect the aspect of the review, as it does not consider the different sampling rates used by the authors and DBSCAN++. This lack of consideration for the different sampling rates is a significant oversight, as it is a critical aspect of the evaluation. The feedback does not provide any direction or hints of what needs to be done, which is essential for actionable feedback. Therefore, the feedback is not useful for the author in terms of providing a clear path for revision. So the overall score is -1. [RESULT] -1",-1,"The feedback provided in the response is not respectful or constructive. The tone is negative and the feedback is aggressive, as seen in the phrase ""The comparison with DBSCAN++ is not fair."" This statement is confrontational and does not encourage the authors to improve their work. Additionally, the request for experiments is not phrased in a way that is helpful or positive. Instead, it comes across as a demand, which could discourage the authors from engaging with the feedback. The feedback does not show any attempt to encourage the authors or to provide a constructive critique. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not follow the aspect of politeness. So the score is -1. 
[RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that the comparison with DBSCAN++ is not fair is not supported by any evidence or logical reasoning. The feedback does not cite any external sources or reference any part of the paper to substantiate the claim. Furthermore, the feedback does not provide any logical reasoning that could be independently verified. Instead, it makes a general statement without any supporting details. This lack of evidence and logical reasoning makes the feedback less credible and does not meet the standards set by the score rubric. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the empirical evaluation of the proposed approach. The feedback is tailored to the content of the paper, as it specifically questions the comparison with DBSCAN++ and requests additional experiments to optimize the sampling rate for both competitors. This shows a clear understanding of the paper's content and a direct engagement with the specific methods and results presented. The feedback is not a general comment for any scientific document, but rather a focused critique of the paper's empirical evaluation. Therefore, the feedback is specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4808,NIPS_2020_1318,"1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space). 2. In a similar vein, I was envisioning a simple extension of SG-MCMC methods where we just do occasional random walks/Brownian motion in the original space (using Gaussian noise and ignoring the energies entirely) using some relatively cheap heuristic to detect if the fancier iterate sequence is stuck in local optima. Something like that would help explore the entire space (triggering some iterations of random walk if heuristic flags a local optima) without changing the underlying energy used in the real Sg-MCMC at each iteration. Wonder how that would compare to a method like the one in the paper. 3. Following from these, it would have been nice if the authors included synthetic experiments with asymmetric modes (one small mode to one side of a 2d space and one taller one on the other side) where the real mean would not be close to the 'center' and thus would not be easily approximable by a flatter version of the energy. Comparisons with something like the approach in 2. above would help understand the main insight of the paper better. 4. Runtime plots in the experiment sections on real data would be nice to have, for example plots of energy/error vs wall clock time / iteration count as provided for the synthetics. 5. Some insight on how to choose \delta_{u} and partition count for real datasets (like the values mentioned on line 173 for synthetic) would be nice to have.","1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space).",-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is quite detailed and offers a clear understanding of the issues with the use of a flattened surrogate energy. It correctly identifies that the iterates are not following the true distribution closely and that this could lead to samples that are not as useful for point estimation. The feedback also provides a nuanced understanding of the implications of using a flattened energy for different types of distributions, such as symmetric versus skewed.

However, the feedback lacks specific, clear, and practical suggestions that the author can use to improve their paper. While it does mention that the original distribution's symmetry would be preserved and that the mean would be close enough, it does not offer concrete steps or recommendations that guide the author in addressing these issues. The feedback is more of a critique than a constructive review, and it does not provide the author with a clear path for revision.

In conclusion, the feedback is relevant to the review and addresses the aspect of using a flattened surrogate energy. However, it does not provide actionable comments that would help the author improve their paper. Therefore, the score is 0. 
[RESULT] 0",1,"The feedback provided is quite detailed and offers a thorough analysis of the paper's approach to using a flattened surrogate energy. It acknowledges the potential benefits of this method, such as avoiding local optima, but also points out the potential drawbacks, particularly in terms of the accuracy of point estimates. The feedback is respectful and constructive, providing a balanced view of the paper's methodology. It also offers a clear explanation of why the flattened energy might not be as accurate for certain types of distributions, which shows a deep understanding of the subject matter. The tone of the feedback is polite and helpful, which aligns with the score rubric's criteria for a score of 1. The feedback is not aggressive or disrespectful, and it maintains a positive and encouraging tone throughout. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response is not supported by any evidence or references. The reviewer makes several claims about the suitability of using a flattened surrogate energy and its impact on the distribution of samples. However, these claims are not substantiated with any references to external sources, parts of the paper, or logical reasoning that could be independently verified. The reviewer's statements are based on assumptions and visualizations, which do not provide a solid basis for the claims made. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. 

The feedback also fails to provide any evidence or references to support the reviewer's claim that the flattened energy might be suitable for point estimation situations where the distribution of modes is symmetric. This claim is not supported by any logical reasoning or references to the paper or external work. 

In conclusion, the feedback provided in the response does not meet the criteria for verifiable feedback as it lacks evidence and references to support its claims. The feedback is based on assumptions and visualizations, which do not provide a solid basis for the claims made. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. 
[RESULT] 0",1,"The feedback provided is quite specific and directly addresses the use of a flattened surrogate energy in the paper. It critically examines the implications of this approach on the accuracy of the samples and point estimates, which is a key aspect of the paper. The feedback also provides a clear explanation of how the flattened energy might affect the convergence of the algorithm and the accuracy of the samples, which is a crucial part of the review process. The feedback is tailored to the content of the paper, and it is easy to identify which part of the document is being discussed in the feedback. Therefore, the feedback is highly relevant to the paper and is specific in its critique. So the overall score is 1. [RESULT] 1"
1482,ICLR_2023_3023,"Weaknesses: 
 1. The proposed technique does not seem to predict uncertainty. To the best
  of my understanding, it predicts where the original model is making
  errors. An easy way to see this would be to consider a point where the
  model is confident but inaccurate, i.e., a bias. The predicted mask
  should cover this point, even though it is not an uncertain prediction
  of the model. The problem is actually in the definition of
  ``uncertainty''. Equation 1 (or 3) do not necessarily only correspond to
  uncertainty. It corresponds to expected error for a sample. It also
  includes the contribution of bias. In this regard, I think the
  uncertainty positioning may not be very accurate. Consequently, the
  comparison with a method that computes confidence intervals may not be
  appropriate. Here, I should note that predicting expected error or the error is not a
  bad target. However, the difference between this and uncertainty
  estimation should probably be made clear. 
 2. I am not sure about the contribution of the corollary 1. The result is
  not very surprising in my opinion. The mask that aims to minimize the
  ``uncertainty'' is bound to be related to the ``uncertainty''. 
 3. It is unclear how authors estimate the expectation in Equations 4
  or 5. This is over y x variable. However, in Equation 6, they drop the
  expectation and simply take only one sample to compute this
  expectation. Using this I am assuming the model learns to predict the
  error and not the expected error. As a result, the model may not be able
  to identify an uncertain prediction at a pixel for a given sample since
  for that pixel, there may be only 1 output that happens to be close to
  the ground truth. 
 4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0,
  m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} =
  \infty. It is unclear how authors deal with this. Furthermore, the
  intuition of this specific calibration form is unclear. ","4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0, m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} = \infty. It is unclear how authors deal with this. Furthermore, the intuition of this specific calibration form is unclear.",-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies a problem with the equation but fails to offer any specific, clear, or practical suggestions for improvement. The feedback merely states that the behavior of the equation is suspicious and that the authors' approach to the calibration form is unclear, without providing any guidance on how to address these issues. This lack of actionable advice makes the feedback less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on the mathematical behavior of the model and the lack of clarity in the authors' approach. The feedback is delivered in a straightforward manner, without any attempt to be overly polite or to provide constructive criticism. It simply states the issues with the model's behavior and the lack of clarity in the calibration form. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a score of 1 or -1. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer simply states that the behavior of Equation 8 is suspicious and that the calibration form is unclear, without offering any logical reasoning or citing external sources to back up these claims. This lack of evidence and logical reasoning makes the feedback less reliable and does not meet the standards of a score of 1. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. So the score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the behavior of Equation 8. The feedback points out the suspicious behavior of the equation when m_{\theta} = 0 and m_{\lambda} = \lambda, and when m_{\theta} = 1, m_{\lambda} = \infty. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback also questions the authors' approach to dealing with this behavior and the intuition behind the calibration form, which shows a deeper understanding of the paper's content. Therefore, the feedback is relevant and specific, and it respects the aspect of the paper that it is discussing. So the score is 1. 
[RESULT] 1"
4527,NIPS_2020_84,"- I think the definition of ""near convex"" is quite restrictive towards the necessities of the sensitivity framework. However, it is presented as a very general and natural class of functions. The limitations are not clearly discussed in my opinion. - related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.",- related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.,-1.0,1.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the SVM result depends on regularization and the structure of the data, without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issues mentioned. It lacks the necessary direction or hints that would help the author in revising their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on the technical aspects of the review, specifically the SVM result and the structure of the data. The feedback is also concise, which is a positive aspect. However, the feedback could have been more constructive by providing specific suggestions or pointing out the strengths of the paper. This would have made the feedback more helpful and respectful, which would have earned a higher score. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim made about the SVM result depending on regularization and the structure of the data. The feedback also fails to provide any logical reasoning that could be independently verified. Instead, the feedback makes a claim without any supporting evidence, which is not in line with the score rubric's requirement for verifiable feedback. Therefore, the feedback does not respect the aspect of verifiability, and as such, it does not meet the criteria for a score of 1. 
[RESULT] 0",-1,"The feedback provided in the response is not specific enough to be relevant to the review. It mentions ""related work"" and ""SVM result,"" but it fails to provide any specific details or examples from the paper that would allow the reader to understand which aspects of the paper are being discussed. The feedback is vague and does not directly address any particular section, statement, or element in the paper. It lacks the specificity required to be a useful critique of the paper. Therefore, based on the score rubric, the feedback does not meet the criteria for specificity and relevance to the content of the paper. So the score is -1. 
[RESULT] -1"
2141,ARR_2022_210_review,"1) Improvements are overstated. The abstract mentions "" two-fold improvements"" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?
2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.
There are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as ""woman"" with prob 2/100, ""king"" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.
3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.
4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?
5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. 
There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.
The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.
Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.
Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.
- Line 195: plane -> hyperplane - Figure 1 caption: middle -> midpoint - Line 1453: ""the tanh removes the magnitude of facets"" -> Could you explain this in more detail? I could not make sense of why tanh ""removes the magnitude"" - what do you mean specifically by ""magnitude"" here?
- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).
- There is a mistake in rows ""MFS - Multi partition"" and ""MFS - Multi Input"" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.
Rephrasing: Line 063: ""output the woman or king."" - > ""output woman or king"" Line 121: ""is sometimes not be able"" -> ""is sometimes not able"" Line 1271: ""the GPT-2"" -> ""GPT-2"" Line 235: ""multi-mode"" -> ""multimodal"" also line 263. 
Line 911: ""As in section 5"" -> ""As in Section 5"" ","5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger. The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful. Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format. Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is not actionable. It identifies several issues with the paper, such as the lack of examples, the absence of an explanation for the softmax issue, and the omission of the bias term in the softmax. However, it fails to offer any specific, clear, or practical suggestions that the author can use to improve their paper. Instead, it merely points out the problems without providing any guidance on how to address them. The feedback also includes a reference to a historical theorem, which, while interesting, does not contribute to the actionability of the feedback. The lack of actionable comments in the feedback makes it less useful for the author seeking to improve their paper. Therefore, based on the score rubric, the feedback is not actionable and does not provide the author with a clear path for revision. 
[RESULT] 0",-1,"The feedback provided in the response is quite critical and lacks the necessary politeness and respect that is expected in a scientific review. The tone is negative and the feedback is delivered in a way that could be perceived as aggressive. The reviewer's comments on the lack of explanation and the omission of the bias term in the softmax are not constructive and could potentially discourage the authors. The feedback also includes a personal opinion on the format of the tables, which is not relevant to the scientific content of the paper. Furthermore, the reviewer's reference to historical discoveries and the citation of a website, while informative, is not appropriately integrated into the feedback. The feedback does not encourage the authors or provide any positive reinforcement. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not follow the aspect of politeness and respect in the review. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response is not aligned with the score rubric. The feedback does not provide any evidence or references to support the claims made. The reviewer's comments on the probability distribution, the bias term, and the format of the tables are not substantiated with any external sources or logical reasoning. The feedback also fails to cite any internal references from the paper, which is a key aspect of the score rubric. The lack of evidence and references in the feedback makes it difficult to verify the reviewer's claims, which is a critical component of the score rubric. Therefore, the feedback does not meet the criteria for a score of 1, as it does not support its claims with evidence or references. 
[RESULT] 0",1,"The feedback provided in the response is highly specific and directly addresses several key points in the paper. It identifies specific sections, such as Section 5, and provides detailed comments on the lack of examples, the absence of an explanation for the softmax issue, and the omission of the bias term in the softmax. The feedback also points out the unusual formatting of the captions in the tables and the missing reference to theorem 1. These specific points demonstrate a clear understanding of the content of the paper and a thorough analysis of the feedback. The feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is highly relevant and specific, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K.",-1.0,1.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies an issue with the paper's claim about the theorem's insights into implementation, but it does not offer any concrete steps or recommendations for the author to address this issue. The feedback also mentions the absence of an ablation study on the hyper-parameter K, but it does not suggest any specific actions or edits that the author should make to include such a study. The feedback is more of a commentary rather than a constructive critique that guides the author in improving their paper. Therefore, the feedback does not meet the criteria of being actionable, as it lacks specific, clear, and practical suggestions that the author can use to improve their paper. So the score is 0. [RESULT] 0",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a score of 1. The tone is constructive, and the feedback is aimed at encouraging the authors to provide more information or clarify their points. The feedback also respects the aspect of the review, which is the relevance of the theorem to the empirical observation. The feedback is not aggressive or disrespectful, and it maintains a positive and helpful tone throughout. The feedback also acknowledges the absence of an ablation study on the hyper-parameter K, which is a relevant point in the context of the review. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected in the review. The reviewer is asking for verification of the claims made in the paper, specifically regarding the theorem and its implications on the implementation. The reviewer also points out a lack of an ablation study on the hyper-parameter K, which is a critical aspect of the paper. This feedback is verifiable as it refers to specific parts of the paper and the reviewer's observations, which can be independently verified. The feedback is also logical, as it questions the consistency of the paper's claims with empirical observations. Therefore, the feedback is well-aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the claim made in theorem 2 regarding the gradient estimation and its relationship with the size of the dataset N. The feedback also points out a lack of an ablation study on the hyper-parameter K, which is a specific method mentioned in the paper. This shows that the feedback is tailored to the content of the paper and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2707,NIPS_2022_776,"Weakness:
1.The technique contribution is limited. This paper mainly uses two existing techniques, DP-SGD and gradient matching.
2.This work only uses one algorithm from data condensation, i.e., gradient matching. It would be better if the authors can try more algorithms so the community can have a better understanding about data condensation for differentially private data generation. For example, distribution matching [1] that minimizes the distance between the averaged feature of real data and the averaged feature of synthetic data, which is also easy to implement with DP.
3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?
4.(Minor) Line 121, minimized -> minimize.
[1]: DATASET CONDENSATION WITH DISTRIBUTION MATCHING, https://arxiv.org/pdf/2110.04181v1.pdf.",3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?,1.0,1.0,0.0,0.0,nan,1,"The feedback provided in the review is actionable and directly addresses the aspect of the visual quality of the algorithm. It compares the visual quality of the algorithm with the visual quality of data directly generated from the generator, which is a clear and practical suggestion for the author to consider. The feedback also hints at the potential for improvement, which is an important aspect of actionable feedback. It guides the author in addressing the issue by suggesting a comparison between the two visual qualities. This comparison is a concrete step that the author can use to improve their paper. Therefore, the feedback is relevant to the review and respects the aspect in question. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a score of 1. The tone is constructive and encourages the authors to compare the visual quality of their algorithm with the visual quality of data directly generated from the generator. This comparison is a relevant aspect of the review, as it directly relates to the improvement of the algorithm. The feedback is also helpful, as it prompts the authors to consider the potential benefits of using the generator from a previous work. The feedback is delivered in a positive manner, which is likely to be well-received by the authors. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which justifies a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected. It directly addresses the aspect of the visual quality of the algorithm and compares it with the visual quality of data generated from a previous work. The feedback is supported by a reference to Section 6 of the paper, which shows an understanding of the content and a logical progression of the review. The feedback also invites further comparison, which is a constructive approach to reviewing. Therefore, the feedback is well-aligned with the score rubric, as it imposes a claim and supports it with an internal reference from the paper. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the comparison of the visual quality of the algorithm with the visual quality of data directly generated from a previous work's generator. The feedback is tailored to the content of the paper, and it is clear which sections, statements, or elements in the paper are being addressed. The feedback is not a general comment for any scientific document, but rather a focused critique on a specific point in the paper. Therefore, the feedback meets the criteria of specificity and relevance to the content of the paper. So the score is 1. [RESULT] 1"
1885,ARR_2022_24_review,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. 
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ","- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy.",1.0,1.0,0.0,0.0,nan,0,"The feedback provided in the response is not actionable. It suggests checking if embeddings of L2 have interpretable topological relations, but it does not offer any concrete steps or recommendations on how to do this. The feedback lacks specificity and does not guide the author in addressing the issue. It merely hints at a potential direction without providing a clear path for revision. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback is focused on the aspect of the review, which is the relevance of the LM pre-trained with conditioned token sampling. It suggests a specific direction for the authors to explore, which is checking if embeddings of L2 have interpretable topological relations. This is a positive and encouraging approach, which is in line with the score rubric's criteria for a score of 1. The feedback is also factual and does not contain any negative or aggressive language, which further supports the score of 1. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which makes it a high-quality review. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected in the review. It suggests a specific method (conditioned token sampling) and a particular aspect to be checked (interpretable topological relations). The feedback also provides a clear direction for the reviewer to follow, which is to check if embeddings of L2 have interpretable topological relations. This is a logical claim that can be independently verified, as it refers to a specific part of the paper (L2 embeddings) and suggests a method to verify the claim (checking for interpretable topological relations). The feedback is well-structured and directly addresses the aspect described in the instruction. Therefore, the feedback is well-aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the use of LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""). The feedback also suggests a method to check if embeddings of L2 have interpretable topological relations, such as analogy. This shows that the feedback is tailored to the content of the paper and not a general comment for any scientific document. The feedback is specific and directly relevant to the content of the draft, which is why it should be given a score of 1. 
[RESULT] 1"
5162,NIPS_2020_1750,"- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence. - Choice of methodology: 3.1: The proposed experimental methodology is poorly justified. The pruning methodology (3.1) does not seem to directly correspond to previous approaches which have found found that pruning increases generalization. The experiments are ran on two unconventional networks on CIFAR-10, using unconventional pruning schedules. These networks reach much lower accuracy than expected for CIFAR-10 (85%-87% test accuracy), possibly due to the fact that these networks are relatively overparameterized for CIFAR-10 (a CIFAR-10 ResNet-20 has 0.27M parameters and typically reaches about 91% test accuracy; the ResNet-18 in the paper has 11.5M parameters, and the VGG-11 has about 110M) and L1/L2 regularization is disabled. It is therefore hard to extrapolate these results beyond the two CIFAR-10 networks and their bespoke hyperparameters chosen in the paper. - 3.3: the choice of duration of holding parameters at zero, and the conclusions drawn from it, don't seem justified from the data: the drop in test accuracy from ""Zeroing 50"" does not seem to be the same as the drop in accuracy from ""Prune_L"", and it is unsurprising to see that any regularization technique improves test accuracy for these heavily overparameterized CIFAR-10 networks, so the claim that ""pruning-based generalization improvements in overparameterized DNNs do not require the model’s parameter count to be reduced."" (lines 232-233) is hard to extract from just this experiment on these networks. Overall: the work would be significantly strengthened by having much more well-justified and clearly presented methodology for the networks that are used, the experiments that are performed, and the conclusions that are drawn from those experiments. # UPDATE AFTER AUTHOR RESPONSE The fact that the results seem to hold almost exactly as strongly on the ResNet-20 as they do on the ResNet-18 seems to imply that the results are not just equivalent to adding regularization into an over-parameterized and under-regularized network, and do in fact provide some effect when applied alongside standard regularization (as opposed to my suspicion that the effect would disappear or even reverse when applied alongside standard regularization). I do still think that there is more work to be done on that front (making more precise exactly the relationship between regularization and this effect), but the rebuttal did satisfactorily address my main concern with the paper. Regarding the addition of Kendall Tau, I am satisfied that they do show a correlation, even with the relatively noisy data. Overall, I'm raising my score to a 6. I still think the paper would be improved by a more thorough discussion of the relationship to other more standard forms of regularization, and I'm hesitant to trust that the methodology will be made substantially more clear without seeing a revised version of the paper, but enough of my concerns were addressed by the experiment on the standard ResNet-20 that I no longer see a strong reason to reject the paper.","- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence.",1.0,1.0,1.0,0.0,nan,1,"The feedback provided in the review is actionable, as it clearly identifies the lack of methodological details in the main body of the paper. It goes beyond merely stating that there is a problem; it provides specific examples of what is missing, such as the pruning process, the pruning schedule, the application of gaussian noise, and the number of batches in an epoch. This level of detail is crucial for the author to understand exactly what needs to be improved. Furthermore, the feedback also acknowledges the existence of methodological details in Appendix A, but points out that these details are not clearly presented enough to be able to reproduce the results with confidence. This additional information helps the author to understand the importance of clarity in methodological details. The feedback is therefore not only actionable but also provides a clear path for the author to follow in order to improve their paper. So the overall score is 1. [RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer has identified a key aspect of the paper, which is the clarity of the methodology. The feedback is specific, pointing out the lack of details in the main body of the paper and the insufficient presentation of methodological details in the appendix. This specificity demonstrates a clear understanding of the paper's content and a constructive approach to providing feedback. The feedback is also positive, as it acknowledges the presence of methodological details in the appendix, even though they are not clearly presented. This positive acknowledgment is a key element in maintaining a respectful tone throughout the feedback. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which is why it would receive a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review of the scientific paper. It addresses the importance of methodological clarity in an empirical paper, which is a critical aspect of scientific research. The feedback points out the lack of methodological details in the main body of the paper, and while it acknowledges the presence of such details in the appendix, it also notes that these details are not clearly presented. This indicates that the feedback is based on the content of the paper and is not merely a general statement.

However, the feedback does not provide any evidence or references to support its claims. It relies on the reviewer's observations and interpretations of the paper's content. While the feedback is logically structured and coherent, it does not meet the criteria for verifiable feedback as it lacks references to external sources or logical reasoning that could be independently verified.

In conclusion, the feedback is relevant to the review and addresses the importance of methodological clarity. However, it falls short in providing evidence or references to support its claims, which is a key aspect of the score rubric. Therefore, the feedback is not fully aligned with the criteria for a score of 1. 
[RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the lack of clarity in the methodology section of the paper, which is crucial for an empirical paper. The feedback is detailed, mentioning specific aspects such as the pruning process, the pruning schedule, the application of gaussian noise, the number of batches in an epoch, and the presentation of methodological details in the appendix. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback is also respectful of the paper's content, as it does not dismiss the paper outright but rather provides constructive criticism on how to improve the clarity of the methodology. Therefore, the feedback aligns well with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1486,ICLR_2023_3777,"Weakness
1: The technical novelty is limited and some details are confusing.
Slimmable networks are a special case of widely studied one-shot NAS (e.g., [R2, R3, R4, R5, R6]), which only considers the width dimension (see discussion in OFA [R2]). There are many techniques to deal with interference among subnetworks. Specifically,
“Slow start” belongs to progressive training in one-shot NAS. For example, OFA proposes a “Progressive Shrinking” strategy, which starts with training the largest sub-network and then progressively fine-tunes the network to support smaller sub-networks by gradually adding them into the sampling space.
“Online distillation” was originally proposed in US-Nets [Yu et al., 2019b]. Apart from the inplace distillation, it also proposes the sandwich rule.
“loss reweights” aims to assign larger weights for sub-networks with large widths. However, it violates the training objective of slimmable networks. The objective is to make each supported sub-network maintain the same level of accuracy as independently training a network with the same architectural configuration, rather than only training an accurate large “supernet”. This is evidenced in Table 2 (e), where adding loss reweighting makes R-50(0.25) perform worse, so what’s the meaning there?
2: Another concern is what are the fundamental differences between self-supervised and supervised training for slimmable networks? This is not clear to me as all the training techniques used are common practices in supervised training.
3: What’s the relationship between unsupervised NAS (e.g., [R4, R5]), including the contrastive self-supervised one (e.g., [R6])?
4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?
5: The paper lacks mathematical modeling for the gradient divergence issue which leads to the optimization difficulty claimed by the authors. I think there are only four possible widths and it is not difficult to analyze the gradient magnitude and directions using SGD with maths formulations. Also, some theoretical analysis on convergence is expected, even assuming a linear neural network is fine [R1].
6: The experiments are far from enough to justify the effectiveness of the proposed method.
6.1: The results are merely based on the ResNet-50 backbone. However, I would like to see more ResNet backbones such as R-101 and R-152. More importantly, experiments on Vision Transformers, such as ViT-B in MoCo v3, must be included in the experiments.
6.2: The paper only evaluates the representation quality using linear probing. However, it must evaluate transfer learning performance which is the standard practice in self-supervised learning (e.g., in MoCo v3). For example, experiments on dataset transfer and downstream tasks such as dense segmentation and detection on COCO and ADE20k are needed.
6.3: How about training the whole network (width 1.0) first then using network pruning (e.g., [R7]) to obtain small networks (width 0.25, 0.5, 0.75)? As this strategy can avoid the interference issue during training.
6.4: It lacks comparisons with methods dealing with sub-network interference, such as switchable BN [Yu et al., 2019], sandwich rules [Yu et al., 2019b] and many others.
7: The discussions and references in related work are far from enough. There are few discussions with single-shot NAS and unsupervised NAS methods. In addition, as I point out in the technical novelty part, the differences and advantages with the related work must be discussed.
8: Writing also needs to be improved.
8.1: What is the definition of the “main parameters” in the introduction?
8.2: In Sec. 3.2, “..., where L
is the loss function”. It should be defined in Eq. (1) where it first appears.
8.3: Many grammar issues. I only point out a few. “Slimmable neworks” in Sec. 2; “server performance degradation” in Sec. 3.2.
9: In Sec. 3.2, authors argue that the two ratios in Figure 3 should be large enough. “In Figure 3f, ..., are larger than 1.0 by a clear margin”. It does provide a clear concept of how large is good enough. In my opinion, it also depends on the network architectures and self-supervised learning frameworks. So Figure 3 may not be statistically significant. References:
[R1]: “On the optimization of Deep Networks: Implicit Acceleration by Overparameterization”, ICML 2018
[R2]: “ONCE FOR ALL: TRAIN ONE NETWORK AND SPECIALIZE IT FOR EFFICIENT DEPLOYMENT”, ICLR 2020
[R3]: “BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models”, ECCV 2020
[R4]: “Are Labels Necessary for Neural Architecture Search?”, ECCV 2020
[R5]: “Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?”, NeurIPS 2020
[R6]: “Contrastive Self-supervised Neural Architecture Search”, Arxiv 2021
[R7]: “Resrep: Lossless cnn pruning via decoupling remembering and forgetting”, CVPR 2022","4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?",-1.0,1.0,-1.0,0.0,nan,1,"The feedback provided in the review is actionable and directly addresses the issue of the default case (1) being used in the paper, despite case (3) showing better performance for most network widths. The reviewer clearly states their disagreement with the author's explanation and provides a clear direction for the author to consider by suggesting that all sub-networks should be equally important. This feedback not only identifies the issue but also offers a practical suggestion for the author to consider, which aligns with the score rubric's criteria for actionability. The reviewer's feedback is specific, clear, and directly related to the aspect in question, making it highly useful for the author to improve their paper. Therefore, the feedback is highly relevant and actionable, which is why it receives a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the authors' work and provides a clear critique of the default case used in the paper. The feedback is focused on the aspect of loss reweighting and its implications for the performance of the network. The reviewer's disagreement with the author's explanation is presented in a manner that is both respectful and encouraging, inviting the authors to consider the importance of all sub-networks. The feedback is also factual and does not contain any negative or aggressive language. Therefore, based on the score rubric, the feedback is relevant to the review and is delivered in a respectful and constructive manner. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is relevant to the review and directly addresses the aspect of loss reweighting. The reviewer has identified a discrepancy in the paper's approach to loss reweighting, specifically pointing out that case (1) is used by default in the paper, despite case (3) showing better performance for most network widths. This claim is supported by references to specific parts of the paper, such as Table 2e and Eq. (5), which demonstrates a clear understanding of the content and a logical reasoning process. The reviewer's disagreement with the author's explanation is also well-articulated, showing a critical analysis of the paper's approach. Therefore, the feedback is both relevant and well-supported, aligning with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is highly specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the choice of case (1) in the reweighting process. The feedback clearly identifies the section of the paper where this issue arises, namely Page 8 and Table 2e. It also provides a detailed critique of the author's explanation, questioning the rationale behind the default choice of case (1). This specificity in the feedback ensures that the reviewer's comments are tailored to the content of the paper, and not a general comment for any scientific document. The feedback is not only specific but also directly challenges the author's decision, which is a clear indication of the feedback's relevance to the paper. Therefore, based on the score rubric, the feedback is highly specific and directly relevant to the content of the draft, making it a score of 1. 
[RESULT] 1"
4671,NIPS_2020_1856,"* See section (Clarity). * 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this). * For better or for worse, there is a lot going on -- and it feel like the pages 5 and 8 were crammed up. I wonder if maybe the theory of OT (pages 3-4)can be reduced to 1 page, so that there can be a more thorough discussion, and explanation of the results and metrics rather than having the reader refer to the Supplementary Material consistently for some minor details.",* See section (Clarity).* 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this).,-1.0,1.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the clarity of the paper, such as the number of trials and the role of the observers, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback merely states the issues without providing any guidance on how to address them. This lack of actionable advice makes the feedback less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on the lack of information in the review, which is a factual observation. The feedback is also constructive, as it points out specific details that are missing from the review, such as the number of trials and the role of the observers. However, the feedback could be more respectful by acknowledging the efforts of the authors and providing more positive reinforcement. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response is not supported by any evidence or references. The reviewer makes a claim about the number of trials and the role of the observers without providing any supporting information. This lack of evidence or references makes the feedback unverifiable and not respectful to the scientific rigor required in a review. The feedback also fails to cite any external work or use logical reasoning to support its claims. Therefore, based on the score rubric, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or references to support its claims. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer mentions the use of two observers and the number of trials, but fails to provide specific details about these aspects. Additionally, the reviewer does not address the potential bias of the observers, which is a critical aspect of the experiment. The feedback is vague and does not directly address the content of the paper, making it difficult to identify which parts of the document are being discussed. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
3428,NIPS_2020_747,"I have a few concerns regarding this paper. 1. How practical/realistic is the threat model? In the paper, chapter 2.2, the NoBox attack demands the training set. In the real world however, isn't the training set even more precious than the trained models? 2. In the AEG objective, the generator needs to get the gradient to be trained. Would AEG still applicable to the non-differentiable robust classifiers, such as: [1] Countering Adversarial Images using Input Transformations [2] THERMOMETER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES [3] Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples The common point of these approaches is they all incorporate some sort of in-differentiability. 3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST. 4. One experiment I'd like to request: (i)- get a model trained on some dataset at epoch N, N+1, N+2... N+k (ii)- use the generator to generate pertubed imagess to attack all of them. (iii)- show the effectiveness of the attack. This should be a more realistic scenario and it aligns with the main point. 5. A portion of the experiment has compared the NoBox attack to the other attacks. These are generally under different threat model assumptions. However the main claim of the paper is that the NoBox is capable of attacking different models in the same function space. It would be better if the authors can present the NoBox's effectiveness attacking more diversified trained neural network models.",3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST.,-1.0,-1.0,-1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely points out a limitation in the experiments used in the paper, without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issue, nor does it provide any direction or hints on what needs to be done. It lacks the necessary detail and actionable steps that would make the feedback useful for the author. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it simply points out a limitation in the experiments described in the paper. The feedback is delivered in a straightforward manner, without any attempt to be overly polite or to provide constructive criticism. It is a factual observation that the experiments in the paper are limited to only CIFAR and MNIST, which is a relevant point in the context of the review. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a higher score. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim made in the feedback, that the experiments in the paper are limited to only CIFAR and MNIST, is not supported by any evidence. There is no reference to the paper itself, nor is there any citation of external sources to back up the claim. The feedback also lacks logical reasoning that could be used to verify the claim. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any evidence to support the claim made. 

In contrast, a score of 1 would require the feedback to provide evidence to support the claim, such as references to the paper itself, citations of external work, or logical reasoning. Without these elements, the feedback does not meet the criteria for a score of 1. 

So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer mentions the use of ImageNet in other papers but fails to address the specific experiments conducted in the paper under review. The feedback does not mention any specific parts or methods in the paper, making it difficult to identify which sections or elements in the paper are being addressed. The lack of specificity in the feedback does not align with the score rubric's requirement for specificity in a review. Therefore, the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. 
[RESULT] 0"
5171,NIPS_2020_880,"1. I could not see a strong motivation for explicitly enforcing sparsity on architecture parameters. This is because there are already many works trying to decouple the dependency of evaluating sub-networks on the training of supernet (i.e., making the correlation higher). This means that we have ways to explicitly decouple the network evaluation with supernet training without adding a sparsity regularizaiton. 2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2. 3. The one-stage method proposed basically focusing on training network weights W after the training of architecture parameters is converged. However, similar idea can also be achieved in other differentiable NAS framework, where one can continue training the supernet weights after the architecture remains little change. For example, in GDAS, after the entropy of edges is well minimized, the sampled architecture will be close to determnistic, and one can keep training W to obtain the optimal weights. Moreover, other one-stage methods like [4] are not compared nor discussed. ======================== After reading the author's response, most of my concerns have been addressed. I choose to accept this submission. ======================== [1] Guo, Zichao, et al. ""Single path one-shot neural architecture search with uniform sampling."" ICLR 2020. [2] X. Dong and Y. Yang. Searching for a robust neural architecture in four gpu hours. CVPR 2019. [3] Bender, Gabriel, et al. ""Understanding and simplifying one-shot architecture search."" ICML2018. [4] Cai, Han, Chuang Gan, and Song Han. ""Once for all: Train one network and specialize it for efficient deployment."" ICLR 2020.","2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2.",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the paper, such as the lack of detail in Table 2 and the absence of comparisons with other works. However, it fails to offer specific, clear, and practical suggestions that the author can use to improve their paper. The feedback does not provide concrete steps or recommendations that guide the author in addressing the issues. Instead, it merely states the problems without offering any solutions or guidance. This lack of actionable feedback makes the review less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is critical and somewhat negative, which is not in line with the rubric's requirement for a polite and respectful tone. The feedback points out the lack of detail in the experiment and the absence of comparison with other works, which are valid points. However, the tone used to convey these points is somewhat accusatory and lacks the constructive approach that would be expected in a review. The feedback does not encourage the authors or provide any positive reinforcement. Instead, it focuses on the shortcomings of the paper without offering any constructive suggestions for improvement. This approach is not conducive to a respectful and positive review process. Therefore, based on the rubric's criteria for politeness and respectfulness, the feedback is not as effective as it could be. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer makes several assertions, such as the need for more experiment details and the comparison of different methods, but these claims are not substantiated with any references to external sources or logical reasoning. The feedback also fails to provide any internal references from the paper, which would have been necessary to support the claims made. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or references to support the claims made. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the aspect of understanding Table 2 and the methods used in the paper. The feedback mentions specific methods such as weight-sharing methods, BN re-calculation, and the comparison with other works like [3] and [2]. It also points out the lack of explanation in the main content regarding Table 2. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's requirement for specificity and relevance to the content of the draft. So the score is 1. [RESULT] 1"
1908,ARR_2022_64_review,"- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:   - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction   - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. 
In line ""544"", I still quite understand why using entity representations can reduce language bias. ","- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:",-1.0,-1.0,-1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the contribution of the paper is incremental and provides an example of a related work. However, it fails to offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback does not provide any direction or hints of what needs to be done to address the issues identified. It lacks the necessary actionability that would make the feedback useful to the author. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is neither negative nor positive, and it does not show any signs of disrespect or aggression. The feedback is focused on the content of the paper and its contribution to the field, without any personal or emotional comments. It is clear and objective, which is appropriate for a scientific review. However, the feedback could have been more constructive by providing specific suggestions for improvement or acknowledging the strengths of the paper. This would have made the feedback more helpful and encouraging for the authors. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. It makes a claim about the relevance of entity embeddings in cross-lingual tasks, but it does not provide any evidence or references to support this claim. The feedback is general and does not cite any specific works or parts of the paper that would allow for independent verification of the claim. It also lacks logical reasoning that could substantiate the claim. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any verifiable evidence or logical reasoning to support the claim made. 

So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is quite general and does not specifically address any particular aspect of the paper. It mentions the relevance of entity embeddings in cross-lingual tasks and acknowledges the incremental contribution of the paper. However, it fails to provide any specific comments or examples that would help to identify which sections, statements, or elements in the paper are being addressed. The feedback does not demonstrate a clear understanding of the content of the paper, nor does it offer any detailed critique or suggestions for improvement. Therefore, based on the score rubric, the feedback is not specific and does not directly relate to the content of the draft. It is a general comment that could apply to any scientific document, rather than being tailored to the content of the paper in question. So the overall score is 0. [RESULT] 0"
3403,NIPS_2020_585,"I think the major problem of this paper is that the novelty is limited. It seems that nearly all the components in the proposed method have been used, and the combination manner is also not so novel. 1. The two-stage method is widely used to deal with partial label examples. The first stage in this paper adopts the widely used label probagation strategy to obtain labeling confidence for partial label examples. However, SSPL [22] also adopts this strategy (which obtain labeling confidence for both partial label examples and unlabeled examples). So there seems no novelty for the first stage. 2. For the second stage, a modified maximum margin formulation is introduced, which can jointly enable the induction the induction of predictive model and the estimation of labeling confidence over unlabeled data. The formulation of the second stage mainly follows [17]. There are two differences between this paper and [17]: Firstly, this paper uses the obtained labeling confidence of partial examples to give different weights on the losses of different examples. Secondly, the unlabeled data is used in the formulation by using the widely-used manifold regularization, so that the labeling confidence of unlabeled data will be jointly estimated. Generally, I think the improvement over [17] is intuitive and reasonable. But I think that the novelty is not enough, especially on such a prestiguous venue NeurIPS. 3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage. 4. This paper may not effectively deal with large-scale datasets because it uses alternating optimization and needs to construct a similarity graph in advance.","3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage.",1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is relevant to the aspect of alternating optimization. It correctly identifies that the paper does not provide a theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage. However, the feedback lacks actionable suggestions that the author can use to improve their paper. It does not offer concrete steps or recommendations that guide the author in addressing the identified issues. The feedback is more of a statement of what is missing rather than a guide for revision. Therefore, the feedback is not as useful as it could be, as it does not provide the author with a clear path for revision. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback points out the lack of theoretical analysis and empirical evidence in the paper, which is a relevant aspect to the review. However, the feedback does not explicitly encourage the authors or provide positive reinforcement, which would have elevated the score to a 1. The feedback is factual and does not contain any negative or aggressive language, which is in line with the score rubric's criteria for a neutral, factual, or passive tone. Therefore, based on the score rubric, the feedback is respectful and constructive, but it lacks the encouragement and positive reinforcement that would have made it a 1. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is relevant to the review and is supported by an external source, which is the related paper [21]. The feedback points out a significant difference between the two papers, specifically the lack of theoretical analysis or empirical evidence in the current paper. This comparison is crucial for the reviewer to assess the quality and completeness of the paper under review. The feedback is also logical, as it directly addresses the aspect of optimization and its theoretical and empirical support. Therefore, the feedback is well-supported and aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the lack of theoretical analysis or empirical evidence regarding the convergence of the modified maximum margin formulation in the second stage. The feedback is tailored to the content of the paper, and it clearly identifies the part of the document that is being discussed. The feedback also compares the paper with a related paper [21], which adds depth to the critique. Therefore, the feedback is specific, relevant, and directly addresses the content of the paper, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3980,NIPS_2020_1314,"1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP. 2. In Section3.1 the authors give a thorough comparison between the discrete Gaussian and discrete Laplace. The conclusions deduced by the discrete distribution are almost the same as the prior conclusions of the continuous distribution. 3. This paper does not compare their methods with the exponential mechanism(Frank McSherry and Kunal Talwa 2007). Exponential mechanism can also output a discrete answer for a query.","1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP.",-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the response does not offer any actionable suggestions or recommendations for the author to improve their paper. It merely states that the discrete Gaussian mechanism is not a big innovation and that it is a natural derivation of the discrete Laplacian noise to DP. This feedback does not provide any guidance or direction for the author to address the issues raised. It lacks the specificity and clarity that would make it actionable. Therefore, based on the score rubric, the feedback is not actionable and does not meet the criteria for providing a clear path for revision. So the score is 0. [RESULT] 0",0,"The feedback provided in the response is factual and does not contain any negative or disrespectful language. It is a critique of the paper's proposed method, comparing it to existing methods. However, the tone of the feedback is somewhat dismissive, implying that the proposed method is not a significant innovation. This could be perceived as lacking in respect for the authors' work. The feedback does not encourage the authors or provide any positive reinforcement. It is more of a critique than a constructive review. Therefore, the feedback does not fully meet the criteria for a score of 1, as it lacks the politeness and encouragement that would be expected in a positive review. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that the discrete Gaussian mechanism is not a big innovation is not supported by any evidence or references. The feedback does not cite any external sources or provide any logical reasoning that could be independently verified. It simply states a personal opinion without any supporting details. This lack of evidence and logical reasoning makes the feedback less credible and does not meet the standards set by the score rubric. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. So the score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the discrete Gaussian mechanism proposed by the paper and compares it to the discrete Laplace distribution introduced in (GRS12) and used in the 2020 US Census. This comparison shows a clear understanding of the paper's content and a direct critique of the proposed mechanism. The feedback is tailored to the content of the paper, and it is easy to identify which part of the document is being discussed in the feedback. Therefore, the feedback meets the criteria of specificity and relevance to the content of the paper. So the score is 1. [RESULT] 1"
608,ICLR_2021_1900,"weaknesses:
Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq. (2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 2) Regarding Eq. (5), what the intuition of the adaptive matrix (i.e., (log_{\mu+1} (N_i+1))^{\delta^{\tau}}) when i = j should be provided to the authors.
The major issue of this paper is the experimental evaluations. 1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic. 2) Some state-of-the-art methods are not involved in the experimental comparisons, such as [ref1-ref5]. Moreover, the accuracy of the proposed method cannot outperform these methods.
Minor issues:
There are several typos and writing problems in this paper. For example, on Page 3, ""Dubey et al.Dubey et al. (2018)"", and ""Chen et al.Chen et al. (2019)"". On Page 4, ""PC Dubey et al. (2018)"". On Page 8, ""And also solves the long-tailed problem by an adaptive matrix term.""
[ref1] Weakly Supervised Fine-grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning, CVPR 2020.
[ref2] Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019.
[ref3] Fine-Grained Visual Classification via Progressive Multi-Granularity Training of Jigsaw Patches, ECCV 2020.
[ref4] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.
[ref5] BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition, CVPR 2020.","1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic.",-1.0,1.0,-1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the classification accuracy is not significantly better than previous work, without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issues raised, nor does it provide any direction or hints on what needs to be done. It lacks the necessary detail and actionable steps that would make it useful for the author to revise their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is factual and does not contain any negative or disrespectful language. It is a straightforward critique of the method's effectiveness, which is a relevant aspect of the review. The tone is neutral and does not attempt to encourage or discourage the authors. The feedback is focused on the accuracy of the method and its comparison to previous work, which is a valid point of critique. Therefore, based on the score rubric, the feedback is neither positive nor negative, but rather neutral and factual. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response is not supported by any evidence or references. It makes a claim about the classification accuracy of the proposed method without providing any data or comparisons to support this claim. The feedback also fails to cite any external sources or use logical reasoning to validate its assertions. This lack of evidence and support for the claims made in the feedback is why it does not meet the criteria for a score of 1. Instead, it aligns with the criteria for a score of -1, as it imposes a claim without providing any evidence for it. Therefore, the feedback does not respect the aspect of verifiable feedback, which is crucial for a scientific review. 
[RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the classification accuracy of the proposed method on fine-grained benchmark datasets and iNat18, which is a critical aspect of the paper. The feedback also points out that the effectiveness of the proposed method is problematic, which is a direct critique of the paper's content. The specificity of the feedback ensures that the review is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2060,ARR_2022_1_review,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. ","- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.",1.0,1.0,1.0,1.0,nan,1,"The feedback provided in the review is actionable and directly addresses the aspect of the review. It suggests a specific method for the authors to follow, which is to conduct an ablation study with a mix of multilingual and monolingual data. This recommendation is clear and practical, guiding the authors on how to improve their paper by ensuring that the performance gains are accurately attributed to the multilingual aspect of the model. The feedback also hints at the potential issue of the dataset size, which is a crucial factor in model performance. By suggesting an ablation study, the feedback provides a clear path for the authors to take in order to address the concerns raised in the review. Therefore, the feedback is highly actionable and directly relevant to the review. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer's request for an ablation study is a valid point that respects the authors' work while also suggesting a way to improve it. The feedback is also positive, as it acknowledges the potential benefits of the multilingual model. However, the feedback could be more encouraging by highlighting the potential impact of the suggested improvements. Despite this, the feedback is still respectful and constructive, which is why it receives a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected in the review. The reviewer has made a claim about the need for an ablation study to determine whether the performance gain is from the large dataset or from the multilingual training. This claim is supported by a logical reasoning that can be independently verified, as it refers to the training data sizes and the potential impact on the model's performance. The reviewer has also suggested a specific method to address the issue, which is to train the model on a mix of multilingual data with the same overall dataset size as the monolingual. This suggestion is a logical extension of the claim made and is directly related to the aspect being discussed. Therefore, the feedback is well-reasoned and aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is highly specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the comparison of the multilingual model with the single model. The feedback is tailored to the content of the paper, as it specifically asks for an ablation study to clarify whether the performance gain is due to the large dataset size or the multilingual aspect. This shows a clear understanding of the paper's content and a direct critique of the methodology used. The feedback is not a general comment but a specific critique that directly relates to the content of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3525,NIPS_2020_324,"The experiments are performed on two simple environments: 1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.","1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.",1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely states that the proposed method AdaMB does not seem to perform any better than AdaQL and that the paper lacks a thorough comparison of AdaMB and AdaQL. This feedback does not provide any guidance or direction for the author to address the issues identified. It fails to offer concrete steps or recommendations that would help the author in revising their paper. Therefore, the feedback does not meet the criteria of being actionable, as it does not provide any direction or hints of what needs to be done. So the score is 0. [RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. The feedback focuses on the comparison between AdaMB and AdaQL, and it points out the lack of thorough comparison in the paper. However, it does not provide any positive or constructive feedback that could help the authors improve their work. The feedback is purely factual and does not engage with the authors in a way that would be considered polite or helpful. Therefore, based on the score rubric, the feedback is neutral and does not respect or encourage the authors. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer's statements regarding the performance of AdaMB and AdaQL are not substantiated with any data or comparisons from the paper. The feedback also lacks logical reasoning that could be used to verify the reviewer's claims. The feedback is more of a general observation rather than a detailed analysis supported by evidence. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claims made. 
[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the paper. It fails to address specific parts or methods in the paper, making it difficult to identify which sections, statements, or elements in the paper are being addressed. The feedback is general and does not provide a clear connection to the content of the paper. It does not demonstrate a detailed understanding of the paper's content, nor does it offer specific insights or critiques. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
114,ICLR_2022_1971,"Weakness: Method:
1. Verification and Optimization:
The proposed HNPF method is for verification (e.g., check whether a given solution x is Pareto optimal), but not for optimization (e.g., find an (approximate) Pareto solution x). It needs an extra search method, such as random sampling in this work, to first generate a large number of feasible solutions to cover the whole search space. Therefore, the underlying optimization is indeed random sampling (independent from HNPF), which could be extremely inefficient for a non-trivial search space. It is not suitable to put and compare the proposed HNPF method with other optimization methods that can directly find the (approximate) Pareto solution.
Since HNPF depends on random sampling, it is not surprising that it can only work for small scale problems.
2. The Reason to Build the Model:
HNPF needs to first build a neural network to check whether a given solution x satisfies the Fritz-John Condition (FJC), which requires a large number of training samples (e.g., 11K for a two-dimensional problem). The learned model is mainly used to classify whether the extra randomly sampled solutions (e.g., 9K) are weak Pareto optimal or not. The reason for model building, such as the advantage over the simple FJC rule-based classification, is not well motivated and justified in this work.
The proposed Pareto filter in stage 2 is not discussed and compared with other related nondominated sorting algorithms (e.g., [2]).
3. Necessary Condition for Pareto Optimality:
The KKT[3] and FJC[4] are two types of first order necessary conditions for (local) Pareto efficiency (Pareto optimality). In my understanding, the multi-objective optimization based MTL algorithms mentioned in this work (Sener & Koltun, 2018; Lin et al., 2019a; Mahapatra & Rajan, 2020; Ma et al., 2020; Navon et al., 2021) mainly use the gradient-based multi-objective optimization methods (e.g., MGDA) [5-7], which is based on the KKT condition. For these methods, in each update step, the gradient can be written as a linear combination of the gradient for each objective with adaptive weights derived from the KKT condition. Therefore, they are all different from the simple linear scalarization with fixed weights. All the claims and analyses in this work that the previous works use simple linear scalarization is not correct.
The FJ condition is also for local Pareto convergence, similar to the KKT condition. The global convergence property is solely due to random sampling that only works for extremely low-dimensional problems. It is unfair to say the proposed algorithm can overcome the local convergence of other gradient-based methods. In addition, the proposed algorithm heavily depends on the Fritz-John condition, but the original work [4] is not cited.
4. Linear Scalarization and Convex Pareto Front:
It is well-known that the simple linear scalarization cannot find the non-convex part of the Pareto front [8]. This finding leads to the seminal work on NBI scalarization (Das & Dennis.,1998), which is indeed one fundamental work that inspires the proposed method in this work (section 4, first sentence). The claim ""it is incorrect to state that LS itself fails if the Pareto front is non-convex"" (appendix, page 15) is questionable.
Since the proposed HNPF can only verify whether a given solution is weak Pareto optimal, its ability to find the whole Pareto front totally depends on the extra sampling method (e.g., random sampling) to generate all the Pareto solutions (might be infinite). It is misleading to indicate the proposed HNPF method itself can find the whole Pareto front. In addition, since the Pareto set has measure zero and infinite cardinality, the random sampling + HNPF method can at most find a dense approximation to the Pareto set. Experiment:
5. Algorithms for Comparison:
All the multi-objective optimization based MTL algorithms are designed for optimizing a deep neural network with millions of parameters. They implicitly depend on the assumption that the deep neural network has good properties (e.g., no bad local optimum [9][10]) on its loss functions, which is consistent with other gradient-based single-objective optimization methods. They are not designed to find the global Pareto front for low-dimensional problems.
For low-dimensional problems, it is more suitable to compare with the model-free multi-objective optimization methods such as the multi-objective evolutionary algorithm [11,12] and multi-objective CMA-ES [13]. If the model building is allowed, Multi-Objective Bayesian Optimization (MOBO) algorithms can have a very good sampling efficiency for the low-dimensional problems [14,15]. It is also very common to conduct non-dominated filtering at the end of those model-free algorithms or MOBOs (e.g., only keeping the current non-dominated solutions).
6. Training + Sampling:
The proposed method needs to first sample 11k solutions to train the neural network model, then randomly generate extra 9K solutions for filtering. Is there any advantage over simply using FJC to filter 9K (or 11k + 9K) randomly sampling solutions?
7. Figure from Other Works:
Many figures in the main paper and the appendix are directly borrowed from other works. I think this is not appropriate even the credits are given to the original works. Reference:
[1] Ruchte, Michael, and Josif Grabocka. Multi-task problems are not multi-objective. arXiv preprint arXiv:2110.07301, 2021.
[2] Roy, Proteek Chandan, Kalyanmoy Deb, and Md Monirul Islam. An efficient nondominated sorting algorithm for large number of fronts. IEEE transactions on cybernetics 49, no. 3: 859-869, 2018.
[3] Kuhn, H. W., and A. W. Tucker. Nonlinear Programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pp. 481-492. University of California Press, 1951.
[4] Da Cunha, N. O., and E. Polak. Constrained minimization under vectorvalued criteria in finite dimensional spaces. Journal of Mathematical Analysis and Applications, 19(1), 103–124 ,1967.
[5] Fliege, Jorg, and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research 51, no. 3: 479-494, 2000.
[6] Fliege, Jorg, and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on SQP techniques. SIAM Journal on Optimization 26, no. 4: 2091-2119, 2016.
[7] Desideri, Jean-Antoine. Multiple-gradient descent algorithm (MGDA) for multiobjective optimization. Comptes Rendus Mathematique 350, no. 5-6: 313-318, 2012.
[8] Das, Indraneel, and John E. Dennis. A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems. Structural optimization 14, no. 1: 63-69, 1997.
[9] Kawaguchi, Kenji. Deep learning without poor local minima. NeurIPS 2016.
[10] Kawaguchi, Kenji, and Leslie Kaelbling. Elimination of all bad local minima in deep learning. AISTATS 2020.
[11] Deb, Kalyanmoy, Amrit Pratap, Sameer Agarwal, and T. A. M. T. Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, no. 2: 182-197, 2002.
[12] Zhang, Qingfu, and Hui Li. ""MOEA/D: A multiobjective evolutionary algorithm based on decomposition."" IEEE Transactions on evolutionary computation 11, no. 6: 712-731, 2007.
[13] Igel, Christian, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evolutionary computation 15, no. 1: 1-28, 2007.
[14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020.
[15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.","1: 1-28, 2007. [14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020. [15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.",0.0,0.0,0.0,0.0,nan,0,"The response provided does not offer any actionable feedback or suggestions for improvement. It merely lists the references of two papers without any commentary or critique. The lack of specific, clear, and practical suggestions makes it difficult for the author to understand what aspects of their paper need attention or revision. The response does not fulfill the criteria of providing actionable feedback, which is essential for the author to improve their paper. Therefore, the score is 0. 
[RESULT] 0",0,"The response provided is a list of references, which is not a feedback paragraph. It does not address the aspect of politeness or the tone of the feedback, as it is not a feedback paragraph at all. The response is purely factual and does not contain any tone or manner of delivery. Therefore, it does not meet the criteria of the score rubric, which requires an evaluation of the tone of the feedback. As such, the response does not receive a score based on the rubric. 
[RESULT] 0",0,"The response provided does not meet the criteria for a score of 1. It does not offer any evidence or logical reasoning to support the claims made. The response merely lists references to external sources without any context or explanation of how these references relate to the feedback. This lack of context and explanation means that the feedback does not provide any verifiable evidence to support its claims. Therefore, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of verifiability. 

The response also does not meet the criteria for a score of 0, as it does not make any general statements or observations that could be considered irrelevant to the review. Instead, it lists specific references, which suggests that the feedback is trying to be relevant, but it fails to provide any logical reasoning or context to support its claims. 

In conclusion, the response does not meet the criteria for either a score of 1 or a score of 0, as it does not provide any verifiable evidence or logical reasoning to support its claims, nor does it make any general statements or observations that could be considered irrelevant to the review. 

So the overall score is 0. [RESULT] 0",0,"The response provided is a list of references, which does not directly address the specific aspect of the review. It lacks any specific comments or direct relevance to the content of the draft. The feedback does not mention any specific parts, statements, or elements in the paper, making it difficult to identify which parts of the document are being addressed. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. It is a general comment that could apply to any scientific document, rather than being tailored to the content of the paper. So the overall score is 0. [RESULT] 0"
2507,NIPS_2021_537,"Weakness: The main weakness of the approach is the lack of novelty. 1. The key contribution of the paper is to propose a framework which gradually fits the high-performing sub-space in the NAS search space using a set of weak predictors rather than fitting the whole space using one strong predictor. However, this high-level idea, though not explicitly highlighted, has been adopted in almost all query-based NAS approaches where the promising architectures are predicted and selected at each iteration and used to update the predictor model for next iteration. As the authors acknowledged in Section 2.3, their approach is exactly a simplified version of BO which has been extensively used for NAS [1,2,3,4]. However, unlike BO, the predictor doesn’t output uncertainty and thus the authors use a heuristic to trade-off exploitation and exploration rather than using more principled acquisition functions.
2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C. 3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper.
Detailed Comments: 1. The authors conduct some ablation studies in Section 3.2. However, a more important ablation would be to modify the proposed predictor model to get some uncertainty (by deep-ensemble or add a BLR final output layer) and then use BO acquisition functions (e.g. EI) to do the sampling. The proposed greedy sampling strategy works because the search space for NAS-Bench-201 and 101 are relatively small and as demonstrated in [6], local search even gives the SOTA performance on these benchmark search spaces. For a more realistic search space like NAS-Bench-301[7], the greedy sampling strategy which lacks a principled exploitation-exploration trade-off might not work well. 2. Following the above comment, I’ll suggest the authors to evaluate their methods on NAS-Bench-301 and compare with more recent BO methods like BANANAS[2] and NAS-BOWL[4] or predictor-based method like BRP-NAS [5] which is almost the same as the proposed approach. I’m aware that the authors have compared to BONAS and shows better performance. However, BONAS uses a different surrogate which might be worse than the options proposed in this paper. More importantly, BONAS use weight-sharing to evaluate architectures queried which may significantly underestimate the true architecture performance. This trades off its performance for time efficiency. 3. For results on open-domain search, the authors perform search based on a pre-trained super-net. Thus, the good final performance of WeakNAS on MobileNet space and NASNet space might be due to the use of a good/well-trained supernet; as shown in Table 6, OFA with evalutinary algorithm can give near top performance already. More importantly, if a super-net has been well-trained and is good, the cost of finding the good subnetwork from it is rather low as each query via weight-sharing is super cheap. Thus, the cost gain in query efficiency by WeakNAS on these open-domain experiments is rather insignificant. The query efficiency improvement is likely due to the use of a predictor to guide the subnetwork selection in contrast to the naïve model-free selection methods like evolutionary algorithm or random search. A more convincing result would be to perform the proposed method on DARTS space (I acknowledge that doing it on ImageNet would be too expensive) without using the supernet (i.e. evaluate the sampled architectures from scratch) and compare its performance with BANANAS[2] or NAS-BOWL[4]. 4. If the advantage of the proposed method is query-efficiency, I’d love to see Table 2, 3 (at least the BO baselines) in plots like Fig. 4 and 5, which help better visualise the faster convergence of the proposed method. 5. Some intuitions are provided in the paper on what I commented in Point 3 in Weakness above. However, more thorough experiments or theoretical justifications are needed to convince potential users to use the proposed heuristic (a simplified version of BO) rather than the original BO for NAS. 6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.
7. The results in Table 2 which show linear-/exponential-decay sampling clearly underperforms uniform sampling confuse me a bit. If the predictor is accurate on the good subregion, as argued by the authors, increasing the sampling probability for top-performing predicted architectures should lead to better performance than uniform sampling, especially when the performance of architectures in the good subregion are rather close. 8. In Table 1, what does the number of predictors mean? To me, they are simply the number of search iterations. Do the authors reuse the weak predictors from previous iterations in later iterations like an ensemble?
I understand that given the time constraint, the authors are unlikely to respond to my comments. Hope those comments can help the authors for future improvement of the paper.
References: [1] Kandasamy, Kirthevasan, et al. ""Neural architecture search with Bayesian optimisation and optimal transport."" NeurIPS. 2018. [2] White, Colin, et al. ""BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search."" AAAI. 2021. [3] Shi, Han, et al. ""Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS."" NeurIPS. 2020. [4] Ru, Binxin, et al. ""Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels."" ICLR. 2020. [5] Dudziak, Lukasz, et al. ""BRP-NAS: Prediction-based NAS using GCNs."" NeurIPS. 2020. [6] White, Colin, et al. ""Local search is state of the art for nas benchmarks."" arXiv. 2020. [7] Siems, Julien, et al. ""NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search."" arXiv. 2020.
The limitation and social impacts are briefly discussed in the conclusion.","6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.",0.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely points out a contradiction between the results in Table 3 and Table 4 without offering any guidance or suggestions on how to resolve this inconsistency. The feedback does not provide any direction or hints on what needs to be done to improve the paper. It lacks the specificity and clarity that would make it a useful tool for the author to revise their work. Therefore, the feedback does not meet the criteria of being actionable, as it does not offer concrete steps or recommendations that would guide the author in addressing the issues identified. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it points out a discrepancy in the results presented in the paper. The feedback is focused on the content of the paper and does not address the tone or manner in which the feedback is delivered. It is a straightforward observation of the inconsistency in the results, which is a factual aspect of the review. Therefore, based on the score rubric, the feedback is neutral and does not meet the criteria for a score of 1 or -1. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback points out a contradiction between the results in Table 3 and Table 4, which is a relevant aspect to the review. However, the feedback does not provide any evidence or logical reasoning to support the claim of contradiction. It merely states the discrepancy without any supporting references or logical validation. This lack of evidence and logical reasoning makes the feedback less reliable and does not meet the criteria for a score of 1. Therefore, the feedback is not fully respecting the aspect of verifiability. 

In contrast, a score of 1 would require the feedback to provide evidence or logical reasoning to support the claim of contradiction. This could be done by referencing specific parts of the paper or citing external sources. Without this, the feedback does not meet the criteria for a score of 1. 

So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a clear discrepancy between the results presented in Table 3 and Table 4, which is a critical aspect of the paper. The feedback is tailored to the content of the paper, and it is easy to identify which part of the document is being discussed in the feedback. The feedback also respects the aspect by pointing out the contradiction in the results, which is a significant issue in the paper. Therefore, the feedback is relevant, specific, and respects the aspect, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3487,NIPS_2020_1312,"- The contribution of local voxel-bounded implicit fields is not motivated and evaluated enough. The authors showed that voxel embeddings improve the performance. However, an experiment that shows the performance with different resolutions (partly done with the progressive training evaluation) would improve the paper. - Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution. - For the experiments on large-scale scenes and dynamic scenes, qualitative results for existing methods are missing. Furthermore, quantitative results are missing for these experiments (also for existing methods). - The geometric result in the ScanNet experiment does not seem to be very good. Therefore, it would be interesting to see how accurate the represented geometry is and how it affects the overall performance of the method. - The effect of the early termination is not properly evaluated in the paper. - The initial grid resolution seems to be already very high. It would be interesting to see how this affects the method. - There should also be comparisons to DeepVoxels since this work also leverages local voxels as a feature representation.","- Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution.",1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies a potential issue with the progressive training aspect but fails to offer any specific, clear, or practical suggestions for improvement. The feedback merely states that the progressive training is not evaluated enough and that the authors need to show that the training is better at the increased resolution. However, it does not provide any guidance on how to achieve this or what steps the authors should take to address the issue. The feedback lacks the necessary detail and direction that would make it actionable. Therefore, based on the score rubric, the feedback is not relevant to the review and does not respect the aspect in question. 
[RESULT] 0",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback addresses the aspect of progressive training and its evaluation, pointing out the need for a more rigorous comparison between different resolutions. This approach is helpful as it guides the authors on how to improve their study. The feedback is also positive, as it acknowledges the potential benefits of progressive training, while also suggesting a way to validate these benefits more effectively. The tone is respectful, as it does not criticize the authors but rather provides a constructive critique. Therefore, based on the score rubric, the feedback is relevant to the review and is delivered in a manner that is polite, helpful, and positive. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim made in the feedback, that the progressive training is not evaluated enough, is not supported by any evidence or references. There is no mention of any external sources or internal references from the paper that could validate the claim. The feedback also lacks logical reasoning that could be independently verified. The feedback simply states the claim without providing any substantiation. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claim. 

In contrast, a score of 1 would require the feedback to provide evidence or logical reasoning to support the claim. This could include references to external sources, references to parts of the paper, or logical reasoning that can be independently verified. Without these elements, the feedback does not meet the criteria for a score of 1. 

So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the aspect of progressive training and its evaluation in the ablation study. The feedback is tailored to the content of the paper, as it specifically points out the need for the authors to show that the progressive training is better than one round of training at both the initial and increased resolutions. This is a clear indication that the feedback is specific and directly relevant to the content of the draft. The feedback also identifies a potential issue with the study, which is the possibility that the improved performance is due to the higher voxel resolution, not the progressive training itself. This shows a deep understanding of the content and a critical evaluation of the study. Therefore, the feedback is highly specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1654,ICLR_2023_1765,"weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, t ∈ R while y , f ( θ ) ∈ R n
, where n
is the size of the training set, so that the operation y − t − f ( θ )
is not well-defined. Moreover, even if y , f ( θ ) ∈ R
, the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2
, rather than ( θ ) ≤ t
. Since, in general, the training problem will be defined for y ∈ R n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are ""reasonable"". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- ""Issue 1: Is non-convex deep neural networks always better?"": ""is"" should be ""are"". - ""While some experiments show that certain local optima are equivalent and yield similar learning performance"" -- this should be supported by a reference. - ""However, the derivation of strong duality in the literature requires the planted model assumption"" --- what do you mean by ""planted model assumption""? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - ""In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m
representation power."" -- A statement like this only makes sense under some definition of ""representation power"". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - ""Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient."" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - ""... too many minimization pooling layers will damage the representation power of the neural network"" --- why? Can the authors expand on this observation?
Section 5: - ""... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y
"" --- note that this observation requires y ≥ 0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): ""conveyers"" -> ""converges"".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - ""It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)"" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References
[1] Robbins, Herbert, and Sutton Monro. ""A stochastic approximation method."" The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. ""Learning representations by back-propagating errors."" nature 323.6088 (1986): 533-536.","- Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere.",1.0,1.0,-1.0,1.0,nan,-1,"The feedback provided in the review is relevant to the paper, as it addresses specific aspects of the model described in Equation 3. However, the feedback lacks actionable suggestions that the author can use to improve their paper. While it identifies the need to mention the model's limitations, such as the single neuron and the restriction to non-negative functions, it does not provide clear guidance on how to incorporate these points into the text. The feedback could have been more helpful by offering specific recommendations or examples of how to address these issues. Therefore, the feedback is not fully actionable, and it does not provide the author with a clear path for revision. So the overall score is -1. 
[RESULT] -1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The feedback is delivered in a respectful manner, offering suggestions for improvement without being negative or aggressive. The feedback points are relevant to the review, and the tone is positive and encouraging. The feedback also respects the authors' work by acknowledging the model's limitations and suggesting that these should be explicitly stated in the text. This approach is likely to be well-received by the authors, as it provides clear guidance on how to improve their paper. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. While the feedback does mention aspects of the model, it fails to provide any evidence or references to support its claims. The feedback simply states that certain aspects should be mentioned or discussed, without providing any logical reasoning or references to the paper itself. This lack of evidence and support for the claims made in the feedback means that it does not meet the criteria for a score of 1. The feedback is more of a general observation rather than a verifiable claim, which is why it would receive a score of 0. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses two distinct aspects of the model described in the paper: the number of neurons and the type of functions it can model. The feedback is tailored to the content of the paper, and it is clear which sections of the paper are being addressed. The feedback is not a general comment for any scientific document, but rather a detailed critique of the model described in the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1526,ICLR_2023_3808,"Weaknesses: • The writing of the paper is at times confusing. For example, it is unclear of the significance of the \cap architecture. No detail is included in the paper. • The rationale of dividing the sequence of intermediate results into 11 sets is not provided. • No ablation studies are provided for \mu and \lambda, and the size of each set. In fact, their values are not provided in the paper (unless I missed them). • Using different sets and varying \phi appear to have a similar effect. However, there is no discussion on this issue. • It appears that there are two types of sets (Table 1 and 2), one for style and one for content. But in the paper, they are regarded to be the same, which is quite confusing. • There is insufficient discussion of the novelty of the proposed method, e.g. what the proposed method can do that other recently proposed style transfer methods cannot do? For example, in Fig. 6, the only discussion is one single sentence on p. 5 stating that the proposed method preserve the semantic structure of the content image and simulate the strokes of the style image from the style domain. Such a statement, which is quite generic, can be easily applied to other style transfer methods. • There is no quantitative comparison, e.g. style loss, user study, and efficiency, between the proposed method and other methods. • One issue not discussed is the importance of using the diffusion model. What can it provide to style transfer that no other methods can? • There are some occasional typos and grammatical mistakes.",• The rationale of dividing the sequence of intermediate results into 11 sets is not provided.,1.0,1.0,-1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies a problem with the sequence of intermediate results but fails to offer any specific, clear, or practical suggestions for improvement. The feedback does not provide any direction or hints on what needs to be done to address the issue. It simply states that the rationale for dividing the sequence into 11 sets is not provided, without offering any guidance on how to rectify this. This lack of actionable feedback makes it difficult for the author to use the review to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is neutral and factual, focusing on the lack of rationale in the sequence of intermediate results. It does not contain any negative or disrespectful language, nor does it attempt to encourage or discourage the authors. The feedback is strictly factual, pointing out the absence of a key aspect in the review. It does not engage with the authors or the content of the paper in any way other than stating the fact. Therefore, based on the score rubric, the feedback is neither positive nor negative, but rather neutral. It does not respect or ignore the aspect in question, but rather points out the absence of the aspect. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper. However, the feedback does not provide any evidence or logical reasoning to support the claim that the rationale for dividing the sequence of intermediate results into 11 sets is not provided. This lack of support makes the feedback less reliable and does not meet the criteria for verifiable feedback. The feedback also fails to cite any external sources or reference any parts of the paper, which further weakens its credibility. Therefore, the feedback does not meet the standards set by the score rubric, as it does not provide any evidence or logical reasoning to support its claims. So the overall score is -1. [RESULT] -1",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the division of the sequence of intermediate results into 11 sets. The feedback is tailored to the content of the paper, and it is clear which part of the document is being discussed. The lack of rationale for this division is a specific point that is not followed or respected in the review. Therefore, the feedback is relevant and specific, and it aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5012,NIPS_2020_1049,"- The experimental results are not significant. Two straightforward error rate indicators: the dimension of the space and LR error is chosen as baselines and the authors claim that the proposed bound is superior to these two baselines. However, from the experimental results, Pearson’s r correlation in Fig.4 shows that the proposed improvement is marginal. The results on image dataset from the supplementary material also shows that the Pearson’s r correlation is very close between the LR Error and MSE. - The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1. - The figures are not clear enough, e.g. what does each point stand for in Fig.1 and Fig.2. The authors could add the necessary information to make this paper more self-contained.",- The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1.,1.0,1.0,1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the paper, which is the choice of the number of parameters (k) in the experiments. However, the feedback does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely points out the lack of clarity in the paper regarding the effect of different values of k on the empirical conclusion. The feedback does not provide any guidance on how to address this issue, nor does it suggest any specific steps or edits that should be made. Therefore, the feedback lacks the actionability required to be helpful to the author. 

In conclusion, the feedback is relevant to the aspect of the paper, but it fails to provide actionable comments that would guide the author in addressing the issues identified. This makes the feedback less useful for the author, as it does not offer a clear path for revision. 

So the overall score is -1. [RESULT] -1",0,"The feedback provided is neutral and factual, focusing on the specific aspect of the review. It does not contain any negative or aggressive language, and it maintains a professional tone throughout. The feedback is also constructive, as it points out a potential issue with the experimental design and suggests that the authors consider the implications of varying the value of k. However, it does not explicitly encourage the authors or express any positive sentiment. Therefore, based on the score rubric, the feedback is neither disrespectful nor overly positive, and it is relevant to the review. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper - the choice of k in the experiments. However, the feedback does not provide any evidence or references to support the claim that the empirical conclusion may change if k becomes larger than 1. The feedback is based on a logical assumption, but it lacks the necessary verification or citation of external sources that would make it more credible. Therefore, while the feedback is relevant, it does not fully meet the criteria for a score of 1, as it does not provide the necessary support for its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the choice of k in the experiments. The feedback is tailored to the content of the paper, as it specifically points out the lack of clarity regarding the empirical conclusion when k is larger than 1. This shows that the feedback is not a general comment for any scientific document, but rather a detailed critique of the paper's methodology. The feedback is also easy to identify, as it directly refers to the figure in question, which is an important part of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2844,NIPS_2022_477,"Weaknesses:
1.In experiments, the PRODEN method also uses mixup and consistency training techniques for fair comparisons. What about other competitive baselines? I'd like to see how much the strong CC method could benefit from the representation training technique.
2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.
3.In App. B.2, a relaxed solution of Sinkhorn-Knopp algorithm is proposed. Why the relaxed problem guarantees to converge?Does Solar always run this relaxed version of Sinkhorn-Knopp?
4.How is gamma in the Sinknhorn-Knopp affect the performance?
5.How does the class distribution estimate for PRODEN in Figure 1?
Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment.
Limitations: The experiments need to be further improved.",5.How does the class distribution estimate for PRODEN in Figure 1? Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment. Limitations: The experiments need to be further improved.,-1.0,-1.0,-1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies a potential negative impact of the class distribution estimate but fails to offer any specific, clear, or practical suggestions for improvement. The feedback also mentions the need for further improvement in the experiments but does not provide any guidance on how to achieve this. The lack of actionable comments in the feedback makes it less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a score of 1. The feedback is constructive, offering a critique of the class distribution estimate and suggesting improvements for the experiments. It maintains a positive tone throughout, which is essential for encouraging the authors. The feedback also addresses the aspect of societal impacts, which is relevant to the review. It acknowledges the potential negative impact of lower annotation costs on employment, demonstrating an understanding of the broader implications of the research. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The claim about the negative impact of lower annotation costs is not substantiated with any data or analysis from the paper. Furthermore, the feedback does not reference any external sources or provide logical reasoning to support the claim. The feedback also fails to address the aspect of class distribution estimate for PRODEN in a meaningful way. The feedback merely states that the experiments need to be further improved without providing any specific details or evidence to support this claim. Therefore, the feedback does not meet the criteria for a score of 1 as it does not provide any evidence or references to support the claims made. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the class distribution estimate for PRODEN in Figure 1. The feedback also provides a critique of the societal impacts and limitations of the paper, which are directly related to the content of the draft. The feedback is tailored to the content of the paper, and it is clear which sections, statements, or elements in the paper are being addressed. Therefore, the feedback is specific and directly relevant to the content of the draft, and it is easy to identify which part of the document is being discussed in the feedback. So the score is 1. [RESULT] 1"
1317,ICLR_2023_3854,"Weakness:
1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.
2.The novelty of the paper is limited as some parts seems like a direct combination of TD3 and SAC, but the paper does not provide detailed insight of why SAC works in complex envs while TD3 works in simple envs.
3.Theorem 2 assumes that Yi follows a uniform distribution, which might be too strong in practice.
4.The experimental results are not quite convincing. TD3 fails to learn in Humanoid-v2, can you explain the reason?","1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.",1.0,1.0,1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the paper's comparison with other methods. However, it falls short in offering actionable suggestions for the author to improve their paper. The feedback points out that the paper does not compare the methods in terms of both estimation bias and performance, but it does not provide any specific guidance on how to address this issue. The feedback lacks the necessary detail and practical steps that would help the author to revise their paper effectively. Therefore, the feedback is not as useful as it could be, and it does not fully meet the criteria for actionability. So the score is -1. [RESULT] -1",0,"The feedback provided is neutral and factual, focusing on the content of the paper without any negative or positive connotations. It points out that the paper claims to reduce the overestimation bias compared to TD3, but it does not compare these methods in terms of both estimation bias and performance. This is a factual observation and does not reflect any disrespect or aggression towards the authors or the paper. The feedback is constructive, as it highlights an area where the paper could be improved. It does not encourage or discourage the authors, but rather provides a critique that could be used to enhance the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 0. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not support its claims with any evidence or references. It merely states that the paper does not compare the methods in terms of both estimation bias and performance, without providing any external sources or logical reasoning to back up this claim. The feedback also fails to cite any parts of the paper that could be used to support its assertion. Therefore, the feedback does not meet the requirements of the score rubric, which emphasizes the importance of verifiable feedback. The feedback does not provide any evidence or references to support its claims, which is a key aspect of the score rubric. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the claim made in the paper regarding the reduction of overestimation bias and underestimation bias compared to TD3. The feedback also points out that the paper does not compare these methods in terms of both estimation bias and performance, which is a critical aspect of the paper's claim. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's requirement for specificity and relevance. 
[RESULT] 1"
1875,ARR_2022_2_review,"-	The authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. 
-	Given that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? 
-	It would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. 
-	It was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. 
-	In addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. 
-	In Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. 
-	In Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? 
-	In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? 
-	The fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). 
-	In Section 4.3.3, how do you deal with the RoBERTa token limit? 
-	In Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. 
-	In Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. 
-	It would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ","- In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold?",-1.0,1.0,0.0,0.0,nan,-1,"The feedback provided in the review is not actionable. It identifies a potential issue with the ""top scoring"" sentences but fails to offer any specific guidance or recommendations on how to address this issue. The feedback does not provide any direction or hints on what needs to be done, which is crucial for the author to improve their paper. It merely points out the ambiguity without offering any concrete steps or recommendations. Therefore, the feedback does not meet the criteria of being actionable, and it lacks the necessary detail to be helpful to the author. So the score is -1. 
[RESULT] -1",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on a specific aspect of the paper, asking for clarification on the ""top scoring"" sentences. This is a constructive approach to reviewing, as it seeks to understand the author's intent and to ensure that the paper's content is accurately represented. However, the feedback does not explicitly express respect or positivity towards the authors or their work, which is a key component of the score rubric. Therefore, while the feedback is relevant and respectful, it does not fully meet the criteria for a score of 1. 
[RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper, which is the scoring system. However, the feedback does not provide any evidence or references to support the claim that the ""top scoring"" sentences are unclear. The feedback also lacks logical reasoning that could be used to validate the claim. It simply states that the aspect is unclear without offering any solution or clarification. Therefore, the feedback does not meet the criteria for verifiable feedback as it does not provide any evidence or logical reasoning to support the claim. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the scoring system for sentences. The feedback is tailored to the content of the paper, as it specifically asks about the criteria for selecting the ""top scoring"" sentences. This shows that the reviewer has carefully considered the details of the paper and has provided feedback that is directly related to the content. The feedback is not a general comment for any scientific document, but rather a targeted critique of a specific part of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5730,NIPS_2018_857,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo?  [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018.  ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.",- The writing should be improved. Some points in the paper is unclear to me.,-1.0,-1.0,-1.0,-1.0,nan,-1,"The feedback provided in the response is vague and lacks specificity. It does not offer any concrete steps or recommendations that the author can use to improve their paper. The feedback merely states that the writing should be improved and that some points in the paper are unclear, without providing any actionable advice or guidance. This lack of actionability makes the feedback less useful to the author, as it does not help them understand what needs to be done to improve their paper. Therefore, based on the score rubric, the feedback is not actionable and does not respect the aspect of providing clear and practical suggestions for improvement. So the score is -1. [RESULT] -1",0,"The feedback provided is quite vague and does not offer specific guidance or constructive criticism. It lacks the necessary detail to be helpful to the authors. The tone of the feedback is neutral, which is not necessarily negative or positive. However, it does not convey a sense of respect or encouragement, which is crucial in a review context. The feedback does not address the aspect of politeness or the manner in which feedback is delivered, which is a key component of the score rubric. Therefore, the feedback does not meet the criteria for a score of 1, as it does not demonstrate a polite, helpful, or positive tone. It also does not show a clear understanding of the aspect being reviewed. So the overall score is 0. [RESULT] 0",-1,"The feedback provided does not meet the criteria for a score of 1. The feedback is vague and does not provide any specific evidence or references to support the claim that the writing needs improvement. There is no mention of any particular points in the paper that are unclear, nor is there any logical reasoning provided to substantiate the claim. The feedback does not follow the aspect of verifiability, as it does not offer any means to verify the reviewer's claims. Therefore, the feedback does not respect the aspect of verifiability, and it is not relevant to the review. So the score is -1. [RESULT] -1",-1,"The feedback provided is vague and lacks specificity. It does not mention any particular points or sections in the paper that are unclear, nor does it provide any concrete examples or suggestions for improvement. This lack of detail and specificity makes it difficult to identify which parts of the document are being addressed in the feedback. The feedback does not meet the criteria for specificity as outlined in the score rubric. Therefore, the feedback does not respect the aspect of specificity in a review. So the score is -1. [RESULT] -1"
229,ICLR_2022_410,"Weaknesses:
. Theorem 4.2: For the downstream classification, the loss is upper and lower bounded in terms of the L_NCE loss. The authors provided comparison with Saunshi et al. (2019) from the technical perspective. Is there any intuitive explanation on how to evaluate the classification performance in terms of contrastive learning (loss)?
. Assumption 4.5 (intra-class connectivity): This assumption is strong. Without the label information, it seems impossible to derive such augmentation set. Please add discussion on the practicality of this assumption, and show an example on some datasets if possible.
. Proposition 4.7: Based on the proof provided in the appendix the conclusion not only relies on the existence of such augmentation set (Assumption 4.5), but also that such augmentation should be applied to intra-class samples, ie, t_i(x_i) = t_j(x_j). This kind of operation is impractical without the label information. Please add comment on that.
. In the experiments, RandomResizedCrop is used to illustrate the relationship between Aug Strength and ACC(ARC). The best performance for different datasets all achieves at Aug Strength = 0.92. Any comments on that? eg., in terms of data augmentation for intra-class samples at Aug Strength = 0.92?
. In practice, there are different kinds of data augmentation, eg, flipping, rotation, and scaling. The authors only showed results on RandomResizedCrop. Can you show results for other data augmentation types? Do you have similar conclusion as that for RandomResizedCrop?
. Different data augmentation types are often used together in practice (eg, randomly pick two augmentations from the augmentation set for the raw image). Then how to apply the proposed analysis in such practical case? In particular, how to measure the Aug Strength?
. The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping). 1) The study on applying the analysis to existing contrastive learning algorithms is, however, preliminary (only with RandomResizedCrop). 2) Based on the proposed analysis how to find the sweet spot of data augmentation for contrastive learning is crucial, but this is not discussed.",". The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping).",0.0,0.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the authors emphasized the importance of the data augmentation design for intra-class samples. However, it does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. It lacks in giving a direction or hints of what needs to be done. The feedback does not provide the author with a clear path for revision, which is essential for a review to be useful. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is respectful and constructive, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback acknowledges the authors' emphasis on the data augmentation design for intra-class samples, indicating that the aspect is relevant to the review. Furthermore, the feedback does not ignore or disrespect the aspect, but rather respects it by highlighting its importance. The feedback is also positive, as it appreciates the authors' focus on this aspect. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected in the review. The authors' emphasis on the importance of the data augmentation design for intra-class samples is clearly stated and supported by the claim that the data augmentation design is crucial for perfect overlapping. This claim is not only relevant to the review but also substantiated by the authors' focus on this aspect. The feedback does not ignore or disrespect the aspect, but rather acknowledges and validates it. Therefore, the feedback aligns with the score rubric's criteria for a score of 1, as it imposes a certain claim and then supports it using internal references from the paper. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the data augmentation design for intra-class samples. The feedback is tailored to the content of the paper, and it is clear that the authors' emphasis on this aspect is being discussed. The feedback is not a general comment for any scientific document, but rather a focused critique on a specific design decision in the paper. Therefore, the feedback meets the criteria of specificity and relevance to the content of the draft. So the score is 1. [RESULT] 1"
2403,NIPS_2021_1571,"How do you validate the estimated uncertainty maps, in the absence of GTs for the same ?
Wonder what is the expansion of the acronym ""D_CBase"" used ? What does it mean ?
What is MAE in fig. 4 ? Not defined anywhere in main doc. - got it in Supple though.
Fig. 9 of Supple material - how do you validate these complexity scores ? Even the GT or human evaluation is not provided. What are the typical ranges of complexity values you get by the entropy based measure ? How do you thus claim it to be consistent with human visual perception ?
Some typos: line 169: ......local minimum, it solve the following estimating.... - change to : it solves the....
Some more references may be added as:
Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011;
Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.","- change to : it solves the.... Some more references may be added as: Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011; Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.",1.0,-1.0,0.0,0.0,nan,0,"The feedback provided in the response is not actionable. It merely suggests adding more references without providing any specific guidance or recommendations on how to improve the paper. The feedback does not offer any concrete steps or suggestions that the author can use to address the issues identified. It lacks the necessary detail and direction that would make it useful for the author to follow. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is not only relevant to the review but also shows a high level of respect and constructiveness. The tone of the feedback is polite and helpful, as it suggests additional references to support the paper's claims. This approach is likely to encourage the authors to improve their work by providing them with valuable resources. The feedback is also positive, as it acknowledges the paper's potential to solve the problem at hand. The manner in which the feedback is delivered is constructive and respectful, which aligns with the score rubric's criteria for a score of 1. Therefore, based on the score rubric, the feedback is of high quality and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not support the claim made about the paper's relevance to Bayesian Learning via Stochastic Gradient Langevin Dynamics. While the feedback does mention external sources, it does not provide any evidence or logical reasoning to support the claim that the paper solves the problem of Bayesian Learning. The feedback merely lists external sources without explaining how they relate to the paper's content or how they support the claim made. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claim made about the paper. 
[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the draft. It does not address any particular point or design decision in the paper. Instead, it offers a general suggestion to add more references, without specifying which parts of the paper these references should support or enhance. This lack of specificity makes it difficult to identify which parts of the document are being discussed in the feedback. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 

The feedback does not mention any specific sections, statements, or elements in the paper, which is a key requirement for a score of 1. Instead, it provides a vague recommendation that could apply to any scientific document. This lack of specificity is why the feedback does not meet the criteria for a score of 1. 

In conclusion, the feedback provided in the response is not specific to the content of the draft, and it does not address any particular point or design decision in the paper. This lack of specificity is why the feedback does not meet the criteria for a score of 1. 
[RESULT] 0"
3498,NIPS_2020_944,"1. Weighted retraining is not new. The cross-entropy method (De Boer et al., 2005; Neil et al., 2018) maximizes the expectation E_p(x)[f(x)] of the objective function f(x) when sampling from a policy p(x) by periodically retraining p(x) on the samples with the highest reward, e.g. those with a reward above a quantile cutoff (i.e. using a stepwise weighting function). Instantiations of the cross-entropy method include DbAs (Brooks et al) and FBGAN (Gupta et al). Reward weighted regression (RWR) (Hachiya et al) is another existing optimization technique that employs weighted retraining. Angermueller et al. (http://arxiv.org/abs/2006.03227) recently employed these techniques as baselines for high-dimensional discrete optimization. 2. The described rank-based weighting function is not new. See RankGAN (Lin et al. 2017) or LeakGAN (Guo et al. 2017) for an example. 3. The evaluation is missing important baselines such a DbAs, FBGAN, RWR, and model-based optimization. 4. Chemical design task: It is unclear how the optimization trajectory of ‘original’ was obtained. How were new data points sampled from JT-VAE? Why does the trajectory stop at 250? 5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning. 6. What do error bars represent? How often were experiments repeated with different random seeds?","5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning.",1.0,1.0,0.0,0.0,nan,1,"The feedback provided in the review is actionable and relevant to the paper. It suggests a comparison with GCPN and reinforcement learning, which are specific and practical suggestions that the author can use to improve their paper. The feedback goes beyond identifying issues and offers concrete steps or recommendations that guide the author in addressing those issues. The inclusion of these aspects in the feedback enhances the usefulness of the review by providing the author with a clear path for revision. Therefore, the feedback is aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The tone is respectful and the feedback is helpful, as it suggests additional comparisons that could enhance the review. The feedback also encourages the authors to consider these additional aspects, which is a positive approach to reviewing. The feedback is not aggressive or disrespectful, and it maintains a positive and encouraging tone throughout. Therefore, based on the score rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it suggests a comparison with GCPN and reinforcement learning, which are indeed related to the topic of the paper. However, the feedback does not provide any evidence or references to support these claims. It is a logical claim that these additional methods could be compared to the JT-VAE, but without any supporting references, it falls short of the score rubric's requirement for verifiable feedback. Therefore, the feedback is not fully supported by references or logical reasoning, which is why it does not meet the criteria for a score of 1. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It requests a comparison with GCPN and reinforcement learning, which are specific methods and models in the field of machine learning. This shows that the reviewer is addressing specific parts of the paper, which is in line with the score rubric's requirement for specificity. The feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is relevant and respects the aspect of the review. So the score is 1. 
[RESULT] 1"
5083,NIPS_2020_734,"- Main math section/derivation could be improved for clarity: it's a very simple idea but it is presented in an obfuscated way that requires a lot of energy from the reader to go through. - It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper. - The gains obtained by the proposed method modification are not that large.","- It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper.",-1.0,1.0,-1.0,-1.0,nan,0,"The feedback provided in the review is not actionable. It merely states that Section 6 does not belong in the paper and suggests that the extra space could be used for extending other parts. However, it fails to provide any specific, clear, or practical suggestions on how to achieve this. The feedback does not offer any concrete steps or recommendations that would guide the author in addressing the issues raised. It lacks the necessary detail and direction that would make the feedback useful for the author's revision process. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is not aligned with the score rubric's criteria for politeness and respect. The tone of the feedback is negative and critical, which is not conducive to a respectful and constructive review. The feedback suggests that Section 6 is unnecessary and should be removed, which is a direct criticism of the authors' work. This approach does not encourage the authors or provide any positive feedback. The feedback also lacks any constructive suggestions for improvement, which is a key aspect of a helpful review. Therefore, based on the score rubric, the feedback is not respectful or constructive, and it does not encourage the authors. So the overall score is -1. 
[RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that Section 6 does not belong in the paper is not supported by any evidence or references. There is no mention of how this claim was reached, nor is there any logical reasoning provided to support the claim. The feedback does not cite any external sources or use internal references from the paper to substantiate the claim. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any evidence or logical reasoning to support the claim. 

In contrast, a score of 1 would require the feedback to provide evidence or logical reasoning to support the claim that Section 6 does not belong in the paper. This could be done by citing external sources or using internal references from the paper to support the claim. Without this, the feedback does not meet the criteria for a score of 1. 

So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. It mentions Section 6 but fails to provide any details about what is wrong with this section or how it could be improved. The feedback also suggests that the extra space could be used to extend other parts of the paper, but it does not specify which parts or how the extension could be beneficial. This lack of specificity makes it difficult to identify which parts of the document are being addressed in the feedback. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
3452,NIPS_2020_1505,"Even though the paper reads well in its current form, I found during the first reading that the presentation of the conceptual messages (which are the most important part in my opinion) were overshadowed by the algorithmic comparison between GOLEM and NOTEARS. The paper is dense and it is not necessarily clear what the take-away messages of the paper are. - I would encourage the authors to elaborate more on the differences between the findings their findings that are related to algorithmic implementations and the findings that pertain to the asymptotic of loss functions. - I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.",- I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.,1.0,1.0,1.0,1.0,nan,-1,"The feedback provided in the review is indeed relevant to the aspect of transforming a constraint objective into a penalized objective. However, the feedback lacks specificity and practicality. It does not offer clear steps or recommendations that the author can use to improve their paper. The feedback merely suggests discussing the topic in more detail and including a paragraph on the limitations of the study, without providing any concrete guidance on how to achieve these improvements. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. It fails to provide the author with a clear path for revision, which is essential for a review to be useful. So the overall score is -1. [RESULT] -1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the importance of the aspect in question and provides a detailed critique, suggesting improvements and additional content. The feedback is delivered in a manner that is encouraging and supportive, which is essential for maintaining a positive and respectful tone. The reviewer also addresses the limitations of the study, which is a crucial aspect of any scientific paper. This demonstrates a thorough understanding of the review process and the importance of providing a balanced critique. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which is why it would receive a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of transforming a constraint objective into a penalized objective. The response also correctly identifies the issue with the penalty parameter asymptotically vanishing for the MLE. However, the feedback lacks the necessary verifiable evidence to support the claims made. The response does not cite any external sources or provide logical reasoning that could be independently verified. It merely states the issues without providing any evidence or references to support the claims. Therefore, the feedback does not meet the criteria for a score of 1 as it does not provide any evidence or references to support the claims made. So the overall score is 0. [RESULT] 0",1,"The feedback provided is quite specific and directly addresses the content of the paper. It points out the importance of discussing the transformation of a constraint objective into a penalized objective and the implications of the penalty parameter. The feedback also suggests the inclusion of a paragraph on the limitations of the study, which is a direct and relevant comment on the paper's content. The specificity of the feedback ensures that the reviewer's comments are tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4673,NIPS_2020_1254,"- The theorem may slightly overstate its result in the following way: it seems that in order for this correspondence between adversarial training and the proposed regularization scheme to hold, \epsilon must be quite small. That is, we are assuming here that all of the points in an \epsilon ball around some data point x are mapped by the model to the same activation pattern \phi_x (i.e. that B_\epsilon^p(x) \subset X(\phi_x)). I would imagine that this may not hold for ""realistic"" values of \epsilon (e.g. 8/255) all the time. Indeed, my concern is that while this theorem is certainly compelling, it may be the case that it only holds for \epsilon so small that it may not hold in practice. Perhaps the authors can clarify here. I see there is an experiment to this effect in Section 7.16, but this seems to be for only one data point. [EDIT: post-rebuttal] Based on the authors response and a closer look at Section 5.4, I'm satisfied that the authors looked into this potential weakness and were able to add explanation as to its implications.] - Figure 1 is too small to really be useful. It's not really clear what the arrows represent. A more detailed and larger figure here would be appreciated. - The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.","- The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is not actionable. It merely suggests a rearrangement of steps without providing any specific guidance or clear instructions on how to implement this change. The feedback does not offer concrete steps or recommendations that would help the author in addressing the issue. It lacks the necessary detail and clarity that would make it actionable. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The reviewer offers a suggestion to improve the clarity of the power iteration notation, which is a helpful and positive approach to feedback. The tone is respectful, and the feedback is aimed at encouraging the authors to improve their work. The reviewer's language is polite and the feedback is delivered in a manner that is likely to be well-received by the authors. The feedback is focused on a specific aspect of the paper, which is the notation used in the power iteration, and it provides a clear and constructive suggestion for improvement. Therefore, based on the score rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The reviewer suggests rearranging the steps in the power iteration notation, but fails to provide any evidence or logical reasoning to support this claim. There are no references to external sources or parts of the paper that could be used to verify the reviewer's suggestion. The feedback is based on a personal opinion and lacks the necessary support to be considered a valid critique. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any evidence or logical reasoning to support the claims made. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the notation used in the power iteration. The feedback suggests a rearrangement of the steps in the notation, which is a clear and specific comment. It directly points out the issue with the notation and provides a solution, making it easy to identify which part of the document is being discussed in the feedback. The feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria of the score rubric, as it is specific and directly relevant to the content of the draft. So the score is 1. [RESULT] 1"
1943,ARR_2022_247_review,"- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text. - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).
- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.
- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. 
Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).
- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.
- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").
- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).
- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""
- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?
Typos: - 166: Space after footnote 5.
- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""? ","- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is relevant to the aspect of intent definitions in the paper. However, it falls short in offering actionable suggestions for the author to improve their paper. The feedback points out that some intents do not conform to standard definitions and that others may be overloaded. While these observations are valid, they do not provide the author with clear guidance on how to address these issues. The feedback lacks specific recommendations or steps that the author can take to improve their paper. It merely states the problems without offering a solution or direction for revision. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer offers specific examples and suggestions for improvement, which demonstrates a positive and encouraging approach. The feedback is also factual and does not contain any negative or aggressive language. The reviewer's tone is consistently respectful and constructive throughout the feedback, which is why the score is 1. 
[RESULT] 1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiability. The feedback makes claims about the paper's use of intents, such as ""card"" and ""less_lower_before"", without providing any evidence or references to support these claims. The feedback also suggests that the authors could articulate their reasoning more explicitly, but again, it does not provide any evidence or references to support this suggestion. The feedback is based on the reviewer's personal interpretation of the paper, which is not verifiable or supported by any external sources or logical reasoning. Therefore, the feedback does not meet the criteria for verifiability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the aspect of intent definitions in the paper, providing clear examples of where the definitions may not align with standard expectations. The feedback also offers constructive suggestions for improvement, such as clarifying the reasoning behind overloading or over-specifying intents. This shows a deep understanding of the paper's content and a thoughtful approach to the review. The feedback is tailored to the specific elements of the paper, making it easy to identify which parts are being discussed. Therefore, the feedback meets the criteria for a score of 1, as it is specific, directly relevant to the content of the draft, and addresses a certain point or design decision in the paper. 
[RESULT] 1"
5689,NIPS_2018_680,"weaknesses are: 1) The statistical aspects are a bit eluded: is the given algorithm robust to statistical imprecision? 2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?","2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?",0.0,1.0,0.0,0.0,nan,1,"The feedback provided in the review is actionable and directly addresses the aspect of algorithm runtime. It suggests the exploration of other heuristics or the improvement of the current one, which is a clear and practical recommendation for the author. The feedback is specific and directly related to the issue at hand, which is the slow runtime of the algorithm. It also hints at the possibility of special cases where the current algorithm might be improved, which adds an extra layer of guidance for the author. The feedback is not only relevant but also respects the aspect by providing a direction for improvement. Therefore, the feedback is actionable and aligns well with the score rubric. 
[RESULT] 1",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone is respectful and the feedback is aimed at encouraging the authors to consider alternative heuristics or improvements to the algorithm. The feedback is also positive, as it acknowledges the potential of the algorithm while suggesting areas for improvement. The feedback is not aggressive or disrespectful, and it maintains a helpful and positive tone throughout. Therefore, based on the score rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and addresses the aspect of algorithm runtime. It suggests the possibility of faster heuristics or improvements in special cases, which is a logical claim based on the information provided in the paper. However, the feedback does not provide any external references or internal references from the paper to support the claim. It also lacks a detailed explanation of why the suggested improvements could be beneficial or how they could be implemented. Therefore, while the feedback is relevant and logical, it does not fully meet the criteria for verifiable feedback as it does not provide any evidence to support the claims made. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the algorithm runtime in N^6. The feedback also suggests the possibility of alternative heuristics or improvements in special cases, which shows a deeper understanding of the topic and a more nuanced critique. This specificity and relevance to the content of the paper make it clear that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria for a score of 1 according to the score rubric. 
[RESULT] 1"
5198,NIPS_2020_1687,"- Since the proposed optimization is non-convex, there is no guarantee for the correctness of the bounds. - Perhaps the most important missing result in this work is confidence intervals for the bounds. - At some parts, for instance the choice of function family for p_\eta, it seems that the only criteria for the choices in the model is to make the optimization task efficient and no other justification is provided. - Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose. - In the experiments, only two cases are considered: linear Gaussian case and a second case in which the treatment is again linear and the outcome is generated by 0.3X^2−1.5XC+e. It seems necessary to consider other instances of non-linear cases as well. - The choice of the outcome equations (X-6C+e and 0.3X^2−1.5XC+e) look random. Was there any specific reason for this choice?","- Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose.",0.0,-1.0,0.0,0.0,nan,-1,"The feedback provided in the response is somewhat vague and lacks specificity. While the reviewer does express a concern about the choice of response functions and the resulting bounds, they do not offer any concrete suggestions or steps that the author could take to address these issues. The feedback is more of a general commentary rather than actionable advice. It does not provide the author with a clear path for revision, which is essential for improving the paper. Therefore, based on the score rubric, the feedback is not actionable and does not respect the aspect of providing specific, clear, and practical suggestions for improvement. So the score is -1. [RESULT] -1",1,"The response provided is a direct question to the authors, which is a polite and respectful way to engage in a discussion about the choice of response functions. It shows an understanding of the topic and a willingness to engage in a constructive dialogue. The tone is positive and encouraging, which aligns with the score rubric's criteria for a score of 1. The feedback is not only relevant to the review but also respects the aspect of the review, which is the choice of response functions. Therefore, the response is well-aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that MLP should be a good choice, nor does it provide any logical reasoning to validate the claim. Instead, the feedback merely states a concern about the loose bounds of the MLP model without offering any solution or further analysis. This lack of evidence and logical reasoning makes the feedback less relevant and less respectful of the aspect in question. Therefore, the feedback does not meet the criteria for a score of 1, as it fails to provide any verifiable claims or suggestions. 
[RESULT] 0",-1,"The feedback provided in the response is not specific to the content of the draft. It does not mention any specific parts or methods in the paper, making it difficult to identify which sections or elements in the paper are being addressed. The feedback is more of a general comment rather than a tailored critique. It lacks the specificity required to effectively critique the paper. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 

The feedback also fails to respect the aspect of the review, which is to provide specific and relevant comments on the paper. Instead, it offers a vague and unspecific critique. This lack of specificity and respect for the aspect of the review is why the score is -1. 

So the overall score is -1. [RESULT] -1"
2019,ARR_2022_292_review,"1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior. ","1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others.",1.0,1.0,-1.0,0.0,nan,0,"The feedback provided in the review is relevant to the aspect of interpretability. However, it falls short in offering actionable suggestions. The feedback merely points out the lack of discussion on the semantic encoder's understanding of semantics, without providing any concrete steps or recommendations for the author to improve their paper. It does not guide the author on how to address the issue of interpretability, which is crucial for the review. Therefore, the feedback lacks the necessary actionability that would make it useful for the author. 

In conclusion, the feedback is relevant to the aspect of interpretability, but it does not provide the author with a clear path for revision. It fails to offer specific, clear, and practical suggestions that the author can use to improve their paper. Hence, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on the lack of interpretability in the paper, which is a relevant aspect to the review. The feedback is constructive, pointing out that the paper could benefit from a more detailed discussion of the semantic encoder's function. However, it does not go beyond this to provide positive reinforcement or to express respect for the authors' work. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a higher score. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the paper lacks interpretability. It merely states that the paper could be discussed in more detail without providing any logical reasoning or references to the paper itself. This lack of evidence and logical reasoning makes the feedback less reliable and does not meet the standards of a score of 1. The feedback also fails to provide any references to external sources, which would have strengthened the claim. Therefore, the feedback does not meet the criteria for a score of 1, as it does not support its claims with evidence or logical reasoning. 
[RESULT] 0",0,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer mentions a lack of interpretability but fails to identify specific parts or methods in the paper that are being addressed. The feedback is vague and does not provide clear guidance on how the paper could be improved. It lacks the specificity required to effectively critique the paper. The reviewer should have provided more detailed comments that directly relate to the content of the draft. This would have made it clear which sections, statements, or elements in the paper are being addressed. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
5808,NIPS_2018_630,"Weaknesses: - While there is not much related work, I am wondering whether more experimental comparisons would be appropriate, e.g. with min-max networks, or Dugas et al., at least on some dataset where such models can express the desired constraints. - The technical delta from monotonic models (existing) to monotonic and convex/concave seems rather small, but sufficient and valuable, in my opinion. - The explanation of lattice models (S4) is fairly opaque for readers unfamiliar with such models. - The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions: - Lattice models expect the input to be scaled in [0, 1]. If this is done at training time using the min/max from the training set, then some test set samples might be clipped, right? Are the constraints affected in such situations? Does convexity hold? - I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize? - Why is this paper categorized under Fairness/Accountability/Transparency, am I missing something? - The SCNN getting ""lucky"" on domain pricing is suspicious given your hyperparameter tuning. Are the chosen hyperparameters ever at the end of the searched range? The distance to the next best model is suspiciously large there. Presentation suggestions: - The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned. - ""We have found it easier to be confident about applying ceterus paribus convexity;"" -- the word ""confident"" threw me off a little here, as I was not sure if this is about model confidence or human interpretability. I suspect the latter, but some slight rephrasing would be great. - Unless I missed something, unconstrained neural nets are still often the best model on half of the tasks. After thinking about it, this is not surprising. It would be nice to guide the readers toward acknowledging this. - Notation: the x[d] notation is used in eqn 1 before being defined on line 133. - line 176: ""corresponds"" should be ""corresponding"" (or alternatively, replace ""GAMs, with the"" -> ""GAMs; the"") - line 216: ""was not separately run"" -> ""it was not separately run"" - line 217: ""a human can summarize the machine learned as"": not sure what this means, possibly ""a human can summarize what the machine (has) learned as""? or ""a human can summarize the machine-learned model as""? Consider rephrasing. - line 274, 279: write out ""standard deviation"" instead of ""std dev"" - line 281: write out ""diminishing returns"" - ""Result Scoring"" strikes me as a bit too vague for a section heading, it could be perceived to be about your experiment result. Is there a more specific name for this task, maybe ""query relevance scoring"" or something? === I have read your feedback. Thank you for addressing my observations; moving appendix D to the main seems like a good idea. I am not changing my score. ","- The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions:",1.0,1.0,1.0,1.0,nan,0,"The feedback provided in the review is not actionable. It does not offer specific, clear, and practical suggestions that the author can use to improve their paper. The feedback merely states that the SCNN architecture is terse and lacks explanation, comparison to ICNN, and a figure. It also mentions that the model is not obvious to be convex and monotonic. However, it does not provide any guidance on how to address these issues. The feedback does not suggest any concrete steps or recommendations that would help the author in revising their paper. Therefore, the feedback does not meet the criteria of being actionable. 
[RESULT] 0",1,"The feedback provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses a desire for more information and comparison to other architectures, which is a positive and respectful approach to providing feedback. The request for a figure and more explanation on the model's properties also demonstrates a helpful and positive tone. The feedback is not aggressive or disrespectful, and it encourages the authors to provide more details, which is a positive aspect of the feedback. Therefore, based on the score rubric, the feedback is relevant to the review and is delivered in a manner that is polite, helpful, and positive. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer's request for more explanation and comparison to ICNN is not supported by any references to the paper or external sources. The feedback also lacks logical reasoning that could be independently verified. The feedback is more of a personal preference rather than a critique based on the paper's content. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or references to support the claims made. 
[RESULT] 0",-1,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer's comments are vague and do not directly address any specific parts or methods in the paper. The feedback does not mention any particular sections, statements, or elements in the paper, which makes it difficult to identify the parts that are being addressed in the document. The reviewer's request for more explanation and comparison to ICNN, as well as a figure, is not specific enough to be actionable. The feedback also fails to provide any concrete suggestions for improvement. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. So the overall score is -1. [RESULT] -1"
589,ICLR_2022_3053,"Weakness:
It’s difficult to identify the real novelty/contribution of the proposed method. The core techniques such as importance score-based pruning and full model distillation (essentially self-distillation) have been explored in the network pruning literature.
The rules designed for parameter redistribution are draw from the observations in Fig. 4. However, there is no ablation study to validate its effectiveness as compared to other design rules.
The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction.
Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters).
There is a lack of comparison with other pruning methods.
Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).","4. However, there is no ablation study to validate its effectiveness as compared to other design rules. The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction. Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters). There is a lack of comparison with other pruning methods. Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).",1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It identifies the lack of an ablation study and the insufficient comparison with other pruning methods, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback merely states the issues without providing any guidance on how to address them. It lacks the necessary direction or hints that would help the author in revising their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is critical and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is respectful and it encourages the authors to improve their work by suggesting specific areas for enhancement, such as conducting an ablation study and comparing the proposed method with other pruning methods. The feedback also acknowledges the authors' efforts by mentioning the small performance gains achieved on the tiny model. This approach is likely to be well-received by the authors, as it provides clear guidance on how to improve their paper. The feedback is also factual and does not contain any negative or aggressive language, which is in line with the score rubric's criteria for a score of 1. Therefore, based on the score rubric, the feedback is relevant, respectful, and constructive, which makes it a high-quality review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of the paper's effectiveness in terms of model compression and latency reduction. The reviewer has made a claim about the effectiveness of the proposed reshaped attention block and has compared it to other design rules and pruning methods. However, the feedback lacks evidence to support the claims made. The reviewer has not provided any references to external sources or logical reasoning that could be independently verified. The feedback is based on the reviewer's opinion and does not offer any concrete evidence to back up the claims. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. The feedback is not supported by references to external sources or logical reasoning, and it does not provide any evidence to support the claims made. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is highly specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the effectiveness of the proposed reshaped attention block for pruning. The feedback also compares the proposed method with other design rules and models, such as the DEIT model family and the NViT model. This comparison is crucial for understanding the relevance and impact of the proposed method. Furthermore, the feedback suggests that an ablation study should be conducted to verify the effectiveness of the proposed method, which is a direct and specific critique of the paper. The feedback is tailored to the content of the paper, and it is clear that the reviewer has carefully considered the specifics of the paper and provided a detailed and relevant critique. Therefore, based on the score rubric, the feedback is highly specific and directly relevant to the content of the draft, and it addresses a certain point or design decision in the paper. So the score is 1. 
[RESULT] 1"
2404,NIPS_2021_655,"weakness about their results in the final section. However, I have some concerns on the paper. First, the related works might have been cited adequately, but the authors could have done better comparing with them. Second, there is a concurrent paper that covers many of the results in this paper. I vote for a weak reject.
The concurrent paper shall not be a concern. After discussion I would raise my vote to accept. Limitations
The recent paper [1] studies the more general tensor bandit, where the reward function can be more general low-rank p-tensors. In particular, the p = 2
case in [1] seems cover the main contributions (Theorem 1-3) in this paper. Besides, Algs. 1 and 2 in [1] look quite similar to Alg. 1 in the submission, but the ones in [1] are much more general. For example, the ones in [1] can solve general ⟨ θ ⋆ , a t ⟩ p
reward, while the submission only studies ⟨ θ ⋆ , a t ⟩ 2
. The authors may need to compare their results with those in [1].
The lower bound (Theorem 2) seems to be the same as [2, Theorem 6] by putting r = 1
. The authors might need to point it out more explicitly, and it would be strange to list it as a contribution.
Other Comments
Line 72, what is x x ?
At Line 125, the idea of using curvature to obtain sqrt regret for ETC could date back to even earlier papers, such as [3].
Societal Impact
The paper is mainly theoretical, and I don't see any potential negative societal impact. Bibliography
[1] Huang, Baihe, Kaixuan Huang, Sham M. Kakade, Jason D. Lee, Qi Lei, Runzhe Wang, and Jiaqi Yang. ""Optimal Gradient-based Algorithms for Non-concave Bandit Optimization."" arXiv preprint arXiv:2107.04518 (2021).
[2] Lu, Yangyi, Amirhossein Meisami, and Ambuj Tewari. ""Low-rank generalized linear bandit problems."" In International Conference on Artificial Intelligence and Statistics, pp. 460-468. PMLR, 2021.
[3] Rusmevichientong, Paat, and John N. Tsitsiklis. ""Linearly parameterized bandits."" Mathematics of Operations Research 35, no. 2 (2010): 395-411.",1 and 2 in [1] look quite similar to Alg.,0.0,0.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the first and second algorithms in [1] look similar to Alg., without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issue, nor does it provide any concrete steps or recommendations. It lacks the necessary detail and direction that would make it useful for the author to revise their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is quite brief and lacks the necessary detail to fully assess the tone and manner of the feedback. It does not offer any specific critique or praise, nor does it provide any constructive advice or encouragement to the authors. The feedback is also not respectful or positive, as it merely states that two aspects of the paper are similar without any further explanation or context. This lack of detail and respectfulness in the feedback does not align with the score rubric's criteria for a score of 1, which requires the feedback to be polite, helpful, positive, respectful, or encouraging. Therefore, based on the score rubric, the feedback provided in the response is not relevant to the review and does not respect the aspect being discussed. So the score is -1. 
[RESULT] -1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that ""1 and 2 in [1] look quite similar to Alg."" is made without any supporting evidence or references. There is no mention of any external sources, nor is there any logical reasoning provided to substantiate the claim. The feedback does not demonstrate an understanding of the importance of verifiable feedback in scientific reviews. It fails to provide any evidence or logical reasoning that could be independently verified, which is a key aspect of the score rubric. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. So the overall score is -1. 
[RESULT] -1",0,"The feedback provided in the response is vague and lacks specificity. It does not clearly identify which sections, statements, or elements in the paper are being addressed. The feedback merely states that ""1 and 2 in [1] look quite similar to Alg."" without providing any context or explanation as to why this is relevant or how it relates to the content of the draft. This lack of detail and direct relevance to the content of the draft makes it difficult to identify the parts that are being addressed in the document. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
5037,NIPS_2020_1663,"Besides the concerns about the assumptions (discussed in Strengths), my other major comment is related to experiments. 1. It seems that the proposed approach was not compared against any existing imputation method. Without doing so, it is very difficult to see the real value of the work. 2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice. 3. I find the results in Section 6.2 a bit weak. Do the results echo findings reported in the literature? Such comparisons could be useful.","2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice.",1.0,1.0,0.0,1.0,nan,0,"The feedback provided in the review is not actionable. It merely questions the choice of probability without offering any concrete steps or recommendations for the author to improve their paper. The feedback does not guide the author in addressing the issue raised. It lacks specificity and does not provide a clear path for revision. The feedback does not fulfill the criteria of actionability as it does not offer any practical suggestions that the author can use to improve their paper. Therefore, the feedback is not useful for the author in terms of actionability. 
[RESULT] 0",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is constructive and encourages the authors to provide more information about the choice of probability and the robustness of their work. The feedback also invites the authors to consider parameter analysis, which is a helpful suggestion for improving the quality of their work. The feedback does not contain any negative or aggressive language, and it maintains a positive and encouraging tone throughout. Therefore, based on the score rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the probability of 0.2 is not appropriate. It also lacks any logical reasoning that could be used to verify the claim. The feedback merely questions the choice of probability without offering any substantiation or analysis. This lack of evidence and logical reasoning makes the feedback less reliable and does not meet the standards of a score of 1. Therefore, the feedback does not respect the aspect of verifiability, as it fails to provide any evidence or logical reasoning to support its claims. 

So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the choice of probability (0.2) for deleting intervals. The feedback also asks for an analysis of the robustness of the method with respect to the probability, which is a direct critique of the paper's methodology. This shows that the feedback is tailored to the content of the paper and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4199,NIPS_2020_630,"- Authors miss to cite and comment some very related work in network interpretability such as: Bau et al, CVPR 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. I think the work in Network Dissection is directly relevant to the topic discussed in the paper, particularly for its hierarchical analysis as well as focus on ImageNet. - It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics. - Why did authors restrict themselves to a smaller number of categories? I think it would be interesting to understand the behavior on a larger category set. - Have authors evaluated qualitatively some of the metrics? I think it would be interesting to see whether, additionally to the quantitative evaluation, the images have some features in common. After rebuttal: The authors have addressed most of my concerns and I will update my score to 7.","- It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics.",1.0,-1.0,-1.0,1.0,nan,0,"The feedback provided in the review is somewhat actionable, as it suggests the need for more variability in the analysis. However, it lacks specificity and clear guidance on how to achieve this. The feedback does not provide concrete steps or recommendations that the author can use to improve their paper. It merely hints at the need for more variability without offering a clear path for revision. Therefore, the feedback does not fully meet the criteria for actionability as outlined in the score rubric. It is more of a general comment rather than a specific, clear, and practical suggestion. So the overall score is 0. [RESULT] 0",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is positive and respectful, as it acknowledges the paper's interesting insights and suggests further analysis to enhance the understanding of the dynamics. The feedback also encourages the authors by implying that their work is valuable and could be improved with additional data and runs. This approach is likely to be well-received by the authors, as it provides a clear direction for improvement without being overly critical or dismissive. Therefore, based on the score rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback suggests that the paper's findings might be specific to certain training runs or datasets, which is a valid point. However, it fails to provide any evidence or references to support this claim. The feedback does not cite any external sources or make logical claims based on the paper's content. It merely states a concern without substantiating it. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any verifiable claims or suggestions. 

In contrast, a score of 1 would require the feedback to include references to external sources, parts of the paper, or logical reasoning that can be independently verified. The feedback should not only raise concerns but also provide a basis for these concerns. Without this, the feedback is not as useful or informative as it could be. 

So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is somewhat specific, as it addresses the need for analysis on multiple runs and other datasets. However, it lacks the depth and detail that would make it highly relevant to the content of the draft. The feedback does not mention specific parts or methods in the paper, which makes it difficult to identify the exact sections or elements that are being addressed. The feedback is more of a general comment rather than a tailored critique. Therefore, it does not fully meet the criteria for specificity as outlined in the score rubric. So the overall score is 0. [RESULT] 0"
2300,ACL_2017_371_review.json,"Weaknesses: - The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?
- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section ",- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?,0.0,1.0,0.0,0.0,nan,0,"The feedback provided in the review is not actionable. It asks questions about the GRU without offering any guidance or suggestions on how to address these questions. The feedback does not provide any direction or hints on what needs to be done to improve the paper. It simply states what needs to be known, without offering any concrete steps or recommendations. This lack of actionable content makes the feedback less useful for the author. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is direct and to the point, which is a positive aspect. However, it lacks the polite and respectful tone that is expected in a scientific review. The questions posed are relevant to the review, but the manner in which they are asked could be perceived as somewhat demanding or critical. The feedback does not encourage the authors or provide any positive reinforcement, which is a key aspect of a constructive review. Therefore, while the feedback is factual and relevant, it does not fully respect the authors' efforts or provide the supportive tone that is necessary for a positive review. So the overall score is 0. [RESULT] 0",-1,"The feedback provided does not meet the criteria for a score of 1. It asks questions about the GRU model, but it does not provide any evidence or references to support the claims made. The feedback does not cite any external sources or use logical reasoning to validate the claims. Instead, it simply asks questions without offering any substantiation. This lack of evidence and logical reasoning means that the feedback does not meet the criteria for a score of 1. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. So the score is -1. [RESULT] -1",-1,"The feedback provided is not specific to the content of the draft. It asks general questions about the GRU without referencing any particular part of the paper. The feedback does not address specific statements, sections, or elements in the paper, which is crucial for a review to be effective. It lacks the detail and direct relevance that is necessary for a review to be tailored to the content of the paper. Therefore, it does not meet the criteria for specificity as outlined in the score rubric. 

The feedback does not provide any insight into how the GRU is performing on the large data set, nor does it explain why the GRU is performing better. It fails to identify which parts of the document are being discussed in the feedback. This lack of specificity makes it difficult to determine which aspects of the paper are being addressed. 

In conclusion, the feedback is not specific to the content of the draft and does not respect the aspect of specificity in a review. It is a general comment that could apply to any scientific document, rather than being tailored to the content of the paper. Therefore, the score is -1. 
[RESULT] -1"
5033,NIPS_2020_1579,"- The presented analysis is limited to a single decision affected by a confounding variable. It is a sensible assumption in many cases, but I got dissapointed by the fact that the current analysis doesn't seem to **conceptually** extend to the general case. - The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one. - Really, the paper presents a **family of bounds** parameterized by Gamma, but Gamma isn't an easily interpretable quantity. Can this be rephrased in terms of a more standard notion (e.g. an information quantity)?",- The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one.,1.0,-1.0,-1.0,1.0,nan,-1,"The feedback provided in the review is relevant to the aspect of the paper's complexity and computational difficulty. However, it falls short in offering actionable suggestions. The feedback merely states the difficulty of the bounds without providing any guidance on how to simplify or make the expression more conservative. It lacks the specificity and clarity that would enable the author to take concrete steps to improve their paper. The feedback does not offer any direct or indirect recommendations, which is crucial for enhancing the usefulness of a review. Therefore, the feedback is not actionable, and it does not respect the aspect of the paper as per the score rubric. So the overall score is -1. 
[RESULT] -1",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The reviewer acknowledges the difficulty of the bounds and suggests a simpler expression, which is a helpful and positive approach to providing feedback. The tone is respectful and encourages the authors to consider a more conservative yet simpler expression. This feedback is not only relevant to the review but also respects the aspect of the paper being discussed. The reviewer's approach is constructive and aims to assist the authors in improving their work. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the bounds are hard to compute. It also lacks logical reasoning that could be independently verified. The feedback merely states that the bounds are hard to compute without offering any substantiation or suggestions for improvement. This lack of evidence and logical reasoning makes the feedback less reliable and does not meet the standards set by the score rubric. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. So the score is -1. 
[RESULT] -1",0,"The feedback provided in the response is somewhat specific, as it addresses the complexity of the bounds in the paper. However, it lacks the depth and detail that would make it highly relevant to the content of the draft. The feedback does not mention specific parts or methods in the paper, and it is not clear which sections, statements, or elements in the paper are being addressed. The feedback is more of a general comment rather than a tailored critique. Therefore, it does not fully meet the criteria for specificity as outlined in the score rubric. So the overall score is 0. [RESULT] 0"
4330,NIPS_2020_1680,"The work is already quite extensive and more cannot be reasonably expected. Still, perhaps the authors can comment on these issues: 1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1. 2) While Manchev and Spratling 2020 claim that their difference target prop for RNNs outperforms BPTT in (simpler) 4 of 5 tasks, why does DRL which is an improvement on DTP not come closer to backprop in these tasks? 3) Would a correction different from GN targets enable to come closer to backprop? 4) In lines 177-178, the authors write 'This damping interpolates between the pseudo-inverse and the transpose of Jf ̄ i,L , so for large λ, GN targets resemble gradient targets.' I would assume that the authors would optimize over lambda as a hyperparameter, but I did not find it in the list of hyperparams in Tables S1-S4. WHy not? In any case, it would be good to explore the dependence of performance on lambda explicitly in a (supplementary) plot and how this impacts performance expecially given that for 'large λ, GN targets resemble gradient targets'.",1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1.,-1.0,1.0,0.0,0.0,nan,1,"The feedback provided in the review is actionable and directly addresses the issues raised in the paper. It specifically points out the lines in the paper where the problem lies and the experimental gap that needs to be addressed. The feedback also provides a clear direction for the author to follow, which is to improve the convergence of the GNT on nonlinear networks and to bridge the gap between DRL+variants and backprop. This guidance is practical and can be used by the author to make the necessary revisions to their paper. The feedback is not only relevant but also respects the aspect of the review, which is to provide constructive criticism that can help the author improve their work. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback addresses specific points in the paper, such as the non-convergence of GNT on nonlinear networks and the experimental gap between DRL+variants and backprop. This shows that the feedback is relevant to the review and that the aspect is respected. The feedback is also positive, as it does not criticize the authors but rather points out areas for improvement. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is directly related to the content of the paper, specifically addressing the claim made in lines 245-246 and the experimental gap between DRL+variants and backprop. The feedback is supported by references to specific parts of the paper, which aligns with the score rubric's requirement for verifiable feedback. The response does not make any unsupported claims, and instead, it provides a logical analysis based on the information presented in the paper. This approach ensures that the reviewer's claims and suggestions can be checked against the paper's content, which is a key aspect of the score rubric. Therefore, the feedback is relevant to the review and is followed and respected in the review. So the overall score is 1. [RESULT] 1",1,"The feedback provided is highly specific and directly addresses the content of the paper. It references specific lines and sections, demonstrating a clear understanding of the paper's content. The feedback also critiques the paper's methodology and experimental results, which are central to the paper's argument. This level of specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback aligns with the score rubric's criteria for a score of 1, as it addresses a certain point or design decision in the paper, making it easy to identify which part of the document is being discussed in the feedback. 
[RESULT] 1"
4757,NIPS_2020_1819,"- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction. - The results are not significantly higher than that of the closest previous work on English. On Chinese it is, but one has to wonder whether that might be due to implementation or hyperparameter differences rather than the particular model. - There are some missing ablations for quantifying where the performance gains come from, and for justifying some design choices (see details below).",- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction.,-1.0,-1.0,-1.0,-1.0,nan,-1,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely states that the approach is not novel and that the main contribution is a more expressive parameterization of parsing action prediction. However, it does not provide any specific, clear, or practical steps or guidance on how to address these issues. The feedback lacks direction and does not help the author in understanding what needs to be done to improve their paper. Therefore, based on the score rubric, the feedback is not actionable and does not meet the criteria for providing useful guidance to the author. So the score is -1. [RESULT] -1",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it offers a straightforward critique of the paper's approach, focusing on the novelty of the method and the expressiveness of the parameterization. The feedback is also constructive, as it points out the main contribution of the paper and its comparison to previous approaches. However, it lacks the positive reinforcement that would be present in a score 1 response. The feedback is more of a critique than an encouragement, which is why it does not score higher. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim that the approach is not novel is not supported by any evidence or references. There is no mention of previous incremental parsing approaches, nor is there any logical reasoning provided to support the claim. The feedback also fails to provide any references to external sources or parts of the paper that could be used to verify the claim. Therefore, the feedback does not meet the requirements for a score of 1, as it does not provide any evidence or logical reasoning to support its claims. 

In contrast, the feedback could have been improved by providing specific references to previous work or by explaining how the parameterization of parsing action prediction is more expressive than previous approaches. This would have made the feedback more relevant and supported by evidence, which is necessary for a score of 1. 

So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the novelty of the approach and the parameterization of parsing action prediction, which are key aspects of the paper. The feedback is tailored to the content of the paper, and it is clear which sections, statements, or elements in the paper are being addressed. The feedback is not a general comment for any scientific document, but rather a focused critique of the paper's main contribution. Therefore, the feedback meets the criteria for a score of 1, as it is relevant to the review and the aspect is followed or respected in the review. 
[RESULT] 1"
3203,NIPS_2020_1444,"1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point. 2. In Theorem 1, it seems that the algorithm can only achieve meaningful results when the bias term decreases very fast with respect to t. In some other cases (such as when the bais is fixed), the regret is linear. 3. In Theorem 2, to achieve meaningful results, one has to know the value of v in advance, which is generally impossible.","1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point.",1.0,1.0,-1.0,1.0,nan,0,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely points out the difficulties in implementing the algorithm and the challenges in sampling an action from the decision set K. While it identifies issues, it does not provide any concrete steps or guidance on how to address these issues. The feedback lacks the specificity and clarity required to be actionable. Therefore, based on the score rubric, the feedback is not actionable and does not meet the criteria for providing a clear path for revision. 
[RESULT] 0",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The feedback acknowledges the authors' motivation and attempts to provide constructive criticism by suggesting additional discussion on the implementation challenges and the sampling of actions. The tone is respectful and encouraging, which is in line with the rubric's criteria for a score of 1. The feedback does not come across as negative, rude, disrespectful, or aggressive, and it maintains a helpful and positive tone throughout. Therefore, based on the rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer simply states that it is difficult to implement the algorithm and that it is not easy to sample an action from the decision set, without providing any logical reasoning or references to the paper. This lack of evidence and logical reasoning makes the feedback less credible and does not meet the standards of a score of 1. The feedback also does not cite any external work, which is a key aspect of the score rubric. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claims made. 
[RESULT] 0",0,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer mentions the difficulty of implementing Algorithm 1 and the challenge of sampling an action from the decision set K, but fails to provide specific details or examples from the paper that would substantiate these claims. The feedback does not directly address any particular section, statement, or element in the paper, making it difficult to identify which parts of the document are being discussed. The lack of specificity in the feedback does not align with the score rubric's requirement for detailed and directly relevant comments. Therefore, the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. 
[RESULT] 0"
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper.",1.0,1.0,1.0,1.0,nan,1,"The feedback provided in the review is actionable and offers specific, clear, and practical suggestions for the author to improve their paper. The feedback identifies the lack of clarity regarding the architecture used in the experiment and the computation of the DIM similarity. It then provides a clear direction for the author to move the information about the architecture to the main text and to motivate the hypothesis of the experiment based on the statement in the appendix. Furthermore, the feedback addresses the organization of the theoretical results and their connection to the main ideas of the paper, offering a suggestion to improve this aspect. The feedback is not only relevant to the review but also respects the aspect by providing actionable steps for the author to follow. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer offers specific suggestions for improving the clarity and organization of the paper, such as moving certain information to the appendix and restructuring the theoretical results. This approach is helpful and encourages the authors to refine their work. The feedback also acknowledges the motivation behind the experiment, which shows an understanding of the paper's content and a respectful approach to the authors' work. The tone of the feedback is positive and respectful, which is in line with the score rubric's criteria for a score of 1. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is not aligned with the score rubric. The feedback does not provide any evidence or references to support the claims made. It suggests moving information from one section to another without providing any logical reasoning or references to the paper's content. The feedback also makes a claim about the DIM loss without providing any evidence or logical reasoning to support this claim. The feedback does not meet the criteria of verifiable feedback as it does not support its claims with references to external sources, parts of the paper, or logical reasoning that can be independently verified. Therefore, the feedback does not meet the requirements of the score rubric and is not relevant to the review. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the lack of clarity regarding the architecture used in the experiment and the computation of the DIM similarity. The feedback also provides a clear solution to the problem by suggesting the reorganization of the information in the appendix and the main text. Furthermore, the feedback elaborates on the theoretical results and their connection to the main ideas of the paper, which is a crucial aspect of the review. The specificity of the feedback ensures that it is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria of the score rubric by being specific and directly relevant to the content of the draft. 
[RESULT] 1"
1580,ICLR_2023_3912,"Weaknesses: 1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance. 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations. 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods. 4. I want to know why the mode parameters in Table 1 and Figure 5 are different. 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.","1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance.",-1.0,1.0,1.0,0.0,nan,0,"The feedback provided in the review is not actionable. It merely states that the proposed method has a ""little performance improvement"" compared to DyTox, without offering any specific, clear, or practical suggestions for improvement. The feedback does not provide any direction or hints on what needs to be done to enhance the performance of the proposed method. It lacks the necessary detail and guidance that would help the author in addressing the issues identified in the review. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. The feedback is focused on the comparison between the proposed method and the existing method, which is relevant to the review. However, the feedback lacks the positive and respectful tone that would be expected for a score of 1. It does not provide any constructive criticism or positive reinforcement to the authors. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a score of 1 or 0. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. It makes a claim about the performance improvement of the proposed method compared to DyTox, but it does not provide any evidence or references to support this claim. The feedback does not cite any external sources or internal references from the paper, nor does it use logical reasoning to validate the claim. Therefore, the feedback does not meet the requirements for verifiable feedback as outlined in the score rubric. It is also noteworthy that the feedback does not mention any specific aspect of the paper, which is a key requirement for the task. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It compares the proposed method with a known method, DyTox, and mentions the performance improvement, the number of parameters, and the variance. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback also respects the aspect of the review, which is the comparison of the proposed method with existing methods. Therefore, the feedback is relevant and specific, and it respects the aspect of the review. So the score is 1. [RESULT] 1"
5533,NIPS_2020_1458,"- As shown in Table1, the proposed Element-weighted TriHard loss actually does not perform better than several existing strategies. Although combining EWTH with HTH or HNTH gets improved results, the effectiveness of the designed method of this paper is incremental. - the best value of t is varying for different methods and datasets (Supplementary materials), so it is too sensitive to selected. - Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.","- Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.",1.0,1.0,1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of mathematical symbols in the paper. However, it falls short in providing actionable suggestions. The feedback merely points out the issue with the symbol x_k under equation (5) without offering any concrete steps or recommendations for the author to follow. It lacks the specificity and clarity that would make the feedback more useful to the author. The feedback does not guide the author in addressing the issue, nor does it suggest any edits that should be made. Therefore, the feedback is not as actionable as it could be. 

So the overall score is -1. [RESULT] -1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The reviewer identifies a specific issue with the mathematical symbols in the paper, which is a relevant aspect to the review. The feedback is delivered in a respectful manner, pointing out the problem without being negative or aggressive. The reviewer also offers a solution by suggesting that all symbols be clearly defined in the paper. This approach is helpful and positive, aiming to encourage the authors to improve their work. The tone of the feedback is constructive and respectful, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is well-aligned with the score rubric's requirements for a score of 1. 
[RESULT] 1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback mentions a problem with mathematical symbols in the paper, specifically pointing out the symbol x_k under equation (5). However, it fails to provide any evidence or references to support this claim. The feedback does not cite any external sources or use logical reasoning to validate the claim. It simply states that the symbols should be defined clearly without offering any concrete examples or explanations. This lack of evidence and logical reasoning makes the feedback less reliable and does not meet the standards of a score of 1. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. So the score is -1. [RESULT] -1",-1,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer mentions a problem with mathematical symbols in the Section of Method but fails to identify which symbols are causing the issue. The feedback does not provide any specific details about the symbols or the equations in question, making it difficult to pinpoint the exact problem. Furthermore, the reviewer suggests defining all the symbols clearly in the paper, which is a general comment that does not address a particular aspect of the draft. This lack of specificity and direct relevance to the content of the draft does not meet the criteria for a score of 1. Therefore, the feedback is not tailored to the content of the paper and is not specific enough to be relevant to the review. So the score is -1. 
[RESULT] -1"
3185,NIPS_2020_1801,"1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers). 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim. 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score. 4. Were other architectures explored besides WideResNet? 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well. 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?",5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?,0.0,1.0,0.0,0.0,nan,1,"The feedback provided in the review is actionable and directly addresses the aspect of the scientific paper. It questions the absence of standard deviation in the results, which is a critical aspect of statistical analysis. The feedback also raises concerns about the randomness of hyper-parameter configurations, which is a key factor in the reproducibility of the results. By doing so, it provides clear and practical suggestions that the author can use to improve their paper. The feedback is not only relevant but also respects the aspect by offering a direction for the author to consider. Therefore, the feedback is aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer's question about the absence of standard deviation in the results is a valid point that shows an understanding of the importance of statistical measures in scientific research. The reviewer also raises a question about the randomness of hyper-parameter configurations, which is a relevant aspect to consider when evaluating the reproducibility of the results. The tone of the feedback is neutral and factual, which is appropriate for a scientific review. The feedback does not contain any negative or aggressive language, and it is clear that the reviewer is trying to encourage the authors to provide more detailed information. Therefore, based on the score rubric, the feedback is relevant and respectful, and it is delivered in a manner that is helpful and positive. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is relevant to the review and directly addresses the aspect of the paper's methodology. It questions the absence of standard deviation in the results, which is a critical aspect of statistical analysis. The feedback also inquires about the nature of the hyper-parameter configurations used in the 10 runs, which is a logical extension of the initial question about standard deviation. This shows that the feedback is not only relevant but also respects the scientific rigor of the paper. The feedback is supported by the internal references from the paper, specifically the details in Appendix B, which is a logical validation of the reviewer's claims. Therefore, the feedback is well-aligned with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is highly specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the methodology used in the experiments. The feedback specifically questions the absence of standard deviation in the results, and it also inquires about the nature of the hyper-parameter configurations used in the 10 runs. This level of detail and specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback is focused on the methodology and the statistical analysis of the results, which are critical components of any scientific paper. Therefore, based on the score rubric, the feedback is relevant and specific, and it respects the aspect of the paper that it is addressing. So the score is 1. 
[RESULT] 1"
88,ICLR_2022_3123,"Weaknesses:
The proposed algorithm is lack of novelty. Compared to the previous video transformer (ViVit, TimeSformer), the difference of this work is: 1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention. 2) this work introduce additional residual connection (with learnable ratio) between attention block from adjacent transformer blocks. 3) the final output is weighted average of the output from each transformer block while the TimeSformer take the output of last block as the final representation.
This paper would need more evidence to support its claim. For example, it's not clear why we should have the current layout of the network. How a set of spatial transformer block followed by a set of temporal transformer block is better than alternating design? considering the cross-stage self-attention could work in both cases.
The improvement of performance is minor. Take the ViVit as the baseline. If we are going to compare between the best performance between ViViT and this paper, the improvement is minor (81.3% from ViViT vs. 81.8% from this paper). If we are going to compare the performance using the same input resolution (16x224x224), the performance of this paper is still on par with ViViT (80.6% from ViViT vs. 80.1% from this paper). In terms of the computational cost, it is wrong to compare GFLOPs between two methods with different inference views. As the performance of any method will not be linearly increased with number of inference views, it is not fair to put 4x3 views for ViViT and 1x3 views for this paper. As a result, the number of 8.6% does not make too much sense. At last, I would suggest to include MViT[1] as one of the reference, which is the SOTA video transformer so far.
The proposed work should be evaluated in at least one or two more video benchmarks. The nature of K400 and K600 is the same. I would suggest also include Something-Something-V2 or Epic-Kitchen datasets.
[1] Multiscale vision transformers, ICCV 2021","1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention.",0.0,0.0,0.0,0.0,nan,0,"The feedback provided in the response is not actionable. It merely describes the structure of the paper without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issues or provide a clear path for revision. It lacks the necessary detail and direction that would make it actionable. Therefore, based on the score rubric, the feedback does not meet the criteria for actionability. 
[RESULT] 0",1,"The response provided is a clear and concise explanation of the methodology used in the paper. It demonstrates a thorough understanding of the subject matter and articulates the process in a manner that is both respectful and constructive. The tone of the feedback is positive and encouraging, which aligns with the score rubric's criteria for a score of 1. The response is not only relevant to the review but also shows respect for the authors' work by acknowledging the complexity of the task and the sophistication of the approach. It is evident that the feedback is aimed at helping the authors improve their work, rather than criticizing it. Therefore, based on the score rubric, the response is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The response provided does not meet the criteria for a score of 1. It does not offer any evidence or references to support the claims made. The feedback is based on a description of the paper's content, but it does not provide any logical reasoning or citation of external sources to validate the reviewer's claims. The feedback is more of a summary of the paper's content rather than a critical evaluation. It lacks the depth and critical analysis expected in a scientific paper review. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or references to support the claims made. 
[RESULT] 0",1,"The response provided is highly specific and directly addresses the content of the paper. It mentions the use of video transformers with spatial and temporal attention, and the TimesFormer with divided space-time attention. This level of detail ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The response clearly identifies the specific methods and design decisions in the paper, which is in line with the score rubric's requirement for specificity. Therefore, the feedback is relevant and respects the aspect of the review. So the score is 1. [RESULT] 1"
198,ICLR_2022_32,"Weaknesses: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper","- The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool.",-1.0,-1.0,-1.0,0.0,nan,0,"The feedback provided in the review is relevant to the paper's content, as it points out a significant omission regarding the application of Tensor Networks in computing. However, the feedback lacks actionable suggestions that the author could use to improve their paper. It merely states the issue without offering any concrete steps or recommendations that would guide the author in addressing the problem. The feedback does not provide a clear path for revision, which is essential for the author to make the necessary improvements. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback points out a significant omission in the paper, which is the application for computing and manipulating Tensor Networks. This is a relevant aspect as it directly addresses the paper's content and its potential impact on the scientific community. The feedback is delivered in a manner that is both respectful and encouraging, aiming to help the authors improve their work. The tone is positive and constructive, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is well-aligned with the score rubric's requirements for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it points out a significant omission in the paper regarding the application of Tensor Networks. However, the feedback does not provide any evidence or references to support the claim that there is a growing audience eager to have such a tool. The feedback is based on a general observation and lacks the verifiable aspect that would make it more credible. Therefore, while the feedback is relevant, it does not meet the criteria for a higher score as it does not provide any evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It identifies a significant omission in the paper, which is the application for computing and manipulating Tensor Networks. This aspect is crucial as it addresses a specific point in the paper, and the feedback is tailored to this particular aspect. The feedback is not a general comment but rather a focused critique on a specific part of the paper. It is clear that the feedback is addressing a particular section of the paper, which is the application of Tensor Networks. Therefore, the feedback is specific and directly relevant to the content of the draft, and it is easy to identify which part of the document is being discussed in the feedback. So the score is 1. 
[RESULT] 1"
4113,NIPS_2020_558,"- Further experiments: I would've liked to see more experimental results on additional graph problems. This could drive your point further by showing more examples of how the loss function can be constructed. - Scalability bottleneck: GNN training can only scale up to some level. Thoughts on whether your method can be scaled up despite this limitation? - Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.","- Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.",1.0,1.0,-1.0,0.0,nan,-1,"The feedback provided in the review is relevant to the aspect of architecture choices/tuning. However, it falls short in offering actionable suggestions. The reviewer identifies the lack of clarity in the selection of the architecture but does not provide any specific guidance or recommendations on how to address this issue. The feedback is more of a general commentary rather than a directive for improvement. It lacks the practical steps or recommendations that would help the author in revising their paper. The feedback does not fulfill the criteria of actionability as it does not offer concrete steps or recommendations that guide the author in addressing the issues identified. Therefore, the feedback is not as useful as it could be in guiding the author towards improving their paper. So the overall score is -1. 
[RESULT] -1",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is respectful and it encourages the authors to provide more information about their architecture choices. The feedback also suggests potential solutions, such as an ablation study or detailed results on the validation sets, which shows an attempt to be helpful and positive. The feedback is delivered in a manner that is likely to be well-received by the authors, as it does not come across as negative or aggressive. Therefore, based on the score rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of architecture choices and tuning. However, the feedback does not provide any evidence or references to support the claims made. The reviewer suggests that an ablation study or detailed results on the validation sets could help clarify the architecture choices, but this is a general suggestion rather than a specific reference to the paper or external work. The feedback does not meet the criteria for verifiable feedback as it lacks the necessary references or logical reasoning to support the claims. Therefore, the feedback is not fully aligned with the score rubric's requirements for a score of 1. 

So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the architecture choices and tuning. The feedback is tailored to the content of the paper, as it specifically mentions lines 219-227 and B.2 in the appendix. It also suggests a method to improve the clarity of the architecture selection, which is an ablation study or detailed results on the validation sets. This shows that the feedback is not a general comment for any scientific document, but rather a targeted critique of the specific aspect of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3237,NIPS_2020_1454,"- The novelty seems limited. The idea of building correlation in low-res and then refine for or facilitate the high-res results might be new in the literature for correspondence search, but it is quite common and has been widely adopted in previous work doing stereo match, where the main job is also to find correspondence but along epipolar line. - The main contribution of this paper, IMO, is to run 4D convolution on low-res correlation volume, which saves computation and possibly achieve comparable performance. If so, the experiment showing the saving of computational resources, e.g. gpu runtime, flip-flop, memory, must be given. - Similar multi-scale approach in stereo matching often runs fast at the cost of losing accuracy, since the correlation volume in low-res is not as informative as the high-res one, and it is not easy to fix if some mistakes are made in low-res. However the experiments show that the result is even better than SOTA. It would be good to add more explanation and analysis. - It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.","- It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.",1.0,-1.0,-1.0,1.0,nan,-1,"The feedback provided in the response is somewhat actionable, as it suggests showing qualitative results and analyzing failure cases. However, it lacks specificity and clarity in its recommendations. The feedback does not provide clear steps or directives for the author to follow, which is crucial for actionability. It also does not offer a clear path for revision, which is essential for the author to improve their paper. The feedback is more of a general suggestion rather than a concrete set of instructions. Therefore, the feedback does not fully meet the criteria for actionability as outlined in the score rubric. So the overall score is -1. [RESULT] -1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone is positive and respectful, offering suggestions for improvement and acknowledging the potential of the proposed method. The feedback also encourages the authors to present qualitative results and to analyze the limitations of their method, which is a helpful and positive approach to reviewing. The feedback is not aggressive or disrespectful, and it maintains a constructive tone throughout. Therefore, based on the score rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it suggests showing qualitative results and analyzing failure cases. However, it does not provide any evidence or references to support these claims. The feedback is based on general observations and does not cite any external work or use logical reasoning to validate its suggestions. Therefore, it does not meet the criteria for verifiable feedback as outlined in the score rubric. The feedback is more of a suggestion rather than a claim that can be independently verified. So the overall score is 0. [RESULT] 0",0,"The feedback provided is somewhat specific, as it suggests showing qualitative results and analyzing failure cases. However, it lacks the depth and detail that would make it highly relevant to the content of the draft. The feedback does not specify which sections, statements, or elements in the paper are being addressed, which is crucial for a review to be tailored to the content of the paper. The feedback also does not mention any specific methods or parts of the paper, which makes it difficult to identify the parts that are being addressed in the document. Therefore, the feedback does not fully meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
5051,NIPS_2020_1889,"- The performance of the method is still sub-optimal on large scale datasets, and hence there is a scope of improvement. - The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.","- The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is somewhat vague and lacks specificity. While it does mention the similarity to an earlier work and the usage of data-dependent methodologies, it fails to provide clear, actionable suggestions for the author to improve their paper. The feedback does not offer concrete steps or recommendations that guide the author in addressing the issues identified. It merely states the similarities and methodologies used without providing any direction or hints on what needs to be done. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. So the overall score is 0. [RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is neither negative nor positive, and it does not show any signs of disrespect or aggression. The feedback simply states the similarities and differences between the proposed method and the earlier work, without any explicit encouragement or discouragement. It is a straightforward comparison, which is a common practice in scientific reviews. Therefore, based on the score rubric, the feedback is neutral and does not violate any aspect of the rubric. 
[RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. While it does mention a previous work [35], it fails to provide a clear connection between the two, and it does not offer any evidence or logical reasoning to support the claim that the proposed method is similar to the earlier work. The feedback is vague and does not provide a clear basis for the claim made. It lacks the necessary detail and specificity that would allow for independent verification of the claims made. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claim made. 
[RESULT] 0",0,"The feedback provided in the response is somewhat specific, as it mentions a previous work [35] and the usage of data-dependent methodologies for grouping. However, it lacks the depth and detail that would make it highly relevant to the content of the draft. The feedback does not clearly identify which sections, statements, or elements in the paper are being addressed, and it does not provide a clear connection between the feedback and the specific aspects of the paper. Therefore, while the feedback is not entirely irrelevant, it does not meet the criteria for a high score in terms of specificity. So the overall score is 0. [RESULT] 0"
663,ICLR_2021_2455,"Weaknesses: In spite of the strengths mentioned above, there are a few questions that are confusing. 1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed. 2. As for the theoretical analysis: a) For Theorem 1, the right hand equation uses L_2 norm of a function of beta. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Therefore, I wonder what does the regularizer proposed in equation(6) means since beta has already been integrated. b) For Theorem 1, the core assumption is ‘the expected loss function as a function of beta belongs to a Sobolev space’, which is confusing. Could you provide some explanations of this assumption or give some examples of it? c) Theorem 1 provides an upper bound for one specific kind of DRO problem whose uncertainty set is formulated as an affine combination of training distributions. However, in this article, the authors do not state what is the definition of the invariance here and why solve such DRO problem could achieve the invariance. 3. As for the proposed objective function: a) As mentioned above, the L_2 norm is taken over a function of beta, which I think is not the Euclidean norm of the vector. Beta has already been integrated and this regularizer has nothing do with beta. I wonder how to compute this when optimizing? b) I wonder how this objective function can be optimized efficiently? The first concern is mentioned above as the computation of L_2 norm. The second concern is how to optimize the variance which is non-convex and hard to optimize. Namkoong et al. [1] convert the optimization of a variance-regularized problem to a f-divergence DRO for better optimization, while in this paper the authors take the opposite way. I wonder is there any theoretical guarantee of the optimization of the objective function(6). 4. As for the experiments: a) The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in 3).
[1] Duchi, J. , & Namkoong, H. . (2016). Variance-based regularization with convex objectives.","1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is not actionable. It identifies issues with the third figure in the simulated experiment but fails to offer any concrete steps or recommendations for the author to improve their paper. The feedback does not provide any direction or hints on what needs to be done, which is crucial for the author to address the issues raised. The feedback merely points out the lack of effectiveness in the proposed methods without suggesting any improvements or modifications. Therefore, the feedback does not meet the criteria of being actionable, as it does not guide the author in addressing the issues. So the score is -1. [RESULT] -1",-1,"The feedback provided in the response is quite critical and lacks the necessary politeness and respect that is expected in a scientific review. The tone is negative and the feedback is delivered in a way that could be perceived as aggressive. The feedback does not encourage the authors or provide any positive reinforcement. Instead, it focuses on the shortcomings of the paper without offering any constructive criticism or suggestions for improvement. The feedback also fails to acknowledge the efforts of the authors, which is an important aspect of respectful feedback. Therefore, based on the score rubric, the feedback does not meet the criteria for a polite, helpful, positive, respectful, or encouraging tone. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer's comments about the third figure in the paper and the comparison of the methods are not supported by any references to the paper or external sources. The feedback also fails to provide logical reasoning that could be independently verified. The reviewer's claim that the result of IRM for the experiment is missed is not substantiated with any evidence. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claims made. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the simulated experiment and the performance of different methods. The feedback is tailored to the content of the paper, as it specifically mentions the third figure in Figure 1 and the results of the DIRM and DRO methods. It also points out the absence of the result of the IRM method, which is a critical part of the experiment. The feedback is not a general comment for any scientific document, but rather a detailed critique of the specific content of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4778,NIPS_2020_770,"1. All components, BGV and TFHE, are borrowed from other papers. Similar to Chimera, the switching mechanism is not proposed by the authors. Even transfer learning has developed for a long time. The authors are suspect of just ensembling these ideas together. The reviewer doubts the novelty of the proposed method. 2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.",2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.,nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It identifies a problem with the comparison between FHESGD and TFHE-BGV but fails to offer any concrete steps or recommendations for the author to improve their paper. The feedback merely states that the authors did not explain why FHESGD performs worse than their system equipped with TFHE-BGV, without providing any guidance on how to address this issue. Additionally, the feedback mentions that the reviewer expects more analysis of mechanics, but it does not specify what kind of analysis is needed or how it should be conducted. The feedback also points out that the figures are supposed to compare with Chimera, but it does not explain why this is important or how it should be done. Therefore, the feedback lacks the necessary actionability to be useful to the author. 
[RESULT] 0",-1,"The feedback provided in the response is critical and somewhat negative, which is not entirely respectful or constructive. The reviewer's expectation for more analysis of mechanics is not clearly stated, and the comment about the comparison with Chimera is vague and lacks specificity. The tone of the feedback is not entirely polite or helpful, as it does not provide clear guidance or encouragement to the authors. The feedback also fails to acknowledge the efforts of the authors, which is an important aspect of respectful criticism. Therefore, the feedback does not fully respect the authors' work and does not encourage them in a positive manner. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV. The feedback also fails to provide any logical reasoning to support the claim that the reviewer expects more analysis of mechanics. Furthermore, the feedback does not cite any external sources or reference any parts of the paper to support the claim that Fig. 5 and Fig. 6 are supposed to compare with Chimera. Therefore, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support its claims. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the comparison between FHESGD and TFHE-BGV. The feedback also mentions specific figures (Fig. 5 and Fig. 6) that are supposed to compare with Chimera, indicating a clear understanding of the paper's content. The reviewer's expectation for more analysis of mechanics is also a specific critique that is directly related to the paper's content. Therefore, the feedback is tailored to the content of the paper, and not a general comment for any scientific document. This specificity and relevance to the paper's content make the feedback valuable and informative. So the overall score is 1. [RESULT] 1"
2812,NIPS_2022_1430,"Weakness:
One major issue with this paper is clarity of text and definitions. Examples:
Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature
Equation 3, P ( < s > | x ) = ∑ y ∈ A P ( y | x )
, I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)?
text issues:page 5, ""is is useful think ...""
Many issues in Table1: 4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean? 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo?
The definition of the randomly split the words in bottom of page 6 is not clear to me.
The notation pDrop, how about p_drop ?
Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant.
Finally, there is a major concern about baseline comparisons:
All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3.
STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?","4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean?",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It merely points out a number under LM without offering any guidance or direction on how to interpret or address this issue. The feedback does not provide any practical suggestions or recommendations that the author can use to improve their paper. It lacks the specificity and clarity needed to be actionable. Therefore, the feedback does not meet the criteria of the score rubric, which emphasizes the importance of actionable comments in a review. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on a specific aspect of the review, asking for clarification on a particular point. This approach is more aligned with a constructive critique rather than a negative or disrespectful one. The feedback is also polite and respectful, which is a positive aspect. However, it lacks the positive and encouraging tone that would be expected for a score of 1. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a score of 1 or 0. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or logical reasoning to support the claim made about the ""TRANSFORMER[36]"" and the numbers under LM. It simply asks a question without offering any form of verification or explanation. This lack of support and reasoning makes the feedback not relevant to the review, and it does not respect the aspect of verifiability. Therefore, the feedback does not meet the requirements of the score rubric. 

In contrast, a score of 1 would require the feedback to provide evidence or logical reasoning to support the claim made about the ""TRANSFORMER[36]"" and the numbers under LM. This would involve referencing external sources, parts of the paper, or using logical reasoning that can be independently verified. 

So the overall score is -1. [RESULT] -1",-1,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. The reviewer's question about the meaning of ""TRANSFORMER[36]"" and the number under LM is vague and does not directly address any specific part or method in the paper. The lack of specificity in the feedback makes it difficult to identify which sections, statements, or elements in the paper are being addressed. This is in contrast to the score rubric, which emphasizes the importance of specificity in a review. Therefore, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of specificity in the review. 
[RESULT] -1"
1572,ICLR_2023_3811,"Weaknesses
Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term ""coherence"" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining ""coherence"" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words ""turn"" and ""utterance"" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: ""entailment"", ""contradiction"" and ""neutral"". I suggest specifying that the 1 label in the numerator corresponds to the ""entailment"" class, and whether you consider both ""contradiction"" and ""neutral"" as a single ""non-entailment"" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the ""instances for evaluation"" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the ""Utterance Level Sampling"" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it ""tends to sample previous utterance to be replaced"" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the ""Coherence Rate"" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results ""w/ Noise"" described on page 6, ""w/ Utterance"" and ""w/ Semi-Utterance"" described on page 4, but you also mention ""w/ Hierarchical"". Up to this point I had understood both ""Utterance Level Sampling"" and ""Semi-utterance Level Sampling"" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the ""w/ Hierarchical"" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating ""Rep""? I suggest clarifying this.
On page 7 in the ""Sampling vs w/o Sampling"" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the ""Human Evaluation"" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the ""Explicit Coherence Optimization"" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your ""hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking"". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you ""demonstrate these methods make chatbots more robust in real-world testing"" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term ""coherence"" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition ""Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts."" is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, ""is coherence"" should be ""that is coherent"".
P. 3, p. 4, l. 1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index ""1"" mean in U ^ 1 i − 1
? Does it mean ""starting from index 1""? If this is the case and you never use anything other than ""1"" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to ""conduct"" a classifier does not make much sense. You can either ""conduct"" classification or ""train"", ""use"", ""create"", etc. a classifier.
P. 4, p. 4, l. 4: here you say ""we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: ""Given a training pair U ^ 1 t − 1
"" should be ""Given a training pair U ^ 1 ′ t − 1
"", i.e. the training pair contains an utterance replaced through the ""Utterance Level Sampling"" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it.
P. 5, p. 3, l. 3: ""two annotators are employed"" -> ""two annotators were employed""
P. 5, p. 5, l. 3: The sentence starting with ""As model-based methods"" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: ""Following previous work (Ritter et al., 2011)"" -> ""Following the work by Ritter et al. (2011),""
P. 5, p. 8, l. 2: ""to online evaluate these two methods"" -> ""to evaluate these two methods online""
P. 6, p. 1, l. 2: ""non-repetitive"" -> ""non-repetitiveness""
P. 6, p. 6, l. 1: ""After sample an utterance"" -> ""After sampling an utterance""
P. 6, p. 8, l. 2: ""generate coherence response"" -> ""generate coherent responses""
P. 6, p. 8, l. 4: ""with the number of turns increases"" -> ""as the number of turns increases""
P. 7, Figure 4, a - b: The y-axis should be labeled ""coherence (%)"" instead of ""coherent (%)"". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence ""since sampled noises are difficult to accurately simulate errors of the inference scene during training"" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): ""Contradition"" -> ""Contradiction""; (b): ""Coherent"" -> ""Coherence"". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: ""hierarchy way"" -> ""hierarchical way""
P. 9, p. 2, l. 1: ""incoherence response"" -> ""incoherent response""
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.","1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow. P. 3, p. 4, l.",nan,nan,nan,nan,nan,1,"The feedback provided in the response is actionable and directly addresses the issue of reading flow. The suggestion to reorder the sentence structure is a clear and practical recommendation that the author can implement to improve their paper. The feedback also includes specific references to the pages and lines where the issue occurs, which adds to the actionability of the feedback. The response is concise and to the point, making it easy for the author to understand and act upon the feedback. Therefore, the feedback is relevant to the review and respects the aspect of actionability. So the overall score is 1. [RESULT] 1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The feedback is delivered in a respectful manner, offering a suggestion to improve the reading flow of the paper. The tone is positive and encouraging, which is essential for a reviewer's feedback. The feedback also directly addresses the aspect in question, which is the placement of references in the text. This shows that the feedback is relevant to the review and that the aspect is respected. The feedback is not aggressive or disrespectful, and it maintains a helpful and positive tone throughout. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected in the review. The feedback suggests a minor change to the reading flow of the paper, which is a logical claim that can be independently verified. The feedback also provides specific references to the paper, which aligns with the score rubric's requirement for verifiable feedback. The feedback does not ignore or not respect any aspect of the review, and it is not irrelevant to the review. Therefore, the feedback meets the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the structure of the sentences and the placement of references. The feedback is tailored to the content of the paper, and it is clear which sections, statements, or elements in the paper are being addressed. The feedback is not a general comment for any scientific document, but rather a specific critique of the draft. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5570,NIPS_2020_133,"(-) Some claims are way too strong. *“we study and prove the incapability of standard activation functions to extrapolate” is debatable. The proof is mainly related with ReLU and tanh. * After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.","* After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is not actionable. It identifies issues with the proposed activation function but fails to offer any concrete steps or recommendations for improvement. The feedback merely points out the lack of clear benefits and the omission of other activation functions in the experiments without suggesting how these issues could be addressed. It also lacks the provision of specific, clear, and practical suggestions that the author can use to improve their paper. The feedback does not guide the author in addressing the issues raised, which is a key aspect of actionable feedback. Therefore, the feedback does not meet the criteria of being actionable, and it does not provide the author with a clear path for revision. So the overall score is -1. [RESULT] -1",-1,"The feedback provided in the response is critical and somewhat confrontational. The tone is not entirely respectful, as it questions the validity of the proposed activation function and the methodology of the paper. The feedback also implies that the authors have not considered alternative methods, which could be perceived as a lack of thoroughness in their research. Furthermore, the feedback does not offer any constructive suggestions or encouragement to the authors. Instead, it focuses on the shortcomings of the paper without providing any positive feedback or acknowledging the potential merits of the proposed method. This approach does not align with the rubric's requirement for a polite and respectful tone. Therefore, the feedback does not meet the criteria for a score of 1, as it lacks the necessary politeness and respectfulness. 
[RESULT] -1",-1,"The feedback provided in the response does not align with the score rubric's criteria for verifiable feedback. The reviewer's claim that the proposed activation function is not more general than existing ones is not supported by any evidence or references. The reviewer's assertion that the proposed method does not provide clear benefits against existing methods is also not substantiated with any data or logical reasoning. The feedback does not cite any external sources or use internal references from the paper to support the claims made. The reviewer's question about why other methods are not used in the last two experiments is not answered with any evidence or logical reasoning. Therefore, the feedback does not meet the criteria for verifiable feedback as it imposes claims without providing any evidence for them. So the overall score is -1. [RESULT] -1",-1,"The feedback provided in the response is not specific to the content of the paper. It fails to address specific parts or methods in the paper, making it difficult to identify which sections, statements, or elements in the paper are being addressed. The feedback is more of a general commentary on the paper rather than a detailed critique. It does not provide a clear understanding of the paper's content or the specific aspects that are being discussed. The feedback does not demonstrate a tailored approach to the content of the paper, which is essential for a scientific review. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] -1"
4522,NIPS_2020_988,"The introduced method relies on sparse distributions, which is a quite strong assumption. While the authors address the main implications of this assumption, I think there should have been an even more detailed discussion/empirical evaluation to increase the impact of this work in the community: - in the semi-supervised learning experiments in section 5.1 you use a VAE model which is relatively simple and by now quite outdated. Do you expect these results to generalize to more complex architectures? For example, if I took any of the SOTA semi-supervised deep generative models and just replaced the softmax with the sparsemax would you expect similar improvements? - how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data? - since sparsemax is such a core component of this method, it would be useful to add some details on its forward/backward passes and their computational complexity wrt the softmax.",- how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data?,nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the aspect of the paper, which is the behavior of the method with challenging tasks containing ambiguous data points. The feedback asks a question about the method's behavior and suggests a possible direction for the author to consider, which is the use of loss evaluations throughout the training procedure or the sparsity assumption. However, the feedback does not provide specific, clear, and practical suggestions that the author can use to improve their paper. It lacks in giving a direction or hints of what needs to be done. The feedback is more of a question than a directive for the author to follow. Therefore, the feedback is not actionable, and it does not enhance the usefulness of the review for the author. So the score is -1. 
[RESULT] -1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback is focused on the method's behavior with challenging tasks and ambiguous data points, which is relevant to the review. It also respects the authors' work by suggesting improvements without being dismissive or disrespectful. The feedback is encouraging and tries to guide the authors towards a better understanding of their method. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and addresses the aspect of how the model behaves with challenging tasks containing ambiguous data points. The feedback suggests that the model may either use many loss evaluations throughout the training procedure or learn to be certain even for ambiguous data, which is a logical claim based on the information provided in the paper. However, the feedback does not provide any evidence or references to support these claims, which is a key aspect of the score rubric. The feedback could have been strengthened by citing specific parts of the paper or external sources to back up the claims made. Therefore, while the feedback is relevant and logically sound, it falls short in terms of verifiability, as it does not provide any evidence or references to support its claims. So the overall score is 1. [RESULT] 1",1,"The feedback provided is highly specific and directly addresses the method's behavior with challenging tasks containing ambiguous data points. It specifically questions whether the model would use numerous loss evaluations throughout the training procedure or whether the sparsity assumption would lead the model to be overly certain with ambiguous data. This feedback is tailored to the content of the paper, as it directly relates to the method's design and its potential implications on the model's performance. The feedback is not a general comment but rather a focused critique that aligns with the specific aspects of the paper. Therefore, the feedback is highly relevant and specific, which is why it would receive a score of 1 according to the score rubric. 
[RESULT] 1"
141,ICLR_2022_1421,"Weakness:
For the Per-SSFL framework, the local (personalized) model and global model are used. The memory consumption aspect should be discussed. For resource-constrained edge clients, high memory cost could be an issue.
Although it mentioned in the implementation setting that the client number selected per round is 10, it is not clear how many total clients are used in the FL setting.
In Table 1, what is the FL method under the supervised setting?
In Figure 2, what does (0.5) for SSFL on IID and SSFL on non-IID mean?
As can be seen in Figure 2, the convergence rates for the IID and non-IID cases are quite similar. Can you provide an explanation for that?
After reading Appendix D and Figure 10 in Appendix, the experimental setting on GLD-23K is still not quite clear. For example, how the local training set for each client is generated? What about the label distribution? Also, it seems that the number of clients used on the GLD-23K is different from that on CIFAR-10.
For ease of comparison and implementation, it would be good to evaluate the method on more commonly used datasets such as CIFAR-100 and Tiny-ImageNet and other datasets besides vision datasets (e.g., text) for FL.
It would be interesting to see the SSFL results under different numbers of selected participant clients. Since λ
is an important parameter that balances consensus and personalization, its effect should be studied.
Minor issues:
Section 2.1, “… the local empirical risk over the heterogeneous dataset D k
.” -> D k
Figure 10 appeared in Sec. 5.2 and 5.3, it should be Fig. 2 and Fig. 3.
In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent.
Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)”
A careful proofread of the paper is highly recommended.","3. In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent. Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)” A careful proofread of the paper is highly recommended.",nan,nan,nan,nan,nan,1,"The feedback provided in the review is actionable and offers specific, clear, and practical suggestions that the author can use to improve their paper. The feedback addresses the inconsistency in the colors used in the figure and provides a direct suggestion to make them consistent. Additionally, the feedback identifies a minor error in the list of contemporary self-supervised learning frameworks and offers a correction. The recommendation for a careful proofread of the paper is also actionable, as it guides the author in addressing potential issues. The feedback is well-structured and directly addresses the issues, making it highly useful for the author. Therefore, the feedback is relevant and respects the aspect of the review. So the overall score is 1. [RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback addresses specific issues in the paper, such as the inconsistency in color and legend in the figure, and the incorrect mention of SimSiam instead of SimCLR. The feedback also suggests a proofread, which is a positive and helpful suggestion. The tone of the feedback is professional and encouraging, which is in line with the rubric's criteria for a score of 1. The feedback is not negative, rude, or disrespectful, and it does not ignore or not respect the aspect in question. Therefore, based on the score rubric, the feedback is relevant and respectful, and it is delivered in a polite and helpful manner. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is relevant to the review and is followed or respected in the review. The feedback addresses the inconsistency in the color scheme of the curve and its legend in Figure 2(a), which is a clear and specific issue. Additionally, the feedback corrects the mention of ""SimSiam"" in Sec. 3.2, suggesting the correct term ""SimCLR"". This shows that the feedback is based on the content of the paper and is not a general statement. The recommendation for a proofread is also a logical suggestion that can be independently verified. Therefore, the feedback is verifiable and aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is highly specific and directly relevant to the content of the draft. It addresses a specific point in the paper, namely the inconsistency in the color scheme of the curve and its legend in Figure 2(a). The feedback also identifies a specific section of the paper, Sec. 3.2, and provides a precise correction to the list of contemporary self-supervised learning frameworks. Furthermore, the feedback recommends a proofread of the paper, which is a constructive suggestion for improving the document. The specificity of the feedback ensures that the reviewer has clearly identified the parts of the paper that are being addressed, making it easy to understand and act upon the feedback. Therefore, based on the score rubric, the feedback is highly specific and directly relevant to the content of the draft, and it respects the aspect of the review. So the score is 1. [RESULT] 1"
157,ICLR_2022_939,"Weaknesses
Novelty is somewhat low.
Discussion of gradient explosion seems inaccurate.
Some discussion of related work is missing, particularly about related works in the model-based RL literature.
Sometimes they are stating things as facts without providing evidence.
There was no detailed discussion of how the results with the current simulator relate to other recent simulators such as Brax. Are the wallclock times similar? The results seem mixed, but having a detailed discussion would have been useful.
The simulator is fast, so it may have been better to do more than 4 environments. Recommendation
I recommend accepting the paper as I have not seen differentiable simulators used for tasks of the difficulty considered here (while also taking advantage of differentiating through the simulator). I was considering a score of 6 or 8, but gave 6 for now.
Discussion of points brought up
Novelty: It seems the contribution is primarily one of engineering, and they don't propose any surprising new idea. The idea of truncated backpropagation is old. Moreover, the policy training scheme resembles that of the the Dreamer algorithm (Hafner et al. 2019). Dreamer also uses short horizon rollouts together with a terminal value function, and backpropagates through these short horizons to optimize the policy. The differences are only: Dreamer uses lambda return weighting during the short horizons (why didn't you use this?), and the method of constructing the sequences is different (Dreamer samples start states from a replay buffer and performs model rollouts from these states, while the current work splits an episode into chunks). I am surprised that Dreamer was not discussed when explaining the methodology. Certainly it should be mentioned that there are prior works using a similar policy optimization procedure (with slight variations).
Discussion of gradient explosion: There are other earlier works that do a more detailed job of discussing the gradient and landscape issues, such as PIPPS (Parmas et al. 2018), which should have been cited and discussed (moreover your methodology was very similar to these previous works). In your work, if gamma were 1, the value function were perfect and the policy were deterministic, the gradient you compute with your method should be exactly the same as the gradient that is computed using BPTT. From this point of view, your discussion is insufficient, as you do not explain why the loss landscape and gradients end up being smooth despite this fact. I can think of two possible reasons: 1. The value function is an approximator that ends up being smooth because of limited capacity to model the complicated landscape. 2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance.
For the other points see the raw notes at the bottom of this section.
Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions
Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?
Q2. In Figure 2, did you use a deterministic or stochastic policy? Was this the same policy as was used during training the value function? If the policy was stochastic, then how did you evaluate the landscape? This would require sampling many trajectories with the same policy and averaging. Are the scales on the left and right figures the same?
Q3. Did you do any ablation study of the policy noise? Does the method still work when the policy is deterministic? How much does the performance drop?
Additional notes made during reviewing
""for systems ranging from robots (e.g., Cheetah, Shadow Hand) to complex anima- tion characters (e.g., muscle-actuated humanoids) using only high-level reward definitions."" Please provide references.
""A differentiable simulator provides accurate first-order gradients of the task performance reward with respect to the control inputs."" This is speculation. You provide no evidence. Problems with accuracy can arise when the task performance depends on a sampled initial state (so that the gradients are inherently stochastic). Perhaps change to ""may provide"".
""However, despite the availability of differentiable simulators, it has not yet been convincingly demonstrated that they can effectively accelerate policy learning in complex high-dementional and contact-rich tasks, such as some traditional RL benchmarks."" While this may be the case for differentiable simulators, there are several model-based RL works that showed effective learning (e.g., Dreamer. The difference with a simulator is just that the model does not have to be learned. I think the claim here is downplaying such previous contributions. Also, ""dementional"" should be ""dimensional"".
""There are several reasons for this: 1.(), 2.(), 3.()"" These reasons are stated as facts, while they are speculations. Perhaps, ""possible reasons"" would be better. At least points 1 and 3 should be the same for model-free RL, so are they really the reason? No references were provided.
""Because of these challenges, previous work has been limited..."" How do you know that those were the challenges that limited the applicability of the previous methods? The publications themselves do not seem to note your reasons as the reason why they limited their experiments. For example, the PODS paper says their method overcomes the issues of exploding gradients.
""In addition, we propose a truncated learning window to shorten the backpropagation path to address problems with vanishing/exploding gradients and reduce mem- ory requirements."" This is known as ""truncated backpropagation"". It is plagiarism to claim that you ""proposed"" this.
In Equation 1, please provide the definition of the Jacobian's and gradients. Usually, the gradient is a row vector, whereas you are using a column vector.
""This makes the reward function smoother..."" What do you mean by this? The reward function is the same in all cases.
""In addition, we apply state normalization as is common in RL algorithms."" Please explain what ""state normalization"" is.
""First, the terminal value function absorbs the discontinuity of long dynamics horizons and early termination into a smooth function, as shown in Figure 2 (Right)."" This explanation is incomplete. If you had no discount factor, a perfect value model and a deterministic policy, your computed policy gradient would be exactly the same as that of BPTT.
""Finally, the use of short horizons allows us to update the actor more frequently, which, when combined with parallel differentiable simulation, results in a significant speed up of training time."" Do you have an ablation study showing that it speeds up?
""In contrast, our method scales well due to direct access to the true gradients from differentiable simulation."" You don't have access to true gradients when you are using a stochastic policy. It may be better to explain this by referring to the fact that reparamterization gradients are often more accurate when computing gradients of smooth functions.","2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance. For the other points see the raw notes at the bottom of this section. Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. Instead, it merely points out a discrepancy in the author's description of their work, without providing any guidance on how to resolve this discrepancy. The feedback also fails to address the potential issue of ill-behaved gradients, which is a critical aspect of the author's work. It simply mentions the issue without offering any solutions or recommendations. Therefore, the feedback does not meet the criteria of being actionable, as it does not provide the author with a clear path for revision. 
[RESULT] 0",-1,"The feedback provided in the response is quite direct and lacks the polite and respectful tone that is expected in a scientific review. The feedback seems to be more of a confrontation rather than a constructive critique. The use of phrases like ""you are using a stochastic policy"" and ""you just used Isaac Gym"" can be perceived as accusatory and disrespectful. Additionally, the feedback does not provide any constructive advice or suggestions for improvement, which is a key aspect of a review. Instead, it focuses on pointing out discrepancies without offering any solutions or guidance. This approach is not conducive to a positive and respectful review process. Therefore, based on the score rubric, the feedback does not meet the criteria for a polite, helpful, positive, respectful, or encouraging tone. So the overall score is -1. 
[RESULT] -1",-1,"The feedback provided in the response is not adequately supported by evidence. The claim that the stochastic policy smooths out the landscape is made without any reference to the paper or external sources. Furthermore, the feedback questions whether the simulator was built or if Isaac Gym was used, but it does not provide any evidence to support this claim. The feedback also refers to a point made by Parmas et al. (2018), but it does not provide any context or explanation of how this point relates to the feedback. The lack of evidence and the absence of logical reasoning to support the claims made in the feedback result in a score of -1. 
[RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point about the stochastic policy and its implications on the landscape of the value function. The feedback also brings up a specific point from Parmas et al. (2018) regarding the ill-behaved gradients, which shows a clear understanding of the paper's content. Furthermore, the feedback questions the author's claim about building the simulator, which is a direct reference to a specific part of the paper. This level of specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback meets the criteria of the score rubric by being specific and directly relevant to the content of the draft. 
[RESULT] 1"
339,ICLR_2022_2842,"Weaknesses: 1. In introduction section, author claimed ""Representation Topology Divergence (RTD) score which measures a dissimilarity between two point clouds of equal size with one-to-one correspondence between points"". The question is how to explain ""one-to-one correspondence between points"", what does ""one-to-one"" mean here? It seems this is an important part of contribution, but there is no specific explanation about this so it is confusing me.
The writing is not clear. For example, some concepts in section 2 are not well explained, such as Barcode, Vietoris Rips complex and homology group are not be familiar with ML community. It should be better to add some figures and intuitive explanation about these abstract topology terms.
3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score?
It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?","3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score? It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It identifies a potential issue with the author's claim about the sensitivity of the RTD score to cluster, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback merely questions the theoretical or topological sides of the RTD score without providing any guidance on how to address these questions. Furthermore, the feedback does not suggest any concrete steps or recommendations that the author can use to improve their paper. Instead, it leaves the author without a clear path for revision. Therefore, based on the score rubric, the feedback is not actionable and does not meet the criteria for providing useful guidance to the author. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on the aspect of the RTD score's sensitivity to cluster and its application to network representation. The feedback questions the theoretical and topological sides of the RTD score, which is a valid point of critique. However, it does not provide any specific guidance or suggestions for improvement, which could have been more helpful to the authors. The feedback is also concise and to the point, which is a positive aspect. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not support the claim made by the author regarding the sensitivity of the RTD score to cluster. Instead, it questions the theoretical and topological aspects of the RTD score without providing any evidence or logical reasoning to support the claim. The feedback also fails to provide any references to external sources or parts of the paper that could be used to verify the author's claim. Therefore, the feedback does not meet the requirements for verifiable feedback as outlined in the score rubric. It is more of a critique rather than a review that respects the aspect of the paper. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. It does not mention any specific parts or methods in the paper, making it difficult to identify which sections, statements, or elements in the paper are being addressed. The feedback seems to be a general comment rather than a tailored critique of the paper. It lacks the specificity required to effectively critique the paper's content. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 
[RESULT] 0"
698,ICLR_2021_1682,"weaknesses
+ The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.
+ The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.
+ The paper is well-written, easy to follow, and well-connected to the existing literature.
- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.
- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.
- A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments. Recommendation
I’m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that’s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.
Detailed justification
The biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I’m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes while keeping other hyperparameters fixed, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that ""NCA performs better than all PN configurations, no matter the batch size"" with a grain of salt, for instance.
I also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission’s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don’t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.
The connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to ""show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis"", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings – albeit with a Euclidean metric rather than a cosine similarity metric – since both perform comparisons on example pairs.
This relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between ""1-NN with class centroids"" and k-NN / Soft Assignment noted in Section 4.1 – as well as the drop in performance observed in Figure 4’s row 6 – be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?
Finally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don’t know how competing approaches would perform if we applied the paper’s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA’s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant. Questions
In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?
Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?
Can the authors elaborate on the ""no S/Q"" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn’t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class’ prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.
Additional feedback
This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven’t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that ""in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]""; can the authors point to the implementations they have in mind when making that assertion?
I would be careful with the assertion that ""during evaluation the triplet {w, n, m} [...] must stay unchanged across methods"". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.
I’m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.","- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the aspect of generalization in few-shot learners. However, it falls short in offering actionable suggestions for the author to improve their paper. The feedback merely points out the lack of evaluation on the generalization of observations to few-shot learners beyond Prototypical Networks. It does not provide any guidance or recommendations on how to address this issue. The feedback lacks the specificity and clarity needed to be actionable, which is crucial for the author to understand and implement the necessary changes. Therefore, the feedback does not meet the criteria of being actionable, as it does not offer concrete steps or recommendations that guide the author in addressing the identified issue. So the score is -1. 
[RESULT] -1",0,"The feedback provided is neutral and factual, focusing on the lack of evaluation regarding the generalizability of the observations to few-shot learners beyond Prototypical Networks. It does not contain any negative or aggressive language, and it maintains a respectful tone throughout. The feedback is constructive, pointing out a potential limitation in the scope of the submission's contributions. However, it does not actively encourage the authors or provide positive reinforcement. Therefore, based on the score rubric, the feedback is neither disrespectful nor overly positive, but it is neutral and factual. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. It does not support the claim that the observations presented in the paper do not generalize to few-shot learners beyond Prototypical Networks. The feedback lacks any evidence or references to support this claim, which is a critical aspect of the score rubric. The feedback also fails to provide any logical reasoning that could be independently verified. Instead, it simply states the claim without any supporting details. This lack of evidence and logical reasoning makes the feedback less credible and does not meet the standards set by the score rubric. Therefore, the feedback does not respect the aspect of verifiable feedback, and as a result, it does not meet the criteria for a score of 1. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the aspect of generalization of observations to few-shot learners beyond Prototypical Networks, which is a critical point in the paper. The feedback is tailored to the content of the paper, and it clearly identifies the part of the document that is being discussed. The feedback is not a general comment for any scientific document, but rather a focused critique on a specific aspect of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
873,ICLR_2023_507,"Weaknesses
I encourage the authors to put forward the general notion of sparsity that is assumed across the paper (as defined in S ( w )
) early on in the introduction so that reader can follow the ideas put forward in Fig. 1.
One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent.
From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal)
By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?","1. One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent. From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal) By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?",nan,nan,nan,nan,nan,1,"The feedback provided in the review is actionable and offers clear and practical suggestions for the authors to improve their paper. It specifically addresses the use of MNIST, FashionMNIST, and CIFAR10 datasets, which is a relevant aspect of the review. The feedback encourages the authors to expand their dataset-DNN pairs, which is a direct and actionable recommendation. Furthermore, the feedback asks the authors to discuss the absolute limit of sparsity and the performance values at the extreme end of sparsity. This is a practical suggestion that guides the authors in addressing the issue of sparsity in their model. The feedback is not only relevant but also provides a clear path for the authors to follow in order to improve their paper. Therefore, the feedback is actionable and aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer encourages the authors to expand their dataset and discuss the sparsity limit of various networks, which is a positive approach to providing feedback. The feedback also asks for more information on the performance values and PQI at the extreme end of sparsity, which is a helpful way to guide the authors towards a more comprehensive understanding of their work. The tone of the feedback is positive and encouraging, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of dataset selection and the implications of sparsity in neural networks. The feedback suggests that the authors should expand their dataset and discuss the limits of sparsity, which aligns with the aspect of verifying the claims made in the paper. However, the feedback does not provide any evidence or references to support these claims, which is a key aspect of the score rubric. The feedback is based on logical reasoning, but it lacks the necessary references to external sources or internal references from the paper to be considered verifiable. Therefore, while the feedback is relevant and addresses the aspect, it does not meet the criteria for a score of 1 as it does not provide the necessary evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is highly specific and directly relevant to the content of the draft. It addresses the use of certain datasets in the evaluation of the proposed model, which is a critical aspect of the paper. The feedback also asks for further expansion of the dataset-DNN pairs and discusses the absolute limit of sparsity, which are specific points that are directly related to the paper's content. The feedback is tailored to the content of the paper, and it is clear that the feedback is addressing specific parts of the paper. Therefore, the feedback is highly specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2056,ARR_2022_18_review,"1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7. The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1, and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? 
See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda = 1.5 nA -> na (short vowel) ",2. No details on the. multi-task learning mentioned in Section 4.4 are available.,nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It merely states that there are no details on the multi-task learning mentioned in Section 4.4, without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issue, nor does it provide any direction or hints on what needs to be done. Therefore, it does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The feedback does not contain any negative or positive language, and it simply states that there are no details on the multi-task learning mentioned in Section 4.4. This feedback is not disrespectful or aggressive, and it does not attempt to encourage or discourage the authors. It is a straightforward statement of fact, which is why it is scored as 0. 

However, the feedback could have been more constructive by providing suggestions or asking questions to guide the authors in improving their paper. This would have made the feedback more helpful and positive, which would have resulted in a higher score. 

In conclusion, the feedback is neutral and factual, but it lacks the constructive and encouraging tone that would have made it more helpful and positive. Therefore, it is scored as 0. 
[RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. It fails to provide any evidence or references to support the claim that there are no details on multi-task learning in Section 4.4. The feedback does not cite any external sources or use logical reasoning to validate the claim. It simply states the absence of details without any supporting evidence. This lack of verifiable information and the absence of logical reasoning or internal references from the paper make the feedback not relevant to the review. Therefore, the feedback does not respect the aspect of verifiability, as it does not provide any evidence or references to support its claims. So the overall score is -1. 
[RESULT] -1",-1,"The feedback provided in the response is not specific enough to be relevant to the content of the draft. It mentions a section (4.4) but fails to provide any details on the multi-task learning aspect. This lack of specificity makes it difficult to identify which parts of the paper are being addressed in the feedback. The feedback does not provide any insight into the content of the draft, nor does it offer any constructive criticism or suggestions for improvement. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 

In conclusion, the feedback is not specific enough to be relevant to the content of the draft, and it does not respect the aspect of multi-task learning mentioned in Section 4.4. This lack of specificity and relevance to the content of the draft is why the feedback would receive a score of -1. 
[RESULT] -1"
1900,ARR_2022_334_review,"- There are some technical aspects of the paper that weren't clear to me:  * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
 * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
   * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. 
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) ",- There are some technical aspects of the paper that weren't clear to me:,nan,nan,nan,nan,nan,-1,"The feedback provided in the response is vague and lacks specificity. It mentions that there are ""some technical aspects of the paper that weren't clear to me,"" but it does not offer any concrete suggestions or recommendations for improvement. The feedback does not provide the author with a clear path for revision, nor does it hint at what needs to be done. It merely states that there are issues without giving any actionable advice. Therefore, based on the score rubric, the feedback is not actionable and does not respect the aspect of providing clear and practical suggestions for improvement. So the score is -1. 
[RESULT] -1",0,"The response provided is neutral and factual, which aligns with the score of 0. The feedback does not contain any negative or positive language, and it does not show any signs of respect or disrespect towards the authors. The tone is simply stating that there were some technical aspects that were not clear, without any further elaboration or commentary. This response does not encourage or discourage the authors, nor does it provide any constructive feedback. It is a straightforward statement of the reviewer's confusion, which is a neutral approach to feedback. Therefore, based on the score rubric, the response is neutral and does not meet the criteria for a score of 1 or -1. 
[RESULT] 0",0,"The feedback provided does not meet the criteria for a score of 1. It mentions that there are some technical aspects of the paper that were not clear to the reviewer, but it fails to provide any evidence or references to support this claim. The feedback does not cite any external sources or use logical reasoning to validate the reviewer's claim. It is a general statement without any supporting details, which is why it does not meet the criteria for a score of 1. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. So the score is 0. [RESULT] 0",-1,"The feedback provided is vague and lacks specificity. It does not mention any particular technical aspects of the paper, nor does it provide any concrete examples or points of confusion. This makes it difficult to identify which parts of the paper are being addressed in the feedback. The feedback does not follow the aspect of specificity, as it does not tailor the feedback to the content of the paper. Instead, it offers a general comment that could apply to any scientific document. Therefore, the feedback does not meet the criteria for specificity, and it is not relevant to the review. So the score is -1. 
[RESULT] -1"
2242,ACL_2017_699_review.json,"Weaknesses: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1. 
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally,    The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc. 
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845). ","4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the training data and the lack of topical details, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback simply states the problems without providing any guidance on how to address them. This lack of actionable advice makes the feedback less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it focuses on the lack of information in the review, which is a factual observation. The feedback does not show any signs of politeness or helpfulness, which are key aspects of the score rubric. It simply states the facts without any attempt to be positive or respectful. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not meet the criteria for a higher score. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback questions the use of all publications in training the baselines and the lack of topical details in the dataset. However, it fails to provide any evidence or logical reasoning to support these claims. The feedback does not cite any external sources or reference any parts of the paper, which is crucial for verifiable feedback. It also does not use logical reasoning that could be independently verified. Therefore, the feedback does not meet the requirements of the score rubric, as it does not provide any evidence or logical reasoning to support its claims. So the score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the number of documents used for training and the number of publications held out for training baselines. The feedback also points out the lack of topical details in the dataset, which is a critical aspect of the paper. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. The feedback is clear and directly addresses the aspects of the paper that are being discussed. Therefore, the feedback is in line with the score rubric, which emphasizes the importance of specificity in a review. So the overall score is 1. [RESULT] 1"
4173,NIPS_2020_1855,"Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as ""undecided"", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting. - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the ""raw"" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening? - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have ""training the neural representation"" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization? - More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.","- More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.",nan,nan,nan,nan,nan,0,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. The comment about the ""Algorithm 1"" box being superfluous and the content being obvious does not provide any guidance on how to address this issue. The feedback does not suggest any specific edits or changes that could be made to improve the paper. Instead, it merely states that the box is unnecessary without offering any constructive advice. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is polite and respectful, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is constructive and encourages the authors by acknowledging the quality of the paper. The feedback also provides a specific suggestion for improvement, which is a positive aspect. However, the feedback could have been more helpful by offering a clearer explanation of why the ""Algorithm 1"" box is considered superfluous and how it could be improved or removed. This would have made the feedback more constructive and actionable for the authors. Despite this, the feedback maintains a positive and respectful tone throughout. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The reviewer's claim that the ""Algorithm 1"" box is superfluous and a repetition is not supported by any evidence or references. The feedback does not cite any external sources or provide logical reasoning that could be independently verified. It is a general statement that lacks the necessary detail to be considered a verifiable claim. Therefore, the feedback does not respect the aspect of verifiability in the review. 

The feedback also fails to provide any constructive criticism or suggestions that could be used to improve the paper. Instead, it merely comments on the presence of the ""Algorithm 1"" box without offering any substantive feedback. This lack of constructive criticism further detracts from the quality of the feedback. 

In conclusion, the feedback provided in the response does not meet the criteria for a score of 1, as it does not support its claims with evidence or logical reasoning, nor does it provide any constructive criticism or suggestions. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiability. 
[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the paper. The reviewer's comment about the ""Algorithm 1"" box being superfluous is a general observation that does not directly address any specific part or method in the paper. The feedback lacks the detail and direct relevance to the content of the draft that is required for a higher score. It does not provide any insight into how the box contributes to the overall paper or how it could be improved. The feedback is more of a general comment rather than a targeted critique. Therefore, based on the score rubric, the feedback does not meet the criteria for specificity and relevance to the paper. So the overall score is 0. [RESULT] 0"
1587,ICLR_2023_2147,"Weakness
While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it.
The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained.
I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.).
While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean).
Question to authors
During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error.
I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample m
has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level ( ϵ m , k
) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable.
[1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188.
[2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594.
Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?","2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188. [2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback merely states the relevance of the aspect without providing any direction or hints of what needs to be done. It lacks the necessary detail and guidance that would help the author in addressing the issues identified in the review. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The response provided is a citation of a scientific paper, which is not a feedback paragraph. It does not contain any tone or manner of delivery, which is the focus of the score rubric. The response is purely factual and does not address the aspect of politeness or the manner in which feedback is delivered. Therefore, it is not possible to evaluate the response based on the score rubric. 

The feedback paragraph is missing, which is a critical component of the task. Without the feedback paragraph, it is impossible to assess the tone or manner of delivery, which is the core of the score rubric. 

In conclusion, the response does not meet the requirements of the task and cannot be evaluated based on the score rubric. 
[RESULT] 0",0,"The feedback provided does not meet the criteria for a score of 1. It lacks any form of evidence or logical reasoning to support the claims made. The feedback does not reference any part of the paper or external sources, which is crucial for verifying the reviewer's claims. The absence of any supporting evidence or logical reasoning makes the feedback unverifiable and therefore not relevant to the review. This is why the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",-1,"The feedback provided does not meet the criteria for specificity as outlined in the score rubric. The response does not mention any specific parts or methods in the paper, nor does it address any particular aspect of the content. It is a general comment that could apply to any scientific document, and it does not provide any tailored feedback to the content of the paper. Therefore, it is not relevant to the review and does not respect the aspect of the paper. The feedback lacks the necessary detail and direct relevance to the content of the draft, which is essential for a review to be effective. So the overall score is -1. [RESULT] -1"
3293,NIPS_2020_1480,"1. The novelty of Theorem 1 is not entirely clear. The authors argue that the classical another parametrization B, Theta = (C Omega^{1/2}, Omega^{1/2}), which also enables joint convexity. Therefore, Theorem 1 doesn't appear to be a significant result in itself. This may be a lemma instead. 2. It is not clear why the classical parametrization does not allow penalty functions. One can include penalties on C by using the classical parametrization and levering C=B Theta^{-1}, e.g., || B Theta^{-1}||_{1,2} where 1,2 is the group L1 penalty. 3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage. 4. This work needs a more detailed comparative analysis in order to prove the superiority of the proposed approach. In particular, paper is lacking an adequate computational complexity and run-time analysis with respect to the existing methods, e.g. the standard parametrization and penalty approach. A similar weakness also exists in the numerical results section, where comprehensive empirical comparisons are lacking. The supplementary material has a numerical table, which shows a very incremental improvement and is not conclusive. 5. In proving Theorem 3, the authors employ standard methods such as restricted eigenvalues. These methods are already known to extend to non-convex objective functions and does not specifically hold for the claimed convex formulation.","3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage.",nan,nan,nan,nan,nan,0,"The feedback provided in the review does not offer any actionable suggestions or recommendations that the author can use to improve their paper. It merely points out the inconsistency in the authors' argument regarding the advantage of convexity and the introduction of non-convex regularizers. The feedback does not provide any guidance on how to address the identified issues, nor does it suggest any concrete steps or edits that should be made. It lacks the specificity and clarity that would make it actionable. Therefore, based on the score rubric, the feedback is not actionable and does not meet the criteria for providing a clear path for revision. 
[RESULT] 0",0,"The feedback provided in the response is critical and somewhat dismissive of the authors' approach. The tone is not overtly negative or disrespectful, but it does not convey a sense of encouragement or respect for the authors' work. The feedback focuses on the perceived flaws in the authors' methodology, without acknowledging any potential merits or contributions. This approach may be perceived as lacking in constructiveness and could potentially discourage the authors. The feedback does not provide any constructive suggestions or acknowledge any potential benefits of the authors' approach. Therefore, the feedback does not fully respect the aspect of politeness and constructiveness, which is crucial in scientific paper reviews. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The reviewer's claim about the authors' argument for the advantage of convexity is not supported by any evidence or references. The reviewer's opinion that the introduction of non-convex regularizers ""destroys the purpose"" is a subjective statement that lacks logical reasoning or external validation. Furthermore, the reviewer's assertion that there is no empirical comparison with the standard parametrization is not substantiated by any reference to the paper or external sources. The feedback does not provide any evidence or logical reasoning to support the claims made, which is a key requirement for a score of 1 according to the score rubric. Therefore, the feedback does not meet the criteria for a score of 1 and instead aligns with the criteria for a score of -1. 
[RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the authors' argument about the advantage of convexity in section 2.1 and their subsequent introduction of non-convex regularizers. The feedback also critiques the lack of empirical comparison with the standard parametrization, which is a key aspect of the paper. The specificity of the feedback is evident as it directly points out the contradiction in the authors' argument and the absence of empirical evidence. This makes it clear which sections and elements in the paper are being addressed. Therefore, the feedback is tailored to the content of the paper, and not a general comment for any scientific document. So the score is 1. [RESULT] 1"
1870,ARR_2022_14_review,"-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here. - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. 
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. ","- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is relevant to the paper, as it identifies specific areas where the paper could be improved. However, the feedback lacks actionable suggestions that the author can use to improve their paper. While it mentions the need for more experimental details and a reference for the claim made in the introduction, it does not provide clear steps or recommendations on how to address these issues. The feedback is more of a critique rather than a constructive guide for the author to follow. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer has identified specific areas where the paper could be improved, such as the lack of experimental details and the need for a reference to support the claim made in the introduction. This approach is helpful and encourages the authors to refine their work. The feedback is also positive, as it acknowledges the potential of the extract-then-generate approach, even while suggesting improvements. The tone is respectful, as it does not criticize the authors but rather provides guidance on how to enhance the paper. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. While the feedback does mention the need for references to support the claim made in the introduction, it fails to provide any specific references or evidence to back up this claim. The feedback also points out the lack of experimental details, but it does not offer any concrete suggestions for how to improve these details. The feedback is relevant to the review, but it does not provide the necessary support for its claims, which is a key aspect of the score rubric. Therefore, the feedback does not fully respect the aspect of verifiability, as it does not provide the necessary evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the lack of experimental details and the need for a reference in the introduction part of the paper. The feedback is tailored to the content of the paper, and it is clear which sections and elements in the paper are being addressed. The feedback is not a general comment for any scientific document, but rather a detailed critique of the paper's specific aspects. Therefore, the feedback meets the criteria of the score rubric, and it is evident that the feedback is relevant to the review and is followed or respected in the review. So the score is 1. [RESULT] 1"
2144,ARR_2022_61_review,"1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. 
1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. ","2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the aspect of significance testing. It specifically asks about the testing done for the results on automatic metrics and human evaluation, which is a crucial part of any scientific study. However, the feedback lacks actionable suggestions or recommendations that would guide the author in addressing the issue. It does not offer concrete steps or hints on what needs to be done to improve the paper. The feedback is more of a question than a directive, which makes it less useful for the author. Therefore, the feedback does not fully meet the criteria of being actionable. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone is respectful and the feedback is aimed at encouraging the authors to provide more detailed information about the significance testing done for the results on automatic metrics and human evaluation. The feedback is also positive, as it suggests that the results could be solidified with more information. The feedback is helpful, as it points out a specific aspect that could be improved in the review. The feedback is also positive, as it implies that the results are close to the method’s metrics, which is a positive aspect. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review, as it asks for the significance testing done for the results on automatic metrics and human evaluation. This aspect is crucial in scientific reviews, as it helps to validate the claims made in the paper. The feedback also suggests that the close values between the baseline and the method's metrics could help solidify the claims, which is a logical reasoning that can be independently verified. However, the feedback does not provide any evidence or references to external sources, which would have strengthened the claim. Therefore, while the feedback is relevant and follows the aspect to some extent, it does not fully respect the aspect by providing the necessary evidence or references. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the significance testing of the results on automatic metrics and human evaluation. The feedback is tailored to the content of the paper, and it is clear which sections, statements, or elements in the paper are being addressed. The feedback is not a general comment for any scientific document, but rather a specific critique of the paper's methodology. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
1678,ICLR_2023_1654,"Weaknesses:
I believe the paper has a couple of fundamental weaknesses as it stands that should be addressed before it could be accepted at ICLR. 1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons. 2. The use of he Oracle twice is circular - therefore invalidates the claims of performance; there are some ways around this that the authors could try More minor weaknesses: 3. the reference used to justify the evaluation metrics is Hoffman et al 2022 - but this paper is about optimising small molecules - which are v different in ""seq distance to function"" relationships - this is especially important in relation to the point about the Novelty measure above.
4. AlphaFold is not at all appropriate to support the claim of functional sequence optimisation - there are may mutations that will cause a protein to unfold that Alpha fold will predict as having almost exactly the same structure as it will align etc - therefore it proves nothing ( From their own FAQ page ""AlphaFold has not been validated for predicting the effect of mutations. In particular, AlphaFold is not expected to produce an unfolded protein structure given a sequence containing a destabilising point mutation."" And there are papers writing about this eg Pak et al 2021","1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is actionable, but it lacks in giving a direction or hints of what needs to be done. The reviewer correctly identifies the issues with the evaluation metrics, such as the lack of a measure of distance from training beyond a single mutation. However, the feedback does not provide specific, clear, and practical suggestions that the author can use to improve their paper. Instead, it merely states the problems without offering concrete steps or recommendations that guide the author in addressing those issues. The feedback also fails to provide a clear path for revision, which is essential for the author to improve their paper. Therefore, the feedback is not as useful as it could be, and it does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] -1",-1,"The feedback provided is quite critical and does not seem to respect the authors' efforts. The tone is negative and the feedback is not constructive. It points out the shortcomings of the paper without offering any constructive criticism or suggestions for improvement. The feedback also seems to dismiss the authors' work as being of poor quality, which could be seen as disrespectful. The feedback does not encourage the authors or acknowledge their contributions positively. Instead, it focuses on the perceived flaws of the paper and the reviewer's own opinions on the matter. This approach does not align with the rubric's requirement for a polite and respectful tone. Therefore, based on the rubric, the feedback is not respectful or constructive, and it does not encourage the authors. So the score is -1. 
[RESULT] -1",-1,"The feedback provided in the response is not supported by any evidence or references. The reviewer makes several claims, such as the inadequacy of the evaluation metrics and the poor performance of the model, but these claims are not substantiated with any data or references to the paper. The reviewer also suggests that the model's performance is poor because it is only 1% better than a random single mutation, but this claim is not backed up by any evidence or logical reasoning. Furthermore, the reviewer's suggestion that the model's metrics and/or model are poor is not supported by any references to the paper or external sources. The feedback does not meet the criteria for verifiable feedback as it lacks any evidence or logical reasoning to support its claims. Therefore, the feedback does not meet the standards set by the score rubric. 
[RESULT] -1",1,"The feedback provided is quite specific and directly addresses the content of the draft. It critiques the evaluation metrics used in the paper, pointing out the lack of consideration for the distance from training data. The feedback also provides a detailed explanation of why this is a significant issue, referencing the difficulty of generating sequences that are different from natural or training examples. Furthermore, it suggests a specific improvement to the evaluation metrics, which is to include the distance from training data as a factor in the evaluation. This feedback is tailored to the content of the paper and not a general comment for any scientific document. It is clear that the feedback is specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. Therefore, the feedback is well-aligned with the score rubric's requirements for specificity and relevance to the content of the draft. 
[RESULT] 1"
4544,NIPS_2020_1461,"1. The title seems misleading, the method from my understanding is a way to use GNNs for learning natural laws, or understand GNNs in terms of symbolic models, rather than extracting symbolic models from general deep learning models. If the latter is their main claim, it would be helpful if the authors show the utility of this model on more general tasks, other than natural laws. 2. The paper lacks a strong related work section, missing key details about models that have been used previously for relevant tasks. For example, the authors have not discussed how other papers like Neural Relational Inference or using Generative Models for their tasks. 3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model. 4. It would be helpful if the authors address the natural question how utilizing this model for a general deep learning model is more interpretable or generalizable especially in the cases of noise? 5. The paper needs to add more details in the main draft. Key details are in the appendix, the authors should include majority of Section A.1 in the main paper rather than the appendix. The simulation details should be explained more, are the authors generating time series datasets, the number of samples, length of the time series, how the trajectory is processed etc. While some details are covered in the appendix, these details are essential to understand the task and model.","3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the paper's main claim of interpretability and generalization. However, it falls short in offering actionable suggestions for improvement. The feedback points out the lack of robustness or generalization on the simulation data, but it does not provide any specific guidance on how to address this issue. The feedback could have been more helpful by suggesting concrete steps or recommendations that the authors could follow to improve their paper. For example, the feedback could have suggested that the authors should test their model on a wider range of data or that they should provide more detailed explanations of their results. Without these actionable suggestions, the feedback is less useful to the authors. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback addresses the main claim of the paper and provides a critique that is both specific and relevant to the paper's content. The feedback also suggests an alternative approach that could strengthen the paper's argument, which is a positive and encouraging aspect. The tone of the feedback is neither negative nor aggressive, and it maintains a professional and constructive demeanor throughout. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",-1,"The feedback provided in the response is relevant to the review, as it addresses the paper's main claim of interpretability and generalization. However, the feedback does not provide any evidence or references to support the claim that the authors do not show robustness or generalization on the simulation data. This lack of evidence or references makes the feedback less convincing and does not meet the criteria for verifiable feedback. The feedback also fails to cite external work or provide logical reasoning that could be independently verified. Therefore, the feedback does not fully respect the aspect of verifiability, as it does not provide the necessary support for its claims. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the main claim of the paper, which is interpretability and generalization, and it critiques the authors' approach to demonstrating these claims. The feedback points out that while the authors show that their model fits the ground truth equation, they do not demonstrate robustness or generalization on the simulation data. This critique is specific and directly related to the content of the paper, making it clear which sections or elements in the paper are being addressed. The feedback is tailored to the content of the paper, and it is not a general comment for any scientific document. Therefore, the feedback meets the criteria for a score of 1 according to the score rubric. 
[RESULT] 1"
2574,NIPS_2019_175,"Weaknesses: 1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.  ","5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It merely points out the presence of errors in the English language without offering any specific, clear, or practical suggestions for improvement. The feedback does not guide the author in addressing the issues, nor does it provide any concrete steps or recommendations. The response to the concerns is also vague and does not offer any actionable advice. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is not aligned with the expected tone of a scientific review. The language used is overly critical and lacks the necessary respect and constructiveness. The feedback begins with a negative comment about the English language, which is not relevant to the review of the scientific paper. Furthermore, the feedback does not provide any constructive criticism or suggestions for improvement. Instead, it ends with a hope that the authors can update the paper, which is not a respectful or helpful way to conclude a review. The tone of the feedback is negative and disrespectful, which is not appropriate for a scientific review. Therefore, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of politeness and constructiveness in the feedback. So the overall score is -1. [RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback mentions errors in the English language, but it fails to provide any evidence or references to support this claim. The feedback also suggests that the authors should update the paper with additional information, but again, it does not provide any evidence or references to support this suggestion. The feedback does not follow the score rubric's requirement for verifiable feedback, as it does not cite external sources or use logical reasoning that can be independently verified. Therefore, the feedback does not respect the aspect of verifiable feedback, and it is not relevant to the review. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response is not specific to the content of the paper. It mentions ""English is full of errors throughout"" but fails to identify any specific errors or how they impact the paper's content. The feedback also lacks detail in addressing the authors' response, merely stating that it ""did a decent job"" without providing any specific examples or suggestions for improvement. Furthermore, the feedback does not directly address the aspect of the paper that was being reviewed, which is crucial for a review to be effective. The lack of specificity and direct relevance to the content of the paper makes it difficult to identify which parts of the document are being discussed in the feedback. Therefore, based on the score rubric, the feedback is not specific and does not respect the aspect of the paper. So the overall score is -1. 
[RESULT] -1"
2182,ACL_2017_614_review.json,"Weaknesses: - I don't understand effectiveness of the multi-view clustering approach. 
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                  - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub? ","- The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).",nan,nan,nan,nan,nan,1,"The feedback provided in the review is actionable, as it offers specific suggestions for improvement. The reviewer identifies the lack of clarity in the paper and provides a clear direction for the author to follow by suggesting the addition of forward/backward pointer references to sections. This recommendation directly addresses the issue of disjointed sections and provides a practical solution for the author to implement.

Furthermore, the reviewer points out the need for editing in the multi-view clustering section, specifically mentioning the out-of-order subsections and missing citations. This feedback not only identifies the problem but also provides a clear path for the author to revise the section, making the feedback highly actionable.

In contrast, the feedback does not suggest any specific steps or edits for the other sections of the paper, which makes it less actionable compared to the feedback provided for the multi-view clustering section.

Therefore, based on the score rubric, the feedback is actionable and provides clear and practical suggestions for the author to improve their paper. The feedback is relevant to the review and respects the aspect of actionability. So the overall score is 1. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer identifies specific issues within the paper, such as the lack of clarity in the connections between sections and the need for editing in the multi-view clustering section. The feedback is delivered in a manner that is both respectful and encouraging, suggesting practical solutions like adding forward/backward pointer references and editing the subsections. The reviewer's tone is positive and constructive, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it addresses specific issues within the paper. The reviewer has identified areas where the paper's clarity and organization could be improved, such as the connections between sections and the order of subsections. However, the feedback lacks the necessary evidence to support the claims made. The reviewer suggests adding forward/backward pointer references and editing the multi-view clustering section, but does not provide any references to external sources or logical reasoning to back up these suggestions. This makes the feedback less verifiable and less convincing. Therefore, while the feedback is relevant, it does not fully meet the criteria for a score of 1, as it does not provide the necessary evidence to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is quite specific, addressing particular sections of the paper and providing clear examples of where the paper falls short. The reviewer has identified specific issues with the connections between sections and the order of subsections in the multi-view clustering section. Furthermore, the reviewer has pointed out the absence of citations in certain parts of the paper. This level of detail and specificity aligns well with the score rubric's criteria for a score of 1. The feedback is directly relevant to the content of the draft, and it is easy to identify which parts of the document are being discussed in the feedback. Therefore, the feedback is tailored to the content of the paper, and not a general comment for any scientific document. So the overall score is 1. [RESULT] 1"
4413,NIPS_2020_1552,"* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs. * Relation to relevant prior work is not mentioned and elaborated. For example, Rajasegaran, et al. ""Random path selection for continual learning."" NeurIPS'19 also propose a similar masking based approach to learn non-overlapping paths for dissimilar tasks. Similarly, PathNet (Evolution Channels Gradient Descent in Super Neural Networks) selectively masks out irrelavent model paramters. These papers should be cited and disucssed (preferably compared against) in this manuscript. * To my understanding, the notion of similar and dissimilar tasks is not accurate. E.g., the prior works on task incremental learning have both sets of similar and dissimilar tasks. (E.g., consider CIFAR100 classes in GEM - NeurIPS'17). In fact the considered set of similar and dissimilar tasks is not too different from the ones considered in earlier works. Specifically, consider a seminal work from Li & Hoeim, ""Learning without forgetting"" (TPAMI), where different datasets such as ImageNet/Places365/VOC/CUB/Scenes/MNIST are considered in continual learning experiments). Nevertheless, the proposed splits and dataset choices should be properly motivated and the authors should also report some experiments on previously considred protocols for fair benchmarking against existing methods. * The annealing strategy is somewhat similar to controller proposed in iTAML (iTAML : An Incremental Task-Agnostic Meta-learning Approach - CVPR'20). * The approach assumes that the task ID is known beforehand. Although this is consistent with some prior works, isn't it a bit restrictive in practical settings? It would be good to explain some application scenarios where tasks ID can be known to motivate the readers. * Equation 3 is wrong, it should be explicitly written. * The caption of Figure 1 should have some description for the MTCL architecture (a) as well.","* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the aspect of dataset choice. However, it falls short in offering actionable suggestions. The feedback merely points out the lack of explanation for the choice of datasets, but it does not provide any guidance on how to improve the paper. It lacks the specificity and clarity that would make it actionable. The feedback does not suggest any concrete steps or recommendations that the author could use to address the issue. Therefore, it does not meet the criteria for actionability as outlined in the score rubric. 

So the overall score is -1. [RESULT] -1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone of the feedback is respectful and it encourages the authors to provide a clearer rationale for their dataset choices. The feedback is also positive, as it suggests that the authors should elaborate on their choices, which implies that the feedback is trying to help the authors improve their work. The feedback does not contain any negative or aggressive language, which is in line with the rubric's criteria for a score of 1. Therefore, based on the rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",-1,"The feedback provided in the response is relevant to the review, as it addresses the aspect of dataset choice. However, the feedback does not provide any evidence or references to support the claim that the dataset choice is arbitrary. The feedback simply states that the dataset choice is arbitrary without offering any logical reasoning or external sources to back up this claim. This lack of evidence and support for the claim makes the feedback less credible and does not meet the criteria for a score of 1. Therefore, the feedback is not fully respected in the review, as it does not provide the necessary support for its claims. So the overall score is -1. [RESULT] -1",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses the choice of datasets, FEMNIST and FCelebA, and questions the rationale behind their selection. This specificity aligns with the score rubric's requirement for detailed and directly relevant feedback. The feedback is tailored to the content of the paper, and it is clear which sections, statements, or elements in the paper are being addressed. The feedback does not make general comments about the paper but instead focuses on a particular aspect, which is the dataset choice. Therefore, the feedback is specific and directly relevant to the content of the draft, and it is easy to identify which part of the document is being discussed in the feedback. So the score is 1. [RESULT] 1"
4950,NIPS_2020_1842,"1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper. 2. The claimed effect of ProtoMod needs more evaluations. The authors provide some qualitative results in Figure 3 and Fig Q.3 & Q.4 (from supp.) to show the proposed ProtoMod can localize the attributes like 'black back' and 'yellow breast'. What concerns me is whether the model is indeed learning the specific attributes (black back) or only the partial semantic like 'black' or 'back'. The part localization experiments (Fig 2 and Table 3) may partially show the model can learn the part information ('back', 'belly') from attributes but does not show if the model can distinguish between 'black back' or 'yellow back'. Neither the attribute similarity maps from Fig 3 and Fig Q.3 & Q.4. I think some qualitative results of the same images with locations of attributes only differ partially (e.g. 'black back' vs. 'yellow back' vs. 'black breast') can further verify whether the ProtoMod is learning the specific attributes or just the color or parts. --------- Ref: R1: Zhu, Pengkai, Hanxiao Wang, and Venkatesh Saligrama. ""Learning classifiers for target domain with limited or no labels."" ICML 2019. --------- Updates: the authors address my concerns in the rebuttal. I raise my score to accept.","1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper.",nan,nan,nan,nan,nan,1,"The feedback provided in the review is actionable, as it not only identifies the issue of limited novelty but also offers a clear direction for the authors to follow. The feedback suggests that the authors should elaborate on the differences between their work and other similar works, which is a practical step that the authors can take to improve their paper. This feedback goes beyond merely identifying the issue and provides a specific recommendation that guides the authors in addressing the issue. The feedback is also clear and concise, making it easy for the authors to understand and act upon. Therefore, based on the score rubric, the feedback is actionable and relevant to the review. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the work's contribution to the field of zero-shot learning but also points out the limitations and the need for further clarification on the paper's specific novelties. This approach is encouraging and aims to help the authors improve their work. The feedback is delivered in a manner that is both respectful and constructive, which is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",1,"The feedback provided in the response is relevant to the review and is supported by references to external sources. The reviewer has cited specific works, such as [58] and [R1], which are directly related to the topic of the paper. This demonstrates a clear understanding of the subject matter and provides a basis for the reviewer's claims. The feedback also encourages the authors to elaborate on the differences between their work and the cited works, which is a logical suggestion that can be independently verified. The response aligns with the score rubric's criteria for a score of 1, as it imposes a certain claim and then supports it using external references and logical reasoning. Therefore, the feedback is well-supported and relevant to the review. 
[RESULT] 1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the novelty of the paper and compares it to other works, which is a clear indication of the reviewer's understanding of the paper's content. The feedback also provides a clear direction for the authors to improve their paper by suggesting they elaborate on the differences between their work and previous works. This specificity in the feedback ensures that the authors can identify the exact parts of their paper that need to be addressed. The feedback is not a general comment but is tailored to the content of the paper, which is why it meets the criteria for a score of 1. 
[RESULT] 1"
1345,ICLR_2023_1035,"Weaknesses: 1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc.
The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments.
I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e. 600 episodes for each dataset. This is helpful for indicating the chance of handling such hard tasks in real-life scenarios where hard and easy tasks are mixed.
[1] Powering Finetuning for Few-shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. AAAI 2022 [2] Cross-domain few-shot learning with task-specific adapters. CVPR 2022 [3] Exploring Efficient Few-shot Adaptation for Vision Transformers. TMLR 2022","1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc. The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments. I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It does not offer specific, clear, and practical suggestions that the author can use to improve their paper. Instead, it merely points out the lack of insight in the analysis and suggests the use of other metrics and adaptation methods without providing any concrete steps or recommendations. The feedback also questions the validity of the methods used, but it does not provide any guidance on how to address these concerns. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is not only relevant to the review but also respectful and constructive. The tone of the feedback is polite and helpful, as it offers suggestions for improvement and acknowledges the efforts of the authors. The feedback also encourages the authors to consider additional metrics and adaptation methods, which shows a positive attitude towards the paper. The feedback is not aggressive or disrespectful, and it maintains a professional tone throughout. Therefore, based on the score rubric, the feedback is of high quality and aligns well with the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiable feedback. The reviewer makes several claims without providing any evidence to support these claims. For instance, the reviewer questions the analysis provided by the authors and suggests that other metrics and adaptation methods should be used. However, the reviewer does not cite any external sources or provide any logical reasoning to back up these claims. The feedback also lacks references to parts of the paper, which would have made the claims more verifiable. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. 

[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the paper. It fails to mention specific parts or methods in the paper, making it difficult to identify which sections, statements, or elements in the paper are being addressed. The feedback is more of a general commentary on the topic rather than a targeted critique of the paper. It does not provide a clear connection between the feedback and the content of the paper, which is crucial for a review. The lack of specificity in the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. Therefore, the feedback does not meet the requirements of the score rubric. 
[RESULT] 0"
2214,ACL_2017_66_review.json,"Weaknesses: The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).
- General Discussion: The current presented approach, in comparison to existing approaches, is promising. ","- General Discussion: The current presented approach, in comparison to existing approaches, is promising.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is quite general and does not offer specific, clear, and practical suggestions for the author to improve their paper. It merely states that the current approach is promising without providing any concrete steps or recommendations. This lack of actionable comments makes the feedback less useful for the author, as it does not guide them in addressing the issues or improving their paper. Therefore, based on the score rubric, the feedback is not actionable and does not respect the aspect of providing specific guidance for the author. So the score is -1. 
[RESULT] -1",1,"The feedback provided is polite and respectful, which aligns with the score rubric's requirement for a score of 1. The tone is positive and encouraging, which is evident in the phrase ""promising."" The feedback also respects the aspect of the review by acknowledging the current approach and comparing it to existing approaches. This comparison is a constructive way to provide feedback, as it gives the authors a clear understanding of where their work stands in relation to others. The feedback is also helpful, as it provides a positive evaluation of the authors' work. Therefore, based on the score rubric, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. It lacks the necessary verifiable evidence to support the claim that the current approach is promising in comparison to existing approaches. There are no references to external sources, parts of the paper, or logical reasoning that could be independently verified. The feedback is a general statement without any supporting details, which is why it does not meet the criteria for a score of 1. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. 
[RESULT] 0",0,"The feedback provided in the response is quite general and does not offer specific details or comments that would directly relate to the content of the draft. It lacks the specificity required to effectively critique the paper. The feedback does not mention any specific parts, statements, or elements in the paper, which makes it difficult to identify the parts that are being addressed in the document. This lack of specificity does not align with the score rubric's requirement for detailed and directly relevant feedback. Therefore, the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. 
[RESULT] 0"
4320,NIPS_2020_386,"The weaknesses are : 1. It is not clear how the reparametrization differ from the original parametrization with low rank covariance in terms of predictive performance. 2. It is not clear why the more expressive diagonal covariance is less predictive than the scaled identity covariance. 3. Why the latter covariance is more computationally efficient than the former for deep neural networks? 4. Given that the ELRG-VI has worse accuracy as in the Modern CNNs, what is the advantage of using this approximate posterior? 5. Evaluation of the quality of the posterior approximation by predictive performance is not appropriate as in many cases MAP can give more accurate predictions.",3. Why the latter covariance is more computationally efficient than the former for deep neural networks?,nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The question posed in the feedback is relevant to the review, but it does not provide any direction or hints of what needs to be done. The feedback simply states a question without offering any guidance or recommendations. This lack of actionable content makes the feedback less useful for the author seeking to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is neither negative nor positive, and it does not show any signs of respect or encouragement towards the authors. The feedback simply states a question without offering any constructive criticism or positive reinforcement. It is a straightforward inquiry about the computational efficiency of different covariance methods in deep neural networks, which is a relevant aspect to the review. However, the feedback does not engage with the authors or provide any additional insights that could help the authors improve their work. Therefore, based on the score rubric, the feedback is neutral and does not demonstrate any particular level of respect or constructiveness. 
[RESULT] 0",1,"The feedback provided in the response is relevant to the review and directly addresses the question posed in the instruction. It asks for an explanation of why the latter covariance is more computationally efficient than the former for deep neural networks. This question is directly related to the content of the paper and requires a detailed explanation, which is a logical claim that can be independently verified. The feedback does not provide any external references or internal references from the paper, but it does follow the aspect of the review by asking for a logical explanation. Therefore, the feedback is relevant and follows the aspect of the review, but it does not provide any evidence or references to support the claim. So the overall score is 1. [RESULT] 1",1,"The feedback provided in the response is highly specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the computational efficiency of different covariance methods in deep neural networks. The feedback is tailored to the content of the paper, and it is clear that the reviewer has considered specific parts of the paper, such as the methods used for the neural networks. The feedback is not a general comment for any scientific document, but rather a focused critique on a specific point in the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5685,NIPS_2018_639,"Weakness: - I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.  - How does the reader know that the reward definitions of rho for this tasks necessitates a non-convex reward function. Surveillance and data collection has been studied in POMDP context by many papers. Fortunately/unfortunately, many of these papers show that the increase in the reward due to a rho based PWLC reward in comparison to a corresponding PWLC state-based reward (R(s,a)) is not that big. (Papers from Mykel Kochenderfer, Matthijs Spaan, Shimon Whiteson are some I can remember from top of my head.) The related work section while missing from the paper, if existed, should cover papers from these groups, some on exactly the same topic (surveillance and data collection).  - This basically means that we have devised a new method for solving non-convex value function POMDPs, but do we really need to do all that work? The current version of the paper does not answer this question to me. Also, follow up question would be exactly what situation do I want to use the methodology proposed by this paper vs the existing methods.  In terms of critisim of significance, the above points can be summarized as why should I care about this method when I do not see the results on problem the method is supposedly designed for.  ","- I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is not actionable. It identifies a problem with the experimental results but fails to offer any specific, clear, or practical suggestions for improvement. The feedback merely questions the absence of experiments in certain settings without providing any guidance on how to address this issue. It lacks the necessary direction or hints that would help the author in revising their paper. The feedback does not fulfill the criteria of actionability, as it does not provide the author with a clear path for revision. Therefore, the score is -1. 
[RESULT] -1",-1,"The feedback provided is quite critical and does not maintain a respectful tone. The reviewer expresses a lack of conviction in the experimental results and questions the absence of experiments in certain settings. However, the tone used is somewhat confrontational, which could be perceived as disrespectful. The reviewer's language, such as ""not quite useful,"" could be seen as dismissive of the paper's efforts. This approach does not encourage the authors or provide constructive feedback. The feedback does not align with the rubric's requirement for a polite and respectful tone. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] -1",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback criticizes the experimental results of the paper, but it fails to provide any evidence or logical reasoning to support its claims. The feedback merely questions the absence of experiments in certain settings without offering any concrete evidence or references to the paper or external sources. This lack of verifiable support for the claims made in the feedback is why it does not meet the criteria for a score of 1. The feedback does not respect the aspect of verifiable feedback, as it does not provide any evidence or logical reasoning that can be independently verified. Therefore, the feedback is not relevant to the review and does not respect the aspect of verifiable feedback. So the score is -1. 
[RESULT] -1",1,"The feedback provided is quite specific and directly addresses the content of the paper. It critiques the experimental results and the examples used to motivate the case for the solution. The feedback also questions the absence of experiments in certain settings, which is a direct critique of the paper's content. This specificity aligns with the score rubric's requirement for detailed and directly relevant feedback. The feedback is tailored to the content of the paper, and it is clear which sections and elements in the paper are being addressed. Therefore, the feedback is relevant and specific, and it respects the aspect of the review. So the score is 1. [RESULT] 1"
304,ICLR_2022_113,"Weaknesses: - The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive? - The method focuses on learning cluster granularity for the object only, and not for the background. - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)
A few comments on the text: - The phrase ""coarse-grained images"" is inaccurate, the ""coarse-grained"" adjective should refer to the clustering and not the images (in the intro). - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro. - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)",- It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline) A few comments on the text:,nan,nan,nan,nan,nan,-1,"The feedback provided in the review does not offer any actionable suggestions or recommendations. It merely states that the transformation matrix is used without explaining why it is used or how it contributes to the overall functionality of PerturbGAN. The feedback does not provide any guidance on how the author can improve their paper based on the aspect of the transformation matrix. It lacks specificity and clarity, which are essential for actionable feedback. Therefore, the feedback does not meet the criteria of being actionable, and it does not respect the aspect of the transformation matrix. So the score is -1. 
[RESULT] -1",0,"The feedback provided is neutral and factual, which aligns with a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. The feedback focuses on the relevance of the transformation matrix in the PerturbGAN pipeline, but it does not provide any constructive criticism or suggestions for improvement. The feedback is more of a statement of confusion rather than a critique or a positive reinforcement. Therefore, based on the score rubric, the feedback is neutral and does not meet the criteria for a higher score. 
[RESULT] 0",-1,"The feedback provided does not offer any evidence or logical reasoning to support the claim that the transformation matrix is used without clear purpose. The feedback merely states that the transformation matrix is used without explaining why this is significant or how it contributes to the overall functionality of PerturbGAN. This lack of detail and support for the claim makes the feedback less reliable and informative. The feedback does not provide any references to external sources or parts of the paper, which would have strengthened the argument. Therefore, the feedback does not meet the criteria for verifiable feedback as outlined in the score rubric. 

In conclusion, the feedback does not provide any evidence or logical reasoning to support the claim, and it does not reference any external sources or parts of the paper. This makes the feedback less reliable and informative, and it does not meet the criteria for verifiable feedback. So the score is -1. 
[RESULT] -1",0,"The feedback provided does not meet the criteria for specificity. It fails to address specific parts or methods in the paper, making it difficult to identify which sections or elements in the paper are being discussed. The feedback is vague and does not provide any concrete or actionable insights for the authors of the paper. It merely states that the transformation matrix is used without explaining why it is used or how it contributes to the overall functionality of the PerturbGAN model. This lack of specificity and detail in the feedback does not align with the score rubric's requirement for specificity in a review. Therefore, the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. 
[RESULT] 0"
1661,ICLR_2023_3449,"Weaknesses
1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf","3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.). [1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It asks a question about the experiment setup in Section 3.3, but it does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback does not provide any direction or hints of what needs to be done. It simply states a question without any guidance or recommendations. Therefore, the feedback does not meet the criteria of being actionable, and it does not enhance the usefulness of the review for the author. So the score is 0. [RESULT] 0",0,"The feedback provided in the response is direct and to the point, which is a positive aspect. However, the tone of the feedback could be perceived as slightly abrupt, which might not be as respectful or encouraging as it could be. The feedback does not include any negative or disrespectful language, but it also lacks the warmth and positivity that could make the authors feel more supported. The feedback is factual and does not ignore the aspect in question, but it could be more constructive in its approach. Therefore, the feedback is neutral and factual, but it could be improved in terms of tone and encouragement. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it asks for specific details about the experiment setup, which is a critical aspect of the paper. However, the feedback does not provide any evidence or references to support the claim. It is a direct question that requires an answer from the paper, but it lacks the necessary verification or citation of external sources. This makes the feedback less reliable and does not meet the criteria for verifiable feedback as outlined in the score rubric. Therefore, the feedback is not fully respected in the review, as it does not provide the necessary support for its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It asks for details about the experiment setup, specifically mentioning data augmentation methods and learning rate, which are critical aspects of the paper. This shows that the reviewer has carefully considered the content of the paper and has provided feedback that is tailored to the specifics of the draft. The feedback is not a general comment but is focused on the specific elements of the paper that are being discussed. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2896,NIPS_2022_1666,"Weaknesses:
I cannot give a clear acceptance to the current manuscript due to the following concerns:
1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work.
For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work.
2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1.
3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper.
In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.
4. Advantage of NAR(heatmap) + RL + Meta-Learning: From Table 1&2, for TSP1000, the generalization performance of AM (G: 31.15, BS: 29.90) trained on TSP100 is not very far from the testing performance of DIMES without meta-learning (27.11) directly trained on TSP1000. It could be helpful to check whether the more powerful POMO approach[4] can have a smaller performance gap. Reporting the results for POMO and DIMES without meta-learning for all instances in Table 1 could make the advantage of the NAR(heatmap) + RL approach in DIMES much clearer.
Hottung et al.[5] shows that POMO + Efficient Active Search (EAS) can achieve promising generalization performance for larger TSP instances on TSP and CVRP. The comparison with POMO + EAS could be important to better evaluate the advantage of meta-learning in DIMES.
[1] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, Thomas Laurent, and Xavier Bresson. Learning tsp requires rethinking generalization. arXiv preprint arXiv:2006.07054,2020.
[2] Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.
[3] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. NeurIPS 2018.
[4] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min. POMO: Policy optimization with multiple optima for reinforcement learning. NeurIPS 2020.
[5] André Hottung, Yeong-Dae Kwon, and Kevin Tierney. Efficient active search for combinatorial optimization problems. ICLR 2022.
Yes, the limitations have been adequately addressed in Section 5 Concluding Remarks. I do not see any potential negative societal impact of this work.","1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work. For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work. 2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1. 3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper. In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.",nan,nan,nan,nan,nan,1,"The feedback provided in the review is highly actionable, offering specific, clear, and practical suggestions that the author can use to improve their paper. The feedback addresses the inaccuracies in the contribution of the work, the actual cost of meta-learning, and the generalization versus testing performance. Each point is not only identified but also explained in detail, providing the author with a clear understanding of the issues and how to address them. The feedback also suggests that the author should clarify the novel contribution of the work, the actual cost of meta-learning, and the generalization gaps in the paper. These suggestions are directly actionable, guiding the author in making the necessary revisions to their paper. The feedback is well-structured and directly addresses the aspects mentioned in the instruction. Therefore, based on the score rubric, the feedback is highly actionable and directly relevant to the review. 
[RESULT] 1",1,"The feedback provided is comprehensive and addresses several key aspects of the review. It critically evaluates the contribution of the paper, the actual cost of the meta-learning approach, and the generalization versus testing performance. The tone of the feedback is respectful and constructive, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback also provides a detailed analysis of the paper's content, which demonstrates a deep understanding of the subject matter. Furthermore, the feedback is well-structured and organized, making it easy to follow and understand. The feedback also encourages the authors to clarify certain points in their paper, which is a positive and constructive approach. Overall, the feedback is well-crafted and meets the criteria for a score of 1, as it is polite, helpful, and respectful. 
[RESULT] 1",1,"The feedback provided in the response is well-structured and addresses several key aspects of the paper. It begins by questioning the contribution of the paper, pointing out that the compact continuous parameterization is not a new contribution but rather an extension of previous work. This claim is supported by references to external sources, which aligns with the score rubric's requirement for verifiable feedback. The response then moves on to discuss the actual cost of meta-learning, providing specific details from the paper's appendices to support its argument. This again demonstrates the use of internal references to substantiate the feedback. Finally, the response addresses the generalization performance of the paper, comparing it with other methods and highlighting the need for clearer clarification in the paper. The feedback is logical and well-reasoned, and it provides a comprehensive analysis of the paper's claims. Therefore, based on the score rubric, the feedback is relevant, well-supported, and logically structured, making it a strong critique of the paper. 
[RESULT] 1",1,"The feedback provided is highly specific and directly addresses the content of the paper. It critically evaluates the contribution of the paper, pointing out that the compact continuous parameterization is not a novel contribution but rather a reiteration of previous work. The feedback also highlights the actual cost of meta-learning, which is a crucial aspect of the paper's methodology. It goes into detail about the meta-gradient update steps and their impact on the runtime of the paper's method. Furthermore, the feedback discusses the generalization performance of the paper's method compared to other methods, providing a clear comparison of in-distribution and out-of-distribution performance. The feedback is tailored to the specific content of the paper, making it easy to identify which parts of the document are being discussed. Therefore, the feedback is highly specific and directly relevant to the content of the paper, meeting the criteria for a score of 1. 
[RESULT] 1"
5265,NIPS_2020_1700,"- There are a number modelling details that are not entirely clear in the paper (see below). - When comparing Gumbel-CRF directly to Perturb-and-MAP MRF, performance is very similar.",- There are a number modelling details that are not entirely clear in the paper (see below).,nan,nan,nan,nan,nan,-1,"The feedback provided in the review is vague and lacks specificity. It mentions that there are ""a number of modelling details that are not entirely clear in the paper,"" but it does not offer any concrete steps or recommendations that the author can use to improve their paper. The feedback fails to provide actionable comments that guide the author in addressing the issues. It does not offer any direction or hints of what needs to be done, which is crucial for the author to make necessary revisions. Therefore, the feedback does not meet the criteria of being actionable, and it does not respect the aspect of providing clear and practical suggestions for improvement. So the score is -1. 
[RESULT] -1",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it simply points out that there are unclear details in the paper. The feedback is delivered in a manner that is neither positive nor negative, which is in line with the score rubric's criteria for a score of 0. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not respect or ignore the aspect in question. 
[RESULT] 0",-1,"The feedback provided in the response is vague and lacks specificity. It mentions ""a number modelling details"" without identifying what these details are or how they are not clear. This lack of detail makes it difficult to verify the feedback or to understand the reviewer's concerns. The feedback does not provide any evidence or references to support its claims, which is a key aspect of the score rubric. Furthermore, the feedback does not offer any logical reasoning that could be independently verified. Therefore, based on the score rubric, the feedback does not meet the criteria for a score of 1, as it does not respect the aspect of verifiability. Instead, it falls into the category of a score of -1, as it imposes a claim without providing any evidence for it. 
[RESULT] -1",0,"The feedback provided is vague and lacks specificity. It mentions ""a number modelling details"" without identifying which details are unclear or how they impact the paper. This lack of specificity makes it difficult to pinpoint the exact aspects of the paper that the reviewer is addressing. The feedback does not provide any concrete examples or direct references to the content of the paper, which is essential for a review to be effective and actionable. Therefore, the feedback does not meet the criteria for specificity as outlined in the score rubric. 

[RESULT] 0"
3860,NIPS_2020_1228,"- The method section looks not self-contained and lacks descriptions of some key components. In particular: * What is Eq.(9) for? Why ""the SL is the negative logarithm of a polynomial in \theta"" -- where is the ""negative logarithm"" in Eq.(9)? * Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained. - The proposed approach essentially treats the structured constraints (a logical rule) as part of the discriminator that supervises the training of the generator. This idea looks not new -- one can simply treat the constraints as an energy function and plug it into energy-based GANs (https://arxiv.org/abs/1609.03126). Modeling structured constraints as a GAN discriminator to train the generative model has also been studied in [15] (which also discussed the relation b/w the structured approach with energy-based GANs). Though the authors derive the formula from a perspective of semantic loss, it's unclear what's the exact difference from the previous work? - The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.","* Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It identifies a problem with the practical implementation of the model but fails to offer any specific, clear, or practical suggestions for improvement. The feedback merely states that the details of knowledge compilation (KC) are missing, without providing any guidance on how to address this issue. It lacks the necessary direction or hints that would help the author in revising their paper. Therefore, the feedback does not meet the criteria of actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the importance of the component in question and expresses a need for more detailed information. This approach is encouraging and aims to help the authors improve their work. The feedback is also factual, focusing on the lack of detail in the knowledge compilation component, which is a relevant aspect of the review. The tone is neither negative nor aggressive, and it maintains a level of respect throughout. Therefore, based on the score rubric, the feedback is well-aligned with the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review and addresses the aspect of the paper's practical implementation. The reviewer has identified a specific part of the paper, ""Evaluating the Semantic Loss"" (L.140), and has pointed out the lack of detail regarding knowledge compilation (KC). This indicates that the reviewer has made an effort to engage with the paper's content and has provided a specific reference to support their claim.

However, the feedback does not fully meet the criteria for a score of 1. While the reviewer has made a claim about the lack of detail in the paper, they have not provided any evidence or logical reasoning to support this claim. The feedback is based on the reviewer's personal opinion and does not include any references to external sources or logical validation.

In conclusion, the feedback is relevant to the review and addresses the aspect of the paper's practical implementation. However, it falls short in providing evidence or logical reasoning to support the claims made. Therefore, the feedback is not fully supported by references or logical reasoning, which is why it does not receive a score of 1. 
[RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the practical implementation of the model. The feedback is tailored to the content of the paper, as it specifically mentions the ""Evaluating the Semantic Loss"" part (L.140) and the Weighted Model Count (WMC). However, it also points out a lack of detail in the knowledge compilation (KC) component, which is a critical part of the proposed approach. This shows that the feedback is not only specific but also respects the importance of each aspect in the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
2038,ARR_2022_256_review,"1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' ","1. I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this!",nan,nan,nan,nan,nan,0,"The feedback provided in the response is not actionable. It raises questions and doubts about the methodology used in the paper, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback is more focused on the reviewer's confusion rather than providing guidance on how the author can address the issues raised. It lacks the direction or hints of what needs to be done, which is crucial for actionable feedback. The feedback does not enhance the usefulness of the review for the author, as it does not provide a clear path for revision. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is not respectful or constructive. The tone is negative and the feedback is delivered in a way that could be perceived as aggressive. The reviewer expresses confusion and questions the validity of the student model without providing any constructive criticism or suggestions for improvement. The feedback also lacks politeness, as it directly challenges the author's approach without offering any alternative solutions or acknowledging the potential merits of the student model. This approach does not encourage the authors or respect the effort they have put into their work. Therefore, based on the score rubric, the feedback is not aligned with the criteria for a score of 1. 
[RESULT] -1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiability. The reviewer's claims are not supported by any evidence or references to the paper or external sources. The reviewer questions the process of teaching the student model by the teacher model and suggests using the teacher model for zero-shot cross-lingual NER. However, these claims are not substantiated with any logical reasoning or references to the paper. The reviewer's confusion about the process and the lack of a baseline mentioned in the paper further detracts from the verifiability of the feedback. Therefore, the feedback does not meet the criteria for verifiability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the draft. It fails to address specific parts or methods in the paper, making it difficult to identify the parts that are being addressed in the document. The feedback is more of a general comment rather than a tailored critique. It does not provide a clear understanding of the student model's role or the process of how the teacher model is used in the paper. The lack of specificity in the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. Therefore, the feedback does not meet the requirements of the score rubric. 
[RESULT] 0"
5573,NIPS_2018_641,"weakness.  First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a ""good"" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless.  Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the ""learning rate"" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD.  The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes.  I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication.  Other comments: 1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. ""Gradient methods for minimizing composite functions"" by Yu. Nesterov. 2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate.  4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties. 5. The proofs in the supplementary material are too terse. ","2. ln.55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It identifies a potential issue with the regularization parameter but fails to offer any specific, clear, or practical suggestions for improvement. The feedback does not provide the author with a clear path for revision, nor does it hint at what needs to be done. It merely states the problem without offering any guidance on how to address it. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is critical and directly addresses the aspect of the review. It questions the validity of the regularized empirical risk minimizer without any condition on the regularization parameter, which is a key aspect of the paper. The tone of the feedback is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The feedback does not contain any negative or aggressive language, and it maintains a professional tone throughout. It is clear that the feedback is focused on the scientific content of the paper, rather than the tone or manner of the feedback itself. Therefore, based on the score rubric, the feedback is neutral and factual, and it does not disrespect or ignore the aspect of the review. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not support its claim with any evidence or references. It simply states a concern without providing any logical reasoning or citing any external sources or parts of the paper. The feedback does not demonstrate an understanding of the paper's content or the methodology used. It also fails to provide any constructive criticism or suggestions for improvement. Therefore, the feedback does not meet the requirements for a score of 1, as it does not respect the aspect of verifiable feedback. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the regularized empirical risk minimizer and its convergence to the risk minimizer. The feedback also questions the condition on the regularization parameter, which is a critical aspect of the paper's methodology. This specificity ensures that the feedback is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way.",nan,nan,nan,nan,nan,1,"The feedback provided in the review is actionable and offers specific suggestions for improvement. It addresses the aspect of the paper by suggesting an alternative setting for theorem 2, which could potentially enhance the paper's relevance and applicability. Furthermore, it points out a potential oversight regarding the dependence of the bound on the batch size, which is a critical aspect of the paper. This feedback not only identifies the issue but also provides a direction for the author to consider, such as the impact of the batch size on the bound. The feedback is clear, practical, and directly addresses the aspects of the paper that need improvement. Therefore, based on the score rubric, the feedback is highly actionable and aligns with the criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the validity of the assumption made in the paper but also suggests an alternative approach, which demonstrates an understanding of the topic and a willingness to engage with the authors' work. The feedback also points out a potential oversight in the paper, which is a helpful way to contribute to the scientific discourse. The tone is positive and encouraging, which is in line with the score rubric's criteria for a score of 1. The feedback is also respectful, as it does not criticize the authors but rather offers suggestions for improvement. Therefore, based on the score rubric, the feedback is well-crafted and meets the criteria for a score of 1. 
[RESULT] 1",0,"The feedback provided in the response does not align with the score rubric's criteria for verifiability. The reviewer's claim about the setting of theorem 2 not being strongly convex is not supported by any evidence or references. The same applies to the claim about the missing dependence of the bound on the batch size in theorem 2 and corollary 3. The reviewer's questions and comments are based on assumptions and general knowledge, which do not constitute verifiable feedback. The response does not cite external sources or use logical reasoning to support its claims, which is a key requirement of the score rubric. Therefore, the feedback does not meet the criteria for verifiability and is not relevant to the review. So the score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the assumption of strong convexity in theorem 2. The feedback also points out a potential oversight regarding the dependence of the bound on the batch size in theorem 2 and corollary 3. This shows a clear understanding of the paper's content and a critical analysis of the assumptions and implications. The feedback is tailored to the specific elements of the paper, making it easy to identify which parts of the document are being discussed. Therefore, the feedback meets the criteria of specificity and relevance to the content of the paper. 
[RESULT] 1"
3822,NIPS_2020_179,"1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack. 2. Suppose the evaluation reflects the model robustness, the region for “small perturbations” is quite small. In CIFAR10, the proposed mechanisms only work on par with adversarial training at epsilon=0.001. I would think even epsilon=8/255 on CIFAR10 are not perceptible by humans. 3. In Table 1, it will be interesting to integrate the proposed fixations with SOTA models. The reported RESNET result is not the SOTA ones. 4. In the gradient obfuscation part, it will be good to clarify what does the authors “verified”. 5. Last paragraph in the conclusion generalizes the work a bit too much. 6. There is no adversarial training baseline in ImageNet10, ImageNet results.","1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It merely questions the strength of the adversarial attack used in the experiments without offering any concrete steps or recommendations for improvement. The feedback does not guide the author in addressing the issues raised, nor does it provide a clear path for revision. It lacks specificity and practicality, which are essential for actionable feedback. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it presents a critique of the methodology used in the review, questioning the strength of the adversarial attack and its ability to evaluate the robustness of the model. The feedback is focused on the aspect of the review, which is the robustness of the model, and it provides a clear and objective analysis of the methodology. However, the feedback could be more constructive by offering suggestions for improvement or by acknowledging the strengths of the review. So the overall score is 0. [RESULT] 0",-1,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claim that the adversarial attack in the experiments is not strong enough. The feedback is based on a personal opinion and lacks logical reasoning or references to external sources or parts of the paper. The feedback does not follow the aspect of verifiability, as it does not provide any means for the reviewer's claims to be checked against external sources or through logical validation. Therefore, the feedback does not respect the aspect of verifiability, and it is not relevant to the review. So the score is -1. [RESULT] -1",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses the aspect of adversarial accuracy and the method used in the experiments, which is the 5-20 step PGD without random restarts or longer steps. The feedback also questions the strength of the adversarial attack and its ability to evaluate the robustness of the model, which is a critical point in the review. The specificity of the feedback ensures that it is tailored to the content of the paper and not a general comment for any scientific document. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3001,NIPS_2022_489,"Weaknesses:
Concern regarding representativeness of baselines used for evaluation
Practical benefits in terms of communication overhead & training time could be more strongly motivated
Detailed Comments:
Overall, the paper was interesting to read and the problem itself is well motivated. Formulation of the problem as an MPG appears sound and offers a variety of important insights with promising applications. There are, however, some concerns regarding evaluation fairness and practical benefits.
The baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE. In particular, there have been a variety of recent works that explore more efficient strategies (e.g., [1-3]) and consistently outperform QMix with relatively low inter-agent communication. Although the proposed work appears effective as a fully-decentralized approach, it is unclear how well it would perform against more competitive CTDE baselines. Comparison against these more recent works would greatly improve the strength of evaluation.
Benefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.
Robustness against dynamic changes in network setup are discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches.
Some confusion in notation: - Algorithm 2, L8 should be t = 1,…,H (for horizon)? - L100, [M] denotes the set of LBs?
Minor notes: - Some abbreviations are not defined, e.g., “NE” on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read.
[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. [2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. [3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.","- L100, [M] denotes the set of LBs? Minor notes:",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is not actionable. It merely states the notation L100, [M] without offering any guidance or suggestions on how to improve the paper. The feedback does not provide any direction or hints on what needs to be done, which is crucial for the author to make necessary revisions. The lack of actionable comments in the feedback makes it less useful for the author. Therefore, the feedback does not meet the criteria of being actionable, and it fails to respect the aspect of providing clear and practical suggestions for improvement. So the score is -1. 
[RESULT] -1",0,"The response provided is a brief note on the notation used in the paper. It does not offer any critique or feedback on the paper's content or the authors' work. The tone of the feedback is neutral and factual, which aligns with the score rubric's description for a score of 0. The feedback does not show any signs of disrespect or aggression, nor does it attempt to encourage the authors. It simply states the notation used, which is a factual statement. Therefore, based on the score rubric, the feedback is neutral and does not meet the criteria for a score of 1 or -1. It neither respects nor ignores the aspect in question. So the overall score is 0. [RESULT] 0",-1,"The response provided does not meet the criteria for a score of 1. It does not offer any evidence or logical reasoning to support the claim that ""L100, [M] denotes the set of LBs?"". The response is a mere statement without any supporting references or logical validation. It does not provide any external sources or internal references from the paper to substantiate the claim. Therefore, it does not meet the requirements of the score rubric, which emphasizes the importance of verifiable feedback. The feedback is not relevant to the review, and it does not respect the aspect of verifiability. So the overall score is -1. 
[RESULT] -1",-1,"The response provided is vague and lacks specificity. It does not address any particular aspect of the paper, nor does it provide any concrete feedback. The mention of ""L100"" and ""[M]"" is not explained or contextualized, which makes it difficult to understand what the reviewer is referring to. The response does not demonstrate an understanding of the content of the paper, nor does it provide any actionable feedback. Therefore, it does not meet the criteria for specificity as outlined in the score rubric. The feedback is not tailored to the content of the paper, and it is not clear which sections, statements, or elements in the paper are being addressed. This lack of specificity and relevance to the content of the paper is why the response would receive a score of -1. 
[RESULT] -1"
924,ICLR_2023_4092,"Weakness: 1. The writing and setting up of this paper are not very clear to me. The motivation does not convincing to me. 2. The novelty of the proposed method seems trivial. Contrastive learning, embedding alignment and etc have been widely discussed in the previous literature.
3. For XQR performance, it is not surprising that with the additional selected XOR finetuning examples, the performance will boost further. 4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.",4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.,nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It identifies issues with the ablation of the paper and the performance boost, but it does not offer any specific, clear, or practical suggestions for improvement. The feedback merely states that the parameter comparison or computing comparison should be seen, and it expresses uncertainty about the source of the performance boost. This lack of actionable guidance does not help the author in addressing the issues raised in the review. The feedback does not provide a clear path for revision, which is essential for the author to improve their paper. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",-1,"The feedback provided in the response is not respectful or constructive. The tone is negative and critical, which is not appropriate for a scientific review. The feedback does not encourage the authors or provide any positive reinforcement. Instead, it focuses on the shortcomings of the paper without offering any constructive suggestions for improvement. The feedback also fails to acknowledge any positive aspects of the paper, which is an important part of a balanced review. The lack of respect and constructiveness in the feedback is why it receives a score of -1. 
[RESULT] -1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The feedback does not provide any evidence or references to support the claims made. The reviewer's suggestions are based on assumptions and personal opinions, rather than logical reasoning or references to the paper. The feedback does not cite any external sources or use internal references from the paper to support the claims. The reviewer's suggestions are not verifiable, and the feedback does not respect the aspect of verifiability. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",0,"The feedback provided in the response is not specific to the content of the paper. It fails to mention any specific parts or methods in the paper, making it difficult to identify which sections or elements are being addressed. The feedback is vague and does not provide clear guidance on how to improve the paper. It lacks the specificity required to effectively critique the paper. The feedback does not follow the aspect of specificity, as it does not tailor the feedback to the content of the paper. Therefore, the feedback does not meet the criteria for a score of 1, as it does not address a certain point or design decision in the paper. 
[RESULT] 0"
4420,NIPS_2020_341,"- For theorem 5.1 and 5.2, is there a way to decouple the statement, i.e., separating out the optimization part and the generalization part? It would be clearer if one could give a uniform convergence guarantee first followed by how the optimization output can instantiate such uniform convergence. - In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.","- In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback merely questions the reasonableness of the training time differences in the experiments, without providing any guidance on how to address this issue. Furthermore, the feedback does not suggest any concrete steps or recommendations that would help the author in revising their paper. It lacks the necessary direction and hints that would make the feedback actionable. Therefore, the feedback does not meet the criteria for actionability as outlined in the score rubric. 
[RESULT] 0",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone is respectful and the feedback is aimed at encouraging the authors to consider the publication of their code. The feedback also addresses the aspect of the review, which is the comparison of training times in different experiments. It does so by questioning the reasonableness of the shorter training time in Gerrymandering compared to Independent, and by suggesting that the code should be published due to its similar performance and computational advantage. This approach is helpful and positive, which is in line with the rubric's criteria for a score of 1. Therefore, the feedback is relevant to the review and is followed and respected in the review. 
[RESULT] 1",0,"The feedback provided in the response does not meet the criteria for a score of 1. The claim made in the feedback regarding the German and Law school dataset having a shorter training time in Gerrymandering than Independent is not supported by any evidence. There are no references to external sources, parts of the paper, or logical reasoning that could be independently verified. The feedback also fails to provide any justification for the claim, which is a critical aspect of the score rubric. The feedback does not respect the aspect of verifiability, as it does not provide any evidence to support its claims. Therefore, the feedback does not meet the criteria for a score of 1. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the training time for the German and Law school dataset in the context of Gerrymandering. The feedback also mentions the comparison with Kearns et al. and the advantage of computation time, which are specific details from the paper. The response also suggests that the code should be published, which is a direct recommendation based on the content of the paper. Therefore, the feedback is tailored to the content of the paper and not a general comment for any scientific document. It is clear that the feedback is specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
5664,NIPS_2018_185,"Weakness: ##The clarity of this paper is medium. Some important parts are vague or missing. 1) Temperature calibration: 1.a) It was not clear what is the procedure for temperature calibration. The paper only describes an equation, without mentioning how to apply it. Could the authors list the steps they took? 1.b) I had to read Guo 2017 to understand that T is optimized with respect to NLL on the validation set, and yet I am not sure the authors do the same. Is the temperature calibration is applied on the train set? The validation set (like Guo 2017)? The test set? 1.c) Guo clearly states that temperature calibration does not affect the prediction accuracy. This contradicts the results on Table 2 & 3, where DCN-T is worse than DCN. 1.d) About Eq (5) and Eq (7): Does it mean that we make temperature calibration twice? Once for source class, and another for target classes? 1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain? 2) Uncertainty Calibration From one point of view it looks like temperature calibration is independent of uncertainty calibration, with the regularization term H. However in lines 155-160 it appears that they are both are required to do uncertainty calibration. (2.a) This is confusing because the training regularization term (H) requires temperature calibration, yet temperature calibration is applied after training. Could the authors clarify this point? (2.b) Regarding H: Reducing the entropy, makes the predictions more confident. This is against the paper motivation to calibrate the networks since they are already over confident (lines 133-136). 3) Do the authors do uncertainty calibration on the (not-generalized) ZSL experiments (Table 2&3)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 4) Do the authors do temperature calibration on the generalized ZSL experiments (Table 4)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 5) The network structure: 5.a) Do the authors take the CNN image features as is, or do they incorporate an additional embedding layer? 5.b) What is the MLP architecture for embedding the semantic information? (number of layers / dimension / etc..) ##The paper ignores recent baselines from CVPR 2018 and CVPR 2017 (CVPR 2018 accepted papers were announced on March, and were available online). These baseline methods performance superceed the accuracy introduced in this paper. Some can be considered complementary to this work, but the paper canât simply ignore them. For example: Zhang, 2018: Zero-Shot Kernel Learning Xian, 2018: Feature Generating Networks for Zero-Shot Learning Arora, 2018: Generalized zero-shot learning via synthesized examples CVPR 2017: Zhang, 2017: Learning a Deep Embedding Model for Zero-Shot Learning  ## Title/abstract/intro is overselling The authors state that they introduce a new deep calibration network architecture. However, their contributions are a novel regularization term, and a temperature calibration scheme that is applied after training. I wouldnât consider a softmax layer as a novel network architecture. Alternatively, I would suggest emphasizing a different perspective: The approach in the paper can be considered as more general, and can be potentially applied to any ZSL framework that outputs a probability distribution. For example: Atzmon 2018: Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning Ba 2015: Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions  Other comments: It will make the paper stronger if there was an analysis that provides support for the uncertainty calibration claims in the generalized ZSL case, which is the focus of this paper. Introduction could be improved: The intro only motivates why (G)ZSL is important, which is great for new audience, but there is no interesting information for ZSL community. It can be useful to describe the main ideas in the intro. Also, confidence vs uncertainty, were only defined on section 3, while it was used in the abstract / intro. This was confusing. Related work: It is worth to mention Transductive ZSL approaches, which use unlabeled test data during training, and then discriminate this work from the transductive setting. For example: Tsai, 2017: Learning robust visual-semantic embeddings. Fu 2015: Transductive Multi-view Zero-Shot Learning I couldnât understand the meaning on lines 159, 160. Lines 174-179. Point is not clear. Sounds redundant. Fig 1 is not clear. I understand the motivation, but I couldnât understand Fig 1. ","1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain?",nan,nan,nan,nan,nan,0,"The feedback provided in the review is not actionable. It raises questions about the methodology used in the paper, but it does not offer any specific, clear, or practical suggestions that the author can use to improve their paper. The feedback is more of a commentary on the methodology rather than a constructive critique that guides the author in addressing the issues. It lacks the necessary direction or hints of what needs to be done, which is crucial for the author to make improvements. Therefore, the feedback does not meet the criteria of being actionable. 
[RESULT] 0",0,"The feedback provided in the response is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. Instead, it poses questions about the methodology used in the paper, which is a factual approach to reviewing scientific papers. The feedback is focused on the aspect of temperature calibration and its relation to hyper-parameter grid search, which is relevant to the review. However, the feedback does not explicitly state whether the aspect is respected or not, which is a key component of the score rubric. Therefore, while the feedback is neutral and factual, it does not fully meet the criteria for a score of 1, as it lacks the explicit respect and encouragement for the authors. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is not aligned with the score rubric. The feedback does not provide any evidence or references to support the claims made. It is a series of questions that are not substantiated with any logical reasoning or references to the paper. The feedback does not demonstrate an understanding of the scientific paper or the methodology described. It also fails to provide any verification of the claims made, which is a critical aspect of the score rubric. Therefore, the feedback does not meet the criteria for a score of 1, as it does not support its claims with any evidence or logical reasoning. 

The feedback also does not meet the criteria for a score of 0, as it does not make general statements or observations that are not relevant to the review. Instead, it poses specific questions that are not supported by any evidence or logical reasoning. 

In conclusion, the feedback does not meet the criteria for either a score of 1 or a score of 0, as it does not provide any evidence or logical reasoning to support its claims. It also does not make general statements or observations that are not relevant to the review. Therefore, the feedback is not aligned with the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the temperature calibration process. The feedback is tailored to the content of the paper, as it questions the sequence of the calibration process and its applicability to other models. This shows a clear understanding of the paper's content and a direct engagement with the specifics of the method described. The feedback is not a general comment for any scientific document, but rather a focused critique on a specific aspect of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3447,NIPS_2020_486,"The method is relatively straightforward and some directions could be explored further. For instance, by using fully connected transformations, typical convolutional weight-sharing is not utilised. It would be good to discuss downsides to fully connected Woodbury transforms, and possible alternative formulations that would utilise convolutional weight-sharing. Further, the models utilised in the experimental section are quite small. As a result, the NLL performance is not very good compared to newer flow-based models. In addition the gains in NLL are quite small, and it would be better if the authors included standard deviations over multiple runs. Minor: - If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck. - For a better overview it would be nice to have a table showing the complexity for different methods for their forward/inverse/logdet in one place.","- If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck.",nan,nan,nan,nan,nan,1,"The feedback provided in the response is actionable and directly addresses the aspect of the review. It suggests a specific experiment, the ""changing bottleneck"" experiment, which is directly related to the required size of the bottleneck. This recommendation is clear and practical, offering a concrete step for the authors to consider for their paper. The feedback is not only relevant but also respects the aspect by providing a specific direction for improvement. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1",1,"The feedback provided is polite and constructive, which aligns with the score rubric's requirement for a score of 1. The tone is respectful and the feedback is aimed at encouraging the authors to include a specific experiment in their paper. The feedback is also helpful, as it suggests a specific experiment that could be relevant to the paper's topic. The feedback is positive and tries to be encouraging, which is in line with the rubric's criteria for a score of 1. The feedback does not contain any negative or aggressive language, and it maintains a professional tone throughout. Therefore, based on the score rubric, the feedback is relevant to the review and is followed or respected in the review. 
[RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it suggests an additional experiment that could be included in the main paper. However, the feedback does not provide any evidence or references to support the claim that this additional experiment is necessary or relevant to the paper. The feedback is based on a general assumption that the ""changing bottleneck"" experiment is important, without any logical reasoning or external references to back up this claim. Therefore, the feedback does not meet the criteria for verifiable feedback, as it lacks the necessary support from external sources or logical reasoning. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It suggests a particular experiment, the ""changing bottleneck"" experiment, which is directly related to the aspect of the required size of the bottleneck. This feedback is tailored to the content of the paper, and it is clear which part of the document is being discussed. The feedback is not a general comment for any scientific document, but rather a specific critique that directly addresses the content of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
3749,NIPS_2020_755,"Sometimes the presentation is dense: a table, for example, would be a more efficient way to compare the derived rates with past results. There are a few discoveries I wish the authors would discuss a bit more, including: - the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible. - showing the bias-variance decomposition explicitly, at least one, would be nice - can you explain why the kernel is redundant when beta=2 (line 204)? - Since the claimed lower bound is novel, can you explain what is new about the construction?","- the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is not actionable. It merely states the generalization of ""adversarial noise"" without offering any specific, clear, or practical suggestions for the author to improve their paper. The feedback does not provide any direction or hints on what needs to be done to address the issue of ""adversarial noise."" It lacks the necessary detail and guidance that would help the author in revising their paper. Therefore, the feedback does not meet the criteria of being actionable, and it does not respect the aspect of the review. So the score is -1. 
[RESULT] -1",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The feedback does not contain any negative or aggressive language, nor does it show any particular level of respect or encouragement. It simply states the aspect to be evaluated without any additional commentary or tone. The feedback is focused on the aspect of generalization to ""adversarial noise,"" but it does not provide any specific critique or praise, which is why it is considered neutral. Therefore, based on the score rubric, the feedback is not disrespectful or aggressive, nor is it overly positive or encouraging. It is simply a statement of the aspect to be evaluated. 
[RESULT] 0",0,"The feedback provided does not meet the criteria for a score of 1. It does not offer any evidence or logical reasoning to support the claim that the generalization to ""adversarial noise"" is plausible. The feedback simply states the claim without any supporting references or logical validation. This lack of verifiable support for the claim is why the feedback does not meet the criteria for a score of 1. 

In contrast, a score of 1 would require the feedback to provide references to external sources, references to parts of the paper, or logical reasoning that can be independently verified. Without these elements, the feedback does not meet the criteria for a score of 1. 

Therefore, based on the score rubric, the feedback does not meet the criteria for a score of 1, as it does not provide any evidence or logical reasoning to support the claim made. 
[RESULT] 0",1,"The feedback provided is quite specific, focusing on the generalization of ""adversarial noise"" in the paper. It directly addresses the aspect of the review, asking for an explanation of why this generalization is plausible. This specificity aligns with the score rubric's requirement for detailed and directly relevant feedback. The feedback is tailored to the content of the paper, and it is clear which part of the document is being discussed. Therefore, the feedback is relevant and respects the aspect of the review. So the score is 1. 
[RESULT] 1"
1451,ICLR_2023_2368,"Weaknesses: 1. There is no theoretical guarantee that the discoveries resulting from the sparse network architecture are unique. 2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here. 3. It lacks model complexity analysis and comparison. Given different levels of biological entities' intra and inter interactions, I am worried about the model's real applicability.","2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is relevant to the aspect of quantifying the top-k frequent interaction pairs. However, it falls short in offering actionable suggestions. The feedback merely points out the absence of this aspect in the paper, without providing any guidance on how to address this issue. It lacks the specificity and clarity that would enable the author to make practical improvements to their paper. The feedback does not offer any concrete steps or recommendations that would help the author in addressing the identified issue. Therefore, the feedback is not actionable, and it does not meet the criteria set out in the score rubric. 
[RESULT] 0",1,"The feedback provided in the response is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The feedback identifies a specific aspect of the paper that was not adequately addressed, namely the quantification of top-k frequent interaction pairs. This is a relevant point that directly relates to the paper's claim of novel discovery. The feedback also acknowledges the paper's advantage, which shows a level of respect for the authors' work. However, the feedback could have been more encouraging by highlighting the potential of the paper if the identified aspect was addressed. Despite this, the feedback maintains a positive and respectful tone throughout. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it addresses a specific aspect of the paper, which is the quantification of top-k frequent interaction pairs. The feedback also correctly identifies that this aspect is not adequately addressed in the paper, as it mentions the lack of results and discussions in section 5.4. However, the feedback does not provide any evidence or references to support its claims, which is a key requirement according to the score rubric. The feedback could have been strengthened by citing specific parts of the paper or external sources to back up its assertions. Therefore, while the feedback is relevant and identifies a shortcoming in the paper, it does not meet the criteria for a score of 1 due to the absence of supporting evidence. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It clearly identifies a specific aspect of the paper, which is the description of how to quantify the top-k frequent interaction pairs. The feedback also directly addresses the claim made in the paper about the framework's advantage for novel discovery, pointing out the lack of results and discussions. This specificity in the feedback ensures that the review is tailored to the content of the paper, and not a general comment for any scientific document. Therefore, the feedback aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once.",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the aspect of optimization in the algorithm. However, it falls short in offering actionable suggestions. The feedback merely points out the lack of clarity in the optimization process without providing any guidance on how to improve it. It does not suggest any specific steps or recommendations that the author could follow to address the issue. The feedback is more of a critique rather than a constructive review that would help the author in revising their paper. Therefore, the feedback does not meet the criteria of being actionable, which is essential for a review to be useful to the author. So the score is -1. [RESULT] -1",0,"The feedback provided in the response is relevant to the review and addresses a specific aspect of the paper. However, the tone of the feedback is somewhat critical and could be perceived as slightly disrespectful. The feedback does not fully respect the authors' work, as it questions the decision to optimize auxiliary losses separately. This could potentially discourage the authors from further exploration of the topic. The feedback could have been more constructive by suggesting a possible alternative or explaining why the current approach might be beneficial. Despite these shortcomings, the feedback does not cross into the realm of rudeness or aggression, and it maintains a level of politeness. Therefore, the feedback is not entirely disrespectful, but it could be improved to be more respectful and encouraging. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is relevant to the review and addresses a specific aspect of the paper. The reviewer questions the rationale behind optimizing each auxiliary loss separately, which is a logical claim that can be independently verified. However, the feedback does not provide any evidence or references to support the claim, which is a key aspect of the score rubric. The feedback is based on the reviewer's understanding of the paper and their logical reasoning, but it lacks the necessary references or citations that would make it more verifiable. Therefore, while the feedback is relevant and respects the aspect of the paper, it does not fully meet the criteria for a score of 1 as it does not provide the necessary evidence or references to support the claim. So the overall score is 0. [RESULT] 0",1,"The feedback provided in the response is specific and directly relevant to the content of the draft. It addresses a particular aspect of the paper, which is the optimization of auxiliary losses in the algorithm. The feedback is tailored to the content of the paper, and it is clear which part of the document is being discussed. The reviewer has identified a specific design decision in the paper and has provided a critique that is directly related to this decision. This level of specificity is in line with the score rubric's criteria for a score of 1. Therefore, the feedback is relevant and respects the aspect in question. 
[RESULT] 1"
2710,NIPS_2022_327,"Weaknesses
I think the paper is hard to follow and read, in particular there are a lot of statistical details and some of the machine learning details can be lost in there. There is excessive use of alternative notation, for example η
is used for the conditional density p(Y = 1 | X = x), I think it is better to use probabilistic notation directly and not introduce additional greek letters as makes the notation hard to follow. Same can be said for risk definitions.
Overall about readability, I think the paper should be rewritten to put the terms that are usable for implementation from a machine learning perspective (like binary and multi-class uncertainty components), and leave derivations and proofs for the supplementary. The audience of NeurIPS is largely machine learning people, and the paper notation and arguments could be simplified so it is easier to follow for this audience. The statistical details are important but after reading this paper, if I want to implement this method, to me it is not clear what parts I should use or directly how this method works conceptually.
- The results in the paper only present out of distribution detection results as main result, and classification with rejection on text datasets. There are additional results on the supplementary but I believe these are minor. Overall I think there are missed opportunities for evaluation, for example, calibration error of epistemic uncertainty (Figure 2 could be a start point).
- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.
- Since density estimation is computationally expensive, the authors resort to an approximation of the kernel density estimate with nearest neighbors in feature space. I think this is fine, but there is no evaluation of approximation quality or ablations on how to select an approximation for nearest neighbors and kernel density estimates, the authors just use HNSW for this purpose.
Minor issues
I think a diagram showing how the method works and its training/inference process would be an easy way to understand the overall proposed approach.
- I think the paper is missing references to the text datasets (CoLA, SST-2, and MRPC) and other datasets used in the text classification experiments.
- I am not sure what score is used for OOD detection, my guess is that it is the epistemic uncertainty U e
, and this should be explicit in the evaluation section.
As I mentioned in weaknesses, the paper does not mention that it uses an approximation for the kernel density estimate, and this could be a limitation that could be addressed in the future.
There are no other limitations mentioned in the paper about the proposed method, even as the checklist says the paper does discuss limitations (I do not see where, I might be wrong).
Overall I see no negative societal impact to discuss.","- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.",nan,nan,nan,nan,nan,0,"The feedback provided in the review is relevant to the aspect of uncertainty disentanglement. However, it falls short in providing actionable suggestions. While the feedback does identify the lack of evaluation in the main paper and the missed opportunity to compare with Kendall and Gall. 2017, it does not offer specific, clear, and practical steps that the authors can take to improve their paper. The feedback merely suggests that the authors should evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected, without providing any guidance on how to achieve this. Therefore, the feedback lacks the necessary actionability to be truly helpful to the authors. 

In conclusion, the feedback is relevant to the aspect of uncertainty disentanglement, but it does not provide the necessary actionable suggestions that would guide the authors in addressing the issues identified. This makes the feedback less useful for the authors in terms of providing a clear path for revision. 
[RESULT] 0",1,"The feedback provided is constructive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the importance of the topic and suggests a comparison with a well-known baseline, which is a positive approach to encouraging the authors to improve their work. However, the feedback could be perceived as slightly critical due to the use of phrases like ""missed opportunity"" and ""not in the main paper,"" which may imply that the authors have not adequately addressed the topic. Despite this, the feedback remains respectful and does not cross into negative or aggressive territory. Therefore, the feedback is relevant to the review and is delivered in a manner that is polite and helpful, but with a slight edge of criticism. So the overall score is 1. [RESULT] 1",0,"The feedback provided in the response is relevant to the review, as it addresses the aspect of uncertainty disentanglement. However, the feedback does not provide any evidence or references to support the claims made. The feedback suggests that the authors should have evaluated the method in a more detailed manner and compared it with Kendall and Gall. 2017, but it does not provide any specific evidence or references to back up these suggestions. The feedback also recommends that the authors should evaluate the aleatoric and epistemic uncertainty separately, but again, it does not provide any evidence or references to support this claim. Therefore, the feedback does not meet the criteria for verifiable feedback as it lacks the necessary evidence or references to support its claims. So the overall score is 0. [RESULT] 0",1,"The feedback provided is specific and directly relevant to the content of the draft. It addresses a particular point in the paper, which is the disentanglement of aleatoric and epistemic uncertainty. The feedback is tailored to the content of the paper, as it mentions the specific method used and the lack of evaluation in the main paper. It also provides a recommendation for future work, which is a constructive way to address the issue. The feedback is not a general comment for any scientific document, but rather a detailed critique of the specific aspect of the paper. Therefore, the feedback is in line with the score rubric's criteria for a score of 1. 
[RESULT] 1"
4202,NIPS_2020_630,"- more analysis on other network architectures are required. analyzing merely the ResNet and VGG family is not enough. the authors shall include results on other families such as DenseNet, GoogleLeNet, etc. - how do the authors estimate the local volume density with kNN (L116)? if one simply uses the region that contains k nearest neighbors to define density, it will have some issues. For instance, it will have a lot of discontinuities. Can the authors be more specific on this and provide more explanations? - I'm slightly confused about Fig. 2(a). the number of measurements for \chi^{l, l+1} is different to (more than) those of \chi^{gt} and \chi^{out}. what happened? there seems to be more variantions for \chi^{l, l+1} when the measurement scale is more fine-grained - does it apply to \chi^{gt} and \chi^{out} too? - the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?","- the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?",nan,nan,nan,nan,nan,-1,"The feedback provided in the review is somewhat actionable, as it suggests that the authors should show preliminary results on some of the directions mentioned. However, it lacks specificity and clarity in its suggestions. The feedback does not provide a clear path for the author to follow in order to improve their paper. It also fails to offer concrete steps or recommendations that would guide the author in addressing the issues identified. The feedback is more of a general commentary rather than a structured critique with actionable advice. Therefore, the feedback does not fully meet the criteria for actionability as outlined in the score rubric. So the overall score is -1. [RESULT] -1",1,"The feedback provided in the response is generally positive and respectful, which aligns with the score rubric's requirement for a polite and helpful tone. The reviewer acknowledges the potential of the authors' observation and agrees with the direction they are suggesting. However, the feedback could be perceived as slightly critical, as it suggests that the paper could be improved by showing preliminary results. This could be seen as a constructive critique, but it might also be interpreted as a lack of respect for the authors' current work. The feedback does not cross into the territory of being rude or disrespectful, but it could be more encouraging and less critical. Therefore, the feedback is respectful and constructive, but it could be more positive and less critical to fully meet the criteria for a score of 1. So the overall score is 1. [RESULT] 1",-1,"The feedback provided in the response does not meet the criteria for verifiable feedback. The reviewer makes a claim about the potential of the observation without providing any evidence or references to support this claim. The feedback also suggests that the authors should show preliminary results on some of the directions, but again, this suggestion is not supported by any evidence or references. The reviewer's comments are based on personal opinion rather than logical reasoning or references to the paper. Therefore, the feedback does not meet the standards of verifiable feedback as outlined in the score rubric. 

The feedback also lacks the depth and specificity that would be expected from an expert critic. The reviewer's comments are general and do not provide a clear direction for improvement. The feedback does not engage with the paper in a way that would allow for independent verification of the reviewer's claims. 

In conclusion, the feedback provided in the response does not meet the criteria for verifiable feedback and does not respect the aspect of the review. The feedback is not relevant to the review and does not provide any evidence or references to support the claims made. Therefore, the score is -1. 
[RESULT] -1",0,"The feedback provided in the response is somewhat specific, as it addresses the potential applications of the observation made in the paper. However, it lacks the depth and detail that would make it truly specific. The feedback mentions the potential for better architecture and more powerful training schemes, but it does not provide any concrete examples or suggestions for how the authors could improve their paper. Additionally, the feedback does not directly address any specific parts or methods in the paper, which makes it difficult to identify the exact elements being discussed. The feedback also fails to provide any constructive criticism or guidance on how the authors could enhance their paper. Therefore, while the feedback is somewhat relevant, it does not meet the criteria for specificity as outlined in the score rubric. So the overall score is 0. [RESULT] 0"
1686,ICLR_2023_1725,"Weakness: • This method utilize a DETR pretrained on COCO which limits the application of this method. For some common used inpainting dataset like CelebA, FFHQ, Places, Paris street View, LSUN, it is hard to acquire the corresponding segmentation map to train the DETR. • The author claims ‘COCO-panoptic is more challenging than center-aligned datasets’, but the author should prove the generalization of the proposed method on other domain datasets like face (FFHQ)、buildings (Paris Street View). • One important application of inpainting is object removing while this method aims for new object generation. What about the performance of the proposed method on the object removing? • I find in most figures (Fig 1, Fig 3-5, Fig 8-9), there exist an obvious color discrepancy between the ground truth and the results of this paper. Can you make an explanation? • When processing large missing area, does the segmentation completion network still has the ability to generate new objects or just inpaint with background contents? • How is the model complexity and inference time compared to other methods?",• How is the model complexity and inference time compared to other methods?,nan,nan,nan,nan,nan,-1,"The feedback provided in the review is relevant to the aspect of model complexity and inference time. However, it falls short in offering actionable suggestions that the author can use to improve their paper. The feedback merely asks a question without providing any guidance or recommendations on how to compare the model complexity and inference time with other methods. It lacks the specificity and clarity needed to be actionable. The feedback does not offer any concrete steps or recommendations that would help the author in addressing the issues related to model complexity and inference time. Therefore, the feedback is not as useful as it could be in guiding the author towards revision. So the overall score is -1. [RESULT] -1",0,"The feedback provided is neutral and factual, which aligns with the score rubric's description for a score of 0. The tone of the feedback is not negative, rude, disrespectful, or aggressive, and it does not attempt to encourage the authors. It simply asks a question about the model complexity and inference time, which is a relevant aspect to the review. However, the feedback does not provide any specific guidance or constructive criticism, which would have been more helpful to the authors. Therefore, while the feedback is polite and neutral, it lacks the depth and helpfulness that would have been expected for a higher score. So the overall score is 0. [RESULT] 0",0,"The feedback provided in the response is relevant to the review, as it asks for a comparison of the model complexity and inference time with other methods. However, the feedback does not provide any evidence or references to support the claim. It is a logical claim that could be verified by comparing the model to other methods, but without any supporting references, it falls short of the score rubric's criteria for verifiable feedback. Therefore, the feedback is not fully respected in the review, as it does not provide the necessary evidence to support the claim. So the score is 0. [RESULT] 0",1,"The feedback provided is quite specific and directly relevant to the content of the draft. It asks for a comparison of the model complexity and inference time with other methods, which is a clear and direct question that can be answered by examining specific parts of the paper. This feedback is tailored to the content of the paper, and it is easy to identify which part of the document is being discussed in the feedback. Therefore, the feedback is specific and directly relevant to the content of the draft, which aligns with the score rubric's criteria for a score of 1. 
[RESULT] 1"

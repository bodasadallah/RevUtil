Focused review:

Some part of this paper need to be clarified: 1. Why is vanilla conv used in l->g and g-l? Take l->g for an example, it introduces local info to global branch. I am just curious whether this transition is required. Can you just reserve l-l and g-g, and use channel shuffling on [Y^l, Y^g] to fuse local and global info, which should be more efficient? 2. Can you show some experimental results to show the necessity of l->g and g->l transitions? 3. For LFU, since the feature map patches are concatenated first and then a FU operator is applied, it also captures the information of whole feature map. Can you share some results of models with only LFU? 4. According to my understanding (correct me if I were wrong), since only 1x1 conv was used at spectral domain, it is equivalent to multiplying original feature map with a simple harmonic function at spatial domain, is it possible to do multiplication at spatial domain directly? The multiplied tensor should only requires several free parameters. 5. Could you clarify how to do channel splitting for group convolutions? Some groups for local operations and some for global ones, or every group will be split into local and global ones? --- I have read the rebuttal and my concerns are partially addressed. I am looking forward to further experimental results.

Review Point: 1. Why is vanilla conv used in l->g and g-l? Take l->g for an example, it introduces local info to global branch. I am just curious whether this transition is required. Can you just reserve l-l and g-g, and use channel shuffling on [Y^l, Y^g] to fuse local and global info, which should be more efficient?
Review Point: 2. Can you show some experimental results to show the necessity of l->g and g->l transitions?
Review Point: 3. For LFU, since the feature map patches are concatenated first and then a FU operator is applied, it also captures the information of whole feature map. Can you share some results of models with only LFU?
Review Point: 4. According to my understanding (correct me if I were wrong), since only 1x1 conv was used at spectral domain, it is equivalent to multiplying original feature map with a simple harmonic function at spatial domain, is it possible to do multiplication at spatial domain directly? The multiplied tensor should only requires several free parameters.
Review Point: 5. Could you clarify how to do channel splitting for group convolutions? Some groups for local operations and some for global ones, or every group will be split into local and global ones? --- I have read the rebuttal and my concerns are partially addressed. I am looking forward to further experimental results.
==================================================

Focused review:

* This method is most suitable for variables that have a single parent in the causal DAG -- the class label. This severely restricts the class of attributes that can be modeled and manifests in the paper as experiments with simple attributes (colors in AO-CLEVr, and materials in Zappos). In fact, prior work has noted that attributes (or other compositional modifiers) manifest very differently for different objects ([36] gives the examples from prior work: "fluffy" for towels vs. dogs, "ripe" for one fruit vs. another etc.). For these attributes, and many others, the data generating process is not so straightforward -- there are edges from both attribute labels and object labels to the core features. The authors do acknowledge this limitation in L326, however it is an important weakness to consider given that _difficult_ instances in real world datasets (where both object and attribute are parents of \phi_a for example) are fairly prevalent. * I'm a little unclear on some details surrounding the mapping from the causal graph to the mappings in Fig (c). A few clarifications: - I do not fully understand the role of the functions g_A^{-1} and g_O^{-1} in the causal graph. The SEM equations in Supp A.1 fully represent the relationships between the variables presented and does not seem to accommodate the extra two functions. Does the inclusion of these functions make additional assumptions on the structure? What do they mean in the context of the existing DAG? - Training these inverse models also seems to go against L130: "... that treats labels as causes, rather than as effects of the image" which is the standard embedding learning approach. I think the point where the analogy starts to break down is the assumption that the prototype feature can be approximated with the inferred feature from the image. This is equivalent to creating two new nodes in the graph (\hat{\phi_a}) and (\hat{\phi_o}), who each have a single parent x. These aren't the same as the nodes \phi_a, and \phi_o (as these are unobserved), and since their parents are only x, they already by construction satisfy the independence constraints. It would be helpful if more detail is provided for why this is consistent with the original proposal. - Given that phi_a, phi_o and x are all represented as multivariate gaussians, for the SEM model with independent noise, do any assumptions fail to hold when the functions relating these distributions are implemented as nonlinear MLPs? * In the final implementation, it is difficult to see where the causal interpretation developed through the paper actually manifests. It would help if the rebuttal contained a clear-cut difference between a standard embedding learning approach and the proposed method. As far as I can tell, the main difference is the inclusion of the independence loss which is inspired from the structure of independences encoded in the proposed DAG. All the other loss functions are commonly explored in prior work: the data likelihood loss (L184) is identical to ones used for embedding learning methods, and the invertible embedding loss (L208) is the same as an auxiliary classification loss in prior work. Is this interpretation correct? - Another place where this confusion arises is in the ablation experiments in Supp Table S2 where weights of various losses are set to zero and models are retrained. Here, it appears that \lambda_{invert} is an essential component of the model, and removing it produces results that are far worse than removing the independence loss. What could be the reason for this? Moreover, it seems like this is a regularizer that has no relation to the original causal interpretation proposed.

Review Point: * This method is most suitable for variables that have a single parent in the causal DAG -- the class label. This severely restricts the class of attributes that can be modeled and manifests in the paper as experiments with simple attributes (colors in AO-CLEVr, and materials in Zappos). In fact, prior work has noted that attributes (or other compositional modifiers) manifest very differently for different objects ([36] gives the examples from prior work: "fluffy" for towels vs. dogs, "ripe" for one fruit vs. another etc.). For these attributes, and many others, the data generating process is not so straightforward -- there are edges from both attribute labels and object labels to the core features. The authors do acknowledge this limitation in L326, however it is an important weakness to consider given that _difficult_ instances in real world datasets (where both object and attribute are parents of \phi_a for example) are fairly prevalent.
Review Point: * I'm a little unclear on some details surrounding the mapping from the causal graph to the mappings in Fig (c). A few clarifications:
Review Point: - I do not fully understand the role of the functions g_A^{-1} and g_O^{-1} in the causal graph. The SEM equations in Supp A.1 fully represent the relationships between the variables presented and does not seem to accommodate the extra two functions. Does the inclusion of these functions make additional assumptions on the structure? What do they mean in the context of the existing DAG?
Review Point: - Training these inverse models also seems to go against L130: "... that treats labels as causes, rather than as effects of the image" which is the standard embedding learning approach. I think the point where the analogy starts to break down is the assumption that the prototype feature can be approximated with the inferred feature from the image. This is equivalent to creating two new nodes in the graph (\hat{\phi_a}) and (\hat{\phi_o}), who each have a single parent x. These aren't the same as the nodes \phi_a, and \phi_o (as these are unobserved), and since their parents are only x, they already by construction satisfy the independence constraints. It would be helpful if more detail is provided for why this is consistent with the original proposal.
Review Point: - Given that phi_a, phi_o and x are all represented as multivariate gaussians, for the SEM model with independent noise, do any assumptions fail to hold when the functions relating these distributions are implemented as nonlinear MLPs?
Review Point: * In the final implementation, it is difficult to see where the causal interpretation developed through the paper actually manifests. It would help if the rebuttal contained a clear-cut difference between a standard embedding learning approach and the proposed method. As far as I can tell, the main difference is the inclusion of the independence loss which is inspired from the structure of independences encoded in the proposed DAG. All the other loss functions are commonly explored in prior work: the data likelihood loss (L184) is identical to ones used for embedding learning methods, and the invertible embedding loss (L208) is the same as an auxiliary classification loss in prior work. Is this interpretation correct?
Review Point: - Another place where this confusion arises is in the ablation experiments in Supp Table S2 where weights of various losses are set to zero and models are retrained. Here, it appears that \lambda_{invert} is an essential component of the model, and removing it produces results that are far worse than removing the independence loss. What could be the reason for this? Moreover, it seems like this is a regularizer that has no relation to the original causal interpretation proposed.
==================================================

Focused review:

Weaknesses 1. There is no theoretical study of AAP. Algorithm 2 removes dead connections in a greedy manner by making the scores of dead weights as zero, even though they can be alive by revived connections at future iterations. Repeating Steps 1 & 2 of Section 3.2 until the unguaranteed convergence can make it remove important weights having high saliency scores at the worst scenario. Theoretical guarantees on the convergence of the algorithm and the scores of removed weights are needed. 2. The experiments are not practical. The proposed approach works well only with high compression ratios which degrade the performance of original models. Considering that the objective of model compression is to maintain the original accuracy requiring less resources, such high compression ratios that decrease the accuracy more than 5 points do not seem practical in real-world scenarios. 3. The novelty is limited. Reviving dead connection is not a new idea.

Review Point: 1. There is no theoretical study of AAP. Algorithm 2 removes dead connections in a greedy manner by making the scores of dead weights as zero, even though they can be alive by revived connections at future iterations. Repeating Steps 1 & 2 of Section 3.2 until the unguaranteed convergence can make it remove important weights having high saliency scores at the worst scenario. Theoretical guarantees on the convergence of the algorithm and the scores of removed weights are needed.
Review Point: 2. The experiments are not practical. The proposed approach works well only with high compression ratios which degrade the performance of original models. Considering that the objective of model compression is to maintain the original accuracy requiring less resources, such high compression ratios that decrease the accuracy more than 5 points do not seem practical in real-world scenarios.
Review Point: 3. The novelty is limited. Reviving dead connection is not a new idea.
==================================================

Focused review:

Weaknesses: 1. The innovation is marginal as main part of proposed method is simply combination of existing works (Transformer (Ashish et al., 2017), cVAE (Kihyuk et al., 2015), ELBO (Simon et al., 2019), and GECO (Danilo et al., 2018).) Some recent advances(e.g. Grphformer) are absence in related work and discussions. 2. The authors introduce the ProbTransfomer model in an effort to solve the challenge of RNA folding. However, the likelihood of alternative RNA structures arising are mostly dictated by biological circumstances, which the authors do not examine. In addition, authors should consider RNA abundance when building datasets.

Review Point: 1. The innovation is marginal as main part of proposed method is simply combination of existing works (Transformer (Ashish et al., 2017), cVAE (Kihyuk et al., 2015), ELBO (Simon et al., 2019), and GECO (Danilo et al., 2018).) Some recent advances(e.g. Grphformer) are absence in related work and discussions.
Review Point: 2. The authors introduce the ProbTransfomer model in an effort to solve the challenge of RNA folding. However, the likelihood of alternative RNA structures arising are mostly dictated by biological circumstances, which the authors do not examine. In addition, authors should consider RNA abundance when building datasets.
==================================================

Focused review:

- I am not fully convinced by the effectiveness of the method, in a larger context. For example, the results are reported with only VGG backbones, it's not clear whether the proposed framework can be used to other backbones, such as ResNet which is used in most SOTA segmentation models. - Besides, the baselines used in table 1 are mainly UDA methods. It would be helpful to see if the proposed framework can be useful in a UDA setting (for example, GTAV->Cityscapes), and how the technique compare with existing methods in a UDA setting. It would be interesting to see if the method can provide some improvements to showcase the advantage of discovering latent domains. - I am also a bit skeptical of the selection of K. In my opinion, it might not be trivial to select K when there is no validation data available. However, the optimal K can be quite different depends on the data distribution. It would be helpful to comment on how to select K in practice. - To further understand how different components can help domain adaptation. I suggest to conduct tsne visualization on the feature distributions of different models. In particular, it would be interesting to see what's the distribution of the K clusters of the target data.

Review Point: - I am not fully convinced by the effectiveness of the method, in a larger context. For example, the results are reported with only VGG backbones, it's not clear whether the proposed framework can be used to other backbones, such as ResNet which is used in most SOTA segmentation models.
Review Point: - Besides, the baselines used in table 1 are mainly UDA methods. It would be helpful to see if the proposed framework can be useful in a UDA setting (for example, GTAV->Cityscapes), and how the technique compare with existing methods in a UDA setting. It would be interesting to see if the method can provide some improvements to showcase the advantage of discovering latent domains.
Review Point: - I am also a bit skeptical of the selection of K. In my opinion, it might not be trivial to select K when there is no validation data available. However, the optimal K can be quite different depends on the data distribution. It would be helpful to comment on how to select K in practice.
Review Point: - To further understand how different components can help domain adaptation. I suggest to conduct tsne visualization on the feature distributions of different models. In particular, it would be interesting to see what's the distribution of the K clusters of the target data.
==================================================

Focused review:

1. I think the paper goes a bit too quickly from the problem description of systems producing extractions which are too specific, to their specific flavor of “compact” extractions designed to solve this problem. The paper does cite different sources which explain the problem in more depth, but I don’t think there’s a consensus that the type of compact extractions the authors promote is the ideal solution for the problem.
E.g. based on the criteria for informative extractions described in the paper “Annotating and predicting non-restrictive noun phrase modifications” (Stanovsky et. al 2016), extracting “Hercule Poirot” “is” “a Belgian detective” from the sentence “Hercule Poirot is a Belgian detective, created by Agatha Christie” might not be specific enough (as the modification here seems restrictive). More discussion of this criteria and how it relates to compact extractions will be helpful.
2. Continuing the above point, the evaluation in this paper is intrinsic and uses datasets modified to include extractions of the type the authors are promoting, so comparison to existing systems is not 1:1. Some kind of extrinsic evaluation (e.g. on a KBC or KB alignment task) could be helpful in showing that the suggested openIE method does in fact work better for upstream tasks.
3. The authors give no indication of whether the modified datasets, code, system, or the data used in manual evaluation will be made available. Assuming these are not public, it is harder still to estimate whether the suggested approach is really a good fit for upstream tasks (this is also why I give this paper low reproducibility scores. If the authors were to share code/data, I would have assigned a higher score).
Despite the stated weaknesses, I'm narrowly in favor of accepting this paper. It deals with an important and not frequently discussed practical problem (making OpenIE triplets more useful for upstream tasks), the approach is interesting and the evaluation shows that the system produces triplets of the type the authors advocate more accurately than other systems (I think more can be done to convince that this style of queries is indeed optimal for upstream tasks, but still, this work seems like a step in the right direction).
I would highly encourage the authors to make the code, datasets and sample extractions public, as it can really increase the impact of this work, allow others to consume and evaluate the outputs in real-world tasks, etc.

Review Point: 1. I think the paper goes a bit too quickly from the problem description of systems producing extractions which are too specific, to their specific flavor of “compact” extractions designed to solve this problem. The paper does cite different sources which explain the problem in more depth, but I don’t think there’s a consensus that the type of compact extractions the authors promote is the ideal solution for the problem. E.g. based on the criteria for informative extractions described in the paper “Annotating and predicting non-restrictive noun phrase modifications” (Stanovsky et. al 2016), extracting “Hercule Poirot” “is” “a Belgian detective” from the sentence “Hercule Poirot is a Belgian detective, created by Agatha Christie” might not be specific enough (as the modification here seems restrictive). More discussion of this criteria and how it relates to compact extractions will be helpful.
Review Point: 2. Continuing the above point, the evaluation in this paper is intrinsic and uses datasets modified to include extractions of the type the authors are promoting, so comparison to existing systems is not 1:1. Some kind of extrinsic evaluation (e.g. on a KBC or KB alignment task) could be helpful in showing that the suggested openIE method does in fact work better for upstream tasks.
Review Point: 3. The authors give no indication of whether the modified datasets, code, system, or the data used in manual evaluation will be made available. Assuming these are not public, it is harder still to estimate whether the suggested approach is really a good fit for upstream tasks (this is also why I give this paper low reproducibility scores. If the authors were to share code/data, I would have assigned a higher score). Despite the stated weaknesses, I'm narrowly in favor of accepting this paper. It deals with an important and not frequently discussed practical problem (making OpenIE triplets more useful for upstream tasks), the approach is interesting and the evaluation shows that the system produces triplets of the type the authors advocate more accurately than other systems (I think more can be done to convince that this style of queries is indeed optimal for upstream tasks, but still, this work seems like a step in the right direction). I would highly encourage the authors to make the code, datasets and sample extractions public, as it can really increase the impact of this work, allow others to consume and evaluate the outputs in real-world tasks, etc.
==================================================

Focused review:

Questions: - constructing a path of decreasing λ > can you detail more at this point ? Given the explosive cardinality of the set of possible lambdas, it seems many choices are possible. Do you fix the ratios between consecutive lambdas and only tune one parameter, etc? This is done L184, you could add a reference to 3.1 on L40. - Recall that we are attempting ... > I don't think you said that you were considering the sequential setting before (some screening rules work in the non sequential setting). this deserves a clarification : your rule is useful in the sequential setting but cannot be applied if the problem is to be solved with a single lambda (eg, fitting on full dataset at optimal lambda, after determining the optimal lambda bia K-fold CV) - the true solution is never available (ie in l 137 you wouldn't have the exact gradient at the previous value on the path). Can you comment ? - which is equivalent to breaking the assumption that the gradient vector is a piece-wise linear function > can you explain ? More than being piecewise linear, you need its slope to be bounded by 1, don't you ? - There is a rich literature on Screening rules that authors fail at citing in the introduction. Experimental questions: - you chose t = 10 −2 (going from lambda_max to lambda_max / 100). From my experience in cross validation settings, you often need to go to alpha_max / 1000. If you keep l = 100, this makes your gride coarser, hence the rule less efficient as the gradient approximation is less correct and the RHS in your screening rules can even become negative (no screening happening)? See [1], Fig 5b. I would like to see the efficiency of your rule in this setting. - The usefulness of the screening rule depends on the frequency by which it is violated. > in my opinion, it is better to have a violating screening rules which screens a lot of variables than an ineffective one being very conservative. This is at the heart of working set policies (see [2,3]) - You often use n = 200 and stop the path when the number of selected variables grow larger than n (is it possible? For the Lasso, there always exists a solution with support of size at most n). So your solutions are quite sparse, and screening remains effective However, because of the bias of the Lasso penalty, optimal solutions from a prediction performance (MSE on left out data) are usually quite dense. To be fully convincing, you may need to run your simulations with larger n, eg the rcv1_train, log1p, news20 of LIBSVM (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) Lesser important questions: - l 74 can you explain how you break ties and why they don't matter ? - l 130 why should the screened set (eliminated variables) be included in the active set ? shouldn't the equation be "we risk of returning S not included in complement(T) ?" l128 your screened set sounded to me like the set of screened variables, ie removed variables; from the sequel I infer that it is in fact the set of variables who survived screening (since "this screened set should resemble the active set"). This should be clarified. If S is the set of selected variables, you should fear returning S not containing T, no ? - in the equation below l145 (if you can number all your equations when there is no horizontal room constraint, in my opinion it makes communication easier), the RHS is a vector. - L184 could plot an example of such sequence to ease understanding ? Reformulations (I don't expect answers if you don't have room): rules that allow predictors to be discarded before estimating the model > most efficient screening rules are dynamic, i.e. discard coefficients *along the optimization process*. I suggest using the latter formulation. Our rule is heuristic > could you consider using the standard terminology "unsafe" here ? Following the "Safe rules" paper by the Lasso, and follow ups in Theorem 1 wedge is not defined, using a plain "and" or '&' may be clearer for someone not familiar with this logic notation - active set can be ambiguous in the literature, you may want to define it as the support of the solution somewhere

Review Point: - Recall that we are attempting ... > I don't think you said that you were considering the sequential setting before (some screening rules work in the non sequential setting). this deserves a clarification : your rule is useful in the sequential setting but cannot be applied if the problem is to be solved with a single lambda (eg, fitting on full dataset at optimal lambda, after determining the optimal lambda bia K-fold CV) - the true solution is never available (ie in l 137 you wouldn't have the exact gradient at the previous value on the path). Can you comment ?
Review Point: - which is equivalent to breaking the assumption that the gradient vector is a piece-wise linear function > can you explain ? More than being piecewise linear, you need its slope to be bounded by 1, don't you ?
Review Point: - There is a rich literature on Screening rules that authors fail at citing in the introduction. Experimental questions:
Review Point: - you chose t = 10 −2 (going from lambda_max to lambda_max / 100). From my experience in cross validation settings, you often need to go to alpha_max / 1000. If you keep l = 100, this makes your gride coarser, hence the rule less efficient as the gradient approximation is less correct and the RHS in your screening rules can even become negative (no screening happening)? See [1], Fig 5b. I would like to see the efficiency of your rule in this setting.
Review Point: - The usefulness of the screening rule depends on the frequency by which it is violated. > in my opinion, it is better to have a violating screening rules which screens a lot of variables than an ineffective one being very conservative. This is at the heart of working set policies (see [2,3]) - You often use n = 200 and stop the path when the number of selected variables grow larger than n (is it possible? For the Lasso, there always exists a solution with support of size at most n). So your solutions are quite sparse, and screening remains effective However, because of the bias of the Lasso penalty, optimal solutions from a prediction performance (MSE on left out data) are usually quite dense. To be fully convincing, you may need to run your simulations with larger n, eg the rcv1_train, log1p, news20 of LIBSVM (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) Lesser important questions:
Review Point: - l 74 can you explain how you break ties and why they don't matter ?
Review Point: - l 130 why should the screened set (eliminated variables) be included in the active set ? shouldn't the equation be "we risk of returning S not included in complement(T) ?" l128 your screened set sounded to me like the set of screened variables, ie removed variables; from the sequel I infer that it is in fact the set of variables who survived screening (since "this screened set should resemble the active set"). This should be clarified. If S is the set of selected variables, you should fear returning S not containing T, no ?
Review Point: - in the equation below l145 (if you can number all your equations when there is no horizontal room constraint, in my opinion it makes communication easier), the RHS is a vector.
Review Point: - L184 could plot an example of such sequence to ease understanding ? Reformulations (I don't expect answers if you don't have room): rules that allow predictors to be discarded before estimating the model > most efficient screening rules are dynamic, i.e. discard coefficients *along the optimization process*. I suggest using the latter formulation. Our rule is heuristic > could you consider using the standard terminology "unsafe" here ? Following the "Safe rules" paper by the Lasso, and follow ups in Theorem 1 wedge is not defined, using a plain "and" or '&' may be clearer for someone not familiar with this logic notation - active set can be ambiguous in the literature, you may want to define it as the support of the solution somewhere
==================================================

Focused review:

Weaknesses ---------- 1. Except the new definition of the Generalized Gauss-Newton matrix (that is not pursued), no other proposition in the paper is original. 2. As the authors point themselves, analyzing the EF as a variance adaptation method would have explained its efficiency and strengthened the paper: "This perspective on the empirical Fisher is currently not well studied. Of course, there are obvious difficulties ahead:" Overcoming these difficulties is what a research paper is about, not only discussing them. 3. The main point of the paper relies in paragraph 3.2. This requires clear and sound propositions such as: for a well-specified model, and a consistent estimator, the empirical fisher matrix converges to the Hessian at a rate ... It is claimed to be specified in Appendix C.3 but there seems to be a referencing problem in the paper. This would highlight both the reasoning of previous papers and the difference with the actual approximation made here. Minor comments: --------------- Typos: - Eq. 5 no square for gradient of a _ n - Eq. 8 subscript theta should be under p not log - Replace the occurrences of Appendix A to Appendix C Conclusion: ---------- Overall I think this is an good lecture on natural gradient and its subtleties, yet not a research paper since almost no new results are demonstrated. Yet, if the choice has to be made between another paper that uses the empirical Fisher and this one that explains it, I'll advocate for this paper. Therefore I tend to marginally accept this paper though I think its place is in lecture notes (in fact Martens long review of natural gradient [New insights and perspectives on the natural gradient method, Martens 2014] should incorporate it, that is where this paper should be from my opinion.) After discussion -------------------- After the discussion, I increased my score, I don't think that it is a top paper as it does not have new results but it should clearly be accepted as it would be much more helpful than "another state of the art technique for deep learning" with some misleading approximations like ADAM. Note that though refining the definition of a generalized gauss-newton method seems to be a detail, I think it could have a real potential for further analysis in optimization.

Review Point: ---------- 1. Except the new definition of the Generalized Gauss-Newton matrix (that is not pursued), no other proposition in the paper is original.
Review Point: 2. As the authors point themselves, analyzing the EF as a variance adaptation method would have explained its efficiency and strengthened the paper: "This perspective on the empirical Fisher is currently not well studied. Of course, there are obvious difficulties ahead:" Overcoming these difficulties is what a research paper is about, not only discussing them.
Review Point: - Eq. 5 no square for gradient of a _ n - Eq. 8 subscript theta should be under p not log - Replace the occurrences of Appendix A to Appendix C Conclusion: ---------- Overall I think this is an good lecture on natural gradient and its subtleties, yet not a research paper since almost no new results are demonstrated. Yet, if the choice has to be made between another paper that uses the empirical Fisher and this one that explains it, I'll advocate for this paper. Therefore I tend to marginally accept this paper though I think its place is in lecture notes (in fact Martens long review of natural gradient [New insights and perspectives on the natural gradient method, Martens 2014] should incorporate it, that is where this paper should be from my opinion.) After discussion -------------------- After the discussion, I increased my score, I don't think that it is a top paper as it does not have new results but it should clearly be accepted as it would be much more helpful than "another state of the art technique for deep learning" with some misleading approximations like ADAM. Note that though refining the definition of a generalized gauss-newton method seems to be a detail, I think it could have a real potential for further analysis in optimization.
==================================================

Focused review:

In my opinion, the paper is very good, but it still has two main weaknesses that slightly hinder its significance: - the accessibility for a more general ML audience and the fact that a lot of the content (including some evaluation) is in the Appendix - not enough emphasis on the strong assumptions, e.g. knowing the causal graph On the other hand, I can understand the first point is difficult to fix, also given the paper would probably fit better a journal in order to be properly self contained.

Review Point: - the accessibility for a more general ML audience and the fact that a lot of the content (including some evaluation) is in the Appendix - not enough emphasis on the strong assumptions, e.g. knowing the causal graph On the other hand, I can understand the first point is difficult to fix, also given the paper would probably fit better a journal in order to be properly self contained.
==================================================

Focused review:

Weaknesses
Conceptual Concerns and situating within the broader concerns surrounding XAI (Explainable AI)
The paper early-on cites the paper on "The Mythos of Model Interpretability" by Lipton to effectively state that deep learning methods can be black boxes. However, I believe it fails to contextualize the proposed method within the larger discourse on XAI methods established in that paper. Specifically, the use of attribution maps (or saliency maps or heat maps as alternatively known) has been shown to suffer from several issues. As Barredo Arrieta et al. point out, “there is absolutely no consistency behind what is known as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approaches alike” [R1, Sec. 5.3]. Also see [R2, R3] for a discussion on the issues with saliency/attribution maps.
The experiments still uses these and other SHAP based methods known to have issues to support their argument. (See [R4] for links to other papers discussing how SHAP can produce inconsistent and unstable explanations).
I'd like to hear the authors' thoughts on why, broadly, their empirical argument which is built upon these methods will not suffer from similar issues. I don't see in the experiments how the proposed method would implicitly be robust to these issues surrounding instability since the method is inherently dependent on having a well-separated concept of foil and corpus samples (or at least that these samples need to already have a human understandable semantic meaning which is subject to biases in the human perception).
One of the core issues of the current state of XAI is how the methods largely depend on biases and interpretation of the observer, and it would be good to establish how this method is not adding to the same issue.
[R1] Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. - Barredo Arrieta et al. [R2] Sanity checks for saliency maps. - Adebayo et al. [R3] Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning - Atrey et al. [R4] The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective - Krishna et al.

Review Point: - Barredo Arrieta et al. [R2] Sanity checks for saliency maps.
Review Point: - Adebayo et al. [R3] Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning - Atrey et al. [R4] The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective - Krishna et al.
==================================================

Focused review:

Weaknesses:
Clarity of results.
A drawback (which I think can be resolved to some extent) is that although the overall idea is clean, the presentation is way too overwhelming. For example, it will help the readers if important steps in the algorithms can be explained / highlighed / commented, or given more wording in explaining their roles. Also, Theorem 1 poses a huge challenge to the whole flow as the conditions are too complicated. I will suggest replacing it with an informal theorem, and put all these conditions to the appendix.
Relation to the literature.
Although the authors generally did a good job in relating to the literature, more discussion on the novelty of this idea can further improve this work and posit this paper appropriately in the literature. For instance, 1) how does the construction of uncertainty quantification differ from that in the online setting (e.g., Jia et al. 2022)? Is there special tricks for dealing with distribution shift? 2) how does the analysis of 2-layer neural networks rely on existing ones, and which parts are specific to the new offline setting? etc. Questions: Since Q ∗
is used for fitting the networks, I am curious about the definition of Q ∗
. Is it equivalent to the class of 2-layer neural networks with ReLU activation, or is it a superset of it?
What is the relationship between H n t k
and the class of 2-layer NN with ReLU activation? Please clarify this to help the readers.
The conditions in Theorem 1 are too lengthy. Is it possible to reduce it to a cleaner version (potentially with some harmless relaxation)?
It will help if there is a sketch or overview of the key theoretical techniques in Theorem 1. For example, why do the randomized rewards lead to a valid uncertainty quantifier.
Minor issues:
The last sentence in Section 4 is difficult to read. What does it mean by `` with a provable implicit uncertainty quantifier and O ~ ( 1 / K ) ''?
Remark 1 is a bit difficult to read. What does it mean by ``data-dependent quantity that measures the number principle dimensions over which the.."?
What does σ ′
mean in Definition 1? Please provide a formal definition.

Review Point: 1) how does the construction of uncertainty quantification differ from that in the online setting (e.g., Jia et al. 2022)? Is there special tricks for dealing with distribution shift?
Review Point: 2) how does the analysis of 2-layer neural networks rely on existing ones, and which parts are specific to the new offline setting? etc. Questions: Since Q ∗ is used for fitting the networks, I am curious about the definition of Q ∗ . Is it equivalent to the class of 2-layer neural networks with ReLU activation, or is it a superset of it? What is the relationship between H n t k and the class of 2-layer NN with ReLU activation? Please clarify this to help the readers. The conditions in Theorem 1 are too lengthy. Is it possible to reduce it to a cleaner version (potentially with some harmless relaxation)? It will help if there is a sketch or overview of the key theoretical techniques in Theorem 1. For example, why do the randomized rewards lead to a valid uncertainty quantifier. Minor issues: The last sentence in Section 4 is difficult to read. What does it mean by `` with a provable implicit uncertainty quantifier and O ~ ( 1 / K ) ''? Remark 1 is a bit difficult to read. What does it mean by ``data-dependent quantity that measures the number principle dimensions over which the.."? What does σ ′ mean in Definition 1? Please provide a formal definition.
==================================================

Focused review:

- Foremost concern: it seems that the only natural case for this framework is the application to bandit feedback in Section 5. Some more discussion of other natural settings captured by the framework would make the paper more convincing. This is reinforced by the fact that for Theorems 1 and 2 to be non-vacuous, the biases B_t must have some polynomial decay 1/t^\beta. The paper claims to resolve an open problem by [Krichene et al. ‘15]; could the authors clarify which open question is being resolved? At a glance (if I’m reading the same version), the discussion on the bandit problem seems to be posing a different open question than what is resolved in this paper. - Especially in light of the need for the biases to decay, it’s not obvious why the general result of Theorem 1 should justify the framework in a standalone manner. The case of unbiased feedback (Corollary 1) seems to have been addressed already by [Krichene et al. ‘15]. The - The claims of improvement of Theorem 2 (dynamic regret bounds) over [Besbes et al. ‘15] need to be clearer. The claimed benefits are (a) not requiring periodic restarts, and (b) applying to non-convex problems. To (a), if the regret comparator is moving, I’m not sure why it’s bad for an algorithm to qualitatively involve forgetting about the past, especially if the regret bound turns out to be optimal. To (b), the convexity is getting bypassed by an infinite-dimensional linearization, which changes the entire computational model. These are both good remarks for comparing this work to [Besbes et al. ‘15], but it’s not clear they should count as improvements per se. - For the bandit static regret section, see below about missing references to this well-studied problem. - The bandit dynamic regret result seems to me to be new, and worth pointing out. This would require more discussion of why the results don’t follow straightforwardly from other approaches to continuous bandit optimization; see the point about missing references. - The experimental methodology in the appendix leaves many questions. What is the domain? What is the “arbitrarily drawn” choice of reward function; what is its Lipschitz constant? Figure 1 shows that discretized-EXP3 doesn’t converge on the best arm; is this due to an unfairly chosen discretization? Without details, it is not clear what conclusion to extract from the experiments. (My overall score does not hinge on this; I don’t think papers in this line of work need to be supplemented by experiments.) - Multiple clarity issues; see below.

Review Point: - Foremost concern: it seems that the only natural case for this framework is the application to bandit feedback in Section 5. Some more discussion of other natural settings captured by the framework would make the paper more convincing. This is reinforced by the fact that for Theorems 1 and 2 to be non-vacuous, the biases B_t must have some polynomial decay 1/t^\beta. The paper claims to resolve an open problem by [Krichene et al. ‘15]; could the authors clarify which open question is being resolved? At a glance (if I’m reading the same version), the discussion on the bandit problem seems to be posing a different open question than what is resolved in this paper.
Review Point: - Especially in light of the need for the biases to decay, it’s not obvious why the general result of Theorem 1 should justify the framework in a standalone manner. The case of unbiased feedback (Corollary 1) seems to have been addressed already by [Krichene et al. ‘15]. The - The claims of improvement of Theorem 2 (dynamic regret bounds) over [Besbes et al. ‘15] need to be clearer. The claimed benefits are (a) not requiring periodic restarts, and (b) applying to non-convex problems. To (a), if the regret comparator is moving, I’m not sure why it’s bad for an algorithm to qualitatively involve forgetting about the past, especially if the regret bound turns out to be optimal. To (b), the convexity is getting bypassed by an infinite-dimensional linearization, which changes the entire computational model. These are both good remarks for comparing this work to [Besbes et al. ‘15], but it’s not clear they should count as improvements per se.
Review Point: - For the bandit static regret section, see below about missing references to this well-studied problem.
Review Point: - The bandit dynamic regret result seems to me to be new, and worth pointing out. This would require more discussion of why the results don’t follow straightforwardly from other approaches to continuous bandit optimization; see the point about missing references.
Review Point: - The experimental methodology in the appendix leaves many questions. What is the domain? What is the “arbitrarily drawn” choice of reward function; what is its Lipschitz constant? Figure 1 shows that discretized-EXP3 doesn’t converge on the best arm; is this due to an unfairly chosen discretization? Without details, it is not clear what conclusion to extract from the experiments. (My overall score does not hinge on this; I don’t think papers in this line of work need to be supplemented by experiments.) - Multiple clarity issues; see below.
==================================================

Focused review:

- The restriction seems quite severe. One wonders if there are even slightly more complicated models that could improve over this result. In particular, the observation that this can be fit almost exactly like an Ising model isn't really surprising. - The analysis looks at a single data set. More detailed experiments are needed to draw the kinds of conclusions that are suggested by this work. This is particularly important for me as the motivation is that lots of applications have the feature necessary for this model to make sense. - I like the motivating problem, but it seems that you might want some correlations between the different classes (even if mild) on any real data. - The technical contributions quite closely follow existing work which limits their novelty. ---Post Rebuttal--- I'm still a bit skeptical that such a coarse approach would really work on a broad range of data sets - though there is enough variety to suggest that perhaps it does. The comment in the rebuttal, "It may not be as severe as it may seem," does little though to help me understand the counterintuitive (at least in my mind) nature of the experimental results. Does changing which level is designated as '0' impact the quality of the results?

Review Point: - The restriction seems quite severe. One wonders if there are even slightly more complicated models that could improve over this result. In particular, the observation that this can be fit almost exactly like an Ising model isn't really surprising.
Review Point: - The analysis looks at a single data set. More detailed experiments are needed to draw the kinds of conclusions that are suggested by this work. This is particularly important for me as the motivation is that lots of applications have the feature necessary for this model to make sense.
Review Point: - I like the motivating problem, but it seems that you might want some correlations between the different classes (even if mild) on any real data.
Review Point: - The technical contributions quite closely follow existing work which limits their novelty. ---Post Rebuttal--- I'm still a bit skeptical that such a coarse approach would really work on a broad range of data sets - though there is enough variety to suggest that perhaps it does. The comment in the rebuttal, "It may not be as severe as it may seem," does little though to help me understand the counterintuitive (at least in my mind) nature of the experimental results. Does changing which level is designated as '0' impact the quality of the results?
==================================================

Focused review:

1. In this paper, the labeling confidences over partial label examples (F_P) are estimated via label propagation and kept unchanged in the follow-up optimization procedure. Is it possible to jointly optimize F_P with the predictive model as in Eq.(4)? 2. It is impressive that the proposed approach achieves significantly better performance over state-of-the-art comparing approaches. Furthermore, it would be more informative if some fine-grained conclusions can be drawn w.r.t. the properties of the data sets. For instance, which factors of the data sets would have stronger influence on the performance of the proposed approach?

Review Point: 1. In this paper, the labeling confidences over partial label examples (F_P) are estimated via label propagation and kept unchanged in the follow-up optimization procedure. Is it possible to jointly optimize F_P with the predictive model as in Eq.(4)?
Review Point: 2. It is impressive that the proposed approach achieves significantly better performance over state-of-the-art comparing approaches. Furthermore, it would be more informative if some fine-grained conclusions can be drawn w.r.t. the properties of the data sets. For instance, which factors of the data sets would have stronger influence on the performance of the proposed approach?
==================================================

Focused review:

Weaknesses/Questions
It is not clear from the main paper why the authors used ADDGCN as a base of their solution. The answer is found in the appendix, that this method allows estimating P ( l i l j , x )
, which is needed to calculate an estimate of combinatorial scores. This should be better explained in the main paper.
It is not entirely clear how the transition matrix works in this paper since T i j = P ( l j ∈ y ¯ ∧ l i ∉ y ¯ l j ∉ y ∧ l i ∈ y )
, the Appendindix describe both symmetrical and pariflip transition matrix T
with noise rate ρ
, but it's presented in the way that suggests that there are no other labels in y
, so it is not entirely clear how dataset generation is handled in case of more than one label in y
to avoid "collisions".
How long is ADDGCN trained before being used as h
in Combinatorial Correction? I don't see this information.
It is not clear to me how it is ensured that y ∗
has the same number of positive labels as y ¯ .
The algorithm requires finding of threshold δ
, the authors in their experiments use a noisy validation set, but this seems to work only because the noise considered in the experiments is instance independent and uniform (almost, since I assume authors prevent label collisions). With instance-dependent noise finding the right threshold might be very difficult without a clean validation set.
It also seems to me that the memorization effect might not be that strong when label noise is not longer instance independent.
So far, it seems that the larger threshold δ
, the better performance, so why weren't the higher thresholds tested?
The authors use the Tsybakov condition but do not cite the original work of Tsybakov et al..
Also, in the introduction, when talking about applications of multi-label classification in different domains and noisy labels, the authors cite only very recent works, I believe these domains and noisy labels were also considered in older works that would be nice to cite.
The method is designed only for the specific type of noisy labels, the noise when labels are flipped to another label, I'm not sure if, indeed, it is so common to be certain about a number of objects, especially with problems of high granularity of labels, where objects might easily overlap. In the provided example, we could say that we are missing labels like: "life jacket", "water" or "waves".
I also do not entirely agree with the authors' statements that it's not accurate to consider classification with missing labels as classification with noisy labels. Missing labels can be seen as one-sided noise and a special case of a more general setting than considered in the paper when any label can be either flipped from positive to negative or from negative to positive. NITs:
Figure 2 and 3 are almost impossible to read without 400% zoom. The document should be readable when printed.
Some language suggestions:
label dependences -> label dependencies (I think this is more correct, in a few places)
(...) the deep model would firstly memorize (...) -> first
Definition 2 shares the similar idea and bound the uncertainty of S. -> bounds
Additionaly, (...) -> Additionally
We exploit pretrained ResNet-50 before. - I would remove before (or change the sentence in another way), it sounds strange in the present simple.

Review Point: - I would remove before (or change the sentence in another way), it sounds strange in the present simple.
==================================================

Focused review:

1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches.
Suggestions: 1. Move the Appendix header to page 11 Questions: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ?

Review Point: 1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. Suggestions:
Review Point: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ?
==================================================

Focused review:

Weakness of this paper 1. The novelty of this paper might be limited. Previous works have explored the possibility of utilizing text as weak supervision for video representation learning (MIL-NCE), from the reviewer’s perspective, the main difference is that the different loss function is adopted. 2. Compared with methods that adopt other information (such as audio) as weak supervision, there is an inherent advantage of using text as supervision since pretrained text models such as BERT can be utilize as a guidance. So a meaningful comparison would be the comparison with TWS and MIL-NCE, although the proposed method can achieve comparable performance with other methods with much less data, the author does not give analysis about what design in the proposed method that enables this. 3. The performance comparison is not convincing enough. From Table 3, we can see that different backbones are used for different methods, the reviewer worries that the superiority of the proposed method might be brought by a stronger backbone.

Review Point: 2. Compared with methods that adopt other information (such as audio) as weak supervision, there is an inherent advantage of using text as supervision since pretrained text models such as BERT can be utilize as a guidance. So a meaningful comparison would be the comparison with TWS and MIL-NCE, although the proposed method can achieve comparable performance with other methods with much less data, the author does not give analysis about what design in the proposed method that enables this.
Review Point: 3. The performance comparison is not convincing enough. From Table 3, we can see that different backbones are used for different methods, the reviewer worries that the superiority of the proposed method might be brought by a stronger backbone.
==================================================

Focused review:

- Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation). - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.
- The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.
- FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper.
[1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering. 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever. 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering
- Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)?

Review Point: - Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation).
Review Point: - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.
Review Point: - The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.
Review Point: - FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper. [1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering.
Review Point: 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever.
Review Point: 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering - Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)?
==================================================

Focused review:

I believe the main weaknesses are: poor/artificial connection to hypernetworks (the analyzed model, the nomenclature and experiments are mostly inconsistent with prior work), and unconvincing/confusing experimental setup. More details follow below. - Connection to Hypernetworks The paper is clearly focused on analyzing hypernetworks, but at the same time does not consider optimization dynamics induced by many (if not most) uses of hypernetworks. The adopted nomenclature is confusing and non-standard: prior work (e.g. references [6,21,28,35] of the submission) refers to the 'meta-network' (the model that generates weights) as the hypernetwork, while the submission adopts the 'meta-network' term and refers to the two models (meta+primary nets) as the 'hypernetwork'. Moreover, the studied dynamics here consider the model given in Eq. 2, where the meta-network parameters are optimized to minimize some empirical loss (Eq. 3) evaluated on a set of points, where each point u_i defines an input for the primary net and for the meta-network -- it is unclear why such training setup was adopted since it does not match nor include typical uses of hypernetworks. While I believe that understanding optimization dynamics of the meta-network parameters is interesting, there is little insight gained if the analysis stops there. Considering the references in the submission: [6] - A hypernetwork receives an architecture descriptor as input and generates weights for its underlying computational graph, and is trained to minimize the loss of the produced network. Once the hypernetwork is trained, the hypernet input (descriptor) is optimized (via sampling) to perform architecture search. [21] - A hypernetwork receives training hyperparameters as input and is trained to match the best-response function for these hyperparams (i.e. output weights that minimize the loss defined by the hyperparams). Once the hypernetwork is trained, the hypernet inputs are optimized to perform hyperparameter search. [28] - A hypernetwork receives task-specific embeddings as input and is trained to produce weights suitable for the task. The hypernetwork inputs (task embeddings) are trained jointly with its parameters. [35] - Closely related to [6], except inputs are node embeddings computed from the architecture's computational graph. Note that all cases involve optimizing the hypernetwork's inputs once its parameters have been trained, which is indeed the standard and most successful use of hypernetworks. Except for [28], the inputs are optimized while keeping the trained parameters fixed. On the other hand, [28] uses hypernetworks to decrease the number of parameters involved in continual learning, hence it is arguably a poor example of a hypernetwork application when studying infinite-width training dynamics of these models. It seems that the adopted training model only includes the use in [28], where the goal is not finding optimal inputs for a trained hypernetwork, but actually minimizing an explicit empirical loss where hypernetwork inputs are given (in that specific case, task indicators). A minimally complete pipeline would be to not only study optimization dynamics of the hypernetwork parameters, but also the dynamics when optimizing the hypernetwork inputs once its parameters have been trained and fixed. Lastly, the experimental setup brings yet another inconsistency with prior work on hypernetworks: none of the experiments characterizes a meaningful and standard use of hypernetworks. More details on that further below. - Experimental setup For the image completion and inpainting, the problem was set up as a regression problem from pixel coordinates to pixel intensities, with the perturbed image given as additional input to the hypernetwork -- there is no motivation given on why the problem was framed in such an artificial manner, and if the goal was to set up the task as a learning a conditional mapping, it would be considerably cleaner and more natural to have the digit number (label) given as input to the meta-network and the corrupted image as input to the primary network, whose goal would be to output a full reconstruction. Similarly for the rotated MNIST experiment, feeding the meta-network with the original image and the primary-net with the rotated one seems arbitrary and artificial. Note that neither of the two tasks seem natural for hypernetworks, and it is unclear whether a standard network would actually perform worse here.

Review Point: - Experimental setup For the image completion and inpainting, the problem was set up as a regression problem from pixel coordinates to pixel intensities, with the perturbed image given as additional input to the hypernetwork -- there is no motivation given on why the problem was framed in such an artificial manner, and if the goal was to set up the task as a learning a conditional mapping, it would be considerably cleaner and more natural to have the digit number (label) given as input to the meta-network and the corrupted image as input to the primary network, whose goal would be to output a full reconstruction. Similarly for the rotated MNIST experiment, feeding the meta-network with the original image and the primary-net with the rotated one seems arbitrary and artificial. Note that neither of the two tasks seem natural for hypernetworks, and it is unclear whether a standard network would actually perform worse here.
==================================================

Focused review:

- The proposed solution to contrast the in-distribution to some generic domain dataset (e.g. TinyImages) is not new and known as Outlier Exposure [3], limiting the novelty of the work. - The two orthogonal solutions overall result in a rather mixed bag of arguments, in my mind. Following the reasoning given in the paper, I would expect the OOD detection performance to be best when the two solutions are combined, i.e. contrasting the distributions via log-likelihood ratio on the final scale. This is not the case (Table 2). Why? Am I missing something here? - I am missing a justification as to why two separate, but identical networks are used for the likelihood ratio model. Following the hierarchy arguments, these models should share similar low-level features. Why not use just one joint network? The bad performance from mixing and not matching models in Table 1 would support this view. - The analysis on scales is specific to invertible multi-scale models such as Glow. Can we draw conclusions for the more general class of DGMs (including VAEs and autoregressive models) for which this issue has been observed [6]?

Review Point: - The proposed solution to contrast the in-distribution to some generic domain dataset (e.g. TinyImages) is not new and known as Outlier Exposure [3], limiting the novelty of the work.
Review Point: - The two orthogonal solutions overall result in a rather mixed bag of arguments, in my mind. Following the reasoning given in the paper, I would expect the OOD detection performance to be best when the two solutions are combined, i.e. contrasting the distributions via log-likelihood ratio on the final scale. This is not the case (Table 2). Why? Am I missing something here?
Review Point: - I am missing a justification as to why two separate, but identical networks are used for the likelihood ratio model. Following the hierarchy arguments, these models should share similar low-level features. Why not use just one joint network? The bad performance from mixing and not matching models in Table 1 would support this view.
Review Point: - The analysis on scales is specific to invertible multi-scale models such as Glow. Can we draw conclusions for the more general class of DGMs (including VAEs and autoregressive models) for which this issue has been observed [6]?
==================================================

Focused review:

Weaknesses:
It should be emphasized in the abstract that the pose transfer refers to human pose; a reader could probably guess from the datasets used, but it should be mentioned, in the interest of clarity;
Incomplete related work (warping) -- e.g. - arxiv 1703.05593
No explanation of what a CST module is (Fig 1) - legend of Fig. 1 could be expanded, or include an explanation in the text.
Why are poses given as image? (Sec 3.1.1) p_{s} \in R^{h*w}
Equation 1: is this just dot product between z_{s}(i, j) and z_{t}(v, u)? The idea can be also found in Rocco et al (Convolutional neural network architecture for geometric matching / arxiv 1703.05593).
Potential clarity and correctness issue with Equations 3 & 4 -- why is (-1)^a1 the first term in the matrix, and how are the values a1, ...a5, b1, b2 chosen? Since all are random variables, wouldn't it be equivalent to choosing four values for the A ([a1, a2; a3, a4]) matrix? What happens if the matrix is not invertible (eqn4)?
no citation for perceptual loss (Eqn (5)) (should be Johnson et al, arxiv: 1603.08155) -- and there is no normalization term in the perceptual loss; Is there a particular reason for using VGG architecture for computing this loss term?
It is not clear how significant are the improvements in Table 1 compared to SoTA.

Review Point: - arxiv 1703.05593 No explanation of what a CST module is (Fig 1) - legend of Fig. 1 could be expanded, or include an explanation in the text. Why are poses given as image? (Sec 3.1.1) p_{s} \in R^{h*w} Equation 1: is this just dot product between z_{s}(i, j) and z_{t}(v, u)? The idea can be also found in Rocco et al (Convolutional neural network architecture for geometric matching / arxiv 1703.05593). Potential clarity and correctness issue with Equations 3 & 4 -- why is (-1)^a1 the first term in the matrix, and how are the values a1, ...a5, b1, b2 chosen? Since all are random variables, wouldn't it be equivalent to choosing four values for the A ([a1, a2; a3, a4]) matrix? What happens if the matrix is not invertible (eqn4)? no citation for perceptual loss (Eqn (5)) (should be Johnson et al, arxiv: 1603.08155) -- and there is no normalization term in the perceptual loss; Is there a particular reason for using VGG architecture for computing this loss term? It is not clear how significant are the improvements in Table 1 compared to SoTA.
==================================================

Focused review:

Weaknesses:
Little description on the drawbacks of previous methods working on removing inconsistent information for abstractive multi-document summarization. Section 2.3 mentioned a method which incorporates an opinion polarity module to the pointer generator network for multi-document summarization. However, the drawbacks of this method and why the proposed method surpasses it are not illustrated in the paper. Moreover, the comparison between it and the proposed method is also not given in the Experiment section.
The model the authors propose doesn’t seem to satisfy the criterion they laid out. Specifically, the attribute conditioned module aims to make the generated summary have a consistent viewpoint. However, the viewpoints are predicted at both the graph-based learning layer of the encoder and each step of the decoder. There is not any explicit constraint to make the predicted viewpoints be consistent. Therefore, is it still possible that the generated summary expressed the conflicting information?
Some important experiments are missing. 1) For the human evaluation, not all the mentioned baselines, i.e., BART, BART+Longformer, and GraphSum, are evaluated on the human metrics. Therefore, it is not clear whether the proposed module outperforms all the baselines on the human metrics. 2) For the ablation study, why are the ablations designed on the BART and BART+Longformer, instead of the original model, i.e., GraphSum? 3) The external classifier of XLNet introduces additional computation. What are the memory and time costs of the proposed model during inference compared to the previous methods?
The presentation of the paper should be improved. Specifically, all the references in the paper are not shown, and there are some grammatical issues and typos that should be fixed.

Review Point: 1) For the human evaluation, not all the mentioned baselines, i.e., BART, BART+Longformer, and GraphSum, are evaluated on the human metrics. Therefore, it is not clear whether the proposed module outperforms all the baselines on the human metrics.
Review Point: 2) For the ablation study, why are the ablations designed on the BART and BART+Longformer, instead of the original model, i.e., GraphSum?
==================================================

Focused review:

The paper has several weaknesses that must be addressed or clarified, most of which are about the generality of the results: 1) While there is no restriction on the loss function, it does appear that the results are limited to regression tasks as suggested by section 2 and the empirical results section. As many of the competing estimation techniques have been developed or extended with classification tasks in mind, the paper would be strengthened by an inclusion of how the results translate to this case both theoretically and empirically. 2) The restrictions on the form of the kernel functions Assumptions 3.1 and 3.3 in conjunction seem severe, it would be good to have a description of what families of kernel functions satisfy these constraints together. 3) The parameters of focus for the theoretical analysis are only the signal variances, this further restricts the expressibility of the resulting processes in addition to point (2). Furthermore, the scope of the theoretical results is constrained to two parameters: the noise variance and the signal variance of the kernel with the slowest eigendecay. It would of course be nice if the results could be extended to convergence guarantees for all the parameters, but a discussion of why this limitation occurs would be elucidating. 4) The empirical results section mentions that the minibatch SGD algorithm is augmented by forming a minibatch by drawing a random datapoint and finding its 15 nearest neighbours. I think this is an acceptable addition to the method, but I would like to see results using only Algorithm 1. This would give a fuller picture. My concern is that the combination of all these points (limited to regression tasks, restrictions on expressibility, and a helping hand in finding the gradient by constructing nice minibatches) leads to a perfect setting for SGD to perform well relative to the other methods. Addressing these points would make the analysis more robust and the paper stronger.

Review Point: 1) While there is no restriction on the loss function, it does appear that the results are limited to regression tasks as suggested by section 2 and the empirical results section. As many of the competing estimation techniques have been developed or extended with classification tasks in mind, the paper would be strengthened by an inclusion of how the results translate to this case both theoretically and empirically.
Review Point: 2) The restrictions on the form of the kernel functions Assumptions 3.1 and 3.3 in conjunction seem severe, it would be good to have a description of what families of kernel functions satisfy these constraints together.
Review Point: 3) The parameters of focus for the theoretical analysis are only the signal variances, this further restricts the expressibility of the resulting processes in addition to point (2). Furthermore, the scope of the theoretical results is constrained to two parameters: the noise variance and the signal variance of the kernel with the slowest eigendecay. It would of course be nice if the results could be extended to convergence guarantees for all the parameters, but a discussion of why this limitation occurs would be elucidating.
Review Point: 4) The empirical results section mentions that the minibatch SGD algorithm is augmented by forming a minibatch by drawing a random datapoint and finding its 15 nearest neighbours. I think this is an acceptable addition to the method, but I would like to see results using only Algorithm 1. This would give a fuller picture. My concern is that the combination of all these points (limited to regression tasks, restrictions on expressibility, and a helping hand in finding the gradient by constructing nice minibatches) leads to a perfect setting for SGD to perform well relative to the other methods. Addressing these points would make the analysis more robust and the paper stronger.
==================================================

Focused review:

- The novelty of this paper is limited and a bit incremental compared to [1]. It seems the differences are pair-wise and list-wise consistency losses which are not used in [1]. The feature aggregation step is pretty standard. The contribution of the proposed relation aggregation steps (i.e., Eqn 3) is not justified in the expeirments. A baseline to compare it to compute M^A, M^X explicitly without the relation aggregation module. - There is nothing special that is designed for segmentation tasks other than you are using a deeplab3+ as the backbone network. Thus, why not try these things on classification tasks? If the whole framework is about ZSL for segmentation, I'd like to more modules that are modeling the spatial information in images. I think this would be more important for segmentation. - What are the word embedding used in the experiments? - Detailed implementation details are missing, like learning rate, optimizer, etc. - The presentation could be further improved, eg., L140, simultaneous --> simultaneously, L175, focus --> focusing. Please carefully examine the paper.

Review Point: - The novelty of this paper is limited and a bit incremental compared to [1]. It seems the differences are pair-wise and list-wise consistency losses which are not used in [1]. The feature aggregation step is pretty standard. The contribution of the proposed relation aggregation steps (i.e., Eqn 3) is not justified in the expeirments. A baseline to compare it to compute M^A, M^X explicitly without the relation aggregation module.
Review Point: - There is nothing special that is designed for segmentation tasks other than you are using a deeplab3+ as the backbone network. Thus, why not try these things on classification tasks? If the whole framework is about ZSL for segmentation, I'd like to more modules that are modeling the spatial information in images. I think this would be more important for segmentation.
Review Point: - What are the word embedding used in the experiments?
Review Point: - Detailed implementation details are missing, like learning rate, optimizer, etc.
Review Point: - The presentation could be further improved, eg., L140, simultaneous --> simultaneously, L175, focus --> focusing. Please carefully examine the paper.
==================================================

Focused review:

Weakness:
Comparison of Complexity: [1] presents the complexity of different efficient transformers. For linformer[2], the time and memory complexity is O(nk). Is there any justification of LSH sampling equipped YOSO with complexity more than O(nm\tau log(d)+nmd)? 2.Experiments: YOSO takes linformer as baselines. However, the pre-training experiment part does not provide steps vs ppl of linformer with YOSO in Figure 4. What is the comparison result of YOSO with linformer on iteration wise convergence? Also, linformer demonstrates better accuracy in downstream tasks such as SST-2. Is there any comparison to an explanation that can analyze this difference in performance? 3.Efficiency: YOSO demonstrates an advantage over linformer and longformer in memory and runtime. However, is there any analysis on why YOSO achieves this superiority with higher complexities? Are there any system-level advantages that YOSO can show?
Some discussions: Reformer[3] design an attention mechanism that computations are held in the neighbor tokens inside the hash buckets. YOSO also uses hash based sampling to compute attention via neighbor tokens that have high collision probability. On the other hand, linformer introduces a more global view for attention by the low rank projection. Is there any analysis of the local vs global intuition?
[1]Efficient Transformers: A Survey https://arxiv.org/pdf/2009.06732.pdf
[2]Linformer: Self-Attention with Linear Complexity https://arxiv.org/abs/2006.04768
[3] Reformer: The Efficient Transformer https://arxiv.org/abs/2001.04451

Review Point: 2.Experiments: YOSO takes linformer as baselines. However, the pre-training experiment part does not provide steps vs ppl of linformer with YOSO in Figure 4. What is the comparison result of YOSO with linformer on iteration wise convergence? Also, linformer demonstrates better accuracy in downstream tasks such as SST-2. Is there any comparison to an explanation that can analyze this difference in performance?
Review Point: 3.Efficiency: YOSO demonstrates an advantage over linformer and longformer in memory and runtime. However, is there any analysis on why YOSO achieves this superiority with higher complexities? Are there any system-level advantages that YOSO can show? Some discussions: Reformer[3] design an attention mechanism that computations are held in the neighbor tokens inside the hash buckets. YOSO also uses hash based sampling to compute attention via neighbor tokens that have high collision probability. On the other hand, linformer introduces a more global view for attention by the low rank projection. Is there any analysis of the local vs global intuition? [1]Efficient Transformers: A Survey https://arxiv.org/pdf/2009.06732.pdf [2]Linformer: Self-Attention with Linear Complexity https://arxiv.org/abs/2006.04768 [3] Reformer: The Efficient Transformer https://arxiv.org/abs/2001.04451
==================================================

Focused review:

- In section 5, using JPEG compression without backpropagation seems questionable. As in this way, the encoder will not adjust the encoding methods to protect the information from the compression. In section 4.2, high-frequency information is significant for recovering the secret data. However, JPEG is known to clip the high-frequency information. If the encoder is not adjusted, the conclusion is kind of contradictory. - In section 4.1, the analysis of spatial and channel dimension is not convincing enough. Is there the possibility it depends more on the deep architecture or the training methods.

Review Point: - In section 5, using JPEG compression without backpropagation seems questionable. As in this way, the encoder will not adjust the encoding methods to protect the information from the compression. In section 4.2, high-frequency information is significant for recovering the secret data. However, JPEG is known to clip the high-frequency information. If the encoder is not adjusted, the conclusion is kind of contradictory.
Review Point: - In section 4.1, the analysis of spatial and channel dimension is not convincing enough. Is there the possibility it depends more on the deep architecture or the training methods.
==================================================

Focused review:

- Bad quality of written language (see details below). - Some experimental details aren't clear (see "correctness" section for details).

Review Point: - Some experimental details aren't clear (see "correctness" section for details).
==================================================

Focused review:

Weaknesses 1. Some confusions. In Parameter Transformation part, you state that “The number of adaptation parameters is given by k (2 d2 + d + 2). This is typically much smaller than the number of MDN parameters (weights and biases from all layers)”. In previous part you state that “The MDN output with all the mixture parameters has dimension p = k (d(d + 1)/2 + d + 1).” Why the adaptation parameters is much smaller than the number of MDN parameters? 2. Some figures are not self-explanatory. For instance, in Figure 4, the line of No adapt or Finetune are covered by other lines, without additional explanation. 3. More experiments. How the unsupervised domain adaptation performs based on the baseline model and how it compares with the proposed approach?

Review Point: 1. Some confusions. In Parameter Transformation part, you state that “The number of adaptation parameters is given by k (2 d2 + d + 2). This is typically much smaller than the number of MDN parameters (weights and biases from all layers)”. In previous part you state that “The MDN output with all the mixture parameters has dimension p = k (d(d + 1)/2 + d + 1).” Why the adaptation parameters is much smaller than the number of MDN parameters?
Review Point: 2. Some figures are not self-explanatory. For instance, in Figure 4, the line of No adapt or Finetune are covered by other lines, without additional explanation.
Review Point: 3. More experiments. How the unsupervised domain adaptation performs based on the baseline model and how it compares with the proposed approach?
==================================================

Focused review:

1. In section 3.5, I wonder why the authors particularly choose L1 loss to optimize and why not using other loss functions, e.g., L2 loss (square loss). Can the authors elaborate more on this? 2. In section 4.1, the authors formulate the problem as a 12-step prediction. Does it suggest that 0-12 hrs are inputs and 13-24 hrs are targets? 3. In section 4.4, I am a little bit confused about the setup for short-term (e.g., 5mins) and long-term predictions (e.g., 60 mins). Are they also multi-step predictions? I would like the authors to clarify this part.

Review Point: 1. In section 3.5, I wonder why the authors particularly choose L1 loss to optimize and why not using other loss functions, e.g., L2 loss (square loss). Can the authors elaborate more on this?
Review Point: 2. In section 4.1, the authors formulate the problem as a 12-step prediction. Does it suggest that 0-12 hrs are inputs and 13-24 hrs are targets?
Review Point: 3. In section 4.4, I am a little bit confused about the setup for short-term (e.g., 5mins) and long-term predictions (e.g., 60 mins). Are they also multi-step predictions? I would like the authors to clarify this part.
==================================================

Focused review:

Weakness: 1 In general, this work is a patchwork of a condition-based generative model and a raw image preprocessing method at the input and output ends, leading to insufficient novelty.
For key point detection, Jakab et al. (2018) thoughts are followed.
The initial graph Gt with its nodes and edges is built through widely used operations, e.g., visual+position; distance
Why the approximate posterior is conditioned on the graph at next step? There is no interpretation, but this design is seem as some condition-VAE-based multi-agent prediction methods [1,2], while the only difference is that existing works infer the agents' dynamics but KINet infers the graph representation.
The message passing is not novel.
The forward prediction with a skip connection is not novel.
2 Why do you use the contrastive loss? The interpretation is not clear; why it helps to learn actionable object-centric representation? How to build the negative G-?
3 As for the experiments, I do not think the dataset matches the authors' claim: hard to annotation/detection and complex scenes.
The objects and scenes are quite simple. I suggest to test the methods on some real-world dataset like SDD and ETH-UCY, which have top-view image and scene information like obstacles.
In the proposed dataset, I notice that there are very large `+' object, but the method only determines a few key points for each object. So how to simulate the positional constraints imposed by the shape of an object？
[1] It Is Not the Journey But the Destination: Endpoint Conditioned Trajectory Prediction (ECCV2020) [2] Contextually Plausible and Diverse 3D Human Motion Prediction (ICCV 2021)

Review Point: 1 In general, this work is a patchwork of a condition-based generative model and a raw image preprocessing method at the input and output ends, leading to insufficient novelty. For key point detection, Jakab et al. (2018) thoughts are followed. The initial graph Gt with its nodes and edges is built through widely used operations, e.g., visual+position; distance Why the approximate posterior is conditioned on the graph at next step? There is no interpretation, but this design is seem as some condition-VAE-based multi-agent prediction methods [1,2], while the only difference is that existing works infer the agents' dynamics but KINet infers the graph representation. The message passing is not novel. The forward prediction with a skip connection is not novel.
Review Point: 2 Why do you use the contrastive loss? The interpretation is not clear; why it helps to learn actionable object-centric representation? How to build the negative G-?
Review Point: 3 As for the experiments, I do not think the dataset matches the authors' claim: hard to annotation/detection and complex scenes. The objects and scenes are quite simple. I suggest to test the methods on some real-world dataset like SDD and ETH-UCY, which have top-view image and scene information like obstacles. In the proposed dataset, I notice that there are very large `+' object, but the method only determines a few key points for each object. So how to simulate the positional constraints imposed by the shape of an object？ [1] It Is Not the Journey But the Destination: Endpoint Conditioned Trajectory Prediction (ECCV2020) [2] Contextually Plausible and Diverse 3D Human Motion Prediction (ICCV 2021)
==================================================

Focused review:

Robustness evaluations depend crucially choosing correct hyperparameters. Here I see several problems: - The authors mainly test robustness against the PGD attack with 5 or 20 steps. In general, this is way too little. To increase confidence in the results, I would need query success curves in the style of Brendel et al, NeurIPS 2019, where they show attack success rates over a logarithmic range of steps reaching up to 1000. Attack rates are often decreasing till the highest number of steps. Such query success curves can show whether the attack has already converged or not for a given number of steps - the authors don't provide justification for the chosen learning rates for PGD. The mostly used value of epsilon/3 seems very rough. I would expect them to test a substantially smaller learning rate (let's say epsilon/20) with substantially more iterations (>=250). Better even, test a substantial range of learning rates for each tested number of steps to find the optimal settings. - a lot of crucial control experiments, while conducted, are barely or not at all mentioned in the manuscript (l 269-291). - tests of decision based attacks are way too little (only one image, only 1000 steps) to show that there is no gradient obfuscation. Figure 4 in the boundary attack paper (Brendel et al, ICLR 2018) shows that even after 10000 steps the attack is still making progress.

Review Point: - The authors mainly test robustness against the PGD attack with 5 or 20 steps. In general, this is way too little. To increase confidence in the results, I would need query success curves in the style of Brendel et al, NeurIPS 2019, where they show attack success rates over a logarithmic range of steps reaching up to 1000. Attack rates are often decreasing till the highest number of steps. Such query success curves can show whether the attack has already converged or not for a given number of steps - the authors don't provide justification for the chosen learning rates for PGD. The mostly used value of epsilon/3 seems very rough. I would expect them to test a substantially smaller learning rate (let's say epsilon/20) with substantially more iterations (>=250). Better even, test a substantial range of learning rates for each tested number of steps to find the optimal settings.
Review Point: - a lot of crucial control experiments, while conducted, are barely or not at all mentioned in the manuscript (l 269-291).
Review Point: - tests of decision based attacks are way too little (only one image, only 1000 steps) to show that there is no gradient obfuscation. Figure 4 in the boundary attack paper (Brendel et al, ICLR 2018) shows that even after 10000 steps the attack is still making progress.
==================================================

Focused review:

Weakness: 1. Regarding the whole framework, which part is vital for using CLIP to guide weakly supervised learning? I think the discussion is necessary (but I didn’t find clear answer in the discussion) and help this paper to be distinguished from the other related work. 2. The knowledge bank is based on classes appearing in the full dataset and is defined by the text. Can you explain how the size of the knowledge bank affects performance? After all, when there exists a brunch of interaction classes, I am not sure about the training efficiency and workload. 3. SRC only does not help much with the detection, according to Table 2. It is not very effective and is kind of counterintuitive. I would recommend providing a more detailed explanation or removing the SRC part. 4. When a CLIP model is used, it is always necessary to explain the potential issue of fair comparison. After all, CLIP has seen quite a lot of training data during pretraining, and there is a risk of potential data leakage. As such, explanation is necessary.

Review Point: 1. Regarding the whole framework, which part is vital for using CLIP to guide weakly supervised learning? I think the discussion is necessary (but I didn’t find clear answer in the discussion) and help this paper to be distinguished from the other related work.
Review Point: 2. The knowledge bank is based on classes appearing in the full dataset and is defined by the text. Can you explain how the size of the knowledge bank affects performance? After all, when there exists a brunch of interaction classes, I am not sure about the training efficiency and workload.
Review Point: 3. SRC only does not help much with the detection, according to Table 2. It is not very effective and is kind of counterintuitive. I would recommend providing a more detailed explanation or removing the SRC part.
Review Point: 4. When a CLIP model is used, it is always necessary to explain the potential issue of fair comparison. After all, CLIP has seen quite a lot of training data during pretraining, and there is a risk of potential data leakage. As such, explanation is necessary.
==================================================

Focused review:

Even though this result provides a nice theoretical justification for the WW surrogate, few questions remain. I would like to ask the author to comment on these questions. 1) The author shows that the WW surrogate is Fisher consistent over the ordered partition loss. However, since the experiments show that The WW surrogate performs well on the 0-1 loss, a question arises on how the performance over the ordered partition loss translates to the 0-1 loss. In particular, since the orderer partition loss that the author proposed is very different from the 0-1 loss. I suggest the author to explain the connection in the revised paper. 2) The author suggested that one of the reasons for the WW success (compares with the CS surrogate) is that the \Omega of the WW surrogate is a strict superset of the \Omega of the CS. These additional areas include cases where there is no majority label. Even though it (partially) explain the success of the WW surrogate in the case where no majority label, it does not explain why the LLW surrogate (the Fisher consistent one) does not perform as well as the WW in some cases. The \Omega of the LLW surrogate is a strict superset of the \Omega of the WW, yet it does not translate to better performances. Could the author provide a comment on the success of the WW surrogate despite its \Omega is a strict subset the LLW one, or from the other perspective, why the LLW surrogate fails even though its \Omega is a strict superset of the WW one. 3) Even though it is not required for a purely theoretical paper, having a small empirical result will be nice. In particular, I am interested to see the empirical performance of different surrogates on the discrete ordered partition loss that the author proposed. This may provide an empirical insight on the superiority of the WW on the loss metric as well as how the performance on the metric translates to the performance on the 0-1 loss metric.

Review Point: 1) The author shows that the WW surrogate is Fisher consistent over the ordered partition loss. However, since the experiments show that The WW surrogate performs well on the 0-1 loss, a question arises on how the performance over the ordered partition loss translates to the 0-1 loss. In particular, since the orderer partition loss that the author proposed is very different from the 0-1 loss. I suggest the author to explain the connection in the revised paper.
Review Point: 2) The author suggested that one of the reasons for the WW success (compares with the CS surrogate) is that the \Omega of the WW surrogate is a strict superset of the \Omega of the CS. These additional areas include cases where there is no majority label. Even though it (partially) explain the success of the WW surrogate in the case where no majority label, it does not explain why the LLW surrogate (the Fisher consistent one) does not perform as well as the WW in some cases. The \Omega of the LLW surrogate is a strict superset of the \Omega of the WW, yet it does not translate to better performances. Could the author provide a comment on the success of the WW surrogate despite its \Omega is a strict subset the LLW one, or from the other perspective, why the LLW surrogate fails even though its \Omega is a strict superset of the WW one.
Review Point: 3) Even though it is not required for a purely theoretical paper, having a small empirical result will be nice. In particular, I am interested to see the empirical performance of different surrogates on the discrete ordered partition loss that the author proposed. This may provide an empirical insight on the superiority of the WW on the loss metric as well as how the performance on the metric translates to the performance on the 0-1 loss metric.
==================================================

Focused review:

- Line 54: Is 'interpretable' program relevant to the notion described in the work of 'Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608'? - Line 19, 37, 39: A reference for the 'Influence maximization' problem may be provided. The distribution may be more formally given (e.g. which p_{ij} sum to 1). To be able to refer to the joint distributions, there should be a more concrete statement of the p_{ij}. Or maybe a preamble of line 103. - Line 52: Some more details about the polynomial time character of the formulation may clarify your statement about the LP. - Line 103: The strategy space of the adversary implied in the equation is strongly pessimistic (why consider all possible correlations?). This can be used in a follow up work. It seems that it does not reduce the value of the current model.

Review Point: - Line 54: Is 'interpretable' program relevant to the notion described in the work of 'Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608'?
Review Point: - Line 19, 37, 39: A reference for the 'Influence maximization' problem may be provided. The distribution may be more formally given (e.g. which p_{ij} sum to 1). To be able to refer to the joint distributions, there should be a more concrete statement of the p_{ij}. Or maybe a preamble of line 103.
Review Point: - Line 52: Some more details about the polynomial time character of the formulation may clarify your statement about the LP.
Review Point: - Line 103: The strategy space of the adversary implied in the equation is strongly pessimistic (why consider all possible correlations?). This can be used in a follow up work. It seems that it does not reduce the value of the current model.
==================================================

Focused review:

Weaknesses:
Problem statement description could be improved. The introductory section does not properly address the tackled task or the way in which this work contributes to current issues.
Hard to grasp from the paper what are the authors' contributions - I did not find the paper an easy read.
The authors rename the Mask2former architecture as SOIS and wrongfully consider it their contribution since the architecture itself did not change as the authors stated themselves but rather the scenario in which it was used (extension to a semi-supervised scenario by employing a proxy constraint, in the form of a consistency loss between predicted masks of two existing branches in the architecture)
Other comments:
Benchmarks are not referenced when first mentioned - same goes for COCO and UVO in the abstract - at least mention that these are datasets.
Figure 1 is crowded and hard to follow all of its parts. It would make more sense to split it into two parts - a, b, c - Part 1 and d, e, f, g - Part 2. In (b) consistency is misspelled and Cityscapes is misspelled in the figure caption.
Figure 1c, 1d, 1e and 1f are not referenced in the paper.
Figure 2 is not informative at all - What is the purpose of that backbone? - What is the format of the output? What is the input of the FCN in the foreground prediction branch and why not use the RGB image? The image caption does not complement the figure - most of the notations are not explained and the flow of the different branches is very hard to follow. Also, sentences such as "the foreground prediction branch segments a foreground region" can be derived from the naming of these components. Authors should highlight only the information that cannot be inferred solely from the naming.

Review Point: - What is the format of the output? What is the input of the FCN in the foreground prediction branch and why not use the RGB image? The image caption does not complement the figure - most of the notations are not explained and the flow of the different branches is very hard to follow. Also, sentences such as "the foreground prediction branch segments a foreground region" can be derived from the naming of these components. Authors should highlight only the information that cannot be inferred solely from the naming.
==================================================

Focused review:

- Generally, the evaluation is rather "narrow" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large).
- The evaluation on CAR appears to have been conducted using "automatic" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided.
- Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims.
- It would be nice to have seen this work on other base models-- particularly a T5-based model.
- This work may feel more "at home" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well.
- Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer?

Review Point: - Generally, the evaluation is rather "narrow" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large).
Review Point: - The evaluation on CAR appears to have been conducted using "automatic" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided.
Review Point: - Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims.
Review Point: - It would be nice to have seen this work on other base models-- particularly a T5-based model.
Review Point: - This work may feel more "at home" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well.
Review Point: - Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer?
==================================================

Focused review:

Weaknesses: 1. In introduction, the authors mentioned that existing mathematical models cannot faithfully represent all factors relevant to Alzheimer’s disease. Based on that, it would be reasonable to either refine the equations or use a different model to take care of the gap. However, it is not very intuitive why reinforcement learning is a natural choice. It would be better if some insights and rationale is given.
Additional feedback: 1. In Figure 4B, it might be helpful to indicate the correlation coefficients, even though it is mentioned in the text. Also, it would make more sense if the predicted vs. ground truth size of PFC is included – it looks weird when both HC and PFC are described in the text but only HC is shown in the corresponding figure. 2. Captions of Figure 5 needs a bit improvement. It is not clear which model variant Figure 5B and 5C correspond to. It would be interesting to see whether the model can perform consistently when choosing different follow-up times (year-1, year-2, etc.) as the initial point. For example, the prediction of N years after year-0 shall be reasonably similar to that of N-1 years after year-1. At the same time, I am aware that some data might only be available at year-0 and this might not be an easy investigation.

Review Point: 1. In introduction, the authors mentioned that existing mathematical models cannot faithfully represent all factors relevant to Alzheimer’s disease. Based on that, it would be reasonable to either refine the equations or use a different model to take care of the gap. However, it is not very intuitive why reinforcement learning is a natural choice. It would be better if some insights and rationale is given. Additional feedback:
Review Point: 1. In Figure 4B, it might be helpful to indicate the correlation coefficients, even though it is mentioned in the text. Also, it would make more sense if the predicted vs. ground truth size of PFC is included – it looks weird when both HC and PFC are described in the text but only HC is shown in the corresponding figure.
Review Point: 2. Captions of Figure 5 needs a bit improvement. It is not clear which model variant Figure 5B and 5C correspond to. It would be interesting to see whether the model can perform consistently when choosing different follow-up times (year-1, year-2, etc.) as the initial point. For example, the prediction of N years after year-0 shall be reasonably similar to that of N-1 years after year-1. At the same time, I am aware that some data might only be available at year-0 and this might not be an easy investigation.
==================================================

Focused review:

- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer). - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper).
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion.

Review Point: - In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer).
Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
Review Point: - The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
Review Point: - The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
Review Point: - One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper).
Review Point: - Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
Review Point: - I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion.
==================================================

Focused review:

- The experiments are limited to learning to include or exclude specific words, which is a quite narrow task in controllability - there has to be something more interesting than trying to get the model to predict the word "cat". - There is a lack of simple baselines, as well as any comparison to previous work on text controllability, to put the results in context (see below). - The model description is not clear enough, in particular with regards to how the NPI model is trained (see below). Moreover, it is unclear how the model is learning what is the effect of the perturbations through multiple layers of the pre-trained network. - In many of the generated text examples (in Table 40, in particular for offense-avoidance, the quality of the NPI text output is worse than that of the original model.

Review Point: - The experiments are limited to learning to include or exclude specific words, which is a quite narrow task in controllability - there has to be something more interesting than trying to get the model to predict the word "cat".
Review Point: - There is a lack of simple baselines, as well as any comparison to previous work on text controllability, to put the results in context (see below).
Review Point: - The model description is not clear enough, in particular with regards to how the NPI model is trained (see below). Moreover, it is unclear how the model is learning what is the effect of the perturbations through multiple layers of the pre-trained network.
Review Point: - In many of the generated text examples (in Table 40, in particular for offense-avoidance, the quality of the NPI text output is worse than that of the original model.
==================================================

Focused review:

Weaknesses:1). Technically speaking, the contribution of this work is incremental. As an analysis paper, its technique depth is shallow. 2). The reported experimental results appear to evidence the findings, while the encoder and decoder are fixed. More model structures. (e.g., transformers) should be taken into consideration to support the findings. 3). The work ends with the ablation study on EL; however, more studies and case analysis are necessary to underline the difference between the two learning methods (e.g., a failed case of disentanglement models but correctly inferred by EL).

Review Point: 2). The reported experimental results appear to evidence the findings, while the encoder and decoder are fixed. More model structures. (e.g., transformers) should be taken into consideration to support the findings.
Review Point: 3). The work ends with the ablation study on EL; however, more studies and case analysis are necessary to underline the difference between the two learning methods (e.g., a failed case of disentanglement models but correctly inferred by EL).
==================================================

Focused review:

Weakness
Upweighting using the influence function merely approximates the retrained parameters and provides no theoretical error bound on non-convex loss functions. This work then constructs another approximation of this already fuzzy target to obtain the unlearnt model. One can hardly be convinced that such a method will result in a set of parameters that resemble the retrained model. The proposed scheme also provides no means of generating a verification of data removal, which is utterly vital for a data provider.
Adding a scaled identity matrix to the hessian to make it positive definite only accounts for the non-invertible problem. It still destroys the basis of Theorem 2, which requires the loss function to be globally convex. This assumption is too strong to make any meaningful sense in real-world scenarios. Also, Theorem 2 seems to be only an adaptation in notations of eq.(3) in [1].
In scenarios where the graph is relatively dense and the GCN is deep (e.g., as described in [2]), the affected set of nodes, as defined in the paper, can easily become the whole graph, which sort of destroys the purpose of the paper, which is fast unlearning.
Unlearning efficacy is only evaluated for the proposed model without comparison with other baselines, unlike classification accuracy. Also, experiments like the behavior difference between the unlearning model and the retrained model on the unlearned part of the dataset should be added to demonstrate the algorithm's efficacy further.
[1] Pang Wei Koh and Percy Liang. “Understanding Black-box Predictions via Influence Functions.” In: Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, 2017, pp. 1885–1894.
[2] Guohao Li et al. “DeepGCNs: Can GCNs Go As Deep As CNNs?” In: 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 2019, pp. 9266–9275.

Review Point: 70. Proceedings of Machine Learning Research. PMLR, 2017, pp. 1885–1894. [2] Guohao Li et al. “DeepGCNs: Can GCNs Go As Deep As CNNs?” In:
Review Point: 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 2019, pp. 9266–9275.
==================================================

Focused review:

Weaknesses === 1) The impact of this work is very limited. Although the model seems to be quite general, it is not clear how difficult it is to implement, nor if it will consistently provide advantages over approaches specifically designed for each domain. Perhaps more experiments would help make this more clear. 2) Presentation. Although the approach is clearly explained, and technical details are roughly at the appropriate level, the paper lacks a discussion as to why this method works well. That is, what is it about these learned bases that make it effective? It would be nice to have some kind of hypothesis here that could be supported by the experimental results. === Questions === 1) Section 4.3 (roughly L267-270) briefly explains why some objects are confused with others (doors and bookcases, etc.). Can this be addressed in this approach? Why does SPGN seem to do better in several of these object categories (Table 3)? 2) Why are there no quantitative results for the Human Shape Bases Synchronization problem? The qualitative results look impressive, but it is difficult to judge effectiveness on only a few pictures. === Conclusion === The paper presents an interesting, novel approach to learning bases/dictionaries for probe functions, and provides sufficient experimental evidence to show effectiveness. The only substantial negative is the limited impact of the approach. I think the paper is borderline, but I lean towards acceptance.

Review Point: 2) Presentation. Although the approach is clearly explained, and technical details are roughly at the appropriate level, the paper lacks a discussion as to why this method works well. That is, what is it about these learned bases that make it effective? It would be nice to have some kind of hypothesis here that could be supported by the experimental results. === Questions ===
Review Point: 1) Section 4.3 (roughly L267-270) briefly explains why some objects are confused with others (doors and bookcases, etc.). Can this be addressed in this approach? Why does SPGN seem to do better in several of these object categories (Table 3)?
Review Point: 2) Why are there no quantitative results for the Human Shape Bases Synchronization problem? The qualitative results look impressive, but it is difficult to judge effectiveness on only a few pictures. === Conclusion === The paper presents an interesting, novel approach to learning bases/dictionaries for probe functions, and provides sufficient experimental evidence to show effectiveness. The only substantial negative is the limited impact of the approach. I think the paper is borderline, but I lean towards acceptance.
==================================================

Focused review:

- Number of instances in a test set: Did the authors analyze if this affects the rankings?
- Section 3: There are no baselines presented for summarisation and image captioning.
- It does not become clear why automatic metrics tend to overrate machine-generated text?
Line 473: missing word - “depending on the downstream task of interest”

Review Point: - Number of instances in a test set: Did the authors analyze if this affects the rankings?
Review Point: - Section 3: There are no baselines presented for summarisation and image captioning.
Review Point: - It does not become clear why automatic metrics tend to overrate machine-generated text? Line 473: missing word - “depending on the downstream task of interest”
==================================================

Focused review:

Weaknesses:
The paper is not well written. (1) Lots of space wasted on trivial points, like prop.1 that simply says that the function is well defined. Or the list of operations later that doesn't really helps understand the work proposed. Also Prop. 2 is trivial from definition (eq. 1). (2) The main technical part of the paper is glossed over, 3 lines in bullet point 3 in page 6. Everything up to that point was simply replacing an integral with an average. (3) The whole mathematical machinery feels redundant or under-utilized. How is it different then convolutions on a non-uniform grid?
The experimental part should compare to previous works such as Jiang et al "CONVOLUTIONAL NEURAL NETWORKS ON NONUNIFORM GEOMETRICAL SIGNALS USING EUCLIDEAN SPECTRAL TRANSFORMATION"

Review Point: 2 is trivial from definition (eq. 1). (2) The main technical part of the paper is glossed over, 3 lines in bullet point 3 in page 6. Everything up to that point was simply replacing an integral with an average. (3) The whole mathematical machinery feels redundant or under-utilized. How is it different then convolutions on a non-uniform grid? The experimental part should compare to previous works such as Jiang et al "CONVOLUTIONAL NEURAL NETWORKS ON NONUNIFORM GEOMETRICAL SIGNALS USING EUCLIDEAN SPECTRAL TRANSFORMATION"
==================================================

Focused review:

Weaknesses: The main weaknesses for me are evaluation and overall presentation/writing.
- The list of baselines is hard to understand. Some methods are really old and it doesn't seem justified to show them here (e.g., Mpttern).
- Memb is apparently the previous state-of-the-art, but there is no mention to any reference.
- While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.
- The paper seriously lacks proofreading, and could not be published until this is fixed – for instance, I noted 11 errors in the first column of page 2.
- The CilinE hierarchy is very shallow (5 levels only). However apparently, it has been used in the past by other authors. I would expect that the deeper the more difficult it is to branch new hyponym-hypernyms. This can explain the very high results obtained (even by previous studies)... - General Discussion: The approach itself is not really original or novel, but it is applied to a problem that has not been addressed with deep learning yet. For this reason, I think this paper is interesting, but there are two main flaws. The first and easiest to fix is the presentation. There are many errors/typos that need to be corrected. I started listing them to help, but there are just too many of them.
The second issue is the evaluation, in my opinion. Technically, the performances are better, but it does not feel convincing as explained above.
What is Memb, is it the method from Shwartz et al 2016, maybe? If not, what performance did this recent approach have? I think the authors need to reorganize the evaluation section, in order to properly list the baseline systems, clearly show the benefit of their approach and where the others fail.
Significance tests also seem necessary given the slight improvement on one dataset.

Review Point: - The list of baselines is hard to understand. Some methods are really old and it doesn't seem justified to show them here (e.g., Mpttern).
Review Point: - Memb is apparently the previous state-of-the-art, but there is no mention to any reference.
Review Point: - While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.
Review Point: - The paper seriously lacks proofreading, and could not be published until this is fixed – for instance, I noted 11 errors in the first column of page 2.
Review Point: - The CilinE hierarchy is very shallow (5 levels only). However apparently, it has been used in the past by other authors. I would expect that the deeper the more difficult it is to branch new hyponym-hypernyms. This can explain the very high results obtained (even by previous studies)...
==================================================

Focused review:

Weakness.
I do not fully agree with the claim of the super-resolution capability of the proposed method, although I do agree with the cascaded generation method. As seen from the red box of Fig.1, the 2x super-resolution breaks the consistency between the input lower-resolution image and the output image. (the mouth part). I think this is because every time a generated image is super-resolved, Gaussian noise is added during the forward diffusion process. Although this is arguable because super-resolution can be ill-posed, however 2x super-resolution should not produce such a mismatch. I believe that's also why only on 4x~ super-resolution task, can this method outperforms existing works.
The comparison with SR3 seems weird. As seen in Figure.4, the generated images of SR3 have severe chromatic distortion. I think the unofficial implementation of SR3 may be buggy.
Other comments Typos: 1. missing closing brackets in the reverse diffusion process in Algorithm.1 and Algorithm.2

Review Point: 1. missing closing brackets in the reverse diffusion process in Algorithm.1 and Algorithm.2
==================================================

Focused review:

- A number of the proofs (given in the supplementary material) are straightforward. -The authors should make clearer which theoretical results required novel proofs and which are routine and/or follow easily from previous work. - It's not surprising that problems involving MLPs have higher complexity than problems involving FBDDs and perceptrons. - The paper of Darwiche and Marquise on the Knowledge Compilation Map should be discussed in related work. -The complexity of the minimum sufficient reason question does not seem like a particularly good measure for interpretability. It's not clear why computing a sufficient reason would need to be "minimum". Also, consider e.g. monotone DNF, which could be regarded as an easily interpretable representation. It's easy to to compute a minimum sufficient reason for a positive output, but NP-hard for a negative output.

Review Point: - A number of the proofs (given in the supplementary material) are straightforward. -The authors should make clearer which theoretical results required novel proofs and which are routine and/or follow easily from previous work.
Review Point: - It's not surprising that problems involving MLPs have higher complexity than problems involving FBDDs and perceptrons.
Review Point: - The paper of Darwiche and Marquise on the Knowledge Compilation Map should be discussed in related work. -The complexity of the minimum sufficient reason question does not seem like a particularly good measure for interpretability. It's not clear why computing a sufficient reason would need to be "minimum". Also, consider e.g. monotone DNF, which could be regarded as an easily interpretable representation. It's easy to to compute a minimum sufficient reason for a positive output, but NP-hard for a negative output.
==================================================

Focused review:

Weaknesses:
Lack of ablation study: 1) the “weakly supervise” mentioned in 3.2.1 is not mentioned in the ablation study. 2) The impact of Age and FaceID features is not studied in the ablation. 3) Is the mask necessary in Detail Hallucination Network? 4) the quantitative result from model w/o AugW and model w/o DSL (SEPARATELY) is not provided.
Based on Figure 5, 6 and 7, I still observe that quite a few details (i.e., wrinkles) that appear in the input are missing in your generated image. It suggests your model still fails to capture the important and significant details when there are substantial wrinkles on the face.
Performance study should be conditioned on (broken down according to) the key factors, such as age or emotions. Obviously, the amount of deformation in the face (wrinkles) will increase with age. Similarly, certain emotions/AUs result in more fine grained deformation than others.
Comparison with DECA seems to be not fair, as you mentioned, DECA uses different morphable models, the reason may be that input is different.
There is only one compared baseline, DECA. More compared methods could be added.
It would be better to separate the contributions of the Detail Hallucination Network and Rendering Network. (e.g. Detail Hallucination Network+existing Rendering method)

Review Point: 2) The impact of Age and FaceID features is not studied in the ablation.
Review Point: 4) the quantitative result from model w/o AugW and model w/o DSL (SEPARATELY) is not provided. Based on Figure 5, 6 and 7, I still observe that quite a few details (i.e., wrinkles) that appear in the input are missing in your generated image. It suggests your model still fails to capture the important and significant details when there are substantial wrinkles on the face. Performance study should be conditioned on (broken down according to) the key factors, such as age or emotions. Obviously, the amount of deformation in the face (wrinkles) will increase with age. Similarly, certain emotions/AUs result in more fine grained deformation than others. Comparison with DECA seems to be not fair, as you mentioned, DECA uses different morphable models, the reason may be that input is different. There is only one compared baseline, DECA. More compared methods could be added. It would be better to separate the contributions of the Detail Hallucination Network and Rendering Network. (e.g. Detail Hallucination Network+existing Rendering method)
==================================================

Focused review:

1) It is more common and accurate to use the term "heatmap classification/regression" rather than "verification". 2) It is not suitable to call the work RepPoints v2, as the main concern is to add extra heatmap classification tasks to improve the precision of regression. Moreover, if I understand the paper correctly, in L201, the paper only makes use of two corners to represent an object (i.e., the explicit corners variant). Using explict corners is in conflict with the motivation of the original RepPoints paper, which attempts to use multiple points to represent an object. 2) In addition, the hyperparameter r controls the search range for refining. Please provide detailed ablations on the hyperparameter.

Review Point: 1) It is more common and accurate to use the term "heatmap classification/regression" rather than "verification".
Review Point: 2) It is not suitable to call the work RepPoints v2, as the main concern is to add extra heatmap classification tasks to improve the precision of regression. Moreover, if I understand the paper correctly, in L201, the paper only makes use of two corners to represent an object (i.e., the explicit corners variant). Using explict corners is in conflict with the motivation of the original RepPoints paper, which attempts to use multiple points to represent an object.
Review Point: 2) In addition, the hyperparameter r controls the search range for refining. Please provide detailed ablations on the hyperparameter.
==================================================

Focused review:

Weaknesses
What are 'j', 'T_{j}' and 'T_{k}' in Eqn (1)?
In Eqn (2), what do 'V_{j}' and 'V_{k}' mean?
'M' is ambiguous, which simultaneously represents the number of prompt tokens and the pixel embedding in the memory bank.
F also simultaneously represents the visual encoder and the pixel embedding.
In Eqn (4), 'V_{j}' is calculated from the previous 'V_{j}^{old}'. So, how to generate the initial 'V_{j}'?
What does ' y' ' mean in Eqn (5) and Eqn (6)?
The authors do not well explain their equations, especially Eqn (2), Eqn (4), Eqn (5) and Eqn (6). Only a part of variables in these equations are explained. The meaning of the entire equation is not provided.
What text encoder do you use? Is it fixed during the training?
Fig 2 is very similar to Fig. 1. It can be removed.
In Fig. 3, what does each column mean?
How do you combine multiple datasets? How do you deal with the same classes in different datasets? How do you deal with the same classes with different names in different datasets (eg, 'airplane' in a dataset and 'aeroplane' in another dataset)?
In Table 1&2, please provide the citation of each method.
In Table 4, compared with 'CE', the proposed 'Visual + Language' only shows slight improvements.
Due to the unclarity of this paper, I cannot fully evaluate the novelty. For the first contribution, the vision loss, I guess it encourages the pixel embeddings of the same class to be similar. For the second contribution, the cross-modal information exchange module, I cannot understand how it works and why it can solve the domain gap problem.

Review Point: 1. It can be removed. In Fig. 3, what does each column mean? How do you combine multiple datasets? How do you deal with the same classes in different datasets? How do you deal with the same classes with different names in different datasets (eg, 'airplane' in a dataset and 'aeroplane' in another dataset)? In Table 1&2, please provide the citation of each method. In Table 4, compared with 'CE', the proposed 'Visual + Language' only shows slight improvements. Due to the unclarity of this paper, I cannot fully evaluate the novelty. For the first contribution, the vision loss, I guess it encourages the pixel embeddings of the same class to be similar. For the second contribution, the cross-modal information exchange module, I cannot understand how it works and why it can solve the domain gap problem.
==================================================

Focused review:

Weaknesses: However, there are many points that need to be address before this paper is ready for publication.
1) Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.
Also, how is the montecarlo sampling done? 2) Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.
3) Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ). 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space. - General Discussion: Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods….
Is it valid to compare the loss from two different loss functions?
Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here 392 : Do you mean 2016?
Nitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform 275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.

Review Point: 1) Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer. Also, how is the montecarlo sampling done?
Review Point: 2) Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together. The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.
Review Point: 3) Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621 ).
Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.
Review Point: -General Discussion: Other: 578: We observe that word-level models tend to have lower valid loss compared with sentence- level methods…. Is it valid to compare the loss from two different loss functions? Sec 3.2, the notations are not clear. What does script(Y) means? How do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed. 320: What approach did you use? You should talk about that here 392 : Do you mean 2016? Nitty-gritty:
Review Point: 742 : import => important 772 : inline citation style 778: can significantly outperform 275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.
==================================================

Focused review:

1. It remains questionable how the scalable the proposed approach will be, as it seems for each target language a different model needs to be finetuned.
2. Baselines chosen by the paper for comparison appear to be weak.
3. The paper could also have been better organized.
Regarding the proposed approach 1. The proposed approach requires producing |L| finetuned models, one for each target language. While this seems to be helpful in improving quality, it remains questionable how scalable this approach is, especially when extending the system to handle hundreds of languages. It would be more interesting and feasible to study the possibility of at least grouping several similar languages into one model during finetuning.
2. If pretraining on multi-centric data performs better than En-centric data (Table 1), seems the problem can be better solved by preparing more diverse and high quality data. It remains unclear whether the improvement comes from the proposed 2-stage training, or simply collecting more relevant data.
Regarding experiments 1. I would suggest adding one or more existing end-to-end MNMT models that enable XY translations into the baselines for comparison. Bilingual and pivot baselines are kind of weak in the multiway translation setup.
Regarding paper organization: 1. I would suggest adding a summary of contributions in the last paragraph of Sec. 1 for clarity.
2. It seems Sec. 2.1 should be a standalone section discussing experimental setups and results. In addition, Sec. 2.2 can also be a new section talking about in-house training procedure and results. It makes the paper a little confusing to nest everything under Sec. 2.

Review Point: 1. It remains questionable how the scalable the proposed approach will be, as it seems for each target language a different model needs to be finetuned.
Review Point: 2. Baselines chosen by the paper for comparison appear to be weak.
Review Point: 3. The paper could also have been better organized. Regarding the proposed approach 1. The proposed approach requires producing |L| finetuned models, one for each target language. While this seems to be helpful in improving quality, it remains questionable how scalable this approach is, especially when extending the system to handle hundreds of languages. It would be more interesting and feasible to study the possibility of at least grouping several similar languages into one model during finetuning.
Review Point: 2. If pretraining on multi-centric data performs better than En-centric data (Table 1), seems the problem can be better solved by preparing more diverse and high quality data. It remains unclear whether the improvement comes from the proposed 2-stage training, or simply collecting more relevant data. Regarding experiments 1. I would suggest adding one or more existing end-to-end MNMT models that enable XY translations into the baselines for comparison. Bilingual and pivot baselines are kind of weak in the multiway translation setup. Regarding paper organization:
Review Point: 1. I would suggest adding a summary of contributions in the last paragraph of Sec. 1 for clarity.
Review Point: 2. It seems Sec. 2.1 should be a standalone section discussing experimental setups and results. In addition, Sec. 2.2 can also be a new section talking about in-house training procedure and results. It makes the paper a little confusing to nest everything under Sec.
==================================================

Focused review:

In constrained optimization, the most expensive oracle is usually the LMO (computing gradients is usually more expensive than solving a linear program). In Theorem 1, the LMO complexity is O(1/\epsilon^2) which is quite slow for a batch method. 2) The proposed algorithms have lots of hyperparameters to tune (since the condition number is usually challenging to compute) and have an inner-outer loop structure. 3) Theorem 2 and 3 consider minibatches with increasing size 4) - L40 is a bit of an over-claim 'the first [...] algorithm for general convex-concave saddle point problem', since the theoretical results are in the convex-strongly-concave setting.

Review Point: 2) The proposed algorithms have lots of hyperparameters to tune (since the condition number is usually challenging to compute) and have an inner-outer loop structure.
Review Point: 3) Theorem 2 and 3 consider minibatches with increasing size
Review Point: 4) - L40 is a bit of an over-claim 'the first [...] algorithm for general convex-concave saddle point problem', since the theoretical results are in the convex-strongly-concave setting.
==================================================

Focused review:

Weaknesses:
The ablation study is not fully conducted: 1) comparison with linear attention methods [1,2] on various visual tasks. 2) Using random walk graph kernel and bipartite graph is somewhat complex, why not use some simpler operations such as those in [3,4].
The experimental results on segmentation do not show much improvement. For example, Segformer-RWGKA (MiT-B1-RWKA) only decrease FLOPs by 25% but leads to 0.4% accuracy drop. Network pruning methods [5,6] can achieve similar or better performance. Could you compare with pruning methods?
[1] Katharopoulos, Angelos, et al. "Transformers are rnns: Fast autoregressive transformers with linear attention." International Conference on Machine Learning. PMLR, 2020.
[2] Wang, Sinong, et al. "Linformer: Self-attention with linear complexity." arXiv preprint arXiv:2006.04768 (2020).
[3] Han, Kai, et al. "Vision GNN: An Image is Worth Graph of Nodes." arXiv preprint arXiv:2206.00272 (2022).
[4] Yu, Weihao, et al. "Metaformer is actually what you need for vision." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
[5] Liu, Zhuang, et al. "Learning efficient convolutional networks through network slimming." Proceedings of the IEEE international conference on computer vision. 2017.
[6] He, Yihui, et al. "Amc: Automl for model compression and acceleration on mobile devices." Proceedings of the European conference on computer vision (ECCV). 2018.

Review Point: 1) comparison with linear attention methods [1,2] on various visual tasks.
Review Point: 2) Using random walk graph kernel and bipartite graph is somewhat complex, why not use some simpler operations such as those in [3,4]. The experimental results on segmentation do not show much improvement. For example, Segformer-RWGKA (MiT-B1-RWKA) only decrease FLOPs by 25% but leads to 0.4% accuracy drop. Network pruning methods [5,6] can achieve similar or better performance. Could you compare with pruning methods? [1] Katharopoulos, Angelos, et al. "Transformers are rnns: Fast autoregressive transformers with linear attention." International Conference on Machine Learning. PMLR, 2020. [2] Wang, Sinong, et al. "Linformer: Self-attention with linear complexity." arXiv preprint arXiv:2006.04768 (2020). [3] Han, Kai, et al. "Vision GNN: An Image is Worth Graph of Nodes." arXiv preprint arXiv:2206.00272 (2022). [4] Yu, Weihao, et al. "Metaformer is actually what you need for vision." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. [5] Liu, Zhuang, et al. "Learning efficient convolutional networks through network slimming." Proceedings of the IEEE international conference on computer vision. 2017. [6] He, Yihui, et al. "Amc: Automl for model compression and acceleration on mobile devices." Proceedings of the European conference on computer vision (ECCV). 2018.
==================================================

Focused review:

Weakness: The major concern is the limited contribution of this work. 1.Using image-to-image translation to unify the representations across-domain is an existing technique in domain adaptation, especially in segmentation tasks [1,2]. 2. The use of morphologic information in this paper is simple as the combination of edge detection and segmentation, which are both employed as tools from existing benchmarks (in this paper the author used DeeplabV3, DexiNed-f, employed as off-the-shelf tools for image pre-processing purpose as mentioned in section 4). 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn’t provide insight into this but assumed morphologic segmentation will be invariant. 4. Results compared to other domain adaptation methods (especially generative methods) are missing. There is an obvious lack of evidence that the proposed method is superior.
In brief, the contribution of this paper is limited, the results provided are not sufficient to support the method being effective. A reject.
[1] Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation [2] Image to Image Translation for Domain Adaptation

Review Point: 1.Using image-to-image translation to unify the representations across-domain is an existing technique in domain adaptation, especially in segmentation tasks [1,2].
Review Point: 2. The use of morphologic information in this paper is simple as the combination of edge detection and segmentation, which are both employed as tools from existing benchmarks (in this paper the author used DeeplabV3, DexiNed-f, employed as off-the-shelf tools for image pre-processing purpose as mentioned in section 4).
Review Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn’t provide insight into this but assumed morphologic segmentation will be invariant.
Review Point: 4. Results compared to other domain adaptation methods (especially generative methods) are missing. There is an obvious lack of evidence that the proposed method is superior. In brief, the contribution of this paper is limited, the results provided are not sufficient to support the method being effective. A reject. [1] Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation [2] Image to Image Translation for Domain Adaptation
==================================================

Focused review:

* Some of the modeling assumptions lack proper justification (see questions below). * The infinite-time analysis (solely) of the algorithm weakness the technical contribution, which is claimed to solve a real-world solution.

Review Point: * Some of the modeling assumptions lack proper justification (see questions below).
Review Point: * The infinite-time analysis (solely) of the algorithm weakness the technical contribution, which is claimed to solve a real-world solution.
==================================================

Focused review:

Weakness: 1. The paper provides much about the experimental results, while the contents for the method and motivation seem insufficient. 2. Not much insights have been given as to why spiking self-attention should be designed in this way. 3. Other concerns detailed in Summary.

Review Point: 1. The paper provides much about the experimental results, while the contents for the method and motivation seem insufficient.
Review Point: 2. Not much insights have been given as to why spiking self-attention should be designed in this way.
==================================================

Focused review:

Weaknesses:
In setting 1) it was already known how to get results of a similar flavor without public data in the setting of approximate DP, so the advance lies only in obtaining pure DP, which is mainly of theoretical interest
In setting 2) the public data must come from the same distribution as the private data, which is arguably a strong assumption. (I would guess it can be relaxed along the lines of case 1), but this is not worked out in the paper.)
There is an adequate discussion of limitations. Potential negative societal impacts are unlikely, so not discussed.

Review Point: 2) the public data must come from the same distribution as the private data, which is arguably a strong assumption. (I would guess it can be relaxed along the lines of case 1), but this is not worked out in the paper.) There is an adequate discussion of limitations. Potential negative societal impacts are unlikely, so not discussed.
==================================================

Focused review:

1. The poor result of reimplemented MoCo in 4.3 Table 4 needs further explanation and reasoning to account for. 2. Need clearer description on difference between the setup of ‘Src. Class + tgt. Cluster (w/ self-paced)’ in ablation study Table 5 and the full model. If the only difference is target instance, where does the ~10 difference in mAP come from?

Review Point: 1. The poor result of reimplemented MoCo in 4.3 Table 4 needs further explanation and reasoning to account for.
Review Point: 2. Need clearer description on difference between the setup of ‘Src. Class + tgt. Cluster (w/ self-paced)’ in ablation study Table 5 and the full model. If the only difference is target instance, where does the ~10 difference in mAP come from?
==================================================

Focused review:

Weaknesses: My concerns with the paper are mostly regarding the motivation (or lack thereof in the current presentation) and the experimental results supporting it:
Motivation is unclear. DynaShare seems to evidently build upon AdaShare [Sun et al., 2020] using a gating policy that is not just task-specific, but also instance-dependent. Although this contribution is clear, the motivation behind introducing an instance-specific gating is unclear, and brings up some questions:
Why is this a limitation (or what is the limitation) of previous approaches like AdaShare? Can the authors comment on this point?
Does adding instance-specific gating reduce the memory/time footprint of the model wrt parameters? Is optimality the motivation, similar to AdaShare?
In this regard, it would be great to introduce the problem with a subsection that can motivate this work with an example where this kind of policy would help. It should clearly compare as to what it can do better with respect to just a task-specific gating policy.
Unclear Experimental Results.
Table 1 showcases the AUC improvements of DynaShare against multiple baselines (including AdaShare that performs only task-level gating). From Table 1, although it is clear that DynaShare performance is competitive on the Census task, it can be seen that AdaShare performs even poorer than learning individual models for each task. That is, task-specific gating seems to reduce performance. This is conflicting when compared to the ablation study in Sec 4.4 and Table 3 on the MIMIC dataset. Table 3 shows that both task-specific gating and instance-specific gating help MTL and that the task-specific policy provides the biggest gains. Can the authors clarify this?
The results in Table 2 show that DynaShare performs better than the state-of-the-art (MMoEEx) in 2/4 heterogeneous tasks in the MIMIC-III dataset. But, it would be great to see a focused comparison with AdaShare here, which is the actual base approach that DynaShare extends. After combining the results from Table 3 and 2, it seems like DynaShare only improves on AdaShare significantly in 1/4 tasks, i.e. Decompensation prediction. Firstly, I recommend that the authors add AdaShare results to Table 2 as well as Table 3. Secondly, can the authors provide results/comments on what improvements DynaShare provides specifically over AdaShare for heterogeneous tasks?
Potentially Missing Experimental Results.
Hard parameter sharing is restrictive and can result in negative transfer. Soft parameter sharing is better but not scalable with increasing #tasks. Assuming the motivation behind DynaShare is to overcome these scalability issues (as stated in Intro and similar to related work), there is no focused evaluation that shows this.
Suggestion: An experimental evaluation against baselines of #parameters as no of tasks increases would showcase the effectiveness of this approach. In the past, approaches like [Sun et al., 2020] [Veit & Belongie, 2018] have used reduction in #params and FLOPS to show the real value of dynamic network executions.'''
2. Previous results have supported the intuition that similar tasks should have similar execution distribution to share knowledge. Any
results on what layers of the network this new hierarchical policy prefers to share among tasks?
Suggestion: It would be interesting to visualize the learned inference graphs, by studying the rates at which different layers are executed or the similarity in policies learnt. The results can be a good contribution to reveal future research ideas.
3. For the method to be easily adapted to various MTL tasks, it is essential that the instance-specific gating is robust to small variations in the instance space. If not, can it end up reducing knowledge sharing among even related tasks, just because of minor variations in instance characteristics? How much of this minor variation can be controlled by the target rate `t` and the weight `λ` instance used to control the loss term Linstance ?
Suggestion: An important aspect to study here can be the effect of t on performance and inference time. References:
[Sun et al., 2020] Adashare: Learning what to share for efficient deep multi-task learning
[Veit & Belongie, 2018] Convolutional networks with adaptive inference graphs

Review Point: 3. For the method to be easily adapted to various MTL tasks, it is essential that the instance-specific gating is robust to small variations in the instance space. If not, can it end up reducing knowledge sharing among even related tasks, just because of minor variations in instance characteristics? How much of this minor variation can be controlled by the target rate `t` and the weight `λ` instance used to control the loss term Linstance ? Suggestion: An important aspect to study here can be the effect of t on performance and inference time. References: [Sun et al., 2020] Adashare: Learning what to share for efficient deep multi-task learning [Veit & Belongie, 2018] Convolutional networks with adaptive inference graphs
==================================================

Focused review:

weaknesses are:
the organization of the paper could be improved, and
the paper lacks any tips for practitioners.
1: Section 3 explains the general ideas while Sections 4 and 5 give rigorous discussions about the loss and optimization dynamics, respectively.
I noticed this structure only after reading through the three sections, which was confusing to me. Explaining the structure of the discussion at an appropriate place (at the end of Introduction or at the beginning of Section 3) may improve the clarity.
2: Most practitioners use nonlinear VAEs, but the paper provides a negative result in that the VAE objective can induce a larger intrinsic dimension and larger support of data distribution. Is there any wisdom or tip from the theory discussed in the paper for practitioners? Such information will strengthen the importance of the paper. For example, the impact of the choice of latent dimensionality r
or a way to choose r
can help VAE users.
Question about gradient flow analysis: can we extend to stochastic gradient setup? In the linear case, the expectation over latent variable z
in the VAE objective is analytic. On the other hand, we need a Monte-Carlo approximation for typical nonlinear cases using the reparameterization trick, etc.

Review Point: 1: Section 3 explains the general ideas while Sections 4 and 5 give rigorous discussions about the loss and optimization dynamics, respectively. I noticed this structure only after reading through the three sections, which was confusing to me. Explaining the structure of the discussion at an appropriate place (at the end of Introduction or at the beginning of Section 3) may improve the clarity.
Review Point: 2: Most practitioners use nonlinear VAEs, but the paper provides a negative result in that the VAE objective can induce a larger intrinsic dimension and larger support of data distribution. Is there any wisdom or tip from the theory discussed in the paper for practitioners? Such information will strengthen the importance of the paper. For example, the impact of the choice of latent dimensionality r or a way to choose r can help VAE users. Question about gradient flow analysis: can we extend to stochastic gradient setup? In the linear case, the expectation over latent variable z in the VAE objective is analytic. On the other hand, we need a Monte-Carlo approximation for typical nonlinear cases using the reparameterization trick, etc.
==================================================

Focused review:

Weaknesses:
It was mentioned that a proof on how Eq. 6 is not unbiased would be provided, but only an empirical example was provided in the supplementary materials. It is still unclear to me how the construction of Eq. 6 is not unbiased.
Although the results are superior, it would be helpful to provide a comparison on the training/inference speed. Since NeuS is still a volume rendering method with hierarchical sampling as adopted in NeRF, I would expect it to be almost as slow as NeRF to render. How slow would NeuS be compared to IDR?
It would be good to also evaluate how well the recovered 3D representation actually satisfy an SDF, as SDF recovery is also one main goal of the paper. This would be important since efficient rendering (e.g. with sphere tracing) and relighting could be applied on true SDF shapes. The current evaluation seem to focus on 3D shape reconstruction in terms of surface accuracy (measured by Chamfer distances). In Fig. 4 of the supplementary material, the exterior region does not seem to correctly satisfy an SDF.
I think it would also be interesting to see whether Neus would be able to reconstruct surfaces on forward-facing scenes, such as in the LLFF dataset originally considered in NeRF. Does the reparametrization of coordinates using NDC affect the solution?
Other minor problems:
It is unclear what s
in L123 -- is it a hyperparameter that should be tuned? δ i
is undefined in Eq. 6. (I'm guessing it corresponds to t i + 1 − t i ?)
L183: the equation seems off, as ϕ ¯ = ϕ s ∘ f
(Eq. 2).
L192: the differential is missing.

Review Point: 6. (I'm guessing it corresponds to t i + 1 − t i ?) L183: the equation seems off, as ϕ ¯ = ϕ s ∘ f (Eq. 2).
==================================================

Focused review:

- This is a good empirical study, yet lacking of new proposals for neural architecture encoding. A good empirical analysis not only provides insightful and inspirational observations or conclusions, but also contains new initial proposals based upon the analysis. - My major concern is the transferability of the architecture encodings. The author(s) should provide the experiments on real search spaces, such as DARTS space and the comment used MobileNetV3 space, and use the encoding methods for performance prediction. Performing comparisons with SOTA NAS approaches to verify the transferability and robustness of encoding based predictor. Although the author(s) provide the experiments on benchmarks, yet as we know, the search space of existing benchmarks are extremely small. The results on Benches are good, yet not sufficient enough for practical scenarios.

Review Point: - This is a good empirical study, yet lacking of new proposals for neural architecture encoding. A good empirical analysis not only provides insightful and inspirational observations or conclusions, but also contains new initial proposals based upon the analysis.
Review Point: - My major concern is the transferability of the architecture encodings. The author(s) should provide the experiments on real search spaces, such as DARTS space and the comment used MobileNetV3 space, and use the encoding methods for performance prediction. Performing comparisons with SOTA NAS approaches to verify the transferability and robustness of encoding based predictor. Although the author(s) provide the experiments on benchmarks, yet as we know, the search space of existing benchmarks are extremely small. The results on Benches are good, yet not sufficient enough for practical scenarios.
==================================================

Focused review:

Weaknesses and Questions:
The paper is not very easy to follow.
The distintion between simple and complex is not very clear. For example: flip results in a large jump in the pixel space. Pad and crop is similar to translate in AA. I'm not sure how the authors draw the line and sperate the two augmentations. Many AA have little effect on the pixel space statistics.
Line 143-151: I don't think the results shown conclusively support the arguments presented. I think the section should be revised to better explain what the authors mean. For example, the authors say (Line 145) There is a large difference between the augmented data and test data in pixel space, although they may be similar in feature space. . I think the differnce between the augmented data and the test data is similar to that between the augmented data and the adverserial test data in the pixel space, since the adversarial examples are only an ϵ
away from the real test data.
Table 1 should include baseline models trained with neither of the data augmentation types and both.
Lines 221-223: I don't understand the reasoning here. γ and β
are learned parameters and expected to be different when trained on different data, even if the data disterbution is similar. γ and β
scale and add a bias to the normalized data (equation in line 208). Figure 1 shows the the mean and var are the same for both augmentation types (in cos similarity) which hints the the statstics of both augmentation types are similar. This contracdicts the argument the authors presenting ?
UPDATE: I appreciate the authors' comprehensive response and clarification of the ambiguous points. I must admit that I am still not convinced that the authors' method of splitting the augmentation is completely sound. However, the authors provide strong empirical evidence to support their approach.

Review Point: . I think the differnce between the augmented data and the test data is similar to that between the augmented data and the adverserial test data in the pixel space, since the adversarial examples are only an ϵ away from the real test data. Table 1 should include baseline models trained with neither of the data augmentation types and both. Lines 221-223: I don't understand the reasoning here. γ and β are learned parameters and expected to be different when trained on different data, even if the data disterbution is similar. γ and β scale and add a bias to the normalized data (equation in line 208). Figure 1 shows the the mean and var are the same for both augmentation types (in cos similarity) which hints the the statstics of both augmentation types are similar. This contracdicts the argument the authors presenting ? UPDATE: I appreciate the authors' comprehensive response and clarification of the ambiguous points. I must admit that I am still not convinced that the authors' method of splitting the augmentation is completely sound. However, the authors provide strong empirical evidence to support their approach.
==================================================

Focused review:

Weaknesses
1.The comparison objects selected in the comparison experiment are very old, and the performance has not been greatly improved.
2.Poor effect in deep network model.
3.Insufficient experiments in image segmentation.
The idea is very good, but I need better experimental results to support it.

Review Point: 1.The comparison objects selected in the comparison experiment are very old, and the performance has not been greatly improved.
Review Point: 3.Insufficient experiments in image segmentation. The idea is very good, but I need better experimental results to support it.
==================================================

Focused review:

1. Some implemented details are not clear. a) In the point head network, are the predictions (including top-k strategies) of top-left points and bottom-right points individual? b) How do the features of two corner points enhance the single regression feature? It seems that there is no detailed explanation about them in the paper, and it would be better if there are figures to illustrate them. 2. The proposed method has an extra point head and two pixel-wise attention layers which are time-consuming. a) What is the input size during the FLOPS calculation? b) What is the time cost of BVR? Can you provide a detailed time cost analysis about BVR?

Review Point: 1. Some implemented details are not clear. a) In the point head network, are the predictions (including top-k strategies) of top-left points and bottom-right points individual? b) How do the features of two corner points enhance the single regression feature? It seems that there is no detailed explanation about them in the paper, and it would be better if there are figures to illustrate them.
Review Point: 2. The proposed method has an extra point head and two pixel-wise attention layers which are time-consuming. a) What is the input size during the FLOPS calculation? b) What is the time cost of BVR? Can you provide a detailed time cost analysis about BVR?
==================================================

Focused review:

Weaknesses: They gloss over the details of their character-based encoder.
There are many different ways to learn character-based representations, and omitting a discussion of how they do this leaves open questions about the generality of their findings. Also, their analysis could've been made more interesting had they chosen languages with richer and more challenging morphology such as Turkish or Finnish, accompanied by finer-grained morphology prediction and analysis.
- General Discussion: This paper brings insight into what NMT models learn about morphology by training NMT systems and using the encoder or decoder representations, respectively, as input feature representations to a POS- or morphology-tagging classification task. This paper is a straightforward extension of "Does String-Based Neural MT Learn Source Syntax?," using the same methodology but this time applied to morphology. Their findings offer useful insights into what NMT systems learn.

Review Point: -General Discussion: This paper brings insight into what NMT models learn about morphology by training NMT systems and using the encoder or decoder representations, respectively, as input feature representations to a POS- or morphology-tagging classification task. This paper is a straightforward extension of "Does String-Based Neural MT Learn Source Syntax?," using the same methodology but this time applied to morphology. Their findings offer useful insights into what NMT systems learn.
==================================================

Focused review:

Weaknesses:
Although the author acknowledged this as well, there was no evaluation on any mitigation. I think it is critical to know the privacy-utility tradeoff for different mitigation strategies to understand the real impact of any attack.
Seems like there are also other easy defenses which the author did not discuss. 1). If the server and client encrypts their traffic then the eavesdropper could not decode anything from the cipher-text. 2). If the model is fine-tuned on all parameters except the word embedding layer then the adversary could not get the bag-of-words in the first place.
In Section 4.2, the author mentioned that they could not determine the frequency of words. I don’t think this claim holds as one could learn frequency information from the norm of word embedding gradients, i.e. more frequent words tend to have seen more SGD updates and thus larger norms.
The precision number in Table 3 seems to suggest that recovering multiple sentences would be hard with FILM and results in quite a number of false positives.
The authors acknowledged their limitations but I think the paper can be improved if there were quantitative measurements on the limitations.

Review Point: 1). If the server and client encrypts their traffic then the eavesdropper could not decode anything from the cipher-text.
Review Point: 2). If the model is fine-tuned on all parameters except the word embedding layer then the adversary could not get the bag-of-words in the first place. In Section 4.2, the author mentioned that they could not determine the frequency of words. I don’t think this claim holds as one could learn frequency information from the norm of word embedding gradients, i.e. more frequent words tend to have seen more SGD updates and thus larger norms. The precision number in Table 3 seems to suggest that recovering multiple sentences would be hard with FILM and results in quite a number of false positives. The authors acknowledged their limitations but I think the paper can be improved if there were quantitative measurements on the limitations.
==================================================

Focused review:

Weaknesses:
Loss function: The proposed Decoupled Uniformity loss extends the uniformity loss in [54] and does not explicitly include the alignment loss in [54], which is elegant. The authors claimed that the proposed loss function can implicitly encourage alignment (also showed theoretically in Theorem 1 with insufficient training samples). The reviewer wonder what if you add the alignment loss similar to [54]. Would that increase or degrade the performance in practice, ie., when the number of the training samples is neither too low nor infinity? As shown in the experiment, without the prior information, the proposed method cannot beat the SOTA.
Assumptions: The authors introduced the Weak-aligned encoder, which is claimed to be weaker than previous assumptions such as L-smoothness. In the implementation, how to ensure this assumption is satisfied?
Assumptions: How to ensure Assumption 2 in implementation?
Definition 3.7: How to find the value of \lambda? Experiments:
. From the experimental results, without the prior information (the same as the benchmarks) the proposed method has no advantage compared to the SOTA. The advantage only shows when using the prior knowledge. Such comparison is a bit unfair, because in this case the proposed method essentially requires two representation models learned based on each dataset, ie., VAE/GAN + CL. Such extra complexity and cost need to be considered.
. Kernel quality is crucial to the downstream performance as shown experimentally in Appendix Fig. 4. In the implementation, how to choose a proper kernel with optimal parameters? Are these parameters jointly learned or picked via the validation sets?
. In Table 2, the performance of the proposed method is shown with 4 views. Do the benchmarks also use 4 views? If not, please provide results with 2 views for the fair comparison.

Review Point: . From the experimental results, without the prior information (the same as the benchmarks) the proposed method has no advantage compared to the SOTA. The advantage only shows when using the prior knowledge. Such comparison is a bit unfair, because in this case the proposed method essentially requires two representation models learned based on each dataset, ie., VAE/GAN + CL. Such extra complexity and cost need to be considered.
Review Point: . Kernel quality is crucial to the downstream performance as shown experimentally in Appendix Fig.
Review Point: 4. In the implementation, how to choose a proper kernel with optimal parameters? Are these parameters jointly learned or picked via the validation sets?
Review Point: . In Table 2, the performance of the proposed method is shown with 4 views. Do the benchmarks also use 4 views? If not, please provide results with 2 views for the fair comparison.
==================================================

Focused review:

1. L003 mentioned the proposed framework is a self-supervised learning framework. IIUC, the model still needs cross-modal (x-modal) alignment to train. Why is the proposed framework a self-supervised learning framework?
2. It is unclear to me how the gradient back-prop in the equation of L165?
3. Cross-modal code matching: The key of the proposed approach is the x-modal code matching. It seems the codebook should be large enough to cover all semantic information in the dataset. I wonder how to determine the codebook space? Would the initalization of the codebook affect the performance? What is the performance under different codebook size?
4. The proposed approach achieves higher performance than the baseline. I wonder is it due to the additional codebook or the increased model capacity?
5. The interpretation part is a little bit confusing. Fig.3 clearly shows the codebook is aligned with the action. However, even with Fig. 3, it is still hard to interpret the output embedding of f^A_{code}. Imagine that given an embedding and a codebook, I wonder how to interpret the embedding?
I think this paper is well-written and easy to follow.

Review Point: 1. L003 mentioned the proposed framework is a self-supervised learning framework. IIUC, the model still needs cross-modal (x-modal) alignment to train. Why is the proposed framework a self-supervised learning framework?
Review Point: 2. It is unclear to me how the gradient back-prop in the equation of L165?
Review Point: 3. Cross-modal code matching: The key of the proposed approach is the x-modal code matching. It seems the codebook should be large enough to cover all semantic information in the dataset. I wonder how to determine the codebook space? Would the initalization of the codebook affect the performance? What is the performance under different codebook size?
Review Point: 4. The proposed approach achieves higher performance than the baseline. I wonder is it due to the additional codebook or the increased model capacity?
Review Point: 5. The interpretation part is a little bit confusing. Fig.3 clearly shows the codebook is aligned with the action. However, even with Fig. 3, it is still hard to interpret the output embedding of f^A_{code}. Imagine that given an embedding and a codebook, I wonder how to interpret the embedding? I think this paper is well-written and easy to follow.
==================================================

Focused review:

weaknesses of the paper are mainly on the experiments:
- While not familiar with the compared models DMM and DVBF in details, the reviewer understood from the paper their differences with KVAE. However, the reviewer would appreciate a little bit more detailed presentation of the compared models. Specifically, the KVAE is simpler as the state space transition are linear, but it requires the computation of the time-dependant LGSSM parameters \gamma. Can the authors comment on the computation requirements of the 3 methods compared in Table 1 ?
- Why the authors did not test DMM and DVBF on the task of imputing missing data ?

Review Point: - While not familiar with the compared models DMM and DVBF in details, the reviewer understood from the paper their differences with KVAE. However, the reviewer would appreciate a little bit more detailed presentation of the compared models. Specifically, the KVAE is simpler as the state space transition are linear, but it requires the computation of the time-dependant LGSSM parameters \gamma. Can the authors comment on the computation requirements of the 3 methods compared in Table 1 ?
Review Point: - Why the authors did not test DMM and DVBF on the task of imputing missing data ?
==================================================

Focused review:

Overall, I enjoy reading this paper: simple but effective idea, clear presentation, and convincing qualitative and quantitative results. However, I have certain concerns as follows. Please address the concerns in the rebuttal and incorporate the feedback in the final version. - The proposed method seems in principle not effective to non-stationary textures, as well as non-repetitive texture patterns. The conclusion discusses this issue a little bit and the supplementary materials showed some examples. I would like more discussions about the shortcomings, for example a short section to illustrate the failure cases and tell possible improvements upon the current framework that can handle them. - The ablation study only gave quantitative comparisons. But as it is a synthesis work, I expect qualitative results as well to convince the readers that each proposed module is effective and solves the limitations of the baseline. - The proposed upsampling operation uses a transposed convolution layer. I wonder whether the transposed convs can be replaced by other upsampling modules? For example, nearest neighbor upsampling + regular convolutional layer and etc. I am afraid a simple transposed conv will introduce aliasing artifact.

Review Point: - The proposed method seems in principle not effective to non-stationary textures, as well as non-repetitive texture patterns. The conclusion discusses this issue a little bit and the supplementary materials showed some examples. I would like more discussions about the shortcomings, for example a short section to illustrate the failure cases and tell possible improvements upon the current framework that can handle them.
Review Point: - The ablation study only gave quantitative comparisons. But as it is a synthesis work, I expect qualitative results as well to convince the readers that each proposed module is effective and solves the limitations of the baseline.
Review Point: - The proposed upsampling operation uses a transposed convolution layer. I wonder whether the transposed convs can be replaced by other upsampling modules? For example, nearest neighbor upsampling + regular convolutional layer and etc. I am afraid a simple transposed conv will introduce aliasing artifact.
==================================================

Focused review:

Weaknesses:
1.The innovation of the article seems limited to me, mainly since the work shares the same perspective as [2]. Both the models build upon the probabilistic formulation and applies the Hilbert-Schmidt Independence Criteria (HSIC). It may be good to clarify a bit more on how novel the paper is compared from [2].
2.There is a lack of qualitative experiments to demonstrate the validity of the conditional independence model. a)It is better to provide some illustrative experimental results to demonstrate that minimising HSICcond-i could indeed perform better than minimising HSIC_HOOD. Possibly, one toy dataset can be used to demonstrate the separability of inlier features and outlier features.
b)The authors propose a new test metric, however, lacking the correctness test and comparative experiments with other metrices. It may be better to provide some visualization results or schematic diagram, which could make readers easier to understand.
3.Current experimental results seem not very convincing to me. Some critical comparative results are missing. a)Under the setting of unseen OOD training data, DIN [34], Mahalanobis distance [31], Energy [36], their original papers did not use fake/augmented OOD training data. These settings need to be clarified in the paper. Moreover, the impact of using different augmentation methods on the result could be explored in the ablation.
b)In CIFAR-100, the experimental setup appears to be consistent with that of HOOD [2]. However, in Table 1 (unseen OOD training data), the HOOD’s results are missing. In [2], the results of HOOD are superior to those of Conditional-I's.
c)In Table 2 (unseen OOD training data), the HOOD’s results are also missing.
d)There are missing both Conditional-i-generative and HOOD results for the NLP OOD detection tasks. As missing the results of the most relevant methods, the present experiments could not convince me of the validity of the improvements.
4.The memory bank architecture is one contribution, but the authors do not provide quantitative results of introducing the memory bank architecture.
The authors seemed not to discuss the limitations of the proposed model.

Review Point: 1.The innovation of the article seems limited to me, mainly since the work shares the same perspective as [2]. Both the models build upon the probabilistic formulation and applies the Hilbert-Schmidt Independence Criteria (HSIC). It may be good to clarify a bit more on how novel the paper is compared from [2].
Review Point: 2.There is a lack of qualitative experiments to demonstrate the validity of the conditional independence model. a)It is better to provide some illustrative experimental results to demonstrate that minimising HSICcond-i could indeed perform better than minimising HSIC_HOOD. Possibly, one toy dataset can be used to demonstrate the separability of inlier features and outlier features. b)The authors propose a new test metric, however, lacking the correctness test and comparative experiments with other metrices. It may be better to provide some visualization results or schematic diagram, which could make readers easier to understand.
Review Point: 3.Current experimental results seem not very convincing to me. Some critical comparative results are missing. a)Under the setting of unseen OOD training data, DIN [34], Mahalanobis distance [31], Energy [36], their original papers did not use fake/augmented OOD training data. These settings need to be clarified in the paper. Moreover, the impact of using different augmentation methods on the result could be explored in the ablation. b)In CIFAR-100, the experimental setup appears to be consistent with that of HOOD [2]. However, in Table 1 (unseen OOD training data), the HOOD’s results are missing. In [2], the results of HOOD are superior to those of Conditional-I's. c)In Table 2 (unseen OOD training data), the HOOD’s results are also missing. d)There are missing both Conditional-i-generative and HOOD results for the NLP OOD detection tasks. As missing the results of the most relevant methods, the present experiments could not convince me of the validity of the improvements.
Review Point: 4.The memory bank architecture is one contribution, but the authors do not provide quantitative results of introducing the memory bank architecture. The authors seemed not to discuss the limitations of the proposed model.
==================================================

Focused review:

1. The explanation could be made clearer. At many places in the methodology the authors lose the forest for the trees. Perhaps, it is because they skip explaining terms like 'knockoff' or control group and treatment group, which is not often found in pruning literature and needs to be motivated/explained in situ. 2. In the result tables, the accuracies of the 'original' architectures keep changing. They should perhaps be reported with mean and std. deviation so that the variance in the baseline doesn't obsure the performance comparison of different approaches. 3. The results with random samples and noise as knockoff data, as reported in Table 4 are particularly interesting. However, to me, they also seem to highlight that fact that similar compression rates can be obtained without much difference in accuracy (half a percent point on cifar10), by using simpler schemes to create 'knockoff feautures' rather than using a computational expensive generator to generate knockoff data. Maybe, more experiments could be done with larger architectures and datasets to explore this.

Review Point: 1. The explanation could be made clearer. At many places in the methodology the authors lose the forest for the trees. Perhaps, it is because they skip explaining terms like 'knockoff' or control group and treatment group, which is not often found in pruning literature and needs to be motivated/explained in situ.
Review Point: 2. In the result tables, the accuracies of the 'original' architectures keep changing. They should perhaps be reported with mean and std. deviation so that the variance in the baseline doesn't obsure the performance comparison of different approaches.
Review Point: 3. The results with random samples and noise as knockoff data, as reported in Table 4 are particularly interesting. However, to me, they also seem to highlight that fact that similar compression rates can be obtained without much difference in accuracy (half a percent point on cifar10), by using simpler schemes to create 'knockoff feautures' rather than using a computational expensive generator to generate knockoff data. Maybe, more experiments could be done with larger architectures and datasets to explore this.
==================================================

Focused review:

- There isn't a clear description of the model and problem studied. - A main difference with previous dynamic streaming results is that previous algorithms are in the streaming setting and use a small amount of memory that is sublinear in the number of elements. This algorithm keeps all elements in memory. This difference with the previous results mentioned in this paper should be discussed. - The presentation and the writing could be improved. Especially the introduction, which could use more motivation and background.

Review Point: - There isn't a clear description of the model and problem studied.
Review Point: - A main difference with previous dynamic streaming results is that previous algorithms are in the streaming setting and use a small amount of memory that is sublinear in the number of elements. This algorithm keeps all elements in memory. This difference with the previous results mentioned in this paper should be discussed.
Review Point: - The presentation and the writing could be improved. Especially the introduction, which could use more motivation and background.
==================================================

Focused review:

1. The designed algorithm may not be empirically efficient. At each iteration, the key step of Line 5 of Algorithm 1 needs to solve a convex problem based on the ellipsoid method, which is impractical especially when n goes large. The problem of online learning setting is motivated from the real-world application where each iteration (a small response time) needs to be very efficient. For example, in the application of web search, the parameter n is usually very large or maybe larger than T in some cases? 2. It would be more interesting if there is any simulation study and check how these regrets decrease as the time T goes large.

Review Point: 1. The designed algorithm may not be empirically efficient. At each iteration, the key step of Line 5 of Algorithm 1 needs to solve a convex problem based on the ellipsoid method, which is impractical especially when n goes large. The problem of online learning setting is motivated from the real-world application where each iteration (a small response time) needs to be very efficient. For example, in the application of web search, the parameter n is usually very large or maybe larger than T in some cases?
Review Point: 2. It would be more interesting if there is any simulation study and check how these regrets decrease as the time T goes large.
==================================================

Focused review:

Weaknesses: 1. The proposed method only gets convergence rate in expectation (i.e. only variance bound), not with high probability. Though Chebyshev's inequality gives bound in probability from the variance bound, this is still weaker than that of Bach [3]. 2. The method description lacks necessary details and intuition: - It's not clear how to get/estimate the mean element mu_g for different kernel spaces. - It's not clear how to sample from the DPP if the eigenfunctions e_n's are inaccessible (Eq (10) line 130). This seems to be the same problem with sampling from the leverage score in [3], so I'm not sure how sampling from the DPP is easier than sampling from the leverage score. - There is no intuition why DPP with that particular repulsion kernel is better than other sampling schemes. 3. The empirical results are not presented clearly: - In Figure 1: what is "quadrature error"? Is it the sup of error over all possible integrand f in the RKHS, or for a specific f? If it's the sup over all f, how does one get that quantity for other methods such as Bayesian quadrature (which doesn't have theoretical guarantee). If it's for a specific f, which function is it, and why is the error on that specific f representative of other functions? Other comment: - Eq (18), definition of principal angle: seems to be missing absolute value on the right hand side, as it could be negative. Minor: - Reference for Kernel herding is missing [?] - Line 205: Getting of the product -> Getting rid of the product - Please ensure correct capitalization in the references (e.g., [1] tsp -> TSP, [39] rkhss -> RKHSs) [3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. The Journal of Machine Learning Research, 18(1):714â751, 2017. ===== Update after rebuttal: My questions have been adequately addressed. The main comparison in the paper seems to be the results of F. Bach [3]. Compared to [3], I do think the theoretical contribution (better convergence rate) is significant. However, as the other reviews pointed out, the theoretical comparison with Bayesian quadrature is lacking. The authors have agreed to address this. Therefore, I'm increasing my score.

Review Point: 1. The proposed method only gets convergence rate in expectation (i.e. only variance bound), not with high probability. Though Chebyshev's inequality gives bound in probability from the variance bound, this is still weaker than that of Bach [3].
Review Point: - It's not clear how to get/estimate the mean element mu_g for different kernel spaces.
Review Point: - It's not clear how to sample from the DPP if the eigenfunctions e_n's are inaccessible (Eq (10) line 130). This seems to be the same problem with sampling from the leverage score in [3], so I'm not sure how sampling from the DPP is easier than sampling from the leverage score.
Review Point: - There is no intuition why DPP with that particular repulsion kernel is better than other sampling schemes.
Review Point: - In Figure 1: what is "quadrature error"? Is it the sup of error over all possible integrand f in the RKHS, or for a specific f? If it's the sup over all f, how does one get that quantity for other methods such as Bayesian quadrature (which doesn't have theoretical guarantee). If it's for a specific f, which function is it, and why is the error on that specific f representative of other functions? Other comment:
Review Point: - Eq (18), definition of principal angle: seems to be missing absolute value on the right hand side, as it could be negative. Minor:
Review Point: - Reference for Kernel herding is missing [?] - Line 205: Getting of the product -> Getting rid of the product - Please ensure correct capitalization in the references (e.g., [1] tsp -> TSP, [39] rkhss -> RKHSs) [3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. The Journal of Machine Learning Research, 18(1):714â751, 2017. ===== Update after rebuttal: My questions have been adequately addressed. The main comparison in the paper seems to be the results of F. Bach [3]. Compared to [3], I do think the theoretical contribution (better convergence rate) is significant. However, as the other reviews pointed out, the theoretical comparison with Bayesian quadrature is lacking. The authors have agreed to address this. Therefore, I'm increasing my score.
==================================================

Focused review:

- Despite giving plenty examples in the introduction of applications where "runtime confounding" is an issue, none of them are addressed in the empirical evaluation. It is true that unbiased evaluation of causal effects is difficult in the general setting, but it is common practice to synthesize either treatment assignments or outcomes based on real-world covariates, such as the commonly used IHDP benchmark. UPDATE: In the rebuttal, the authors have addressed this point and provided additional results which will strengthen the paper. - The problem is an instance of a more general problem which is not discussed (see Relation to prior work). UPDATE: In the rebuttal, the authors have addressed this point and provided additional discussion which will strengthen the paper. - I take issue with the idea that standard practice for this setting is "treatment-conditional regression", as defined by the authors. To me, this is a straw-man baseline. For the case where important confounders are known at training time, I have never heard of them being removed from analysis due to not being available at test time. I would expect that runtime imputation would be closer to standard practice, but this baseline is never discussed in the paper, nor used as a baseline. - As acknowledged by the authors, the plugin (and doubly-robust) approach(es) are "simple" but reasonable solutions to the problem at hand. (In fact, I would be surprised if the former is not already in wide-spread use). As such, I wish that the authors would include a discussion on the potential optimality of these solutions. Is a two-stage estimator optimally sample efficient or should we hope to find a better algorithm? Theorem 3.1 shines some light on this issue but is not compared to an alternative approach which could induce a different tradeoff. - The authors state that the biased baseline (TCR) is expected to perform best at low levels of confounding due to the (potentially) compounding error of the two-stage estimators. However, this error should be reduced when sample size increases, while the bias of TCR should remain. Regrettably, this is not confirmed by experiments. In light of this, the results in Figure 1 c) are a little odd as they show that all methods perform about as well. The prediction that "we expect this increase [in error] to be significantly larger for the TCR method that has confounding bias" appears to not be true. Is this an effect due to sample size or something else?

Review Point: - Despite giving plenty examples in the introduction of applications where "runtime confounding" is an issue, none of them are addressed in the empirical evaluation. It is true that unbiased evaluation of causal effects is difficult in the general setting, but it is common practice to synthesize either treatment assignments or outcomes based on real-world covariates, such as the commonly used IHDP benchmark. UPDATE: In the rebuttal, the authors have addressed this point and provided additional results which will strengthen the paper.
Review Point: - The problem is an instance of a more general problem which is not discussed (see Relation to prior work). UPDATE: In the rebuttal, the authors have addressed this point and provided additional discussion which will strengthen the paper.
Review Point: - I take issue with the idea that standard practice for this setting is "treatment-conditional regression", as defined by the authors. To me, this is a straw-man baseline. For the case where important confounders are known at training time, I have never heard of them being removed from analysis due to not being available at test time. I would expect that runtime imputation would be closer to standard practice, but this baseline is never discussed in the paper, nor used as a baseline.
Review Point: - As acknowledged by the authors, the plugin (and doubly-robust) approach(es) are "simple" but reasonable solutions to the problem at hand. (In fact, I would be surprised if the former is not already in wide-spread use). As such, I wish that the authors would include a discussion on the potential optimality of these solutions. Is a two-stage estimator optimally sample efficient or should we hope to find a better algorithm? Theorem 3.1 shines some light on this issue but is not compared to an alternative approach which could induce a different tradeoff.
Review Point: - The authors state that the biased baseline (TCR) is expected to perform best at low levels of confounding due to the (potentially) compounding error of the two-stage estimators. However, this error should be reduced when sample size increases, while the bias of TCR should remain. Regrettably, this is not confirmed by experiments. In light of this, the results in Figure 1 c) are a little odd as they show that all methods perform about as well. The prediction that "we expect this increase [in error] to be significantly larger for the TCR method that has confounding bias" appears to not be true. Is this an effect due to sample size or something else?
==================================================

Focused review:

Weakness/Questions: - Introduction: You mention intrinsic motivation, but you miss another branch of exploration work, based on uncertain value functions. See for example [1,2]. -In the beginning of Section 4 I did not understand why you could suddenly sample checkpoints, while in Sec. 3 you only focused on learning a frame-wise embedding. The end of Sec 5. does explain how this is implemented (with a held-out video), which could be mentioned a bit earlier for clarity. - How much did you select the YouTube demonstrations? Were they expert players, who were very good at solving these games? That might explain why you results are better than e.g. DqfD? - Atari games have a fixed viewpoint, and you additionally spatially align the demonstrations if the videos donât. To what extend is spatial alignment crucial? Would this approach scale to first-person-view games, where alignment is probably much harder? Conclusion: I believe this paper studies an interesting problem. There is big potential for the use of (human) supervision to guide learning, but supervision is a time consuming process. This paper studies video data as a potential form of supervision, which would indeed provide access to a huge amount of demonstrations. The paper contributes a novel method for alignment, and a simple but effective way to incorporate this embedding in an RL agent, which shows good results. It would be good to mention how expert the demonstration videos were, and maybe discuss the potential to generalize to non-fixed viewpoints. Besides that, I think this paper makes a clear contribution. [1] White, D. J. "Uncertain value functions." Management Science 19.1 (1972): 31-41. [2] Osband, Ian, et al. "Deep exploration via randomized value functions." arXiv preprint arXiv:1703.07608 (2017). After rebuttal: It is important to mention the scores and acquisition procedure of the demonstration videos indeed. They were expert demonstrations, but as you mention yourself, a benefit of your approach is that (through the alignment) you can leverage a series of unlabeled expert demonstrations.

Review Point: - Introduction: You mention intrinsic motivation, but you miss another branch of exploration work, based on uncertain value functions. See for example [1,2]. -In the beginning of Section 4 I did not understand why you could suddenly sample checkpoints, while in Sec. 3 you only focused on learning a frame-wise embedding. The end of Sec 5. does explain how this is implemented (with a held-out video), which could be mentioned a bit earlier for clarity.
Review Point: - How much did you select the YouTube demonstrations? Were they expert players, who were very good at solving these games? That might explain why you results are better than e.g. DqfD?
Review Point: - Atari games have a fixed viewpoint, and you additionally spatially align the demonstrations if the videos donât. To what extend is spatial alignment crucial? Would this approach scale to first-person-view games, where alignment is probably much harder? Conclusion: I believe this paper studies an interesting problem. There is big potential for the use of (human) supervision to guide learning, but supervision is a time consuming process. This paper studies video data as a potential form of supervision, which would indeed provide access to a huge amount of demonstrations. The paper contributes a novel method for alignment, and a simple but effective way to incorporate this embedding in an RL agent, which shows good results. It would be good to mention how expert the demonstration videos were, and maybe discuss the potential to generalize to non-fixed viewpoints. Besides that, I think this paper makes a clear contribution. [1] White, D. J. "Uncertain value functions." Management Science 19.1 (1972): 31-41. [2] Osband, Ian, et al. "Deep exploration via randomized value functions." arXiv preprint arXiv:1703.07608 (2017). After rebuttal: It is important to mention the scores and acquisition procedure of the demonstration videos indeed. They were expert demonstrations, but as you mention yourself, a benefit of your approach is that (through the alignment) you can leverage a series of unlabeled expert demonstrations.
==================================================

Focused review:

Weaknesses.
The paper is not as self-contained as it could be, i.e. it is near impossible to understand without reading the core referred literature first. It would help readability a lot if core intuitions and concepts would be briefly discussed when introduced. E.g. when introducing MCR2 in equation 4, it would help the reader a lot what the target of each of the two terms is. Another example, after equation 6 it is stated that this measures the volume - why? (if this is not easy to explain, then there should be a reference to where this is explained). The math also needs more clarification, for example what is the union in equation 6 (Z and \hat Z are both \in Rˆ{dxn}, or)? Is the \Delta R in equation 4 and equation 6 really the same (or is it semantically overloaded)? writing (and logic of the writing) needs to be worked on.
Experiments. The comparisons on generative models in Table 1 are done with rather old baselines (all being at least 4 years old). E.g. DCGAN (Radford 2015) could be replaced by StyleGAN2/3 etc (similar for the VAE methods). It should also be reported what the training time for the method on the datasets is, and how it scales with dataset size and resolution. Also, it should be discussed that the model only roughly encodes the semantics, and sometimes disregards even color (e.g. as seen in Figure 5 where the red car turns yellow). In Figure 14 in the abstract it can be seen that the the model seems to sometimes collapse inputs into the same output? If so this should be discussed.
Already in the Abstract it is claimed that the model learns a discriminative representation, and it is shown in Table 3 to perform in the ballpark of VAE methods on MNIST data. As the model is also trained on ImageNet (see Fig5), why is no discrimnative comparison performed on ImageNet? It'd be interesting to see if the model can scale up to more classes and more complex datasets (what is the scaling behaviour theoretically of the model in the latent space? Does the dimensionality need to be proportional to the number of classes to ensure the possibility of orthogonality?) .
Minor Issues.
The Introduction should start with a section contextualizing the work, i.e. review of context and background, what has been done by others broadly in the field that led to the current work.
Equation 2,3 etc. - the result of function g is not a tuple (X, \hat X). The figure should be changed.
After eq. 3 it is written "later studies [...] surrogate to earth mover's distance". I think this is wrong. The original GAN paper showed that GAN's minimize Jensen-Shannon distance, and Arjovsky et al showed how to build the WGAN that operates on earth mover's distance.
Also on pg 2. - Combination of AE and GAN: ".. started with somewhat different motivation, they have evolved...". While true that both methods are quite different, as both are generative models, it was this motivation (generative modeling) that they become successful in modeling of real-world data. So, the logic behind this sentence is confusing.

Review Point: - the result of function g is not a tuple (X, \hat X). The figure should be changed. After eq. 3 it is written "later studies [...] surrogate to earth mover's distance". I think this is wrong. The original GAN paper showed that GAN's minimize Jensen-Shannon distance, and Arjovsky et al showed how to build the WGAN that operates on earth mover's distance. Also on pg 2.
Review Point: - Combination of AE and GAN: ".. started with somewhat different motivation, they have evolved...". While true that both methods are quite different, as both are generative models, it was this motivation (generative modeling) that they become successful in modeling of real-world data. So, the logic behind this sentence is confusing.
==================================================

Focused review:

1. While the presented model can be posed as a transfer learning problem. This paper is more about concept drift. Therefore, the title Transfer learning via l1 Regularization is a bit too broad and can be misleading for some readers. 2. In the experiments on concept drift and "transfer learning", only Lasso is the only method studied and compared. However, Lasso is not considered as a state-of-the-art (SOTA) for concept drift and transfer learning. Lasso is designed for neither of these problem. On the other hand, there are many other methods for concept drift and transfer learning, with some discussed in the Related Work section but none is compared against in the experiment. 3. There are many existing works on concept drift (e.g. twitter activities, anomaly detection), as the authors have cited. However, this paper studies only synthetic concept drift problems. It is not clear whether the proposed solution can deal with concept drift in real data successfully. 4. Similar to the above, there are many transfer learning benchmarks and methods. However, this paper studies only a synthetic example without comparing to any transfer learning methods (as said above, Lasso is not designed for transfer learning). 5. The end of Sec. 4.2 states that Transfer Lasso showed the best accuracy in feature screening. However, previous works on Lasso screening are not cited or compared, e.g. Ren et al. "Safe feature screening for generalized LASSO." TPAMI 40.12 (2017): 2992-3006. 6. Section 4.3 follows the experiments in [17]. However, the presented results did not include [17] (and related works on the same data) in comparison. 7. Line 253: how was the data divided into 30 batches? 8. Line 258: What is the cause of such computational instability for binary features? What are the ways to mitigate this problem? 9. Figure 5-right: annotations on the colours used are missing. 10. Minor issues. Typo. Line 179: unchaing

Review Point: 1. While the presented model can be posed as a transfer learning problem. This paper is more about concept drift. Therefore, the title Transfer learning via l1 Regularization is a bit too broad and can be misleading for some readers.
Review Point: 2. In the experiments on concept drift and "transfer learning", only Lasso is the only method studied and compared. However, Lasso is not considered as a state-of-the-art (SOTA) for concept drift and transfer learning. Lasso is designed for neither of these problem. On the other hand, there are many other methods for concept drift and transfer learning, with some discussed in the Related Work section but none is compared against in the experiment.
Review Point: 3. There are many existing works on concept drift (e.g. twitter activities, anomaly detection), as the authors have cited. However, this paper studies only synthetic concept drift problems. It is not clear whether the proposed solution can deal with concept drift in real data successfully.
Review Point: 4. Similar to the above, there are many transfer learning benchmarks and methods. However, this paper studies only a synthetic example without comparing to any transfer learning methods (as said above, Lasso is not designed for transfer learning).
Review Point: 5. The end of Sec. 4.2 states that Transfer Lasso showed the best accuracy in feature screening. However, previous works on Lasso screening are not cited or compared, e.g. Ren et al. "Safe feature screening for generalized LASSO." TPAMI 40.12 (2017): 2992-3006.
Review Point: 6. Section 4.3 follows the experiments in [17]. However, the presented results did not include [17] (and related works on the same data) in comparison.
Review Point: 7. Line 253: how was the data divided into 30 batches?
Review Point: 8. Line 258: What is the cause of such computational instability for binary features? What are the ways to mitigate this problem?
Review Point: 9. Figure 5-right: annotations on the colours used are missing.
==================================================

Focused review:

Weaknesses + Clarifications: - The question of the latent variable model seems relevant and interesting. It seems that the mixup method is only as good as the model, and also the trained model might add its own biases to the classification task. It would be nice to see some discussion of this in the paper - I am surprised that mixup improves precision on the adult task. It would be good to see some exploration of this - For experiments, are all runs shown? Or just the Pareto fronts. - A number of hyperparameters (e.g. regularization) are not given - For all the latent path figures (eg Fig 3) why is the y value at x= 0 always 0? Is it normalized to this? Be clear in your description (or maybe I missed it) - I would be interested in seeing some further analysis on this model, perhaps using the interpolations themselves

Review Point: + Clarifications:- The question of the latent variable model seems relevant and interesting. It seems that the mixup method is only as good as the model, and also the trained model might add its own biases to the classification task. It would be nice to see some discussion of this in the paper - I am surprised that mixup improves precision on the adult task. It would be good to see some exploration of this - For experiments, are all runs shown? Or just the Pareto fronts.
Review Point: - A number of hyperparameters (e.g. regularization) are not given - For all the latent path figures (eg Fig 3) why is the y value at x= 0 always 0? Is it normalized to this? Be clear in your description (or maybe I missed it) - I would be interested in seeing some further analysis on this model, perhaps using the interpolations themselves
==================================================

Focused review:

Weaknesses: 1. Complexity and practicability. First of all, the authors didn’t provide any complexity evaluation, such as model size and encoding/decoding time. Despite the authors claimed that the use of lattice VQ solves the problem of codebook size bloating and provides fast quantization process, there is no experimental result to support this claim. More importantly, for lattice VQ, the complexity of practical entropy coding still exponentially increases with VQ dimensions. As mentioned in Section 3.2.2, the model calculates the cumulative probability table of a huge amount of sorted codewords, where the complexity is even larger than that of conventional vector quantization process. The entropy model needs to first produce and store the density integration of each codeword in the lattice VQ codebook, and it then obtains the cumulative probability table at both the encoder and decoder sides. In this paper, the VQ dimensions are set to 24, which is quite large for practical entropy coding. It is important to provide a detailed complexity evaluation for each part of the proposed method. Besides, are the claimed R-D results in Figure 4 & Figure 5 the practical coding results or the estimated results (by using -log(P))? 2. More results on more datasets. The proposed method is only evaluated on Kodak with PSNR metric. What about the performance on high-resolution datasets such as CLIC-2021 and Tecnick? And it would be better to provide the results in terms of MS-SSIM.
Minor concerns: 1. A related work [1] for VQ-based image compression is missing. Please provide the performance comparison and analysis. [1] Zhu, Xiaosu, et al. "Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression. CVPR2022

Review Point: 1. Complexity and practicability. First of all, the authors didn’t provide any complexity evaluation, such as model size and encoding/decoding time. Despite the authors claimed that the use of lattice VQ solves the problem of codebook size bloating and provides fast quantization process, there is no experimental result to support this claim. More importantly, for lattice VQ, the complexity of practical entropy coding still exponentially increases with VQ dimensions. As mentioned in Section 3.2.2, the model calculates the cumulative probability table of a huge amount of sorted codewords, where the complexity is even larger than that of conventional vector quantization process. The entropy model needs to first produce and store the density integration of each codeword in the lattice VQ codebook, and it then obtains the cumulative probability table at both the encoder and decoder sides. In this paper, the VQ dimensions are set to 24, which is quite large for practical entropy coding. It is important to provide a detailed complexity evaluation for each part of the proposed method. Besides, are the claimed R-D results in Figure 4 & Figure 5 the practical coding results or the estimated results (by using -log(P))?
Review Point: 2. More results on more datasets. The proposed method is only evaluated on Kodak with PSNR metric. What about the performance on high-resolution datasets such as CLIC-2021 and Tecnick? And it would be better to provide the results in terms of MS-SSIM. Minor concerns:
Review Point: 1. A related work [1] for VQ-based image compression is missing. Please provide the performance comparison and analysis. [1] Zhu, Xiaosu, et al. "Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression. CVPR2022
==================================================

Focused review:

Weakness: 1) Generally lacking a quantitative measure to evaluate the generated VCEs. Evaluation is mainly performed with visual inspection. 2) As the integration of cone projection shown to be helpful, however it is not clear why this particular projection is chosen. Are there other projections that are also helpful? Is there a theoretical proof that this cone projection resolves the noise of the gradients in non-robust classifiers?
Overall, I think the proposed technique yield better VCE and interesting for the community. I also think that the strengths outweigh the weakness. However, I would be open to hear other reviewers opinion here.

Review Point: 1) Generally lacking a quantitative measure to evaluate the generated VCEs. Evaluation is mainly performed with visual inspection.
Review Point: 2) As the integration of cone projection shown to be helpful, however it is not clear why this particular projection is chosen. Are there other projections that are also helpful? Is there a theoretical proof that this cone projection resolves the noise of the gradients in non-robust classifiers? Overall, I think the proposed technique yield better VCE and interesting for the community. I also think that the strengths outweigh the weakness. However, I would be open to hear other reviewers opinion here.
==================================================

Focused review:

Weaknesses:
The study is limited to only certain kinds of network models. Most notably, this work does not study artificial neural network models and tasks prevalent in machine learning, so some of the results might be misleading or confusing to the NeurIPS community. In particular, the only models used are a max-entropy style statistical model of recurrent dynamics and a simple dynamical RNN, both linear (in the max-ent model the sufficient statistics are linear) in the previous system state (in contrast to nonlinearities used for machine learning and present in real brains). Furthermore, the stimuli are assumed to be static vectors (eqn 1). The networks don't really have to solve any kind of task, as far as I can tell, except to reproduce certain distributions. These model choices are minimally motivated in the text, despite standing out to me as not standard compared to the work I know.
My confidence in the results is lower because it feels like a lot of details were relegated to the supplement, which I did not have time to read.
Small points by line
1: "implies" seems like wrong word
2: "number of possible architectures" this sentence struck me as imprecise, is this a new (minor) result? If so present as such.
89: "whose entries are all larger than zero" is not obvious, explain 100: s i g n ( i )
is incorrect notation, all indices i > 0
, what you mean is there is a label s i ∈ ± 1
for each neuron i
106: "100 ms" is arbitrary, you make no connection to a timescale in the text, the only mention of a step size I see is in Fig 1b, which was hard to find
181: Introducing this new model seems to require a new section. I had a hard time distinguishing between models 1 & 2 throughout the results, as the paper switches back and forth. Be explicit throughout.
183: add comma after "Here" in "Here neural activity"
207: "In learning and optimizing their function" is awkward
The models seem narrow to me, can the authors better motivate them and better discuss their limitations?
The last paragraph of section 3 discusses limitations much too briefly, expand.

Review Point: 89: "whose entries are all larger than zero" is not obvious, explain 100: s i g n ( i ) is incorrect notation, all indices i > 0 , what you mean is there is a label s i ∈ ± 1 for each neuron i 106: "100 ms" is arbitrary, you make no connection to a timescale in the text, the only mention of a step size I see is in Fig 1b, which was hard to find 181: Introducing this new model seems to require a new section. I had a hard time distinguishing between models 1 & 2 throughout the results, as the paper switches back and forth. Be explicit throughout. 183: add comma after "Here" in "Here neural activity" 207: "In learning and optimizing their function" is awkward The models seem narrow to me, can the authors better motivate them and better discuss their limitations? The last paragraph of section 3 discusses limitations much too briefly, expand.
==================================================

Focused review:

Weakness: 1. Even the proposed AV-HuBERT shows impressive performances, but it is an expansion of HuBERT to work in multi-modal data. 2. There are many typos and errors in the manuscript. Please refer to below detailed comments. Moreover, Figure 1 should be improved, the arrows are arranged confusingly. Questions:
The authors use concatenation as the default fusion operator. Which dimension is utilized for concatenation, temporal or channel? If the temporal dimension is used for multi-modal fusion, better to refer to the two papers [R1, R2].
For the loss function in equation (4), their final performance seems obtained by setting the alpha as 0. However, it is hard to find because it is placed in the results of ablation studies in the appendix. Since setting the alpha as zero (the same as omitting the second loss term) is better performed, it would be better to describe the effect of alpha in AV-HuBERT after equation (4) or at the experimental setup, briefly.
The ASR performance of AV-HuBERT is not presented. It is just described as the AV-HuBERT performs worse than HuBERT in text. Moreover, the last sentence in Sec 4.5 seems not enough to explain why AV-HuBERT is not better than HuBERT on ASR, even if multi-modal data is utilized during training. Have the authors re-examined hyper-parameters (m_a, m_v, p_m, p_a, alpha) for ASR task?
2nd paragraph in Sec A.4. What does the sentence “Both modalities are used at test time for feature extraction” mean? There is no downstream task utilizes both modalities. Errors:
Sec 3.2 line #4. In “The cluster assignment z_(1:T)^a~”, Does z_(1:T)^a mean z_(1:T)^i?
3rd paragraph on page 4. “When only modality is used” -> "only one modality".
Last paragraph on page 5. “where B consists of all possible...” -> “where B^-1 maps all possible...”. B seems a mapping function thus the word “consist” seems not appropriate.
2nd paragraph on page 6. “we rely on the joint decoder module”. What does the joint decoder mean? Do the authors use a joint CTC/S2S decoder?
2nd paragraph on page 8. ‘iterative pe-training’ -> ‘iterative pre-training’
1st paragraph on page 14. CMUDict (cmu) -> maybe missing reference.
[R1] Chen, Yen-Chun, et al. "Uniter: Universal image-text representation learning." European conference on computer vision. Springer, Cham, 2020. [R2] Lee, Sangho, et al. "Parameter Efficient Multimodal Transformers for Video Representation Learning." International Conference on Learning Representations. 2020.

Review Point: 1. Even the proposed AV-HuBERT shows impressive performances, but it is an expansion of HuBERT to work in multi-modal data.
==================================================

Focused review:

Weaknesses
I noticed that details are missing for the experiments section. In neither the main text nor the appendix, are there details regarding the architectures being used i.e. number of hidden units and activation functions. Moreover, I don't see any details regarding the annealing schedule for the tempered softmax function. Questions/Comments
In line 556, shouldn't it be P ∈ R K × ( d m a x − d m i n )
, not P ∈ R K × d m a x ?
In line 602, since the latent dimension is 2, I don't think it qualifies as a univariate time series.
In lines 619-629, the dancing bees dataset was described where it was stated that the data is comprised of 6 bees. Were the methods only trained on data from one bee or where each bee considered iid?
In line 671, it was stated the dimensionality of the state variables was set to 4 for the bouncing ball and 3 mode datasets. I find this weird (and slightly concerning), as 1) the true latent dimension is known and 2) the data generating process is a subset of RED-SDS, thus setting the latent dimension to be the true dimension should suffice.
In lines 769-773, the authors noted that they took the scaling of the data into effect by using the change of variables formula. This is interesting because the standard is just to use scale the data and treat it as if it wasn't scaled. I'm curious as to why the authors did this (Note, that the change of variables formula is only for one-to-one function; the sample average and standard deviation are not one-to-one).
In line 776, the authors stated they considered a range of K ∈ [ 2 , 5 ]
. I think it would be more appropriate to write it as K ∈ { 2 , 3 , 4 , 5
}, as K
can only be discrete. Conclusion
I think this was a good paper overall! Only minor modifications are needed. References
[1] Collapsed amortized variational inference for switching nonlinear dynamical systems, https://arxiv.org/abs/1910.09588
The authors adequately addressed the limitations and potential negative societal impact of their work.

Review Point: 2) the data generating process is a subset of RED-SDS, thus setting the latent dimension to be the true dimension should suffice. In lines 769-773, the authors noted that they took the scaling of the data into effect by using the change of variables formula. This is interesting because the standard is just to use scale the data and treat it as if it wasn't scaled. I'm curious as to why the authors did this (Note, that the change of variables formula is only for one-to-one function; the sample average and standard deviation are not one-to-one). In line 776, the authors stated they considered a range of K ∈ [ 2 , 5 ] . I think it would be more appropriate to write it as K ∈ { 2 , 3 , 4 , 5 }, as K can only be discrete. Conclusion I think this was a good paper overall! Only minor modifications are needed. References [1] Collapsed amortized variational inference for switching nonlinear dynamical systems, https://arxiv.org/abs/1910.09588 The authors adequately addressed the limitations and potential negative societal impact of their work.
==================================================

Focused review:

Weaknesses: 1. The problem discussed in this paper is relatively straightforward, even it is targeted for black box variational inference. The overall discussed topic is still relying on the existence of affine mapping for location scale distributed latent variables. The more general reparameterization trick has been discussed in [12] and "Implicit reparameterization gradients (NIPS 2018)". I admit there is no error in this manuscript, but I think that a thorough analysis for more challenging task would meet the acceptance bar of NIPS venue. 2. In the experimental section, only two toy models (Baysian linear and logistic regression) are considered. I understand the experiments are supportive to verify the variance bound proposed in this paper. However, in order to recognize the value of this work, I would like to see at least one additional experiment with amortized variational inference, such as a simple VAE whose bound may be easy to calculate. Typos: In Appendix: Eq below Line 298: \epsilon -> u Eq below Line 301: t -> T

Review Point: 1. The problem discussed in this paper is relatively straightforward, even it is targeted for black box variational inference. The overall discussed topic is still relying on the existence of affine mapping for location scale distributed latent variables. The more general reparameterization trick has been discussed in [12] and "Implicit reparameterization gradients (NIPS 2018)". I admit there is no error in this manuscript, but I think that a thorough analysis for more challenging task would meet the acceptance bar of NIPS venue.
==================================================

Focused review:

1. A critical limitation is how practical the method is given that the proposed method relies on a series of source tasks for building the prompt pool. For real-world applications, for example, it may occur that (1) the quality (annotation, data amount) of source tasks is poor, (2) the source and target tasks are dissimilar so that the knowledge transfer among these tasks may be helpless, as shown by [1], (3) when the number and diversity of source tasks increase to a certain degree, could the proposed PTG still work? How would the proposed method perform under such situations? Or at least analyses should be conducted to show the impacts of the above factors.
2. The proposed method is only evaluated on BART-large model, it would make the paper much stronger if other representative PLMs are also tested, e.g., T5.
3. Baselines need to be introduced more clearly. It is unknown whether both the PREFIX-TUNING baseline and the SPOT baseline also choose BART-large as the backbone PLM same as the proposed method PTG.
4. ( Minor) The proposed method is tied to prompt tuning & generation tasks. It would be interesting if PTG could be extended to other parameter-efficient algorithms (e.g., Adapter and LoRA) and discriminative tasks.
[1] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer
Typos: (Line 514) For each size, we sample and 5 different datasets and average over 2 training random seeds. ( remove the first ''and'')

Review Point: 1. A critical limitation is how practical the method is given that the proposed method relies on a series of source tasks for building the prompt pool. For real-world applications, for example, it may occur that (1) the quality (annotation, data amount) of source tasks is poor, (2) the source and target tasks are dissimilar so that the knowledge transfer among these tasks may be helpless, as shown by [1], (3) when the number and diversity of source tasks increase to a certain degree, could the proposed PTG still work? How would the proposed method perform under such situations? Or at least analyses should be conducted to show the impacts of the above factors.
Review Point: 2. The proposed method is only evaluated on BART-large model, it would make the paper much stronger if other representative PLMs are also tested, e.g., T5.
Review Point: 3. Baselines need to be introduced more clearly. It is unknown whether both the PREFIX-TUNING baseline and the SPOT baseline also choose BART-large as the backbone PLM same as the proposed method PTG.
==================================================

Focused review:

- While the paper certainly stands on its own merits even without the knowledge distillation results, I noticed that you are reporting test accuracy in both Table 1 and Table 2. It is not clear to me whether you (a) performed hyperparameter tuning on the validation set for each model enumeration (e.g. KD, CRD, L_1.0, L_0.1, J_1.0, etc.) and *then* got the best model for each and evaluated it on the test set; or (b) whether you just used the test set as your validation set. The latter approach (although seemingly common in papers nowadays) is not very well principled and can lead to optimistic results. It would be good if you can elaborate on this. - Table 1 and Table 2 should also report variances.

Review Point: - While the paper certainly stands on its own merits even without the knowledge distillation results, I noticed that you are reporting test accuracy in both Table 1 and Table 2. It is not clear to me whether you (a) performed hyperparameter tuning on the validation set for each model enumeration (e.g. KD, CRD, L_1.0, L_0.1, J_1.0, etc.) and *then* got the best model for each and evaluated it on the test set; or (b) whether you just used the test set as your validation set. The latter approach (although seemingly common in papers nowadays) is not very well principled and can lead to optimistic results. It would be good if you can elaborate on this.
Review Point: - Table 1 and Table 2 should also report variances.
==================================================

Focused review:

- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain - Insufficient evaluation: - only one dataset is used - lack of detailed dataset statistics - the comparison is not persuasive
Although OSD is an interesting topic, the submission should address the following issues before it can be published: - Only one dataset is used for evaluation, is there anything more?
- Is it possible to add more evaluations about the synthetic data itself? In the current paper, the comparison is between the whole pipeline and other baselines. Not sure how much performance is made by the data itself, or the training mode itself?
- Is it possible to generalize the application? For example, from the OSD to general opinion and aspect detection? - The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?
- The definitions of usefulness and diversity seem quite intuitive. Any motivations to justify this definition, or is there any alternative form of definition?

Review Point: - Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain - Insufficient evaluation:
Review Point: - only one dataset is used - lack of detailed dataset statistics - the comparison is not persuasive Although OSD is an interesting topic, the submission should address the following issues before it can be published:
Review Point: - Only one dataset is used for evaluation, is there anything more?
Review Point: - Is it possible to add more evaluations about the synthetic data itself? In the current paper, the comparison is between the whole pipeline and other baselines. Not sure how much performance is made by the data itself, or the training mode itself?
Review Point: - Is it possible to generalize the application? For example, from the OSD to general opinion and aspect detection?
Review Point: - The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?
Review Point: - The definitions of usefulness and diversity seem quite intuitive. Any motivations to justify this definition, or is there any alternative form of definition?
==================================================

Focused review:

Weakness: 1. I’m confused about the application scenarios of the proposed method. The authors did not show any application value, though they claimed that MAXENT has a large number of applications in applied machine learning. The proposed method is not easy to understand and implement. The author should at least add one real data application. 2. Differences between the proposed method and the existing methods (Good, 1970; Zhu et al,. 1997; Pandey & Dukkipati, 2013) should be stressed more. The differences can be presented theoretically and numerically.

Review Point: 1. I’m confused about the application scenarios of the proposed method. The authors did not show any application value, though they claimed that MAXENT has a large number of applications in applied machine learning. The proposed method is not easy to understand and implement. The author should at least add one real data application.
Review Point: 2. Differences between the proposed method and the existing methods (Good, 1970; Zhu et al,. 1997; Pandey & Dukkipati, 2013) should be stressed more. The differences can be presented theoretically and numerically.
==================================================

Focused review:

First of all I should say that I like this paper. The following should be taken more like 'issues in need of clarification' or 'things a reader might be confused about' rather than 'weaknesses'. * The main result (Thm 2) seems a bit inconclusive, in the sense that it only holds for a range of p. It's not clear to me what happens when p exceeds the upper bracket; does the risk go up again? As far as I can tell, Thm 2 and Prop 4 don't resolve this, and I find the experimental evidence difficult to interpret (more on that below). * The gap between the upper (Thm 2) and the lower (Prop 4) bound seems quite large to me. In particular, I'd be curious if it's possible to get rid of the constant term on the RHS of (9)? Or should the constant feature in the lower bound? * I don't understand some of the interpretations of Thm 2. For example, in line 170, it says "intuitively, when the signal is strong, it should be easier to detect the true model", which I agree with, and would therefore expect the risk to decrease with ||beta||. Why doesn't it? * I find the experiments (Fig 1) confusing, maybe the authors could explain them more? First, I'd expect the risk curves (of both l1 and l2 minimisers) to be decreasing in p, isn't that what this is all about? Second, it is claimed in Section 3(i), that the risk of l1-minimisers is unaffected by the norm of beta, but there is a clear difference between the green and the orange curve (BP, beta norm = 1 or 0.1). * Related to the above, the term 'descent region' is used multiple times (e.g. in line 190) throughout the paper. It seems central to understanding the figures and the risk bounds, however, it isn't introduced or explained anywhere. For a reader who thinks of double descent as <descent-ascent-descent>, rather than <descent-ascent-descent-ascent> (which seems to be implied), this is confusing. Nitpicks: * [Sec 2] It seems to me that the distinction between scaled and unscaled coefficients complicates the setting, perhaps unnecessarily. Would it be cleaner to just assume X with normalised-Gaussian columns to start with? * In lines 127--128, it says "...even when Eq (8) has multiple solutions, there must exist one with at most n non-zero elements." This seems true geometrically, but it would be nice to have a proof. * In lines 230--231, it says "Assuming that X_train has full row-rank (...), w^I exists if and only if p - s >= n ...". Strictly speaking this isn't true, e.g. if X_train = [Identity, zero]. (But yes, w^I exists almost surely.)

Review Point: * The main result (Thm 2) seems a bit inconclusive, in the sense that it only holds for a range of p. It's not clear to me what happens when p exceeds the upper bracket; does the risk go up again? As far as I can tell, Thm 2 and Prop 4 don't resolve this, and I find the experimental evidence difficult to interpret (more on that below).
Review Point: * The gap between the upper (Thm 2) and the lower (Prop 4) bound seems quite large to me. In particular, I'd be curious if it's possible to get rid of the constant term on the RHS of (9)? Or should the constant feature in the lower bound?
Review Point: * I don't understand some of the interpretations of Thm 2. For example, in line 170, it says "intuitively, when the signal is strong, it should be easier to detect the true model", which I agree with, and would therefore expect the risk to decrease with ||beta||. Why doesn't it?
Review Point: * I find the experiments (Fig 1) confusing, maybe the authors could explain them more? First, I'd expect the risk curves (of both l1 and l2 minimisers) to be decreasing in p, isn't that what this is all about? Second, it is claimed in Section 3(i), that the risk of l1-minimisers is unaffected by the norm of beta, but there is a clear difference between the green and the orange curve (BP, beta norm = 1 or 0.1).
Review Point: * Related to the above, the term 'descent region' is used multiple times (e.g. in line 190) throughout the paper. It seems central to understanding the figures and the risk bounds, however, it isn't introduced or explained anywhere. For a reader who thinks of double descent as <descent-ascent-descent>, rather than <descent-ascent-descent-ascent> (which seems to be implied), this is confusing. Nitpicks:
Review Point: * [Sec 2] It seems to me that the distinction between scaled and unscaled coefficients complicates the setting, perhaps unnecessarily. Would it be cleaner to just assume X with normalised-Gaussian columns to start with?
Review Point: * In lines 127--128, it says "...even when Eq (8) has multiple solutions, there must exist one with at most n non-zero elements." This seems true geometrically, but it would be nice to have a proof.
Review Point: * In lines 230--231, it says "Assuming that X_train has full row-rank (...), w^I exists if and only if p - s >= n ...". Strictly speaking this isn't true, e.g. if X_train = [Identity, zero]. (But yes, w^I exists almost surely.)
==================================================

Focused review:

1. I think the key contribution of this work is the unjustifiable assumptions about identifiability for existing work. Unfortunately, the authors do not provide a theoretical analysis of the difference between the existing counterfactual learning on recommendation and the proposed method. The analysis of supervised learning on recommendation is not convincing. 2. The experimental analysis is nos sufficient; more concrete experiments are needed. It is very important to prove its effectiveness with SOTA methods (recommendation with casual inference), as well as show results in other standard datasets, which can show the effectiveness of the proposed with respect to different dataset complexities. More detailed analysis on the effect of the identifiability issue would be desirable. 3. No broader impact section.

Review Point: 1. I think the key contribution of this work is the unjustifiable assumptions about identifiability for existing work. Unfortunately, the authors do not provide a theoretical analysis of the difference between the existing counterfactual learning on recommendation and the proposed method. The analysis of supervised learning on recommendation is not convincing.
Review Point: 2. The experimental analysis is nos sufficient; more concrete experiments are needed. It is very important to prove its effectiveness with SOTA methods (recommendation with casual inference), as well as show results in other standard datasets, which can show the effectiveness of the proposed with respect to different dataset complexities. More detailed analysis on the effect of the identifiability issue would be desirable.
==================================================

Focused review:

weakness points are identified, and clarifications are requested: - Is random routing really the major contributing factor in your recipe? What will happen if we apply the same curriculum learning of k to training learnable MoEs? Will they also become self-slimmable and suffer less collapse? - Another main critique I hold against this work is that in many experiments, the empirical gains of SMoE-Dropout seem just marginal over other baselines, although the performance increase trend seems consistent as more experts are activated. - In Figure 1, it also appears that SMoE-Dropout performs worse than learnable MoE when the model is small. - I am also curious that, given the “self-slimming” property displayed on the same task, whether and how SMoE-Dropout could help address the compositional generalization challenge, that most MoEs nowadays can only solve with the assistance of clever prompting? For example, it would be very interesting to see the authors testing their trained models on multi-step commonsense or algorithm reasoning cases, perhaps with or without prompting.

Review Point: - Is random routing really the major contributing factor in your recipe? What will happen if we apply the same curriculum learning of k to training learnable MoEs? Will they also become self-slimmable and suffer less collapse?
Review Point: - Another main critique I hold against this work is that in many experiments, the empirical gains of SMoE-Dropout seem just marginal over other baselines, although the performance increase trend seems consistent as more experts are activated.
Review Point: - In Figure 1, it also appears that SMoE-Dropout performs worse than learnable MoE when the model is small.
Review Point: - I am also curious that, given the “self-slimming” property displayed on the same task, whether and how SMoE-Dropout could help address the compositional generalization challenge, that most MoEs nowadays can only solve with the assistance of clever prompting? For example, it would be very interesting to see the authors testing their trained models on multi-step commonsense or algorithm reasoning cases, perhaps with or without prompting.
==================================================

Focused review:

Weakness
There are multiple overclaim in this paper. 1) In the abstract, there is no empirical nor theoretical evidence that ARPO finds optimal policy. It is highly skeptical that ARPO can find an optimal policy in practice as the added KL regularization will rarely converge to zero.
It might be better to discuss whether the policy will converge to the optimal policy, where KL term in eq 3 reaches zero at the end.
An important ablation study could be showing how does the KL regularization change during the training.
The three run averages of results present large variance, and it is not very convincing to me ARPO really achieves statistically better generalization results. Based on the results in Table 1, there is no significant improvement from ARPO compared to baselines.
There is no discussion nor ablations on whether cycle consistency loss finds the underlying structure of MDP. It would be better at least to provide examples of style-transferred images compared with original images.

Review Point: 1) In the abstract, there is no empirical nor theoretical evidence that ARPO finds optimal policy. It is highly skeptical that ARPO can find an optimal policy in practice as the added KL regularization will rarely converge to zero. It might be better to discuss whether the policy will converge to the optimal policy, where KL term in eq 3 reaches zero at the end. An important ablation study could be showing how does the KL regularization change during the training. The three run averages of results present large variance, and it is not very convincing to me ARPO really achieves statistically better generalization results. Based on the results in Table 1, there is no significant improvement from ARPO compared to baselines. There is no discussion nor ablations on whether cycle consistency loss finds the underlying structure of MDP. It would be better at least to provide examples of style-transferred images compared with original images.
==================================================

Focused review:

Weaknesses: 1. 1. The symbols in Section 4.3 are not very clearly explained. 2. This paper only experiments on the very small time steps (e.g.1、2) and lack of some experiments on slightly larger time steps (e.g. 4、6) to make better comparisons with other methods. I think it is necessary to analyze the impact of the time step on the method proposed in this paper. 3. Lack of experimental results on ImageNet to verify the method.
Questions: 1. Fig. 3 e. Since the preactivation values of two networks are the same membrane potentials, their output cosine similarity will be very high. Why not directly illustrate the results of the latter loss term of Eqn 13? 2. Is there any use of recurrent connections in the experiments in this paper? Apart from appendix A.5, I do not see the recurrent connections.

Review Point: 1.1. The symbols in Section 4.3 are not very clearly explained.
Review Point: 2. This paper only experiments on the very small time steps (e.g.1、2) and lack of some experiments on slightly larger time steps (e.g. 4、6) to make better comparisons with other methods. I think it is necessary to analyze the impact of the time step on the method proposed in this paper.
Review Point: 3. Lack of experimental results on ImageNet to verify the method. Questions:
Review Point: 1. Fig. 3 e. Since the preactivation values of two networks are the same membrane potentials, their output cosine similarity will be very high. Why not directly illustrate the results of the latter loss term of Eqn 13?
Review Point: 2. Is there any use of recurrent connections in the experiments in this paper? Apart from appendix A.5, I do not see the recurrent connections.
==================================================

Focused review:

Weaknesses:
Some descriptions are less rigorous and need to be clarified:
The first corollary (line 120) states that setting power parameter 0.5 is optimal according to Eq. 2 and Fig. 1, while it lacks either theoretical proof or comprehensive empirical studies. Is the behavior of power parameter fairly invariant to dataset or network backbone?
For the proposed Adaptive Power Normalization, the power parameter is optimized according to eigenvalues of feature covariance, while the features are updated during training. How does it realize in practice? There is a similar question for the dropout probability in Eq. 5. Could the authors provide their training behavior if they are variable during training? If the parameters are adaptive with model training from scratch, primitive parameters may be far from optimal values. On the other hand, if the variance of probability distribution is rather small, it may indicate less necessary for tuning it during training.
The motivation of performing dropout on convolution channels is not new. The main difference is specializing the strategy with global covariance pooling.
Equipping DropCov into baseline backbones (e.g., ResNet50) brings significant increase on parameter size and computational overhead.
The paper analyzes post-normalization for global covariance pooling unit and introduces channel dropout to improve it, which is not restricted to certain downstream cv tasks. It seems like no potential negative societal impact.

Review Point: 5. Could the authors provide their training behavior if they are variable during training? If the parameters are adaptive with model training from scratch, primitive parameters may be far from optimal values. On the other hand, if the variance of probability distribution is rather small, it may indicate less necessary for tuning it during training. The motivation of performing dropout on convolution channels is not new. The main difference is specializing the strategy with global covariance pooling. Equipping DropCov into baseline backbones (e.g., ResNet50) brings significant increase on parameter size and computational overhead. The paper analyzes post-normalization for global covariance pooling unit and introduces channel dropout to improve it, which is not restricted to certain downstream cv tasks. It seems like no potential negative societal impact.
==================================================

Focused review:

- The technical description of the method (sec 3) was quite difficult to follow. The central methodological decisions (adaptive binning, discretization thereafter by doing what appears to be essentially isotonic regression) were difficult to find the motivation of. I believe these decisions center around being able to make a sample efficiency claim (line 317), but I'm not sure why this method gives that sample efficiency property, and no proof of the claim is presented. More concrete questions/concerns are given in "comments" below, written as I read the presentation.
- There are a few properties of the evaluation/results which are a bit confusing and call conclusions into question. For example: - PosPS (post-hoc Platt Scaling) is lower than MLE in accuracy on a few tasks---If I understand correctly, this should not be true in principle, since the max predictions are supposed to be invariant. Is this just model retrain noise from starting off at different initial model params? Or do I misunderstand?
- The reliability diagrams (Fig 1, top), that is the accuracy vs confidence plots, certainly don't make the proposed methods look better (a perfect system will have the identify function here), and in fact exhibit some pretty notable pathologies (high-accuracy spikes in low confidence regimes, e.g.). Is this a property of dataset pathologies or does it reflect variance/unpredictability in the proposed methods?
- In the ECE-versus-number-of-bins plots in Figure 2, the two calibrated systems (PosCal and the proposed) all have ECEs (calibration performance) very close to the MLE for most values. This hints at the fact that the methods may be very sensitive to bin size and often provide no actual expected calibration improvement, is that right? How was this bin-size hyperparam selected? Was its selected value chosen from the held-out dev set without looking at test at all?
- Difficult to see how this can extend to the structured prediction encoder/decoder setups used quite often across NLP. Does this only work for relatively few-class multiclass classification? This is not a fatal flaw.
- "These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainity-a testimony of an ideal calibrator" this is a pretty over-the-top claim! " Perfect balance?" This doesn seem like isn't an "ideal calibrator," it has nontrivial ECE still, right? I'd scale this paragraph's claims back to things that are empirically supportable - tab 2 is somewhat confusing. Can you turn P1 and P2 into one confusion matrix per row and present it that way? These different columns are all just very different sort of things, strange that they're presented next to each other - What is Fig 2's dataset? the average across all the tasks? Just one task?
- The definition of perfect calibration in the info is perhaps a bit confusing as-is (namely, $P$ has to be a joint over the covariates $x$ and the predictions $f$ right so you need some sort of metric over both, is that right? Or perhaps you just require $f$ to be Borel-measurable and deterministic or something. ( Unrelatedly, I also realize that if $f$ is itself nondeterministic, then you probably need an expectation operator right?). Anyways, it might be helpful to add a short sentence to the text here explaining this eq in words, no need to be too precise.
- Is the output of step (3) differentiable? Do you need subgradients or something to get the loss? I guess this is why it's helpful to know exactly what $\beta$ is.
- A nit but i might use something other than "distance" to describe $d$ in line 231, since it's not in general a valid distance metric, maybe "calibration mismatch" or something.
- Also maybe change $q$ to $\hat q$ in the eq on line 232? Since the thing you're minimizing isn't the true value $q$, which we don't have access to even in principle.
- I'm a bit perplexed by the discrepancy between the matrix $Q$, which is essentially a function of the predictor network, and the $q$ in 232, which is not. Does $Q$ as given suffice to allow us to calculate arbitrary distance functionals between $p$ and $q$, as described in line 231?
- You haven't defined calibration error but instantiate it via $\epsilon$ in line 254. It's abs(p - q) to use the terminology of line (232), is that right? ECE is usually given with the expectation randomness integrating over the simplex $D_K$ right, do we have to do that here? Not sure if this is essential to get all the details precise here but probably it would be good to define what "calibration error" is at the least.
- Don't think you define what the Platt-scaling set $G$ is ranged over by the argmin in step 1.
- What is the set $\beta$ in line 311?
- It's not clear to me why step (3) of the algorithm should be necessary at all. It seems like this wouldn't change the calibration but allows us to estimate the ECE? Is that right? Or does this adaptive binning + discretization (essentially isotonic regression, right?) actually affect calibration in expectation?
- Line (7) of alg 1's pseucode is referenced in the text but the fig doesn't have line numbers, is it possible to add them "the achieved reduction in ECE as compared to all baselines is significant" what does this mean? Paired t-test? p < 0.05? Either describe the test or remove this significance claim - do you have a proof of the sample-efficiency claim in line 317? would be good to put this into an appendix - Is it possible to compare to MCDropout? It would be really nice to see that comparison, but this isn't crucial.

Review Point: - The technical description of the method (sec 3) was quite difficult to follow. The central methodological decisions (adaptive binning, discretization thereafter by doing what appears to be essentially isotonic regression) were difficult to find the motivation of. I believe these decisions center around being able to make a sample efficiency claim (line 317), but I'm not sure why this method gives that sample efficiency property, and no proof of the claim is presented. More concrete questions/concerns are given in "comments" below, written as I read the presentation.
Review Point: - There are a few properties of the evaluation/results which are a bit confusing and call conclusions into question. For example:
Review Point: - PosPS (post-hoc Platt Scaling) is lower than MLE in accuracy on a few tasks---If I understand correctly, this should not be true in principle, since the max predictions are supposed to be invariant. Is this just model retrain noise from starting off at different initial model params? Or do I misunderstand?
Review Point: - The reliability diagrams (Fig 1, top), that is the accuracy vs confidence plots, certainly don't make the proposed methods look better (a perfect system will have the identify function here), and in fact exhibit some pretty notable pathologies (high-accuracy spikes in low confidence regimes, e.g.). Is this a property of dataset pathologies or does it reflect variance/unpredictability in the proposed methods?
Review Point: - In the ECE-versus-number-of-bins plots in Figure 2, the two calibrated systems (PosCal and the proposed) all have ECEs (calibration performance) very close to the MLE for most values. This hints at the fact that the methods may be very sensitive to bin size and often provide no actual expected calibration improvement, is that right? How was this bin-size hyperparam selected? Was its selected value chosen from the held-out dev set without looking at test at all?
Review Point: - Difficult to see how this can extend to the structured prediction encoder/decoder setups used quite often across NLP. Does this only work for relatively few-class multiclass classification? This is not a fatal flaw.
Review Point: - "These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainity-a testimony of an ideal calibrator" this is a pretty over-the-top claim! " Perfect balance?" This doesn seem like isn't an "ideal calibrator," it has nontrivial ECE still, right? I'd scale this paragraph's claims back to things that are empirically supportable - tab 2 is somewhat confusing. Can you turn P1 and P2 into one confusion matrix per row and present it that way? These different columns are all just very different sort of things, strange that they're presented next to each other - What is Fig 2's dataset? the average across all the tasks? Just one task?
==================================================

Focused review:

Weaknesses: The paper suffers from a convincing and thorough discussion on writing style and implications of the experiments on discourse or pragmatics.
(1) For example, regarding "style", the authors could have sought answers to the following questions: what is the implication of starting an incoherent end-of-story sentence with a proper noun (l. 582)? Is this a sign of topic shift? What is the implication of ending a story coherently with a past tense verb, etc.
(2) It is not clear to me why studies on deceptive language are similar to short or long answers in the current study. I would have liked to see a more complete comparison here.
(3) The use of terms such as "cognitive load" (l. 134) and "mental states" (l. 671) appears somewhat vague.
(4) There is insufficient discussion on the use of coordinators (line 275 onwards); the paper would benefit from a more thorough discussion of this issue (e.g. what is the role of coordinators in these short stories and in discourse in general? Does the use of coordinators differ in terms of the genre of the story? How about the use of "no" coordinators?) (5) The authors do not seem to make it sufficiently clear who the target readers of this research would be (e.g. language teachers? Crowd-sourcing experiment designers? etc.) The paper needs revision in terms of organization (there are repetitions throughout the text). Also, the abbreviations in Table 5 and 6 are not clear to me. - General Discussion: All in all, the paper would have to be revised particularly in terms of its theoretical standpoint and implications to discourse and pragmatics.
===== In their response to the reviewers' comments, the authors indicate their willingness to update the paper and clarify the issues related to what they have experimented with. However, I would have liked to see a stronger commitment to incorporating the implications of this study to the Discourse and Pragmatics area.

Review Point: -General Discussion: All in all, the paper would have to be revised particularly in terms of its theoretical standpoint and implications to discourse and pragmatics. ===== In their response to the reviewers' comments, the authors indicate their willingness to update the paper and clarify the issues related to what they have experimented with. However, I would have liked to see a stronger commitment to incorporating the implications of this study to the Discourse and Pragmatics area.
==================================================

Focused review:

1. The writing felt a bit rushed, with a few typos and confusing notations (see charity below). 2. I'm not entirely sure if I understand the motivation of certain parts of the analysis. In particular, the CI is only established for the linear part of the target (which kind of makes sense since in this asymptotic limit, it is the only thing that RF model can learn); therefore, I do not know the value of including a nonlinear component in addition. See the following sections for additional comments and questions. 3. While I am not an expert in the area, my feeling is that a considerable part of the analysis is known if not rather standard. For instance the setup for CLT resembles Bellec and Zhang 2019, the risk of the RF model is provided in Mei and Montanari 2019, and the utilized linearization in high dimensions is relatively well-known (e.g. previous analysis of the kernel Gram matrix).

Review Point: 2. I'm not entirely sure if I understand the motivation of certain parts of the analysis. In particular, the CI is only established for the linear part of the target (which kind of makes sense since in this asymptotic limit, it is the only thing that RF model can learn); therefore, I do not know the value of including a nonlinear component in addition. See the following sections for additional comments and questions.
Review Point: 3. While I am not an expert in the area, my feeling is that a considerable part of the analysis is known if not rather standard. For instance the setup for CLT resembles Bellec and Zhang 2019, the risk of the RF model is provided in Mei and Montanari 2019, and the utilized linearization in high dimensions is relatively well-known (e.g. previous analysis of the kernel Gram matrix).
==================================================

Focused review:

1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper.
2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline.
3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY).
4. The paper writing is really confusing. First is the paper structure. A lot of model descriptions are describe in the experiment part. And the section 5.4 is the description of ablation methods, which would be more clear by listing together with other baseline methods.
In sentence aggregation model, the details are not quite clear. Given the input sentences, the supervision is comming from the delimiter term. What is the standard of defining the delimiter term? Is it only using strict sentence string matching or semantic matching? There are some missing references in this paper.
[1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019.
[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019.
[3] An architecture for data-to-text systems. Reiter et al., 2007. Typos Line 409 we adopt BART-base for -> we adopt BART-base model for

Review Point: 1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper.
Review Point: 2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline.
Review Point: 3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY).
==================================================

Focused review:

Weaknesses: General:
The NQM experiment shown in Figure 3 is not very meaningful. Given the simplicity of NQM, many learning rate decay schemes can reduce the loss to near 0.
The experiments are only done on a limited selection of architectures on a few image classification datasets only. Given the lack of strong theoretical justification, I would want to see strong empirical results on a wider range of problems (e.g. language models)
The experiment results are not strong. Considering the confidence interval, the AutoDrop results don’t appear to be necessarily better than the baselines (as can be seen from Figure 6). Also, ImageNet experiments are only with 1 seed.
Claims not well supported:
There are many hyperparameters in Algorithm 1. How come the authors claim in the introduction that they introduce no additional hyperparameters?
Page 4: it claims “tracking the saturation of angular velocity is more plausible than tracking the saturation of the loss function because the angular velocity curves have a much harder saturation pattern”. Two issues: 1) just that the angular velocity saturates does not justify learning rate decay; 2) unclear why a “harder saturation pattern” is helpful. Angular velocity saturates before the loss has converged -- should LR decay immediately?
Page 5: the paper claims that the fundamental difference between NQM and DL models is the noisy angular velocity at saturation. I don’t see any supporting evidence for why this happens, and why this is fundamental.
Section 4 claims that the recommended hyperparameters “guarantees good performance for a wide range of model architectures and datasets”. “Guarantee” is a strong word and I expect to see extensive empirical (if not theoretical) evidence. However, experiments are only done on CIFAR and ImageNet with a few ResNet variants, and I don’t think these account for “a wide range of model architectures and datasets”.
Section 5 begins with the claim that it will theoretically show decreasing the LR when angular velocity saturates guarantees sub-linear convergence rate for (momentum) SGD methods. However, this is simply not true. Section 5.2 merely builds a heuristic model v(t), and proceeds to use that in place of the angular velocity.
Clarity & writing issues:
Theorem 1 is rigorous. According to the paragraph below, C* is only approximately between ½ and 0. A theorem should quantify this rigorously, and definitely not claiming C* \in [-½, 0].
Feedback for improvement:
The first two paragraphs in the introduction are too long as a general motivation for deep learning. Also, when you mention statistics (e.g. how much is needed for training a large model), you should always include details such as the type of problem and architecture. Avoid identifying individual companies unless there is a good reason to do so.
Definition 1 is a bit out of place in the introduction. Consider moving it to the background or method section.
Should cite (Zhang et al., 2019) “Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model.” for noisy quadratic model related contents. (currently, the Zhang et al. 2019b citation is for a different paper).
Figure 1&3, don’t abbreviate “iteration” in the labels.
Section 5.1 It’s odd to call a convergence analysis “state-of-the-art”.

Review Point: 1) just that the angular velocity saturates does not justify learning rate decay;
==================================================

Focused review:

Weaknesses.
Writing requires some important improvements. Section 3 and 4 describing the proposed method are hard to follow. (I) The notations are a bit confusing. I would recommend using “t” and “g” to denote text and graph related quantities as superscript instead of subscript. (II) The transition from eq. 2 to eq. 3 is not obvious. An alternative is to introduce eq. 4 first, which corresponds to a Beta-VAE with a cluster-membership distribution as a prior, and then provide a connection to the IB objective as a supportive analysis for eq. 4. (III) Eq. 8 regarding the KL-term is not obvious either, is p(Z_t x_n,e) Gaussian?
The focus of this work is on explainability, however the results regarding this aspect are weak. I would recommend including some human evaluations (user study) to better assess the quality of the explanations generated by proposed method compared to the baselines.
The performance of the proposed recommender system is measured using prediction metrics such MAE and RMSE. In general, MAE and RMSE do not necessarily reflect the quality of item recommendation. Reporting retrieval measures such as Precision and Recall would be more convincing.
Additional comments.
The clustering component is central in the proposed method. It would be useful to have more experiments regarding this aspect. For instance, what is the impact of number of clusters on the performance of the proposed method.

Review Point: 4. (III) Eq. 8 regarding the KL-term is not obvious either, is p(Z_t x_n,e) Gaussian? The focus of this work is on explainability, however the results regarding this aspect are weak. I would recommend including some human evaluations (user study) to better assess the quality of the explanations generated by proposed method compared to the baselines. The performance of the proposed recommender system is measured using prediction metrics such MAE and RMSE. In general, MAE and RMSE do not necessarily reflect the quality of item recommendation. Reporting retrieval measures such as Precision and Recall would be more convincing. Additional comments. The clustering component is central in the proposed method. It would be useful to have more experiments regarding this aspect. For instance, what is the impact of number of clusters on the performance of the proposed method.
==================================================

Focused review:

Weaknesses:
Policy gradient and Monte-Carlo estimation may suffer from high variances and could be unstable, which may not be able to generate explanation consistently. Monte-Carlo estimation could also be inefficient. More analysis on rollout reward evolution with respect to the training timestamps may help.
Line 200 "In this way, we can solve the sparse reward problem and also distribute reward signals at all steps." The sparse reward is a very challenging problem. I understand that Monte-Carlo estimation could help since it gives a reward to each step. However, Monte-Carlo estimation suffers from high variance, which could slow down the convergence. To my knowledge, this is far from "solve " sparse rewards. A term like "alleviate" could be more appropriate. I would also expect more discussion of why Valina Policy Gradient and Monte-Carlo estimation are adopted here, why they can tackle the sparse reward, and why not using other RL algorithms that could be more stable and may generate more consistent explanations, like DQN or A2C. 3.More justification of leveraging the self-attention mechanism to generate stop signal is needed as it seems a bit arbitrary to directly incorporate the \tilde{H}_t(STOP) into action space. In addition, the author mentioned that a maximum number of generation steps is still needed to avoid generating large subgraphs. It is unclear whether the self-attention mechanism is effective. Ablation studies with respect to the sizes of the generated graph without maximum generation steps would be helpful.
-------------------------------- Post rebuttal --------------------------------
The authors' response has addressed my concerns. The score is raised.

Review Point: 3.More justification of leveraging the self-attention mechanism to generate stop signal is needed as it seems a bit arbitrary to directly incorporate the \tilde{H}_t(STOP) into action space. In addition, the author mentioned that a maximum number of generation steps is still needed to avoid generating large subgraphs. It is unclear whether the self-attention mechanism is effective. Ablation studies with respect to the sizes of the generated graph without maximum generation steps would be helpful. -------------------------------- Post rebuttal -------------------------------- The authors' response has addressed my concerns. The score is raised.
==================================================

Focused review:

1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.
2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020))
1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?
2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is "incorporates all components", line 73 also says the "whole encoder", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.
3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.
4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research.

Review Point: 1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.
Review Point: 2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020)) 1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?
Review Point: 2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is "incorporates all components", line 73 also says the "whole encoder", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.
Review Point: 3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.
Review Point: 4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research.
==================================================

Focused review:

Weaknesses:
The approach is connected to a large number of literatures, most of which were not reviewed. Given space, it would be hard to do so. However, current discussion of related work omits several literatures mentioned by the authors. Other possible connections include soft supervised approaches and probabilistic versions of teaching / cooperation.
The exposition is frustratingly vague at many points. Examples are supplied below. This is the largest weakness of the paper, which is that it is nowhere near self-encapsulated or even clear at many points. I would strongly implore the authors to adjust the exposition as the paper could be much more approachable for the broad audience who might find it interesting.
Specific questions and comments:
"Then the student keeps learn22 ing from this batch dataset for the target concept." What does this mean?
"[29] connects sequential teaching to optimal control and gains interesting insights, but it can not produce a practical teaching policy." This is not an adequate discussion of prior work. What constitutes interesting insights? What does "practical teaching policy" mean?
There are several related literatures that are not discussed. Soft supervised learning is one that comes to mind. A second that is more closely related is the probabilistic literature on teaching and cooperation, e.g. Sequential cooperative bayesian inference.
"Machine teaching is shown useful in reinforcement learning [53, 24, 45, 17], human-in the-loop learning [23, 6, 40], crowd sourcing [50, 67, 68] and cyber security [42, 2, 66, 64, 65]. [10, 14, 74, 47, 6, 26] study machine teaching from a more theoretical aspect." Please re-read this sentence.
"We first consider the cleanest teaching protocol following" What does this mean? Why is it the cleanest? One should provide an explanation for a strong statement.
"All these methods can be viewed as using a customized label synthesis policy in the black-box teaching scenario" It would be nice to have a more precise statement of this claim.
Given that the approach unifies knowledge distillation, label smoothing, and self-training, it would be good to review at least some work in those literatures too.
" This result validates our argument that it is not always optimal to feed the learner with ground truth label" I am not sure validates is the word you want here. Validation seems best provided by empirical demonstrations that it works on real data. "Is consistent with" may be a more suitable choice.
The move from predicted to ground truth labels is interesting. I would be interested to hear the author's thoughts on probabilistic interpretations.
" The predicted label is usually easy to learn for the current learner and the optimal label is more difficult to learn, implying that the learner should be taught in an easy-to-hard fashion." It would be helpful to explain why the predicted label is usually easy.
"This implication nicely matches the conclusion in curriculum learning [5] and IM" What conclusion?
" Moreover, greedy LAST also has the property of teaching monotonicity [33] if the learner loss satisfies certain conditions (Appendix C)." The conditions should be stated in the main text.
Figure 3 didn't help me much. It would require more explaining in the caption than simply "example".
" This nice property implies that LAST always converges to w∗ faster than a random teacher (i.e., SGD) if both of them sample teaching examples uniformly from the pool. An illustration of how LAST works and why LAST yields better convergence to w∗ is given in Fig. 3. If the learner loss is properly designed, LAST can converge faster than SGD" These passages are repetitive and do not effectively explain.
" The teaching monotonicity comes from the first iteration where the gradient update in LAST is guaranteed to better minimize the discrepancy between w1 and w∗." A more precise derivation/comparison would be helpful.
" we propose a simple yet effective heuristic to teach MLP learners." How do we know this is an effective heuristic? A more detailed exposition here would be nice.
" This shares similar spirits with back-propagation through time in recurrent networks and meta-learning" What does this mean? A more precise statement would be desirable.
" The greedy policy is in fact the optimal solution to Eq. (5) for T = 1. For larger T, unrolling builds a larger computational graph for back-propagation and is more costly". It is helpful for the reader to provide a clear statement of why optimality is obtained. Also, given that computational considerations are one of the arguments in favor of LAST, an analysis of computational complexity here would clarify the argument.
"Optimizing the unrolled teaching policy is conceptually similar to optimizing recurrent neural networks with back-propagation through time, as illustrated in Fig. 4. Unrolling T steps is equivalent to seeking a teaching policy that best minimizes the weight discrepancy after T-step gradient descent. Broadly speaking, our method also has intrinsic connections to learning an optimizer [4, 30] and teaching a loss function [61] in the sense that both aim to learn better gradients." Again, more precise statements would be helpful.
"We can simply define rt =−kwt −w∗ k as the reward and directly apply policy gradient [59] to solve it." Please explain more.
Theorem 1 contains several terms that are not defined.
"Theorem 1 shows that g(y) plays a critical role, similar in spirit to the way that in line search, the step size is adaptively adjusted." Please re-read the sentence.
"We connect Theorem 1 to Armijo linear search" A more precise term than connect would be desirable.
Figure 5 did not help me. The caption is inadequate.
"Based on [33], it is easy to verify that mixed teaching can achieve ET" Please explain.
"is able to achieve faster teaching empirically" please point to where this is demonstrated.
The discussion of the data in figure 11 is interesting! (but not convincing) I would be interested to hear more about this.

Review Point: 3. If the learner loss is properly designed, LAST can converge faster than SGD" These passages are repetitive and do not effectively explain. " The teaching monotonicity comes from the first iteration where the gradient update in LAST is guaranteed to better minimize the discrepancy between w1 and w∗." A more precise derivation/comparison would be helpful. " we propose a simple yet effective heuristic to teach MLP learners." How do we know this is an effective heuristic? A more detailed exposition here would be nice. " This shares similar spirits with back-propagation through time in recurrent networks and meta-learning" What does this mean? A more precise statement would be desirable. " The greedy policy is in fact the optimal solution to Eq. (5) for T = 1. For larger T, unrolling builds a larger computational graph for back-propagation and is more costly". It is helpful for the reader to provide a clear statement of why optimality is obtained. Also, given that computational considerations are one of the arguments in favor of LAST, an analysis of computational complexity here would clarify the argument. "Optimizing the unrolled teaching policy is conceptually similar to optimizing recurrent neural networks with back-propagation through time, as illustrated in Fig.
Review Point: 4. Unrolling T steps is equivalent to seeking a teaching policy that best minimizes the weight discrepancy after T-step gradient descent. Broadly speaking, our method also has intrinsic connections to learning an optimizer [4, 30] and teaching a loss function [61] in the sense that both aim to learn better gradients." Again, more precise statements would be helpful. "We can simply define rt =−kwt −w∗ k as the reward and directly apply policy gradient [59] to solve it." Please explain more. Theorem 1 contains several terms that are not defined. "Theorem 1 shows that g(y) plays a critical role, similar in spirit to the way that in line search, the step size is adaptively adjusted." Please re-read the sentence. "We connect Theorem 1 to Armijo linear search" A more precise term than connect would be desirable. Figure 5 did not help me. The caption is inadequate. "Based on [33], it is easy to verify that mixed teaching can achieve ET" Please explain. "is able to achieve faster teaching empirically" please point to where this is demonstrated. The discussion of the data in figure 11 is interesting! (but not convincing) I would be interested to hear more about this.
==================================================

Focused review:

1. The quality of the texture is not very satisfying, in Fig.4, there are some artifacts on the arm area (line 2) and even on the front-view (line 3). Also, the results show that the texture copying from the image a little "randomly" when the texture is complicated (Fig.1 DeepFashion part), the results shows few texture details and patterns. 2. It seems that the texture tends to have front-back symmetry in Fig.1. Will the texture have similar phenomena in other data, e.g., when there is a big logo on the front, will the generated texture have a logo on the back? I don't see back-view results with logo on the front. There should be more discussion on this phenomena and more back-view results especially with the above conditions. 3. The comparison with RSTG[33] should also add results rendered in other views, in order to better claim the effectiveness of the cross-view loss. 4. More comparison can be established, e.g. with PIFu.

Review Point: 1. The quality of the texture is not very satisfying, in Fig.4, there are some artifacts on the arm area (line 2) and even on the front-view (line 3). Also, the results show that the texture copying from the image a little "randomly" when the texture is complicated (Fig.1 DeepFashion part), the results shows few texture details and patterns.
Review Point: 2. It seems that the texture tends to have front-back symmetry in Fig.1. Will the texture have similar phenomena in other data, e.g., when there is a big logo on the front, will the generated texture have a logo on the back? I don't see back-view results with logo on the front. There should be more discussion on this phenomena and more back-view results especially with the above conditions.
Review Point: 3. The comparison with RSTG[33] should also add results rendered in other views, in order to better claim the effectiveness of the cross-view loss.
==================================================

Focused review:

- The transition between Sections 3 and 4 is quite abrupt. It would be great if the authors could discuss how Schatten packing can be applied to the robust PCA problem in a high level at the beginning of Section 4; - I would like to see some discussions on the space complexity of the proposed SDP solver, and a comparison against other methods; - Some numerical experiments would be helpful to demonstrate the effectiveness and practicality of the proposed algorithms.

Review Point: - The transition between Sections 3 and 4 is quite abrupt. It would be great if the authors could discuss how Schatten packing can be applied to the robust PCA problem in a high level at the beginning of Section 4;
Review Point: - I would like to see some discussions on the space complexity of the proposed SDP solver, and a comparison against other methods;
Review Point: - Some numerical experiments would be helpful to demonstrate the effectiveness and practicality of the proposed algorithms.
==================================================

Focused review:

Weakness 1 The number of experts is limited to 2. Indeed it would maintain the computation load. But the power of experts may not be exhibited sufficiently. It would be great to investigate the performance by changing the number of experts. 2 I am wondering if this mechanism can be used in the vision transformers?

Review Point: 1 The number of experts is limited to 2. Indeed it would maintain the computation load. But the power of experts may not be exhibited sufficiently. It would be great to investigate the performance by changing the number of experts.
Review Point: 2 I am wondering if this mechanism can be used in the vision transformers?
==================================================

Focused review:

Weaknesses: It is hard to assess what is the contribution of the paper from a practical point of view as SGD automatically avoids possible training problems related to AV. On the theoretical side, the paper does not investigate deeply the relationship between AV and usual flat or sharp minima. For example, how are AV connected to each others and which properties of sharp/flat minima generalize to AV? Questions: - Under what conditions (on the network architecture) do AV appear? Is there an intuitive interpretation of why they can be found in the loss function associated with many `modern' neural networks? - Is the presence of AV restricted to the over-parameterized case? If yes, what happens in the under-parameterized situation? Which of the given theoretical properties extend to that case (as local minima cannot be expected to be equivalent in the under-parameterized case). - Do the structure of AV depend on the type of objective function used for training? What happens if a L-2 penalty term on the weights is added to the loss function? - Would it be possible to built a 2-dimensional analytical example of AV? - The averaged SGD performs well also in the case of convex loss. Is its good behaviour around a AV related to this? - In Section 3.2, it is claimed that AV can be found with `decent probability'. What is the order of magnitude of such probability? Does it depend on the complexity of the model? Does this mean that most of the minima are AV?

Review Point: - Under what conditions (on the network architecture) do AV appear? Is there an intuitive interpretation of why they can be found in the loss function associated with many `modern' neural networks?
Review Point: - Is the presence of AV restricted to the over-parameterized case? If yes, what happens in the under-parameterized situation? Which of the given theoretical properties extend to that case (as local minima cannot be expected to be equivalent in the under-parameterized case).
Review Point: - Do the structure of AV depend on the type of objective function used for training? What happens if a L-2 penalty term on the weights is added to the loss function?
Review Point: - Would it be possible to built a 2-dimensional analytical example of AV?
Review Point: - The averaged SGD performs well also in the case of convex loss. Is its good behaviour around a AV related to this?
Review Point: - In Section 3.2, it is claimed that AV can be found with `decent probability'. What is the order of magnitude of such probability? Does it depend on the complexity of the model? Does this mean that most of the minima are AV?
==================================================

Focused review:

weakness of the work is the strong perhaps not practical assumptions that data from many domains are available and that the causal parameters across domains are random samples. The experimental evaluations are done by simulating data under these assumptions. It is not clear how the proposed methods would perform under real world situations. It would be desirable the paper could motivate the work with possible applications and evaluate the methods in more realistic settings. The paper is mostly clearly written. One key idea I couldnât understand is the test statistic described in line 162-167. What is the meaning of this test statistic and how exactly is the expression computed? The paper mentioned the motivation for the proposed IB and MC methods is that âa general non-parametric independence test may not be efficient.â Should you actually try a general independence test and compare with the proposed IB and MC methods as a baseline? -Line 95, âformâ should be âfromâ. After author feedback: - I donât think the assumption that the causal parameters across domains are random samples is a consequence of not having latent confounders in the system. -It looks to me the performance of the proposed method and the relative performances of MC, IB, and HSIC test would strongly depend on how the parameters are actually changing (or not changing) across domains. In reality, if we don't know how parameters are changing, how much can we trust the output or the output by which test methods should we trust more?

Review Point: - I donât think the assumption that the causal parameters across domains are random samples is a consequence of not having latent confounders in the system. -It looks to me the performance of the proposed method and the relative performances of MC, IB, and HSIC test would strongly depend on how the parameters are actually changing (or not changing) across domains. In reality, if we don't know how parameters are changing, how much can we trust the output or the output by which test methods should we trust more?
==================================================

Focused review:

Weaknesses: the paper has many technical inconsistencies which impair significantly my trust in the presented results.
First, the phrase 'optimal sampling' is used through the paper, including formal statements, while it is clear that the sample distribution is based on theoretical error bounds, which are not tight, and has sampling is not 'optimal'.
Second, the proof, provided in the appendix, of the central theorem of the paper, 3.1, contains errors. 1) the derivative of the 'Lagrangian' is wrong, the derivative of n^{-1/2} is -1/2n^{-3/2} rather than 1/2n^{-3/2} (the minus is omitted). 2) the last line is obviously wrong. 3) this is not a proof of constrained optimization but rather a sketch of it, for a normal proof one has to show that the zero-gradient point is indeed the minimum (it does not have to be).
Third, the formula in Theorem 3.1 has a paradoxical property --- as the number c of strata/branches increases, their relative complexity affects 'optimal' sample distribution less and less. I am not convinced that this is indeed 'optimal' behavior rather than an artifact of a particular form of upper bound used for the proof (and more suitable for theoretical analysis than for practical sampling). This should at least be discussed.
Fourth, there is no supplementary material, so one cannot reproduce the results, or even look at the source code. The only artifact of the empirical evaluation is the 'full code' of the renderer case study in Figure 18, which is a single function written in a version of Turaco different from introduced in the paper.

Review Point: 1) the derivative of the 'Lagrangian' is wrong, the derivative of n^{-1/2} is -1/2n^{-3/2} rather than 1/2n^{-3/2} (the minus is omitted).
Review Point: 3) this is not a proof of constrained optimization but rather a sketch of it, for a normal proof one has to show that the zero-gradient point is indeed the minimum (it does not have to be). Third, the formula in Theorem 3.1 has a paradoxical property --- as the number c of strata/branches increases, their relative complexity affects 'optimal' sample distribution less and less. I am not convinced that this is indeed 'optimal' behavior rather than an artifact of a particular form of upper bound used for the proof (and more suitable for theoretical analysis than for practical sampling). This should at least be discussed. Fourth, there is no supplementary material, so one cannot reproduce the results, or even look at the source code. The only artifact of the empirical evaluation is the 'full code' of the renderer case study in Figure 18, which is a single function written in a version of Turaco different from introduced in the paper.
==================================================

Focused review:

Weaknesses:
\mathcal{S} is used for both state space (configuration space) and for abstract states? Since the state space \mathcal{S} is equal to \mathcal{X}, why not use \mathcal{X} instead?
"This shows that using RL for stochastic motion planning produces robust solutions.": It is shown to be more robust than RRT (in these environments), but 90% success rate is not robust.
Not necessarily something that has to change, but there are possibly more suitable baselines to compare with from the path/motion planning literature. Large-scale path planning has and still is often approached using hierarchical planning, e.g. by A* over an "abstract" graph, then A* in between the nodes of the graph [1]. Modern non-learning based motion planning approaches (e.g. lattice-based motion planning [2]) use translation invariant motion primitives (generated offline using optimal control) as actions in e.g. A*-search to generate physically feasible trajectories in large and complex environments in real-time, with both static and dynamic obstacles. Such methods would provide probably provide a more suitable baseline than RRT (with or without hierarchical comparisons.)
Why call it "Multi-Task" when it is only path planning? I get that it is for many possible different path goals.
[1] Holte, Robert C., et al. "Hierarchical A*: Searching abstraction hierarchies efficiently." AAAI/IAAI, Vol. 1. 1996.
[2] Pivtoraiko, Mihail, Ross A. Knepper, and Alonzo Kelly. "Differentially constrained mobile robot motion planning in state lattices." Journal of Field Robotics 26.3 (2009): 308-333.

Review Point: 1. 1996. [2] Pivtoraiko, Mihail, Ross A. Knepper, and Alonzo Kelly. "Differentially constrained mobile robot motion planning in state lattices." Journal of Field Robotics 26.3 (2009): 308-333.
==================================================

Focused review:

1. Some detaills are missing as I mention below. I undertand this situation becuase of the lots of details for the experiments. Hope it will be more clear in the next version. 2. I am not sure whether the method are called search-based. It seems like it is simulated annealing for approximating the global optimum based on the scores in Equation (1). It is still a sampling-based method. 3. In this framework, the final outputs are from fine-tuned GPT2, not the SA outputs based on the scorer. How is the performance on SA output? In your work, the fine-tuned GPT2 could be treated as the inference network during the decoding. It looks similar to the dirction of these work [1] [2][3]. 4. I think the reader could see the effect of scores. How is the learned scorers? Maybe an ablatioin study could be done? Or compasion bettwen several outputs? [1]. Lifu Tu, Kevin Gimpel. 2018. Learning Approximate Inference Networks for Structured Prediction [2]. LIfu Tu, Kevin Gimpel. 2019. Benchmarking Approximate Inference Methods for Neural Structured Prediction. [3]. Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel. 2020. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation

Review Point: 1. Some detaills are missing as I mention below. I undertand this situation becuase of the lots of details for the experiments. Hope it will be more clear in the next version.
Review Point: 2. I am not sure whether the method are called search-based. It seems like it is simulated annealing for approximating the global optimum based on the scores in Equation (1). It is still a sampling-based method.
Review Point: 3. In this framework, the final outputs are from fine-tuned GPT2, not the SA outputs based on the scorer. How is the performance on SA output? In your work, the fine-tuned GPT2 could be treated as the inference network during the decoding. It looks similar to the dirction of these work [1] [2][3].
Review Point: 4. I think the reader could see the effect of scores. How is the learned scorers? Maybe an ablatioin study could be done? Or compasion bettwen several outputs? [1]. Lifu Tu, Kevin Gimpel. 2018. Learning Approximate Inference Networks for Structured Prediction [2]. LIfu Tu, Kevin Gimpel. 2019. Benchmarking Approximate Inference Methods for Neural Structured Prediction. [3]. Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel. 2020. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
==================================================

Focused review:

The primary weaknesses of this paper are outlined in the following points, for which I would appreciate the authors' response. 1. The optimal transport map estimator \hat \phi^* on line 183 is not formally defined, if I am not mistaken. In the case where the sample sizes of both classes are equal, there is a natural transport map estimator since the Monge problem is feasible. However, there does not exist a transport map between two empirical measures with different sample sizes, thus a transport map estimator cannot simply be obtained by the plug-in principle. Please clarify these aspects before stating Theorem 2. 2. In the proof of Theorem 2, the rate O_p(n^{-1/p}) for the optimal transport map error in the third-to-last-line of the display before line 57 is nontrivial to obtain, to my knowledge. I am not aware of a result yielding this rate under the conditions stated in the Theorem. This rate can be obtained for Wasserstein distances between the empirical measure and the truth, as in the references given, but this does not trivially translate into a rate for the quantity the authors bound here. If I am mistaken about this, I would appreciate a more precise reference from the authors, or a proof. 3. The proof of Theorem 2 also contains some minor technical mistakes and/or typos. I believe the rate O_p(n_1^{-2}) on line 55, and the rate O_p(n_1^{-1}) in the second-to-last line of the display before line 57, should be O_p(n_1^{-1}) and O_p(n_1^{-1/2}) respectively, by the Central Limit Theorem. Furthermore, in equation (6), one of the \hat \phi^{(j)} should be replaced by \hat \phi^{(i)}, I believe. I would not be opposed to the authors removing Theorem 2 entirely. Theorem 1 contains the key result about the method. 4. The main method described in the paper, lines 152-158, is hard to find. I personally would have appreciated seeing this earlier on in the paper, possibly isolated in its own subsection, and not necessarily combined with the technical aspects at the top of page 4 which are not all needed for describing the method. ---------- Comments following rebuttal. I would like to reiterate my interest in the methodology proposed by the authors, and I would be happy to see it published. The authors' rebuttal has addressed points (1), (3), (4) satisfactorily. My concern remains with point (2), since the authors have not provided specific references in their rebuttal. I would point the authors to [arXiv:1905.05828] and references therein for L^2 guarantees for certain transport map estimators, and encourage them to either update the Theorem with appropriate references, or to simply state as an assumption that the transport map estimator is consistent with the suitable rate. While it would be ideal to verify these modifications in a second round of revision, I maintain that they are minor in scope compared to the main contributions of the paper, and thus do not alter my score, under the assumption that the authors will make the appropriate modifications.

Review Point: 1. The optimal transport map estimator \hat \phi^* on line 183 is not formally defined, if I am not mistaken. In the case where the sample sizes of both classes are equal, there is a natural transport map estimator since the Monge problem is feasible. However, there does not exist a transport map between two empirical measures with different sample sizes, thus a transport map estimator cannot simply be obtained by the plug-in principle. Please clarify these aspects before stating Theorem 2.
Review Point: 2. In the proof of Theorem 2, the rate O_p(n^{-1/p}) for the optimal transport map error in the third-to-last-line of the display before line 57 is nontrivial to obtain, to my knowledge. I am not aware of a result yielding this rate under the conditions stated in the Theorem. This rate can be obtained for Wasserstein distances between the empirical measure and the truth, as in the references given, but this does not trivially translate into a rate for the quantity the authors bound here. If I am mistaken about this, I would appreciate a more precise reference from the authors, or a proof.
Review Point: 4. The main method described in the paper, lines 152-158, is hard to find. I personally would have appreciated seeing this earlier on in the paper, possibly isolated in its own subsection, and not necessarily combined with the technical aspects at the top of page 4 which are not all needed for describing the method. ---------- Comments following rebuttal. I would like to reiterate my interest in the methodology proposed by the authors, and I would be happy to see it published. The authors' rebuttal has addressed points (1), (3), (4) satisfactorily. My concern remains with point (2), since the authors have not provided specific references in their rebuttal. I would point the authors to [arXiv:1905.05828] and references therein for L^2 guarantees for certain transport map estimators, and encourage them to either update the Theorem with appropriate references, or to simply state as an assumption that the transport map estimator is consistent with the suitable rate. While it would be ideal to verify these modifications in a second round of revision, I maintain that they are minor in scope compared to the main contributions of the paper, and thus do not alter my score, under the assumption that the authors will make the appropriate modifications.
==================================================

Focused review:

1. The "baseline" may not be appropriate. Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages. The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset).
2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.
3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21 4. paper writing needs improvement (see below)
L76: incomplete sentence "... and ."
I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing.

Review Point: 1. The "baseline" may not be appropriate. Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages. The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset).
Review Point: 2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.
Review Point: 3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21 4. paper writing needs improvement (see below) L76: incomplete sentence "... and ." I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing.
==================================================

Focused review:

Weaknesses:
Figure 1 as well as its caption is not clear to me. 1) The authors claim that “GradCam visualizations in the 6th row indicate that MViTV2 mistakenly consider the text and lyrics” but do not explain how and where it does so. I do not see any supporting details from the visualizations that MViTV treats it as lyrics. 2) I'm not convinced by what the authors conclude that “our MMT can align the sound with the objects”. The highlights are not always on the woodpecker (4th row) or shoes (8th row). 3) In each of the two test cases, these frames look so close that the video appears to be still. Is it essential to show them all?
The description of the AVBottleneck is unclear. 1) Eq. 5 indicates E A F = [ E A , E ^ F ]
but figure 2 shows E A F = [ E F , E A ]
. What is E ^ F
? Does the position order of each token matter? Please be consistent. 2) The authors claim that “the multimodal bottleneck transformer can be stacked into K blocks”. Does every block contain the initial multimodal tokens? I assume not from the descriptions of this part, but it contradicts Eq. 5.
There are three transformer modules in the proposed approach. Are they trained iteratively or in an end-to-end manner?
Figure 5 shows the visualization of another three cases which is similar to figure 1 and has the same flaws. I don’t think it is necessary to repeat visualization if no new point is demonstrated.

Review Point: 1) The authors claim that “GradCam visualizations in the 6th row indicate that MViTV2 mistakenly consider the text and lyrics” but do not explain how and where it does so. I do not see any supporting details from the visualizations that MViTV treats it as lyrics.
Review Point: 2) I'm not convinced by what the authors conclude that “our MMT can align the sound with the objects”. The highlights are not always on the woodpecker (4th row) or shoes (8th row).
Review Point: 3) In each of the two test cases, these frames look so close that the video appears to be still. Is it essential to show them all? The description of the AVBottleneck is unclear.
Review Point: 1) Eq. 5 indicates E A F = [ E A , E ^ F ] but figure 2 shows E A F = [ E F , E A ] . What is E ^ F ? Does the position order of each token matter? Please be consistent.
Review Point: 2) The authors claim that “the multimodal bottleneck transformer can be stacked into K blocks”. Does every block contain the initial multimodal tokens? I assume not from the descriptions of this part, but it contradicts Eq.
Review Point: 5. There are three transformer modules in the proposed approach. Are they trained iteratively or in an end-to-end manner? Figure 5 shows the visualization of another three cases which is similar to figure 1 and has the same flaws. I don’t think it is necessary to repeat visualization if no new point is demonstrated.
==================================================

Focused review:

1. While the experimental section in the paper is nice, it could be improved with some more details, please see below. 2. The paper has both a quite broad focus (on explanations in AI, for black boxes, etc.) and narrow focus (on explanations for Naive Bayes and linear classifiers). There is substantial related work in the area of explanations in Bayesian networks that is not considered. Please see below for further information about this.

Review Point: 1. While the experimental section in the paper is nice, it could be improved with some more details, please see below.
Review Point: 2. The paper has both a quite broad focus (on explanations in AI, for black boxes, etc.) and narrow focus (on explanations for Naive Bayes and linear classifiers). There is substantial related work in the area of explanations in Bayesian networks that is not considered. Please see below for further information about this.
==================================================

Focused review:

Weaknesses: 1) no method adapted from related work for a result comparison 2) some explanations about the uniqueness of the task and discussion on limitations of previous research for solving this problem can be added to emphasize the research contributions further. - General Discussion: The paper presents supervised and weakly supervised models for frame classification in tweets. Predicate rules are generated exploiting language-based and Twitter behavior-based signals, which are then supplied to the probabilistic soft logic framework to build classification models. 17 political frames are classified in tweets in a multi-label classification task. The experimental results demonstrate the benefit of the predicates created using the behavior-based signals. Please find my more specific comments below: The paper should have a discussion on how frame classification differs from stance classification. Are they both under the same umbrella but with different levels of granularity?
The paper will benefit from adding a brief discussion on how exactly the transition from long congressional speech to short tweets adds to the challenges of the task. For example, does past research rely on any specific cross-sentential features that do not apply to tweets? Consider adapting the method of a frame classification work on congressional speech (or a stance classification work on any text) to the extent possible due to its limitations on Twitter data, to compare with the results of this work.
It seems “weakly supervised” and “unsupervised” – these two terms have been interchangeably used in the paper (if this is not the case, please clarify in author response). I believe "weakly supervised" is the more technically correct terminology under the setup of this work that should be used consistently throughout. The initial unlabeled data may not have been labeled by human annotators, but the classification does use weak or noisy labels of some sort, and the keywords do come from experts. The presented method does not use completely unsupervised data as traditional unsupervised methods such as clustering, topic models or word embeddings would. The calculated Kappa may not be a straightforward reflection of the difficulty of frame classification for tweets (lines: 252-253), viewing it as a proof is a rather strong claim. The Kappa here merely represents the annotation difficulty/disagreement. Many factors can contribute to a low value such as poorly written annotation guidelines, selection of a biased annotator, lack of annotator training etc.
(on top of any difficulty of frame classification for tweets by human annotators, which the authors actually intend to relate to).
73.4% Cohen’s Kappa is strong enough for this task, in my opinion, to rely on the annotated labels. Eq (1) (lines: 375-377) will ignore any contextual information (such as negation or conditional/hypothetical statements impacting the contributing word) when calculating similarity of a frame and a tweet. Will this have any effect on the frame prediction model? Did the authors consider using models that can determine similarity with larger text units such as perhaps using skip thought vectors or vector compositionality methods? An ideal set up would exclude the annotated data from calculating statistics used to select the top N bi/tri-grams (line: 397 mentions entire tweets data set has been used), otherwise statistics from any test fold (or labeled data in the weakly supervised setup) still leaks into the selection process. I do not think this would have made any difference in the current selection of the bi/tri-grams or results as the size of the unlabeled data is much larger, but would have constituted a cleaner experimental setup. Please add precision and recall results in Table 4. Minor: please double check any rules for footnote placements concerning placement before or after the punctuation.

Review Point: 1) no method adapted from related work for a result comparison
Review Point: 2) some explanations about the uniqueness of the task and discussion on limitations of previous research for solving this problem can be added to emphasize the research contributions further.
Review Point: -General Discussion: The paper presents supervised and weakly supervised models for frame classification in tweets. Predicate rules are generated exploiting language-based and Twitter behavior-based signals, which are then supplied to the probabilistic soft logic framework to build classification models.
==================================================

Focused review:

Weakness:
The idea presented in the paper seems incremental for algorithmic/learning representation point of view. The paper maybe better suited for a robotics conference like ICRA/IROS.
The paper is not well-written and has lots of ambiguous statements. - Example: Third sentence in abstract could be re-written for maintaining better flow.
There are no Future Work mentioned in the paper?

Review Point: - Example: Third sentence in abstract could be re-written for maintaining better flow. There are no Future Work mentioned in the paper?
==================================================

Focused review:

Weaknesses:
It seems like a time complex scheme for decoding based on the formulation
Lacks generated samples in the main paper for qualitative assessment.
Human evaluation missing. Questions:
Current neural text generation models are trained to optimize the cross-entropy loss. In an ideal scenario lower the cross-entropy (hence perplexity), the better should be the samples. Since this work (as well as its corresponding related works) show that humans don't form sentences that have the lowest perplexity (when measured using the trained model), does this work help in highlighting that cross-entropy loss might not be the correct objective for optimization?
In Algo 1: Do you first perform top-m for obtaining s and then perform top-k later based on the value obtained? If so, what is the time complexity of this process, as a function of m?
From Fig 4(c), it seems that humans prefer sentences having cross-entropy rates ~ 5.25 on avg. Why do you suggest 3 as the target average surprise?
The point about ad-hoc settings being applied for top-p and not for mirostat seems weird. This paper suggests 3.0 as the setting for target average surprise (based on empirical data) while top-p suggests ~0.9 as the setting for p (again based on empirical data - and shown in this paper that CE remains more or less constant wrt the number of generated tokens at p=0.9). In the end, a certain bit of ad-hoc tuning needs to be made for both cases. Why would this algorithm be better than nucleus sampling (Given it is more time consuming)? Overall:
a) The theoretical analysis helps in understanding why top-k and top-p perform the way they do.
b) Based on the underlying assumptions:
Zipfian distribution
cross-entropy loss optimization
Lowest perplexity does not always mean good quality generation.
(I somehow feel a lot of disconnect between assumption 2 and 3), this work proposes an algorithm for adaptive top-k decoding to produce arguably "better" samples.
Please let me know if I am misunderstanding something. I am open to revising my assessment based on the answers to the questions raised. Update:
Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m.

Review Point: 3), this work proposes an algorithm for adaptive top-k decoding to produce arguably "better" samples. Please let me know if I am misunderstanding something. I am open to revising my assessment based on the answers to the questions raised. Update: Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m.
==================================================

Focused review:

Weaknesses and Questions: 1) Can the results be improved so that ProxSVRG+ becomes better than SCSG for all minibatch sizes b? 2) How does the PL condition you use compare with the PL conditions proposed in âGlobal Convergence of Arbitrary-Block Gradient Methods for Generalized Polyak-Åojasiewicz Functionsâ, arXiv:1709.03014 ? 3) Line 114: Is the assumption really necessary? Why? Or just sufficient? 4) I think the paper would benefit if some more experiments were included in the supplementary, on some other problems and with other datasets. Otherwise the robustness/generalization of the observations drawn from the included experiments is unclear. Small issues: 1) Lines 15-16: The sentence starting with âBesidesâ is not grammatically correct. 2) Line 68: What is a âsuper constantâ? 3) Line 75: âmatchesâ -> âmatchâ 4) Page 3: âorcaleâ â âoracleâ (twice) 5) Caption of Table 1: âare definedâ -> âare givenâ 6) Eq (3): use full stop 7) Line 122: The sentence is not grammatically correct. 8) Line 200: ârestatedâ -> ârestartedâ 9) Paper [21]: accents missing for one authorâs name ***** I read the rebuttal and the other reviews, and am keeping my score.

Review Point: 2) How does the PL condition you use compare with the PL conditions proposed in âGlobal Convergence of Arbitrary-Block Gradient Methods for Generalized Polyak-Åojasiewicz Functionsâ, arXiv:1709.03014 ?
Review Point: 3) Line 114: Is the assumption really necessary? Why? Or just sufficient?
Review Point: 4) I think the paper would benefit if some more experiments were included in the supplementary, on some other problems and with other datasets. Otherwise the robustness/generalization of the observations drawn from the included experiments is unclear. Small issues:
Review Point: 1) Lines 15-16: The sentence starting with âBesidesâ is not grammatically correct.
Review Point: 5) Caption of Table 1: âare definedâ -> âare givenâ
Review Point: 9) Paper [21]: accents missing for one authorâs name ***** I read the rebuttal and the other reviews, and am keeping my score.
==================================================

Focused review:

1. It would be better to compare with AtlasNet, which is not an implicit representation but can deal with various topology and produces dense correspondence in a certain sense. 2. It could be better if the authors can show whether this dense correspondence estimation affects the shape reconstruction quality.

Review Point: 1. It would be better to compare with AtlasNet, which is not an implicit representation but can deal with various topology and produces dense correspondence in a certain sense.
Review Point: 2. It could be better if the authors can show whether this dense correspondence estimation affects the shape reconstruction quality.
==================================================

Focused review:

Weaknesses
There is little insight into when the proposed method may break or may not work as well as other competing methods. How does the efficacy of the proposed method vary with the size of the validation dataset? When would the single-step approximation be assumed in eq. (9). Is there any relevant form of data augmentation that may be hard to parametrize in a differentiable way? If so and if such transformations were to be critical for some applications, then wouldn't the competing approaches based on discrete optimization be more preferable?
It is unclear what the benefits of the Bayesian approach are. For example, one could set the noise term η = 0
in eq.(13) and I suspect that the method may still work reasonably well (not shown in the manuscript). The noise injection may potentially improve the diversity of sampled augmentations, but I feel this should be shown to substantiate the benefits of the Bayesian approach.
Minor comments
line 103: "differential" => "differentiable"
line 124: "we pick k<< n for simplicity" -> what do we lose from this assumption?
line 182: "pseudo one-step update" -> why pseudo?
line 204: "second-order derivative … computed more efficiently" => efficient in time or space?
line 232: no space after '.'
line 255: "Tab. 3 and Tab. 2" => Tab. 2 and Tab. 3
The main limitations in my view are the lack of insights into the limitations of the proposed approach as elaborated above. NA for societal impact.

Review Point: 3 and Tab. 2" => Tab. 2 and Tab. 3 The main limitations in my view are the lack of insights into the limitations of the proposed approach as elaborated above. NA for societal impact.
==================================================

Focused review:

Weaknesses: 1) The contribution of this paper is to exploit the knowledge from the multiple pretrained deep models for VQA, while the practical method is doubtful. First, I am wondering why there are different pretrained models for different datasets. It is better to have a deeper analysis or insight into the fusion mechanism of various pretrained deep models. How the different pretrained models contribute to the quality-aware feature extraction is also valuable to be studied. 2) There are many pretrained models for various tasks, more pretrained models are encouraged to study for VQA. 3) I am still unclear about the motivation of intra-consistency and inter-divisibility loss related to quality assessment.

Review Point: 1) The contribution of this paper is to exploit the knowledge from the multiple pretrained deep models for VQA, while the practical method is doubtful. First, I am wondering why there are different pretrained models for different datasets. It is better to have a deeper analysis or insight into the fusion mechanism of various pretrained deep models. How the different pretrained models contribute to the quality-aware feature extraction is also valuable to be studied.
Review Point: 2) There are many pretrained models for various tasks, more pretrained models are encouraged to study for VQA.
Review Point: 3) I am still unclear about the motivation of intra-consistency and inter-divisibility loss related to quality assessment.
==================================================

Focused review:

- Despite the effectiveness of the proposed approach as proven by extensive experiments, the technical novelties are fairly limited. Applying dynamic operations for dynamic mask generation has been previously explored in the segmentation literature, e.g. conditional batch normalization [1], adaptive instance normalization [2], but are not discussed here. What is the benefit of using dynamic convolution as opposed to other conditional operations? - The Fig.4 in SOLO illustrates the location-awareness of each learned kernel, i.e. different instance is activated at different channel (among S^2 channels) depending on the position of the grid in the image. It would be interesting to also provide a similar plot for DFIS (each subfigure corresponds to the mask produced by one of the S^2 kernels) to study whether such position-aware property still holds in DFIS. - Eqn(1) should be M_i,j = G_i,j * F instead. - Typo: threshing in L207 References: [1] Sofiiuk et al. “AdaptIS: Adaptive instance selection network”, in ICCV 2019. [2] Yang et al. “Efficient video object segmentation via network modulation”, in CVPR 2018.

Review Point: - Despite the effectiveness of the proposed approach as proven by extensive experiments, the technical novelties are fairly limited. Applying dynamic operations for dynamic mask generation has been previously explored in the segmentation literature, e.g. conditional batch normalization [1], adaptive instance normalization [2], but are not discussed here. What is the benefit of using dynamic convolution as opposed to other conditional operations?
Review Point: - The Fig.4 in SOLO illustrates the location-awareness of each learned kernel, i.e. different instance is activated at different channel (among S^2 channels) depending on the position of the grid in the image. It would be interesting to also provide a similar plot for DFIS (each subfigure corresponds to the mask produced by one of the S^2 kernels) to study whether such position-aware property still holds in DFIS.
Review Point: - Eqn(1) should be M_i,j = G_i,j * F instead.
==================================================

Focused review:

The authors compare their work to concatenation and alignment baselines. However, I am missing additional method-based related benchmarks. For instance, another mid-fusion baseline (or ablation) could be: - summing activation (only keep the second part of Eq6) - summing activation and adding a soft/hard gating mechanism - performing CBN \gamma = gamma_m1 + gamma(f(x_m2)) - turning the mixture of expert into a fusion block In the end, this paper shows the interest of performing non-naive mid-fusion, but some fundamental part of the approach remains obscure. Can we exchange channels without l1-normalization (e.g. thresholds) ? How does it compare with the modulation approach? Can we unshare the convnet? Why reglazing on half-of the params? As discussed below, the theoretical motivation has a few weaknesses, and it seems to not correlate with the experimental setting.

Review Point: - summing activation (only keep the second part of Eq6) - summing activation and adding a soft/hard gating mechanism - performing CBN \gamma = gamma_m1 + gamma(f(x_m2)) - turning the mixture of expert into a fusion block In the end, this paper shows the interest of performing non-naive mid-fusion, but some fundamental part of the approach remains obscure. Can we exchange channels without l1-normalization (e.g. thresholds) ? How does it compare with the modulation approach? Can we unshare the convnet? Why reglazing on half-of the params? As discussed below, the theoretical motivation has a few weaknesses, and it seems to not correlate with the experimental setting.
==================================================

Focused review:

Weaknesses: 1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. "Seldom previous works", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.

Review Point: 1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it.
Review Point: 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific.
Review Point: 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win.
Review Point: 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN.
Review Point: 5. English is full of errors throughout. "Seldom previous works", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.
==================================================

Focused review:

Weaknesses 1. I am not fully convinced by the motivation. I would like to see supports on the claim that data pre-processing based methods suffer from high dimensionality problem and addressing the dimensionality problem in modeling adversarial noise is good for robustness. In addition, supposed that modeling perturbations in low-dimensional space is beneficial, it is still not clear that the proposed low-dimensional transition matrix actually models the adversarial noise. It would be helpful if the authors can address these concerns. 2. It seems like the proposed transition network works as a kind of post-processing. Instead of parameterizing p ( y x )
using a neural network f ( x )
, the proposed method parameterizes p ( y x ) using g ω ( x , h θ ( x ) )
. Following this lens, why cannot we take g ω ( x , h θ ( x ) )
as a whole and expect a single function f ( x )
to learn and perform as the same? Please kindly elaborate if I am mistaken. 3. In experiments, the comparison with SOTAs may not be fair. Does the method improve robustness merely because it uses a more complicated neural network architecture with more capacity (as it introduces an additional network g ω
)? It would be great if the authors can align the number of network parameters used by each method in the comparison.
Other questions: How does the proposed method perform combined with TRADES[1]?
[1] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.

Review Point: 1. I am not fully convinced by the motivation. I would like to see supports on the claim that data pre-processing based methods suffer from high dimensionality problem and addressing the dimensionality problem in modeling adversarial noise is good for robustness. In addition, supposed that modeling perturbations in low-dimensional space is beneficial, it is still not clear that the proposed low-dimensional transition matrix actually models the adversarial noise. It would be helpful if the authors can address these concerns.
Review Point: 2. It seems like the proposed transition network works as a kind of post-processing. Instead of parameterizing p ( y x ) using a neural network f ( x ) , the proposed method parameterizes p ( y x ) using g ω ( x , h θ ( x ) ) . Following this lens, why cannot we take g ω ( x , h θ ( x ) ) as a whole and expect a single function f ( x ) to learn and perform as the same? Please kindly elaborate if I am mistaken.
Review Point: 3. In experiments, the comparison with SOTAs may not be fair. Does the method improve robustness merely because it uses a more complicated neural network architecture with more capacity (as it introduces an additional network g ω )? It would be great if the authors can align the number of network parameters used by each method in the comparison. Other questions: How does the proposed method perform combined with TRADES[1]? [1] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
==================================================

Focused review:

- The overall novelty is incremental. The main contribution mentioned in L62, i.e. recover skeleton + shape refinement, has been studied in previous works as mentioned in the related work. In my opinion, the fact that this work learns skeleton from partial scan, compared to learning skeleton from other modality [31], does not form a strong contribution. - It would be much better if a figure showing the whole pipeline is given. Fig 3, 4 are not easy to follow. -The ablation study is very helpful in understanding the effectiveness of the proposed components but not sufficient. I would suggest to provide the performance of the model with each component removed. Also, it is better to provide justification for the detailed design of modules in Fig 3 and 4 if they are not extended from previous work. - It is better to provide experiments on real dataset in order to understand the robustness against real data noise.

Review Point: - The overall novelty is incremental. The main contribution mentioned in L62, i.e. recover skeleton + shape refinement, has been studied in previous works as mentioned in the related work. In my opinion, the fact that this work learns skeleton from partial scan, compared to learning skeleton from other modality [31], does not form a strong contribution.
Review Point: - It would be much better if a figure showing the whole pipeline is given. Fig 3, 4 are not easy to follow. -The ablation study is very helpful in understanding the effectiveness of the proposed components but not sufficient. I would suggest to provide the performance of the model with each component removed. Also, it is better to provide justification for the detailed design of modules in Fig 3 and 4 if they are not extended from previous work.
Review Point: - It is better to provide experiments on real dataset in order to understand the robustness against real data noise.
==================================================

Focused review:

* Time complexity-bounds depends on input dataset, i.e., \Delta, i.e., maximum over minimum distane which can be unbounded * Experimental evaluation is based on only 2 datasets. * Resulting algorithm is quite evolved (tree embeddings, LSH and sample tree) to be practical.

Review Point: * Time complexity-bounds depends on input dataset, i.e., \Delta, i.e., maximum over minimum distane which can be unbounded * Experimental evaluation is based on only 2 datasets.
Review Point: * Resulting algorithm is quite evolved (tree embeddings, LSH and sample tree) to be practical.
==================================================

Focused review:

weakness of this manuscript. If we have a look at the results obtained by Graph U-Nets (reported in Table 3 of [R1]), GraphCGAN does not seem to outperform Graph U-Nets in any dataset, including margin of errors.
The authors refer to an ablation study but only a couple of parameters are analyzed, e.g., number of fake nodes in Figure 1 and loss function of the generator in Table 2. The many other parameters of the model are listed in section 5.2 without details about how they were derived.
5. Questions to be addressed during rebuttal period
How does GraphCGAN compare to Graph U-Nets on the Cora, Citeseer and Pubmed datasets?
The margin of errors are quite high, similarly to those reported for GraphSGAN. Can the authors elaborate on why this is the case?
Can the authors confirm on what set (hopefully a partition of the training set) were the plots in Figure 1 obtained? There is no margin of error associated to each point in the plots and it gives the impression of overfitting to the test set.
6. Additional references
[R1] Gao, Hongyang, and Shuiwang Ji. "Graph U-Nets." ICML 2019. PMLR 97:2083-2092

Review Point: 5. Questions to be addressed during rebuttal period How does GraphCGAN compare to Graph U-Nets on the Cora, Citeseer and Pubmed datasets? The margin of errors are quite high, similarly to those reported for GraphSGAN. Can the authors elaborate on why this is the case? Can the authors confirm on what set (hopefully a partition of the training set) were the plots in Figure 1 obtained? There is no margin of error associated to each point in the plots and it gives the impression of overfitting to the test set.
Review Point: 6. Additional references [R1] Gao, Hongyang, and Shuiwang Ji. "Graph U-Nets." ICML 2019. PMLR 97:2083-2092
==================================================

Focused review:

Weaknesses: 1. In Section 3.2, the bipartite graph was mentioned, but I didn't see the authors use the property of the bipartite graph since there are edges constructed in the support set and also in the query set (Figure 4-a). This may not utilize the property from a bipartite graph. 2. As the paper mentioned, the prime and dual networks share the same network design, but I'd like to know whether the authors have used different network designs for prime and dual. What would be the pros and cons with the same/different designs? 3. In Figure 7, the 6 examples are shown for the 5-way 1-shot image classification experiment, I'd like to know why we show the position up to 20? Will the maximum be calculated by K(N + 1) as mentioned in the paper? 3. For the cross-domain result, the discussion may need to add more. For example, why the proposed method has less gain in Cars compared to 1-shot and 5-shot, and why does only CUB increase the performance gain when it's 5-way 5-shot?

Review Point: 1. In Section 3.2, the bipartite graph was mentioned, but I didn't see the authors use the property of the bipartite graph since there are edges constructed in the support set and also in the query set (Figure 4-a). This may not utilize the property from a bipartite graph.
Review Point: 2. As the paper mentioned, the prime and dual networks share the same network design, but I'd like to know whether the authors have used different network designs for prime and dual. What would be the pros and cons with the same/different designs?
Review Point: 3. In Figure 7, the 6 examples are shown for the 5-way 1-shot image classification experiment, I'd like to know why we show the position up to 20? Will the maximum be calculated by K(N + 1) as mentioned in the paper?
Review Point: 3. For the cross-domain result, the discussion may need to add more. For example, why the proposed method has less gain in Cars compared to 1-shot and 5-shot, and why does only CUB increase the performance gain when it's 5-way 5-shot?
==================================================

Focused review:

Weakness: There are several points in the article that confused me, as follows:
In Table 4, the number of parameters of Pretraining + BCT is 43.59, while the number of parameters of Pretraining + BCT + Fix Trunk is 2.29. It seems unreasonable.
As can be seen from Table 4, it is the model distillation operation that provides the largest performance improvement across multi-resolution and same resolution task. What puzzles me is: 1) whether distillation is capable of such a significant improvement and 2) if the distillation operation can improve to such an extent, then the innovation points claimed in this paper may not be reliable and more gains come from the distillation operation.
The task of this paper is to obtain a better cross-resolution representation, and I think it may be more intuitive to visualize it using t-SNE.

Review Point: 1) whether distillation is capable of such a significant improvement and
Review Point: 2) if the distillation operation can improve to such an extent, then the innovation points claimed in this paper may not be reliable and more gains come from the distillation operation. The task of this paper is to obtain a better cross-resolution representation, and I think it may be more intuitive to visualize it using t-SNE.
==================================================

Focused review:

1. Some more ablations would be nice. The authors could for example investigate the impact of removing for the second stage (multi source scenario) the sounding area (li stage) as well as sounding object location (si). 2. Some implementation details are missing. E.g. how long do the authors train the first stage vs the second stage? More details would be better for reproducibility.

Review Point: 1. Some more ablations would be nice. The authors could for example investigate the impact of removing for the second stage (multi source scenario) the sounding area (li stage) as well as sounding object location (si).
Review Point: 2. Some implementation details are missing. E.g. how long do the authors train the first stage vs the second stage? More details would be better for reproducibility.
==================================================

Focused review:

The paper claims that they derive a new notion of variance which helps explain why RR performs better than SGD from the beginning. I find this discussion a bit confusing (see point 1 below). Also, is it possible to compute this analytically for some simple problem settings and show that it is indeed smaller as compared to the original notion of variance. A few questions for the authors: 1) The authors mention on line 189 that which algorithm is faster boils down to which variance is smaller and they discuss how it can be smaller when the batch size is large. As mentioned in proposition 1, \sigma^2_shuffle is lower bounded by a term. If I assume the condition number to be 1 and put step size gamma=1/L, we get \sigma^2_shuffle \geq n\sigma^*/8 where n is the number of functions. Hence, it seems like \sigma^2_shuffle is much bigger than \sigma^* unless the step size is very small even if the batch size is some constant. Can the authors clarify this? 2) I think it would be useful to write the SGD convergence rate as in equation 4 to explicitly see when RR is better than SGD. In particular, looking at lines 173 and 188, it is hard to know which is better unless we know the step sizes schedules are used in each of the algorithms. 3) For figure 2, how are the values \sigma^2_shuffle and \sigma^* estimated? And, what are the values of L and \mu that are used? -------------------------------------------------------------------------------- After reading the author response and other reviewers' comments, I have increased the rating since my concerns have been sufficiently addressed. The techniques in this paper are also interesting/novel as noted by other reviewers which I had overlooked before.

Review Point: 1) The authors mention on line 189 that which algorithm is faster boils down to which variance is smaller and they discuss how it can be smaller when the batch size is large. As mentioned in proposition 1, \sigma^2_shuffle is lower bounded by a term. If I assume the condition number to be 1 and put step size gamma=1/L, we get \sigma^2_shuffle \geq n\sigma^*/8 where n is the number of functions. Hence, it seems like \sigma^2_shuffle is much bigger than \sigma^* unless the step size is very small even if the batch size is some constant. Can the authors clarify this?
Review Point: 2) I think it would be useful to write the SGD convergence rate as in equation 4 to explicitly see when RR is better than SGD. In particular, looking at lines 173 and 188, it is hard to know which is better unless we know the step sizes schedules are used in each of the algorithms.
Review Point: 3) For figure 2, how are the values \sigma^2_shuffle and \sigma^* estimated? And, what are the values of L and \mu that are used? -------------------------------------------------------------------------------- After reading the author response and other reviewers' comments, I have increased the rating since my concerns have been sufficiently addressed. The techniques in this paper are also interesting/novel as noted by other reviewers which I had overlooked before.
==================================================

Focused review:

[Update after Author Response] Thanks for the author response which has mitigated some minor concerns of mine about specific details of implementation. The only unresolved concern I still have is about the effectivity of co-tuning for simple binary classification tasks in NLP like sentiment analysis, NLI, etc which are widely prevalent in NLP. However, this is a minor concern, and the strong results on the CV tasks and the NER task with a seemingly simple approach is prompting me to increase my score to 7. I would suggest the authors: 1) To dial down the tone of the "uniform applicability of their approach to both CV and NLP tasks" in a future version of their paper as currently, based on the evaluation, I am of the opinion that co-tuning works better in practice for image classification tasks than NLP tasks. 2) To include the references I pointed out in the "Relation to prior work" section for a well rounded related work section. ----------------------------- While the methodology and evaluation conducted shows performance improvements for CV tasks, I have some concerns with regards to the evaluation and applicability to NLP tasks. I am unsure about the authors claim that their technique is applicable to any different domain and hence am assigning this conservative score. I am willing to increase my score if the authors can clarify my concerns: 1) More often than not, since the target task is from a specific domain (say news classification), the probability distribution of the source labels given a target label will be confined to a small overlapping subset of source classes. If multiple target classes have very similar source class distributions, then it is not intuitively clear as to why co-tuning should improve over vanilla-FT. Furthermore, it is not clear if co-tuning is useful when the number of classes for the target task is considerably more than that for the source task? 2) The most common use-case for FT pre-trained transformer models for NLP tasks is for classification tasks like sentiment analysis, NLI, answer sentence selection, etc which are inherently binary classification tasks. Without experiments, a-priori it is unclear if co-tuning might be beneficial for these tasks with so few target classes. 3) The BERT model has been pre-trained with 2 different tasks: masked language modeling (where the target labels are over the vocabulary of size ~30k) and next sentence prediction (where the target label space is discrete: True or False). The authors have only used the MLM task as the source task where the # of classes is close to 30k. If NSP is used as the source task, I am uncertain about any sizable improvements being obtained due to co-tuning. If the authors wanted to consider on a model only pre-trained via the MLM task, they should have used RoBERTa[1] instead of BERT. [1] RoBERTa: A Robustly Optimized BERT Pre-training Approach, Liu et al, 2019

Review Point: 1) To dial down the tone of the "uniform applicability of their approach to both CV and NLP tasks" in a future version of their paper as currently, based on the evaluation, I am of the opinion that co-tuning works better in practice for image classification tasks than NLP tasks.
Review Point: 2) To include the references I pointed out in the "Relation to prior work" section for a well rounded related work section. ----------------------------- While the methodology and evaluation conducted shows performance improvements for CV tasks, I have some concerns with regards to the evaluation and applicability to NLP tasks. I am unsure about the authors claim that their technique is applicable to any different domain and hence am assigning this conservative score. I am willing to increase my score if the authors can clarify my concerns:
Review Point: 1) More often than not, since the target task is from a specific domain (say news classification), the probability distribution of the source labels given a target label will be confined to a small overlapping subset of source classes. If multiple target classes have very similar source class distributions, then it is not intuitively clear as to why co-tuning should improve over vanilla-FT. Furthermore, it is not clear if co-tuning is useful when the number of classes for the target task is considerably more than that for the source task?
Review Point: 2) The most common use-case for FT pre-trained transformer models for NLP tasks is for classification tasks like sentiment analysis, NLI, answer sentence selection, etc which are inherently binary classification tasks. Without experiments, a-priori it is unclear if co-tuning might be beneficial for these tasks with so few target classes.
Review Point: 3) The BERT model has been pre-trained with 2 different tasks: masked language modeling (where the target labels are over the vocabulary of size ~30k) and next sentence prediction (where the target label space is discrete: True or False). The authors have only used the MLM task as the source task where the # of classes is close to 30k. If NSP is used as the source task, I am uncertain about any sizable improvements being obtained due to co-tuning. If the authors wanted to consider on a model only pre-trained via the MLM task, they should have used RoBERTa[1] instead of BERT. [1] RoBERTa: A Robustly Optimized BERT Pre-training Approach, Liu et al, 2019
==================================================

Focused review:

- Some of the evaluation procedures and measures used (e.g. SSIM) seem ad-hoc. Each of the type of bug could and should be investigated more carefully and thoroughly, potentially in a separate paper. - Purely empirical study, lack of deeper technical insights.

Review Point: - Some of the evaluation procedures and measures used (e.g. SSIM) seem ad-hoc. Each of the type of bug could and should be investigated more carefully and thoroughly, potentially in a separate paper.
==================================================

Focused review:

Weaknesses]
Instruction they construct may have somewhat unrealistic assumptions: 1) all attributes are known to be user before searching, 2) preferences are defined before searching the items, and 3) even those conditions are not changed during browsing the web shop.
Including ablation study on the effect of using image feature would be needed.

Review Point: 1) all attributes are known to be user before searching,
Review Point: 3) even those conditions are not changed during browsing the web shop. Including ablation study on the effect of using image feature would be needed.
==================================================

Focused review:

* The main issue, to this reviewer, is that the datasets chosen seem somewhat simplistic in their explanations. This shows in some of the example explanations (in the supplemental) like "The word "best" directly precedes the term." -- which function on the word level, and thus are probably pretty simple for a model to learn (and use as an internal code when forcing the label to be recoverable by the predictor). To this reviewer, it would be more exciting to compare with a dataset such as e-SNLI [3] or Cos-E [4], since those seem to require more complex reasoning. * Slight: it would be helpful to see the dependence on the number of retrieved explanations (set to 10 everywhere). === Update: thanks for the response! I read through the other reviewers' responses and the rebuttal. Adding in e-SNLI (or some more interesting datasets) would greatly improve the paper to this reviewer, and I also advise following the suggestions raised by R4. I still recommend acceptance though.

Review Point: * The main issue, to this reviewer, is that the datasets chosen seem somewhat simplistic in their explanations. This shows in some of the example explanations (in the supplemental) like "The word "best" directly precedes the term." -- which function on the word level, and thus are probably pretty simple for a model to learn (and use as an internal code when forcing the label to be recoverable by the predictor). To this reviewer, it would be more exciting to compare with a dataset such as e-SNLI [3] or Cos-E [4], since those seem to require more complex reasoning.
Review Point: * Slight: it would be helpful to see the dependence on the number of retrieved explanations (set to 10 everywhere). === Update: thanks for the response! I read through the other reviewers' responses and the rebuttal. Adding in e-SNLI (or some more interesting datasets) would greatly improve the paper to this reviewer, and I also advise following the suggestions raised by R4. I still recommend acceptance though.
==================================================

Focused review:

weaknesses, listed below, that should not be difficult to fix in the final version of the paper. - in the comparison with the prior work, the authors do not present the latest results. Therefore, the comparison embellishes the contributions of this paper. Indeed, in table 1, in the row corresponding to the Langevin diffusion, the authors should present the result of "Analysis of Langevin Monte Carlo via convex optimization", which improves on [13,17]. In addition, for the Underdamped Langevin Diffusion, in the table,in the abstract and in the text, it would be more appropriate to compare with [15], which provides an improved bound (kappa^1.5 instead of kappa^2). - The authors do not keep track of numerical constants. I would very much appreciate to see results with precise values of constants, as it is done in many recent references cited by the authors. SPECIFIC REMARKS/TYPOS - Line 9: compare with [15] instead of [10] - Line 14: applies -> apply - Line 16: problems -> problem - Lines 32-33: this lines oversell the contribution (which is not needed). If one uses the methodology of evaluating a method adopted by the authors, that I find fully justified, the current best algorithm is independent of d. By the way, the sentence on lines 32-33 contradicts the content of table 1. - Lines 44-45: when mentioning ``the current fastest algorithm'', it might be helpful to cite [15] as well. - Lines 106-107 - same remark - Line 131: depend -> depends - Line 132: I guess Omega(d^1.5) should be replaced by Omega(d^2) - Line 136: I suggest to use another letter than f, which refers to a precise function (the potential) - Lines 170-171: [18,15] -> [18,13] - Lines 180-181: the meaning of "accurate" is not clear here. My suggestion is to remove any comment about accuracy here and to keep only the comment on unbiasedness. - Line 191: please emphasize that the distribution is Gaussian conditionnally to alpha. - Lemma 2: it should be clearly mentioned that these claims are true when u=1/L - Lemma 2: I suggest to replace v_n(0) and x_n(0) by v_n and x_n - Line 201: same remark - Section A: everything is conditional to alpha - Lemma 6: mention that u=1/L - Line 400: Schwarz is mispelled

Review Point: - in the comparison with the prior work, the authors do not present the latest results. Therefore, the comparison embellishes the contributions of this paper. Indeed, in table 1, in the row corresponding to the Langevin diffusion, the authors should present the result of "Analysis of Langevin Monte Carlo via convex optimization", which improves on [13,17]. In addition, for the Underdamped Langevin Diffusion, in the table,in the abstract and in the text, it would be more appropriate to compare with [15], which provides an improved bound (kappa^1.5 instead of kappa^2).
Review Point: - Lines 44-45: when mentioning ``the current fastest algorithm'', it might be helpful to cite [15] as well.
Review Point: - Lines 106-107 - same remark - Line 131: depend -> depends - Line 132: I guess Omega(d^1.5) should be replaced by Omega(d^2) - Line 136: I suggest to use another letter than f, which refers to a precise function (the potential) - Lines 170-171: [18,15] -> [18,13] - Lines 180-181: the meaning of "accurate" is not clear here. My suggestion is to remove any comment about accuracy here and to keep only the comment on unbiasedness.
Review Point: - Line 191: please emphasize that the distribution is Gaussian conditionnally to alpha.
Review Point: - Lemma 2: it should be clearly mentioned that these claims are true when u=1/L - Lemma 2: I suggest to replace v_n(0) and x_n(0) by v_n and x_n - Line 201: same remark - Section A: everything is conditional to alpha - Lemma 6: mention that u=1/L - Line 400: Schwarz is mispelled
==================================================

Focused review:

Weaknesses:
In the related work, I would like to see more comparisons to how active IRL relates to the standard IRL problem. For example, what is the relationship between Active IRL methods and other IRL methods such as Apprenticeship IRL [4] or MaxEnt IRL [3]? Furthermore, in contrast to the claim on line 63, many recent IRL algorithms that use function approximation do not assume the underlying transition model is known, such as the f-IRL method [2]. Are these methods not applicable to the active IRL setting?
How does Theorem 1 differ from Theorem 3.1 of [1] since both characterize the impact of error propagation from learned generative models?
[1] A. M. Metelli et al. Provably efficient learning of transferable rewards. In Proceedings of International Conference on Machine Learning (ICML), 2021. [2] Ni, Tianwei, et al. "f-irl: Inverse reinforcement learning via state marginal matching." arXiv preprint arXiv:2011.04709 (2020). [3] Ziebart, Brian D., et al. "Maximum entropy inverse reinforcement learning." Aaai. Vol. 8. 2008. [4] Abbeel, Pieter, et al. "Apprenticeship learning via inverse reinforcement learning." Proceedings of the twenty-first international conference on Machine learning. 2004.
The paper discusses that the method is limited to discrete state and action spaces, and future work will extend it to continuous states and actions. The paper also states the efficiency limitations of iteratively solving convex optimization problems in the algorithm. Potential negative social impacts are also discussed.

Review Point: 8. 2008. [4] Abbeel, Pieter, et al. "Apprenticeship learning via inverse reinforcement learning." Proceedings of the twenty-first international conference on Machine learning. 2004. The paper discusses that the method is limited to discrete state and action spaces, and future work will extend it to continuous states and actions. The paper also states the efficiency limitations of iteratively solving convex optimization problems in the algorithm. Potential negative social impacts are also discussed.
==================================================

Focused review:

Weaknesses] 1. The minimum patch size can be one to be added in Table 4, which can be the comparison in the per-pixel setting. 2. The experiment about time cost can be added to verify the superiority of the PAR. 3. This method is to reduce the size of the adversarial noise, but in practical applications, reducing the number of queries is a more important goal that needs to be optimized.

Review Point: 1. The minimum patch size can be one to be added in Table 4, which can be the comparison in the per-pixel setting.
Review Point: 2. The experiment about time cost can be added to verify the superiority of the PAR.
Review Point: 3. This method is to reduce the size of the adversarial noise, but in practical applications, reducing the number of queries is a more important goal that needs to be optimized.
==================================================

Focused review:

Weakness:
[2*] I think that this study should be viewed as an RF-version of (Chen et al., 2021). The authors should clearly state this perspective in Section 1. For example, communication strategy has also proposed in (Chen et al., 2021), but it sounds like the authors considered the strategy newly in the current style of writing. I cannot find a sufficient (for ICLR) novelty from (Chen et al., 2021).
[3] Some results of Table 2 overlap. It can be simplified.
[4] I do not know the notation manner of 2 [ − 2 : 0 : 5 : 5 ] and 2 [ − 13 : 2 : − 3 ]
. Please define it. Question:
Regarding the following points, it may be that the authors' description is appropriate and there is no problem, just because I have not understood it correctly. I do not reflect these points in my current recommendation score. Depending on the authors' response, I may change my recommendation score.
[5] Almost all of the theorems in this paper are similar to those of (Chen et al., 2021). Are there any difficulties in theoretical analysis associated with application of RF?
As noted in Remark 1, the authors provide a sharper convergence rate than in that of (Chen et al., 2021). It would be better to clearly describe the reason of this improvement (is it a miss of (Chen et al., 2021) or merit of RF?).
[6*] I cannot prove the statement "Note that, the gradient ~ all ( x i , y i ) , ( x k , y k ) ∈ D
." located between (6) and (7). Is this statement correct? Pairwise learning and distributed learning may have bad compatibility: In distributed learning, one cannot calculate the cross term between an object depending on ( x i , y i ) ∈ D 1
and an object depending on ( x j , y j ) ∈ D 2
of original pairwise objective. I think that the statement is incorrect.
[7] Why did the authors change the experimental setting (e.g., data generating distribution in Section 6.2, and p
in Section 6.3) from (Chen et al., 2021)? If the proposed methods did not work well with the previous settings, it would be better to conduct experiments with the previous settings and the changed settings, and consider and describe the reasons why the proposed methods work or do not work well.
[8*] Even for a random prediction f , R ( f )
is 0.5. I am not satisfied with all the experimental results in Table 2 in Section 6.3. I fear that the LSRank (and DLSRank) on which this study is based may not be good. Thus, I want the authors to additionally try other type methods such as OLS (and ordinal regression or standard classification methods since y i
of MovieLens are discrete).
Results of this paper are similar to those of (Chen et al., 2021), but I think that the experiment of (Chen et al., 2021) also may not be sufficient (see [6*]).
[9*] This comment relates to [8*]. What is the advantage of LSRank compared with ordinary least squares (OLS) regression? I am interested in the relationship between LSRank and OLS since their optimal predictors are the same conditional mean. I would like the authors to make an experimental comparison at least. Additionally, if there are advantages, it would be better to mention them in the paper. I think distributed OLS is more easy.

Review Point: 0 :5 :5 ] and 2 [ − 13 :
==================================================

Focused review:

1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure "unlabelled" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived. To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels. 2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1. 3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples? 4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT. 5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
6. It is claimed in lines 128-132 that "it would be beneficial for the LD to be trained with examples containing events". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training.
1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043.
2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method.
3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT.
4. In equation (4), L2 distance is used. In OT, earth mover's distance is more common. What is the benefit of L2 distance?
5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern.

Review Point: 1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure "unlabelled" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived. To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels.
Review Point: 2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1.
Review Point: 3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples?
Review Point: 4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT.
Review Point: 5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
Review Point: 6. It is claimed in lines 128-132 that "it would be beneficial for the LD to be trained with examples containing events". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
Review Point: 7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training.
Review Point: 2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method.
Review Point: 3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT.
Review Point: 4. In equation (4), L2 distance is used. In OT, earth mover's distance is more common. What is the benefit of L2 distance?
Review Point: 5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern.
==================================================

Focused review:

weaknesses of the paper: [Strengths]
The problem is relevant.
Good ablation study.
[Weaknesses] - The statement in the intro about bottom up methods is not necessarily true (Line 28). Bottom-up methods do have a receptive fields that can infer from all the information in the scene and can still predict invisible keypoints. - Several parts of the methodology are not clear. - PPG outputs a complete pose relative to every part’s center. Thus O_{up} should contain the offset for every keypoint with respect to the center of the upper part. In Eq.2 of the supplementary material, it seems that O_{up} is trained to output the offset for the keypoints that are not farther than a distance \textit{r) to the center of corresponding part. How are the groundtruths actually built? If it is the latter, how can the network parts responsible for each part predict all the keypoints of the pose. - Line 179, what did the authors mean by saying that the fully connected layers predict the ground-truth in addition to the offsets? - Is \delta P_{j} a single offset for the center of that part or it contains distinct offsets for every keypoint? - In Section 3.3, how is G built using the human skeleton? It is better to describe the size and elements of G. Also, add the dimensions of G,X, and W to better understand what DGCN is doing. - Experiment can be improved: - For instance, the bottom-up method [9] has reported results on crowdpose dataset outperforming all methods in Table 4 with a ResNet-50 (including the paper one). It will be nice to include it in the tables - It will be nice to evaluate the performance of their method on the standard MS coco dataset to see if there is a drop in performance in easy (non occluded) settings. - No study of inference time. Since this is a pose estimation method that is direct and does not require detection or keypoint grouping, it is worth to compare its inference speed to previous top-down and bottom-up pose estimation method. - Can we visualize G, the dynamic graph, as it changes through DGCN? It might give an insight on what the network used to predict keypoints, especially the invisible ones.
[Minor comments]
In Algorithm 1 line 8 in Suppl Material, did the authors mean Eq 11 instead of Eq.4?
Fig1 and Fig2 in supplementary are the same
Spelling Mistake line 93: It it requires…
What does ‘… updated as model parameters’ mean in line 176
Do the authors mean Equation 7 in line 212?
The authors have talked about limitations in Section 5 and have mentioned that there are not negative societal impacts.

Review Point: - PPG outputs a complete pose relative to every part’s center. Thus O_{up} should contain the offset for every keypoint with respect to the center of the upper part. In Eq.2 of the supplementary material, it seems that O_{up} is trained to output the offset for the keypoints that are not farther than a distance \textit{r) to the center of corresponding part. How are the groundtruths actually built? If it is the latter, how can the network parts responsible for each part predict all the keypoints of the pose.
Review Point: - Line 179, what did the authors mean by saying that the fully connected layers predict the ground-truth in addition to the offsets?
Review Point: - Is \delta P_{j} a single offset for the center of that part or it contains distinct offsets for every keypoint?
Review Point: - In Section 3.3, how is G built using the human skeleton? It is better to describe the size and elements of G. Also, add the dimensions of G,X, and W to better understand what DGCN is doing.
Review Point: - For instance, the bottom-up method [9] has reported results on crowdpose dataset outperforming all methods in Table 4 with a ResNet-50 (including the paper one). It will be nice to include it in the tables - It will be nice to evaluate the performance of their method on the standard MS coco dataset to see if there is a drop in performance in easy (non occluded) settings.
Review Point: - No study of inference time. Since this is a pose estimation method that is direct and does not require detection or keypoint grouping, it is worth to compare its inference speed to previous top-down and bottom-up pose estimation method.
Review Point: - Can we visualize G, the dynamic graph, as it changes through DGCN? It might give an insight on what the network used to predict keypoints, especially the invisible ones. [Minor comments] In Algorithm 1 line 8 in Suppl Material, did the authors mean Eq 11 instead of Eq.4? Fig1 and Fig2 in supplementary are the same Spelling Mistake line 93: It it requires… What does ‘… updated as model parameters’ mean in line 176 Do the authors mean Equation 7 in line 212? The authors have talked about limitations in Section 5 and have mentioned that there are not negative societal impacts.
==================================================

Focused review:

Weaknesses
Prior work has already studied the claimed contributions.
Poor comparison with the literature on accessing privacy risks.
Weak evaluations.
Detailed Comments
The idea of evaluating the risk of membership inference under data poisoning attacks is interesting. As more and more data is collected from various sources, the privacy risks of machine learning models trained on such data is an important topic.
1. Contributions were shown by the prior work
However, data poisoning for increasing privacy risks has already been initially studied by Mahloujifar et al. [1], and all the contributions (claimed from Line 41 to Line 52) have already been shown by Tramer et al. [2]. Moreover, the paper uses the techniques and tools for measuring the membership inference risks already known as meaningless by Carlini et al. [3]. Thus, I believe this paper is largely detached from the state-of-the-art privacy studies, and unfortunately, the contributions are the repetition of what we have known so far.
[1] Mahloujifar, et al., Property Inference from Poisoning, IEEE Security and Privacy, 2022. [2] Tramer et al., Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets, Preprint, 2022. [3] Carlini et al., Membership Inference Attacks From First Principles, IEEE Security and Privacy, 2022.
Note: The studies I mentioned had appeared 3-12 months before the NeurIPS submission deadline and were even accepted before then, so I wouldn’t review this paper as concurrent work.
2. Poor comparison to the prior work
My second concern is that the paper just combines two threat models (data poisoning and membership inference attacks) while it largely ignores important research questions in the community, such as:
RQ 1. Why does this poisoning work? RQ 1. What are the training samples that become more vulnerable after poisoning? RQ 2. What does it mean to increase the AUC? RQ 3. Why are clean-label poisoning attacks important? (As the paper mentioned in the introduction, sanitizing the training data is not feasible.) RQ 4. If someone wants to mitigate this attack, what can this person do?
which (those questions) are partially already answered in the prior work [2, 3].
3. Weak Evaluation
My last concern is that there is unclear interpretation of the results in the evaluation section:
Q1. (Line 257) I am unclear why the clean-label poisoning attack can be considered an “approximate” version of the dirty-label poisoning attack in the feature space?
As shown in visualization (Figure 5), it seems that clean-label attacks and dirty-label attacks cause a completely different impact on the models. If this is true, wouldn’t it make more sense in Sec 3 to present a single attack with different objectives?
Q2. (Line 261) I am also unclear how this paper measures the distributional differences between D_train and D_shadow. I believe it’s still a hard question to quantify the distributional differences and actively studied in domain adaption and robustness, so I don’t think we can compare.
Q3. (Line 284) I am a bit confused about the fine-tuning scenario. Is it the case where we take an ImageNet pre-trained model and fine-tune it on CIFAR10? Then why don’t the attacker make membership inference on ImageNet instead of attacking CIFAR10? Isn’t it easier to spot poisoning samples if we inject them into the training data for fine-tuning?
Q4. (Line 304) I am unclear about the connection between the presented attacks and adversarial training. Adversarial training crafts adversarial examples in each training iteration and update the model parameters, while this attack just injects a set of static poisons into the training data and trains on it.
My lesser concern is that the paper conducts offensive research, contaminating the training data for increasing privacy risks but does not discuss any ethical concerns when a miscreant uses this attack. At least, I think it's okay to talk about some potential defense mechanisms, but the paper just concludes that defenses are future work.

Review Point: 1. Contributions were shown by the prior work However, data poisoning for increasing privacy risks has already been initially studied by Mahloujifar et al. [1], and all the contributions (claimed from Line 41 to Line 52) have already been shown by Tramer et al. [2]. Moreover, the paper uses the techniques and tools for measuring the membership inference risks already known as meaningless by Carlini et al. [3]. Thus, I believe this paper is largely detached from the state-of-the-art privacy studies, and unfortunately, the contributions are the repetition of what we have known so far. [1] Mahloujifar, et al., Property Inference from Poisoning, IEEE Security and Privacy, 2022. [2] Tramer et al., Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets, Preprint, 2022. [3] Carlini et al., Membership Inference Attacks From First Principles, IEEE Security and Privacy, 2022. Note: The studies I mentioned had appeared 3-12 months before the NeurIPS submission deadline and were even accepted before then, so I wouldn’t review this paper as concurrent work.
Review Point: 2. Poor comparison to the prior work My second concern is that the paper just combines two threat models (data poisoning and membership inference attacks) while it largely ignores important research questions in the community, such as: RQ 1. Why does this poisoning work? RQ 1. What are the training samples that become more vulnerable after poisoning? RQ 2. What does it mean to increase the AUC? RQ 3. Why are clean-label poisoning attacks important? (As the paper mentioned in the introduction, sanitizing the training data is not feasible.) RQ 4. If someone wants to mitigate this attack, what can this person do? which (those questions) are partially already answered in the prior work [2, 3].
Review Point: 3. Weak Evaluation My last concern is that there is unclear interpretation of the results in the evaluation section:
==================================================

Focused review:

Weaknesses:
• Not clear what the algorithmic contribution is; the methods provided in section 2 and algorithm1 seem to be already existent.
• Recently developed EBM approaches have been effectively applied for HEP event detection but no new machine learning approaches have been proposed
• Ablation experiments to understand the effect of the effect of various components and especially the transformer network is crucial to understand the model performance
• How are the hyperparameters for the learning chosen?
• Do the results hold with multiple random initializations of the model?

Review Point: • Not clear what the algorithmic contribution is; the methods provided in section 2 and algorithm1 seem to be already existent.
Review Point: • Recently developed EBM approaches have been effectively applied for HEP event detection but no new machine learning approaches have been proposed • Ablation experiments to understand the effect of the effect of various components and especially the transformer network is crucial to understand the model performance • How are the hyperparameters for the learning chosen?
Review Point: • Do the results hold with multiple random initializations of the model?
==================================================

Focused review:

weaknesses of the approach, the proposed message passing scheme, relying on the hierarchical representation, does not seem very principled. It would be nice to know more precisely what has inspired the authors to make these design choices. Perhaps as a consequence, in qualitative results show sometimes objects seem to rip appart or smoothly deform into independent pieces. Qualitatively, some results are impressive, while some do not conform to our expectations of what would happen; cf my previous comment on unexpected deformations of objects, but also some motions: cubes moving slightly on the ground although they should be still, etc. More importantly, the applicability of the method is questionable, as it requires ground truth for the local and global deltas of each particle for training. Furthermore, even for inference, it would require the initial states of the particles. This would be a huge challenge in computer vision applications, and in any other applications relying on real videos, rather than synthetic videos. This greatly limits the applicability of the proposed representation, and as a consequence of the proposed architecture. Therefore I have doubts at this point that the proposed architecture has "the potential to form the basis of next generation physics predictors for use in computer vision, robotics, and quantitative cognitive science". To claim this, the authors should be able to show preliminary results of how their method could be used, even in very simple, but realistic settings. Nevertheless I believe that it is worthwhile sharing these results with the community, as well as the code for the method and the environment, as this is an important problem, and the authors attack it with an ambitious and novel (although slightly ad hoc) approach. To be fully confident that this paper should be accepted, I would like the authors to clarify the following points: - it is not clear how the quantitative results are obtained: what data exactly is used for training, validating and testing ? - ternary input to the phi network, encoding the type of pairwise relationship -> is this effective ? What happens otherwise, if this is not given, or if weights are not shared ? Also wouldn't it make sense to have categorical input in the form of a one hot vector instead ? - the algorithm described for the creation of the graph G_H is inconsistent with the description of the remaining edges (l.122 to 1.131). According to the algorithm, there should only be edges between siblings, from the root to the leaves and back, and from the root to all intermediate nodes. Maybe this is due to l.127: "each new node taking the place for its child leaves" should be "each new node taking the place of the root" ? This would then also be more consistent with Algorithm 1. Please clarify. In Figure 3b) it is not clear what the red links represent, if the edges are supposed to be directed. - I don't understand the process for learning the r_ij. If it can be different for each pair of particle (or even for each source particle i), how can it be learned using the losses described in 4.3 ? We can obtain their values on the objects of the training set, but then for each new object, we need to gather synthetic videos of the object moving, in order to learn its properties? Wouldn't it be simpler to have fixed parameters ? Furthermore, if the stiffness of the material is to be learned, why not learn the mass as well ? - in the algorithm described for the creation of the graph G_H, it is said that one should "add edges between cluster nodes if the clusters are connected by edges between leaves" - how is there a presence or an absence of an edge in the first place ? Nitpicking: - are humans really able to predict subtle deformations of masses at different scales ? Or can they simply perceive them and say whether they are plausible or not ?(l.39-44) Typos: - par(i) should be par(p) l.128 - Figure 3: "is constraint" should be "is constrained" - l.152; l should be p_l ? - l 155 repetition with the following: "Effects are summed together" -------------------------------- Final decision after rebuttal: The authors have addressed all of my concerns, except for one which I share with R3 that the experimental protocol for the quantitative results is not described. Overall however I think that the paper presents an ambitious and novel method to a very important problem, which they convincingly test in a challenging experimental set up. I agree with R3 that the qualitative evaluation should include the weaknesses in the main paper and I think it is beneficial that the authors have agreed to do so. I remain convinced that it would be worth sharing these results with the community. I can only urge the authors to respect their commitment to share the code and environment, and to add the missing details about the experimental protocol, to ease further research in this area and comparison to their work. I stand by my initial decision that the paper should be accepted as the approach is a very well executed stepping stone for others to improve on in the extremely challenging setting proposed by the authors.

Review Point: - it is not clear how the quantitative results are obtained: what data exactly is used for training, validating and testing ?
Review Point: - ternary input to the phi network, encoding the type of pairwise relationship -> is this effective ? What happens otherwise, if this is not given, or if weights are not shared ? Also wouldn't it make sense to have categorical input in the form of a one hot vector instead ?
Review Point: - the algorithm described for the creation of the graph G_H is inconsistent with the description of the remaining edges (l.122 to 1.131). According to the algorithm, there should only be edges between siblings, from the root to the leaves and back, and from the root to all intermediate nodes. Maybe this is due to l.127: "each new node taking the place for its child leaves" should be "each new node taking the place of the root" ? This would then also be more consistent with Algorithm 1. Please clarify. In Figure 3b) it is not clear what the red links represent, if the edges are supposed to be directed.
Review Point: - I don't understand the process for learning the r_ij. If it can be different for each pair of particle (or even for each source particle i), how can it be learned using the losses described in 4.3 ? We can obtain their values on the objects of the training set, but then for each new object, we need to gather synthetic videos of the object moving, in order to learn its properties? Wouldn't it be simpler to have fixed parameters ? Furthermore, if the stiffness of the material is to be learned, why not learn the mass as well ?
Review Point: - in the algorithm described for the creation of the graph G_H, it is said that one should "add edges between cluster nodes if the clusters are connected by edges between leaves" - how is there a presence or an absence of an edge in the first place ? Nitpicking:
Review Point: - par(i) should be par(p) l.128 - Figure 3: "is constraint" should be "is constrained" - l.152; l should be p_l ?
Review Point: - l 155 repetition with the following: "Effects are summed together" -------------------------------- Final decision after rebuttal: The authors have addressed all of my concerns, except for one which I share with R3 that the experimental protocol for the quantitative results is not described. Overall however I think that the paper presents an ambitious and novel method to a very important problem, which they convincingly test in a challenging experimental set up. I agree with R3 that the qualitative evaluation should include the weaknesses in the main paper and I think it is beneficial that the authors have agreed to do so. I remain convinced that it would be worth sharing these results with the community. I can only urge the authors to respect their commitment to share the code and environment, and to add the missing details about the experimental protocol, to ease further research in this area and comparison to their work. I stand by my initial decision that the paper should be accepted as the approach is a very well executed stepping stone for others to improve on in the extremely challenging setting proposed by the authors.
==================================================

Focused review:

weaknesses: 1. When the authors say `white box attacks`, I assume this means that the adversary can see the full network with the final layers, every network in the ensemble, every rotation used by networks in the ensemble. I would like them to confirm this is correct. 2. Did the authors study numbers of bits in logits helps against a larger epsilon in the PGD attack? Because intuition suggests that having a 32 bit logit should improve robustness against a more powerful adversary. This experiment isn't absolutely necessary, but does strengthen the paper. 3. Did the authors study the same approach on Cifar? It seems like this approach should be readily applicable there as well. ---Edit after rebuttal--- I am updating my score to 8. The improved experiments on Cifar10 make a convincing argument for your method.

Review Point: 1. When the authors say `white box attacks`, I assume this means that the adversary can see the full network with the final layers, every network in the ensemble, every rotation used by networks in the ensemble. I would like them to confirm this is correct.
Review Point: 2. Did the authors study numbers of bits in logits helps against a larger epsilon in the PGD attack? Because intuition suggests that having a 32 bit logit should improve robustness against a more powerful adversary. This experiment isn't absolutely necessary, but does strengthen the paper.
Review Point: 3. Did the authors study the same approach on Cifar? It seems like this approach should be readily applicable there as well. ---Edit after rebuttal--- I am updating my score to 8. The improved experiments on Cifar10 make a convincing argument for your method.
==================================================

Focused review:

This might be a bit unfair, but a serious concern of mine is the task itself. Barring any further information on the evaluation protocol, most notably on the *number of induced tags allowed for the system*, one can think of a trivial system that outperforms any of the ones presented by almost 10 points on PTB - just learn a tag for each vocabulary item. The authors offer no analysis of their system's actual outputs, of which "many" correlate with which "one" in the evaluation scheme, and although they use a second metric that contains a component that mitigates this problem, they leave this detail out and send the reader to the appendix to understand what the metric does (this is completely unacceptable. If you're using a metric, especially if you're arguing that it's better than the other one, at least explain the virtues of it along general lines in the main text).
The multilingual evaluation claims to be of "the UD data", a dataset of over 100 languages, yet the authors only select ten without explaining why. The availability of ELMo embeddings, in a resource tucked away in the appendix as well, is for about 45 languages and the intersection is significantly more than 10.
Section 3 (the Methods) is very difficult to follow. Almost every subsection selectively gets rid of part of the notation without warning, and inconsistencies abound. For example, in Figure 3, are h_i vectors or lists of vectors (with different layers) and if the latter, at what part are they aggregated? What's the input and output of the minus operand (it has two outgoing arrows?); Is W^s the same as W^{hourglass}? Is it a new parameter? Why is the (unnumbered) equation before line 427 referring to equation (8) as replacing it but taking different inputs (forward and backward h's as opposed to h's from different indices, which by the way may or may not be indicating layers or words, since the variable "i" is reserved for neither). Using vague terminology like "purified representations" (l.253) doesn't help this overall messiness, making the system very hard to understand.
In equation (12), there's a dependence on the vocabulary. How do OOV words get generated under this formulation?
Do you fine-tune the ELMo parameters?
Other stuff: - The last sentence of the first paragraph is unfinished.
- l.64 an-->a - l.294 spareness-->sparseness - l.328 I'm pretty sure you meant "supervised" rather than "unsupervised", these are the standard splits for *all* WSJ tasks, not limited to dependency parsing either.

Review Point: - The last sentence of the first paragraph is unfinished.
Review Point: - l.64 an-->a - l.294 spareness-->sparseness - l.328 I'm pretty sure you meant "supervised" rather than "unsupervised", these are the standard splits for *all* WSJ tasks, not limited to dependency parsing either.
==================================================

Focused review:

- The paper is not self-contained. Lines 133/144: the paper shuld be self contained and no explicit reference to the supplementary material shall be find in the text. The author have used the supplementary material for section 6 and 7 which describes essential components of the method: loss function, curve proposals, curve selection. - My main concern is the relevance for the NeuRIPS community which is usually more focused on machine learning than computer graphics / computational geometry. The interest for the geometry commmunity is strong. However the machine learning side seems not to be the objective of the paper: close curve loss is described in the supplementary material. The parameter $\tau_c$ and $\tau_e$ are not really discussed.

Review Point: - The paper is not self-contained. Lines 133/144: the paper shuld be self contained and no explicit reference to the supplementary material shall be find in the text. The author have used the supplementary material for section 6 and 7 which describes essential components of the method: loss function, curve proposals, curve selection.
Review Point: - My main concern is the relevance for the NeuRIPS community which is usually more focused on machine learning than computer graphics / computational geometry. The interest for the geometry commmunity is strong. However the machine learning side seems not to be the objective of the paper: close curve loss is described in the supplementary material. The parameter $\tau_c$ and $\tau_e$ are not really discussed.
==================================================

Focused review:

- Table 1 does not contain any sort of error bar its values, which makes it hard to quantify the robustness of these findings. While the authors mainly rely on the average of multiple different models to draw their final conclusions, it still would have been great to see whether any of the individual models are consistently more susceptible to the generated samples than others. The same is true for the facial verification results in Figure 5. - Unclear whether results hold beyond the L_inf norm. The approach's performance degrades sharply when comparing results from the main paper (eps=0.1) with results from the supplementary (eps=0.08)

Review Point: - Table 1 does not contain any sort of error bar its values, which makes it hard to quantify the robustness of these findings. While the authors mainly rely on the average of multiple different models to draw their final conclusions, it still would have been great to see whether any of the individual models are consistently more susceptible to the generated samples than others. The same is true for the facial verification results in Figure 5.
Review Point: - Unclear whether results hold beyond the L_inf norm. The approach's performance degrades sharply when comparing results from the main paper (eps=0.1) with results from the supplementary (eps=0.08)
==================================================

Focused review:

Weakness: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world. 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling. Detailed Comments: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.

Review Point: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world.
Review Point: 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling. Detailed Comments:
Review Point: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.
==================================================

Focused review:

Weaknesses: Strengths: 1. The paper is well organized and structured. 2. The advantage and usefulness of the proposed approach is well justified, and demonstrated with experiments. Weaknesses: 1. Some details of the proposed approach are not mentioned or clarified. a) Which task is used for the test in Fig. 3? This should be noted in the caption. b) In Fig. 5, are the transferred LIP winning tickets obtained using single-IMP or multi-IMP? is the test image used in finding the transferred LIP winning tickets? c) The algorithms in the Appendices are not clarified. How is the mask mu' created (the sparsity of mu' is not updated in each iteration)? The objective function for training is not given. How is the j determined? Is the algorithm intended to only find the desired subnet or it also gives the final restored image? 2. Some statements in the paper are incorrect. In section 4, the authors say “Results presented in Fig. 4 show that multi-image IMP significantly can improve the quality of the LIPs in the cross-domain setting.” However, from Fig. 4, the multi-IMP shows essentially the same performance as others. 3. Supporting results are missing for some of the statements in the paper. To enable transferability across images, the authors propose a multi-IMP method to improve on the basic single-IMP method, and show results for the multi-IMP method in Fig. 4. However, the single-IMP might also have this transferability though not as strong. A comparison is necessary. 4. Some part of the paper might not be relevant to the main topic. The paper discusses the transferability of the LIP to high-level tasks like image classification. However, image classification requires training on massive dataset and is thus not relevant to the DIP (untrained networks) topic.

Review Point: 2. The advantage and usefulness of the proposed approach is well justified, and demonstrated with experiments. Weaknesses:
Review Point: 1. Some details of the proposed approach are not mentioned or clarified. a) Which task is used for the test in Fig. 3? This should be noted in the caption. b) In Fig. 5, are the transferred LIP winning tickets obtained using single-IMP or multi-IMP? is the test image used in finding the transferred LIP winning tickets? c) The algorithms in the Appendices are not clarified. How is the mask mu' created (the sparsity of mu' is not updated in each iteration)? The objective function for training is not given. How is the j determined? Is the algorithm intended to only find the desired subnet or it also gives the final restored image?
Review Point: 2. Some statements in the paper are incorrect. In section 4, the authors say “Results presented in Fig. 4 show that multi-image IMP significantly can improve the quality of the LIPs in the cross-domain setting.” However, from Fig. 4, the multi-IMP shows essentially the same performance as others.
Review Point: 3. Supporting results are missing for some of the statements in the paper. To enable transferability across images, the authors propose a multi-IMP method to improve on the basic single-IMP method, and show results for the multi-IMP method in Fig.
Review Point: 4. However, the single-IMP might also have this transferability though not as strong. A comparison is necessary.
Review Point: 4. Some part of the paper might not be relevant to the main topic. The paper discusses the transferability of the LIP to high-level tasks like image classification. However, image classification requires training on massive dataset and is thus not relevant to the DIP (untrained networks) topic.
==================================================

Focused review:

Weaknesses:
It is mentioned that “all subsets are fed to the same encoder to get their corresponding latent representation” (L65-66) and that “we don’t change the relative order of features in a subset since standard neural network architectures are not permutation invariant” (L80-81). It is unclear how this happens - do the authors mask out features of other subsets (e.g. by setting them to 0)? If that’s the case, on what basis do they claim that SubTab allows to “use smaller models by reducing input dimension, making it less prone to overfitting“ (L53)? Or do they reuse the same input neurons for different input features?
The claim that SubTab can “do training and inference in the presence of missing features by ignoring corresponding subsets” (L52-53) is not well supported in the paper. Can the authors demonstrate this? While SubTab might work well in some scenarios (e.g. when the variables are missing completely at random; MCAR), in many real-world scenarios the MCAR assumption does not hold. For example, for single-cell RNA-seq, it is well known that dropout patterns are informative of cell-type (e.g. see https://www.nature.com/articles/s41467-020-14976-9) and thus ignoring this information can result in degraded performance.
How does the proposed approach compare to a plain autoencoder with the same encoder/decoder architectures? How does it compare to self-supervised TabNet? (see https://arxiv.org/abs/1908.07442)
Minor comments/questions:
“We shuffle the order of subsets in every batch during training to avoid introducing any unintentional bias“ (L79-80). How exactly would the order introduce a bias? The forward pass of SubTab appears to be invariant to subset permutations.
Equations 4 and 5 might be incorrect and can potentially be simplified. 1) The indices a and b are undefined - don’t the authors loop over the subset indices a and b? (similar for Equation 6), 2) isn’t the overall contrastive loss computed by averaging the individual positive (pairs of subsets within the same sample) and negative (pairs of subsets within different samples) losses? Equation 4 seems to imply that the loss is computed only on contiguous samples within the batch (in which case the order of subsets might indeed result in an unintentional bias).
How did the authors optimise the hyperparameters of the baselines? To avoid biases in the comparison, all methods should be optimised in the same way.
Results for some baseline methods are taken from Yoon et al. “Extending the success of self-and semi-supervised learning to tabular domain”. Did the authors ensure that the train/validation/test splits are the same?
The untrained model achieves better than random performances. How did the authors initialise the weights? How sensitive is the initialisation? It would be helpful to provide the standard error in Figure 5 (and all figures in general).
Intuitively, it seems that SubTab should work well with highly redundant datasets where several sets of features are informative of the underlying latent variables (e.g. MNIST or TCGA). How does the model perform when this is not the case? Do the claims from L49-L53 still hold?
The authors adequately addressed the limitations and potential negative social impact of their work.

Review Point: 1) The indices a and b are undefined - don’t the authors loop over the subset indices a and b? (similar for Equation 6),
Review Point: 2) isn’t the overall contrastive loss computed by averaging the individual positive (pairs of subsets within the same sample) and negative (pairs of subsets within different samples) losses? Equation 4 seems to imply that the loss is computed only on contiguous samples within the batch (in which case the order of subsets might indeed result in an unintentional bias). How did the authors optimise the hyperparameters of the baselines? To avoid biases in the comparison, all methods should be optimised in the same way. Results for some baseline methods are taken from Yoon et al. “Extending the success of self-and semi-supervised learning to tabular domain”. Did the authors ensure that the train/validation/test splits are the same? The untrained model achieves better than random performances. How did the authors initialise the weights? How sensitive is the initialisation? It would be helpful to provide the standard error in Figure 5 (and all figures in general). Intuitively, it seems that SubTab should work well with highly redundant datasets where several sets of features are informative of the underlying latent variables (e.g. MNIST or TCGA). How does the model perform when this is not the case? Do the claims from L49-L53 still hold? The authors adequately addressed the limitations and potential negative social impact of their work.
==================================================

Focused review:

Weakness
1 The novelty is limited. The low-rank design is relevant to (Hu et al., 2021). The sparse design is similar to Taylor pruning (Molchanov et al., 2019).
2 The experiments are not quite convincing. The authors choose the old baseline like R3D and C3D. To reduce computation complexity, many papers have been proposed in 3D CNN (X3D, SlowFast, etc). Does the proposed method also works on these 3D CNNs? Or compared to these approaches, what is the advantage of the proposed method?
3 The paper is hard to follow. In fact, I have to read many times to get what it is. I understand it is a theoretical-kind paper. But please further explain the mathmetical formulation clearly to show why and how it works.

Review Point: 1 The novelty is limited. The low-rank design is relevant to (Hu et al., 2021). The sparse design is similar to Taylor pruning (Molchanov et al., 2019).
Review Point: 2 The experiments are not quite convincing. The authors choose the old baseline like R3D and C3D. To reduce computation complexity, many papers have been proposed in 3D CNN (X3D, SlowFast, etc). Does the proposed method also works on these 3D CNNs? Or compared to these approaches, what is the advantage of the proposed method?
Review Point: 3 The paper is hard to follow. In fact, I have to read many times to get what it is. I understand it is a theoretical-kind paper. But please further explain the mathmetical formulation clearly to show why and how it works.
==================================================

Focused review:

1) While the proposed idea is novel, the overall proposed method seems to be the adaptation from the previous works [1,2], thus seemingly incremental. In addition,thorough explanation of energy based models (EBMs) is not sufficient. Preliminary explanation of EBMs is necessary. 2) There is no qualitative comparison between this work and previous works, such as generative adversarial network. It is better to shown qualitative comparison between EBMs and other generative models related to your work. Moreover, in looking at the qualitative result on the proposed method, the generated images seem to be not promising, showing blurry and not detailed facial images. [1] G. E. Hinton. Products of experts. International Conference on Artificial Neural Networks, 1999. [2] Y. Du and I. Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint 318 arXiv:1903.08689, 2019. [3] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of 374 the 28th International Conference on Machine Learning (ICML-11), pages 681–688, 2011.

Review Point: 1) While the proposed idea is novel, the overall proposed method seems to be the adaptation from the previous works [1,2], thus seemingly incremental. In addition,thorough explanation of energy based models (EBMs) is not sufficient. Preliminary explanation of EBMs is necessary.
Review Point: 2) There is no qualitative comparison between this work and previous works, such as generative adversarial network. It is better to shown qualitative comparison between EBMs and other generative models related to your work. Moreover, in looking at the qualitative result on the proposed method, the generated images seem to be not promising, showing blurry and not detailed facial images. [1] G. E. Hinton. Products of experts. International Conference on Artificial Neural Networks, 1999. [2] Y. Du and I. Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint 318 arXiv:1903.08689, 2019. [3] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of 374 the 28th International Conference on Machine Learning (ICML-11), pages 681–688, 2011.
==================================================

Focused review:

- It would be nice if there was some general statement connecting all three settings. What property does the problem need for these sorts of algorithms to work? - Is there any sense in which you can show that the tradeoffs you find between competitive ratio for different values of eta are tight? (I suspect this might be very hard).

Review Point: - It would be nice if there was some general statement connecting all three settings. What property does the problem need for these sorts of algorithms to work?
Review Point: - Is there any sense in which you can show that the tradeoffs you find between competitive ratio for different values of eta are tight? (I suspect this might be very hard).
==================================================

Focused review:

- The paper is not well written and organized. Therefore, it is not clear about its contribution. - The conditions may be not practical. - Hard to provide any meaningful evidence to show the real benefits for the machine learning problems.

Review Point: - The paper is not well written and organized. Therefore, it is not clear about its contribution.
Review Point: - Hard to provide any meaningful evidence to show the real benefits for the machine learning problems.
==================================================

Focused review:

I think there is a major technical mistake in the paper. To prove their main result, the authors rely on an inequality presented after line 259 which says that the expected zero-one *test* error is at most the expected *training* error. This should not be correct because the test error is usually higher than the training error. There is no generalization analysis in the paper, e.g., using Rademacher complexity as in Ji and Telgarsky (2019). The proof of this inequality is given after line 561 in the supplementary, but there is no explanation and it seems incorrect. Am I missing something? Another issue with the paper is that it seems that the guarantees hold for the initialization and only "worsen" with the gradient updates. Therefore, if we consider the algorithm which initializes W_1 and then satisfies W_t = W_1 for all t, it has the same guarantees as SGD. Is this correct? Where in the proofs does SGD have an advantage over the latter algorithm? If it does not, then this is a clear limitation of the result and it does not explain the benefits of training with SGD. In the discussion, the authors claim that following Nagarajan et al., implicit bias may not explain the generalization of SGD. I think that this statement is incorrect. Nagarajan et al. show that uniform convergence bounds may not explain generalization, this is not the same. The implicit bias of SGD implies good generalization (because this is what we see in practice), and it may be possible to explain it and derive generalization guarantees by other methods (not uniform convergence bounds). Other minor comments: 1. I think it would be good to emphasize the differences between the results on SGD with dropout and SGD without dropout. We should expect better guarantees for dropout because it generalizes better in practice. Is this the case in this paper? 2. In Page 8 it is not clear which Lemma is used to prove which Lemma. This should be written more clearly. 3. In Algorithm 1, what is p? 4. Typos: throughout the paper miss-classification should be misclassification. Line 122: "min" is missing. Line 271: ">0" is missing.

Review Point: 1. I think it would be good to emphasize the differences between the results on SGD with dropout and SGD without dropout. We should expect better guarantees for dropout because it generalizes better in practice. Is this the case in this paper?
Review Point: 2. In Page 8 it is not clear which Lemma is used to prove which Lemma. This should be written more clearly.
==================================================

Focused review:

1. As the paper’s main proposal is to use wavelet transforms to obtain a natural hierarchy of normalizing flows, it is surprising to me that the authors have not shown an exploration on the effect of the different possible choices for the wavelets. The influence of different wavelet choices on the sample quality and quantitative performance seems like an important topic to investigate, as wavelet transforms for normalizing flows form such a central part of the novelty of this paper. 2. One of the main claims of the authors is that the proposed method can be trained up to 15 times faster than other flow methods like Glow. Although it is likely that the parallel training of the flow components can decrease the training time, the comparison between training times presented in the paper has some issues. The authors only report the total number of GPU hours required to train their method to convergence on their hardware, and compare this to *estimates* of the total training time (in GPU hours) of Glow by looking at log files and through discussions on github. This seems problematic for several reasons. If the number of epochs used for training in the methods is not the same, then even if the estimates are correct, the numbers are hard to compare. A more informative comparison would be training time per epoch, or the time required for a single forward pass (with and without a parameter update). Since Glow’s code is publicly available, and the authors have used part of Glow in their own code, running such a comparison *on the same hardware* should not pose a problem, especially for the lower resolution images. Furthermore, the authors state that for the LSUN bedroom dataset, wavelet flows are 7.5 times faster than Glow, but wavelet flows also perform worse than Glow in terms of bpd. If you would train Glow to the same performance level as Wavelet flows, what would the difference in training time then be? Finally, the authors claim that Wavelet flows have a 15x speedup in training time for CelebA at a 256x256 resolution, but for this resolution of CelebA no results are shown in terms of bits-per-dimension for wavelet flows, so it is not clear if there is a trade off between training time and quantitative performance on this dataset. 3. A significant portion of the paper is devoted to sampling from the unnormalized annealed density with MCMC, with the authors arguing that samples from flow models are better at lower temperatures. As stated in the experiment section, the authors pick a temperature T=0.97 for the annealing temperature, which is very close to T=1 for which no MCMC is required and one can sample exactly from the flow model. This makes me wonder if running the MCMC procedure, which can also misbehave in high-D, is worth it when one can sample exactly at T=1.

Review Point: 1. As the paper’s main proposal is to use wavelet transforms to obtain a natural hierarchy of normalizing flows, it is surprising to me that the authors have not shown an exploration on the effect of the different possible choices for the wavelets. The influence of different wavelet choices on the sample quality and quantitative performance seems like an important topic to investigate, as wavelet transforms for normalizing flows form such a central part of the novelty of this paper.
Review Point: 3. A significant portion of the paper is devoted to sampling from the unnormalized annealed density with MCMC, with the authors arguing that samples from flow models are better at lower temperatures. As stated in the experiment section, the authors pick a temperature T=0.97 for the annealing temperature, which is very close to T=1 for which no MCMC is required and one can sample exactly from the flow model. This makes me wonder if running the MCMC procedure, which can also misbehave in high-D, is worth it when one can sample exactly at T=1.
==================================================

Focused review:

1. Related work on curriculum learning is missing. 2. I found section 4.4. that discusses the proposed curriculum algorithm pretty difficult to read (see more details below). 3. The current version of the curriculum algorithm seems to have some scalability limitations.

Review Point: 2. I found section 4.4. that discusses the proposed curriculum algorithm pretty difficult to read (see more details below).
Review Point: 3. The current version of the curriculum algorithm seems to have some scalability limitations.
==================================================

Focused review:

Weaknesses
Though simple yet effective, the proposed method is not well explained or motivated. What essential qualities does OneRing encourage beneficial for open-set recognition? Why is it necessary to re-weight the entropy minimization term for known and unknown samples? The presentation in the methodology part (Section 3.2- 3.3) seems to plainly describe the steps of the proposed method. Analysis and ablation experiments in Section 4 also only show the efficacy and good results of the method without either intuitive understanding or theoretical analysis.
Missing technical details: 1). What is the batch size of different tasks? How do weights in Eq. 2 change during the training? Because these directly affect the batch-level weighted entropy minimization objective. 2). Why did OneRing underperform other open-set recognition methods on the PACS benchmark which has the least category number? 3). Why did the method fail to converge on DomainNet, as was mentioned in Section 4.2?
All domain adaptation experiments in the submission belong to the open-partial setting. How does the method perform on open-set domain adaptation, since Table 1 also covers this setting?
Typo: "target date" in Table 1 caption.
Missing related works: 1). UMAD: Universal Model Adaptation under Domain and Category Shift, arXiv:2112.08553 2). Open-set Hypothesis Transfer with Semantic Consistency. IEEE Transactions on Image Processing
Post-rebuttal Rating
I have read all reviews and responses and discussed them with the authors. The authors did not address my concern about the overclaimed contributions, misleading problem settings, and the reliability of the experiments. Although the authors replied that they have already revised the paper to address the above concerns, I find the up-to-date revision still has remaining problems. For example, in the abstract, UNDA was mentioned without any explanation. Throughout the whole context, there are still mixed uses of OPDA and UNDA, which is confusing. Besides, the authors still cannot ensure the correctness and fairness of experimental comparisons.
From my perspective, this submission requires careful revision to avoid misleading claims and unreliable experimental results before being accepted to a top venue. As a result, I would not recommend a paper without a careful check on claims and experimental results. I will decrease my final score to 5.

Review Point: 1). What is the batch size of different tasks? How do weights in Eq. 2 change during the training? Because these directly affect the batch-level weighted entropy minimization objective.
Review Point: 2). Why did OneRing underperform other open-set recognition methods on the PACS benchmark which has the least category number?
Review Point: 1). UMAD: Universal Model Adaptation under Domain and Category Shift, arXiv:2112.08553
Review Point: 2). Open-set Hypothesis Transfer with Semantic Consistency. IEEE Transactions on Image Processing Post-rebuttal Rating I have read all reviews and responses and discussed them with the authors. The authors did not address my concern about the overclaimed contributions, misleading problem settings, and the reliability of the experiments. Although the authors replied that they have already revised the paper to address the above concerns, I find the up-to-date revision still has remaining problems. For example, in the abstract, UNDA was mentioned without any explanation. Throughout the whole context, there are still mixed uses of OPDA and UNDA, which is confusing. Besides, the authors still cannot ensure the correctness and fairness of experimental comparisons. From my perspective, this submission requires careful revision to avoid misleading claims and unreliable experimental results before being accepted to a top venue. As a result, I would not recommend a paper without a careful check on claims and experimental results. I will decrease my final score to 5.
==================================================

Focused review:

- The performance of the method is still sub-optimal on large scale datasets, and hence there is a scope of improvement. - The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.

Review Point: - The performance of the method is still sub-optimal on large scale datasets, and hence there is a scope of improvement.
Review Point: - The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.
==================================================

Focused review:

Weaknesses:
Environments chosen: My biggest critique points target the experiment section. The authors, rightfully, claim that focusing on environments entailing long-term memory tasks makes sense in their case. However, I believe this also should give rise to a larger set of long-term memory task environments and notably, with a higher degree in variety. For example, the authors could have used goal-conditioned tasks from the OpenAI Gym Robotics suite, Atari or gridworld environments such as four-rooms (2d object collection environment proposed in Barreto et al., 2017). By moving the experiment section “short-term memory tasks in DMC and Atari” to the appendix, the authors would be able to free space for, what I believe, would be more relevant experiments. I agree that 5.5 is a sanity check and such it could be very well placed in the appendix. Without a larger variety in the set of evaluated environments, I believe the capabilities of coping with long-term memory tasks are insufficiently addressed. Moreover, can the authors please elaborate how the subset of DMC environments was chosen? It seems to me that for a proper comparison and sanity check, one should choose an almost identical set or a close subset of the DMC environments shown in the Dreamer paper.
Footnote 1: I do not understand the reasoning behind the claim that the exploration problem is not addressed in this work. Can the authors please elaborate on this because I believe the Hidden Order Discovery tasks is very much an exploration problem. Moreover, I see world models as a way to cope well with suboptimal exploration capabilities in agents. Frankly, why should we not use world models for addressing hard exploration problems? Also, the way Footnote 1 is written reads that TransDreamers would not be able to cope well with exploration. In summary, I believe the argument given in Footnote 1 is weak and even motivates an experiment where the proxy task (world) resembles a difficult problem due to hard exploration. Can the authors please also elaborate on how the difficulty to learn a world model with TransDreamer would scale with an increased need of exploration in the world/proxy task? I believe that in the Hidden Order Discovery task we did not see the TransDreamer break when going from 4 to 6 balls, indicating that the sensitivity of the chosen environment is too low to show scalability from easy to very hard exploration and long-term memory task problem.
Minor points:
Parameter count comparison: Can the authors please compare the number of parameters between the Dreamer and TransDreamer in the paper?
Figure 3: why is there no standard deviation for the red curve in the top right (2D 6-Ball) subfigure?
Table 2: can you please point out explicitly what the numbers 60, 70 and 80 in Table 2 a) and b) correspond to? During my first read it did not become clear to me.
Code: Can the authors please provide a link to an anonymous code repository that contains runnable scripts to reproduce some of the results, e.g. Figure 5 or Table 2)?
Presentation of results: I believe the work could be better presented/visualized by showing videos (gifs) of the imagined trajectories on a simple google template website.

Review Point: 2)? Presentation of results: I believe the work could be better presented/visualized by showing videos (gifs) of the imagined trajectories on a simple google template website.
==================================================

Focused review:

Weakness]
1 In my opinion, this paper's novelty is not strong, e.g., sub-models should be diverse, sub-models are adversarially robust, and MIMO training strategy.
2 Sections 3.1 and 3.2 is not clearly written. E.g., what does V in equation 5 mean?
3 Compared with MSD (Maini et at.), the proposal MAT has marginal improvements against multiple attacks. [Questions]
1 Could authors illustrate more about Eq. 6 and Eq. 7. From the current writing, I do not understand cross gradient in detail.
2 A.2 PSEUDO CODE, where does penalty of cross gradient apply?

Review Point: 1 In my opinion, this paper's novelty is not strong, e.g., sub-models should be diverse, sub-models are adversarially robust, and MIMO training strategy.
Review Point: 2 Sections 3.1 and 3.2 is not clearly written. E.g., what does V in equation 5 mean?
Review Point: 3 Compared with MSD (Maini et at.), the proposal MAT has marginal improvements against multiple attacks. [Questions] 1 Could authors illustrate more about Eq. 6 and Eq.
Review Point: 7. From the current writing, I do not understand cross gradient in detail.
Review Point: 2 A.2 PSEUDO CODE, where does penalty of cross gradient apply?
==================================================

Focused review:

Weakness
The authors revealed their identity (affiliation) in the code snippets of appendix. It might violate the double-blind rule of ICLR.
The novelty of this paper is limited. Packing is not a new idea and it has been widely used in the official tensor2tensor library and achieved good results: https://github.com/tensorflow/tensor2tensor/blob/3f12173b19c1bad2a7c37eb390f3ad46baee0c19/tensor2tensor/data_generators/ops/pack_sequences_ops.cc. So this can be a useful trick but the contribution might be not significant enough to publish at ICLR.
The scenario discussed in this paper is too restricted. For example, 1) it only discuss the wikipedia dataset (that’s how the number 50% comes), but there are a lot more datasets; 2) it only works for BERT training, but there are quite a few other important tasks, such as language modeling (GPT-3). All the numbers reported in this paper are based on this setting, making its generalization capability questionable.
The experiment section only shows the training loss of pretraining, but never talks about the downstream fine-tuning. Then how do you conclude that the performance is little affected? After all, the accuracies of downstream applications is the final metric.
The paper is a bit hard to read for the following reasons: 1) The contents are not self-contained in the main text — quite a few important contents are deferred to appendix that one cannot easily follow the ideas in the main text; 2) The paragraphs are usually lengthy and verbose — they can be as long as 30 lines! 3) There are quite a few typos, e.g. “For achieve this”, “¡CLS¿” etc.

Review Point: 1) it only discuss the wikipedia dataset (that’s how the number 50% comes), but there are a lot more datasets;
Review Point: 2) it only works for BERT training, but there are quite a few other important tasks, such as language modeling (GPT-3). All the numbers reported in this paper are based on this setting, making its generalization capability questionable. The experiment section only shows the training loss of pretraining, but never talks about the downstream fine-tuning. Then how do you conclude that the performance is little affected? After all, the accuracies of downstream applications is the final metric. The paper is a bit hard to read for the following reasons:
Review Point: 1) The contents are not self-contained in the main text — quite a few important contents are deferred to appendix that one cannot easily follow the ideas in the main text;
Review Point: 2) The paragraphs are usually lengthy and verbose — they can be as long as 30 lines!
==================================================

Focused review:

Weaknesses:
The writing can be improved as it causes difficulty even for experienced readers. Examples include but not limit to 1) Last column in Table 1 should refer to Theorem 7 rather than Theorem 6; 2) Using r
to denote the risk for minimization problems and primal risk for minimax problem at the same time is confusing; 3) Overall, the paper is very dense and somehow lack of organization, especially Section 4. For example, Lemma 2 and Proposition 1 can be combined. Lemma 4 is underfull box.
The necessity of proposing the "primal gap" can be checked and I think it is an artifact of writing the paper. In Line 220, we see the relation between the generalization error of the primal gap and primal risk is the extra error between the empirical minimax learner and the population one. This term is of course requiring attention since the final target is always to study the primal population risk defined in (2), but it is also algorithm-independent. The analyses of GDA and GDmax still focus on the generalization error of the primal risk. Overall, I think the main contribution should be Lemma 1 and Theorem 7 in the Appendix, and the title might be a little exaggerated.
Despite it being the first work establishing generalization bounds for ϵ
-stable algorithms without strong concavity, the dependence on ϵ
is not as good as before. While it can be argued that the assumption is weaker, it still draws back the primal population risk and primal dual population risk. For example, in the (non)convex minimization counterpart, such dependence is known to be ϵ
[Hardt et al., 2016]. Furthermore, when the empirical risk is considered together in realization of Assumption 5 to study the population risk, it seems provide worse trade-offs.

Review Point: 1) Last column in Table 1 should refer to Theorem 7 rather than Theorem 6;
Review Point: 2) Using r to denote the risk for minimization problems and primal risk for minimax problem at the same time is confusing;
==================================================

Focused review:

1. While the proposed method could make change to the predicted token, whether it make an improvement is questionable. During training, let the i-th position see the (i+1)th position is an information leak, because the token at (i+1)th position is exactly the target. The training process do not teach the model to do any correction. 2. Lack of a comparison to show refinement during inference improves the generation quality. It's good to see the statistic of the number of refinements, but this only approves the change instead of an improvement.
1. In Table 2, the ablation experiment has a setting only add "local Constraint" but without "Refinement Mask". How could it make a change on the prediction result? As the input (includes the tokens can be attended) is the same for every prediction at a position.

Review Point: 1. While the proposed method could make change to the predicted token, whether it make an improvement is questionable. During training, let the i-th position see the (i+1)th position is an information leak, because the token at (i+1)th position is exactly the target. The training process do not teach the model to do any correction.
Review Point: 2. Lack of a comparison to show refinement during inference improves the generation quality. It's good to see the statistic of the number of refinements, but this only approves the change instead of an improvement.
Review Point: 1. In Table 2, the ablation experiment has a setting only add "local Constraint" but without "Refinement Mask". How could it make a change on the prediction result? As the input (includes the tokens can be attended) is the same for every prediction at a position.
==================================================

Focused review:

Weaknesses:
Overall, the datasets considered are fairly uncluttered and simplistic. The video examples do not highlight the capabilities of DLH to handle many objects at many different speeds (e.g., in crowded urban scenes). The data also does not showcase what happens when the background is not stationary and there are moving objects. I would recommend considering a more complicated, dynamic dataset, for example, from the autonomous driving setting (i.e., Waymo Open Dataset [1], NuScenes [2], or KITTI [3]). The simplicity of the toy Moving Ball dataset is also underscored by the results of Table 2, where the full capacity of the hierarchical model is not necessary to model the data. Although it is great to see that the model can dynamically adapt to use less of the latent space when the underlying data distribution is simpler, it would be compelling to see how the full latent space would be used in a more complex setting.
Although there is a discussion on the importance of stochasticity, this capability is only explored in a toy-dataset setting with random color changes. No discussion of multimodality in the distribution is included. Could DHL handle multimodal outputs (e.g., multiple equally valid possibilities for the future)? This would again be relevant to more complex video datasets (e.g., in the urban setting), where given observations of a person walking straight, they could choose to continue walking straight or turn in the future.
It would be helpful in Fig. 4 or in an appendix to show the outputs of the baseline approaches for comparison.
In Fig. 5, it is not entirely clear that levels 1 and 2 for the KTH Action dataset are disentangled.
It would be interesting to benchmark against a deterministic video prediction method in Table 1 to see if the considered datasets are sufficiently stochastic to warrant modeling of stochasticity.
Is there a way to report a measure of statistic significance of the proposed method's metric performance over the baselines in Table 1?
In Fig. 6, it is not very clear to me what is wrong with some of the highlighted frames output by the CW-VAE. Is the issue that the ball reduces in size for those frames? Why were the other baseline results not shown?
Further explanation for the values in Table 3 would be helpful.
[1] Sun, Pei, et al. "Scalability in perception for autonomous driving: Waymo open dataset." CVPR, 2020.
[2] Caesar, Holger, et al. "nuScenes: A multimodal dataset for autonomous driving." CVPR, 2020.
[3] Geiger, Andreas, et al. "Vision meets robotics: The KITTI dataset." IJRR, 2013.
Some typos and minor points of confusion are listed below:
I am not sure I fully followed the diagrams in Fig. 2. Are there temporal indices missing from the states?
Are there hierarchical levels and ψ
parameters missing in the 'Estimating' paragraphs in Sec. 2.2 and similarly dropped indexes in p ( e ∣ s )
in the paragraph before Eq. 3?
In Eq. 3, Λ
is not defined.
Missing period at the end of Eq. 3.
In Sec. 2.3, I am having some trouble understanding the notation. Should q ( e n + 1 ∣ e n = 0 ) = 0 be q ( e n + 1 = 1 ∣ e n = 0 ) = 0 ?
It would be helpful to derive Eq. 8 from Eq. 7 in the appendix for completeness.
Fig. 3 was referenced much earlier than it appears.
Am I correct in understanding that at the first hierarchical level e 1 = 1 always?
In Fig. 4, it should be made clear whether the 30 past context frames are included in the visualization or only the 100 predicted frames are shown.
In Sec. 3, temporal abstraction paragraph, the sentence "Temporal abstraction models ..." has a grammatical typo, is a bit long, and is missing a period at the end.
Missing periods at the end of table captions. Unclear what the * symbol signifies.
In Sec. 4.3, I did not fully understand what it means that "DLH learns transition between progressively slower features in the higher levels of its hierarchy". Does this mean that minor variations in the scene are faster features than location changes of the view?
The references should be proofread (e.g., to ensure the year is not entered twice in a citation, the conference venue is listed instead of ArXiv when available, the confererence name formatting is consistent, etc.).

Review Point: 2. Are there temporal indices missing from the states? Are there hierarchical levels and ψ parameters missing in the 'Estimating' paragraphs in Sec. 2.2 and similarly dropped indexes in p ( e ∣ s ) in the paragraph before Eq. 3? In Eq. 3, Λ is not defined. Missing period at the end of Eq.
==================================================

Focused review:

While debatable, my biggest concern with the paper, or this line of work in general, is if the direction is really worth it.
Let me explain myself.
The paper begins with the famous "king, man, queen, women" example and motivates the work by saying the proper next word following the "After debating whether to bow to the woman or the king first, the jester decided on the" context should be "woman" and "king", instead of "queen" and "king".
This "golden answer" is already questionable.
The model learned from other contexts that "queen" could be a candidate here and in some sense, "queen" is not such a bad guess for who the "woman" is.
In other words, the geometrical structure of words is probably what we want the model to learn in the first place.
Jumping out from the example and we could discuss the issue in a broader sense.
The "softmax bottleneck" originates from the inner dimension of the dot product, i.e. the hidden dimension, being much smaller than the vocabulary size.
If we really want to model arbitrary output distributions, then why not just define a fixed set of orthogonal word vectors and let the hidden states do the magic?
The limitation of the memory size restricts us to using a hidden dimension size smaller than the vocabulary size, and this results in the low rank of the log probability matrix, and this results in a series of complicated methods that try to overcome the bottleneck.
If one looks at the numbers, is 15.64 vs 15.89 PPL (GPT-2 Medium in Table 1) really worth the effort?
I know this point is quite subjective and it is definitely not personal against the authors.
I simply want to raise a bit of concern with the direction we are going in.
The authors did a good job in providing a unified view of the softmax enhancement methods (including theirs of course), and it is the direction in which the methods move that I have a doubt about.
Maybe expand on the hyperparameter choices a bit more.
How should one decide on the number of softmaxes, input hidden states and partitions?
How did you do it?
L166, theory.
L280, there exists... L351, why 0.4 epochs?
Which 40% of the training data is used?
How is it decided?
L425, the reference to Tab.2 in the main text happens after the reference to Tab. 3.
The following papers could be nice additions to the reference: https://arxiv.org/abs/1412.6623 https://arxiv.org/abs/1704.08424 https://arxiv.org/abs/1705.08039 https://arxiv.org/abs/1806.04313 https://arxiv.org/abs/1910.12554

Review Point: 3. The following papers could be nice additions to the reference: https://arxiv.org/abs/1412.6623 https://arxiv.org/abs/1704.08424 https://arxiv.org/abs/1705.08039 https://arxiv.org/abs/1806.04313 https://arxiv.org/abs/1910.12554
==================================================

Focused review:

1. According to me the simulations results clearly show that there exists a trade-off. It seems the proposed methodology achieves speed-up, many a time, by loosing ground in terms of generalization. Hence it would be very interesting to know if some knobs/hyper-parameters can be varied to achieve various trade-off points. For e.g., a time vs accuracy kind of plot for various hyper-parameters would be more illustrative of the merit of the proposal. 2. In section4, when solutions from the various partitions are merged, normalization issues may occur. For e.g., the scores may not be comparable across partitions. Discussion regarding this seems to be completely missing. Minor: 3. The idea in section 4 is ok, but I think it is all about using the label correlation information rather than any hierarchical information. So simply naming it so would be ideal and infact more generic as correlations may occur even when there is no hierarchy. Also, is there an automatic way to figure out when there may not exist well-defined partitions in the labels (as per correlations)? Currently, this seems to be done via manual inspection.

Review Point: 1. According to me the simulations results clearly show that there exists a trade-off. It seems the proposed methodology achieves speed-up, many a time, by loosing ground in terms of generalization. Hence it would be very interesting to know if some knobs/hyper-parameters can be varied to achieve various trade-off points. For e.g., a time vs accuracy kind of plot for various hyper-parameters would be more illustrative of the merit of the proposal.
Review Point: 2. In section4, when solutions from the various partitions are merged, normalization issues may occur. For e.g., the scores may not be comparable across partitions. Discussion regarding this seems to be completely missing. Minor:
Review Point: 3. The idea in section 4 is ok, but I think it is all about using the label correlation information rather than any hierarchical information. So simply naming it so would be ideal and infact more generic as correlations may occur even when there is no hierarchy. Also, is there an automatic way to figure out when there may not exist well-defined partitions in the labels (as per correlations)? Currently, this seems to be done via manual inspection.
==================================================

Focused review:

Weaknesses: 1. The authors run their network S times repeatedly and collect S latent regions as suggestions each iteration. They set S to 2 to achieve a good balance between accuracy and efficiency. More details are missing to support this choice. 2. Several important references are missing. For example, 1) Ning Xu, Brian Price, Scott Cohen, Thomas Huang, Deep Image Matting. CVPR 2017. the authors are encouraged to add comparisons. 2) Liang Yang, Xiaochun Cao, Di Jin, Xiao Wang, Dan Meng: A Unified Semi-Supervised Community Detection Framework Using Latent Space Graph Regularization. IEEE Trans. Cybernetics 45(11): 2585-2598 (2015) where a similar "active" must-link priors are utilized. 3) Chuan Wang, Hua Zhang, Liang Yang, Xiaochun Cao, Hongkai Xiong: Multiple Semantic Matching on Augmented N-Partite Graph for Object Co-Segmentation. IEEE Trans. Image Processing 26(12): 5825-5839 (2017) where co-matting is addressed. 3. In my point of view, the most important significance of this paper is that is can help establishing large-scale matting dataset, which is rarely mentioned in this paper.

Review Point: 1. The authors run their network S times repeatedly and collect S latent regions as suggestions each iteration. They set S to 2 to achieve a good balance between accuracy and efficiency. More details are missing to support this choice.
Review Point: 1) Ning Xu, Brian Price, Scott Cohen, Thomas Huang, Deep Image Matting. CVPR 2017. the authors are encouraged to add comparisons.
Review Point: 2) Liang Yang, Xiaochun Cao, Di Jin, Xiao Wang, Dan Meng: A Unified Semi-Supervised Community Detection Framework Using Latent Space Graph Regularization. IEEE Trans. Cybernetics 45(11): 2585-2598 (2015) where a similar "active" must-link priors are utilized.
Review Point: 3) Chuan Wang, Hua Zhang, Liang Yang, Xiaochun Cao, Hongkai Xiong: Multiple Semantic Matching on Augmented N-Partite Graph for Object Co-Segmentation. IEEE Trans. Image Processing 26(12): 5825-5839 (2017) where co-matting is addressed.
Review Point: 3. In my point of view, the most important significance of this paper is that is can help establishing large-scale matting dataset, which is rarely mentioned in this paper.
==================================================

Focused review:

The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.

Review Point: - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant.
Review Point: - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments.
Review Point: - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted.
Review Point: - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper.
Review Point: - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together.
Review Point: - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once.
Review Point: - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent?
Review Point: - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.
==================================================

Focused review:

1. Data preparation. As the authors pointed out, data serve a very important role in the whole work. However, the authors did not describe clearly how a) training images are rendered b) query points are sampled during training c) normalizations are applied for 2D and 3D data. Are they the same as PiFu? In implicit function network e.g. PiFu and DeepSDF, both b) and c) are extremely important and could greatly affect the quality of results. 2. Study of global feature. Methods like PiFu purposely avoid using voxel-like feature because of their high computational and memory cost. What is the resolution of the 3D voxel, and does it introduce unnecessary overhead to the whole network? It would be more convincing to study the importance of the global feature in Sec4.2 by comparing with different resolutions of voxel features. Notice when the resolution is reduced to 1x1x1, this is actually the case of using a single global feature.

Review Point: 1. Data preparation. As the authors pointed out, data serve a very important role in the whole work. However, the authors did not describe clearly how a) training images are rendered b) query points are sampled during training c) normalizations are applied for 2D and 3D data. Are they the same as PiFu? In implicit function network e.g. PiFu and DeepSDF, both b) and c) are extremely important and could greatly affect the quality of results.
Review Point: 2. Study of global feature. Methods like PiFu purposely avoid using voxel-like feature because of their high computational and memory cost. What is the resolution of the 3D voxel, and does it introduce unnecessary overhead to the whole network? It would be more convincing to study the importance of the global feature in Sec4.2 by comparing with different resolutions of voxel features. Notice when the resolution is reduced to 1x1x1, this is actually the case of using a single global feature.
==================================================

Focused review:

Weaknesses.
The method.
Although I like the experimental findings in Section 4. I have a lot of concerns: 1) experimental settings for Figure 1 to Figure 9 are totally missing, which makes them hard to be convincing; 2) how about the observations when using different teacher-student pairs (with either similar or different network architectures? 3) the metrics (Eq. 4 and its reduced version Eq. 7) are not explicitly used in the designs of FT-KD and PESF-KD. This breaks the flow of the method section. Why not to add a related loss in the optimization?
The proposed FT-KD and PESF-KD are heavily based on existing works deep mutual learning (DML) [1] and parameter-efficient transfer learning methods [2-3], leading to limited technical novelty. What's more, they actually need huge training costs instead of more efficient claimed by the authors, in my understanding (see below comments for details).
To me, claiming FT-KD as a new contribution (the authors call it an adaptive knowledge transfer learning method) is misleading to some degree. As FT-KD merely removes the online knowledge transfer path from the student to the teacher, in DML. Besides, in FT-KD, is the teacher model pre-trained beforehand or trained from scratch jointly the student model? I think, it should be pre-trained beforehand as the authors state that it will be fine-tuned during the training of the student model, is it true?
Note that PESF-KD (may also include FT-KD) uses the pre-trained teacher model. That is, PESF-KD is not a real online KD method. It not only needs the pre-trained stage, but also the online joint training stage. As a result, the claimed 'more efficient' advantage is totally misleading.
[1] Deep Mutual Learning, CVPR 2018.
[2] TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning, NeurIPS 2020.
[3] Towards a Unified View of Parameter-Efficient Transfer Learning, ICLR 2022.
The experiments.
The improvement of the proposed methods is mostly marginal, compared to DML, or other competitive methods.
Note that PESF-KD (may also include FT-KD) uses the pre-trained teacher model, and thus the experimental comparisons of training cost in Table 2, Table 3, etc. are wrong. That is, PESF-KD is not a real online KD method. It not only needs the pre-trained stage, but also the online joint training stage. As a result, the claimed 'more efficient' advantage is totally misleading.
In performance comparison, the authors only consider relatively old KD methods, many recently proposed high-performance KD methods are ignored. Some representative methods are SSKD, SRRL, SemCKD, ReviewKD and SimKD. Others.
The writing can be improved significantly: 1) there are so many bolded descriptions; 2) the organization of figures is bad; 3) there are many typos and grammar errors. ----Update----
I keep my original score as my concerns are not well addressed or ignored in the rebuttal.

Review Point: 1) experimental settings for Figure 1 to Figure 9 are totally missing, which makes them hard to be convincing;
==================================================

Focused review:

1. There is the following recent paper that came up with optimal first order methods for the same overconstrained L2 regression: https://proceedings.icml.cc/static/paper_files/icml/2020/3430-Paper.pdf. In the above paper, the authors used a fixed sketch (in contrast to the current work in which authors used different sketching matrices generated independently in each iteration) which achieved similar or better convergence rate than this paper. The above paper was based on a larger class of pre-conditioned quasi-Newton like methods. While the analyses of these two papers are somewhat different from each other, I am wondering why would one care about the variable sketches (i.e. the current work), rather than the fixed sketch (i.e. the paper mentioned above) that performs better both in terms of the convergence rate and the computation (as one has to generate S only once)? 2. Can this analysis be also extended to accommodate count-sketch or any other sparse sketching matrices ?

Review Point: 1. There is the following recent paper that came up with optimal first order methods for the same overconstrained L2 regression: https://proceedings.icml.cc/static/paper_files/icml/2020/3430-Paper.pdf. In the above paper, the authors used a fixed sketch (in contrast to the current work in which authors used different sketching matrices generated independently in each iteration) which achieved similar or better convergence rate than this paper. The above paper was based on a larger class of pre-conditioned quasi-Newton like methods. While the analyses of these two papers are somewhat different from each other, I am wondering why would one care about the variable sketches (i.e. the current work), rather than the fixed sketch (i.e. the paper mentioned above) that performs better both in terms of the convergence rate and the computation (as one has to generate S only once)?
Review Point: 2. Can this analysis be also extended to accommodate count-sketch or any other sparse sketching matrices ?
==================================================

Focused review:

I have two main concerns regarding this paper: 1) as the goal is to match the attributions between two networks, the idea of adding an attribution-based regularization term in the cost functions seems a trivial and straight-forward solution to me. Moreover, a very similar regularization term was previously proposed by Zagoruyko et al. 2017 who investigated not only activation matching but also the use of gradient-based attributions (in particular, sensitivity map by Simonyan et al. ). While Zagoruyko et al. formulated the problem as "attention transfer", practically their motivation was the same: ensuring that a student model "pays attention" to the same features as the parent. Although it is true that they were only interested in improving the network accuracy, I believe this paper does not add a significant contribution to the method. The authors suggest the use of Grad-Cam as an attribution method but there is no theoretical nor empirical evidence that this method provides superior results than sensitivity map as suggested by Zagoruyko et al. or other gradient-based attribution methods (Gradient x Input, Integrated Gradients, DeepLIFT, or others). Finally, Stochastic matching does not seem to find a theoretical justification. What is the rationale for dropping randomly selected channels and why this should work better than using actual attributions? The connection to dropout is not clear to me. 2) the experimental section does not provide error bounds. As the performance gap between the different methods seems marginal (in particular between EWA and (S)SWA), I wonder if the difference is significant at all. The results might be affected by some stochasticity in the pipeline (e.g. SGD and channel sampling in SSWA). Without providing any standard deviation for these results across different runs, it is impossible to assess the significance of the results.

Review Point: 1) as the goal is to match the attributions between two networks, the idea of adding an attribution-based regularization term in the cost functions seems a trivial and straight-forward solution to me. Moreover, a very similar regularization term was previously proposed by Zagoruyko et al. 2017 who investigated not only activation matching but also the use of gradient-based attributions (in particular, sensitivity map by Simonyan et al. ). While Zagoruyko et al. formulated the problem as "attention transfer", practically their motivation was the same: ensuring that a student model "pays attention" to the same features as the parent. Although it is true that they were only interested in improving the network accuracy, I believe this paper does not add a significant contribution to the method. The authors suggest the use of Grad-Cam as an attribution method but there is no theoretical nor empirical evidence that this method provides superior results than sensitivity map as suggested by Zagoruyko et al. or other gradient-based attribution methods (Gradient x Input, Integrated Gradients, DeepLIFT, or others). Finally, Stochastic matching does not seem to find a theoretical justification. What is the rationale for dropping randomly selected channels and why this should work better than using actual attributions? The connection to dropout is not clear to me.
Review Point: 2) the experimental section does not provide error bounds. As the performance gap between the different methods seems marginal (in particular between EWA and (S)SWA), I wonder if the difference is significant at all. The results might be affected by some stochasticity in the pipeline (e.g. SGD and channel sampling in SSWA). Without providing any standard deviation for these results across different runs, it is impossible to assess the significance of the results.
==================================================

Focused review:

Weaknesses: - The authors claim that scattering networks yield rotationally invariant features. It seems natural to me to have a pre-computation step for a spherical CNN generate spherical or SO(3) signals. So how are the scattering network outputs used? Are they added as a constant signal to the sphere as extra channels? If so, why didn’t the authors explore pre-computation steps that result in non-constant signals? Please let me know if I misunderstand this. - The authors claim that some methods, like DeepSphere [Perraudin 2019] are not equivariant. Can the authors clarify their claim, as this directly contradicts the claims in that paper? As this graph-based method seem very scalable to high resolutions, I don’t see why DeepSphere doesn’t solve the problem the authors invent a new method for. - The experimental section is very weak. The authors cite many other spherical CNN papers, but only compare to a single instance of one other paper. This is insufficient in informing the reader when the proposed method is best used. For example, why didn’t the authors compare to the DeepSphere method, which also experiments on CMB data? - I find the presentation of the wavelets and scattering transform unclear. For example, it doesn’t say how explicitly the wavelets are constructed. Also, I don’t follow the discussion of the dilation parameter. - The method has limited novelty: doing a pre-computation step before applying a neural networks is very widely explored. Also, the theoretical contributions seem incremental changed to previous theoretical results.
Minor points: - The usage of w an ω
together is confusing. - Why is the output of the convolution with the wavelet a spherical signal, rather than a SO(3) signal? Does the wavelet contain a SO(2) symmetry? - In thm 1, V ζ
is undefined.

Review Point: - The authors claim that scattering networks yield rotationally invariant features. It seems natural to me to have a pre-computation step for a spherical CNN generate spherical or SO(3) signals. So how are the scattering network outputs used? Are they added as a constant signal to the sphere as extra channels? If so, why didn’t the authors explore pre-computation steps that result in non-constant signals? Please let me know if I misunderstand this.
Review Point: - The authors claim that some methods, like DeepSphere [Perraudin 2019] are not equivariant. Can the authors clarify their claim, as this directly contradicts the claims in that paper? As this graph-based method seem very scalable to high resolutions, I don’t see why DeepSphere doesn’t solve the problem the authors invent a new method for.
Review Point: - The experimental section is very weak. The authors cite many other spherical CNN papers, but only compare to a single instance of one other paper. This is insufficient in informing the reader when the proposed method is best used. For example, why didn’t the authors compare to the DeepSphere method, which also experiments on CMB data?
Review Point: - I find the presentation of the wavelets and scattering transform unclear. For example, it doesn’t say how explicitly the wavelets are constructed. Also, I don’t follow the discussion of the dilation parameter.
Review Point: - The method has limited novelty: doing a pre-computation step before applying a neural networks is very widely explored. Also, the theoretical contributions seem incremental changed to previous theoretical results. Minor points:
Review Point: - The usage of w an ω together is confusing.
Review Point: - Why is the output of the convolution with the wavelet a spherical signal, rather than a SO(3) signal? Does the wavelet contain a SO(2) symmetry?
==================================================

Focused review:

- The paper lacks discussion of and comparison with other neural rendering methods such as [A] and [B]. Although those methods do not learn the geometry explicitly, I think it is necessary to compare against them by evaluating the quality and accuracy of image rendering. - The authors only demonstrate the results of single objects. I wonder how well the method works on scenes with complex geometry. Is it possible to present the results on data of complex scenes such as the dataset in [C]? [A] DeepVoxels: Learning Persistent 3D Feature Embeddings. CVPR 2019 [B] Deferred Neural Rendering: Image Synthesis using Neural Textures. SIGGRAPH 2019 [C] Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. SIGGRAPH 2019 =========================================================== I appreciate that the authors addressed all reviews clearly in the response. Since the authors emphasized in the rebuttal that their method is aimed to reconstruct the geometry with masked RGB images, I won't regard the two weakness points mentioned above as "weaknesses" anymore. I read comments from other reviewers and the response, and I think the authors clarified their concerns . In summary, I am also positive on the paper and would like to keep my original rating.

Review Point: - The paper lacks discussion of and comparison with other neural rendering methods such as [A] and [B]. Although those methods do not learn the geometry explicitly, I think it is necessary to compare against them by evaluating the quality and accuracy of image rendering.
Review Point: - The authors only demonstrate the results of single objects. I wonder how well the method works on scenes with complex geometry. Is it possible to present the results on data of complex scenes such as the dataset in [C]? [A] DeepVoxels: Learning Persistent 3D Feature Embeddings. CVPR 2019 [B] Deferred Neural Rendering: Image Synthesis using Neural Textures. SIGGRAPH 2019 [C] Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. SIGGRAPH 2019 =========================================================== I appreciate that the authors addressed all reviews clearly in the response. Since the authors emphasized in the rebuttal that their method is aimed to reconstruct the geometry with masked RGB images, I won't regard the two weakness points mentioned above as "weaknesses" anymore. I read comments from other reviewers and the response, and I think the authors clarified their concerns . In summary, I am also positive on the paper and would like to keep my original rating.
==================================================

Focused review:

Major Comments: (1) Organization of theoretical results. * It is not clear to me the significance of Lemma 1 and Theorem 1, specifically, why they are surprising or important for CE (e.g., lemma 1 seem to be a standard application of LLN), and what insight can be derived from them (e.g., why should I care about the asymptotic rate of the stochastically bound O(1/nm)? Does it related to any operating characteristics of the model?). Comparing to these results, Proposition 1 seem to be more central to this paper, and may merit more detailed explanation. If authors agree, it may be beneficial to shift the emphasis in the presentation of the theoretical results, for example, move Lemma 1 to the Appendix. Unless Theorem 1 has crucial implications that is central to the story of the paper, I suggest author move it to the appendix also, or alternatively give a bit more explanation to justify its position in the paper. * As mentioned in the last paragraph, Proposition 1 seem to be more important and deserves its own background section. In particular, author should explain what $\alpha$ in Equation (2) represents, and explain how to compute it in theory and in practice, thereby providing context for Algorithm 1. It will also reader to understand the importance of Proposition 1 by explaining the connection between Var(K) and model's generalization ability, like what you illustrated empirically in Figure 4. (2) (Optionally,) adding baseline comparison to experiments 4.2, and more discussion. While I find the experiments interesting, it might be beneficial to add at least one standard baseline method (e.g., standard neural architecture search) to compare against primal formulation. So author can compare the difference in terms of resulting architecture, estimated kernel variance, testing error and total wall clock time used to better illustrate the benefit of the existing method. Minor: * line 68, "respectively.." (should be only one period) * line 148 cifar10/100 -> CIFAR-10/-100 * There's some relevant recent work on NTK for ensemble models, e.g., [1] and references therein. It appears rather relevant and should be included in Section 5 as well. [1] Bobby He, Balaji Lakshminarayanan, Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. (https://arxiv.org/abs/2007.05864)

Review Point: * It is not clear to me the significance of Lemma 1 and Theorem 1, specifically, why they are surprising or important for CE (e.g., lemma 1 seem to be a standard application of LLN), and what insight can be derived from them (e.g., why should I care about the asymptotic rate of the stochastically bound O(1/nm)? Does it related to any operating characteristics of the model?). Comparing to these results, Proposition 1 seem to be more central to this paper, and may merit more detailed explanation. If authors agree, it may be beneficial to shift the emphasis in the presentation of the theoretical results, for example, move Lemma 1 to the Appendix. Unless Theorem 1 has crucial implications that is central to the story of the paper, I suggest author move it to the appendix also, or alternatively give a bit more explanation to justify its position in the paper.
Review Point: * As mentioned in the last paragraph, Proposition 1 seem to be more important and deserves its own background section. In particular, author should explain what $\alpha$ in Equation (2) represents, and explain how to compute it in theory and in practice, thereby providing context for Algorithm 1. It will also reader to understand the importance of Proposition 1 by explaining the connection between Var(K) and model's generalization ability, like what you illustrated empirically in Figure 4. (2) (Optionally,) adding baseline comparison to experiments 4.2, and more discussion. While I find the experiments interesting, it might be beneficial to add at least one standard baseline method (e.g., standard neural architecture search) to compare against primal formulation. So author can compare the difference in terms of resulting architecture, estimated kernel variance, testing error and total wall clock time used to better illustrate the benefit of the existing method. Minor:
Review Point: * line 68, "respectively.." (should be only one period) * line 148 cifar10/100 -> CIFAR-10/-100 * There's some relevant recent work on NTK for ensemble models, e.g., [1] and references therein. It appears rather relevant and should be included in Section 5 as well. [1] Bobby He, Balaji Lakshminarayanan, Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. (https://arxiv.org/abs/2007.05864)
==================================================

Focused review:

1. The main concern is the lack of experimental results. This paper focuses on convergence and stability of large graphs, however, there is no simulation experiment to support the proposed theory, e.g. evaluating the convergence of GNN on real/simulation data. 2. The assumptions used in this paper are questionable, especially those in line 144 and 261. The authors must give clear illustrations to demonstrate that these assumptions are valid and not far from the practical case.

Review Point: 1. The main concern is the lack of experimental results. This paper focuses on convergence and stability of large graphs, however, there is no simulation experiment to support the proposed theory, e.g. evaluating the convergence of GNN on real/simulation data.
Review Point: 2. The assumptions used in this paper are questionable, especially those in line 144 and 261. The authors must give clear illustrations to demonstrate that these assumptions are valid and not far from the practical case.
==================================================

Focused review:

Weaknesses.
Although the key idea is plausible, it shares similar insights to the previous memory network-based autoencoder approaches for anomaly detection, such as MNAD. The main objective there is to learn less yet compact prototypes to avoid the overfitting of the data. Although the specific way to achieve this objective is different from each other, the new prototype learning in this work does not make clear major contributions. The proposed method seems to simplify the memory learning, but its effectiveness is not clear (see comments below).
The presented results are not convincing. 1) The reported results of the competing methods in the paper are significantly worse than the ones in the their original papers and some recent relevant papers [A,B]. 2) The performance of the presented method is far below that of the current SOTA models (e.g., see the results on the three datasets in [A,B]).
There are a number of false/misleading claims, such as what does it mean by unlabeled normal samples in "Unsupervised anomaly detection (UAD) only needs to fit the unlabeled normal samples to learn the normal patterns" (why is it unlabeled if the samples are known to be normal), and "most of the experiments were not conducted based on the USAD setup" when reviewing memory network-based AD methods.
It is unclear why the learned memory can be named as Hippocampus and Cortex memory.
It claims that the method can work as a plug-and-play component, but not empirical results are given to support this claim. References.
A. "Anomaly Detection via Reverse Distillation from One-Class Embedding." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9737-9746. 2022.
B. "Deep one-class classification via interpolated gaussian descriptor." In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 1, pp. 383-392. 2022.

Review Point: 1) The reported results of the competing methods in the paper are significantly worse than the ones in the their original papers and some recent relevant papers [A,B].
Review Point: 2) The performance of the presented method is far below that of the current SOTA models (e.g., see the results on the three datasets in [A,B]). There are a number of false/misleading claims, such as what does it mean by unlabeled normal samples in "Unsupervised anomaly detection (UAD) only needs to fit the unlabeled normal samples to learn the normal patterns" (why is it unlabeled if the samples are known to be normal), and "most of the experiments were not conducted based on the USAD setup" when reviewing memory network-based AD methods. It is unclear why the learned memory can be named as Hippocampus and Cortex memory. It claims that the method can work as a plug-and-play component, but not empirical results are given to support this claim. References. A. "Anomaly Detection via Reverse Distillation from One-Class Embedding." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9737-9746. 2022. B. "Deep one-class classification via interpolated gaussian descriptor." In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 1, pp. 383-392. 2022.
==================================================

Focused review:

Weaknesses: - There is not much technical novelty. Given the distinct GPs modeling the function network, the acquisition function and sampling procedure are not novel - The theoretical guarantee is pretty weak (random search is asymptotically optimal).
The discussion of not requiring dense coverage to prove the method is asymptotically consistent is interesting, but the utility of proposition 2 is not clear because although dense coverage is a consideration for proving consistency, it is not really a practical reality in sample-efficient optimization—typically BO would not have dense coverage.
Questions/comments: - There is no discussion of observation noise, which is a practical concern in many of the real world use cases mentioned in the paper. The approach of using GPs to model nodes in function network can naturally handle noisy observations, so only the acquisition function would need to be adjusted to account for noisy observations since the best objective value would be unknown. I expect that the empirical performance would remain the same (e.g. using Noisy EI from Letham et al. 2019), but the computation would be much more expensive. It would be good to discuss and demonstrated performance under noisy observations. - How does the number of MC samples affect performance, empirically? How does the network structure affect this? - It would be interesting to see a head-to-head comparison with deep GPs. How different are the runtimes (including inference times) and empirical performances?
Since the core contribution is modeling each node in the function network with a distinct GP, it would be good to see more evaluation of the function network model's predictive performance compared to a alternative modeling choices (e.g. individual models with a compositional objective, vanilla global gp, deep gp)
Grammar: - L238 “out method” -> “our method” - L335 “structurre” -> “structure”
The discussion of the work's limitations is quite thorough, and it proposes interesting directions for future work. The authors have addressed potential negative societal impacts.

Review Point: - There is not much technical novelty. Given the distinct GPs modeling the function network, the acquisition function and sampling procedure are not novel - The theoretical guarantee is pretty weak (random search is asymptotically optimal). The discussion of not requiring dense coverage to prove the method is asymptotically consistent is interesting, but the utility of proposition 2 is not clear because although dense coverage is a consideration for proving consistency, it is not really a practical reality in sample-efficient optimization—typically BO would not have dense coverage. Questions/comments:
Review Point: - There is no discussion of observation noise, which is a practical concern in many of the real world use cases mentioned in the paper. The approach of using GPs to model nodes in function network can naturally handle noisy observations, so only the acquisition function would need to be adjusted to account for noisy observations since the best objective value would be unknown. I expect that the empirical performance would remain the same (e.g. using Noisy EI from Letham et al. 2019), but the computation would be much more expensive. It would be good to discuss and demonstrated performance under noisy observations.
Review Point: - How does the number of MC samples affect performance, empirically? How does the network structure affect this?
Review Point: - It would be interesting to see a head-to-head comparison with deep GPs. How different are the runtimes (including inference times) and empirical performances? Since the core contribution is modeling each node in the function network with a distinct GP, it would be good to see more evaluation of the function network model's predictive performance compared to a alternative modeling choices (e.g. individual models with a compositional objective, vanilla global gp, deep gp) Grammar:
==================================================

Focused review:

- The upper bounds established in the paper do not quite match the best-known lower bounds (for the general strongly convex-strongly concave case). - The problem studied by this paper is somewhat esoteric; often in minimax optimization one phrases bounds solely in terms of the maximum L of the Lipschitz constants L_x, L_y, L_{xy}, in which case there is no difference between this paper's bounds and prior work. [post-rebuttal: apart from the improvement in log(1/eps) factors]

Review Point: - The upper bounds established in the paper do not quite match the best-known lower bounds (for the general strongly convex-strongly concave case).
Review Point: - The problem studied by this paper is somewhat esoteric; often in minimax optimization one phrases bounds solely in terms of the maximum L of the Lipschitz constants L_x, L_y, L_{xy}, in which case there is no difference between this paper's bounds and prior work. [
==================================================

Focused review:

weaknesses of the method. CLARITY: The paper is well organized, partially well written and easy to follow, in other parts with quite some potential for improvement, specifically in the experiments section. Suggestions for more clarity below. SIGNIFICANCE: I consider the work significant, because there might be many settings in which integrated data about the same quantity (or related quantities) may come at different cost. There is no earlier method that allows to take several sources of data into account, and even though it is a fairly straightforward extension of multi-task models and inference on aggregated data, it is relevant. MORE DETAILED COMMENTS: --INTRO & RELATED WORK: * Could you state somewhere early in the introduction that by "task" you mean "output"? * Regarding the 3rd paragraph of the introduction and the related work section: They read unnaturally separated. The paragraph in the introduction reads very technical and it would be great if the authors could put more emphasis there in how their work differs from previous work and introduce just the main concepts (e.g. in what way multi-task learning differs from multiple instance learning). Much of the more technical assessment could go into the related work section (or partially be condensed). --SECTION 2.3: Section 2 was straightforward to follow up to 2.3 (SVI). From there on, it would be helpful if a bit more explanation was available (at the expense of parts of the related work section, for example). More concretely: * l.145ff: $N_d$ is not defined. It would be good to state explicitely that there could be a different number of observations per task. * l.145ff: The notation has confused me when first reading, e.g. $\mathbb{y}$ has been used in l.132 for a data vector with one observation per task, and in l.145 for the collection of all observations. I am aware that the setting (multi-task, multiple supports, different number of observations per task) is inherently complex, but it would help to better guide the reader through this by adding some more explanation and changing notation. Also l.155: do you mean the process f as in l.126 or do you refer to the object introduced in l.147? * l.150ff: How are the inducing inputs Z chosen? Is there any effect of the integration on the choice of inducing inputs? l.170: What is z' here? Is that where the inducing inputs go? * l.166ff: It would be very helpful for the reader to be reminded of the dimensions of the matrices involved. * l.174 Could you explicitly state the computational complexity? * Could you comment on the performance of this approximate inference scheme based on inducing inputs and SVI? --EXPERIMENTS: * synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by "support data" and what by "predicted training count data"? Could you write down the model used here explicitly, e.g. add it to the appendix? * Fertility rates: - It is unclear to me how the training data is aggregated and over which inputs, i.e. what you mean by 5x5. - Now that the likelihood is Gaussian, why not go for exact inference? * Sensor network: - l.283/4 You might want to emphasize here that CI give high accuracy but low time resolution results, e.g. "...a cheaper method for __accurately__ assessing the mass..." - Again, given a Gaussian likelihood, why do you use inducing inputs? What is the trade-off (computational and quality) between using the full model and SVI? - l.304ff: What do you mean by "additional training data"? - Figure 3: I don't understand the red line: Where does the test data come from? Do you have a ground truth? - Now the sensors are co-located. Ideally, you would want to have more low-cost sensors that high-cost (high accuracy) sensors in different locations. Do you have a thought on how you would account for spatial distribution of sensors? --REFERENCES: * please make the style of your references consistent, and start with the last name. Typos etc: ------------- * l.25 types of datasets * l.113 should be $f_{d'}(v')$, i.e. $d'$ instead of $d$ * l.282 "... but are badly bias" should be "is(?) badly biased" (does the verb refer to measurement or the sensor? Maybe rephrase.) * l.292 biased * Figure 3: biased, higher peaks, 500 with unit. * l.285 consisting of? Or just "...as observations of integrals" * l.293 these variables

Review Point: * Could you state somewhere early in the introduction that by "task" you mean "output"?
Review Point: * Regarding the 3rd paragraph of the introduction and the related work section: They read unnaturally separated. The paragraph in the introduction reads very technical and it would be great if the authors could put more emphasis there in how their work differs from previous work and introduce just the main concepts (e.g. in what way multi-task learning differs from multiple instance learning). Much of the more technical assessment could go into the related work section (or partially be condensed). --SECTION 2.3: Section 2 was straightforward to follow up to 2.3 (SVI). From there on, it would be helpful if a bit more explanation was available (at the expense of parts of the related work section, for example). More concretely:
Review Point: * l.145ff: $N_d$ is not defined. It would be good to state explicitely that there could be a different number of observations per task.
Review Point: * l.145ff: The notation has confused me when first reading, e.g. $\mathbb{y}$ has been used in l.132 for a data vector with one observation per task, and in l.145 for the collection of all observations. I am aware that the setting (multi-task, multiple supports, different number of observations per task) is inherently complex, but it would help to better guide the reader through this by adding some more explanation and changing notation. Also l.155: do you mean the process f as in l.126 or do you refer to the object introduced in l.147?
Review Point: * l.150ff: How are the inducing inputs Z chosen? Is there any effect of the integration on the choice of inducing inputs? l.170: What is z' here? Is that where the inducing inputs go?
Review Point: * l.166ff: It would be very helpful for the reader to be reminded of the dimensions of the matrices involved.
Review Point: * Could you comment on the performance of this approximate inference scheme based on inducing inputs and SVI? --EXPERIMENTS:
Review Point: * synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by "support data" and what by "predicted training count data"? Could you write down the model used here explicitly, e.g. add it to the appendix?
Review Point: * Fertility rates:- It is unclear to me how the training data is aggregated and over which inputs, i.e. what you mean by 5x5.
Review Point: - Now that the likelihood is Gaussian, why not go for exact inference?
Review Point: * Sensor network:- l.283/4 You might want to emphasize here that CI give high accuracy but low time resolution results, e.g. "...a cheaper method for __accurately__ assessing the mass..." - Again, given a Gaussian likelihood, why do you use inducing inputs? What is the trade-off (computational and quality) between using the full model and SVI?
Review Point: - l.304ff: What do you mean by "additional training data"?
Review Point: - Figure 3: I don't understand the red line: Where does the test data come from? Do you have a ground truth?
Review Point: - Now the sensors are co-located. Ideally, you would want to have more low-cost sensors that high-cost (high accuracy) sensors in different locations. Do you have a thought on how you would account for spatial distribution of sensors? --REFERENCES:
Review Point: * l.285 consisting of? Or just "...as observations of integrals" * l.293 these variables
==================================================

Focused review:

weakness.
Other comments • The proposed method, plastic gates, which performs best amongst the baselines used when combined with product of experts models, seems simple and effective but I am inclined to question how novel it is, since it just amounts to multi-step online gradient descent on the mixture weights. • The metrics used for evaluating continual learning, loss after switch and recovery time after switch, which are one of the main selling points of the paper are suitable for the datasets provided, but would not be applicable in a setting where either the task boundaries are not known or there are no hard task boundaries to be identified. • Typo Section 2 Paragraph 2: “MNNIST” -> “MNIST”

Review Point: • The metrics used for evaluating continual learning, loss after switch and recovery time after switch, which are one of the main selling points of the paper are suitable for the datasets provided, but would not be applicable in a setting where either the task boundaries are not known or there are no hard task boundaries to be identified.
==================================================

Focused review:

- The technical novelty is minor - The attacks results for batch training seem not good as claimed. - The theoretical analysis for fully-connected layer is not quite meaningful.

Review Point: - The technical novelty is minor - The attacks results for batch training seem not good as claimed.
Review Point: - The theoretical analysis for fully-connected layer is not quite meaningful.
==================================================

Focused review:

1. I wonder if R2D2 or an extension version of a conventional model-free RL that supports POMDP will have similar performance with the proposed method in this paper. For example, what if we simply use a trajectory encoder to capture the true state through history and then add it to the value network and policy network as an additional input feature. Maybe this naïve method can also address tasks like “Catch” and “5-state Random Walk”. More experiments will be helpful to further understand the contribution of this paper. 2. The proposed approach has only comparable or slightly better performance than baseline method (IMPALA) on large-scale standard Atari benchmark, but it has a much more complex implementation than baseline method (a meta learning algorithm seems to be harder to train and use compared to a simple model-free method, and may be less stable in practice). In addition, some recent stronger baselines（e.g., R2D2, NGU, Agent57, and MuZero）on this dataset are not included.

Review Point: 1. I wonder if R2D2 or an extension version of a conventional model-free RL that supports POMDP will have similar performance with the proposed method in this paper. For example, what if we simply use a trajectory encoder to capture the true state through history and then add it to the value network and policy network as an additional input feature. Maybe this naïve method can also address tasks like “Catch” and “5-state Random Walk”. More experiments will be helpful to further understand the contribution of this paper.
Review Point: 2. The proposed approach has only comparable or slightly better performance than baseline method (IMPALA) on large-scale standard Atari benchmark, but it has a much more complex implementation than baseline method (a meta learning algorithm seems to be harder to train and use compared to a simple model-free method, and may be less stable in practice). In addition, some recent stronger baselines（e.g., R2D2, NGU, Agent57, and MuZero）on this dataset are not included.
==================================================

Focused review:

Weaknesses,
W1: The motivation of the objective, to maximize the expectation in Eq. 2, is not convincing and strong;
W2: There are several unclear assumptions in the model, e.g., how to choose p 0 and q 0 ;
W3: The method sections in this paper are hard to follow. And Some symbols are not introduced, e.g., p d and D e l t a
in Eq. 2. Questions:
Q1: What is the rationale of “expectation-maximization” principle to learn P by maximizing the expectation in (1)?
Q2: Why is the P optimized by maximizing the inner product of X and P? Why the P can benefit the preservation of information of X?
Q3: Why the requirement on q 0
, a permutation-equivariant function, can be considered a mild constraint? And how to choose a designable p 0 and q 0
in practice.

Review Point: 2. Questions:Q1: What is the rationale of “expectation-maximization” principle to learn P by maximizing the expectation in (1)?
==================================================

Focused review:

1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested. 2. I am not an expert in the are, but my impression is that the novelty of the work is somewhat limited. In particular the novelty is mainly to refine the framework of Dai et al. [4] on the particular problem, and to introduce the components of noise predictor and importance sampling for scalability.

Review Point: 1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested.
Review Point: 2. I am not an expert in the are, but my impression is that the novelty of the work is somewhat limited. In particular the novelty is mainly to refine the framework of Dai et al. [4] on the particular problem, and to introduce the components of noise predictor and importance sampling for scalability.
==================================================

Focused review:

1. This study is conducted under the assumption of Wiener process and continuity limit, on which the conclusion that the equilibrium state does not rely on its initial learning rate and initialization strongly depends on. Actually the performance of weight normalization + gradient noise, which should be a better candidate for the current analysis, does not reach as high as BN. Therefore, the mechanism of learning dynamics of BN is only partially addressed in this study. 2. As for the experimental verification, only mnist and cifar-10 examples are shown here. The authors should show its effectiveness on larger datasets such as ImageNet to gain more credit. 3. As also pointed out by the author, the mixing in parameter space does not exist for optimization without weight decay since the weight norm monotonically increases. The authors should give a short discussion on the threshold of weight decay that learning falls into the "no mixing in parameter space" zone.

Review Point: 1. This study is conducted under the assumption of Wiener process and continuity limit, on which the conclusion that the equilibrium state does not rely on its initial learning rate and initialization strongly depends on. Actually the performance of weight normalization + gradient noise, which should be a better candidate for the current analysis, does not reach as high as BN. Therefore, the mechanism of learning dynamics of BN is only partially addressed in this study.
Review Point: 2. As for the experimental verification, only mnist and cifar-10 examples are shown here. The authors should show its effectiveness on larger datasets such as ImageNet to gain more credit.
Review Point: 3. As also pointed out by the author, the mixing in parameter space does not exist for optimization without weight decay since the weight norm monotonically increases. The authors should give a short discussion on the threshold of weight decay that learning falls into the "no mixing in parameter space" zone.
==================================================

Focused review:

1) Although this method is conceptually solid, the experimental results seem to be close to the two other simpler baselines especially "Two Step". From Table 1 I feel that Two Step actually do a very good job already and MAME seems to have not much difference. Especially there are some hyperparameters choices (e.g. the sparsity penalty that keeps only 5 features) that might affect the result. 2) I appreciate there are two human studies (4.3, 4.4). But I don't understand intuitively why users can guess model's output much better in MAME (Fig. 2b), even after seeing the visualization interface in the supplementary. Is it because the user can guess which cluster it belongs to more easily? But isn't this method supposed to tell the user which cluster this examples already? I guess I don't know why the user need to guess which cluster it belongs to? Also, in Sec. 4.4 (line 284), "the SME observed that ... MAME were completely homogenous individually and captured the prior knowledge completely ...". Is it possible to have a qualitative example of what this means and why this is important? (3) The previous two points make me wonder how useful this method would be in the real world (similar to two-step baseline and no intuitive understanding of why practioners would find it useful). I also think the biggest Achilles heel for this paper is the required existence of the prior knowledge graph, which is usually unavailable in the real world data. Also, sometimes the prior knowledge graph might not be valid in the data. If that's the case, will this method produce something wrong but because of its homogeneity that misleads the user to trust it more?

Review Point: 1) Although this method is conceptually solid, the experimental results seem to be close to the two other simpler baselines especially "Two Step". From Table 1 I feel that Two Step actually do a very good job already and MAME seems to have not much difference. Especially there are some hyperparameters choices (e.g. the sparsity penalty that keeps only 5 features) that might affect the result.
Review Point: 2) I appreciate there are two human studies (4.3, 4.4). But I don't understand intuitively why users can guess model's output much better in MAME (Fig. 2b), even after seeing the visualization interface in the supplementary. Is it because the user can guess which cluster it belongs to more easily? But isn't this method supposed to tell the user which cluster this examples already? I guess I don't know why the user need to guess which cluster it belongs to? Also, in Sec. 4.4 (line 284), "the SME observed that ... MAME were completely homogenous individually and captured the prior knowledge completely ...". Is it possible to have a qualitative example of what this means and why this is important? (3) The previous two points make me wonder how useful this method would be in the real world (similar to two-step baseline and no intuitive understanding of why practioners would find it useful). I also think the biggest Achilles heel for this paper is the required existence of the prior knowledge graph, which is usually unavailable in the real world data. Also, sometimes the prior knowledge graph might not be valid in the data. If that's the case, will this method produce something wrong but because of its homogeneity that misleads the user to trust it more?
==================================================

Focused review:

Weakness] 1. I don’t see any severe weakness (to reject this paper) in this work.
[Comments] 1. I enjoyed reading this paper, and I think that the approach is “kind of good”. For the following reasons, I’d not put a score of 8 for this paper. A. The suggested proof approach is not easily generalizable, so it would be hard to say that this is a major discovery in the theory of certified robustness. (If possible, some discussion on the possible uses of the proof strategy would make the paper better.) B. I might have missed some potential weaknesses that I should have found. For this comment, I want to discuss with other reviewers during the review process. 2. I can understand how the authors compared their method ( ℓ 0
-certified robustness) to existing counterparts (some of them are ℓ 2
-certified robustness) and I can see there are not that many possible ways to compare them. However, I’m still not convinced whether it is a fair comparison or not, and I’m not even sure whether we can directly compare those methods or not. Also, in my personal opinion, there could be better experiment questions than just showing “our method is excelling other methods”.

Review Point: 1. I don’t see any severe weakness (to reject this paper) in this work. [Comments] 1. I enjoyed reading this paper, and I think that the approach is “kind of good”. For the following reasons, I’d not put a score of 8 for this paper. A. The suggested proof approach is not easily generalizable, so it would be hard to say that this is a major discovery in the theory of certified robustness. (If possible, some discussion on the possible uses of the proof strategy would make the paper better.) B. I might have missed some potential weaknesses that I should have found. For this comment, I want to discuss with other reviewers during the review process.
Review Point: 2. I can understand how the authors compared their method ( ℓ 0 -certified robustness) to existing counterparts (some of them are ℓ 2 -certified robustness) and I can see there are not that many possible ways to compare them. However, I’m still not convinced whether it is a fair comparison or not, and I’m not even sure whether we can directly compare those methods or not. Also, in my personal opinion, there could be better experiment questions than just showing “our method is excelling other methods”.
==================================================

Focused review:

The authors don't quite succeed on giving convincing answers to pressing questions regarding their work. Mainly 1) Why their method should be chosen over popular alternatives for inferring population dynamics like LFADS (which is also VAE based). And 2) why the given method for obtaining disentangled latents is specifically suited for neural data. To address these points the authors would have to compare their method against stronger baselines (instead of just a vanillaVAE / tuning curve fitting). Furthermore, there is a lack of explanation and motivation for the GIN method. Without reading the relevant references it remains unclear why the conditioning on the labels or the volume-preserving generative model is needed. Furthemore the ground truth labels seem to play a central role for the method, but are not much discussed. The fact that latent dimensions are well separated for different conditions when they are used as input to the encoder seems trivial, so there should be more focus on other information that can be extracted from the latent space (like the analysis of the power spectrum in Fig. 4, there it is not clear however, if the same information can be found in the vanilla VAE latent space) or on the evaluation of pi-VAE without the latent prior.

Review Point: 1) Why their method should be chosen over popular alternatives for inferring population dynamics like LFADS (which is also VAE based). And
Review Point: 2) why the given method for obtaining disentangled latents is specifically suited for neural data. To address these points the authors would have to compare their method against stronger baselines (instead of just a vanillaVAE / tuning curve fitting). Furthermore, there is a lack of explanation and motivation for the GIN method. Without reading the relevant references it remains unclear why the conditioning on the labels or the volume-preserving generative model is needed. Furthemore the ground truth labels seem to play a central role for the method, but are not much discussed. The fact that latent dimensions are well separated for different conditions when they are used as input to the encoder seems trivial, so there should be more focus on other information that can be extracted from the latent space (like the analysis of the power spectrum in Fig. 4, there it is not clear however, if the same information can be found in the vanilla VAE latent space) or on the evaluation of pi-VAE without the latent prior.
==================================================

Focused review:

Weaknesses: Like many other papers in the "network embedding" literature, which use neural network techniques inspired by word embeddings to construct latent representations of nodes in a network, the previous line of work on statistical/probabilistic modeling of networks is ignored. In particular, all "network embedding" papers need to start citing, and comparing to, the work on the latent space model of Peter Hoff et al., and subsequent papers in both statistical and probabilistic machine learning publication venues: P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002.
This latent space network model, which embeds each node into a low-dimensional latent space, was written as far back as 2002, and so it far pre-dates neural network-based network embeddings.
Given that the aim of this paper is to model differing representations of social network actors' different roles, it should really cite and compare to the mixed membership stochastic blockmodel (MMSB): Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membership stochastic blockmodels. Journal of Machine Learning Research.
The MMSB allows each node to randomly select a different "role" when deciding whether to form each edge.
- General Discussion: The aforementioned statistical models do not leverage text, and they do not use scalable neural network implementations based on negative sampling, but they are based on well-principled generative models instead of heuristic neural network objective functions and algorithms. There are more recent extensions of these models and inference algorithms which are more scalable, and which do leverage text.
Is the difference in performance between CENE and CANE in Figure 3 statistically insignificant? ( A related question: were the experiments repeated more than once with random train/test splits?)
Were the grid searches for hyperparameter values, mentioned in Section 5.3, performed with evaluation on the test set (which would be problematic), or on a validation set, or on the training set?

Review Point: -General Discussion: The aforementioned statistical models do not leverage text, and they do not use scalable neural network implementations based on negative sampling, but they are based on well-principled generative models instead of heuristic neural network objective functions and algorithms. There are more recent extensions of these models and inference algorithms which are more scalable, and which do leverage text. Is the difference in performance between CENE and CANE in Figure 3 statistically insignificant? ( A related question: were the experiments repeated more than once with random train/test splits?) Were the grid searches for hyperparameter values, mentioned in Section 5.3, performed with evaluation on the test set (which would be problematic), or on a validation set, or on the training set?
==================================================

Focused review:

- the paper seems too applied for the concrete application, and does not give any novel machine learning contributions, all loss functions and network design choices seem only moderately different than cited related texture generation approaches. Please emphasize more the novelty, e.g. key insights related to the 3D texture case. - some unclear details, see detailed comments in below sections for "clarity" and "related to prior work"

Review Point: - the paper seems too applied for the concrete application, and does not give any novel machine learning contributions, all loss functions and network design choices seem only moderately different than cited related texture generation approaches. Please emphasize more the novelty, e.g. key insights related to the 3D texture case.
Review Point: - some unclear details, see detailed comments in below sections for "clarity" and "related to prior work"
==================================================

Focused review:

- For the image and feed-forward NN case, existing methods such as abstract interpretation and other methods are also applicable, which should be compared for bound tightness analysis and efficiency. - The evaluations are performed on small datasets. But I do not see this to be a severe issue for formal bound analysis.

Review Point: - For the image and feed-forward NN case, existing methods such as abstract interpretation and other methods are also applicable, which should be compared for bound tightness analysis and efficiency.
Review Point: - The evaluations are performed on small datasets. But I do not see this to be a severe issue for formal bound analysis.
==================================================

Focused review:

- One important analysis that may be missing is the ablation of whether to have separate student/teacher queues or not. Since the teacher network is anyhow used to provide the target for the student network, I feel the negative keys can come from the same queue (either fixed and provided by the teacher network, or computed online by a key-encoder using the student network), this can also decouple the two changes made in the transition from *regression* baseline to the "ours" method, as regression does NOT need queues at all in my understanding, but "ours" changed both the loss form and the queue -- it would be good to know which part plays a more important role here, the loss or the queue. - I am not sure how to add BN to the teacher network (R182), since I assume the teacher network is already fixed and only provides features, isn't this similar to just whitening the features? - According to Table 2, CC seems highly competitive to ours (achieving on-part performance to "ours" when it is ResNet-50), please show the performance of Ours with R504x for a fair comparison.

Review Point: - One important analysis that may be missing is the ablation of whether to have separate student/teacher queues or not. Since the teacher network is anyhow used to provide the target for the student network, I feel the negative keys can come from the same queue (either fixed and provided by the teacher network, or computed online by a key-encoder using the student network), this can also decouple the two changes made in the transition from *regression* baseline to the "ours" method, as regression does NOT need queues at all in my understanding, but "ours" changed both the loss form and the queue -- it would be good to know which part plays a more important role here, the loss or the queue.
Review Point: - I am not sure how to add BN to the teacher network (R182), since I assume the teacher network is already fixed and only provides features, isn't this similar to just whitening the features?
Review Point: - According to Table 2, CC seems highly competitive to ours (achieving on-part performance to "ours" when it is ResNet-50), please show the performance of Ours with R504x for a fair comparison.
==================================================

Focused review:

- Experimental results seem to be on a single run. Given that some of the differences are not very large, ideally results from multiple runs are included to show the variance in metrics. - "Clean" accuracy and accuracy in the case of small perturbations is worse than MACER; this could compromise practical application where the clean accuracy is also important.

Review Point: - Experimental results seem to be on a single run. Given that some of the differences are not very large, ideally results from multiple runs are included to show the variance in metrics.
Review Point: - "Clean" accuracy and accuracy in the case of small perturbations is worse than MACER; this could compromise practical application where the clean accuracy is also important.
==================================================

Focused review:

I am not sure there are many weaknesses as such. Although conceptually simple the approach relies on several tools to create the Wikifluent corpus (e.g., decontextualization, sentence splitting) and domain expertise to turn the trips into facts, so even though it zero shot wrt generation task itself, it is not data-lean at all. For this reason, I would have liked to see an assessment of how noisy these intermediate steps are.
- Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input). - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit). -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results -

Review Point: - Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input).
Review Point: - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit). -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results -
==================================================

Focused review:

The empirically results are not very strong because the examples are more similar to toy examples than real-world tasks. However I think it's acceptable because the main contribution of this work is introducing direct optimization under RL setting and it does have some merits that PG algorithms don't have such as using the information from heuristic search. Besides, I have some questions as follows: 1. Does DirPG also require the state to be finite? If the state is continuous, the pdf of the trajectory can be larger than 1, can it still be modeled by Gumbel(0)? If DirPG can only be applied to finite state cases, I think it will be more clear to point it out. 2. How will the perfectness of heuristic search affect the algorithm? For example, one crude upper bound U(R) would be (T-t) * max(reward), how much improvement can it bring compared with a perfect heuristic? In other words, will it be challenging to obtain the information that DirPG requires? Minor points: 1. I didn’t get the connection between the gradient formula in l.66 and eq(5), in eq(5) the quantity inside the integration becomes reward, could you elaborate more on this? 2. I didn’t fully understand the context of the motivation example; if we have a perfect heuristic and know which trajectory has reward m, why do we need to learn a policy? 3. On eq(14), should the partial derivative w.r.t theta instead of theta_i? 4. Should eq(15) have a partial derivative w.r.t theta on LHS? Besides, I think it will be better to put Proposition 1 in the main text since it's an important theoretical property.

Review Point: 1. Does DirPG also require the state to be finite? If the state is continuous, the pdf of the trajectory can be larger than 1, can it still be modeled by Gumbel(0)? If DirPG can only be applied to finite state cases, I think it will be more clear to point it out.
Review Point: 2. How will the perfectness of heuristic search affect the algorithm? For example, one crude upper bound U(R) would be (T-t) * max(reward), how much improvement can it bring compared with a perfect heuristic? In other words, will it be challenging to obtain the information that DirPG requires? Minor points:
Review Point: 1. I didn’t get the connection between the gradient formula in l.66 and eq(5), in eq(5) the quantity inside the integration becomes reward, could you elaborate more on this?
Review Point: 2. I didn’t fully understand the context of the motivation example; if we have a perfect heuristic and know which trajectory has reward m, why do we need to learn a policy?
Review Point: 3. On eq(14), should the partial derivative w.r.t theta instead of theta_i?
Review Point: 4. Should eq(15) have a partial derivative w.r.t theta on LHS? Besides, I think it will be better to put Proposition 1 in the main text since it's an important theoretical property.
==================================================

Focused review:

1. It’s not clear to me that as the sparse constraint is enforced at the beginning, is it sufficient to explore the entire search space? Could the authors specify the proportion of the explored architectures? 2. The results on ImageNet are not very impressive as it is comparable but not surpass previous methods.

Review Point: 1. It’s not clear to me that as the sparse constraint is enforced at the beginning, is it sufficient to explore the entire search space? Could the authors specify the proportion of the explored architectures?
Review Point: 2. The results on ImageNet are not very impressive as it is comparable but not surpass previous methods.
==================================================

Focused review:

Weaknesses: 1. Compared with the VPT method, the LPT method only adds a small number of fine-tuning parameters, but the manuscript does not mention whether the 2-stage method will increase the training time. 2. The LPT method is robust to the domain shift problem. This paper proposes a possible explanation, but does not specify that domain-specific knowledge is obtained by what kind of prompt and at which stage.

Review Point: 1. Compared with the VPT method, the LPT method only adds a small number of fine-tuning parameters, but the manuscript does not mention whether the 2-stage method will increase the training time.
Review Point: 2. The LPT method is robust to the domain shift problem. This paper proposes a possible explanation, but does not specify that domain-specific knowledge is obtained by what kind of prompt and at which stage.
==================================================

Focused review:

weaknesses are. And they do strike me as having at least the following significant weaknesses (I will focus on Local Counterfactual Rules here):
Definition 3.3 of Local Counterfactual Rules only provides a necessary requirement that the rectangles should satisfy, so it is in fact not a full definition.
The rectangles produced by Local Counterfactual Rules may actually be implausible, i.e. definition 3.3. leaves open the possibility that P(C_S(x;y*)) is arbitrarily small, because it is only a requirement about a probability conditional on C_S(x;y*).
The approach critically relies on estimating a conditional probability for x. There should be a discussion on when this is reliable. For instance, this cannot work well when explaining inputs x from low-probability regions for which the exact distribution cannot be estimated very well, or if the true distribution is very irregular, or if the input dimension is large. The conditional probability is also estimated for each possible rectangle C_S(x), which will inflate the estimation error.
It is not clear how a user is supposed to obtain recourse from hearing a rectangular region. Definition 3.3 implies that, if the user picks a point from the region according to the true distribution of x, then they will obtain recourse with high probability. But a) the user does not know the true distribution of x, and b) even if they did, it would seem unreasonable to expect them to modify their features in accordance with this distribution. The introduction (line 98) claims that their approach "partly answer[s] the problem of noisy responses to 99 prescribed recourses" identified by Pawelczyk et al., 2022, but this claim is not backed up. Presentation:
The presentation is sufficient, but clarity could still be improved and claims could be substantiated better. For example:
There are multiple cases where terms are unclear or become clear only later, especially in the introduction. For example, line 51 refers to "the best promising directions", but it is not clear by which measure 'best' is determined.
In line 34 it is claimed that solutions from prior work are "not entirely satisfactory", but it is not specified in which way they are not satisfactory.
Definition 3.2 confuses the distinction between a "Divergent Explanation" and a "Minimal Divergent Explanation".
Limitations of the approach have not been sufficiently addressed, as discussed above. In particular, it is not clear when it can be expected to work and when not, or how to detect whether it can be applied for a particular type of data.

Review Point: 98) claims that their approach "partly answer[s] the problem of noisy responses to 99 prescribed recourses" identified by Pawelczyk et al., 2022, but this claim is not backed up. Presentation: The presentation is sufficient, but clarity could still be improved and claims could be substantiated better. For example: There are multiple cases where terms are unclear or become clear only later, especially in the introduction. For example, line 51 refers to "the best promising directions", but it is not clear by which measure 'best' is determined. In line 34 it is claimed that solutions from prior work are "not entirely satisfactory", but it is not specified in which way they are not satisfactory. Definition 3.2 confuses the distinction between a "Divergent Explanation" and a "Minimal Divergent Explanation". Limitations of the approach have not been sufficiently addressed, as discussed above. In particular, it is not clear when it can be expected to work and when not, or how to detect whether it can be applied for a particular type of data.
==================================================

Focused review:

Weaknesses
in section 3.3, the authors proved the invertibility and concluded that the proposed method is manifold intrusion-free.
The proof of invertibility assumes the graph edge’s weight must be 0 or 1. Can the proposed method be used for the graph with real-value edges [0,1]?
When using a graph with real-value edge weights, does the proposed method still satisfy the invertibility?
3)The proof also requires the lambda not to be 0.5. But in standard mixup, the mixed sample can be generated in the case of lambda being 0.5. Thus, the assumption in this proof may not be correct. 4)The most important question in this part is how the invertibility can induce “manifold intrusion free”? The connection between them is unclear. Assume your mixed sample can recover two original interpolation graphs, how to ensure that there is no conflict between the mixed sample’s representation and other training samples’ ones. Please explain this connection.
As you used 5- and 8-layer GNNs in experiments, why not consider shallower GNNs? As seen in Figure 5, baselines show better performance when using a shallower structure.
it is unclear what do you want to prove in section 4.3.1 as there are no connections to the proposed method.
there are several existing related papers that all use mixup for graph node-classification tasks. As your proposed method can be used for mixing two graphs, it can also mix one graph with itself. Besides, as in your title, it should be a general mixup method for graphs, right? For these reasons, it will be good to also do the evaluation on node-classification tasks. Then SOTA graph mixup method should be [1]. I would like to see a comparison between the proposed method and baseline [1]. ([2] should be a weak baseline as it does not work significantly better than the basic baseline GNN, see Table3.)
[1] 2021AAAI GraphMix: Improved Training of GNNs for Semi-Supervised Learning [2]2021 WWW MixupGrpah: Mixup for node and graph classification

Review Point: 3)The proof also requires the lambda not to be 0.5. But in standard mixup, the mixed sample can be generated in the case of lambda being 0.5. Thus, the assumption in this proof may not be correct.
Review Point: 4)The most important question in this part is how the invertibility can induce “manifold intrusion free”? The connection between them is unclear. Assume your mixed sample can recover two original interpolation graphs, how to ensure that there is no conflict between the mixed sample’s representation and other training samples’ ones. Please explain this connection. As you used 5- and 8-layer GNNs in experiments, why not consider shallower GNNs? As seen in Figure 5, baselines show better performance when using a shallower structure. it is unclear what do you want to prove in section 4.3.1 as there are no connections to the proposed method. there are several existing related papers that all use mixup for graph node-classification tasks. As your proposed method can be used for mixing two graphs, it can also mix one graph with itself. Besides, as in your title, it should be a general mixup method for graphs, right? For these reasons, it will be good to also do the evaluation on node-classification tasks. Then SOTA graph mixup method should be [1]. I would like to see a comparison between the proposed method and baseline [1]. ([2] should be a weak baseline as it does not work significantly better than the basic baseline GNN, see Table3.) [1] 2021AAAI GraphMix: Improved Training of GNNs for Semi-Supervised Learning [2]2021 WWW MixupGrpah: Mixup for node and graph classification
==================================================

Focused review:

Some of the claims made in this paper have already been explored in previous works or are rather obvious. 1. PointNet features are not rotation invariant. We can try to make them robust by using data-augmentation. Rigidly aligning the shapes will obviously make the features consistent across shapes, especially when shapes share same human template. Also, note that Donati etal used learnable point based features, so it is not a new contribution, as is claimed in Line-44. 2. The importance of functional map representation with reduced basis has already being shown in Donati etal (See Section 2). 3. The importance of structural properties of the functional maps are well justified in the paper. Though, the loss functions have already been used in previous works and their efficacy has been described in previous works. 4. The paper also claims an independent contribution on partial shape matching, though effective, seems out of the place in the paper. It would have been more interesting paper, if it focused on one aspect. Even focusing only on partial shape matching and detailed analysis on only one aspect would have been a good contribution to the community.

Review Point: 1. PointNet features are not rotation invariant. We can try to make them robust by using data-augmentation. Rigidly aligning the shapes will obviously make the features consistent across shapes, especially when shapes share same human template. Also, note that Donati etal used learnable point based features, so it is not a new contribution, as is claimed in Line-44.
Review Point: 2. The importance of functional map representation with reduced basis has already being shown in Donati etal (See Section 2).
Review Point: 3. The importance of structural properties of the functional maps are well justified in the paper. Though, the loss functions have already been used in previous works and their efficacy has been described in previous works.
Review Point: 4. The paper also claims an independent contribution on partial shape matching, though effective, seems out of the place in the paper. It would have been more interesting paper, if it focused on one aspect. Even focusing only on partial shape matching and detailed analysis on only one aspect would have been a good contribution to the community.
==================================================

Focused review:

I was not able to find any major concerns. Some minor comments are written in the next section.
- I am a little unconvinced by the discussion in lines 477 to 486. I speculate that sentence embeddings learned by DiffCSE focus more on superficial information than SimCSE in order to solve replaced token detection, but the result is the opposite. Why?
- It would be insightful to investigate the impact of the model size of the encoder. In the past few years, I have repeatedly observed that a method that works well on small models has only a marginal impact on large models. This paper will be further strengthened if the effectiveness of the proposed method is shown even when large-scale encoders are employed.

Review Point: - I am a little unconvinced by the discussion in lines 477 to 486. I speculate that sentence embeddings learned by DiffCSE focus more on superficial information than SimCSE in order to solve replaced token detection, but the result is the opposite. Why?
Review Point: - It would be insightful to investigate the impact of the model size of the encoder. In the past few years, I have repeatedly observed that a method that works well on small models has only a marginal impact on large models. This paper will be further strengthened if the effectiveness of the proposed method is shown even when large-scale encoders are employed.
==================================================

Focused review:

weaknesses. Strengths: * This is the first inconsistency analysis for random forests. (Verified by quick Google scholar search.) * Clearly written to make results (mostly) approachable. This is a major accomplishment for such a technical topic. * The analysis is relevant to published random forest variations; these include papers published at ICDM, AAAI, SIGKDD. Weaknesses: * Relevance to researchers and practitioners is a little on the low side because most people are using supervised random forest algorithms. * The title, abstract, introduction, and discussion do not explain that the results are for unsupervised random forests. This is a fairly serious omission, and casual readers would remember the wrong conclusions. This must be fixed for publication, but I think it would be straightforward to fix. Officially, NIPS reviewers are not required to look at the supplementary material. Because of having only three weeks to review six manuscripts, I was not able to make the time during my reviewing. So I worry that publishing this work would mean publishing results without sufficient peer review. DETAILED COMMENTS * p. 1: I'm not sure it is accurate to say that deep, unsupervised trees grown with no subsampling is a common setup for learning random forests. It appears in Geurts et al. (2006) as a special case, sometimes in mass estimation [1, 2], and sometimes in Wei Fan's random decision tree papers [3-6]. I don't think these are used very much. * You may want to draw a connection between Theorem 3 and isolation forests [7] though. I've heard some buzz around this algorithm, and it uses unsupervised, deep trees with extreme subsampling. * l. 16: "random" => "randomized" * l. 41: Would be clearer with forward pointer to definition of deep. * l. 74: "ambient" seems like wrong word choice * l. 81: Is there a typo here? Exclamation point after \thereexists is confusing. * l. 152; l. 235: I think this mischaracterizes Geurts et al. (2006), and the difference is important for the impact stated in Section 4. Geurts et al. include a completely unsupervised tree learning as a special case, when K = 1. Otherwise, K > 1 potential splits are generated randomly and unsupervised (from K features), and the best one is selected *based on the response variable*. The supervised selection is important for low error on most data sets. See Figures 2 and 3; when K = 1, the error is usually high. * l. 162: Are random projection trees really the same as oblique trees? * Section 2.2: very useful overview! * l. 192: Typo? W^2? * l. 197: No "Eq. (2)" in paper? * l. 240: "parameter setup that is widely used..." This was unclear. Can you add references? For example, Lin and Jeon (2006) study forests with adaptive splitting, which would be supervised, not unsupervised. * Based on the abstract, you might be interested in [8]. REFERENCES [1] Ting et al. (2013). Mass estimation. Machine Learning, 90(1):127-160. [2] Ting et al. (2011). Density estimation based on mass. In ICDM. [3] Fan et al. (2003). Is random model better? On its accuracy and efficiency. In ICDM. [4] Fan (2004). On the optimality of probability estimation by random decision trees. In AAAI. [5] Fan et al. (2005). Effective estimation of posterior probabilities: Explaining the accuracy of randomized decision tree approaches. In ICDM. [6] Fan el al. (2006). A general framework for accurate and fast regression by data summarization in random decision trees. In KDD. [7] Liu, Ting, and Zhou (2012). Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data, 6(1). [8] Wager. Asymptotic theory for random forests. https://arxiv.org/abs/1405.0352

Review Point: * The analysis is relevant to published random forest variations; these include papers published at ICDM, AAAI, SIGKDD. Weaknesses:
Review Point: * Relevance to researchers and practitioners is a little on the low side because most people are using supervised random forest algorithms.
Review Point: * The title, abstract, introduction, and discussion do not explain that the results are for unsupervised random forests. This is a fairly serious omission, and casual readers would remember the wrong conclusions. This must be fixed for publication, but I think it would be straightforward to fix. Officially, NIPS reviewers are not required to look at the supplementary material. Because of having only three weeks to review six manuscripts, I was not able to make the time during my reviewing. So I worry that publishing this work would mean publishing results without sufficient peer review. DETAILED COMMENTS * p.
Review Point: 1: I'm not sure it is accurate to say that deep, unsupervised trees grown with no subsampling is a common setup for learning random forests. It appears in Geurts et al. (2006) as a special case, sometimes in mass estimation [1, 2], and sometimes in Wei Fan's random decision tree papers [3-6]. I don't think these are used very much.
Review Point: * You may want to draw a connection between Theorem 3 and isolation forests [7] though. I've heard some buzz around this algorithm, and it uses unsupervised, deep trees with extreme subsampling.
Review Point: 41: Would be clearer with forward pointer to definition of deep.
Review Point: * l.74: "ambient" seems like wrong word choice * l.
Review Point: * l. 152; l. 235: I think this mischaracterizes Geurts et al. (2006), and the difference is important for the impact stated in Section 4. Geurts et al. include a completely unsupervised tree learning as a special case, when K = 1. Otherwise, K > 1 potential splits are generated randomly and unsupervised (from K features), and the best one is selected *based on the response variable*. The supervised selection is important for low error on most data sets. See Figures 2 and 3; when K = 1, the error is usually high.
Review Point: * l. 162: Are random projection trees really the same as oblique trees?
Review Point: * l. 240: "parameter setup that is widely used..." This was unclear. Can you add references? For example, Lin and Jeon (2006) study forests with adaptive splitting, which would be supervised, not unsupervised.
Review Point: * Based on the abstract, you might be interested in [8]. REFERENCES [1] Ting et al. (2013). Mass estimation. Machine Learning, 90(1):127-160. [2] Ting et al. (2011). Density estimation based on mass. In ICDM. [3] Fan et al. (2003). Is random model better? On its accuracy and efficiency. In ICDM. [4] Fan (2004). On the optimality of probability estimation by random decision trees. In AAAI. [5] Fan et al. (2005). Effective estimation of posterior probabilities: Explaining the accuracy of randomized decision tree approaches. In ICDM. [6] Fan el al. (2006). A general framework for accurate and fast regression by data summarization in random decision trees. In KDD. [7] Liu, Ting, and Zhou (2012). Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data, 6(1). [8] Wager. Asymptotic theory for random forests. https://arxiv.org/abs/1405.0352
==================================================

Focused review:

weakness of MAVEN. 3. Did the authors perform an ablation test for evaluating MAVEN against QMIX with each agent using Bootstrapped DQN? 4. How stable is the learning of MAVEN using the alternative training of the value function and the hierarchical control policy? 5. As shown in Figure 1, is the epsilon-greedy exploration still used in MAVEN? Some typos: Line 113: removing âaâ Line 116: missing a â)â UPDATE: After reading the author's rebuttal, I have chosen to increase my score from 5 to 6 because the authors provide stronger results and partially address my Question 2.

Review Point: 4. How stable is the learning of MAVEN using the alternative training of the value function and the hierarchical control policy?
==================================================

Focused review:

Weakness:
The proposed moving object mask modeling in monocular depth estimation is not novel. Previous works including [1] are not discussed and compared.
The proposed method is only compared with methods before 2020, and the performance gain is very marginal, e.g. on improved split, the proposed method uses more data CS+K but the abs rel is only improved by 0.001
In theory, the proposed module models not only moving object but any inconsistent depth variance. So from visualization, the moving object mask will focus on both moving object and also some objects which has large depth variance (e.g. sky in background)
[1] Luo, Chenxu, Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, Ram Nevatia, and Alan Yuille. "Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding." IEEE transactions on pattern analysis and machine intelligence 42, no. 10 (2019): 2624-2641.
[2] Klingner, Marvin, Jan-Aike Termöhlen, Jonas Mikolajczyk, and Tim Fingscheidt. "Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance." In European Conference on Computer Vision, pp. 582-600. Springer, Cham, 2020. [3] Dai, Qi, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, and Konrad Schindler. "Self-supervised object motion and depth estimation from video." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 1004-1005. 2020.

Review Point: 10 (2019): 2624-2641. [2] Klingner, Marvin, Jan-Aike Termöhlen, Jonas Mikolajczyk, and Tim Fingscheidt. "Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance." In European Conference on Computer Vision, pp. 582-600. Springer, Cham, 2020. [3] Dai, Qi, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, and Konrad Schindler. "Self-supervised object motion and depth estimation from video." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 1004-1005. 2020.
==================================================

Focused review:

1. The co-designing mechanism is the core contribution of the paper but the detailed process is not described clearly. It is suggested to provide an elaborate diagram or pseudocode to introduce the whole framework. 2. MCUNet achieves convincing results on STM32. It is meaningful to explore its generalization ability. Please demonstrate the potential/ability of MCUNet being deployed on more scenarios and tasks, e.g., more devices or tasks like object detection, semantic segmentation. 3.The method is described as a co-design scheme. But the experiments failed to highlight the improvements of the co-design scheme, when compared to those single design schemes. 4.The author introduce the tiny engine with the motivation that "TinyEngine optimizes the memory scheduling according to the overall network topology to get a better strategy." But it is unclear whether the overall network topology indeed is the main reason for the huge improvements. The authors need to add supporting results or supporting articles. 5. Some typo issues need to be checked, e.g., "to achieves" in line 14.

Review Point: 1. The co-designing mechanism is the core contribution of the paper but the detailed process is not described clearly. It is suggested to provide an elaborate diagram or pseudocode to introduce the whole framework.
Review Point: 2. MCUNet achieves convincing results on STM32. It is meaningful to explore its generalization ability. Please demonstrate the potential/ability of MCUNet being deployed on more scenarios and tasks, e.g., more devices or tasks like object detection, semantic segmentation.
Review Point: 3.The method is described as a co-design scheme. But the experiments failed to highlight the improvements of the co-design scheme, when compared to those single design schemes.
Review Point: 4.The author introduce the tiny engine with the motivation that "TinyEngine optimizes the memory scheduling according to the overall network topology to get a better strategy." But it is unclear whether the overall network topology indeed is the main reason for the huge improvements. The authors need to add supporting results or supporting articles.
==================================================

Focused review:

1. It is good to see a study of the causes of hidden stratification problem before formally proposed GEORGE. According to the paper, there are two main causes, i.e. inherent hardness and data imbalance. However, it is unclear how the proposed GEORGE address these two causes. Any discussion about this would further improve this paper. 2. The computation cost of GEORGE might be high since it is a two-step framework. Any results to show the computation cost?

Review Point: 1. It is good to see a study of the causes of hidden stratification problem before formally proposed GEORGE. According to the paper, there are two main causes, i.e. inherent hardness and data imbalance. However, it is unclear how the proposed GEORGE address these two causes. Any discussion about this would further improve this paper.
Review Point: 2. The computation cost of GEORGE might be high since it is a two-step framework. Any results to show the computation cost?
==================================================

Focused review:

The main weakness of this paper is that it is not sufficiently grounded in the sociolinguistic side of bias measurement, and therefore makes some significant claims and design decisions that are not entirely appropriate.
The proposed task and the experimental design are predicated on the assumption that word senses as semantically disjoint with respect to social biases, but this is not really true (which is actually nodded to in the paper's discussion section). References to nationality and language can often be used metonymically to imply one another (particularly for voicing negative opinions), and colour and race are most certainly strongly correlated in visual design and storytelling (eg the use of dark colours, especially black, for evil in Western European storytelling, which is less common in African stories). Occupations and associated actions, while not used metonymically, may still be correlated: it is likely higher surprisal to refer to a woman "engineering a solution" than a man, for example. ( It is worth noting that the maximum pairwise similarity approach used in WAT calculation runs counter to this assumption of disjoint senses.)
There is also an important distinction between observability of a bias (e.g., through surveying/prompting native speakers) and measuring it using computational tools. Social biases may be observable without being measured by current approaches. These nuances are important to discuss throughout the paper: social bias assessment is not a computational task alone, but must incorporate human judgment and be framed through specific human viewpoints with acknowledged limitations.
There are several further issues with the precision of the language used in the paper that are important to address when discussing a sensitive topic such as social bias measurement.
- The term "bias" itself needs some context. Biases may be positive or neutral (eg preferring to complete "they sat at the computer to write ___" with "code" vs "a book" when trained on different data), as well as negative. As such, it is not clear what it means to have an "unbiased" model, or when that is desirable. A clearer definition of what kinds of bias are being assessed, what their impact is, and what mitigation of these biases might look like will help set the context better.
- The term "stereotype" is not used correctly. A stereotype is a commonly-held association between one group and some attribute or behavior; although most stereotypes involve negative attributes or disfavored behaviors, a stereotype is not inherently negative. The labels of "stereotype/anti-stereotype" are therefore not appropriate; positive/negative would be better. ( An anti-stereotype would be something that contradicts the stereotypical attribute or behavior.) Stereotypes are also socially-derived based on common beliefs/associations; for example, "Japanese people are stupid" is not a commonly-held belief, so this is not a stereotype.
- The terms "pleasant" and "unpleasant" are used, but are not clearly defined and feel subjective enough to mask the import of the difference being discussed. " Positive" and "negative" are the standard words used to discuss emotional valence of this type; these should either be used or the use of alternatives should be justified and clearly defined.
- Race and ethnicity are distinct constructs; the evaluation described in Section 4.2 is comparing race senses with colour senses, not ethnicity. ( Ethnicity is related more to heritage and origin, while race is a social group that one can be assigned to by others or identify with voluntarily.)
- Referring to nationality groups as "disadvantaged groups" (Line 286) doesn't quite work; while the racial identities that nationalities are correlated with may be disadvantaged, non-US nationalities themselves are not disadvantaged. ( Eg Scandinavian nationalities are very unlikely to be considered disadvantaged.)
It is not clear what the impact could be of measuring bias in sense embeddings or of debiasing sense embeddings. These models are much more rarely used than word embeddings, and in much more specialized settings.
Other issues with the presentation: - Section 3 is difficult to follow. It is not clear how (X and Y) and (A and B) are related, or what a high or low score on the s and w functions means.
- Figures 2 and 3 are unreadable.
- P-value calculation is discussed in Section 3, but no p-values are reported in the experimental results.
- The inclusion of the verb sense of "carpenter" in Section 4.3 is a little questionable; this is a very rarely-used sense. The other occupation/action words are commonly used and could reasonably be measured, but very few texts (for embedding training) will use carpenter as a verb, and many native speakers will not recognize it as correct.
- There is growing consensus to capitalize racial identifiers (certainly "Black", increasingly "White").
- Lines 442-445: This actually does not accurately simulate the word-level embedding case, as it assigns equal weight to all senses. In practice, word senses are more likely to be Zipf-distributed, so a word-level embedding model will be exposed to much more training data for some senses than for others.
- It is not clear how the categories in WEAT (Table 2) are associated with the social biases this paper is framed around.
- It would be helpful to mark the extended WEAT/WAT contributed by this paper using a modified label (eg WEAT*), in Table 2 and in text.
- The Ethical Statement is well written, but should be extended with some more concrete discussion of challenges not addressed in the dataset (eg gender beyond the binary, races other than Black, other sources of social bias).
- It would be helpful to have a listing of nationalities/racial identities/occupations included in the dataset (along with the adjectives used) as part of the paper, such as in supplementary tables.
- It would be interesting to see what human behavior is for these prompts/comparisons... - Dev et al 2019 and Nadeem et al 2020 references are missing publication information.
Typos - Figure 1, graph titles "embembeddings" - Line 051: extra "is" - Line 122: missing "are" - Line 603: "bises" -> "biases" - Line 610: missing "being" - Spaces not needed between section symbol and number - Spaces after non-terminal periods ("vs.", "cf.") should be escaped to avoid spacing issues

Review Point: - The term "bias" itself needs some context. Biases may be positive or neutral (eg preferring to complete "they sat at the computer to write ___" with "code" vs "a book" when trained on different data), as well as negative. As such, it is not clear what it means to have an "unbiased" model, or when that is desirable. A clearer definition of what kinds of bias are being assessed, what their impact is, and what mitigation of these biases might look like will help set the context better.
Review Point: - The term "stereotype" is not used correctly. A stereotype is a commonly-held association between one group and some attribute or behavior; although most stereotypes involve negative attributes or disfavored behaviors, a stereotype is not inherently negative. The labels of "stereotype/anti-stereotype" are therefore not appropriate; positive/negative would be better. ( An anti-stereotype would be something that contradicts the stereotypical attribute or behavior.) Stereotypes are also socially-derived based on common beliefs/associations; for example, "Japanese people are stupid" is not a commonly-held belief, so this is not a stereotype.
Review Point: - The terms "pleasant" and "unpleasant" are used, but are not clearly defined and feel subjective enough to mask the import of the difference being discussed. " Positive" and "negative" are the standard words used to discuss emotional valence of this type; these should either be used or the use of alternatives should be justified and clearly defined.
Review Point: - Race and ethnicity are distinct constructs; the evaluation described in Section 4.2 is comparing race senses with colour senses, not ethnicity. ( Ethnicity is related more to heritage and origin, while race is a social group that one can be assigned to by others or identify with voluntarily.) - Referring to nationality groups as "disadvantaged groups" (Line 286) doesn't quite work; while the racial identities that nationalities are correlated with may be disadvantaged, non-US nationalities themselves are not disadvantaged. ( Eg Scandinavian nationalities are very unlikely to be considered disadvantaged.) It is not clear what the impact could be of measuring bias in sense embeddings or of debiasing sense embeddings. These models are much more rarely used than word embeddings, and in much more specialized settings. Other issues with the presentation:
Review Point: - Section 3 is difficult to follow. It is not clear how (X and Y) and (A and B) are related, or what a high or low score on the s and w functions means.
Review Point: - P-value calculation is discussed in Section 3, but no p-values are reported in the experimental results.
Review Point: - The inclusion of the verb sense of "carpenter" in Section 4.3 is a little questionable; this is a very rarely-used sense. The other occupation/action words are commonly used and could reasonably be measured, but very few texts (for embedding training) will use carpenter as a verb, and many native speakers will not recognize it as correct.
Review Point: - There is growing consensus to capitalize racial identifiers (certainly "Black", increasingly "White").
Review Point: - Lines 442-445: This actually does not accurately simulate the word-level embedding case, as it assigns equal weight to all senses. In practice, word senses are more likely to be Zipf-distributed, so a word-level embedding model will be exposed to much more training data for some senses than for others.
Review Point: - It is not clear how the categories in WEAT (Table 2) are associated with the social biases this paper is framed around.
Review Point: - It would be helpful to mark the extended WEAT/WAT contributed by this paper using a modified label (eg WEAT*), in Table 2 and in text.
Review Point: - The Ethical Statement is well written, but should be extended with some more concrete discussion of challenges not addressed in the dataset (eg gender beyond the binary, races other than Black, other sources of social bias).
Review Point: - It would be helpful to have a listing of nationalities/racial identities/occupations included in the dataset (along with the adjectives used) as part of the paper, such as in supplementary tables.
Review Point: - It would be interesting to see what human behavior is for these prompts/comparisons...
==================================================

Focused review:

Weaknesses
The proposed algorithm is not parameter-free (unlike SNIP, which is virtually parameter-free), is quite complicated (and I imagine difficult to implement), and there is little justification for certain components of the method, e.g., the dynamic scaling function (and choices of lambda), whether the simplification of m_{t-1} = … = m_{T} is mild enough. It is not clear to me how a practitioner can run the proposed algorithm in a parameter-free way without having to conduct ablation studies of their own first, especially since, as the authors note, “We observed that the penalty parameter was difficult to tune properly, either being too aggressive at pruning, or too passive” as the justification for the dynamic scaling function
Parts of the paper are too dense and notation-heavy, and this hurts readability and understanding significantly, e.g., Lemma 1, paragraph regarding the introduction of the saliency function on pg. 2.
The presented experimental results are not very compelling. For example, in Table 3, we see that BEP 1e-4 achieves a ~.4% improvement over SNIP and GRASP, at the cost of ~7-8.4 more hours of training time. This calls into question the effectiveness of the proposed approach -- which is, at the end of the day, meant to speed up training + pruning by removing unnecessary components of the network early on. Clarity
The paper is reasonably well-written and organized overall. It was clear that the authors compressed some of the mathematical expressions/lemmas (e.g., statement of Lemma 1), which is somewhat understandable given the page limit, but this hurt readability and understandability.

Review Point: 2. The presented experimental results are not very compelling. For example, in Table 3, we see that BEP 1e-4 achieves a ~.4% improvement over SNIP and GRASP, at the cost of ~7-8.4 more hours of training time. This calls into question the effectiveness of the proposed approach -- which is, at the end of the day, meant to speed up training + pruning by removing unnecessary components of the network early on. Clarity The paper is reasonably well-written and organized overall. It was clear that the authors compressed some of the mathematical expressions/lemmas (e.g., statement of Lemma 1), which is somewhat understandable given the page limit, but this hurt readability and understandability.
==================================================

Focused review:

This key motivation of this work is just applying the idea of zero-shot multilingual sentence-level NMT to the multilingual document-level NMT task. It appears that innovation of this paper is less.
1. The reference of a related work (https://aclanthology.org/2020.wmt-1.53) is missing in this paper.

Review Point: 1. The reference of a related work (https://aclanthology.org/2020.wmt-1.53) is missing in this paper.
==================================================

Focused review:

Weaknesses:
The analysis is limited in the following aspects: (1) The analyzed two-layer neural network does not allow the weight evolvement in the second-layer, which is different from the adversarial training practice. (2) The latent assumption of linear separation (second point of Asumption 3.3) seems strong. (3) Extension of the analysis to multi-class settings may be hard.
The proof can be made more rigorous. For instance, I got lost in the following places:
Proof of Thm 3.2: Let G := W F and κ m W F =: G
seem to be contradicting.
Proof of Thm 3.2, page 13 upper part: how does average case guarantee 1 T ∑ t = 1 T − ℓ − ( y f W ( δ t ) ) ≤ . . .
lead to single instance guarantee − ℓ − ( y f W ( δ a t t ) ) ≤ . . .
? From the bottom of page 13, it seems to be because − ℓ − ( y f W ( δ a t t ) ) = min t ∈ [ T ] − ℓ − ( y f W ( δ t ) )
. However, why is this guaranteed? Since it is a project GD, when the step size is sufficiently small I can see it holds, but does it hold generally?
Proof of Lemma 3.7, page 14: why is it an equality in y t ∑ r = 1 m ⟨ a r σ ′ ( w r ⋅ δ ) δ , 1 m sgn ( a r ) v ∗ ⟩ = κ m ∑ r = 1 m σ ′ ( w r ⋅ δ ) y t ⟨ δ , v ∗ ⟩
? We only know a ∞ ≤ κ
but does it lead to a r = κ
for any r ?
Proof of Lemma 3.7, bottom of page 15: how does τ ≤ . . .
connect to T ≤ . . .
? If I understand correctly, we need T ≥ ⌈ τ ⌉ + 1
to lead to contradiction. But here, RHS of τ ≤ . . .
seems to be squared which may not imply T ≥ ⌈ τ ⌉ + 1 .
How does this convergence-guaranteed adversarial training of two-layer DNN compare to exact-convex-opt-solving-based adversarial training (e.g., [1])?
[1] Bai, Yatong, Tanmay Gautam, and Somayeh Sojoudi. "Efficient global optimization of two-layer relu networks: Quadratic-time algorithms and adversarial training." arXiv preprint arXiv:2201.01965 (2022). Minor:
After Definition 3.1, "in terms of the and the negated loss derivative": missing noun
After Theorem 3.2, O ( β 2 / ϵ )
iteration may need further justification
Section 4.1 may be moved to appendix to gain space for proof sketching of Thm. 3.2 and Lemma 3.7
Table 2's result is hard to parse. Why larger attack steps for RA columns can lead to significantly larger robust accuracy? Also, what does SA mean? When attach size ν
is specified in the table, how is it different from caption of Table 2 which says ν = 8 / 255 ?

Review Point: .. lead to single instance guarantee − ℓ − ( y f W ( δ a t t ) ) ≤ .
Review Point: .. ? From the bottom of page 13, it seems to be because − ℓ − ( y f W ( δ a t t ) ) = min t ∈ [ T ] − ℓ − ( y f W ( δ t ) ) . However, why is this guaranteed? Since it is a project GD, when the step size is sufficiently small I can see it holds, but does it hold generally? Proof of Lemma 3.7, page 14: why is it an equality in y t ∑ r = 1 m ⟨ a r σ ′ ( w r ⋅ δ ) δ , 1 m sgn ( a r ) v ∗ ⟩ = κ m ∑ r = 1 m σ ′ ( w r ⋅ δ ) y t ⟨ δ , v ∗ ⟩ ? We only know a ∞ ≤ κ but does it lead to a r = κ for any r ? Proof of Lemma 3.7, bottom of page 15: how does τ ≤ .
Review Point: .. ? If I understand correctly, we need T ≥ ⌈ τ ⌉ + 1 to lead to contradiction. But here, RHS of τ ≤ .
Review Point: .. seems to be squared which may not imply T ≥ ⌈ τ ⌉ + 1 . How does this convergence-guaranteed adversarial training of two-layer DNN compare to exact-convex-opt-solving-based adversarial training (e.g., [1])? [1] Bai, Yatong, Tanmay Gautam, and Somayeh Sojoudi. "Efficient global optimization of two-layer relu networks: Quadratic-time algorithms and adversarial training." arXiv preprint arXiv:2201.01965 (2022). Minor: After Definition 3.1, "in terms of the and the negated loss derivative": missing noun After Theorem 3.2, O ( β 2 / ϵ ) iteration may need further justification Section 4.1 may be moved to appendix to gain space for proof sketching of Thm. 3.2 and Lemma 3.7 Table 2's result is hard to parse. Why larger attack steps for RA columns can lead to significantly larger robust accuracy? Also, what does SA mean? When attach size ν is specified in the table, how is it different from caption of Table 2 which says ν = 8 / 255 ?
==================================================

Focused review:

Weaknesses: The authors lack experiments for fair comparisons with the considered baselines. In addition, there is a gap between the proposed simplified operator and its implementation regarding sampling data. Reproducibility could not be well verified. Investigation into the multiple optimal policies was not considered. Please see the following comments and questions.
C1. Regarding a fair comparison with MA2QL
The MA2QL paper states that “for a fair comparison, the total number of Q-function updates for each agent in MA2QL is set to be the same with that in IQL.” Is this rule explicitly applied to the conducted experiments? Please clarify the (fine-tuned) training steps for updating an agent at each turn in every environment.
In Fig.4, there are three easy SMAC environments and one hard environment (2c_vs_64zg). In MA2QL paper, three other environments are considered(3s_vs_4z (easy), 5m_vs_6m (hard) and corridor (super hard)). Among the three environments, MA2QL shows superiority over IQL in the latter two environments. For a fair comparison with MA2QL, SMAC experiments in those three environments should be performed.
In Multi-agent Mujoco, BQL only considered two-agent cases in the partially observable setting, which seems an empirical limitation of this paper. On the other hand, MA2QL conducted experiments on 2-agent HalfCheetah, 3-agent Hopper, and partially observable 3-agent Walker2d in multi-agent MuJoCo. For a fair comparison with MA2QL, Multi-agent Mujoco experiments in those three environments are required.
C2. Regarding a fair comparison with H-IQL
As far as I understand, the H-IQL method represents the equation in Appendix A. If yes, then the authors should report the (fine-tuned) value of λ ( < 1 ) .
C3. Regarding sampling from the replay buffer
In my opinion, the (hidden) key part of BQL is how to sample data as well as the formulation itself in Eqs. 9 and 10. There is a gap between Eq. 7 and Eq. 9 (with one replay buffer) regarding sampling data. The authors explicitly need any method to reduce the gap. For example, I think the minibatch should contain samples from “similar” transitions. Partitioning one buffer into several parts in chronological order, and choosing one part of them for sampling can apply to Eq. 7. (Note that this partitioning is different from using buffer series in Appendix B since the former uses one buffer, but just changes the sampling method inside the same buffer.)
Can we guarantee that D i
sufficiently goes through all possible transition probabilities? (page 6)
On page 5 and Appendix B, the authors claim the sample efficiency of Eq. 7 over Eq. 6. While Eq. 7 is practical since we ideally require samples from every π − i
to compute Eq. 6. However, other than that, the explanation in the second paragraph on page 13 is not clearly understood, especially the complexity of Eq.6. In my opinion, this additional explanation requires clarification.
C4. Regarding multiple optimal policies & scalability
We can extend the matrix game on page 8 to observe what happens to BQL when there are multiple optimal policies. We can also check whether the proposed approach in Appendix D is practically applicable in this case.
Another promising experiment can be a scalability test on the SMAC environment (e.g., 10m vs 11m, 27m vs 30m, bane_bane) since there may be a relatively higher possibility of multiple optimal policies when the number of (homogeneous) agents increases. This SMAC test can also strengthen the scalability of BQL. (I checked Appendix C.2.)
C5. Regarding reproducibility
I could not check the reproducibility of the experiments as the authors did not provide their implementation codes.
Why are the performance of IQL in 2c_vs_64zg and 1c3s5z much higher than those in *Samvelyan et al. (2019)? In the paper, the win rates are 7% and 21%, respectively, while in this paper, about 50% and 70%, respectively. Did the authors tune the IQL code? * Samvelyan et al., The StarCraft Multi-Agent Challenge, 2019.
For clarity, do the BQL, H-IQL, and IQL use synchronized sampling (i.e., each agent samples the same episodes) or not?
On page 3, does “ Q i 0
is initialized to be the minimal return” mean Q i 0 = − r m i n
? Does this initialization is applied in Algorithms 1 and 2? (I could not find any explicit mention regarding this.)
Are the proofs of Lemma 2, 3, and 4 available for stochastic π − i
? For example, the first inequality in Lemma 2 may be valid for P i ∗
under stochastic π − i
. If yes, is that a reason for considering only deterministic policies?
On page 5, can the authors clarify how to select “a subset of state S i m ”?
Instead of Eq.10, can we directly apply the max operation as seen in Eq. 8?
Could you explain “the positive random noise of Q i
”? (page 6) The necessity of λ
in Eq. 10 is empirically appealing, but less conceptually.
Regarding independent SAC in Appendix C.3
How can BQL be applied to SAC as (i) SAC contains entropy term and (ii) the considered policy is stochastic, unlike deterministic in DDPG? This part should be clarified with the explicit formulation.
Does independent SAC use two Q functions for each agent? Two Q ¯ i e
functions for each agent?
What is the base code for implementing independent SAC?
What is the target update period or coefficient in Line 8 of Algorithm 2?
On page 8, it seems that “the narrow region r > 3
is surrounded by the region with r = 0
” is written in reverse order.
Is there any explanation regarding the fine-tuned value of λ
increasing as the environment becomes more complicated? Minor: m a x P i ( s ′ s , a ) → m a x P i ( ⋅ s , a )
in Eq.6.

Review Point: 9 and 10. There is a gap between Eq. 7 and Eq. 9 (with one replay buffer) regarding sampling data. The authors explicitly need any method to reduce the gap. For example, I think the minibatch should contain samples from “similar” transitions. Partitioning one buffer into several parts in chronological order, and choosing one part of them for sampling can apply to Eq.
Review Point: 7. (Note that this partitioning is different from using buffer series in Appendix B since the former uses one buffer, but just changes the sampling method inside the same buffer.) Can we guarantee that D i sufficiently goes through all possible transition probabilities? (page 6) On page 5 and Appendix B, the authors claim the sample efficiency of Eq. 7 over Eq.
Review Point: 6. While Eq. 7 is practical since we ideally require samples from every π − i to compute Eq.
Review Point: 6. However, other than that, the explanation in the second paragraph on page 13 is not clearly understood, especially the complexity of Eq.6. In my opinion, this additional explanation requires clarification.
==================================================

Focused review:

Weakness 1. Although the method is kind of new, I feel there are a lot of redundance in the model design and many assumptions or modules may not be necessary. For example, the node representations do not need to be a Gaussian distribution, they can be just deterministic embeddings and it can make the inference easier and saves much computational cost. Only \pi and t_u need to be distributions, and they may be not impacted if nodes are not distributions. 2. It is not necessary to select the global similar nodes. We can just cache the similarity \phi(z_u, \hat{z}_v) in each iteration. After all, only the labels of these global nodes are used, their embeddings are only used to calculate the similarity to the target nodes. 3. I did not check the correctness of (6), but it is important. The ELBO inference process should be described in Appendix.

Review Point: 1. Although the method is kind of new, I feel there are a lot of redundance in the model design and many assumptions or modules may not be necessary. For example, the node representations do not need to be a Gaussian distribution, they can be just deterministic embeddings and it can make the inference easier and saves much computational cost. Only \pi and t_u need to be distributions, and they may be not impacted if nodes are not distributions.
Review Point: 2. It is not necessary to select the global similar nodes. We can just cache the similarity \phi(z_u, \hat{z}_v) in each iteration. After all, only the labels of these global nodes are used, their embeddings are only used to calculate the similarity to the target nodes.
Review Point: 3. I did not check the correctness of (6), but it is important. The ELBO inference process should be described in Appendix.
==================================================

Focused review:

Weaknesses
1 Generalization. The proposed method could only be used to explain a specific kind of NN, which learn a mapping starting from an extracted feature to an attribute-related label embedding. It is hard to explain general pre-trained neural networks (e.g., ResNet, GNN, Transformers).
2 Partial explainable. The whole model has three parts, feature extractor, added learnable mapping, and attribute to label mapping; only the middle part can be explained, and the best part still performs as a black box.
3 Fidelity issue. The method treats not the true label Y but an attribute set as a target during training. How to guarantee the GPT-3 described attributes are useful features for prediction? If you directly train a model using true label Y, they use different features during the decision. So the explained attributes do friendly to human understanding but may not follow the original model's logic.
4 Need to improve accuracy results. The final accuracy is lower and has a relatively large gap compared with the original model, even though the proposed method uses more learnable parameters. These results also aligned with point 3 above. The newly learned explainable mapping does not use the same logic as the original model.
5 Accumulated bias. There may be some bias in the feature extractor model (Resnet) and GPT-3. The new trained mapping can not remove those biases if they are input and target.

Review Point: 1 Generalization. The proposed method could only be used to explain a specific kind of NN, which learn a mapping starting from an extracted feature to an attribute-related label embedding. It is hard to explain general pre-trained neural networks (e.g., ResNet, GNN, Transformers).
Review Point: 2 Partial explainable. The whole model has three parts, feature extractor, added learnable mapping, and attribute to label mapping; only the middle part can be explained, and the best part still performs as a black box.
Review Point: 3 Fidelity issue. The method treats not the true label Y but an attribute set as a target during training. How to guarantee the GPT-3 described attributes are useful features for prediction? If you directly train a model using true label Y, they use different features during the decision. So the explained attributes do friendly to human understanding but may not follow the original model's logic.
Review Point: 4 Need to improve accuracy results. The final accuracy is lower and has a relatively large gap compared with the original model, even though the proposed method uses more learnable parameters. These results also aligned with point 3 above. The newly learned explainable mapping does not use the same logic as the original model.
Review Point: 5 Accumulated bias. There may be some bias in the feature extractor model (Resnet) and GPT-3. The new trained mapping can not remove those biases if they are input and target.
==================================================

Focused review:

Weaknesses
[Working mechanism needs more discussion]
The proposed Domain-Switch Learning (DSL) includes only a single domain into a training iteration and switches to another domain for the following iteration. The work highlights several times that such a way makes “the deep model favors domain-general knowledge and is prone to ignoring the domain-specific knowledge”. This point is not very clear and needs more validations.
First, the domain-mix pipeline learns images from different domains in each iteration. In order to learn well from these images, the model tends to learn common information. This also makes the model favors domain-general knowledge. Thus, it is still not clear why domain-switch learning should be better than domain-mix learning.
Second, In Sec. 3. 2 the work includes a sentence to eliminate the concern of the contradiction when using mini-ImageNet images at each iteration. However, it is still not very convincing. A. One could also regard each mixed batch in domain-mix learning as a single “compound” domain. B. Also, following the logic of the motivation, using mini-ImageNet in each iteration could make model memory the information of mini-ImageNet (which is domain-specific). Such mini-imagenet specific information might already be general, which is helpful when testing on bird/car/place images. This might be the reason why including mini-ImageNet images in each iteration helps to learn.
Third, experimental comparison between domain-switch and domain-mix needs clarification (Please see the first point in the following).
[Experimental evaluation needs clarification]
First, Table 3 needs more clarification. 1) When reporting results on CUB, does the model use four domain images (mini-ImageNet, Car, Places, Plantae) for training? 2) when using the domain-mix manner, does each mini-batch contain mini-ImageNet images? 3) one possible baseline could be first pertaining on mini-ImageNet, and then finetuning using domain-mix manner or domain-switch manner. 4) do the methods listed in Tables 1 and 2 also use mini-ImageNet image at each training iteration or just finetuning?
Second, Tables 1 and 2 list basic baselines. GNN (Tseng et al. (2020)) should be included as well. Moreover, the results of NASE and BSR under the multi-domain setting (“m” in the table) are missing.
Third, when using the teacher network, why BKLD is better (as shown in Fig4 (a))? It would be better if this work could provide a discussion on this. Minor:
[-] The captions of Tables 1 and 2 are not clear. The definitions of TF, ATA, s, m are missing. [-] Summarization of the datasets (domains) used in the experiment could be included in the appendix.
---Post Rebuttal---
First, the major concern of "working mechanism" is well resolved by the clarification. The authors tune down their claim and clarify that "The superiority of domain-switch (without teacher or prompter) is not because it learns the domain-general knowledge better. Instead, it is because domain-switch learner better suppresses the domain-specific knowledge". This is reasonable and much clearer.
One additional comment is that the introduction does not fully highlight this point. The corresponding text should be modified.
Second, the evaluation is well clarified. One suggestion is the main paper should clearly mention the experiments in the appendix. These results are helpful to understand the effectiveness of the proposed method.
In addition, I notice that the experimental results on the newly-introduced test sets (Aircraft and Traffic Sign) also show the effectiveness of DSL. It would be better if the authors could add them in the revised paper (e.g., appendix), which helps understand the effectiveness of DSL.
Based on the above points, I am inclined to accept this paper.

Review Point: 3.2 the work includes a sentence to eliminate the concern of the contradiction when using mini-ImageNet images at each iteration. However, it is still not very convincing. A. One could also regard each mixed batch in domain-mix learning as a single “compound” domain. B. Also, following the logic of the motivation, using mini-ImageNet in each iteration could make model memory the information of mini-ImageNet (which is domain-specific). Such mini-imagenet specific information might already be general, which is helpful when testing on bird/car/place images. This might be the reason why including mini-ImageNet images in each iteration helps to learn. Third, experimental comparison between domain-switch and domain-mix needs clarification (Please see the first point in the following). [Experimental evaluation needs clarification] First, Table 3 needs more clarification.
Review Point: 1) When reporting results on CUB, does the model use four domain images (mini-ImageNet, Car, Places, Plantae) for training?
Review Point: 2) when using the domain-mix manner, does each mini-batch contain mini-ImageNet images?
Review Point: 3) one possible baseline could be first pertaining on mini-ImageNet, and then finetuning using domain-mix manner or domain-switch manner.
==================================================

Focused review:

Weaknesses:
- The paper is a bit incremental. Basically, knowledge distillation is applied to object detection (as opposed to classification as in the original paper).
- Table 4 is incomplete. It should include the results for all four datasets.
- In the related work section, the class of binary networks is missing. These networks are also efficient and compact. Example papers are:
* XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, ECCV 2016
* Binaryconnect: Training deep neural networks with binary weights during propagations, NIPS 2015
Overall assessment: The idea of the paper is interesting. The experiment section is solid. Hence, I recommend acceptance of the paper.

Review Point: - The paper is a bit incremental. Basically, knowledge distillation is applied to object detection (as opposed to classification as in the original paper).
Review Point: - Table 4 is incomplete. It should include the results for all four datasets.
Review Point: - In the related work section, the class of binary networks is missing. These networks are also efficient and compact. Example papers are:
Review Point: * XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, ECCV 2016 * Binaryconnect: Training deep neural networks with binary weights during propagations, NIPS 2015 Overall assessment: The idea of the paper is interesting. The experiment section is solid. Hence, I recommend acceptance of the paper.
==================================================

Focused review:

weaknesses
There are some paragraphs that are poorly written, and make some key concepts difficult to understand.
There are some conceptual gaps that make me doubt about the overall rigor of the paper.
The more complex M U S T a d a p t i v e
model is only slightly better than the simpler M U S T u n i f o r m
baseline as measured by the in-domain automatic and human evaluations.
Suggestions and questions for the authors.
Good work! Do you intend to release the code in case the paper is accepted?
In the second paragraph of page 2 you mention that "Extensive experimental results [...] show that the dialogue system trained by our proposed MUST achieves a better performance than those trained by any single user simulator", however I feel like the amount of experimentation described is not extensive, so I would suggest toning down this statement.
In page 4 you introduce M U S T C R L
but you don't include that model in any of your results. In the paragraph "Challenges to leverage multiple user simulators" you mention some challenges related to this model. Are these challenges the reason why you did not analyze it? If that is the case then I suggest not giving it its own item (II) above, and instead give an overview of the "adaptive" variant of your framework. If this is not the case, then why was it not included?
In the same "Challenges to leverage multiple user simulators." paragraph you mention: "unnecessary efforts will be costed for easily-adapted user simulators". I suggest rephrasing this statement as I don't think it's understandable in its current form.
In the first paragraph of page 5 you link the "uniform adaptation" to reducing the catastrophic forgetting issue. However there is no experiment providing evidence that without "uniform forgetting" there's catastrophic forgetting. I suggest rephrasing this, or providing evidence that this is actually the case.
In the first paragraph of section 5.2.2 you mention: " S y S − M U S T m e r g i n g
is trained by G P T I L
for implementing M U S T m e r g i n g
strategy". First of all the statement is redundant by mentioning the "merging" strategy twice, but more importantly, I had understood that the merging strategy implied sampling dialogues from a set of user simulators and use these to train the final model. Does this statement mean that you generated dialogues with G P T I L
, which was trained on dialogues produced by 3 agenda-based simulators and MultiWoZ restaurant data, and define this method as sampling dialogues from several simulators?
In the "Automatic Evaluation" paragraph of section 5.3 you say that the reason for the "merging" strategy not performing as well as the "uniform" and "adaptive" could be "because the merging strategy cannot effectively leverage multiple user simulators" which is a cyclical explanation. I suggest removing it or providing a more plausible explanation.
In the third paragraph of section 5.3 you mention that the "uniform" and "adaptive" strategies achieve 2.4 absolute value improvements", but improvement over what?
Finally, I suggest providing more details about the human evaluation. In Appendix B.2 you give some more details and mention that you "tell them how to judge if the generated dialogue is successful", "them" being the evaluators. But what was your definition of a successful dialogue? Every slot filled, for example? or something else?
Typos and minor corrections
Page 1, paragraph 4, line 3: best-performed -> best-performing
P. 2, p. 1, l. 10: You reference challenges i and ii
but you used 1 and 2 in the same paragraph for describing challenges. In the abstract you use i and ii
for referring to the two types of adaptation rather than the challenges they address. I suggest being consistent with numbering.
P. 2, p. 2, l. 5: Here you mention " M U S T a d a p t i v e
is indeed more efficient for leveraging multiple user simulators by our visualization analysis", which makes me wonder: more efficient than what? and what visualization analysis do you mean? I suggest clarifying this in the paper.
P. 2, p. 3, l. 8: convergences -> converges
P. 2, p. 6, l. 4: if accomplishing -> if it is accomplishing
P. 3, p. 1, l. 1: Once the database result -> When the database result
P. 4, p. 2, l. 1: first sample -> first samples
P. 4, p. 2, l. 4: by RL algorithms -> with RL
P. 4, p. 4, l. 11: Not sure what is meant by "unnecessary efforts will be costed for easily-adapted user simulators". I suggest rephrasing this.
P. 4, p. 6, l. 1: recalls us a similar thought -> reminds us of a similar concept
P. 4, p. 6, l. 3: weakly-performed -> weakly-performing; well-performed -> well-performing
P. 4, p. 6, l. 4: "should reduce the interaction with user simulators that dialogue system has performed well and allocate more interactions with those user simulators that dialogue system has not performed well" -> "should reduce the interaction with user simulators with which the dialogue system has performed well and increase interactions in the opposite case."
P. 5, p. 2, l. 5: masker's -> maker's
P. 5, p. 4, l. 7: "p is expected to assign lower weights to user simulators that the system agent S already performs well and higher weights to those user simulators that S performs not well" -> "p is expected to assign lower weights to user simulators with which the system agent S already performs well and higher weights to those user simulators with which S performs poorly".
P. 5, p. 7, l. 2: what do you mean with "latter" and "former" terms? Do you mean $\underbrace{\bar{x}j}{\text{exploitation}} + \underbrace{\sqrt{\frac{2\ln t}{T_{j,t}}}}_{\text{exploration}}$ in equation 2?
P. 5, p. 8, l. 2: has been interacted so far -> has been interacted with so far
P. 5, footnote, l. 2: "Then the index of the arm will be played from t = K + 1 to T is the sum of two terms: ..." makes no grammatical sense. Please correct it.
P. 6, Algorithm 1: I suggest changing the verbs from -ing form to imperative, so initializing -> initialize, synthesizing -> synthesize, using -> use, evaluating -> evaluate, updating -> update. I also suggest clarifying what the lowercase s mentioned in the input is used for.
P. 6, p. 1, l. 2-3: I think here you used both τ and s
to refer to the smoothing factor for distribution p
P. 6, p. 3, l. 3: "that the dialogue system has performed well" -> "with which the dialogue system has performed well"
P. 7, p. 4, l. 4: "test the systems by them" -> "test the systems with them"
P. 7, p. 4, l. 5: "there usually has a gap" -> "there usually is a gap"
P. 9, Figure 2: There's no need to write "Tested by" for each subfigure. The name of the user simulator is enough. Also the label for (a) seems to be wrong.
P. 14: There's mention to U-GPT here, but nowhere in the main text.

Review Point: 10: You reference challenges i and ii but you used 1 and 2 in the same paragraph for describing challenges. In the abstract you use i and ii for referring to the two types of adaptation rather than the challenges they address. I suggest being consistent with numbering. P. 2, p. 2, l.
Review Point: 5: Here you mention " M U S T a d a p t i v e is indeed more efficient for leveraging multiple user simulators by our visualization analysis", which makes me wonder: more efficient than what? and what visualization analysis do you mean? I suggest clarifying this in the paper. P. 2, p. 3, l.
Review Point: 4: if accomplishing -> if it is accomplishing P. 3, p. 1, l.
Review Point: 1: Once the database result -> When the database result P. 4, p. 2, l.
Review Point: 1: first sample -> first samples P. 4, p. 2, l.
Review Point: 4: by RL algorithms -> with RL P. 4, p. 4, l.
Review Point: 11: Not sure what is meant by "unnecessary efforts will be costed for easily-adapted user simulators". I suggest rephrasing this. P. 4, p. 6, l.
Review Point: 1: recalls us a similar thought -> reminds us of a similar concept P. 4, p. 6, l.
Review Point: 3: weakly-performed -> weakly-performing; well-performed -> well-performing P. 4, p. 6, l.
Review Point: 2: what do you mean with "latter" and "former" terms? Do you mean $\underbrace{\bar{x}j}{\text{exploitation}} + \underbrace{\sqrt{\frac{2\ln t}{T_{j,t}}}}_{\text{exploration}}$ in equation 2? P. 5, p. 8, l.
Review Point: 2: has been interacted so far -> has been interacted with so far P. 5, footnote, l.
Review Point: 2: "Then the index of the arm will be played from t = K + 1 to T is the sum of two terms: ..." makes no grammatical sense. Please correct it. P. 6, Algorithm 1: I suggest changing the verbs from -ing form to imperative, so initializing -> initialize, synthesizing -> synthesize, using -> use, evaluating -> evaluate, updating -> update. I also suggest clarifying what the lowercase s mentioned in the input is used for. P. 6, p. 1, l. 2-3: I think here you used both τ and s to refer to the smoothing factor for distribution p P. 6, p. 3, l.
Review Point: 14: There's mention to U-GPT here, but nowhere in the main text.
==================================================

Focused review:

The paper doesn't seem to have any glaring weaknesses. The main concern might be tied to the functioning of the generative model, in particular the fact that it relies on sequential multi-step generation with a number of steps that in this particular case is rather high (1000 steps). While this gives some flexibility to the model (as shown by the conditioning experiments at different latent steps), this also raises the question of whether the fact of having to sequentially run hundreds of steps to obtain a sample might not be excessively slow for practical applications. - Providing more details on the neural network parametrization of the reverse process (in particular how position embeddings are being provided) could help reproducibility. On the other hand, provide the code of their implementation, which is as much as one can ask in terms of reproducibility.

Review Point: - Providing more details on the neural network parametrization of the reverse process (in particular how position embeddings are being provided) could help reproducibility. On the other hand, provide the code of their implementation, which is as much as one can ask in terms of reproducibility.
==================================================

Focused review:

1. As the paper claimed, the uncertainty is modelled based on individual reward function and transition probability. It seems ref [12] has the same definition of reward and transition function (eq.3 in ref[12]), but they do not use the description like ‘nature’ agent and they use minmax to formalize the objective. This paper looks like another application of [12] and use a different optimizing trick. As such, it is critical to provide detailed discussion about the difference between the proposed method and ref [12]. 2. Introducing MADDPG (section 3.2) to optimize the hard objective is certainly of interest. The paper gives the gradient of policy. However, it might be better to give the theoretical guarantee of the discrepancy between the learned policy and true policy as well as the equilibrium guarantee by using such an optimization framework (MADDPG). 3. It might be better to report standard errors in tables. Experiment part lacks details (like the number of agents etc.) and experimental environments are too simple to support the effectiveness of your method. It is better to extend the experiment to more complex environments with multiple kinds of uncertainty.

Review Point: 1. As the paper claimed, the uncertainty is modelled based on individual reward function and transition probability. It seems ref [12] has the same definition of reward and transition function (eq.3 in ref[12]), but they do not use the description like ‘nature’ agent and they use minmax to formalize the objective. This paper looks like another application of [12] and use a different optimizing trick. As such, it is critical to provide detailed discussion about the difference between the proposed method and ref [12].
Review Point: 2. Introducing MADDPG (section 3.2) to optimize the hard objective is certainly of interest. The paper gives the gradient of policy. However, it might be better to give the theoretical guarantee of the discrepancy between the learned policy and true policy as well as the equilibrium guarantee by using such an optimization framework (MADDPG).
Review Point: 3. It might be better to report standard errors in tables. Experiment part lacks details (like the number of agents etc.) and experimental environments are too simple to support the effectiveness of your method. It is better to extend the experiment to more complex environments with multiple kinds of uncertainty.
==================================================

Focused review:

1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance "I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558).
I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria?
1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation?
2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant.
3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching).

Review Point: 1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance "I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification?
Review Point: - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558). I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
Review Point: 2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria?
Review Point: 1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation?
Review Point: 2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant.
Review Point: 3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching).
==================================================

Focused review:

The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper.
- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?

Review Point: - You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
Review Point: - Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?
==================================================

Focused review:

Weakness: - [Major]: I have certain reservations about the novelty of the method. The high-level general idea is directly borrowed from the language literature and adapted to the graph transformer. The technical novelty is limited, however I appreciate the authors in verifying that the idea works for graph transformers. It would be beneficial to add some insights early on in the paper, on why designing prompting techniques for graph transformers is challenging.
- Can the authors provide ablations with different pre-training strategies for graph transformers?

Review Point: - [Major]: I have certain reservations about the novelty of the method. The high-level general idea is directly borrowed from the language literature and adapted to the graph transformer. The technical novelty is limited, however I appreciate the authors in verifying that the idea works for graph transformers. It would be beneficial to add some insights early on in the paper, on why designing prompting techniques for graph transformers is challenging.
Review Point: - Can the authors provide ablations with different pre-training strategies for graph transformers?
==================================================

Focused review:

I have two major concerns: 1. Similar to EXTRA, the inner loop of IDEAL also needs the communications between different nodes. So the communication cost and the computation cost of IDEAL+AGD should be the same in Table 2. In other words, the communication cost also hides the log factor. As a comparison, SSDA does not need communications in the inner loop. Thus, I wonder whether the communication cost is lower than that of SSDA in practice. I suggest the authors to plot the communication cost and computation cost separately. 2. I wonder whether SSDA+AGD+warm start can achieve the same computation cost by using the proof technique proposed in this paper. If it is, please clarify that the higher computation cost of SSDA is just beacuse the weakness of the proof, rather than the method itselt. If it is not, please give some details in the supplementary material, for example, the analysis of the computation cost of SSDA+AGD+warm start. It might be better to explain why IDEAL+AGD+warm start is theoretically better than SSDA+AGD+warm start (I guess it is because of rho=0 in SSDA?). Then, it may support the superioty of the proposed method. Minor comments: 1. line 113. The best rate for non-accelerated method is O( (kappa_f + kappa_w)log(1/eps) ). Xu, J., Tian, Y., Sun, Y., and Scutari, G. (2020). Distributed algorithms for composite optimization: Unified and tight convergence analysis. arXiv preprint arXiv:2002.11534. Li, H. and Lin, Z. Revisiting extra for smooth distributed optimization. SIAM Journal on Optimization, 30(3):1795-1821, 2020. arXiv:2002.10110. 2. The citation on line 506 may be incorrect. 3. Please give more details for the analysis of IDEAL+AGD, for example, why choose rho=L/lambda_max. 4. It might be better to give the rate for IDEAM+SVRG 5. It might be better to give the explicit log cost in Table 2, even if it is a constant of O( log (kappa_f kappa_W) ).

Review Point: 1. Similar to EXTRA, the inner loop of IDEAL also needs the communications between different nodes. So the communication cost and the computation cost of IDEAL+AGD should be the same in Table 2. In other words, the communication cost also hides the log factor. As a comparison, SSDA does not need communications in the inner loop. Thus, I wonder whether the communication cost is lower than that of SSDA in practice. I suggest the authors to plot the communication cost and computation cost separately.
Review Point: 2. I wonder whether SSDA+AGD+warm start can achieve the same computation cost by using the proof technique proposed in this paper. If it is, please clarify that the higher computation cost of SSDA is just beacuse the weakness of the proof, rather than the method itselt. If it is not, please give some details in the supplementary material, for example, the analysis of the computation cost of SSDA+AGD+warm start. It might be better to explain why IDEAL+AGD+warm start is theoretically better than SSDA+AGD+warm start (I guess it is because of rho=0 in SSDA?). Then, it may support the superioty of the proposed method. Minor comments:
Review Point: 1. line 113. The best rate for non-accelerated method is O( (kappa_f + kappa_w)log(1/eps) ). Xu, J., Tian, Y., Sun, Y., and Scutari, G. (2020). Distributed algorithms for composite optimization: Unified and tight convergence analysis. arXiv preprint arXiv:2002.11534. Li, H. and Lin, Z. Revisiting extra for smooth distributed optimization. SIAM Journal on Optimization, 30(3):1795-1821, 2020. arXiv:2002.10110.
Review Point: 3. Please give more details for the analysis of IDEAL+AGD, for example, why choose rho=L/lambda_max.
Review Point: 4. It might be better to give the rate for IDEAM+SVRG 5. It might be better to give the explicit log cost in Table 2, even if it is a constant of O( log (kappa_f kappa_W) ).
==================================================

Focused review:

Weaknesses:
My complaints most come from the writing and illustration part. Please see the detailed comments as follows.
Figure 1: what does the solid curve in policy/reward spaces mean? What about the dotted angles? The authors are encouraged to elaborate more on the illustration.
“For instance, ψ t + 1
can represent a softmax policy for a discrete space, ...”: is ψ t + 1
a regularized reward function or a policy? I am assuming that it should be a reward function.
Equation 5: It is a maximization problem of π
but I cannot find π
in the objective. Also, I am not clear how the form of J Ω ( π t , ψ t + 1 )
is derived in equation 5, and how it is related to equation 2. The authors are encouraged to elaborate more to make it clear.
Below equation 8: Is π ∗
a function of t
? What do the authors mean by “the expectation is taken over the entire steps”? How is the optimality condition E [ . . . ] = 0
is derived? I am confused about the whole paragraph.
Theorem 1: When will the assumptions on Ω
in theorem 1 be satisfied? Are there any examples? I am thinking about KL but it does not satisfy the conditions here. Please correct me if I am wrong.
Theorem 2: The last inequalities involve s
. What is this s here?
Proof of Theorem 2: what is n
below equation 33? I am not sure about the relation between f t and D Ω
throughout the proof.
Proof of Lemma 6: The authors state "Assuming all states are reachable". Is this an assumption of the results? What if such a statement does not hold?
Minor issues:
Section 8: functinos -> functions
Above Lemma 1: greed manner -> greedy manner
Below Figure 1: a update -> an update
Equation 7: gradient of D Ω
is a bit misleading; the authors should mention that the gradient is taken w.r.t. the first argument.
Equation 8: Ω ( π ( ⋅ s i ) ) -> Ω ( π t ( ⋅ s i ) )

Review Point: .. ] = 0 is derived? I am confused about the whole paragraph. Theorem 1: When will the assumptions on Ω in theorem 1 be satisfied? Are there any examples? I am thinking about KL but it does not satisfy the conditions here. Please correct me if I am wrong. Theorem 2: The last inequalities involve s . What is this s here? Proof of Theorem 2: what is n below equation 33? I am not sure about the relation between f t and D Ω throughout the proof. Proof of Lemma 6: The authors state "Assuming all states are reachable". Is this an assumption of the results? What if such a statement does not hold? Minor issues: Section 8: functinos -> functions Above Lemma 1: greed manner -> greedy manner Below Figure 1: a update -> an update Equation 7: gradient of D Ω is a bit misleading; the authors should mention that the gradient is taken w.r.t. the first argument. Equation 8: Ω ( π ( ⋅ s i ) ) -> Ω ( π t ( ⋅ s i ) )
==================================================

Focused review:

Weakness: 1. The experiments are all single-dimensional. I'm not sure whether the proposed method works for high-dimensional x. If it works, it is better to include some high-dimensional experiments. If it doesn't work, why? 2. A new training point is added to the training set only if the variance of prediction is smaller than some pre-defined threshold (Line 14 in Algorithm 1). I wonder is there any case that no data points satisfy the criteria? If so, is there any possibility that the algorithm will stuck when sigma are too small? If not, is there any theoretical guarantee? Thanks!

Review Point: 1. The experiments are all single-dimensional. I'm not sure whether the proposed method works for high-dimensional x. If it works, it is better to include some high-dimensional experiments. If it doesn't work, why?
Review Point: 2. A new training point is added to the training set only if the variance of prediction is smaller than some pre-defined threshold (Line 14 in Algorithm 1). I wonder is there any case that no data points satisfy the criteria? If so, is there any possibility that the algorithm will stuck when sigma are too small? If not, is there any theoretical guarantee? Thanks!
==================================================

Focused review:

- In section 3.1, the logic of extending HOGA from second order is not consistent with the extension from first order to second order; i.e., second order attention creates one more intermediate state U compared to the first order attention module. However, from the second order to higher order attention module, although intermediate states U0, U1 … are created, they are only part of the intermediate feature (Concatenating them will form U with full channel resolution). In this way, it seems we could regard the higher order attention module as a special form of second order attention module. - The paper does not clearly explain the intuition as to why different channel groups should have different attention mechanisms; i.e., in what specific way the network can benefit from the proposed channel group specific attention module. - Experiments are not solid enough: 1. There are no ablation studies on the effect of parameter numbers, so it is not clear whether the performance gain is due to the proposed approach or additional parameters. 2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152. 3. There are no results using strong object detection frameworks; the current SSD framework is relatively weak (e.g. Faster RCNN would be a stronger, more standard approach); it is not clear whether the improvements would be retained with a stronger base framework. - The proposed approach requires larger FLOPS compared to baselines; i.e., any performance gain requires large computation overhead (this is particularly pronounced in Table 3). - In Table 3 shows ResNet32/56 but L222 refers to ResNet34/50, which is confusing.

Review Point: - In section 3.1, the logic of extending HOGA from second order is not consistent with the extension from first order to second order; i.e., second order attention creates one more intermediate state U compared to the first order attention module. However, from the second order to higher order attention module, although intermediate states U0, U1 … are created, they are only part of the intermediate feature (Concatenating them will form U with full channel resolution). In this way, it seems we could regard the higher order attention module as a special form of second order attention module.
Review Point: - The paper does not clearly explain the intuition as to why different channel groups should have different attention mechanisms; i.e., in what specific way the network can benefit from the proposed channel group specific attention module.
Review Point: 1. There are no ablation studies on the effect of parameter numbers, so it is not clear whether the performance gain is due to the proposed approach or additional parameters.
Review Point: 2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152.
Review Point: 3. There are no results using strong object detection frameworks; the current SSD framework is relatively weak (e.g. Faster RCNN would be a stronger, more standard approach); it is not clear whether the improvements would be retained with a stronger base framework.
Review Point: - The proposed approach requires larger FLOPS compared to baselines; i.e., any performance gain requires large computation overhead (this is particularly pronounced in Table 3).
Review Point: - In Table 3 shows ResNet32/56 but L222 refers to ResNet34/50, which is confusing.
==================================================

Focused review:

- The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model. - The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines.
- I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers?

Review Point: - The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model.
Review Point: - The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines.
Review Point: - I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers?
==================================================

Focused review:

**Exposition** - I think the paper contains interesting ideas with good empirical results. However, the exposition of the method is not easy to follow and require significant revision. Here are a couple of examples that were unclear. - L6: “coherent HOI.” What does it mean to have “coherent HOI”? What are the incoherent ones? - L8: “transformations between human-object pairs.” The “transformation” is vague. Later in the paper, it turns that this is merely replacing instance-level (human or object) from similar HOI examples. The exposition is unnecessarily complicated. - The analogy between HOI analysis and Harmonic analysis is interesting at first glance, but the link is quite weak. In the problem contexts, there is only two “basis” (human and object) to form an HOI. The decomposition/integration steps introduced in this paper also do not have a close connection with the Fourier analysis as claimed. - On L33, what does the “eigen” structure of HOI mean? - On L51, “IDN can learn to represent the interaction/verb with T_I and T_D.” What does this mean? - On L205, I was not able to follow the concept of Interactive validity. There is no definition of these loss terms and no figures to illustrate this part. - Figure 2: o What does “X” mean? o g_h and g_o are not discussed. Later I found that this is just identity (swapping instance features) - Figure 3: o (a) Please specify the loss terms here. o (b) I know that the f_u^{v_i} is predicted from the concatenated feature f_h and f_o (the integration step). However, for the decomposition step, why not use f_u as input (as discussed in Eqn 1) and predict f_h and f_o? - When using the autoencoder for compressing the features f_h + f_o, isn’t that the encoded features already are “integrated”? How can we “slice” the features to get individual features? **Novelty** - The inter-pair transformation idea has been exploited in [A]. The paper should cite and discuss the differences with respect to [A] (as it was published before the NeurIPS submission). [A] Detecting Human-Object Interactions via Functional Generalization. AAAI 2020 **Method** - The proposed approach seems to require a much larger model size. For example, the method needs two (T_I and T_D) two-layer fully connected networks for *each* verb interaction. This is certainly not scalable and can be slow at test time. For example, for HICO-DET, this requires evaluating the T_I and T_D 117 times. Unfortunately, the paper did not discuss the model size and runtime performance. At least this should be discussed as a limitation. **Evaluation** - In Table 4, which dataset is this conducted on? It seems to me that this is done on the *testing set* of the HICO-DET dataset. The ablation should be done in the validation set without seeing the testing set. This may suggest that all the model tuning may also be conducted on the testing datasets, which may lead to overfitting.

Review Point: **Exposition** - I think the paper contains interesting ideas with good empirical results. However, the exposition of the method is not easy to follow and require significant revision. Here are a couple of examples that were unclear.
Review Point: - L6: “coherent HOI.” What does it mean to have “coherent HOI”? What are the incoherent ones?
Review Point: - L8: “transformations between human-object pairs.” The “transformation” is vague. Later in the paper, it turns that this is merely replacing instance-level (human or object) from similar HOI examples. The exposition is unnecessarily complicated.
Review Point: - The analogy between HOI analysis and Harmonic analysis is interesting at first glance, but the link is quite weak. In the problem contexts, there is only two “basis” (human and object) to form an HOI. The decomposition/integration steps introduced in this paper also do not have a close connection with the Fourier analysis as claimed.
Review Point: - On L33, what does the “eigen” structure of HOI mean?
Review Point: - On L51, “IDN can learn to represent the interaction/verb with T_I and T_D.” What does this mean?
Review Point: - On L205, I was not able to follow the concept of Interactive validity. There is no definition of these loss terms and no figures to illustrate this part.
Review Point: - Figure 2: o What does “X” mean? o g_h and g_o are not discussed. Later I found that this is just identity (swapping instance features) - Figure 3: o (a) Please specify the loss terms here. o (b) I know that the f_u^{v_i} is predicted from the concatenated feature f_h and f_o (the integration step). However, for the decomposition step, why not use f_u as input (as discussed in Eqn 1) and predict f_h and f_o?
Review Point: - When using the autoencoder for compressing the features f_h + f_o, isn’t that the encoded features already are “integrated”? How can we “slice” the features to get individual features? **Novelty** - The inter-pair transformation idea has been exploited in [A]. The paper should cite and discuss the differences with respect to [A] (as it was published before the NeurIPS submission). [A] Detecting Human-Object Interactions via Functional Generalization. AAAI 2020 **Method** - The proposed approach seems to require a much larger model size. For example, the method needs two (T_I and T_D) two-layer fully connected networks for *each* verb interaction. This is certainly not scalable and can be slow at test time. For example, for HICO-DET, this requires evaluating the T_I and T_D 117 times. Unfortunately, the paper did not discuss the model size and runtime performance. At least this should be discussed as a limitation. **Evaluation** - In Table 4, which dataset is this conducted on? It seems to me that this is done on the *testing set* of the HICO-DET dataset. The ablation should be done in the validation set without seeing the testing set. This may suggest that all the model tuning may also be conducted on the testing datasets, which may lead to overfitting.
==================================================

Focused review:

The following weaknesses are minor: - discussion with related work can be improved, specifically w.r.t. [1]. - The evaluation dataset (PASCAL Context) is rather old. Evaluation on a more modern dataset such as the COCO panoptic dataset or ADE20k would be desirable, especially since knowledge transfer is expected to work better when there exist more unseen classes. (Pascal Context has 33 classes, COCO has more in the range of 160, ADE20k has many more but has more label ambiguities).

Review Point: - discussion with related work can be improved, specifically w.r.t. [1].
Review Point: - The evaluation dataset (PASCAL Context) is rather old. Evaluation on a more modern dataset such as the COCO panoptic dataset or ADE20k would be desirable, especially since knowledge transfer is expected to work better when there exist more unseen classes. (Pascal Context has 33 classes, COCO has more in the range of 160, ADE20k has many more but has more label ambiguities).
==================================================

Focused review:

weakness of this paper is the experimental section. For example, ConvCNP (Gordon et al., 2019) from last year’s ICLR evaluated on the following datasets: a 1D synthetic dataset, PLastiCC dataset, MNIST, SVHN, CelebA32, CelebA64, ZSMM. On the other hand, this work conducted experiments on a 1D synthetic time series data, and a “digital clock dataset” where the template is shared across train and test sets.
The author explained MNIST is “not suitable for evaluating EquivCNP with group equivariance: translation, scaling, and rotation”. I am not convinced by this argument; I think the paper can be strengthened if there is an experiment demonstrating that on a real dataset EquivCNP is beneficial.
Furthermore, the image completion results do not compare to any prior benchmark. I don’t see a reason for not comparing. I think the organization makes the paper difficult to read at times. For example, on page 5, they mentioned ρ , ϕ , ψ
, which are defined back on page 2. Maybe re-organize such that the modeled conditional distribution, in Sec 4.3, is at the top of Sec 4; similar to Gordon et al., (2019)’s paper organization.
4. Additional feedbacks
Adding equation numbers and referring to them in the text may help the readers to understand the paper better.

Review Point: 4. Additional feedbacks Adding equation numbers and referring to them in the text may help the readers to understand the paper better.
==================================================

Focused review:

Weaknesses:
The paper's convergence analysis is based on two relatively strong assumptions: 1) the analysis targets a binary classification problem and its extension to multi-class classification problems looks challenging, 2) the two classes are supposed to be linearly separable.
While I very much like the reflection idea for the inner maximization problem, this change leads to a different bi-level optimization problem from the standard min-max adversarial training problem. Therefore, while the paper's introduction sets the goal of understanding the convergence properties of robust adversarial training, the proposed framework deviates from standard robust training approaches. I am wondering what the results of the paper could imply about the convergence of standard min-max adversarial training.

Review Point: 1) the analysis targets a binary classification problem and its extension to multi-class classification problems looks challenging,
Review Point: 2) the two classes are supposed to be linearly separable. While I very much like the reflection idea for the inner maximization problem, this change leads to a different bi-level optimization problem from the standard min-max adversarial training problem. Therefore, while the paper's introduction sets the goal of understanding the convergence properties of robust adversarial training, the proposed framework deviates from standard robust training approaches. I am wondering what the results of the paper could imply about the convergence of standard min-max adversarial training.
==================================================

Focused review:

Weaknesses and Questions 1. For the distribution estimation, this paper uses three Gradient flow networks to learn different parameters. According to Section 4.3, the networks learn specific parameters for each class using different inputs (i.e., x ¯ j
). That is to say, the network F 1
will output c j j = 1 n for n
classes. However, in Line 142, the authors point that the parameter c
is shared between all classes. How to unify this c
? 2. How to update F 2
, and F 3
via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1
can be trained. 3. Some experimental details are missing. 3.1. What is the ratio of training data D t
to validation data D v
in the training stage. 3.2. What is the value of initial c
. 4. In Table 2, the metric-based baseline FEAT performs similar accuracy to the proposed method. It’s better to discuss the superiority of the method in terms of time consumption. According to Algorithm 2, the upper bound Eq. (14) simplifies the training of classifiers, but Eq. (16) is still difficult to compute. Typo: In Table 2, the description does not match the content, e.g., "Euclidean Metric" (or "Hyperbolic Metric") and "Model".

Review Point: 2. How to update F 2 , and F 3 via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1 can be trained.
Review Point: 3. Some experimental details are missing. 3.1. What is the ratio of training data D t to validation data D v in the training stage. 3.2. What is the value of initial c .
==================================================

Focused review:

weaknesses of this paper are 1) Why do sampled subgraphs (segments of the very large graph one wishes to learn) used in feature learning need to be similar in any way to the larger graph, the enormous discrepancy between their node/edge sizes notwithstanding, 2) what actual graph classification tasks did the computational experiments solve? and 3) How does the proposed method compare with prior art?

Review Point: 2) what actual graph classification tasks did the computational experiments solve? and
Review Point: 3) How does the proposed method compare with prior art?
==================================================

Focused review:

Weaknesses:
- I am curious how the performance varies quantitatively if the training "shot" is not the same as "test" shot: In realistic applications, knowing the "shot" before-hand is a fairly strong and impractical assumption.
- I find the zero-shot version and the connection to density estimation a bit distracting to the main point of the paper, which is that one can learn to produce good prototypes that are effective for few-shot learning. However, this is more an aesthetic argument than a technical one.

Review Point: - I am curious how the performance varies quantitatively if the training "shot" is not the same as "test" shot: In realistic applications, knowing the "shot" before-hand is a fairly strong and impractical assumption.
Review Point: - I find the zero-shot version and the connection to density estimation a bit distracting to the main point of the paper, which is that one can learn to produce good prototypes that are effective for few-shot learning. However, this is more an aesthetic argument than a technical one.
==================================================

Focused review:

Weaknesses
The paper has serious flaws in presentation that hinder the comprehension of the method, assumptions, and novelty. For instance, in the preliminaries section, a potential outcome is never defined, ϕ
is not defined, the notation ϕ ( ⋅ t )
is not defined, Def. 2 and the notation do(t) independent of U are highly non standard and should be explained. Given this I don't understand the motivation section of the paper.
Assumptions 5 and 6 should be discussed in more detail. It is not clear how plausible they are. Especially, assumption 6 seems to directly contradict the causal graph in Fig. 1.
To my knowledge none of the semi-synthetic datasets incorporate a proxy structure in their data generating mechanisms. Are these changed to fit the setting in this paper. Do you loose performance when the correct thing to do is simply condition on observed covariates, i.e. if there are no unobserved confounders? Suggestions
One reference on ITE with continuous treatment could be added: Bellot, Alexis, Anish Dhir, and Giulia Prando. "Generalization bounds and algorithms for estimating conditional average treatment effect of dosage." arXiv preprint arXiv:2205.14692 (2022).

Review Point: 2 and the notation do(t) independent of U are highly non standard and should be explained. Given this I don't understand the motivation section of the paper. Assumptions 5 and 6 should be discussed in more detail. It is not clear how plausible they are. Especially, assumption 6 seems to directly contradict the causal graph in Fig.
Review Point: 1. To my knowledge none of the semi-synthetic datasets incorporate a proxy structure in their data generating mechanisms. Are these changed to fit the setting in this paper. Do you loose performance when the correct thing to do is simply condition on observed covariates, i.e. if there are no unobserved confounders? Suggestions One reference on ITE with continuous treatment could be added: Bellot, Alexis, Anish Dhir, and Giulia Prando. "Generalization bounds and algorithms for estimating conditional average treatment effect of dosage." arXiv preprint arXiv:2205.14692 (2022).
==================================================

Focused review:

I do not see a critical weakness overall. 1. It is useful to add weaknesses/limitations of the proposed method. For example, my guess is that when the number of classes is large, this approach may be inappropriate because mixture proportion estimation can be unreliable. 2. Do we need to commit ourselves to the OVR loss? It may be a bit restrictive to focus on the OVR loss. It seems to me that we can straightforwardly consider a general infinite-sample consistent multiclass surrogate loss. If the authors can clarify how difficult it may occur when considering a loss function such as softmax cross-entropy loss apart from the convex formulation may not be obtained, I may consider increasing the score.

Review Point: 1. It is useful to add weaknesses/limitations of the proposed method. For example, my guess is that when the number of classes is large, this approach may be inappropriate because mixture proportion estimation can be unreliable.
Review Point: 2. Do we need to commit ourselves to the OVR loss? It may be a bit restrictive to focus on the OVR loss. It seems to me that we can straightforwardly consider a general infinite-sample consistent multiclass surrogate loss. If the authors can clarify how difficult it may occur when considering a loss function such as softmax cross-entropy loss apart from the convex formulation may not be obtained, I may consider increasing the score.
==================================================

Focused review:

Although the method performs consistently well with different training languages, there is less discussion on why and how this method helps for the test languages. For example, it is not clear why both the highest and lowest gain come from the skewed samples group (e.g. ROM+EU and ROM+AR).
1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.
2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not. 3. The paper does not refer to the table 3 when discussing the results in section 4.

Review Point: 1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.
Review Point: 2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not.
Review Point: 3. The paper does not refer to the table 3 when discussing the results in section 4.
==================================================

Focused review:

- As the authors mentioned, their experimental results don’t effectively show a difference between the serial and joint methods. The authors mention that this could be due to the small scale of the experiments. I agree that including an experiment with a larger dataset would make this experimental section stronger. - There are issues with clarity in the notation and technical results (see below comments on “Clarity”). - Can the authors provide more details on the “separate” baseline in the experiments? Currently, Line 292 just says, “each classifier is learned separately on the training data.” However, can the authors specify which features each separate classifier uses? Does each classifier only use the direct parents of each label in the causal graph, or does each classifier use all available features? The same question applies to the Serial comparison. I could not find this information in the attached supplemental pdf. - The empirical results reported from Lines 300-304 are not at all clear to me. I’m not sure what point is being made here, what takeaways I should have, or what the 71.43% number really means. Can the authors describe these results more clearly, perhaps by further describing what they mean when they say multiple classifiers are “produced by” a given method? - The authors briefly mention that they perform “oversampling” on the Adult dataset to handle imbalanced data. Somehow, they obtain a dataset with 101,472 examples, when the original Adult dataset contains 48842 examples. However, they do not explicitly define what they mean by “imbalanced,” nor do they specifically outline what oversampling procedure they use (other than referencing a toolbox by Lemaitre et al.). Can the authors more explicitly outline their oversampling procedure?

Review Point: - As the authors mentioned, their experimental results don’t effectively show a difference between the serial and joint methods. The authors mention that this could be due to the small scale of the experiments. I agree that including an experiment with a larger dataset would make this experimental section stronger.
Review Point: - There are issues with clarity in the notation and technical results (see below comments on “Clarity”).
Review Point: - Can the authors provide more details on the “separate” baseline in the experiments? Currently, Line 292 just says, “each classifier is learned separately on the training data.” However, can the authors specify which features each separate classifier uses? Does each classifier only use the direct parents of each label in the causal graph, or does each classifier use all available features? The same question applies to the Serial comparison. I could not find this information in the attached supplemental pdf.
Review Point: - The empirical results reported from Lines 300-304 are not at all clear to me. I’m not sure what point is being made here, what takeaways I should have, or what the 71.43% number really means. Can the authors describe these results more clearly, perhaps by further describing what they mean when they say multiple classifiers are “produced by” a given method?
Review Point: - The authors briefly mention that they perform “oversampling” on the Adult dataset to handle imbalanced data. Somehow, they obtain a dataset with 101,472 examples, when the original Adult dataset contains 48842 examples. However, they do not explicitly define what they mean by “imbalanced,” nor do they specifically outline what oversampling procedure they use (other than referencing a toolbox by Lemaitre et al.). Can the authors more explicitly outline their oversampling procedure?
==================================================

Focused review:

Weakness: - some details are missing. For example, how to design the rewards is not fully understandable. - some model settings are arbitrarily set and are not well tested. For example, what is the sensitivity of the model performance w.r.t. the number of layers used in GCN for both the generator and discriminator?

Review Point: - some details are missing. For example, how to design the rewards is not fully understandable.
Review Point: - some model settings are arbitrarily set and are not well tested. For example, what is the sensitivity of the model performance w.r.t. the number of layers used in GCN for both the generator and discriminator?
==================================================

Focused review:

Weaknesses 1. The empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa. 2. Actually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple.
Plus, some technical details are not clear to me. See my questions below. Questions:
Should the probability ratio in Eq 4 be inside the ∑
? Or shall we use w ′
inside the ∑ ?
For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch?
The proposed method can help defend against backdoor attacks with only 1% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast.
What is the proposed soft template optimization for prompt?

Review Point: 1. The empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa.
Review Point: 2. Actually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple. Plus, some technical details are not clear to me. See my questions below. Questions: Should the probability ratio in Eq 4 be inside the ∑ ? Or shall we use w ′ inside the ∑ ? For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch? The proposed method can help defend against backdoor attacks with only 1% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast. What is the proposed soft template optimization for prompt?
==================================================

Focused review:

Weakness
When discussing the difference over [Tulyakov et al. 2018], the paper states “…applies h_t as the motion code for the frame to be generated, while the content code is fixed for all frames. However, such a design requires a recurrent network to estimate the motion while preserving consistent content from the latent vector, … difficult to learn in practice”. I do not fully understand why this is the case. It would be clearer if the paper can explain why such a design causes difficulty in learning and why the proposed design could alleviate such problems.
For motion diversity, why maximizing the mutual information between the hidden vector and the noise vector can prevent mode collapse?
It seems to me that the proposed method can only handle 1) “subtle” motion, such as facial expressions and 2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps).
In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.

Review Point: 2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps). In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.
==================================================

Focused review:

However, there are some concerns to be further improved as well: 1) The author gives the operation of RCB and RIB step by step but does not give a reasonable starting point. Although the final experiment has proved that such an operation may be effective, the author needs a more explicit motivation in this paper. 2) The contrast experiment compares the backbone network's effect, but the backbone network cannot be directly used for segmentation, so the author adds what structure to complete the segmentation after the backbone network, and the subsequent structure will have a significant impact on the segmentation results. The author should explain clearly. 3) There is a large amount of accumulation in the typesetting of the formula in this paper, and the author can further optimize and adjust it.

Review Point: 1) The author gives the operation of RCB and RIB step by step but does not give a reasonable starting point. Although the final experiment has proved that such an operation may be effective, the author needs a more explicit motivation in this paper.
Review Point: 2) The contrast experiment compares the backbone network's effect, but the backbone network cannot be directly used for segmentation, so the author adds what structure to complete the segmentation after the backbone network, and the subsequent structure will have a significant impact on the segmentation results. The author should explain clearly.
Review Point: 3) There is a large amount of accumulation in the typesetting of the formula in this paper, and the author can further optimize and adjust it.
==================================================

Focused review:

Weaknesses (and questions) 1. The paper proposes several sufficient conditions that VI-LCB could have a better worst-case sub-optimality gap. However, it does not provide practitioners a clear guidance on when they should run BC and when they should run VI-LCB, which I think is one of the main purpose of the paper. How should we check if the sufficient conditions are satisfied in real world problems? Some of the parameters are even unknown (e.g., b in Corollary 4.2). 2. I find it difficult to compare these upper bounds. Is it possible that some algorithms get larger bounds simply because the analysis is not tight? Can we say that algorithm A is better than algorithm B because an upper bound (of the sup-optimality gap) for A is lower than an upper bound for B? 3. Some conditions are not well-justified. The paper studies some sufficient conditions that VI-LCB outperforms BC, however, some conditions seems a little arbitrary to me (e.g., condition 3.2, condition 4.1 and condition 4.2). I guess the paper has some specific applications in mind and makes these conditions based on these applications. However, this makes the results less clear and less general. For example, how do the results extend without condition 3.2? do we expect to just see an extra H term in all bounds without the condition? If the paper considers some specific applications, the paper should clearly mention it instead of saying the conclusion holds in general.
4. The paper studies a particular type of offline RL algorithm based on VI with LCB style penalty. However, the title and the paper seems to suggest that this particular algorithm can represent the class of offline RL algorithms? I don’t see how study one algorithm can generalize to a conclusion for offline RL algorithms? 5. I am not sure what is the novelty and significance of the theoretical results. It seems the theoretical results are mostly borrowed from existing literature with some modifications. I want to make sure I am not missing something important there. Can you explain more about the novelty or significance for the theoretical results?
Minor questions and comments:
In the empirical section, does CQL lie in the VI-LCB framework? If not, why is it used in the empirical section while all theoretical results are for VI-LCB.
VI-LCB with Bernstein style penalty is also used in [1], which is not cited in this paper.
[1] Xie et al. (2021) “Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning”

Review Point: 2. I find it difficult to compare these upper bounds. Is it possible that some algorithms get larger bounds simply because the analysis is not tight? Can we say that algorithm A is better than algorithm B because an upper bound (of the sup-optimality gap) for A is lower than an upper bound for B?
Review Point: 3. Some conditions are not well-justified. The paper studies some sufficient conditions that VI-LCB outperforms BC, however, some conditions seems a little arbitrary to me (e.g., condition 3.2, condition 4.1 and condition 4.2). I guess the paper has some specific applications in mind and makes these conditions based on these applications. However, this makes the results less clear and less general. For example, how do the results extend without condition 3.2? do we expect to just see an extra H term in all bounds without the condition? If the paper considers some specific applications, the paper should clearly mention it instead of saying the conclusion holds in general.
Review Point: 4. The paper studies a particular type of offline RL algorithm based on VI with LCB style penalty. However, the title and the paper seems to suggest that this particular algorithm can represent the class of offline RL algorithms? I don’t see how study one algorithm can generalize to a conclusion for offline RL algorithms?
Review Point: 5. I am not sure what is the novelty and significance of the theoretical results. It seems the theoretical results are mostly borrowed from existing literature with some modifications. I want to make sure I am not missing something important there. Can you explain more about the novelty or significance for the theoretical results? Minor questions and comments: In the empirical section, does CQL lie in the VI-LCB framework? If not, why is it used in the empirical section while all theoretical results are for VI-LCB. VI-LCB with Bernstein style penalty is also used in [1], which is not cited in this paper. [1] Xie et al. (2021) “Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning”
==================================================

Focused review:

weakness of the paper, and would require significant revisions throughout. Notably:
W1. How is posterior collapse defined? [Post discussion: thank you for addressing this. I strongly recommend putting the revised definition in the main paper.]
W2. How is posterior collapse being measured? [Post discussion: thank you for addressing this. I strongly recommend clearly tying this to the experimental setup in the main paper.]
W3. What experimental results demonstrate that posterior collapse, as measured by point (2), has actually been mitigated? [Post discussion: Please add the revised table in the appendix to the main paper.]
Beyond that overarching weakness, this paper does not adequately address other considerations, including broader take-aways:
W4. This paper presents dc-ETM, and two learning algorithms for it. What aspects of this work prevent posterior collapse? Is it the definition of dc-ETM, the policy gradient approach, or both? [Post discussion: thank you for addressing this]
W5. Fundamentally, why does policy gradient result in better results? The two objectives (Eq. 11 and Eq. 16) are very similar, sharing the same subloss computations. From what I can tell, a core difference is the quadratic nature of the policy gradient objective. Is this what is responsible for the improved results, or is there something else at play?
One strength of this work is that it examines different ways of decoding, notably through Eqns 2 and 3.
W6. Unfortunately, the presentation of the model(s) needs to be improved. While some of this could be a spacing issue, e.g., around equation 1), the corresponding prose is quite dense and more difficult to get through than it should be. Further, the core factorization and probabilistic identities that this paper relies (e.g., the deconstruction in Eq. 4) on are not specified clearly. I recommend significantly rewriting and streamlining the presentation of the model and underlying justifications. [Post discussion: thank you for addressing this]
W7. Finally, I found the qualitative results to be unclear and difficult to interpret. Figure 5 does not have enough explanation, either in the figure itself, the caption, or the corresponding prose. [Post discussion: thank you for addressing this]

Review Point: 1), the corresponding prose is quite dense and more difficult to get through than it should be. Further, the core factorization and probabilistic identities that this paper relies (e.g., the deconstruction in Eq. 4) on are not specified clearly. I recommend significantly rewriting and streamlining the presentation of the model and underlying justifications. [Post discussion: thank you for addressing this] W7. Finally, I found the qualitative results to be unclear and difficult to interpret. Figure 5 does not have enough explanation, either in the figure itself, the caption, or the corresponding prose. [Post discussion: thank you for addressing this]
==================================================

Focused review:

1. The novelty of the proposed method is arguably limited. See my comments below. 2. There are minor issues in presentation

Review Point: 1. The novelty of the proposed method is arguably limited. See my comments below.
==================================================

Focused review:

- The ResNet50 accuracy on ImageNet looks marginal: 20.73+0.17 = 20.9, which is close to 21.07 from OHL. Would be better to compare with the Enlarge Batch trick. - This paper makes the observation based on image classification task, which is training from scratch. It would also be important to evaluate that if transfer learning with a pre-trained backbone still have this phenomenon. Update after Rebuttal: I read the opinions from other reviewers, and the feedback from the authors. The feedback is valid, but not strong enough to improve my rating, thus I'd like to keep my score.

Review Point: - The ResNet50 accuracy on ImageNet looks marginal: 20.73+0.17 = 20.9, which is close to 21.07 from OHL. Would be better to compare with the Enlarge Batch trick.
Review Point: - This paper makes the observation based on image classification task, which is training from scratch. It would also be important to evaluate that if transfer learning with a pre-trained backbone still have this phenomenon. Update after Rebuttal: I read the opinions from other reviewers, and the feedback from the authors. The feedback is valid, but not strong enough to improve my rating, thus I'd like to keep my score.
==================================================

Focused review:

Weaknesses
Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term "coherence" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining "coherence" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words "turn" and "utterance" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: "entailment", "contradiction" and "neutral". I suggest specifying that the 1 label in the numerator corresponds to the "entailment" class, and whether you consider both "contradiction" and "neutral" as a single "non-entailment" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the "instances for evaluation" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the "Utterance Level Sampling" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it "tends to sample previous utterance to be replaced" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the "Coherence Rate" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results "w/ Noise" described on page 6, "w/ Utterance" and "w/ Semi-Utterance" described on page 4, but you also mention "w/ Hierarchical". Up to this point I had understood both "Utterance Level Sampling" and "Semi-utterance Level Sampling" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the "w/ Hierarchical" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating "Rep"? I suggest clarifying this.
On page 7 in the "Sampling vs w/o Sampling" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the "Human Evaluation" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the "Explicit Coherence Optimization" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your "hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you "demonstrate these methods make chatbots more robust in real-world testing" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term "coherence" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition "Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts." is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, "is coherence" should be "that is coherent".
P. 3, p. 4, l. 1: You write "a conventional practice [REFERENCES] for evaluating [...]."; I suggest writing "a conventional practice for evaluating [...] [REFERENCES]." to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index "1" mean in U ^ 1 i − 1
? Does it mean "starting from index 1"? If this is the case and you never use anything other than "1" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to "conduct" a classifier does not make much sense. You can either "conduct" classification or "train", "use", "create", etc. a classifier.
P. 4, p. 4, l. 4: here you say "we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: "Given a training pair U ^ 1 t − 1
" should be "Given a training pair U ^ 1 ′ t − 1
", i.e. the training pair contains an utterance replaced through the "Utterance Level Sampling" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence "While a smaller j to simulate more accumulate errors along with the inference steps.", please rewrite it.
P. 5, p. 3, l. 3: "two annotators are employed" -> "two annotators were employed"
P. 5, p. 5, l. 3: The sentence starting with "As model-based methods" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: "Following previous work (Ritter et al., 2011)" -> "Following the work by Ritter et al. (2011),"
P. 5, p. 8, l. 2: "to online evaluate these two methods" -> "to evaluate these two methods online"
P. 6, p. 1, l. 2: "non-repetitive" -> "non-repetitiveness"
P. 6, p. 6, l. 1: "After sample an utterance" -> "After sampling an utterance"
P. 6, p. 8, l. 2: "generate coherence response" -> "generate coherent responses"
P. 6, p. 8, l. 4: "with the number of turns increases" -> "as the number of turns increases"
P. 7, Figure 4, a - b: The y-axis should be labeled "coherence (%)" instead of "coherent (%)". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence "since sampled noises are difficult to accurately simulate errors of the inference scene during training" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): "Contradition" -> "Contradiction"; (b): "Coherent" -> "Coherence". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: "hierarchy way" -> "hierarchical way"
P. 9, p. 2, l. 1: "incoherence response" -> "incoherent response"
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.

Review Point: 1: You write "a conventional practice [REFERENCES] for evaluating [...]."; I suggest writing "a conventional practice for evaluating [...] [REFERENCES]." to improve the reading flow. P. 3, p. 4, l.
Review Point: 7: What does the sub-index "1" mean in U ^ 1 i − 1 ? Does it mean "starting from index 1"? If this is the case and you never use anything other than "1" as the starting index, I suggest removing it, and simply defining U ^ i − 1 as the context up to the i − 1 -th utterance. P. 3, p. 5, l.
Review Point: 9: to "conduct" a classifier does not make much sense. You can either "conduct" classification or "train", "use", "create", etc. a classifier. P. 4, p. 4, l.
Review Point: 4: here you say "we first ask the model to predict the response r ^ i based on the previous context U ^ 1 ′ i − 1 but if I understand the explanation correctly, then it should be U ^ 1 i − 1 i.e. the original context. P. 4, p. 4, l.
Review Point: 7: "Given a training pair U ^ 1 t − 1 " should be "Given a training pair U ^ 1 ′ t − 1 ", i.e. the training pair contains an utterance replaced through the "Utterance Level Sampling" method. P. 4, eq.
Review Point: 3: U ^ 1 ′ l − 1 should be U ^ 1 ′ t − 1 i.e. the super-index of U ′ should be t − 1 not l − 1 P. 4, p. 5, l.
Review Point: 5: I can't understand the meaning of the sentence "While a smaller j to simulate more accumulate errors along with the inference steps.", please rewrite it. P. 5, p. 3, l.
Review Point: 3: The sentence starting with "As model-based methods" is ungrammatical. I suggest reformulating it. P. 5, p. 7, l.
==================================================

Focused review:

Weakness: - Theoretical contribution is minor. There is no novelty in the policy update and the policy structure. The proposed method looks like a minor modification of FuN to make it work in an off-policy manner. - It seems that authors have employed various tricks to make the method work in re-labeling the goal. - The benefit of the off-policy correction is not sufficiently discussed in the experiment section.

Review Point: - Theoretical contribution is minor. There is no novelty in the policy update and the policy structure. The proposed method looks like a minor modification of FuN to make it work in an off-policy manner.
Review Point: - It seems that authors have employed various tricks to make the method work in re-labeling the goal.
Review Point: - The benefit of the off-policy correction is not sufficiently discussed in the experiment section.
==================================================

Focused review:

Weakness 1. Comparison with the prior methods in large dataset is missing. For example, in Table 3, the comparison results are provided only for small datasets or reduced version of large datasets. However, the proposed method is not restricted to small dataset. The ablation experimental result in Figure 2 shows that proposed MLTI is still effective when the full miniImageNet/DermNet dataset is used, although the performance gain becomes small when the full dataset is used. I suggest the authors to include the comparison of MLTI and prior methods with full size of miniImageNet and DermNet.
Question 1. In Section 3, the authors mentioned that it is intractable to calculate prototypes with mixed labels. However, in prior work on semi-supervised few-shot learning [1], the prototypes are computed using soft-labels. What happens if we compute prototypes using soft-labels as done in [1]? 2. Some additional studies on the interpolation layer would be helpful for understanding the proposed method. In Algorithm 1 and 3, the interpolation layer l
is randomly chosen in step 7. What happens if we fix l
instead of randomly sampling l
for every iteration? In that case, how are interpolating at lower layer and interpolating higher layer different?
Typo: In last line of page 4, there is a typo (regularizaiton -> regularization)
[1] Ren, Mengye, et al. "Meta-Learning for Semi-Supervised Few-Shot Classification." International Conference on Learning Representations. 2018.

Review Point: 1. Comparison with the prior methods in large dataset is missing. For example, in Table 3, the comparison results are provided only for small datasets or reduced version of large datasets. However, the proposed method is not restricted to small dataset. The ablation experimental result in Figure 2 shows that proposed MLTI is still effective when the full miniImageNet/DermNet dataset is used, although the performance gain becomes small when the full dataset is used. I suggest the authors to include the comparison of MLTI and prior methods with full size of miniImageNet and DermNet. Question 1. In Section 3, the authors mentioned that it is intractable to calculate prototypes with mixed labels. However, in prior work on semi-supervised few-shot learning [1], the prototypes are computed using soft-labels. What happens if we compute prototypes using soft-labels as done in [1]?
==================================================

Focused review:

Weaknesses:
Relation to prior work
Upon reading the related work in more detail, it is clear that this work is an extension of the work by Bradshaw et al. 2020 NeurIPS, who reported 1) generation of synthesis trees (synthesis trees are a special case of DAGs) using a generative model for DAGs, and 2) conditional tree generation p ( T , x )
. Conditioned on an embedding of the desired product structure x
, the generative model generate a synthesis tree T
. This model is called "RetroDOG". Furthermore, the MDP setting has been explored by Horwood and Gottipatti.
To be more concrete: In section D.3 of the appendix https://papers.nips.cc/paper/2020/file/4cc05b35c2f937c5bd9e7d41d3686fff-Supplemental.pdf of their Neurips paper, Bradshaw et al. write: `Our generative model of synthesis DAGs can be seen as a parametrizable mapping from a vector of real numbers to a synthesis DAG. As a module it can be mixed and matched in different ML frameworks as we have already seen in the main paper with DoG-AE and DoG-Gen. In this section we describe some preliminary results with a third model architecture, called RetroDoG. This model consists of the composition of a GNN followed by our generative synthesis DAG model to produce a learnable mapping from a molecular graph to a synthesis DAG. By training this model on pairs of product molecules and their associated synthesis DAGs we can use this model to perform retrosynthesis (i.e. predict how a particular product can be made). [...] the model described in this section would additionally allow one to feed in a potentially hard or impossible to synthesize molecule, and obtain a similar molecule which is easy to synthesize, which is impossible with current planners." [...] "RetroDoG in contrast tries to construct the DAG in a bottom-up manner [...]. we believe such an approach as RetroDoG may be worthy of future research interest, and may for instance be useful in combination with more complex tools to amortize and reduce the cost of searching for synthetic routes."
Building on other people's work is an essential part of science, but it needs to be acknowledged accordingly, and there is no harm in doing so. In fact, the bigger harm to the scientific community comes from not doing so.
Unfortunately, Bradshaw et al. 2020, Gottipatti et al and Horwood et al are only cited in passing in the related work section, and not in the introduction, and there is not even a mention of the RetroDog model by Bradshaw. In the way the contributions of this paper here are presented, it leads to the impression that they were actually invented by the authors, which is not the case.
We formulate the tasks of multi-step synthesis planning and synthesizable molecular design as a single shared task of conditional synthetic tree generation. => this has already been done by Bradshaw et al as the RetroDog model
We formulate a Markov decision process to model the generation of synthetic trees, allowing the generation of multi-step and convergent synthetic pathways. => Gottipatti & Horwood used the MDP framing already
We propose a model that is capable of (1) rapid bottom-up synthesis planning and (2) constrained molecular optimization that can explore a chemical space => this is the RetroDog model by Bradshaw et al (who also explicitly did not restrict their model to the molecular transformer).
The paper here should thus be clearly presented as an investigation and extension of the work by Bradshaw, Gottipatti and Horwood, rather than novel modelling. This would not detract from its quality or value, which in my opinion is considerable.
This reviewer would also suggest to cite non-neural reaction-driven molecular design work, for example https://doi.org/10.1371/journal.pcbi.1002380 or https://doi.org/10.1021/jm030809x which predate generative models for molecules, and already point out the requirement of synthesizability. In fact, these algorithms have also been used in conjunction with fingerprint similarity to a target molecule as the objective, and would thus be a good baseline to explore in this work. Modelling
The model the authors describe should be more correctly referred to as a generative model for synthesis DAGs. There is no MDP here, since there is no notion of reward.
Also, it can be argued that a synthesis tree does not really satisfy the Markov assumption. For example, one would never introduce a protecting group A, and then immediately deprotect A.
Experiments and Baselines
Most if not all molecules that come from the baselines do not appear to be very drug-like. This raises the question whether the used benchmark functions are really realistic enough for drug discovery, or whether other objectives should be considered. Furthermore, are cLogP or QED really a quantity that is desirable as an objective for drug discovery on its own? Also, are docking scores really reliable enough to be used a quantitative oracle?

Review Point: 1) generation of synthesis trees (synthesis trees are a special case of DAGs) using a generative model for DAGs, and
==================================================

Focused review:

weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms. Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.

Review Point: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details.
Review Point: - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks:
Review Point: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches.
Review Point: - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid.
Review Point: - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning.
Review Point: - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret?
Review Point: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing.
Review Point: - p13, l457: some notations seem to be undefined (w_t, W_t).
Review Point: - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.
==================================================

Focused review:

weaknesses of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail. This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices. In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times. Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful. Lastly, the details of the experiment are lacking. In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.
Specific comments:
- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
- l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
- Throughout, the authors use the term constrains and should change to constraints.
- l124: What is meant by the (*)?
- l134: Do the authors mean m=2?
- l148: known, instead of know
- l156: please define \gamma_0^{***}
- Figure 1: Please specify the meaning of the colors in the caption as well as the text.
- l280: "Then we made it explicit" instead of "Then we have explicit it"

Review Point: - l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
Review Point: - l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
Review Point: - l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
Review Point: - Throughout, the authors use the term constrains and should change to constraints.
Review Point: - l148: known, instead of know - l156: please define \gamma_0^{***} - Figure 1: Please specify the meaning of the colors in the caption as well as the text.
Review Point: - l280: "Then we made it explicit" instead of "Then we have explicit it"
==================================================

Focused review:

- The large sample size needed for theoretical guarantees makes the result uninformative regarding practical sample sizes. - The theoretical results rely on a condition on residual variances that may not hold in typical practical instances.

Review Point: - The large sample size needed for theoretical guarantees makes the result uninformative regarding practical sample sizes.
Review Point: - The theoretical results rely on a condition on residual variances that may not hold in typical practical instances.
==================================================

Focused review:

The weakness of the paper is provided below along with some explanations: • Heavy reliance on attention network: The authors focus on understanding or realizing the illumination from subject’s body part and skin which requires the attention network to have access to these information. Also, the implementation of Shafer’s DRM helps in differentiating the foreground and body illumination, however the need to label them still exist. Therefore, the approach provided here has a high reliance on use of attention networks to present the subject and details about the subject. • Need for a higher resolution optical equipment: As the algorithm focusses on optical illumination and reflection, there is a need to focus and capture pixels with high density of clarity and information. Therefore, even though the accuracy of the algorithm seems good, it to a large extent depends high resolution camera picking up the optical information. • No explanation on external effects: Human beings tend to cover body with different types of clothes that may come in various sizes and layers. The explanation of authors depends on obtaining ques from subject’s body based on changes in body position, raising of head or chest, and even changes in color variation of skin which would be difficult with layers of clothes on subject and authors do not provide any option towards a solution to this. • Firefly-RK3399: Firefly RK 3300 is a 6-core 64 bit high performance platform. This is a high performing collection and the algorithm with such complex computations might work faster in such combination. Authors should try implementing the algorithm and run over different lesser strong processors to see how the algorithm works. • Minor comments: o The topic is about on-device, but the presentation of this point is not enough. The author just uses a small section to explain the on-device evaluation. o The position of Fig.1 and Fig.2 should be close to the corresponding explanatory text, which helps readers understand them better. Especially for Fig.1, there is a comparison for four models, but it is hard to understand the content of it just with the simple legend. Or, the author can add the legend with more details (what is A, B, C, and D?!) to make it easier to understand.

Review Point: • Heavy reliance on attention network: The authors focus on understanding or realizing the illumination from subject’s body part and skin which requires the attention network to have access to these information. Also, the implementation of Shafer’s DRM helps in differentiating the foreground and body illumination, however the need to label them still exist. Therefore, the approach provided here has a high reliance on use of attention networks to present the subject and details about the subject.
Review Point: • Need for a higher resolution optical equipment: As the algorithm focusses on optical illumination and reflection, there is a need to focus and capture pixels with high density of clarity and information. Therefore, even though the accuracy of the algorithm seems good, it to a large extent depends high resolution camera picking up the optical information.
Review Point: • No explanation on external effects: Human beings tend to cover body with different types of clothes that may come in various sizes and layers. The explanation of authors depends on obtaining ques from subject’s body based on changes in body position, raising of head or chest, and even changes in color variation of skin which would be difficult with layers of clothes on subject and authors do not provide any option towards a solution to this.
Review Point: • Firefly-RK3399: Firefly RK 3300 is a 6-core 64 bit high performance platform. This is a high performing collection and the algorithm with such complex computations might work faster in such combination. Authors should try implementing the algorithm and run over different lesser strong processors to see how the algorithm works.
Review Point: • Minor comments: o The topic is about on-device, but the presentation of this point is not enough. The author just uses a small section to explain the on-device evaluation. o The position of Fig.1 and Fig.2 should be close to the corresponding explanatory text, which helps readers understand them better. Especially for Fig.1, there is a comparison for four models, but it is hard to understand the content of it just with the simple legend. Or, the author can add the legend with more details (what is A, B, C, and D?!) to make it easier to understand.
==================================================

Focused review:

Some weaknesses/limitations of this work are: - The proposed method is not addressing the more general "problem of inference under full model misspecification": it only accounts for likelihood misspecification. In their rebuttal, the authors agree to change their terminology, as they "indeed focus on likelihood misspecification although the GBI framework of Bissiri, et. al. can in principle be used more widely, which we have now also highlighted." - The evaluation section could be slightly strengthen with some additional comparisons (see specific suggestions below). In their rebuttal, the authors have provided further evidence of their algorithm's performance, which I suggest they include in the updated manuscript (or clearly refer to, if left in appendix).

Review Point: - The proposed method is not addressing the more general "problem of inference under full model misspecification": it only accounts for likelihood misspecification. In their rebuttal, the authors agree to change their terminology, as they "indeed focus on likelihood misspecification although the GBI framework of Bissiri, et. al. can in principle be used more widely, which we have now also highlighted." - The evaluation section could be slightly strengthen with some additional comparisons (see specific suggestions below). In their rebuttal, the authors have provided further evidence of their algorithm's performance, which I suggest they include in the updated manuscript (or clearly refer to, if left in appendix).
==================================================

Focused review:

Weaknesses:
The expert descriptions exemplified in the paper encompass 1) quantifiable information (musical tempo, ...), 2) subjective information (valence, ...), and 3) writing style. I could also imagine that some contain 4) factual information, which is not possible to extract from only the music signal itself (e.g. name of the piece, composer). The proposed model seems to somewhat capture each element and is able to generate much more coherent examples than the other pre-trained models, but it also generates a lot of non-sensical descriptions such as "begins with a theme of the main theme of the main theme in the...," "the music is a fugue... to the sonata form of pizzicato," "the music of the movement."
I find similarities to models trained for question answering task, in particular, how LLMs fail to give correct answers - sometimes in a dangerously confident tone (e.g. "will my stomach be cleaner if I drink bleach?"). For me, a more interesting discussion would be about the challenges of this task, and how different models can or cannot cope with the elements mentioned above, potentially via an ablation study.
The dataset is quite small, which probably contributes to the limitations. I appreciate that it's very difficult to collect data for such a task, however, the impact will be minor unless the data could be collected in scale. For instance, the researchers could add a second dataset consisting of Anglo-american pop music (e.g. available in Lakh dataset) and crawl expert or fan reactions (e.g. from social media) and repeat the experiments. This way we could understand how generalizable the approaches are and contrast the modes of good and bad descriptions across datasets.
Classical pieces in the dataset are typically instrumental. It may not possible to draw out a universal (or a consensus between experts) sentiment from instrumental classical music (or any other type of music). The paper doesn't address subjectivity much in the account.

Review Point: 3) writing style. I could also imagine that some contain
==================================================

Focused review:

- In the paper, the goal is to bound the expected \ell_1 error as the loss function. In contrast, in the PAC learning model, we are looking for the worst-case \ell_1 error, which is guaranteed with some high probability 1 - \delta (known as the confidence parameter). It would be interesting to discuss the relationship between the two models. In particular, for a constant confidence parameter, the results of this submission can be easily extended to the PAC learning model. Obtaining an algorithm with the right dependency to \delta might be more challenging. What are the advantages of the expected \ell_1 error? Does it simplify the proof? - There is no experiment that echos the main result of the paper. An experiment is provided to validate an auxiliary theorem (Theorem 5), which is about learning multinomial distributions.

Review Point: - In the paper, the goal is to bound the expected \ell_1 error as the loss function. In contrast, in the PAC learning model, we are looking for the worst-case \ell_1 error, which is guaranteed with some high probability 1 - \delta (known as the confidence parameter). It would be interesting to discuss the relationship between the two models. In particular, for a constant confidence parameter, the results of this submission can be easily extended to the PAC learning model. Obtaining an algorithm with the right dependency to \delta might be more challenging. What are the advantages of the expected \ell_1 error? Does it simplify the proof?
Review Point: - There is no experiment that echos the main result of the paper. An experiment is provided to validate an auxiliary theorem (Theorem 5), which is about learning multinomial distributions.
==================================================

Focused review:

1. Algorithm 1 is proposed to solve Eq. (1), but the derivation process is not very clear and the notations are somewhat confused (e.g. X denotes the input, does it mean the same thing in Algorithm 1?). 2. It is declared in Sec. 3.2 that "DARP increases at most 20% of the overall training time of an existing SSL scheme". I don't quite understand. 3. The experiments show that DARP could improve the performance of the SLL methods. But I observe that the results of SLL methods which are not specifically designed for imbalanced SLL (VAT, Mean-Teacher...) are comparable with that of imbalanced SLL methods. This may seem a little strange, so I wonder the reason.

Review Point: 1. Algorithm 1 is proposed to solve Eq. (1), but the derivation process is not very clear and the notations are somewhat confused (e.g. X denotes the input, does it mean the same thing in Algorithm 1?).
Review Point: 2. It is declared in Sec. 3.2 that "DARP increases at most 20% of the overall training time of an existing SSL scheme". I don't quite understand.
Review Point: 3. The experiments show that DARP could improve the performance of the SLL methods. But I observe that the results of SLL methods which are not specifically designed for imbalanced SLL (VAT, Mean-Teacher...) are comparable with that of imbalanced SLL methods. This may seem a little strange, so I wonder the reason.
==================================================

Focused review:

Weaknesses:
The support samples of "difficult task" is selected by fixing the query samples which means the difficulty of each task is highly correlated with the fixed query data. The hypothesis is somewhat unreasonable, since the FSL aims to adapt to the whole new class not the fixed query samples. The reviewer supposes that the tasks in the HARD-META-DATASET++ may not be a reliable estimator in the test-time in FSL.
The proposed method can be seen as an extension of the paper[1], since the paper[1] have already proposed that FSL are extremely sensitive to the data used for adaptation and used a greedy algorithm to find those difficult tasks. Although the proposed method is more efficient, the novelty is somewhat limited.
3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000.
As for questions, I would like to ask:
1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD a better criterion for evaluation?
2 How to use the "difficult task" in the training phase in FSL? Can the "difficult task" in base classes help the model improve the generality to novel classes?

Review Point: 3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000. As for questions, I would like to ask:
Review Point: 1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD a better criterion for evaluation?
Review Point: 2 How to use the "difficult task" in the training phase in FSL? Can the "difficult task" in base classes help the model improve the generality to novel classes?
==================================================

Focused review:

Weaknesses: Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.
Other minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.
2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.
While the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems.
Given these strengths, I am changing my recommendation score to 3. I have read the authors' responses.

Review Point: 1) how well the tree-variant of the model does on its own and
Review Point: 2) the effect of removing inference composition from the model. Other minor issues:
Review Point: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.
Review Point: 2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis. While the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems. Given these strengths, I am changing my recommendation score to 3. I have read the authors' responses.
==================================================

Focused review:

- The results of the BERT experiments are not strong enough. Most gains come from the RTE that is a very small dataset. However, on the most important task MNLI, the performance degrades significantly. In terms of performance, it seems that knowledge distillation outperforms the proposed method, with the same reduction of memory size. - The speedup in terms of detailed GPU hours shouls also be reported in the tables. - The previous attention clustering methods are not throughly compared with in the experiments. - It's unclear how GPU friendly the method is. - More analysis would help readers to understand the limitation of the proposed method. - The method is evaluated for fine-tuned BERT models. It's more insightful to show that the proposed method can also work well in the pre-training setting.

Review Point: - The results of the BERT experiments are not strong enough. Most gains come from the RTE that is a very small dataset. However, on the most important task MNLI, the performance degrades significantly. In terms of performance, it seems that knowledge distillation outperforms the proposed method, with the same reduction of memory size.
Review Point: - The speedup in terms of detailed GPU hours shouls also be reported in the tables.
Review Point: - The previous attention clustering methods are not throughly compared with in the experiments.
Review Point: - More analysis would help readers to understand the limitation of the proposed method.
Review Point: - The method is evaluated for fine-tuned BERT models. It's more insightful to show that the proposed method can also work well in the pre-training setting.
==================================================

Focused review:

1. The paper overall lacks clarity, in particular the figures of computational graphs (Fig. 1, 2, 4) are not well explained and lack meaningful captions. 2. The demonstrated advantages are mainly relative to either purely activation or purely timing-based rules, but do not compare to state-of-the-art methods. The accuracies in MNIST and N-MNIST are well below those reached in other papers. The paper admits this and highlights the efficient use of very few spikes. While this is an advantage, I would like to see a recommendation or even better evidence that the accuracy gap can be closed, e.g. with larger networks. 3. Broader impact is not addressed.

Review Point: 1. The paper overall lacks clarity, in particular the figures of computational graphs (Fig. 1, 2, 4) are not well explained and lack meaningful captions.
Review Point: 2. The demonstrated advantages are mainly relative to either purely activation or purely timing-based rules, but do not compare to state-of-the-art methods. The accuracies in MNIST and N-MNIST are well below those reached in other papers. The paper admits this and highlights the efficient use of very few spikes. While this is an advantage, I would like to see a recommendation or even better evidence that the accuracy gap can be closed, e.g. with larger networks.
==================================================

Focused review:

Weakness:
Experiments: 1. Why experiments do not contained the same as RIS? Just for completeness to show that you do better than RIS on their experiments. 2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?

Review Point: 2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?
==================================================

Focused review:

There are several aspects that I think the author could improve on. 1. The author could provide better insight on what has been learned for the offset, and provide qualitative examples; 2. The author should conduct a more thorough literature study on the deep stereo methods, as a plethora of important papers is missing. KL-divergence perspective: When there is no multi-modal GT, I found it’s hard to justify the usage of Wasserstein distance, as target distribution is simply a Dirac. S 1.3. provides a helpful derivation of Eq. 10 based on KL divergence in which the prediction is a complicated mixture of Gaussian/Laplacians. However, I am wondering if you could explain this from a different perspective. That is to say, Eq. 10 could be interpreted in a way that we are minimizing the cross-entropy / KL divergence between prediction distribution p and target distribution p* over discrete support space. But unlike the conventional method, in which p* is a predefined distribution, e.g., one-hot or soft-logit label computed from l2, the proposed method learns an offset to produce an adaptive target logit. I am wondering if the author could comment on this. Explanation/Insights: It’s good to see that adding a small continuous offset over the output space would bring a difference. However, I am wondering if the author has any insight into what the offset learns. E.g. sub-pixel disparity, better multi-modal support towards GT? I would strongly suggest the author could visualize the offset and offers some discussions. Moreover, the offset bin seems too small for me, which may suggest it merely learns a sub-pixel refinement. And it seems from Supp the well-tuned hyper-parameter choice is critical to achieving performance gain. Could you better justify this? Ablation on mode/mean: it’s quite surprising that the Wasserstein distance itself without offset brings such a significant drop in EPE. Do you use the mode or the mean in this ablation study for Wasserstein only? Have you tried both? Furthermore, have you tried to use PSMNet and use its mode for prediction instead of mean? What is the performance if you use mean as your final disparity estimation. Uncertainty: One thing interesting to me is it’s learned model uncertainty, which I expect to be better calibrated using Wasserstein distance than the soft-max logits trained with loss over soft-argmax mean. I suggest the author conduct a visualization of the learned mixture of Dirac PDF over some pixels, and a visualization of the dense uncertainty map (e.g. per pixel variance of the prediction, etc.). Example in Fig.4 does not explain the improvement and observations on the foreground. Maybe you could choose another example to showcase how foreground pixels have improved. Marginal improvement: The proposed method achieves a relatively marginal increase in terms of stereo estimation metrics. In this work, additional computation is brought by the offset header. However, as far as I know, at least for PSM-Net, a very light-weight disparity refinement header could easily bring additional improvement at the same level.

Review Point: 1. The author could provide better insight on what has been learned for the offset, and provide qualitative examples;
==================================================

Focused review:

Weaknesses
I have the following questions to which I wish the author could respond in the rebuttal. If I missed something in the paper, I would appreciate it if the authors could point them out.
Main concerns: - In my understanding, the best scenarios are those generated from the true distribution P (over the scenarios), and therefore, the CVAE essentially attempts to approximate the true distribution P. In such a sense, if the true distribution P is independent of the context (which is the case in the experiments in this paper), I do not see the rationale for having the scenarios conditioned on the context, which in theory does not provide any statistical evidence. Therefore, the rationale behind CVAE-SIP is not clear to me. If the goal is not to approximate P but to solve the optimization problem, then having the objective values involved as a predicting goal is reasonable; in this case, having the context involved is justified because they can have an impact on the optimization results. Thus, CVAE-SIPA to me is a valid method. - While reducing the scenarios from 200 to 10 is promising, the quality of optimization has decreased a little bit. On the other hand, in Figure 2, using K-medoids with K=20 can perfectly recover the original value, which suggests that K-medoids is a decent solution and complex learning methods are not necessary for the considered settings. In addition, I am also wondering the performance under the setting that the 200 scenarios (or random scenarios of a certain number from the true distributions) are directly used as the input of CPLEX. In addition, to justify the performance, it is necessary to provide information about robustness as well as to identify the case where simple methods are not satisfactory (such as larger graphs).
Minor concerns: - Given the structure of the proposed CVAE, the generation process takes the input of z and c where z
is derived from w
. This suggests that the proposed method requires us to know a collection of scenarios from the true distribution. If this is the case, it would be better to have a clear problem statement in Sec 3. Based on such understanding, I am wondering about the process of generating scenarios used for getting K representatives - it would be great if codes like Alg 1 was provided. - I would assume that the performance is closely related to the number of scenarios used for training, and therefore, it is interesting to examine the performance with different numbers of scenarios (which is fixed as 200 in the paper). - The structure of the encoder is not clear to me. The notation q_{\phi} is used to denote two different functions q(z w,D) and q ( c , D )
. Does that mean they are the same network? - It would be better to experimentally justify the choice of the dimension of c and z. - It looks to me that the proposed methods are designed for graph-based problems, while two-stage integer programming does not have to be graph problems in general. If this is the case, it would be better to clearly indicate the scope of the considered problem. Before reaching Sec 4.2, I was thinking that the paper could address general settings. - The paper introduces CVAE-SIP and CVAE-SIPA in Sec 5 -- after discussing the training methods, so I am wondering if they follow the same training scheme. In particular, it is not clear to me by saying “append objective values to the representations” at the beginning of Sec 5. - The approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table. It would be better to provide a mathematical characterization.

Review Point: - In my understanding, the best scenarios are those generated from the true distribution P (over the scenarios), and therefore, the CVAE essentially attempts to approximate the true distribution P. In such a sense, if the true distribution P is independent of the context (which is the case in the experiments in this paper), I do not see the rationale for having the scenarios conditioned on the context, which in theory does not provide any statistical evidence. Therefore, the rationale behind CVAE-SIP is not clear to me. If the goal is not to approximate P but to solve the optimization problem, then having the objective values involved as a predicting goal is reasonable; in this case, having the context involved is justified because they can have an impact on the optimization results. Thus, CVAE-SIPA to me is a valid method.
Review Point: - While reducing the scenarios from 200 to 10 is promising, the quality of optimization has decreased a little bit. On the other hand, in Figure 2, using K-medoids with K=20 can perfectly recover the original value, which suggests that K-medoids is a decent solution and complex learning methods are not necessary for the considered settings. In addition, I am also wondering the performance under the setting that the 200 scenarios (or random scenarios of a certain number from the true distributions) are directly used as the input of CPLEX. In addition, to justify the performance, it is necessary to provide information about robustness as well as to identify the case where simple methods are not satisfactory (such as larger graphs). Minor concerns:
Review Point: - Given the structure of the proposed CVAE, the generation process takes the input of z and c where z is derived from w . This suggests that the proposed method requires us to know a collection of scenarios from the true distribution. If this is the case, it would be better to have a clear problem statement in Sec 3. Based on such understanding, I am wondering about the process of generating scenarios used for getting K representatives - it would be great if codes like Alg 1 was provided.
Review Point: - I would assume that the performance is closely related to the number of scenarios used for training, and therefore, it is interesting to examine the performance with different numbers of scenarios (which is fixed as 200 in the paper).
Review Point: - The structure of the encoder is not clear to me. The notation q_{\phi} is used to denote two different functions q(z w,D) and q ( c , D ) . Does that mean they are the same network?
Review Point: - It would be better to experimentally justify the choice of the dimension of c and z.
Review Point: - It looks to me that the proposed methods are designed for graph-based problems, while two-stage integer programming does not have to be graph problems in general. If this is the case, it would be better to clearly indicate the scope of the considered problem. Before reaching Sec 4.2, I was thinking that the paper could address general settings.
Review Point: - The paper introduces CVAE-SIP and CVAE-SIPA in Sec 5 -- after discussing the training methods, so I am wondering if they follow the same training scheme. In particular, it is not clear to me by saying “append objective values to the representations” at the beginning of Sec 5.
Review Point: - The approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table. It would be better to provide a mathematical characterization.
==================================================

Focused review:

weaknesses are discussed in a short but concise discussion in the paper. The results are technically sound, and these results are explained in a simple and clear way. The proposed method outperforms the state of the art. The experiments section is well-presented, and the results there support the theoretical findings. Empirical evidence is limited to a logistic regression setup (with a variety of real data). It could have been expanded for some other settings, either in the main text or supplements. Overall, I enjoyed reading this submission. The paper is very clear about its focus and contributions, and all technical results and the figures are accompanied by discussions and take-home messages. The proposed method can be attractive to the practitioners since it is scale invariant (hence no tuning or preconditioning) and it outperforms the baseline. I recommend the acceptance of this paper. Some minor comments: - There is a typo in footnote 4 on page 5. - "n" is defined as the Newton direction in (5). It is also used to denote the number of data points in pages 6-7. Consider changing one of these to avoid confusion. - line 259: "out run" or "outrun"? - [12] and [13] are the same papers the bibliography ========== After author feedback ========== I have read the author feedback and takin it into account in my final scores.

Review Point: - "n" is defined as the Newton direction in (5). It is also used to denote the number of data points in pages 6-7. Consider changing one of these to avoid confusion.
Review Point: - [12] and [13] are the same papers the bibliography ========== After author feedback ========== I have read the author feedback and takin it into account in my final scores.
==================================================

Focused review:

Weaknesses:
The idea of using points to represent keyframes or objects is not very new, as it was already mentioned in [1,2,3,4]. The authors should add a subsection to the related work section summarizing the current related work and explaining the differences from these approaches.
From Table 1, the method proposed in this paper does not have a significant performance improvement over the SOTA methods such as MS-TCT.
More related methods such as [1,2,3,4] should be compared and discussed in the experimental part.
[1]Guan, Genliang, Zhiyong Wang, Shiyang Lu, Jeremiah Da Deng, and David Dagan Feng. "Keypoint-based keyframe selection." IEEE Transactions on circuits and systems for video technology 23, no. 4 (2012): 729-734. [2]Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. "Tracking objects as points." In European Conference on Computer Vision, pp. 474-490. Springer, Cham, 2020. [3]Tang, Hao, Hong Liu, Wei Xiao, and Nicu Sebe. "Fast and robust dynamic hand gesture recognition via key frames extraction and feature fusion." Neurocomputing 331 (2019): 424-433. [4]Li, Yixuan, Zixu Wang, Limin Wang, and Gangshan Wu. "Actions as moving points." In European Conference on Computer Vision, pp. 68-84. Springer, Cham, 2020.
The author did not provide the limitations and potential negative societal impact of their work.

Review Point: 4 (2012): 729-734. [2]Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. "Tracking objects as points." In European Conference on Computer Vision, pp. 474-490. Springer, Cham, 2020. [3]Tang, Hao, Hong Liu, Wei Xiao, and Nicu Sebe. "Fast and robust dynamic hand gesture recognition via key frames extraction and feature fusion." Neurocomputing 331 (2019): 424-433. [4]Li, Yixuan, Zixu Wang, Limin Wang, and Gangshan Wu. "Actions as moving points." In European Conference on Computer Vision, pp. 68-84. Springer, Cham, 2020. The author did not provide the limitations and potential negative societal impact of their work.
==================================================

Focused review:

Weakness:
Table 1,2 shows that fine-tuning a pruned network requires much more training time and tuning efforts to achieve a satisfying result. Although the author claims that OrthP can “complete” resolve the dynamical isometry issue, the pruned network converges still only after prolonged training epochs (90 and 900 epochs). Does that means the pruned networks weights are more difficult to optimize than random initialized ones, which contradicts the "the value of weights" claim?
The application area of the proposed OrthP is limited to MLP (Table 6). Although the author proposed a Strong L2 Regularization for more complicated networks, this section seems to be “unfinished” — the connection with dynamical isometry seems a little bit weak to me, can the authors provides more elaborations? This seems to be more of practical interests.
I have some confusions regarding the experiment designs and presented results (see the Questions section below). Questions:
For Table 1: In “Our rerun”, are “Scratch” results trained under 120 epochs with a decayed LR schedule? For Table 2, what are the hyper-parameters for the “Scratch” results, are they carefully tuned?
Have the authors investigated the JSV of randomly initialized sparse networks? I am not sure if applying OrthP initialization or other orthogonal regularization methods on “Scratch” can also boost its performance. I believe this would be a fairer comparison to support "the value of weights" claim.
Table 5: Can the authors also provide the “Scratch” results and their hyper-parameters? I am asking because the reasons stated in Weakness 1) — if the “Scratch” result is comparable to OrthP without using prolonged epochs, then this means even when the dynamical isometry is recovered, the pruning selected weights need more training time to optimize than randomly initialized ones. Also, in Table 2, it looks like “Scratch” results work well under large PR, which is exactly the case in Table 5.
Related to 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. It would be nice to see if OrthP works for non-prolonged epochs, e.g., < 90.

Review Point: 1) — if the “Scratch” result is comparable to OrthP without using prolonged epochs, then this means even when the dynamical isometry is recovered, the pruning selected weights need more training time to optimize than randomly initialized ones. Also, in Table 2, it looks like “Scratch” results work well under large PR, which is exactly the case in Table 5. Related to
Review Point: 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. It would be nice to see if OrthP works for non-prolonged epochs, e.g., < 90.
==================================================

Focused review:

Weaknesses:
I feel that some of the algorithmic details are not clearly explained, particularly the connection between Algorithm 1 (for training HN-GFN) and BO. For example, does the dataset D
correspond to the currently available observations from all previous iterations of BO? Does the reward function R
here correspond to the acquisition function calculated in BO? How is the set of target preference vectors built? More importantly, do you need to run Algorithm 1 after every iteration (or every batch) of BO? If yes, then the computational costs may become an issue and hence should discussed.
I think Section 4.3 needs to be revised to make it clearer, in the current form, it's not easy to understand.
Top paragraph of page 2: it's still unclear to me why limitation 1) makes "existing discrete molecular optimization methods" not applicable as "acquisition function optimizer". Can't you simply use those acquisition functions which directly take diversity into account? For example, if you use the GP-BUCB acquisition function from paper [1] below, to select an input in a batch, you can simply invoke an existing discrete molecular optimization method to maximize the acquisition function (whose GP posterior standard deviation is updated every time a new input is selected), which will naturally lead to a diverse set of inputs in a batch.
[1] Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization, JMLR 2014
Top paragraph of page 9: the number of rounds N = 8
is in fact unusually small in BO, and the batch size b = 100
is also unusually large for BO as well. Are these choices the common practice in molecular optimization using GFlowNet?
(minor) In the Related Work section, the previous works on multi-objective BO should also be discussed.

Review Point: 1) makes "existing discrete molecular optimization methods" not applicable as "acquisition function optimizer". Can't you simply use those acquisition functions which directly take diversity into account? For example, if you use the GP-BUCB acquisition function from paper [1] below, to select an input in a batch, you can simply invoke an existing discrete molecular optimization method to maximize the acquisition function (whose GP posterior standard deviation is updated every time a new input is selected), which will naturally lead to a diverse set of inputs in a batch. [1] Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization, JMLR 2014 Top paragraph of page 9: the number of rounds N = 8 is in fact unusually small in BO, and the batch size b = 100 is also unusually large for BO as well. Are these choices the common practice in molecular optimization using GFlowNet? (minor) In the Related Work section, the previous works on multi-objective BO should also be discussed.
==================================================

Focused review:

My main concern about the manuscript is the lack of analysis. For study like this, not only the experiment results are important, but understanding underlying reasons/patterns/conditions for the result is more important for me. For example: * Since CNNs vs. CNNs achieve higher scores than humans vs. humans, what are the differences in terms of the errors? * BrainScore has many conditions, e.g., the cortical regions (V1, V2, V4, IT, etc.)/object categories/neural recording vs. behavioral, etc. We have to understand the error consistency under these different dimensions and analyze the insights. More figures/tables regarding this are needed. * For figure 3(c), why do silhouette experiments show a linear correlation between CNNs and humans, compared with “cue conflict” and “edge”? Again, conditional analysis is needed. Also, I am a bit confused about the motivation of using “cue conflict”, “edge”, and “silhouette” for experiments. Normal images should be used as baseline as well. In addition, although the authors argue the CORnet-S model has different processing mechanisms from the human brain given the low score vs. humans, the model does perform well on brain predictivity. The authors should discuss a better unifying way to present the score in the BrainScore benchmark so every valuable aspect is not missing. ======Post-rebuttal======= The authors have addressed most of my concerns (comparison on natural images, more analyses across models and conditions), so I would like to increase my score accordingly. I do think the metric of error consistency is useful and can contribute to further research endeavors in the literature.

Review Point: * Since CNNs vs. CNNs achieve higher scores than humans vs. humans, what are the differences in terms of the errors?
Review Point: * BrainScore has many conditions, e.g., the cortical regions (V1, V2, V4, IT, etc.)/object categories/neural recording vs. behavioral, etc. We have to understand the error consistency under these different dimensions and analyze the insights. More figures/tables regarding this are needed.
Review Point: * For figure 3(c), why do silhouette experiments show a linear correlation between CNNs and humans, compared with “cue conflict” and “edge”? Again, conditional analysis is needed. Also, I am a bit confused about the motivation of using “cue conflict”, “edge”, and “silhouette” for experiments. Normal images should be used as baseline as well. In addition, although the authors argue the CORnet-S model has different processing mechanisms from the human brain given the low score vs. humans, the model does perform well on brain predictivity. The authors should discuss a better unifying way to present the score in the BrainScore benchmark so every valuable aspect is not missing. ======Post-rebuttal======= The authors have addressed most of my concerns (comparison on natural images, more analyses across models and conditions), so I would like to increase my score accordingly. I do think the metric of error consistency is useful and can contribute to further research endeavors in the literature.
==================================================

Focused review:

Weakness:
-- The work is heavily dependent on FedBN. The main difference is that author of this work designed an adaptive interpolation parameter estimation method. This jeopardizes the novelty and technical contribution of the whole work. -- I am a little conservative about Eq. 4. If Eq. 4 stands, does that mean the u^l in Eq.3 tends to be 1? -- The improvement of the designed solutions in Table 5, is not significant on some datasets. For example, on OfficeHome, the CSAC achieves 64.35, and the proposed solution achieves 64.71, which is a marginal improvement.

Review Point: -- The work is heavily dependent on FedBN. The main difference is that author of this work designed an adaptive interpolation parameter estimation method. This jeopardizes the novelty and technical contribution of the whole work. -- I am a little conservative about Eq.
Review Point: 4. If Eq. 4 stands, does that mean the u^l in Eq.3 tends to be 1? -- The improvement of the designed solutions in Table 5, is not significant on some datasets. For example, on OfficeHome, the CSAC achieves 64.35, and the proposed solution achieves 64.71, which is a marginal improvement.
==================================================

Focused review:

Weaknesses/questions:
On objective 1: how can one compare doing LIME over the concatenated (x,z) to having A be diagonal?
On objective 2: Optimization over λ 1 , λ 2
is unclear, how can one systematically search over them to get intuitive analogies? Furthermore, why is α
set to 0 in all the experiments? What is the effect of α
both quantitatively and qualitatively ?
Figure 2: why are the words on the x and y axis shuffled from their original order? Also this kind of visualization is a bit hard to parse, is there a better way to visualize the cross weights (off diagonal elements of A) ?
Issues with user study: 1) using google forms and non-paid participants raises questions on the effort each put into performing the user study. 2) showing participants the same example with different explanation methods: as I understand, there are 10 test examples, and you show each participant the same 10 with 3 different random methods to explain. Thus the participant has access to 3 explanations when they get to third time they see a given example, this clearly biases the results. A correct methodology is to assign each participant to a condition ( an explanation baseline) and only show them 10 examples with explanations from that condition. Then you compare between conditions. 3) no alignment between objective of user study (replicate BB model scores) and practical use cases: supposedly the explanations are to check if the BB is correct or not, why wasn’t that the use case for the user study? I expect it’s because humans are perfect at judging similarity, then it might have been more interesting to introduce an end task where judging similarity is used.
I am not sure why is fidelity the “metric” to compare things across for judging similarity. Furthermore, it would have been helpful to understand the implications of a low generalized infidelity score and a high score. (Ramamurthy et al., 2020) also relies on comparing the feature importance weights, is it possible to do something here that is similar?

Review Point: 1) using google forms and non-paid participants raises questions on the effort each put into performing the user study.
Review Point: 2) showing participants the same example with different explanation methods: as I understand, there are 10 test examples, and you show each participant the same 10 with 3 different random methods to explain. Thus the participant has access to 3 explanations when they get to third time they see a given example, this clearly biases the results. A correct methodology is to assign each participant to a condition ( an explanation baseline) and only show them 10 examples with explanations from that condition. Then you compare between conditions.
Review Point: 3) no alignment between objective of user study (replicate BB model scores) and practical use cases: supposedly the explanations are to check if the BB is correct or not, why wasn’t that the use case for the user study? I expect it’s because humans are perfect at judging similarity, then it might have been more interesting to introduce an end task where judging similarity is used. I am not sure why is fidelity the “metric” to compare things across for judging similarity. Furthermore, it would have been helpful to understand the implications of a low generalized infidelity score and a high score. (Ramamurthy et al., 2020) also relies on comparing the feature importance weights, is it possible to do something here that is similar?
==================================================

Focused review:

* It seems quite computation- and communication- intensive and may limit its applicability in the real-world setting. * The idea is quite a natural extension into federated learning setting.

Review Point: * It seems quite computation- and communication- intensive and may limit its applicability in the real-world setting.
Review Point: * The idea is quite a natural extension into federated learning setting.
==================================================

Focused review:

Weakness:
For the Per-SSFL framework, the local (personalized) model and global model are used. The memory consumption aspect should be discussed. For resource-constrained edge clients, high memory cost could be an issue.
Although it mentioned in the implementation setting that the client number selected per round is 10, it is not clear how many total clients are used in the FL setting.
In Table 1, what is the FL method under the supervised setting?
In Figure 2, what does (0.5) for SSFL on IID and SSFL on non-IID mean?
As can be seen in Figure 2, the convergence rates for the IID and non-IID cases are quite similar. Can you provide an explanation for that?
After reading Appendix D and Figure 10 in Appendix, the experimental setting on GLD-23K is still not quite clear. For example, how the local training set for each client is generated? What about the label distribution? Also, it seems that the number of clients used on the GLD-23K is different from that on CIFAR-10.
For ease of comparison and implementation, it would be good to evaluate the method on more commonly used datasets such as CIFAR-100 and Tiny-ImageNet and other datasets besides vision datasets (e.g., text) for FL.
It would be interesting to see the SSFL results under different numbers of selected participant clients. Since λ
is an important parameter that balances consensus and personalization, its effect should be studied.
Minor issues:
Section 2.1, “… the local empirical risk over the heterogeneous dataset D k
.” -> D k
Figure 10 appeared in Sec. 5.2 and 5.3, it should be Fig. 2 and Fig. 3.
In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent.
Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)”
A careful proofread of the paper is highly recommended.

Review Point: 3. In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent. Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)” A careful proofread of the paper is highly recommended.
==================================================

Focused review:

Overall the idea introduced in the paper is novel and inspiring, though the regularization terms in Eq. 3 was proposed in [17]. The reviewer has some concerns about the experiments. 1. The experiments are mainly performed using different face datasets in the image size of 256*256. Can the approach be used for high-resolution settings or synthesis where the images contain many structured details? (e.g., urban driven images and indoor images)? 2. Since a key contribution is using Eq. 2 (F_i) to weighting the regularization terms (\theta_i - \theta_{S, i})^. Some straightforward options (baselines) should be compared. For example, give some fixed weights for each convolutional layer according to Figure. 2 (middle). Besides, How about removing F_i?

Review Point: 1. The experiments are mainly performed using different face datasets in the image size of 256*256. Can the approach be used for high-resolution settings or synthesis where the images contain many structured details? (e.g., urban driven images and indoor images)?
Review Point: 2. Since a key contribution is using Eq. 2 (F_i) to weighting the regularization terms (\theta_i - \theta_{S, i})^. Some straightforward options (baselines) should be compared. For example, give some fixed weights for each convolutional layer according to Figure. 2 (middle). Besides, How about removing F_i?
==================================================

Focused review:

Weaknesses 1. The rates for the smooth case depend on the dimension d (not the rank as in the Lipschitz case). While the authors show tight lower bounds up to factors that depend on |w*|, this lower bound is probably obtained by taking rank=d and therefore the upper bound may not have tight dependence on the rank. It is important to clarify this in the paper. 2. One minor weakness is that all the algorithms in the paper are somewhat standard as similar algorithms have been used in the literature. However, the obtained results and some of the techniques are interesting (such as using average stability instead of uniform stability to improve the rates), therefore I don’t consider this to be a real limitation. 3. The rates obtained for the smooth case are very similar to the rates for DP-SCO in ell_1 geometry [AFKT21, BGN21] which also have a phase transition. I’m not sure if there is any fundamental connection here but it may be useful to explore this a little and comment on this. 4. Why didn’t the authors use the algorithms from “private selection from private candidates” [LT18] for adaptivity to |w*|? This will only require to privatize the score functions (as [LT18] assumes a non-private one) but may be simpler than redoing everything from scratch. More minor comments: 1. Text in table 1 is too small and hard to read 2. Algorithm 1: gradient symbol is missing in line 4
References: [AFKT21] Private Stochastic Convex Optimization: Optimal Rates in ℓ1 Geometry [BGN21] Non-Euclidean Differentially Private Stochastic Convex Optimization [LT18] Private selection from private candidates

Review Point: 1. The rates for the smooth case depend on the dimension d (not the rank as in the Lipschitz case). While the authors show tight lower bounds up to factors that depend on |w*|, this lower bound is probably obtained by taking rank=d and therefore the upper bound may not have tight dependence on the rank. It is important to clarify this in the paper.
Review Point: 2. One minor weakness is that all the algorithms in the paper are somewhat standard as similar algorithms have been used in the literature. However, the obtained results and some of the techniques are interesting (such as using average stability instead of uniform stability to improve the rates), therefore I don’t consider this to be a real limitation.
Review Point: 3. The rates obtained for the smooth case are very similar to the rates for DP-SCO in ell_1 geometry [AFKT21, BGN21] which also have a phase transition. I’m not sure if there is any fundamental connection here but it may be useful to explore this a little and comment on this.
Review Point: 4. Why didn’t the authors use the algorithms from “private selection from private candidates” [LT18] for adaptivity to |w*|? This will only require to privatize the score functions (as [LT18] assumes a non-private one) but may be simpler than redoing everything from scratch. More minor comments:
Review Point: 1. Text in table 1 is too small and hard to read 2. Algorithm 1: gradient symbol is missing in line 4 References: [AFKT21] Private Stochastic Convex Optimization: Optimal Rates in ℓ1 Geometry [BGN21] Non-Euclidean Differentially Private Stochastic Convex Optimization [LT18] Private selection from private candidates
==================================================

Focused review:

I emphasize that the weaknesses of this paper are minor, in my view. - There is occasional exposition that I don't feel precisely captures causal inference. For example, in the introduction, the authors say "However, the price we pay for using observational data is lower certainty in our causal estimates, due to the possibility of unmeasured confounding, and the measured and unmeasured differences between the populations who were subject to different treatments." This isn't quite right --- for example, under unmeasured confounding, the estimates can be arbitrarily biased even if they are low variance. The estimate is simply not identified. Another example is in section 2 after Equation 2. We consider the expected treatment effect not just because the outcome is random, but because the individual level treatment effect (inside the expectation) cannot be identified without parametric assumptions on the causal model. - I couldn't really follow the section on negative sampling. It wasn't clear what its role was in this particular method --- does it produce better calibrated estimates of the variance of CATE? Or does it do something else. This is part where exposition can be improved to help the reader follow better.

Review Point: - There is occasional exposition that I don't feel precisely captures causal inference. For example, in the introduction, the authors say "However, the price we pay for using observational data is lower certainty in our causal estimates, due to the possibility of unmeasured confounding, and the measured and unmeasured differences between the populations who were subject to different treatments." This isn't quite right --- for example, under unmeasured confounding, the estimates can be arbitrarily biased even if they are low variance. The estimate is simply not identified. Another example is in section 2 after Equation 2. We consider the expected treatment effect not just because the outcome is random, but because the individual level treatment effect (inside the expectation) cannot be identified without parametric assumptions on the causal model.
Review Point: - I couldn't really follow the section on negative sampling. It wasn't clear what its role was in this particular method --- does it produce better calibrated estimates of the variance of CATE? Or does it do something else. This is part where exposition can be improved to help the reader follow better.
==================================================

Focused review:

Weaknesses: • The overall idea is not novel, while the specific of the methods suggested here are. The idea of reweighting the training data according to their anomalousness degree has been done before (e.g. recent references [2,3,4]). • To construct the memory bank (i.e., the reference of the normality), the authors propose to reject “the top τ% patches with the highest outlier scores”. Do they implicitly assume that the ratio of the noise in the training data is known in advance? This assumption is strong in a real-world environment where the ratio of the contamination may vary according to the collected training data. • Experiments on other challenging state-of-the-art datasets are desirable. • Limitations are not thoroughly discussed (e.g., the complexity).
Clarity: • The paper is not particularly well written. But, the main points are comprehensible. • The notations of the formulas are sometimes confusing, which makes it difficult to follow. For example, in line 193, “w” denotes the anomaly weight. However, w denotes the batch position at line 140.
Relation to prior work: The authors state that “we are the first one to study the abnormal detection with noisy data”. However, many studies have been proposed in this research field: e.g., [5,6,7]
Reproducibility: • The conducted experiments are well described. • It would be helpful if the entire code would be made accessible in case the paper is accepted.

Review Point: • The overall idea is not novel, while the specific of the methods suggested here are. The idea of reweighting the training data according to their anomalousness degree has been done before (e.g. recent references [2,3,4]).
Review Point: • To construct the memory bank (i.e., the reference of the normality), the authors propose to reject “the top τ% patches with the highest outlier scores”. Do they implicitly assume that the ratio of the noise in the training data is known in advance? This assumption is strong in a real-world environment where the ratio of the contamination may vary according to the collected training data.
Review Point: • Limitations are not thoroughly discussed (e.g., the complexity). Clarity:
Review Point: • The paper is not particularly well written. But, the main points are comprehensible.
Review Point: • The notations of the formulas are sometimes confusing, which makes it difficult to follow. For example, in line 193, “w” denotes the anomaly weight. However, w denotes the batch position at line 140. Relation to prior work: The authors state that “we are the first one to study the abnormal detection with noisy data”. However, many studies have been proposed in this research field: e.g., [5,6,7] Reproducibility:
Review Point: • It would be helpful if the entire code would be made accessible in case the paper is accepted.
==================================================

Focused review:

- Apart from ridge regression, there are some regression models like lasso regression the authors might try. - For Table 1, the notation should be aligned with its description. - Overall, I am quite curious why 2v2 accuracies for all hypotheses are just slightly better than the random chance. Why is that? Correct me if I am wrong, thanks.

Review Point: - Apart from ridge regression, there are some regression models like lasso regression the authors might try.
Review Point: - For Table 1, the notation should be aligned with its description.
Review Point: - Overall, I am quite curious why 2v2 accuracies for all hypotheses are just slightly better than the random chance. Why is that? Correct me if I am wrong, thanks.
==================================================

Focused review:

Weakness:
-There is a lack of qualitative analysis and discussion of the proposed method. -In Section 4.3 "Performance with different amount of negative pairs", it is not clear the reasoning of the provided observation from Figure 3a. -It is not clear the motivation behind selecting a teacher-student network for obtaining different views of the graph. These networks are normally used for knowledge transfer, but here used for contrastive learning. How is this more beneficial than an ensemble model w/o knowledge transfer step of Eq. 3. -The core difference of IGSD from CMC-graph is that CMC uses MI based contrastive between local patch representation and graph rep, while IGSD uses L2 based contrastive between 2 graph representations. Input, Encoders and projections are the same for both architectures. It could be useful to add some analysis to discuss these differences and their contributions to clearly understand the significance of IGSD. -This paper seems to have state-of-the-art results (although it is based on graph kernel). Why the results are not included?
Convolutional Kernel Networks for Graph-Structured Data, ICML-2020 =======================
after rebuttal:
I thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant.
Therefore, I will keep my original rating.

Review Point: -There is a lack of qualitative analysis and discussion of the proposed method. -In Section 4.3 "Performance with different amount of negative pairs", it is not clear the reasoning of the provided observation from Figure 3a. -It is not clear the motivation behind selecting a teacher-student network for obtaining different views of the graph. These networks are normally used for knowledge transfer, but here used for contrastive learning. How is this more beneficial than an ensemble model w/o knowledge transfer step of Eq.
Review Point: 3. -The core difference of IGSD from CMC-graph is that CMC uses MI based contrastive between local patch representation and graph rep, while IGSD uses L2 based contrastive between 2 graph representations. Input, Encoders and projections are the same for both architectures. It could be useful to add some analysis to discuss these differences and their contributions to clearly understand the significance of IGSD. -This paper seems to have state-of-the-art results (although it is based on graph kernel). Why the results are not included? Convolutional Kernel Networks for Graph-Structured Data, ICML-2020 ======================= after rebuttal: I thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant. Therefore, I will keep my original rating.
==================================================

Focused review:

Weaknesses: - General Discussion: This paper investigates sentiment signals in companies’ annual 10-K filing reports to forecast volatility. The authors evaluate information retrieval term weighting models which are seeded with a finance-oriented sentiment lexicon and expanded with word embeddings. PCA is used to reduce dimensionality before Support Vector Regression is applied for similarity estimation.
In addition to text-based features, the authors also use non-text-based market features (e.g. sector information and volatility estimates).
Multiple fusion methods to combine text features with market features are evaluated.
COMMENTS It would be interesting to include two more experimental conditions, namely 1) a simple trigram SVM which does not use any prior sentiment lexica, and 2) features that reflect delta-IDFs scores for individual features.
As an additional baseline, it would be good to see binary features.
This paper could corroborate your references: https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf

Review Point: -General Discussion: This paper investigates sentiment signals in companies’ annual 10-K filing reports to forecast volatility. The authors evaluate information retrieval term weighting models which are seeded with a finance-oriented sentiment lexicon and expanded with word embeddings. PCA is used to reduce dimensionality before Support Vector Regression is applied for similarity estimation. In addition to text-based features, the authors also use non-text-based market features (e.g. sector information and volatility estimates). Multiple fusion methods to combine text features with market features are evaluated. COMMENTS It would be interesting to include two more experimental conditions, namely
Review Point: 1) a simple trigram SVM which does not use any prior sentiment lexica, and
Review Point: 2) features that reflect delta-IDFs scores for individual features. As an additional baseline, it would be good to see binary features. This paper could corroborate your references: https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf
==================================================

Focused review:

Weaknesses ---
W1. The paper claims (e.g., in the abstract and the conclusion) to show that the proposed TTLM is a generalization of 2nd-order RNNs, RACs, and MI-RNNs. The TTLM model takes the form given in Eq (2). In order to say that TTLM generalizes another model, that model must be expressible in the form given in Eq (2). However, according to Claim 4.1 and 4.2, in order to be able to express 2nd-order RNNs and MI-RNNs additional nonlinearities need to be inserted into the formula in Eq (2), therefore yielding a model which is not expressible as a TTLM (i.e., as in Eq (2)). Therefore, it is incorrect to say that TTLM generalizes those models! If TTLM truly generalized these models, there shouldn't be a need to further modify Eq (2).
This is a bit like saying that the set of linear functions (i.e., matrices) generalize functions f : R m → R n
of the form f ( x ) = σ ( A x )
, where A
is a matrix σ
is a non-linear functions.
W2. The paper is full of typos, missing/redundant commas and periods, and notational inconsistencies that a careful proof reading should have caught; there are two inconsistencies already in the first sentence in Sec 1.
W3. On page 1, you say that "we propose a novel Tensor Train Language Model, as a first attempt to apply tensor networks on language modeling tasks". This makes it sound like applying tensor networks, and tensor trains in particular, to language modeling is new. But that's not the case. For example, Miller et al. (2021) use matrix product states (which is the same as tensor trains) with language modeling as a potential application. Your contribution needs to be clarified.
W4. The paper is difficult to follow in several places. For example:
The discussion in Sec 3.3 doesn't make sense. You insert Eq (4) into Eq (3) to get Eq (5)---which is the same as Eq (3). What is the point of this calculation?
Below Eq (5) you show diagrammatic notation and say that it represents the tensor A
---but doesn't that tensor network graph correspond to the contraction in Eq (2)?
The discussion at the start of Sec 4 is difficult to follow. TNLM is never defined.
How exactly is the slice operator T [ i ]
defined? This is unclear below Eq (6). Is the 2nd or 3rd index kept fixed when slicing?
In Fig 3: Plot (a) shows that f θ ( x ( j ) )
is contracted with the tensor M ( j )
. But then (b) and (c) seem to indicate that M ( j )
itself is a reshaped version (a matrix, rather than tensor) of a contraction that involves f θ ( x ( j ) )
. Also, since TTML-Large and TTML-Tiny add additional operations like reshape, it's not immediately clear that they can even be expressed on the form in Eq (2). Again, if additional functions need to be added into Eq (2) to make the model expressible in that format, then it's not a TTLM.
In Fig 4: The caption refers to labels (RNNs-100, RNNs-200, etc) that don't appear in the figure.
W5. The experiment results aren't represented properly. For example, you say that "TTLM-Large obtains the best PPL among these models," but that's not true. Two of the LSTM-based methods achieve lower PPL in Table 1 (for PTB). For the PTB dataset, the PPL for TTLM-Large is bolded even though it's not the best number.

Review Point: --- W1. The paper claims (e.g., in the abstract and the conclusion) to show that the proposed TTLM is a generalization of 2nd-order RNNs, RACs, and MI-RNNs. The TTLM model takes the form given in Eq (2). In order to say that TTLM generalizes another model, that model must be expressible in the form given in Eq (2). However, according to Claim 4.1 and 4.2, in order to be able to express 2nd-order RNNs and MI-RNNs additional nonlinearities need to be inserted into the formula in Eq (2), therefore yielding a model which is not expressible as a TTLM (i.e., as in Eq (2)). Therefore, it is incorrect to say that TTLM generalizes those models! If TTLM truly generalized these models, there shouldn't be a need to further modify Eq (2). This is a bit like saying that the set of linear functions (i.e., matrices) generalize functions f : R m → R n of the form f ( x ) = σ ( A x ) , where A is a matrix σ is a non-linear functions.
==================================================

Focused review:

1. The methods are not novel as they are largely borrowing from existing work.
2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix.
3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset.
4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time.
1. The prequential evaluation is well-written. It would be interesting to see more such analysis and discussion of the datasets.
2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction?
3. As your dataset is imbalanced, it would be better to see some analysis of the outputs.

Review Point: 1. The methods are not novel as they are largely borrowing from existing work.
Review Point: 2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix.
Review Point: 3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset.
Review Point: 4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time.
Review Point: 1. The prequential evaluation is well-written. It would be interesting to see more such analysis and discussion of the datasets.
Review Point: 2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction?
Review Point: 3. As your dataset is imbalanced, it would be better to see some analysis of the outputs.
==================================================

Focused review:

1. Some experimental results do not support the authors’ claim on the effectiveness of the proposed method, or require further explanation. 2. The proposed linear evaluation of RoCL is a very standard adversarial learning technique and the significance of the proposed transformation smoothed inference is limited.

Review Point: 1. Some experimental results do not support the authors’ claim on the effectiveness of the proposed method, or require further explanation.
Review Point: 2. The proposed linear evaluation of RoCL is a very standard adversarial learning technique and the significance of the proposed transformation smoothed inference is limited.
==================================================

Focused review:

Parts of the writing could be improved: 1. It is not clear why the assumption that the objective is strongly concave is needed. It will be beneficial if the authors comment on it in the paper. 2. It seems that the bounds are loose at several points and it was good if the authors would comment on the bound and whether they believe there is room for improvement. For example in Corr 1. the linear oracle bound scales as standard for cond. grad. but then there is also kappa^2 and instead of having something like D_x^2+D_y^2 we have D_x^2D_y^2. I strongly suspect this is suboptimal and it will be good if the authors relate to it. Also in stochastic case, since objective is strongly concave we may expect that parts of the sample complexity will only scale linearly with 1/eps but they all scale with 1/eps^2. Eq between lines 143-144: minimization over v? (not u) Line 4 of Alg 3: not clear what we get v_k here as one of the outputs of prox-step if its updated in the following line via CndG

Review Point: 1. It is not clear why the assumption that the objective is strongly concave is needed. It will be beneficial if the authors comment on it in the paper.
Review Point: 2. It seems that the bounds are loose at several points and it was good if the authors would comment on the bound and whether they believe there is room for improvement. For example in Corr 1. the linear oracle bound scales as standard for cond. grad. but then there is also kappa^2 and instead of having something like D_x^2+D_y^2 we have D_x^2D_y^2. I strongly suspect this is suboptimal and it will be good if the authors relate to it. Also in stochastic case, since objective is strongly concave we may expect that parts of the sample complexity will only scale linearly with 1/eps but they all scale with 1/eps^2. Eq between lines 143-144: minimization over v? (not u) Line 4 of Alg 3: not clear what we get v_k here as one of the outputs of prox-step if its updated in the following line via CndG
==================================================

Focused review:

1) I think it might be hard to extend.The architecture is designed for this specific environment. Would the grammar construction process be needed for every new test setting? 2) I find some terminologies used in the paper confusing. For example, I wonder what SR (success rates) refer to and why it could captures "generalization".

Review Point: 1) I think it might be hard to extend.The architecture is designed for this specific environment. Would the grammar construction process be needed for every new test setting?
Review Point: 2) I find some terminologies used in the paper confusing. For example, I wonder what SR (success rates) refer to and why it could captures "generalization".
==================================================

Focused review:

- The main weakness of the work relates to the computational complexity of 1) computing the local subgraphs (are shortest paths computed ahead of the training process?), 2) evaluating each node's label individually. Can authors comment on the impact on training/evaluation time? - Another important missing element from the paper is the value of neighborhood size h, as well as an analysis of its influence over the model's performance. This is the key parameter of the proposed strategy and providing readers with intuitive knowledge of the value of h to use, and the robustness of the method with respect to larger or smaller neighborhoods is essential. Similarly, different hyperparameter sets are used per dataset, which is not ideal. Can authors provide insights into how performance varies with a constant set of parameters? - Certain aspects of the training set-up needs clarifying. Mainly the task generation process (what constitutes a task, can one task contain multiple graphs, are local substructures randomly sampled regardless of the original graph, are all nodes labelled in the training set, etc) - Certain sections are too condensed and would be much clearer and informative if expanded (e.g. related work, training setup, testing setup, baseline methods). In the interest of space, table 1 could be moved to supplementary.

Review Point: - The main weakness of the work relates to the computational complexity of
Review Point: 1) computing the local subgraphs (are shortest paths computed ahead of the training process?),
Review Point: 2) evaluating each node's label individually. Can authors comment on the impact on training/evaluation time?
Review Point: - Another important missing element from the paper is the value of neighborhood size h, as well as an analysis of its influence over the model's performance. This is the key parameter of the proposed strategy and providing readers with intuitive knowledge of the value of h to use, and the robustness of the method with respect to larger or smaller neighborhoods is essential. Similarly, different hyperparameter sets are used per dataset, which is not ideal. Can authors provide insights into how performance varies with a constant set of parameters?
Review Point: - Certain aspects of the training set-up needs clarifying. Mainly the task generation process (what constitutes a task, can one task contain multiple graphs, are local substructures randomly sampled regardless of the original graph, are all nodes labelled in the training set, etc) - Certain sections are too condensed and would be much clearer and informative if expanded (e.g. related work, training setup, testing setup, baseline methods). In the interest of space, table 1 could be moved to supplementary.
==================================================

Focused review:

Weaknesses
However, below are some major concerns on the tractability of the reformulated single-level problem and the lack of supporting simulations.
1. The paper proposes using switching gradient descent or primal-dual method to solve the resultant single-level constrained optimization problem, yet the conditions needed for these methods to converge may be difficult to hold.
1a) In Assumption 2, f ( z )
and each entry of h ( z )
are assumed to be strongly-convex in z = ( x , y , ω )
, which cannot be true. First, f ( z ) = f ( x , y )
is a constant function with respect to the Lagrange multiplier ω
, and thus cannot be strongly-convex with respect to ω
. Thus, f ( z )
cannot be jointly strongly-convex in z
. Also, the entries of h ( z )
cannot all be strongly-convex since there exist additive-inverse pairs like the second entry and the third entry. If the second entry of h ( z )
is strongly-convex, then the third entry is strongly-concave, which contradicts the assumption. Furthermore, all entries of h(z) are linear in ω
, and thus cannot be strongly convex in ω and z .
1b) In the last sentence of Assumption 2, I am not sure z ~
is strictly feasible for which problem. If it is assumed to be strictly feasible for the problem (6), then this assumption does not hold since there are opposite entries in h ( z )
, e.g., if the second entry is satisfied strictly, then the third entry is violated.
1c) A relatively minor concern is that Algorithms 1 and 2 both require the projection to the set Z, which can be impractical. The projection to Z includes the projection to Y
. Yet by Proposition 2, Y
must contain the (relaxed) lower-level optimality point set which is unknown. Thus Y
is a set that is hard to know a-priori and the projection to Y
is not practical. Justifying this projection by arguing that Y
can be chosen as large as possible is not reasonable since the convergence rate has at least a polynomial dependence on the size of Y
. The rate will become trivial for a Y
that can be chosen arbitrarily large.
2. The experiments provided in this work are not convincing enough.
2a) The two tasks in simulations are toyish as they both use synthetic data. They also do not serve to verify the theorems since Assumption 2 is violated in these examples. It would be better to provide sufficient empirical support for the method.
2b) There is no baseline algorithm so the performance of the algorithm cannot be accurately evaluated.

Review Point: 2. The experiments provided in this work are not convincing enough. 2a) The two tasks in simulations are toyish as they both use synthetic data. They also do not serve to verify the theorems since Assumption 2 is violated in these examples. It would be better to provide sufficient empirical support for the method. 2b) There is no baseline algorithm so the performance of the algorithm cannot be accurately evaluated.
==================================================

Focused review:

Weakness: 1) Although each part of the proposed method is effective, the overall algorithm is still cumbersome. It has multiple stages. In contrast, many of existing pruning methods do not need fine-tuning. 2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty. 3) The experimental results are not convincing. The compared methods are few. Although few authors have attempted to prune EfficientNet, other networks can be compressed in experiments such as ResNet. In addition, the performance gains compared with SOTAs are also marginal, which are also within 1%. 4) The paper is poorly written. There are many typos and some are listed as follows: --In caption of Figure 2, “An subset of a network” should be “A subset of a network”. --In Line157 of Page4, “The output output vector” should be “The output vector”. --In Line283 of Page7, “B0V2 as,” should be “B0V2 as teacher,”. --In Line301 of Page7, “due the inconsistencies” should be “due to the inconsistencies”.

Review Point: 1) Although each part of the proposed method is effective, the overall algorithm is still cumbersome. It has multiple stages. In contrast, many of existing pruning methods do not need fine-tuning.
Review Point: 2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty.
Review Point: 3) The experimental results are not convincing. The compared methods are few. Although few authors have attempted to prune EfficientNet, other networks can be compressed in experiments such as ResNet. In addition, the performance gains compared with SOTAs are also marginal, which are also within 1%.
==================================================

Focused review:

Weaknesses
About the motivation.
Though the reproducity of scientific findings are very important, I do not really understand why we need to design reproducible algorithms for bandit problems. Maybe one can claim that we can reproduce others' experimental results easily when the designed algorithms are reproducible, but in this case, I think i) the definition of reproducible (Definition 1) is too strict; and ii) the cost is too large. And I guess that ii) maybe a consequence of i). In fact, to reproduce others' experimental results, we do not really need the sequence to be exact the same.
About the regret upper bounds.
The regret upper bound seems to be not very tight. For example, when we choose ρ → 1
, the algorithms does not have any reproducible properties, but we still suffer an extra K 2
factor in the regret upper bound. What is the reason of this?
Lack of regret lower bounds.
I guess that the regret lower bound analysis could be difficult, especially for the linear bandit case. However, I think some discussions would be very helpful here. After I read the paper, I cannot really understand the difficulty of designing reproducible algorithms, and also do not know which kinds of instances could be very hard to solve in this case. Besides, it is mentioned in the comclusion that the factor of 1 ρ 2
is tight (according to (Impagliazzo et al., 2022)). However, after I read that paper, I do not think its proof can be directly used for the bandit case. Maybe I miss some important things, but I think there should be more detailed explanations.

Review Point: 1) is too strict; and ii) the cost is too large. And I guess that ii) maybe a consequence of i). In fact, to reproduce others' experimental results, we do not really need the sequence to be exact the same. About the regret upper bounds. The regret upper bound seems to be not very tight. For example, when we choose ρ → 1 , the algorithms does not have any reproducible properties, but we still suffer an extra K 2 factor in the regret upper bound. What is the reason of this? Lack of regret lower bounds. I guess that the regret lower bound analysis could be difficult, especially for the linear bandit case. However, I think some discussions would be very helpful here. After I read the paper, I cannot really understand the difficulty of designing reproducible algorithms, and also do not know which kinds of instances could be very hard to solve in this case. Besides, it is mentioned in the comclusion that the factor of 1 ρ 2 is tight (according to (Impagliazzo et al., 2022)). However, after I read that paper, I do not think its proof can be directly used for the bandit case. Maybe I miss some important things, but I think there should be more detailed explanations.
==================================================

Focused review:

Weaknesses
1. Literature Review
The paper regrettably fails to acknowledge a vast body of related literature, on (i) intention-conditioned trajectory prediction, (ii) variational graph methods for trajectory prediction, and (iii) models that explicitly model social interactions for forecasting. At the very least, these references ought to be mentioned and discussed for a diligent representation of the research space, even if the methods are not directly compared against.
(i) Intention-Conditioned Trajectory Prediction:
[R1, R2, R3] talk about intention-conditioned trajectory prediction for autonomous vehicles. Apart from the data the methods are applied to, the architectures can be applicable to, and are relevant for, the problem being addressed here. Crucially, the DROGON paper defines intention explicitly (more on this in Weakness 2. below).
(ii) Variational Graph Methods:
[R4] from the Neurips I Can't Believe It's Not Better Workshop explicitly deals with graph conditional variational methods for multi-agent trajectory prediction. The results in that paper are very relevant for this research area and should be included.
(iii) Encoding Social Interactions:
Graph and other stochastic methods that encode social interactions between agents have been long applied to trajectory and behavior forecasating problems. [R5] explicitly incorporates a spatiotemporal graph for incorporating social interactions between agents. [R6] more recently explicitly takes a meta-learning approach for modeling the dynamics unique to a group for probabilistic forecasting. A sports team is a group, and if each team is viewed as having unique social dynamics resulting from the team's strategy then [R6]'s core modeling idea is directly applicable. The cue in [R6] terms is simply player location here. Their modeling of social influence of other agents is also permutation invariant, a limitation this paper claims about existing methods. References:
[R1] DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning - Choi et al.
[R2] Intention-Driven Trajectory Prediction for Autonomous Driving - Fan et al.
[R3] LOKI: Long Term and Key Intentions for Trajectory Prediction - Girase et al.
[R4] Graph Conditional Variational Models: Too Complex for Multiagent Trajectories? - Rudolph et al.
[R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al.
[R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.
2. Unsupported claims and definitions
The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.
3. Notation
There are a few notational errors. For instance, the variable used for the sequence cannot be the same as the individual elements: x < t = [ x 1 , . . . ]
. See [R4] for this. In many places there exist grammatical errors and incomplete sentences. Please do a pass to fix these.

Review Point: - Rudolph et al. [R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al. [R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.
Review Point: 2. Unsupported claims and definitions The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.
Review Point: 3. Notation There are a few notational errors. For instance, the variable used for the sequence cannot be the same as the individual elements: x < t = [ x 1 , .
Review Point: .. ] . See [R4] for this. In many places there exist grammatical errors and incomplete sentences. Please do a pass to fix these.
==================================================

Focused review:

1. Only the reporting clause is examined while the that clause that contains the statement is ignored: In previous bias probing studies, the input content is the entire sentence with the complete context. However, in this paper, only the prompt part (reporting clause) is fed to the language model for analysis. Therefore, the proposed intervention setup effectively only focuses on word level bias probing. 2. Only Nouns are Examined: While it is obvious nouns/professions embody gender bias, verbs could also reveal gender bias. In the templates shown in Figure 8 in the Appendix, the verb “cry” or “drive” could embody implicit bias. However, under the current framework, such potential biases are not investigated. 3. Inter-sentential interactions not considered: For a more comprehensive analysis at the sentence level or beyond, it will require more contextual understanding with more variety of syntactic structures. Therefore, the conclusions drawn in this study that gender bias effects are concentrated in specific components of the model may not generalize well when more complex syntactic and semantic structures and interactions are considered. May I know if the corpus examined contains slightly more complex examples like the following: The nurse cried when he started cooking dinner at home. “nurse”, “cry” and “cook dinner at home” are potentially more likely to be associated with females. How would such a sentence change the probing analysis? How about a sentence like the following? The officer killed the suspect but got away with it due to her political clout. The words/phrases “officer”, “kill”, and “political clout” might have stronger associations with the male gender

Review Point: 1. Only the reporting clause is examined while the that clause that contains the statement is ignored: In previous bias probing studies, the input content is the entire sentence with the complete context. However, in this paper, only the prompt part (reporting clause) is fed to the language model for analysis. Therefore, the proposed intervention setup effectively only focuses on word level bias probing.
Review Point: 2. Only Nouns are Examined: While it is obvious nouns/professions embody gender bias, verbs could also reveal gender bias. In the templates shown in Figure 8 in the Appendix, the verb “cry” or “drive” could embody implicit bias. However, under the current framework, such potential biases are not investigated.
Review Point: 3. Inter-sentential interactions not considered: For a more comprehensive analysis at the sentence level or beyond, it will require more contextual understanding with more variety of syntactic structures. Therefore, the conclusions drawn in this study that gender bias effects are concentrated in specific components of the model may not generalize well when more complex syntactic and semantic structures and interactions are considered. May I know if the corpus examined contains slightly more complex examples like the following: The nurse cried when he started cooking dinner at home. “nurse”, “cry” and “cook dinner at home” are potentially more likely to be associated with females. How would such a sentence change the probing analysis? How about a sentence like the following? The officer killed the suspect but got away with it due to her political clout. The words/phrases “officer”, “kill”, and “political clout” might have stronger associations with the male gender
==================================================

Focused review:

The main weaknesses of the paper are 1) the lack of comparison to other biologically plausible rules, 2) the simplicity of the datasets the rule is tested on, and 3) several unsubstantiated points from the abstract. (1) While the paper compares the performance of their proposed rule with various kernels across 4 small datasets, the paper lacked a thorough comparison to other state-of-the-art biologically plausible learning rules. (2) Even on the relatively simple datasets of MNIST and CIFAR10, the proposed methods significantly underperform small networks trained with SGD. Moreover, the most biologically plausible of the proposed kernels - the Gaussian kernel - performs the worst of all methods displayed. This suggests that the proposed rule does not scale to larger problems. (3) The abstract suggests the new rule is novel due to its superior performance in the absence of large amounts of data and the absence of an inference/learning bifurcation. Neither of these claims were adequately supported by the data presented. For one, evidence was not supplied suggesting these new methods performed better than competing biologically plausible rules when trained on little data. Second, the inference/learning bifurcation contained in the weight updates suggested here did not appear any different from previously proposed methods.

Review Point: 1) the lack of comparison to other biologically plausible rules,
Review Point: 2) the simplicity of the datasets the rule is tested on, and
Review Point: 3) several unsubstantiated points from the abstract. (1) While the paper compares the performance of their proposed rule with various kernels across 4 small datasets, the paper lacked a thorough comparison to other state-of-the-art biologically plausible learning rules. (2) Even on the relatively simple datasets of MNIST and CIFAR10, the proposed methods significantly underperform small networks trained with SGD. Moreover, the most biologically plausible of the proposed kernels - the Gaussian kernel - performs the worst of all methods displayed. This suggests that the proposed rule does not scale to larger problems. (3) The abstract suggests the new rule is novel due to its superior performance in the absence of large amounts of data and the absence of an inference/learning bifurcation. Neither of these claims were adequately supported by the data presented. For one, evidence was not supplied suggesting these new methods performed better than competing biologically plausible rules when trained on little data. Second, the inference/learning bifurcation contained in the weight updates suggested here did not appear any different from previously proposed methods.
==================================================

Focused review:

1. The writing and presentation of this paper can be further improved. 2. The proposed method is not compared with the SOTA methods, and some important baselines are missing. 3. The codes and datasets are not provided, which decreases the reproducibility. I understand that the data may be confidential, but the authors should at least provide the codes for their algorithm.

Review Point: 1. The writing and presentation of this paper can be further improved.
Review Point: 2. The proposed method is not compared with the SOTA methods, and some important baselines are missing.
Review Point: 3. The codes and datasets are not provided, which decreases the reproducibility. I understand that the data may be confidential, but the authors should at least provide the codes for their algorithm.
==================================================

Focused review:

1) The method proposed in the paper is limited to the domain of speech denoising. 2) The model relies on silent interval detection in order to work at it's full potential. However, the paper clearly mentions this and shows that their model can also work without ground truth silent intervals.

Review Point: 1) The method proposed in the paper is limited to the domain of speech denoising.
Review Point: 2) The model relies on silent interval detection in order to work at it's full potential. However, the paper clearly mentions this and shows that their model can also work without ground truth silent intervals.
==================================================

Focused review:

Weaknesses: Some parts of the paper are hard to follow. It is unclear to me why D((e, p, o)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is explained as the product of how often e has been observed with some property and the weight of that property for the class MH. In addition, it also seems unclear how effective introducing compositional models itself is in increasing the coverage. I think one of the major factors of the increase of the coverage is the modifier expansion, which seems to also be applicable to the baseline 'Hearst'. It would be interesting to see the scores 'Hearst' with modifier expansion.
- General Discussion: Overall, the task is interesting and the approach is generally solid. However, since this paper has weaknesses described above, I'm ambivalent about this paper.
- Minor comment: I'm confused with some notations. For example, it is unclear for me what 'H' stands for. It seems that 'H' sometimes represents a class such as in (e, H) (- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is my understanding correct?
In Paragraph "Precision-Recall Analysis", why the authors use area under the ROC curve instead of area under the Precision-Recall curve, despite the paragraph title "Precision-Recall Analysis"?
- After reading the response: Thank you for the response. I'm not fully satisfied with the response as to the modifier expansion. I do not think the modifier expansion can be applied to Hearst as to the proposed method. However, I'm wondering whether there is no way to take into account the similar modifiers to improve the coverage of Hearst. I'm actually between 3 and 4, but since it seems still unclear how effective introducing compositional models itself is, I keep my recommendation as it is.

Review Point: -General Discussion: Overall, the task is interesting and the approach is generally solid. However, since this paper has weaknesses described above, I'm ambivalent about this paper.
Review Point: - Minor comment: I'm confused with some notations. For example, it is unclear for me what 'H' stands for. It seems that 'H' sometimes represents a class such as in (e, H) (- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is my understanding correct? In Paragraph "Precision-Recall Analysis", why the authors use area under the ROC curve instead of area under the Precision-Recall curve, despite the paragraph title "Precision-Recall Analysis"? -
==================================================

Focused review:

1. The mathematical rigorousness renders the reading much more difficult which I don't think it's necessary for a ML paper. 2. The connection between the discretized versions and Laplace regularization for semi-supervised learning should be made more clear. It was not clear for me why it is relevant to consider the discretized versions. 3. Since the loss minimization problems are wrt functions, the problems are convex, but in practice, we don't optimize high-dimensional functions directly; instead, a parameterization of the function is introduced and the loss minimization wrt the parameters is not necessarily convex. Can the results of this paper be extended to those cases? Are these results applicable for neural networks (since the main applications of Lip constraints are there)?

Review Point: 1. The mathematical rigorousness renders the reading much more difficult which I don't think it's necessary for a ML paper.
Review Point: 2. The connection between the discretized versions and Laplace regularization for semi-supervised learning should be made more clear. It was not clear for me why it is relevant to consider the discretized versions.
Review Point: 3. Since the loss minimization problems are wrt functions, the problems are convex, but in practice, we don't optimize high-dimensional functions directly; instead, a parameterization of the function is introduced and the loss minimization wrt the parameters is not necessarily convex. Can the results of this paper be extended to those cases? Are these results applicable for neural networks (since the main applications of Lip constraints are there)?
==================================================

Focused review:

Some (minor) weaknesses: - The image experiments for different augmentations are only run on MNIST. Given that all the code is already set up, I think it should be relatively easy to run this also on e.g. CIFAR10. Since the difference in performance is usually bigger on this dataset, it would be interesting to see if the augmentation strategies help more in this case. - It would also be nice to see a discussion of which aspects of the improvements proposed in the paper could be relevant for continuous normalizing flows. For example, augmentation is in general hard to use in the context of CNFs since it requires marginalizing out the added dimensions. It would be interesting if you could comment on how some of the improvements you proposed could be used in this context (e.g. depth variance, data control, adaptive depth and so on).

Review Point: - It would also be nice to see a discussion of which aspects of the improvements proposed in the paper could be relevant for continuous normalizing flows. For example, augmentation is in general hard to use in the context of CNFs since it requires marginalizing out the added dimensions. It would be interesting if you could comment on how some of the improvements you proposed could be used in this context (e.g. depth variance, data control, adaptive depth and so on).
==================================================

Focused review:

1. All tasks are toy/synthetic, and there is no experiment result on more realistic tasks. Section 6.4 mentions "human activity recognition" and ECG readings. It would be interesting to see if SIRE helps there. That would make the results stronger. 2. Lack of details/reproducibility. The procedure to approximate SIRE by sampling is not described in details. This might make it hard to reproduce the results. In particular, in the definition of SIRE (eq 4.4), one is to sample s from S_{D, \Theta}. However, S_{D, \Theta} is a set, not a distribution. Should one sample from the uniform distribution on this set (which I think is hard)? Or any distribution would work? How do different distribution affect the result? In practice, how many samples should it be averaged over? Is this tuned with cross-validation, or does the same hyperparam work across experiments?

Review Point: 1. All tasks are toy/synthetic, and there is no experiment result on more realistic tasks. Section 6.4 mentions "human activity recognition" and ECG readings. It would be interesting to see if SIRE helps there. That would make the results stronger.
Review Point: 2. Lack of details/reproducibility. The procedure to approximate SIRE by sampling is not described in details. This might make it hard to reproduce the results. In particular, in the definition of SIRE (eq 4.4), one is to sample s from S_{D, \Theta}. However, S_{D, \Theta} is a set, not a distribution. Should one sample from the uniform distribution on this set (which I think is hard)? Or any distribution would work? How do different distribution affect the result? In practice, how many samples should it be averaged over? Is this tuned with cross-validation, or does the same hyperparam work across experiments?
==================================================

Focused review:

- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.
If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work.
- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ?

Review Point: - The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work.
Review Point: - The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ?
==================================================

Focused review:

Weakness:
The major weakness is the novelty. The main contributions are the error identification and a quantization ReLU activation, both of which have been proposed in some previous work [1], [2]. Also, the negative spike has been proposed by spiking-yolo. Please clarify the difference, or is this work a combination of these techniques?
Experiments are not enough to demonstrate the effectiveness of the proposed method. For example, it is mentioned in line 177 that λ is learnable, according to Eqn. 9. Please clarify whether T needs to be set. If T needs to be set, what is it equal to?
The authors use a newly proposed neuron model which may not be supported by the current hardware, which will diminish the significance of this paper.
There are some ambiguous expressions in the paper that are hard to understand.
[1] Yan, Z., Zhou, J., & Wong, W. F. (2021). Near lossless transfer learning for spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence. [2] Bu, T., Fang, W., Ding, J., Dai, P., Yu, Z., & Huang, T. (2022). Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks. In International Conference on Learning Representations.
This paper does not do enough quantitative analysis and explanation of the proposed negative spike method. Neurons that generate negative spikes belong to the special category of spiking neurons. Since the authors use a new type of neuron, and the new neuron is obviously more complex than the traditional neuron, the authors should include more analysis about the power consumption and hardware implementation of this neuron.

Review Point: 9. Please clarify whether T needs to be set. If T needs to be set, what is it equal to? The authors use a newly proposed neuron model which may not be supported by the current hardware, which will diminish the significance of this paper. There are some ambiguous expressions in the paper that are hard to understand. [1] Yan, Z., Zhou, J., & Wong, W. F. (2021). Near lossless transfer learning for spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence. [2] Bu, T., Fang, W., Ding, J., Dai, P., Yu, Z., & Huang, T. (2022). Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks. In International Conference on Learning Representations. This paper does not do enough quantitative analysis and explanation of the proposed negative spike method. Neurons that generate negative spikes belong to the special category of spiking neurons. Since the authors use a new type of neuron, and the new neuron is obviously more complex than the traditional neuron, the authors should include more analysis about the power consumption and hardware implementation of this neuron.
==================================================

Focused review:

- The first major concern is the limited methodological contribution compared to FDA. The proposed method just aggregates (i.e., sum) FDA objectives of multiple layers and adding the cross-entropy term like other attack methods; in other words, these approaches are straightforward. Although the improvements of the proposed method are meaningful, it is not surprising or interesting results. - Secondly, the comparision between TMIM/SGM and FDA-based frameworks seems to be unfair. TMIM/SGM methods do not use the training data for the white-box model while FDA-based frameworks use the data for training auxiliary functions g. In my opinion, access to only pre-trained white-box models largely differs from that to whole training data, and thus the latter uses more knowledge than the former. So the improvements over baselines seem to be somewhat overclaimed, especially when the white-box and black-box models are trained on the same dataset. If using the intermediate features is crucial in adversarial attack, then how to utilize the features without the training data? The authors partially cover this issue in "cross-distribution" scenarios (Section 4.2), but in that case the source's and target's label spaces are largely overlapped. I think a harder case should be considered; for example, all labels are exclusive, or the number of available training samples is small. - Is the greedy layer optimization important? How about selecting layers heuristically, for example, the feature maps right before the pooling layers? ========= I generally agree with the author's response about my concerns.

Review Point: - The first major concern is the limited methodological contribution compared to FDA. The proposed method just aggregates (i.e., sum) FDA objectives of multiple layers and adding the cross-entropy term like other attack methods; in other words, these approaches are straightforward. Although the improvements of the proposed method are meaningful, it is not surprising or interesting results.
Review Point: - Secondly, the comparision between TMIM/SGM and FDA-based frameworks seems to be unfair. TMIM/SGM methods do not use the training data for the white-box model while FDA-based frameworks use the data for training auxiliary functions g. In my opinion, access to only pre-trained white-box models largely differs from that to whole training data, and thus the latter uses more knowledge than the former. So the improvements over baselines seem to be somewhat overclaimed, especially when the white-box and black-box models are trained on the same dataset. If using the intermediate features is crucial in adversarial attack, then how to utilize the features without the training data? The authors partially cover this issue in "cross-distribution" scenarios (Section 4.2), but in that case the source's and target's label spaces are largely overlapped. I think a harder case should be considered; for example, all labels are exclusive, or the number of available training samples is small.
Review Point: - Is the greedy layer optimization important? How about selecting layers heuristically, for example, the feature maps right before the pooling layers? ========= I generally agree with the author's response about my concerns.
==================================================

Focused review:

1. The motivation takes too much of the paper and interesting methodological contributions such as elaborations of the concepts in Sec 3 is too brief. This section consists of too many concepts without enough intuition. 2. The assumptions like i.i.d returns across tasks and group-dependency should be discussed further. 3. The proof techniques should be discussed briefly in the paper. Specifically, the hardness of continuous time problem, Lyapunouv drift etc would be interesting to a theoretically inclined reader. 4. The empirical results are too primary with respect to the motivation. Some work on real data for one of the examples would satisfy the expectation built-up in introduction.

Review Point: 1. The motivation takes too much of the paper and interesting methodological contributions such as elaborations of the concepts in Sec 3 is too brief. This section consists of too many concepts without enough intuition.
Review Point: 2. The assumptions like i.i.d returns across tasks and group-dependency should be discussed further.
Review Point: 3. The proof techniques should be discussed briefly in the paper. Specifically, the hardness of continuous time problem, Lyapunouv drift etc would be interesting to a theoretically inclined reader.
Review Point: 4. The empirical results are too primary with respect to the motivation. Some work on real data for one of the examples would satisfy the expectation built-up in introduction.
==================================================

Focused review:

Although I am leaning towards accepting the paper, I still doubt the contribution of the picker for the generator. This picker only affects the generator if the T5 encoding representation is altered accordingly. I think the connection between the picker and the generator is somewhat weak. Meanwhile, the proposed `soft` labeling method seems useless, compared with the straightforward `hard` labeling. Considering the `hard` labeling method seems to just pick overlapped words between the incomplete utterances and the rewritten utterances, the nolvety of the proposed method seems smaller.
Besides the above, the contribution part (line 99-111) is not well structured. I would recommend to add experimental contributions to the third point.
There are several minor typos that should be fixed in the revision: - line 080: incorrect use of quotation marks and repeated quotation marks.
- line 151: why the length of R is not equal to the one of U?
- line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \softmax to represent the symbol - line 310: you should use \dot between \alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?
- line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one

Review Point: - line 080: incorrect use of quotation marks and repeated quotation marks.
Review Point: - line 151: why the length of R is not equal to the one of U?
Review Point: - line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \softmax to represent the symbol - line 310: you should use \dot between \alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?
Review Point: - line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one
==================================================

Focused review:

Weaknesses: - I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?

Review Point: - I don't understand effectiveness of the multi-view clustering approach. Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.
Review Point: - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
Review Point: -General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms. Some additional questions for the authors :
Review Point: - Lines 367-368 : Why does X^{P} need to be symmetric?
Review Point: - Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
Review Point: - Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
Review Point: - What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?
==================================================

Focused review:

There are some questions/concerns however. 1. Haven't you tried to set hyperparameters for the baseline models via cross-validation (i.e. the same method you used for your own model)? Setting it to their default values (even taken from other papers) may have a risk of unfair comparison aganist yours. I do not think this is the case but I would recommend the authors to carry out the corresponding experiments. 2. It is unclear for me why the performance of DNN+MMA becomes worse than vanilla DNN when lambda becomes small? See fig.3-4. I would expect it will approach vanilla methods from above but from below.

Review Point: 1. Haven't you tried to set hyperparameters for the baseline models via cross-validation (i.e. the same method you used for your own model)? Setting it to their default values (even taken from other papers) may have a risk of unfair comparison aganist yours. I do not think this is the case but I would recommend the authors to carry out the corresponding experiments.
Review Point: 2. It is unclear for me why the performance of DNN+MMA becomes worse than vanilla DNN when lambda becomes small? See fig.3-4. I would expect it will approach vanilla methods from above but from below.
==================================================

Focused review:

Weaknesses
The main weakness is that the relationship between ADU and constituency tree is not clearly described. Is it true that ADU are often phrases that occur in the constituency tree? How often does this happen? Does the new BERT-based model adhere to constituency constraints? Is the BENEPAR parser appropriate for this data? Based on Trautmann et al.’s comments on grammaticality and clauses, my intuition is that an ADU is almost always a phrase in the tree, in which case it is somewhat less surprising that it helps this task (and maybe it should be helping even more). If this is the case, it’s worth considering related work in distant supervision to include.
The treatment of constituency trees is haphazard. 1) At times it is not clear if dependency or constituency trees are being used; 2) Table 1 should be made more clear, what do the percentage values indicate, and why not use the full tree at all; 3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree.
(low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.

Review Point: 1) At times it is not clear if dependency or constituency trees are being used;
Review Point: 2) Table 1 should be made more clear, what do the percentage values indicate, and why not use the full tree at all;
Review Point: 3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree. (low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.
==================================================

Focused review:

The main weakness of the paper is not being able to separate the effect of "their choice of data augmentations" and "their method of selecting the k-best augmentations" in explaining the performance gain observed in the clean/corrupted datasets. Since they propose the latter, it would be nice to separate the effects and focus on the method. This could be done by the following: Not introducing new augmentations. They can use 5-crop x 2 right-left flips (=10) and use their method to predict which of the 10 augmentations produce the lowest loss. This can already improve the efficiency with respect to brute-force approach of using 10 augmentations without introducing new distortions. This may be challenging, as the authors note in the limitations section, since the network will learn to be invariant over these transformations in the training set (as crops and flips are used at training time). The authors failed to study their data augmentations. What is the performance of using them at training time? If they are used at training time, it is still necessary to have the loss-prediction network or all the gain is explained by the introduction of new augmentations? Finally, the choice of data augmentations can be particularly chosen to "undo" the corruptions of the corrupted dataset and artificially boost the performance in the corruption dataset. Their chosen augmentations include "auto-contrast"and "sharpness" that can potentially revert the distortions "contrast"and "Gaussian noise"present in the corruption test set. Again, it is hard to separate the gains from the prediction network and the data augmentations introduced in section 3.2. One of the fair comparisons is the random baseline, where they show that their method is able to pick a better prediction than the one coming from a random augmentation. I suppose this can be an easy task if one of the augmentations degrade a lot performance. ==UPDATE== The authors addressed the weaknesses I pointed out in my initial review. 1) Although the improvement is small compared to their chosen augmentations, their method also improves accuracy when using training time augmentations which is very attractive to reduce the inference cost of test-time augmentation. 2) I believe the robustness improvements in imagenet-c and cifar-10c benefit from the author's particular choice of augmentations which revert some of the corruptions. At first it seems like a weakness, but the authors pointed that just using the chosen augmentations do not lead to improvement and their prediction network is necessary to pick the best augmentation for each particular instance..

Review Point: 1) Although the improvement is small compared to their chosen augmentations, their method also improves accuracy when using training time augmentations which is very attractive to reduce the inference cost of test-time augmentation.
Review Point: 2) I believe the robustness improvements in imagenet-c and cifar-10c benefit from the author's particular choice of augmentations which revert some of the corruptions. At first it seems like a weakness, but the authors pointed that just using the chosen augmentations do not lead to improvement and their prediction network is necessary to pick the best augmentation for each particular instance..
==================================================

Focused review:

- The proposed method is tailored for VLN and may limit its generalization to other domains (it is not new for other vision-and-language tasks). - Equation 2 in Section 3.2 is very confusing. If the same h_t and u are feed into the three attentions, how could different contexts be learned? It seems impossible. There seems to be something wrong, either the technique or the notations. - It seems only results of single runs are reported. However, VLN models may be sensitive to hyper-parameter tuning. It would be better if the authors can demonstrate the mean and standard deviation of multiple runs. - Error analysis is missing in the paper. In what cases the proposed model would fail? Can the authors provide some error analysis? - How would the model perform if it is allowed to pre-explore the unseen environments? It seems building a graph would also facilitate the effectiveness of pre-exploration, which is indeed an important setting in many cases. - In Table 3, it is unclear to me how Model #4 and #5 shows the importance of the connections between two graphs. - In Table 3, Model 3-6 also use the object features, but Model 4 and 6 have an efficient average path while the others don't. Why? - What is the self-supervised learning method mentioned in Line 231 as it is not introduced elsewhere? -------------------------------- after rebuttal ---------------------------------- After reading the authors' response and the other reviews, my major concerns have been addressed. The fact that this work is tailored for VLN is rather a minor comment than a major concern to me, as I believe that VLN is an interesting and important research area that deserves more attention and further study. But I think the authors could provide more insights of improving VLN or the general vision-language grounding with more in-depth discussions on the experimental results. I am inclined to accept this paper and would like to raise my score to 7. I hope the authors can include error analysis and average of multiple runs in the future version.

Review Point: - The proposed method is tailored for VLN and may limit its generalization to other domains (it is not new for other vision-and-language tasks).
Review Point: - Equation 2 in Section 3.2 is very confusing. If the same h_t and u are feed into the three attentions, how could different contexts be learned? It seems impossible. There seems to be something wrong, either the technique or the notations.
Review Point: - It seems only results of single runs are reported. However, VLN models may be sensitive to hyper-parameter tuning. It would be better if the authors can demonstrate the mean and standard deviation of multiple runs.
Review Point: - Error analysis is missing in the paper. In what cases the proposed model would fail? Can the authors provide some error analysis?
Review Point: - How would the model perform if it is allowed to pre-explore the unseen environments? It seems building a graph would also facilitate the effectiveness of pre-exploration, which is indeed an important setting in many cases.
Review Point: - In Table 3, it is unclear to me how Model #4 and #5 shows the importance of the connections between two graphs.
Review Point: - In Table 3, Model 3-6 also use the object features, but Model 4 and 6 have an efficient average path while the others don't. Why?
Review Point: - What is the self-supervised learning method mentioned in Line 231 as it is not introduced elsewhere? -------------------------------- after rebuttal ---------------------------------- After reading the authors' response and the other reviews, my major concerns have been addressed. The fact that this work is tailored for VLN is rather a minor comment than a major concern to me, as I believe that VLN is an interesting and important research area that deserves more attention and further study. But I think the authors could provide more insights of improving VLN or the general vision-language grounding with more in-depth discussions on the experimental results. I am inclined to accept this paper and would like to raise my score to 7. I hope the authors can include error analysis and average of multiple runs in the future version.
==================================================

Focused review:

Weaknesses
I am not sure if the empirical comparison is fair, as the different methods seem to have different runtime requirements; in particular, you claim that the inference runtime of FED scales as 1 + (# of epsilon samples) / 35, which for 120 samples (i.e. the number you use in the experiments) is roughly 4.5; in my understanding, this means that the inference time of FED is 4.5x slower than that of a single deterministic model (while in contrast, the vanilla ensemble is 120x slower; I requested clarification for this in the Questions below, so please let me know if my understanding is incorrect); if this assumption is correct, this would mean that FED is significantly slower than some of the baselines considered; e.g., in my understanding, EnDD should have a runtime similar to that of a single model (again, please correct me if I'm mistaken); all in all, my concern is that the performance comparison should be conducted more carefully, taking inference runtime into account, to make it more meaningful; to this end, one could e.g. plot the results in 2D with runtime on the x-axis and performance on the y-axis; different methods would then result in different trade-offs on this plot, yielding Pareto curves that would be much more insightful than just single performance numbers; at the end of the day, a practitioner is probably interested in one of two questions: 1) given a fixed runtime budget X, what’s the best performance Y that I can achieve?, or 2) given a desired performance Y, what’s the fastest runtime X to achieve it? to make the comparison even more interesting, it would be great to consider different settings for each individual method for trading off between runtime and performance; I'm not fully sure how to best do this, but e.g. for FED, one could try varying the number of epsilon samples (for a fixed number of ensemble members) to make the method cheaper; conversely, one could try to make the baselines more expensive in more way; this would allow you to compare performance of the methods for a given fixed inference budget.
The empirical evaluation could be significantly improved by considering a more diverse set of model architectures and benchmarks; the paper focuses on a single architecture (ResNet18) and three small-to-medium-sized image classification benchmarks (CIFAR-10, CIFAR-100, STL-10); it is not clear if the drawn conclusions would extend to other model families (e.g. transformers), data modalities (e.g. text) and benchmark sizes (e.g. ImageNet); to gain more space for reporting additional major empirical results, one could e.g. move Section 5.4 (on the OOB methods for inducing diversity without an auxiliary dataset) to the appendix -- while the experiment and conclusion in that section are certainly interesting, they do not seem to necessary to have in the main text of the paper due to the comparably poor results.
The paper emphasises that, in contrast to previous methods (which are e.g. based on fitting a Dirichlet distribution over predictions), FED is able to capture correlations between outputs, and claims that this is an important feature for many real-world applications (e.g. l. 9-11, l. 52-55, l.149-153); as this is mentioned already prominently in the abstract as a main selling point for the method (l. 9-11), I was expecting at least some empirical evidence for the benefit of this feature; unfortunately, no explicit evidence is provided beyond some vague intuitive written arguments (l. 52-55, l. 149-153). It would significantly strengthen the paper if capturing these covariances could be shown to improve performance in some application. (While I understand that FED is shown to improve upon previous methods, it is not clear to me how this gain can be directly attributed to the aforementioned property.)
Minor Issues
Table 3: bolding the best values would improve clarity
It would be useful to put a brief take-away message into the caption of each table/figure
The paper misses the Laplace approximation as another fundamental Bayesian inference approach for neural networks (in addition to VI and MCMC); for a recent overview, see e.g. Daxberger et al., Laplace Redux -- Effortless Bayesian Deep Learning, NeurIPS 2021
In addition to the runtime comparison requested above, it would also be useful to have a more clear/complete comparison of the memory requirements of all methods; you mention the memory burden of FED in Appendix C.2, and e.g. that of the Hydra baseline in Appendix A; it would be useful to have a table/plot comparing this for all methods considered

Review Point: 1) given a fixed runtime budget X, what’s the best performance Y that I can achieve?, or
==================================================

Focused review:

Weaknesses: 1. Secure model predictions requires customized design for particular model architectures. I am wondering how to deal with some privacy-sensitive layers, like for example, batch normalization. 2. It would be better if this paper could provide some attacks from an adversary perspective to demonstrate privacy.

Review Point: 1. Secure model predictions requires customized design for particular model architectures. I am wondering how to deal with some privacy-sensitive layers, like for example, batch normalization.
Review Point: 2. It would be better if this paper could provide some attacks from an adversary perspective to demonstrate privacy.
==================================================

Focused review:

I agree with the author's answer in 1.c) about societal impact. Generally, the paper does not introduce concerns beyond the concerns of Active Learning itself, which are not part of the paper (but prior art): acquisition functions can introduce additional bias given how they select data to be labelled and included.

Review Point: 1.c) about societal impact. Generally, the paper does not introduce concerns beyond the concerns of Active Learning itself, which are not part of the paper (but prior art): acquisition functions can introduce additional bias given how they select data to be labelled and included.
==================================================

Focused review:

Weaknesses: 1 - The main idea of using ensemble of neural networks is trivial and very common in machine learning literature. The paper doesn't provide any specific adaptation to the homomorphic encryption domain. 2 - The discussion on the homomorphic encryption schemes is completely missing. What type of HE do you use? 3 - How do you preform majority voting in the encrypted domain? Most of HE schemes do not support argmax operation. 4 - For sequential ensembling, it is important to study the effect of noise accumulation in the context of homomorphic encryption. This limitations prevents the use of even single deep neural networks on homomorphically encrypted data.

Review Point: 1 - The main idea of using ensemble of neural networks is trivial and very common in machine learning literature. The paper doesn't provide any specific adaptation to the homomorphic encryption domain.
Review Point: 2 - The discussion on the homomorphic encryption schemes is completely missing. What type of HE do you use?
Review Point: 3 - How do you preform majority voting in the encrypted domain? Most of HE schemes do not support argmax operation.
Review Point: 4 - For sequential ensembling, it is important to study the effect of noise accumulation in the context of homomorphic encryption. This limitations prevents the use of even single deep neural networks on homomorphically encrypted data.
==================================================

Focused review:

Weakness: 1/ The motivation of this paper is unreasonable. In this paper, the low-rankness of the Winograd-domain weight matrix is demonstrated in Fig. 2. Building upon this, the authors infer that the residual of the pre-trained Winograd-domain weight matrix is also low-rank. In my opinion, this inference is not rigorous. There is no direct relevance between the low-rankness of the Winograd-domain weight matrix and the pre-trained Winograd-domain weight matrix residual, it depends on the nature of the pre-trained Winograd-domain weight matrix.
2/ There are too few competing methods in the part of experiments. Readers cannot tell whether this approach achieves SOTA performance.
3/ The section of literature review is inadequate. There is no mention of methods that focus on the Winograd transform of 3D CNNs. Such methods should be described to help the reader better evaluate the proposed methods
4/ Many claims in this paper are not easy to follow. For example, the authors claim that [1-3] derive irregular sparse weight matrix, but the proposed method leads to regular sparse weight matrix. What is the basis of this claim? Relevant theoretical analysis is necessary
5/ The experiments are insufficient. In this paper, a speedup mechanism is proposed. But there are no experiments about the inference time.
6/ The purpose of Sec. 4.2.1 is to validate the effectiveness of low-rank Winograd transformation, i.e., the reduction of the trainable parameters. But there is no parameter scale analysis in this section.
7/ In Sec. 4.2.2, there is no need to show the results on model R3D-18, and R3D-34. Because such experiments cannot demonstrate the superiority of the proposed method.
[1] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. 28, 2015b. [2] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 1989. [3] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations (ICLR), 2018.

Review Point: 1/ The motivation of this paper is unreasonable. In this paper, the low-rankness of the Winograd-domain weight matrix is demonstrated in Fig.
==================================================

Focused review:

- The importance of the SGC condition remains unclear. In Line 129, the authors claimed that SGC condition is satisfied in some practical settings such as the training of deep neural networks, therefore the SGC condition should be regarded as an interesting special setting for nonconvex optimization. However, recent work [1,2] showed that the training of deep neural networks can be further regarded as a special task of convex optimization in the Neural tangent kernel (NTK) regime, which is a stronger condition than SGC. Therefore, the authors may want to clarify the importance of SGC by showing some more examples in machine learning. - The technique contribution of this work seems incremental. As the authors suggested, [VBS18] firstly studied the SGC condition under nonconvex setting and proposed that SGD costs O(1/\epsilon^2) gradient complexity to find first-order stationary points. Meanwhile, note that [AZL18] proposed a generic framework which could turn any algorithms for finding first-order stationary points into algorithms for finding approximate local minimizer, without hurting the convergence rate. Therefore, is it true that the convergence rate O(1/\epsilon^2) for SGD to find approximate local minimizer established in this paper can be directly deduced by combining existing results from [VBS18] and [AZL18]? The authors may want to discuss above issues to highlight their technique contribution in this paper. - The authors may want to add some discussion about how the SGC condition will affect the variance-reduction based algorithms such as [3,4,5,6]. [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. "A convergence theory for deep learning via over-parameterization." International Conference on Machine Learning. 2019. [2] Zou, Difan, et al. "Gradient descent optimizes over-parameterized deep ReLU networks." Machine Learning 109.3 (2020): 467-492. [3] Johnson, Rie, and Tong Zhang. "Accelerating stochastic gradient descent using predictive variance reduction." Advances in neural information processing systems. 2013. [4] Zhou, Dongruo, Pan Xu, and Quanquan Gu. "Stochastic nested variance reduction for nonconvex optimization." Advances in Neural Information Processing Systems. 2018. [5] Wang, Zhe, et al. "SpiderBoost and momentum: Faster variance reduction algorithms." Advances in Neural Information Processing Systems. 2019. [6] Nguyen, Lam M., et al. "Finite-sum smooth optimization with sarah." arXiv preprint arXiv:1901.07648 (2019).

Review Point: - The importance of the SGC condition remains unclear. In Line 129, the authors claimed that SGC condition is satisfied in some practical settings such as the training of deep neural networks, therefore the SGC condition should be regarded as an interesting special setting for nonconvex optimization. However, recent work [1,2] showed that the training of deep neural networks can be further regarded as a special task of convex optimization in the Neural tangent kernel (NTK) regime, which is a stronger condition than SGC. Therefore, the authors may want to clarify the importance of SGC by showing some more examples in machine learning.
Review Point: - The technique contribution of this work seems incremental. As the authors suggested, [VBS18] firstly studied the SGC condition under nonconvex setting and proposed that SGD costs O(1/\epsilon^2) gradient complexity to find first-order stationary points. Meanwhile, note that [AZL18] proposed a generic framework which could turn any algorithms for finding first-order stationary points into algorithms for finding approximate local minimizer, without hurting the convergence rate. Therefore, is it true that the convergence rate O(1/\epsilon^2) for SGD to find approximate local minimizer established in this paper can be directly deduced by combining existing results from [VBS18] and [AZL18]? The authors may want to discuss above issues to highlight their technique contribution in this paper.
Review Point: - The authors may want to add some discussion about how the SGC condition will affect the variance-reduction based algorithms such as [3,4,5,6]. [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. "A convergence theory for deep learning via over-parameterization." International Conference on Machine Learning. 2019. [2] Zou, Difan, et al. "Gradient descent optimizes over-parameterized deep ReLU networks." Machine Learning 109.3 (2020): 467-492. [3] Johnson, Rie, and Tong Zhang. "Accelerating stochastic gradient descent using predictive variance reduction." Advances in neural information processing systems. 2013. [4] Zhou, Dongruo, Pan Xu, and Quanquan Gu. "Stochastic nested variance reduction for nonconvex optimization." Advances in Neural Information Processing Systems. 2018. [5] Wang, Zhe, et al. "SpiderBoost and momentum: Faster variance reduction algorithms." Advances in Neural Information Processing Systems. 2019. [6] Nguyen, Lam M., et al. "Finite-sum smooth optimization with sarah." arXiv preprint arXiv:1901.07648 (2019).
==================================================

Focused review:

Weakness: 1. The authors claim that “We believe that such a mechanism (Grad-CAM or CAM) could guide f to differentiate the true label from the candidate set because it is constructed by taking advantage of internal elements in f” and conducted a pilot experiment to check whether CAM with the most foreground seeds belongs to the true label. However, the pilot experiment cannot empirically validate this claim, as the experiment is only conducted on the CIFAR-10 with artificial candidate labels generated by two simple strategies but not conducted on real-world PLL datasets or adopts some realistic candidate labels generation strategies such as class-dependent strategy. 2. The proposed method CAVL adopts the strategy that selects the “true” label from the candidate label set and only considers the loss on the “true” label after the first training epoch. However, the selected “true” label may be false-positive and the authors also claim that the potential true label would be updated. In this case, the training model could be affected by false-positive labels since CAVL only considers the “true” label. [1] proves that models trained by this strategy cannot achieve good performance. 3. There is no theoretical result provided to validate the effectiveness of CAV for PLL. 4. The authors adopt the PLL experimental settings in [1], but they do not compare the method in [1]. 5. The authors claim that the hyper-parameters are selected to exploit the best performance on the validation set containing 10% of the training examples. They should show the train/ validation/ test split. RC, CC and CAVL are suggested to have the same learning rate, weight decay and mini-batch size as they adopt the same model for fair comparisons.
Suggestion: The authors are suggested to provide source code as supplementary material so that the reviewers could check the reproducibility of the experimental results.
[1] Progressive Identification of True Labels for Partial-Label Learning, ICML 2020.

Review Point: 1. The authors claim that “We believe that such a mechanism (Grad-CAM or CAM) could guide f to differentiate the true label from the candidate set because it is constructed by taking advantage of internal elements in f” and conducted a pilot experiment to check whether CAM with the most foreground seeds belongs to the true label. However, the pilot experiment cannot empirically validate this claim, as the experiment is only conducted on the CIFAR-10 with artificial candidate labels generated by two simple strategies but not conducted on real-world PLL datasets or adopts some realistic candidate labels generation strategies such as class-dependent strategy.
Review Point: 2. The proposed method CAVL adopts the strategy that selects the “true” label from the candidate label set and only considers the loss on the “true” label after the first training epoch. However, the selected “true” label may be false-positive and the authors also claim that the potential true label would be updated. In this case, the training model could be affected by false-positive labels since CAVL only considers the “true” label. [1] proves that models trained by this strategy cannot achieve good performance.
Review Point: 3. There is no theoretical result provided to validate the effectiveness of CAV for PLL.
Review Point: 4. The authors adopt the PLL experimental settings in [1], but they do not compare the method in [1].
Review Point: 5. The authors claim that the hyper-parameters are selected to exploit the best performance on the validation set containing 10% of the training examples. They should show the train/ validation/ test split. RC, CC and CAVL are suggested to have the same learning rate, weight decay and mini-batch size as they adopt the same model for fair comparisons. Suggestion: The authors are suggested to provide source code as supplementary material so that the reviewers could check the reproducibility of the experimental results. [1] Progressive Identification of True Labels for Partial-Label Learning, ICML 2020.
==================================================

Focused review:

1. There is a log factor gap in the bounds. 2. No experimental experiments are given in this paper, though I think it's fine for a theory paper.

Review Point: 1. There is a log factor gap in the bounds.
Review Point: 2. No experimental experiments are given in this paper, though I think it's fine for a theory paper.
==================================================

Focused review:

Weaknesses While the evaluation conducted by the authors is thorough, my main critique of this paper is that the results are not convincing enough to show the value of the proposed model.
The quantitative results do not outperform the state-of-the-art models consistently across all metric. For instance, the mdoel does not outperform PointFlow and PQ-Net across all metrics in Table 1. Same is the case for ObjGAN (IS=7.5) vs proposed (IS=7.1) in Figure 7. The authors fail to comment on why is that the case.
The qualitative samples are also not realistic in some cases. Some of the COCO results in Figure 4 and 5 do not look realistic -- layouts are too cluttered leading to incomprehensible scene. (row 2,col 3) in Figure 4 and (row 3,col 4) in Figure 5). From the layout samples presented in the paper, it seems the model produces cluttered layouts when the model is trying to generate higher number of objects. The authors do not discuss this aspect in the paper. Is the quality of the layout dependent on the number of entities in the scenes? If yes, I think it is important for a scene layout generation model should be robust enough to handle flexible lengths.
Overall, while the paper has interesting approach to model the relations between the elements of a scene, the results are not convincing enough to demonstrate the effectiveness of the proposed model. Therefore, my initial rating is 5.
=========================================Post-rebuttal comments=======================================
I appreciate the revisions and additional visualization provided the authors. The revised version of the paper provides clarifications that make the paper easier to follow. While the paper presents an interesting idea, I am not convinced of the effectiveness of the proposed method based on the results provided by the authors. Therefore, my final rating as 5 (same as the initial rating).

Review Point: 5). From the layout samples presented in the paper, it seems the model produces cluttered layouts when the model is trying to generate higher number of objects. The authors do not discuss this aspect in the paper. Is the quality of the layout dependent on the number of entities in the scenes? If yes, I think it is important for a scene layout generation model should be robust enough to handle flexible lengths. Overall, while the paper has interesting approach to model the relations between the elements of a scene, the results are not convincing enough to demonstrate the effectiveness of the proposed model. Therefore, my initial rating is 5. =========================================Post-rebuttal comments======================================= I appreciate the revisions and additional visualization provided the authors. The revised version of the paper provides clarifications that make the paper easier to follow. While the paper presents an interesting idea, I am not convinced of the effectiveness of the proposed method based on the results provided by the authors. Therefore, my final rating as 5 (same as the initial rating).
==================================================

Focused review:

1. There is no evidence provided to demonstrate the problem and some claims (such as L91-L93). 2. There is no training dynamics to identify why the proposed two techniques boost performance. 3. The proposed method is simple, like a combination of tricks. 4. Some parts of this paper are not clear, which makes reading difficult. I am not clear how do you plot figure 2 using D_{g1,g2}.

Review Point: 1. There is no evidence provided to demonstrate the problem and some claims (such as L91-L93).
Review Point: 2. There is no training dynamics to identify why the proposed two techniques boost performance.
Review Point: 3. The proposed method is simple, like a combination of tricks.
Review Point: 4. Some parts of this paper are not clear, which makes reading difficult. I am not clear how do you plot figure 2 using D_{g1,g2}.
==================================================

Focused review:

Weaknesses
Despite the claims on its effectiveness on complex and naturalistic videos, the datasets seem to be very limiting. 6 of them are procedurally generated datasets, CATER (Girdhar & Ramanan, 2020), CATERTex, MOVi-Solid, MOVi-Tex, MOVi-D, and MOVi-E (Greff et al., 2022). 2 natural datasets are only collected for this paper: Traffic and Aquarium. The paper does not provide details on the complexity and diversity of these datasets.
The paper seems to lack novelty as the design seems to be a straightforward combination of (1) a CNN-based image encoder, (2) a recurrent slot encoder that updates slots temporally with recurrent neural networks (RNNs), and (3) the slot-transformer decoder of SLATE.
The paper does not address its limitations. A section clearly articulating the technical limitations is important.

Review Point: 6 of them are procedurally generated datasets, CATER (Girdhar & Ramanan, 2020), CATERTex, MOVi-Solid, MOVi-Tex, MOVi-D, and MOVi-E (Greff et al., 2022).
Review Point: 2 natural datasets are only collected for this paper: Traffic and Aquarium. The paper does not provide details on the complexity and diversity of these datasets. The paper seems to lack novelty as the design seems to be a straightforward combination of (1) a CNN-based image encoder, (2) a recurrent slot encoder that updates slots temporally with recurrent neural networks (RNNs), and (3) the slot-transformer decoder of SLATE. The paper does not address its limitations. A section clearly articulating the technical limitations is important.
==================================================

Focused review:

Weaknesses:
Duplicate task settings. The proposed new task, cross-supervised object detection, is almost the same as the task defined in (Hoffman et al. 2014, Tang et al. 2016, Uijlings et al. 2018). Both of these previous works study the task of training object detectors on the combination of base class images with instance-level annotations and novel class image with only image-level annotations. The work (Uijlings et al. 2018) also conducts experiments on COCO which contains multi-objects in images. In addition, the work (Khandelwal et al. 2020) unifies the setting of training object detectors on the combination of fully-labeled data and weakly-labeled data, and conducts experiments on multi-object datasets PASCAL VOC and COCO. The task proposed by this paper could be treated as a special case of the task studied in (Khandelwal et al. 2020). We should avoid duplicate task settings.
Limited novelty. The novelty of the proposed method is limited. Combining recognition head and detection head is not new in weakly supervised object detection. The weakly supervised object detection networks (Yang et al. 2019, Zeng et al. 2019) also generate pseudo instance-level annotations from recognition head to train detection head (i.e., head with bounding box classification and regression) for weakly-labeled data.
Review summary: In summary, I would like to give a rejection to this paper due to the duplicate task settings and limited novelty.
Khandelwal et al., Weakly-supervised Any-shot Object Detection, 2020
---------- Post rebuttal ----------
After discussions with authors and reading other reviews, I acknowledge the contribution that this paper advances the performance of cross-supervised object detection.
However, I would like to keep my original reject score. The reasons are as follows.
Extending datasets from PASCAL VOC to COCO is not a significant change comparing to previous tasks. The general object detection papers also evaluated on PASCAL VOC only about five years ago and now evaluate mainly on COCO. With the development of computer vision techniques, it is natural to try more challenging datasets. So although this paper claims that this paper focuses on more challenging datasets, there is no significant difference between the tasks studied in previous works like [a] and this paper.
In addition, apart from ImageNet, the work [b] also evaluates their method on the Open Images dataset which is even larger and more challenging than COCO. The difference between the tasks studied in [b] and this paper is only that, [b] adds a constraint that weakly-labeled classes have semantic correlations with fully-labeled classes and this paper doesn't. This difference is also minor.
Therefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper). I would like to suggest the authors change their title / introduction / main paper by 1) giving lower wights to the task parts 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method.
[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017
[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019

Review Point: 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method. [a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017 [b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019
==================================================

Focused review:

- The use of the appendix is a little more integral than I think would be ideal. You have to read the appendix in order to know the conclusions of some analysis; the main text only says that such-and-such "matters," but now how.
- It's not always clear which results are meaningful. Some rows of the main table have statistical significance tests, but other metrics, analysis, and data points do not. Since the score differences are overall quite small, this detracts from the reliability of the results. Readers might be inclined to decide that the techniques aren't worth trying.
SUGGESTIONS - It's great to have at least some statistical significance tests. Since they were added only after the first revision of the paper, I felt like they weren't fully integrated into the main text or all of the results tables (like Table 5). For example, the beginning of Section 5.2 (line 201) says that LS "outperforms" VS+LS... but we do not have any significance tests between these two configurations. These differences of 0.1 BLEU may actually be ties, in which case "outperforms" is incorrect.
- In Table 3(b), is it really the case that 23.19 BLEU for RO-EN is statistically better than 23.15, or is this a typo? I've never seen significance at p < 0.01 for such a tiny score change.
- Even without a formal significance test, can we have some intuition about this ECE score in Section 6.1 / Table 4? How is this score computed and what properties of MT training does it reflect? I've never heard of this metric before, so I don't really understand what it is or whether an improvement of 0.7 is meaningful. (Whereas I know quite well how much intuitive importance to give 0.7 BLEU.)
- In Table 5, I would think that one intuitive configuration is to devalue the source vocabulary without shutting it off completely, via parameter settings like 0.5-0.5-0.25. If your original motivation is correct, I would expect a configuration like that to do better than your current best result, 0.5-0-0.5. Doesn't the current result indicate that the source-side vocabulary is somehow quite helpful -- the opposite of your initial claim?
OTHER TYPOS OR CONFUSION - Why don't the numbers at the top of Table 5 (for baseline label smoothing) match the scores given in Table 3 for LS? Aren't these identical configurations?
- How do you decide when model training is over? Is it a fixed number of epochs or updates, or after perplexity on the dev set converges (for some definition), etc. Appendix B doesn't say. Since your experiments modify the kind of signal that the model receives during training, I was wondering if that has a confounding effect on the results. (For example, if MLS makes the training last 25 percent more updates than the baseline, then is the increased performance partially due to the mere fact of training longer?)
- The WLS parameters "1-1-0" in Section 4.2 (line 175) directly contradict the claim in Section 4.1 (line 150) that these three values must sum to 1.

Review Point: - The use of the appendix is a little more integral than I think would be ideal. You have to read the appendix in order to know the conclusions of some analysis; the main text only says that such-and-such "matters," but now how.
Review Point: - It's not always clear which results are meaningful. Some rows of the main table have statistical significance tests, but other metrics, analysis, and data points do not. Since the score differences are overall quite small, this detracts from the reliability of the results. Readers might be inclined to decide that the techniques aren't worth trying. SUGGESTIONS - It's great to have at least some statistical significance tests. Since they were added only after the first revision of the paper, I felt like they weren't fully integrated into the main text or all of the results tables (like Table 5). For example, the beginning of Section 5.2 (line 201) says that LS "outperforms" VS+LS... but we do not have any significance tests between these two configurations. These differences of 0.1 BLEU may actually be ties, in which case "outperforms" is incorrect.
Review Point: - How do you decide when model training is over? Is it a fixed number of epochs or updates, or after perplexity on the dev set converges (for some definition), etc. Appendix B doesn't say. Since your experiments modify the kind of signal that the model receives during training, I was wondering if that has a confounding effect on the results. (For example, if MLS makes the training last 25 percent more updates than the baseline, then is the increased performance partially due to the mere fact of training longer?) - The WLS parameters "1-1-0" in Section 4.2 (line 175) directly contradict the claim in Section 4.1 (line 150) that these three values must sum to 1.
==================================================

Focused review:

- The experimental setup is rather limited with respect to the variety of GNN approaches tested. The paper only looks at the GCN model. Given that the effectiveness of the proposed method mainly relies on the experimental results, I'd recommend to provide a more extensive comparison. - In Figure 1 the results for PageRank are missing - The parameter k determining the neighbour is a key parameter to the selection strategy. Possibly, the authors could investigate the effect of varying it in more detail.

Review Point: - The experimental setup is rather limited with respect to the variety of GNN approaches tested. The paper only looks at the GCN model. Given that the effectiveness of the proposed method mainly relies on the experimental results, I'd recommend to provide a more extensive comparison.
Review Point: - In Figure 1 the results for PageRank are missing - The parameter k determining the neighbour is a key parameter to the selection strategy. Possibly, the authors could investigate the effect of varying it in more detail.
==================================================

Focused review:

The main issue I have is the assumptions made in the paper. I will divide this into two items: grounding and validation. By grounding I mean, how realistic are the assumptions made, and by validation I mean, how can someone test for these assumptions given real data? While I understand that some of the assumptions are model assumptions, I think it would be nice to discuss methods that such model assumptions could be validated. If we cannot validate our models then we have no way of knowing how applicable our assumptions are. Specifically regarding each assumption made in the paper: 1. Assumption A (overlap): this assumption is fine and reasonable in a tabular setting. How would you generalize it to the function approximation setting? Since you do mention this in the section about consistency, I think it would be beneficial to discuss this. 2. Assumption B: This assumption seems reasonable, since mostly we have control over the evaluation policy. 3. Assumption C (sequential ignorability given confounder): This assumption is general and is thus not really an assumption. 4. Assumption D (confounding bound): This is a standard assumption in causal inference. It would be interesting though if you could discuss its relation to the assumption of Zhang and Bareinboim [53] for sequential boundness. 5. Assumption E (single confounder): This is an assumption I am having a hard time with. First, I cannot see a realistic scenario in which this assumption would hold. I can believe that the bound on confoundess may diminish with time, or even be very small, but it is unreasonable to believe that the confounding bias would be zero for all other time steps. Second, with regard to validation, how can one validate this assumption? How can one find t* for which it might hold approximately? What happens when this assumption breaks? 6. Assumption F: (confounding bound): This is again a standard assumption.

Review Point: 1. Assumption A (overlap): this assumption is fine and reasonable in a tabular setting. How would you generalize it to the function approximation setting? Since you do mention this in the section about consistency, I think it would be beneficial to discuss this.
Review Point: 2. Assumption B: This assumption seems reasonable, since mostly we have control over the evaluation policy.
Review Point: 3. Assumption C (sequential ignorability given confounder): This assumption is general and is thus not really an assumption.
Review Point: 4. Assumption D (confounding bound): This is a standard assumption in causal inference. It would be interesting though if you could discuss its relation to the assumption of Zhang and Bareinboim [53] for sequential boundness.
Review Point: 5. Assumption E (single confounder): This is an assumption I am having a hard time with. First, I cannot see a realistic scenario in which this assumption would hold. I can believe that the bound on confoundess may diminish with time, or even be very small, but it is unreasonable to believe that the confounding bias would be zero for all other time steps. Second, with regard to validation, how can one validate this assumption? How can one find t* for which it might hold approximately? What happens when this assumption breaks?
Review Point: 6. Assumption F: (confounding bound): This is again a standard assumption.
==================================================

Focused review:

1. Network design: In the interaction representation learning, each verb/interaction is represented by a MLP subnet T_I^{v_i} and T_D^{v_i}. Wouldn't it result in too many parameters to learn if there are a large number of verb/interaction to represent? 2. Hyperparams in v_i classification: What are the thresholds for determining if the feature distances are small or large? Do different verb classes have different thresholds? How sensitive is the final performance to these thresholds? It would be great to see such experiments and analysis. 3. Losses: My understanding of the losses in integration and decomposition validity (L_{cls}^u and L_{cls}^{ho}) is that these losses encourage the integrated and decomposed feature vectors of the ground truth verb class to be similar to f_u or f_h + f_o. How about the feature vectors of other verb classes? Shouldn't they be pushed away from f_u and f_h + f_o? The "pull and push" losses are commonly used in representation learning, while I only see "pull" here in the method. Without the "push" term, one trivial optimization outcome is that all f_u^{v_i} become mutually similar and close to f_u (so as all the f_h^{v_i} + f_u^{v_i}). How do you avoid such a trivial outcome? Is there a reason for not considering the "push" term?

Review Point: 1. Network design: In the interaction representation learning, each verb/interaction is represented by a MLP subnet T_I^{v_i} and T_D^{v_i}. Wouldn't it result in too many parameters to learn if there are a large number of verb/interaction to represent?
Review Point: 2. Hyperparams in v_i classification: What are the thresholds for determining if the feature distances are small or large? Do different verb classes have different thresholds? How sensitive is the final performance to these thresholds? It would be great to see such experiments and analysis.
Review Point: 3. Losses: My understanding of the losses in integration and decomposition validity (L_{cls}^u and L_{cls}^{ho}) is that these losses encourage the integrated and decomposed feature vectors of the ground truth verb class to be similar to f_u or f_h + f_o. How about the feature vectors of other verb classes? Shouldn't they be pushed away from f_u and f_h + f_o? The "pull and push" losses are commonly used in representation learning, while I only see "pull" here in the method. Without the "push" term, one trivial optimization outcome is that all f_u^{v_i} become mutually similar and close to f_u (so as all the f_h^{v_i} + f_u^{v_i}). How do you avoid such a trivial outcome? Is there a reason for not considering the "push" term?
==================================================

Focused review:

Weaknesses: ++++++++++
Novelty/Significance: The reformulation of the robust regression problem (Eq 6 in the paper) shows that robust regression is reducible to standard k-sparse recovery. Therefore, the proposed CRR algorithm is basically the well-known IHT algorithm (with a modified design matrix), and IHT has been (re)introduced far too many times in the literature to count.
The proofs in the appendix seem to be correct, but also mostly follow existing approaches for analyzing IHT (see my comment below). Note that the âsubset strong convexityâ property (or at least a variation of this property) of random Gaussian matrices seems to have appeared before in the sparse recovery literature; see âA Simple Proof that Random Matrices are Democraticâ (2009) by Davenport et al.
Couple of questions:
- What is \delta in the statement of Lemma 5?
- Not entirely clear to me why one would need a 2-stage analysis procedure since the algorithm does not change. Some intuition in the main paper explaining this would be good (and if this two-stage analysis is indeed necessary, then it would add to the novelty of the paper). +++++++++
Update after authors' response +++++++++
Thanks for clarifying some of my questions. I took a closer look at the appendix, and indeed the "fine convergence" analysis of their method is interesting (and quite different from other similar analyses of IHT-style methods). Therefore, I have raised my score.

Review Point: ++++++++++ Novelty/Significance: The reformulation of the robust regression problem (Eq 6 in the paper) shows that robust regression is reducible to standard k-sparse recovery. Therefore, the proposed CRR algorithm is basically the well-known IHT algorithm (with a modified design matrix), and IHT has been (re)introduced far too many times in the literature to count. The proofs in the appendix seem to be correct, but also mostly follow existing approaches for analyzing IHT (see my comment below). Note that the âsubset strong convexityâ property (or at least a variation of this property) of random Gaussian matrices seems to have appeared before in the sparse recovery literature; see âA Simple Proof that Random Matrices are Democraticâ (2009) by Davenport et al. Couple of questions:
Review Point: - What is \delta in the statement of Lemma 5?
Review Point: - Not entirely clear to me why one would need a 2-stage analysis procedure since the algorithm does not change. Some intuition in the main paper explaining this would be good (and if this two-stage analysis is indeed necessary, then it would add to the novelty of the paper). +++++++++ Update after authors' response +++++++++ Thanks for clarifying some of my questions. I took a closer look at the appendix, and indeed the "fine convergence" analysis of their method is interesting (and quite different from other similar analyses of IHT-style methods). Therefore, I have raised my score.
==================================================

Focused review:

1. The overall novelty of the proposed model is limited to some extend. 1) The structural module is generally a traditional graph embedding approaches; 2) the meta-learning module utilizes the prototypical model to calculate the centers of the positive and negative nodes respectively, and the transformation generally leverages an self-attention mechanism to achieve the adapted node embeddings for different labels. I think this module is very similar to meta-GNN. Both of them conduct adaptation on the support set, then do evaluation on the query set, though they employ prototype and MAML respectively. 3) the optimization module applies a simple iteration strategy to optimize the two loss functions. In my view, the overall model stands on the shoulder on some traditional approaches, and seems a bit incremental. 2. For the motivation, why to use meta-learning? Could some other approaches, such as fine-tune (which is often utilized as the comparison with meta-learning), solve this novel label problem? The authors should give more explanations, and verify them by experiments. 3. Some concerns about the first contribution "To the best of our knowledge, this is the first work that only uses the graph structure and some known labels to study the problem of NCFNL". I think this contribution is over claimed. Actually, Meta-GNN [33] also utilizes the graph structure (GNN) and some known labels (the labeled nodes) to study the problem of NCFNL (to predict the node labels for novel classes). We cannot regard it as overlooking the graph structure just because it utilizes the GNN models but not graph embedding models. 4. I agree with the usage of transformation, which actually does adaptation to transform the task agnostic embeddings to some task-specific ones for each task. My concern is that, why to use self-attention as the transformation, and what is the insight? The authors should give more explanations. 5. The baselines are not quite sufficient. 1) For GNN models, the authors could apply the graph embeddings learned by some traditional approaches (e.g., deepwalk) as the node features, which would be better than identity matrix, since the graph embeddings could preserve the graph structure. 2) Some more baselines should be taken into consideration, such as the fine-tuning approaches. For example, we can first train a GCN model on the training data for label prediction, then in test we can fine tune the GNN parameters on the novel labels for label prediction. I think fine-tuning approaches are important baselines for meta-learning models, and this could verify the performance comparison between fine-tuning methods and meta-learning based approaches.

Review Point: 1. The overall novelty of the proposed model is limited to some extend.
Review Point: 1) The structural module is generally a traditional graph embedding approaches;
Review Point: 2) the meta-learning module utilizes the prototypical model to calculate the centers of the positive and negative nodes respectively, and the transformation generally leverages an self-attention mechanism to achieve the adapted node embeddings for different labels. I think this module is very similar to meta-GNN. Both of them conduct adaptation on the support set, then do evaluation on the query set, though they employ prototype and MAML respectively.
Review Point: 3) the optimization module applies a simple iteration strategy to optimize the two loss functions. In my view, the overall model stands on the shoulder on some traditional approaches, and seems a bit incremental.
Review Point: 2. For the motivation, why to use meta-learning? Could some other approaches, such as fine-tune (which is often utilized as the comparison with meta-learning), solve this novel label problem? The authors should give more explanations, and verify them by experiments.
Review Point: 3. Some concerns about the first contribution "To the best of our knowledge, this is the first work that only uses the graph structure and some known labels to study the problem of NCFNL". I think this contribution is over claimed. Actually, Meta-GNN [33] also utilizes the graph structure (GNN) and some known labels (the labeled nodes) to study the problem of NCFNL (to predict the node labels for novel classes). We cannot regard it as overlooking the graph structure just because it utilizes the GNN models but not graph embedding models.
Review Point: 4. I agree with the usage of transformation, which actually does adaptation to transform the task agnostic embeddings to some task-specific ones for each task. My concern is that, why to use self-attention as the transformation, and what is the insight? The authors should give more explanations.
Review Point: 1) For GNN models, the authors could apply the graph embeddings learned by some traditional approaches (e.g., deepwalk) as the node features, which would be better than identity matrix, since the graph embeddings could preserve the graph structure.
Review Point: 2) Some more baselines should be taken into consideration, such as the fine-tuning approaches. For example, we can first train a GCN model on the training data for label prediction, then in test we can fine tune the GNN parameters on the novel labels for label prediction. I think fine-tuning approaches are important baselines for meta-learning models, and this could verify the performance comparison between fine-tuning methods and meta-learning based approaches.
==================================================

Focused review:

Weaknesses: I was a bit puzzled by the fact that using larger contexts, beyond the sentences with blanks in them, did not help the models. After all, you were in a way using additional context in the HierEnc model, which accumulates knowledge from other contexts. There are two possible explanations: Either the sentences with blanks in them are across the board more informative for the task than the sentences without. This is the explanation suggested in the paper, but it seems a bit unintuitive that this should be the case. Another possible explanation is that the way that you were using additional context in HierEnc, using the temporal network, is much more useful than by enlarging individual contexts C and feeding that larger C into the recurrent network. Do you think that that could be what is going on?
- General Discussion: I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution.

Review Point: -General Discussion: I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution.
==================================================

Focused review:

3. Weaknesses 1) There are a lot of notations in this paper. I recommend the authors to give a table or index in appendix for all the notations, which would make this paper more readable. 2) The empirical finding about the unique solution path is very interesting. However, more details about the experimental configuration should be provided. For example, whether the training samples/mini batches are shuffled or not in each training process. And can we have the same conclusion when the training samples are shuffled and not shuffled? 3) Some typos, e.g., ‘reformualted’ in line 396->’reformulated’. ----------------------------------------------- Reviewer has read the author response. I would like to stick to the original score.

Review Point: 3. Weaknesses1) There are a lot of notations in this paper. I recommend the authors to give a table or index in appendix for all the notations, which would make this paper more readable.
Review Point: 2) The empirical finding about the unique solution path is very interesting. However, more details about the experimental configuration should be provided. For example, whether the training samples/mini batches are shuffled or not in each training process. And can we have the same conclusion when the training samples are shuffled and not shuffled?
==================================================

Focused review:

Weaknesses 1. The theoretical analysis assumes T i , j > 0
. Although the paper claims that the assumption can be satisfied by substituting the zero entries with small numbers, doesn’t that change $ d_\phi – d_b 1 ? D o e s t h a t b o u n d s t i l l h o l d ( h o w m u c h e r r o r w o u l d b e i n t r o d u c e d i f w e s u b s t i t u t e a l l z e r o e n t r i e s ) ? 2. I d o n ′ t u n d e r s t a n d t h e p u r p o s e o f t h e t h e o r e t i c a l a n a l y s i s . T h e p a p e r m e n t i o n s t h a t t h e g o a l i s t o s h o w t h a t “ t h i s l i n e o f p r i o r w o r k i s c o m p a t i b l e i n t h e o r y w i t h o u r p r o p o s e d f r a m e w o r k ” , b u t i t s e e m s l i k e t h e t h e o r y j u s t s a y s t h a t r e g u l a r i z i n g p o l i c y i m p l i e s r e g u l a r i z i n g t h e s t a t e − a c t i o n v i s i t a t i o n . I s t h a t w h a t t h e p a p e r t r y i n g t o s h o w o r d o I m i s s a n y t h i n g h e r e ? M o r e o v e r , I t h i n k s i m i l a r a n a l y s i s h a s b e e n d o n e i n t h e e x i s t i n g w o r k s ( e . g . , A p p e n d i x A a n d B i n [ 1 ] o r S e c t i o n 4.2 o f [ 2 ] ) . C a n y o u e x p l a i n h o w t h e a n a l y s i s i n t h e p a p e r d i f f e r s f r o m t h e e x i s t i n g w o r k s ? W h a t i s n o v e l t y i n t h e t h e o r e t i c a l a n a l y s i s i n t h i s p a p e r ? 3. I n t h e e m p i r i c a l s e c t i o n , t h e p a p e r m e n t i o n s t h a t “ w e d o n o t t e s t o n t h e r a n d o m a n d e x p e r t d a t a s e t s a s t h e y a r e l e s s p r a c t i c a l ” . H o w e v e r , I t h i n k t h e p o i n t o f t h e e m p i r i c a l s t u d y i s t o u n d e r s t a n d h o w t h e p r o p o s e d a l g o r i t h m p e r f o r m s c o m p a r e d t o b a s e l i n e s , n o t h o w p r a c t i c a l t h e a l g o r i t h m o r t h e d a t a s e t i s . T h e r e f o r e , I t h i n k t h e e m p i r i c a l r e s u l t s w o u l d b e c l e a r e r w i t h a l l d a t a s e t r e p o r t e d . 4. T h e r e s u l t s i n T a b l e 1 d o n o t s h o w c o n f i d e n c e i n t e r v a l f o r b a s e l i n e s a n d t h e a v e r a g e s c o r e s . I f i n d i t h a r d t o s e e a c l e a r c o n c l u s i o n f r o m t h e t a b l e . F o r e x a m p l e , f o r m a n y t a s k s , i t i s n o t c l e a r w h e t h e r t h e p r o p o s e d a l g o r i t h m o u t p e r f o r m s b a s e l i n e s s i g n i f i c a n t l y o r n o t . M o r e o v e r , s h o w i n g t h e p r o p o s e d a l g o r i t h m s h a v e h i g h e r a v e r a g e s c o r e s s e e m s p o i n t l e s s s i n c e t h e s c o r e s a r e n o t n o r m a l i z e d ( p l e a s e c o r r e c t m e i f I a m w r o n g ) . I t i s p o s s i b l e t h a t o n e a l g o r i t h m w i n s o n o n e t a s k w i t h a b i g m a r g i n a n d l o s e s o n o t h e r t a s k s w i t h a s m a l l m a r g i n t o h a v e a h i g h e r a v e r a g e s c o r e . I s t h i s a l g o r i t h m b e t t e r ? 5. O n e o f t h e m a i n p o i n t s o f t h e p a p e r i s a b o u t r e g u l a r i z i n g t h e s t a t e − a c t i o n v i s i t a t i o n d i s t r i b u t i o n . H o w e v e r , i n S e c t i o n 4.1 , t h e p a p e r u s e s t h e s t a t e d i s t r i b u t i o n f r o m t h e d a t a s e t t o a p p r o x i m a t e
d{\phi}(s)$. Doesn’t the algorithm basically reduce to policy regularizing algorithm by doing this approximation? Approximating the state-action visitation of a target policy is still an ongoing topic in the OPE community, so I don’t think it is okay to just say that we can use the dataset to approximate the state distribution. This is my biggest concern of the paper.
[1] John Schulman, et al. “Trust region policy optimization.”
[2] Sergey Levine, et al. “Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems”

Review Point: 1. The theoretical analysis assumes T i , j > 0 . Although the paper claims that the assumption can be satisfied by substituting the zero entries with small numbers, doesn’t that change $ d_\phi – d_b 1 ? D o e s t h a t b o u n d s t i l l h o l d ( h o w m u c h e r r o r w o u l d b e i n t r o d u c e d i f w e s u b s t i t u t e a l l z e r o e n t r i e s ) ?
==================================================

Focused review:

The main weaknesses of the paper are the following: - it is unclear in which case this geometry can be useful "in real applications". In the experiments, authors state that it better embeds graph that contain cycles: a deeper analysis of this statement should be provided, together with all other relevant cases. If I have a problem with some tree- or graph-like data, in which case should I consider the ultrahyperbolic geometry rather than the hyperbolic one? - regarding the experimental evaluation, several questions are also raised. Authors propose to use the capacity of the nodes to learn the representations, as it is difficult to get ancestors in the presence of cycles. How does the ultrahyperbolic geometry (with q>1) behaves in "classical" scenario in which the hyperbolic assumption makes sense? How choosing the appropriate q value ? To what phenomenom it relates to?

Review Point: - it is unclear in which case this geometry can be useful "in real applications". In the experiments, authors state that it better embeds graph that contain cycles: a deeper analysis of this statement should be provided, together with all other relevant cases. If I have a problem with some tree- or graph-like data, in which case should I consider the ultrahyperbolic geometry rather than the hyperbolic one?
Review Point: - regarding the experimental evaluation, several questions are also raised. Authors propose to use the capacity of the nodes to learn the representations, as it is difficult to get ancestors in the presence of cycles. How does the ultrahyperbolic geometry (with q>1) behaves in "classical" scenario in which the hyperbolic assumption makes sense? How choosing the appropriate q value ? To what phenomenom it relates to?
==================================================

Focused review:

Some concerns: 1. One big concern is, the proposed method is combined with existing trigger-set-based IP protection methods to support the black-box verification, however, if the existing trigger-set-based method is strong enough to verify the suspect model do we need an extra step (i.e., the proposed method) to confirm? Or is this two-step verification necessary? I think the authors should provide some discussions or data to claim it. 2. The training process of the passport-free branch and the passport-aware branch is in an alternative way. What's the training cost? Why the authors use this training strategy, rather than train them simultaneously? More details are appreciated. 3. The details of the trigger-set-based method are missing. What existing method did the author use? How many special sets of data are used to identify a suspect model? Will this impact the DNN performance? Could the authors provide some detailed evaluation results of this part?

Review Point: 2. The training process of the passport-free branch and the passport-aware branch is in an alternative way. What's the training cost? Why the authors use this training strategy, rather than train them simultaneously? More details are appreciated.
Review Point: 3. The details of the trigger-set-based method are missing. What existing method did the author use? How many special sets of data are used to identify a suspect model? Will this impact the DNN performance? Could the authors provide some detailed evaluation results of this part?
==================================================

Focused review:

Although this paper gives a different view on the policy gradient methods to readers, there are some limitations: 1. The theory used in its propositions are not in depth as compared to other theoretical papers in this area. The policy improvement operator and the projection operator can be found in few lines, and all propositions except for the proposed surrogate loss and the bridge between value-based and policy-based methods, are also followed by few lines. 2. It seems that the proposed view can be used for interpretation and understanding of RL algorithms, but not designing new practical RL algorithms. Since the policy improvement operator maps a parameterized policy to a better policy that may not be included in the set of parameterized policies, the result of improvement operator cannot be implemented in a practical manner.

Review Point: 1. The theory used in its propositions are not in depth as compared to other theoretical papers in this area. The policy improvement operator and the projection operator can be found in few lines, and all propositions except for the proposed surrogate loss and the bridge between value-based and policy-based methods, are also followed by few lines.
Review Point: 2. It seems that the proposed view can be used for interpretation and understanding of RL algorithms, but not designing new practical RL algorithms. Since the policy improvement operator maps a parameterized policy to a better policy that may not be included in the set of parameterized policies, the result of improvement operator cannot be implemented in a practical manner.
==================================================

Focused review:

Weakness
It is not surprising that Lp-based defenses (smoothing) are not robust to non-Lp threat models in the community. The authors aim to present evidence that smoothing techniques are not robust enough because the proposed non-Lp attacks can break the explanations, which by itself seems to be an unfair comparison for the prior work. In fact, the contributions showing the proposed non-Lp attacks can break Lp defense is not a new observation.
The proposed attacks do not seem to produce very different results. In Fig 5, resulting adversarial attacks from new technologies remain very similar to Delta attack. In Fig 4, it seems that the new attacks only make more than 0.05 difference with the Delta attack on β
-smoothing in cosine distance. On other attributions, the improvement seems to be minimal.
Gini Index for evaluating sparseness. The motivation to use Gini Index seems to be fair, however, a lot of prior works have proposed several different metrics [1, 2, 3, 4] in measuring the sparseness (or the concentrations and the localizations on the relevant features) of attributions. At least some discussions and justifications of the proposed metrics should be included. The robustness-related and fidelity-related evaluations are motivated from the paper’s main idea, understanding if smoothing techniques provide faithful explanations, however, the transition to study sparsity is somewhat sudden to me and it seems to be unconnected from the previous content, only because the following reason authors provide: “To create explanations that are human-accessible, it is advantageous to have a sparse explanation map”
Some related prior work, i.e. ROAR [5] , that finds smoothing techniques do not create significant degeneration to explanations should be included and discussed, especially when the paper is trying to provide the shortcoming of the smoothing techniques.
[1] A. Chattopadhay, A. Sarkar, P. Howlader and V. N. Balasubramanian, "Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks," 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 2018, pp. 839-847, doi: 10.1109/WACV.2018.00097.
[2] Poppi, S., Cornia, M., Baraldi, L., & Cucchiara, R. (2021). Revisiting The Evaluation of Class Activation Mapping for Explainability: A Novel Metric and Experimental Analysis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2299-2304.
[3] Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel, P., & Hu, X. (2020). Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 111-119.
[4] Fong, R., Patrick, M., & Vedaldi, A. (2019). Understanding Deep Networks via Extremal Perturbations and Smooth Masks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2950-2958.
[5] Hooker, Sara et al. “A Benchmark for Interpretability Methods in Deep Neural Networks.” NeurIPS(2019).

Review Point: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2299-2304. [3] Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel, P., & Hu, X. (2020). Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks.
Review Point: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 111-119. [4] Fong, R., Patrick, M., & Vedaldi, A. (2019). Understanding Deep Networks via Extremal Perturbations and Smooth Masks.
Review Point: 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2950-2958. [5] Hooker, Sara et al. “A Benchmark for Interpretability Methods in Deep Neural Networks.” NeurIPS(2019).
==================================================

Focused review:

- The authors compared their method to the baseline approach only. However, there are plenty of curriculum learning methods that could have been used as relevant state-of-the-art competing methods to compare with, e.g. [R1, R2, R3, R4]. Comparison with such competing methods is mandatory, in my opinion. - In Eq. (2.1), I believe that the non-linearity is typically applied before the pooling operation. - In terms of novelty, the idea of adding some Gaussian kernels to the network is quite straightforward and simple. Even so, it is not clear why it works so well. The provided motivation is not enough. I would have like to see some visualizations of low-level, mid-level and high-level filters and how these evolve during training in order to figure out what is happening. All the experiments are performed on images, so I would consider this a vision paper. A vision paper without figures is not a properly written vision paper. - Does the approach apply to data other than images? Until proven otherwise, it should clearly stated in the title that the approach applies to images only, e.g. "Curriculum by Smoothing for Images". - Are the improvements statistically significant? A statistical test should be performed to test the null hypothesis. Missing references: [R1] Saxena, S., Tuzel, O. and DeCoste, D., 2019. Data parameters: A new family of parameters for learning a differentiable curriculum. In Advances in Neural Information Processing Systems (pp. 11093–11103). [R2] Soviany, P., Ardei, C., Ionescu, R.T. and Leordeanu, M., 2020. Image difficulty curriculum for generative adversarial networks (CuGAN). In The IEEE Winter Conference on Applications of Computer Vision (pp. 3463-3472). [R3] Penha, G. and Hauff, C., 2020, April. Curriculum Learning Strategies for IR. In European Conference on Information Retrieval (pp. 699-713). [R4] Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. "Progressive growing of gans for improved quality, stability, and variation." arXiv preprint arXiv:1710.10196 (2017).

Review Point: - The authors compared their method to the baseline approach only. However, there are plenty of curriculum learning methods that could have been used as relevant state-of-the-art competing methods to compare with, e.g. [R1, R2, R3, R4]. Comparison with such competing methods is mandatory, in my opinion.
Review Point: - In Eq. (2.1), I believe that the non-linearity is typically applied before the pooling operation.
Review Point: - In terms of novelty, the idea of adding some Gaussian kernels to the network is quite straightforward and simple. Even so, it is not clear why it works so well. The provided motivation is not enough. I would have like to see some visualizations of low-level, mid-level and high-level filters and how these evolve during training in order to figure out what is happening. All the experiments are performed on images, so I would consider this a vision paper. A vision paper without figures is not a properly written vision paper.
Review Point: - Does the approach apply to data other than images? Until proven otherwise, it should clearly stated in the title that the approach applies to images only, e.g. "Curriculum by Smoothing for Images".
Review Point: - Are the improvements statistically significant? A statistical test should be performed to test the null hypothesis. Missing references: [R1] Saxena, S., Tuzel, O. and DeCoste, D., 2019. Data parameters: A new family of parameters for learning a differentiable curriculum. In Advances in Neural Information Processing Systems (pp. 11093–11103). [R2] Soviany, P., Ardei, C., Ionescu, R.T. and Leordeanu, M., 2020. Image difficulty curriculum for generative adversarial networks (CuGAN). In The IEEE Winter Conference on Applications of Computer Vision (pp. 3463-3472). [R3] Penha, G. and Hauff, C., 2020, April. Curriculum Learning Strategies for IR. In European Conference on Information Retrieval (pp. 699-713). [R4] Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. "Progressive growing of gans for improved quality, stability, and variation." arXiv preprint arXiv:1710.10196 (2017).
==================================================

Focused review:

The model formulation is very confusing. For example, the notion of "feedback" is apparently pretty important to this model--however the notion of "feedback" is not described before it's attached to a mathematical quantity H_k, n-1. Furthermore, it's not clear what kind of mathematical object H is. I would encourage the authors to re-structure their section introducing their formalism, and pass it by researchers not familiar with this project--this might help improve the clarity. This entire section was difficult to follow, even after reading through it several times. Some specific points of confusion (perhaps due to my not fully understanding the formalism): - how are tasks assigned to individuals within a group? is this important? - why is there a notion of "groups", and why not just "individuals"? since there seems to be no formalism describing individuals, it seems that groups are not *sets* of servers or agents, but rather groups *are* the agents. If my understanding is correct, then it's somewhat dubious to claim that this paper considers group fairness. If my understanding is incorrect, then the authors should please please clarify the model and formalism. - is the goal of your allocation policy to learn? or is it to allocate resources efficienctly? Or both, as in some bandit settings? Correctness: Are the claims and method correct? Is the empirical methodology correct? * (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers) Not sure. Since I wasn't able to fully understand the model, I could not fully assess the authors claims.

Review Point: - how are tasks assigned to individuals within a group? is this important?
Review Point: - why is there a notion of "groups", and why not just "individuals"? since there seems to be no formalism describing individuals, it seems that groups are not *sets* of servers or agents, but rather groups *are* the agents. If my understanding is correct, then it's somewhat dubious to claim that this paper considers group fairness. If my understanding is incorrect, then the authors should please please clarify the model and formalism.
Review Point: - is the goal of your allocation policy to learn? or is it to allocate resources efficienctly? Or both, as in some bandit settings? Correctness: Are the claims and method correct? Is the empirical methodology correct?
Review Point: * (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers) Not sure. Since I wasn't able to fully understand the model, I could not fully assess the authors claims.
==================================================

Focused review:

Weaknesses: 1. The proposed model has many good attributes, but a disadvantage that sort of stand out to me is the complexity of explicitly computing the eigen-decomposition of the graph Laplacian. In my opinion, the merits of the paper outweigh this drawback, but it is something the paper can improve upon. The O(N^3) complexity can easily be prohibitive on large datasets. For example, the COVID analysis is done at a country level. If this resolution were to be increased to state/county level, the resulting graph would be massive. I wonder if using approximations of the eigen-modes affect the performance of the model. Also, going by the results of the ablation study, removing the GFT leads to only an incremental reduction in performance. So I wonder how critical the GFT is, given that it is an expensive operation. 2. The paper can also mention/ give examples where the time-series prediction did not do great. For example, in the COVID-19 dataset, UK and Russia are learnt to be highly correlated, even though are not neighboring countries. Further, were there countries where the predictions were not great? How did the trajectories of these countries relate to the top eigenvectors of the Laplacian?

Review Point: 1. The proposed model has many good attributes, but a disadvantage that sort of stand out to me is the complexity of explicitly computing the eigen-decomposition of the graph Laplacian. In my opinion, the merits of the paper outweigh this drawback, but it is something the paper can improve upon. The O(N^3) complexity can easily be prohibitive on large datasets. For example, the COVID analysis is done at a country level. If this resolution were to be increased to state/county level, the resulting graph would be massive. I wonder if using approximations of the eigen-modes affect the performance of the model. Also, going by the results of the ablation study, removing the GFT leads to only an incremental reduction in performance. So I wonder how critical the GFT is, given that it is an expensive operation.
Review Point: 2. The paper can also mention/ give examples where the time-series prediction did not do great. For example, in the COVID-19 dataset, UK and Russia are learnt to be highly correlated, even though are not neighboring countries. Further, were there countries where the predictions were not great? How did the trajectories of these countries relate to the top eigenvectors of the Laplacian?
==================================================

Focused review:

weaknesses of the two variants (does the VI version run faster in terms of wall-clock time, is it more sample efficient, does it generalize better, â¦?). Given the small size of the toy domain, other (brute-force, or inefficient sampling-based) methods could potentially be included as well, but it would be OK to dismiss them by showing results on a larger-scale task where competitor methods can no longer be applied. Another competitor for comparison in Fig 1 would be the Dirichlet estimate with (a) copying the action-distribution from the nearest observed neighbour state or (b) taking the average over all observed neighbour states within a certain radius. 2) Larger-scale experiments. Why were there no experiments with larger state-action spaces and non-trivial dynamics included (at least grid-worlds with walls, and other non-trivial tiles)? Currently it is hard to judge whether this was simply due to a lack of time or because the method has severe scalability issues. Very convincing experiments would be e.g. on simple video-game domains, (which naturally have a low-cardinality discrete state- and action-space) - simulators for such experiments are publicly available and comparison against other approaches would be easier. 3) Literature: there is a considerable body of literature on (hierarchical) inference in latent-variable models that is barely mentioned. E.g. in the language domain, topic models are mentioned but dismissed as ânot discussing the problem of decision-makingâ. Can some of these methods be straight-forwardly be applied to the tasks/domains shown in the paper - it seems so, since the model in the paper is not explicitly used for decision-making. Please correct me if Iâm wrong and discuss this in greater detail. Hierarchical inference for discrete-variable models is also discussed in the literature on Bayesian deep (reinforcement) learning and hierarchical representation learning with deep networks - importantly, these models are also optimized via variational inference (ELBO maximization), however under different approximate distributions (the emphasis is on differentiability, rather than closed-form expressions). What are the advantages/disadvantages compared to the presented method (scalability, data-efficiency, â¦)? I am of course happy to also see non-deep-neural-network approaches, but this literature must be discussed in order to put the method into perspective. See e.g. [1] for lots of up-to-date pointers to literature. [1] https://duvenaud.github.io/learn-discrete/ 4) Shortcomings of the method and implications of the simplifications/approximations. Please discuss the implications of the mean-field approximation for the variational distributions, beyond simply stating the mathematical form. The same applies for restricting \theta to be a scale parameter (line: 165) - ideally compare empirically against no restrictions and doing the full matrix inversion numerically (particularly since the experiments are on small domains), or against using a low-rank matrix factorization. Finally, please discuss the implications of using a square-exponential kernel - would it for instance still be suitable in grid-worlds with walls, or other situations where simple Euclidean distance of states is not indicative of the âgeneralizabilityâ of state-dependent action-distributions. Originality: Medium - the derivation of the VI scheme and the EM scheme is interesting and novel, but replacing a sampling-based ELBO optimization with a VI-based one is a rather straightforward idea. Quality: Low - while the derivations are well-presented and sufficient detail is given, the experimental section lacks comparison against important methods. Some ablation studies and sensitivity-analysis w.r.t. Hyper-parameters would have been nice and results regarding larger-scale applications are crucially required to judge the significance of the approach. The literature-discussion lacks important parts. Clarity: High - the paper is generally well written. The only important improvement is a qualitative/informal discussion of some of the restrictions/approximations, such as the mean-field approximation - though the mathematical statement is sufficient in principle, adding one, two sentences would not hurt. Significance: Currently low - the method could potentially be quite significant, but this needs to be shown with experiments that compare against other state-of-the-art methods and larger-scale experiments. It remains unclear whether the same results could have been achieved with the sampling-based approach and where the advantages of the VI approach lie.

Review Point: 2) Larger-scale experiments. Why were there no experiments with larger state-action spaces and non-trivial dynamics included (at least grid-worlds with walls, and other non-trivial tiles)? Currently it is hard to judge whether this was simply due to a lack of time or because the method has severe scalability issues. Very convincing experiments would be e.g. on simple video-game domains, (which naturally have a low-cardinality discrete state- and action-space) - simulators for such experiments are publicly available and comparison against other approaches would be easier.
Review Point: 3) Literature: there is a considerable body of literature on (hierarchical) inference in latent-variable models that is barely mentioned. E.g. in the language domain, topic models are mentioned but dismissed as ânot discussing the problem of decision-makingâ. Can some of these methods be straight-forwardly be applied to the tasks/domains shown in the paper - it seems so, since the model in the paper is not explicitly used for decision-making. Please correct me if Iâm wrong and discuss this in greater detail. Hierarchical inference for discrete-variable models is also discussed in the literature on Bayesian deep (reinforcement) learning and hierarchical representation learning with deep networks - importantly, these models are also optimized via variational inference (ELBO maximization), however under different approximate distributions (the emphasis is on differentiability, rather than closed-form expressions). What are the advantages/disadvantages compared to the presented method (scalability, data-efficiency, â¦)? I am of course happy to also see non-deep-neural-network approaches, but this literature must be discussed in order to put the method into perspective. See e.g. [1] for lots of up-to-date pointers to literature. [1] https://duvenaud.github.io/learn-discrete/
==================================================

Focused review:

There are several serious weaknesses from my point of views. 1. The definition of boundary thickness is rather complex and might hinder the understanding of this notion and the practical use. How to choose \alpha and \beta? How to select the distribution p? It might happen that boundary thickness of classifier A is better than that of classifier B under one set of parameters, but changing another set of parameters this case will not hold. 2. I think the theory about boudary thickness is lacking in this paper right now. How about the generalization bound with respect to boundary thickness? Is there anything like the generalization bound of margin? 3. Can we develop new algorithms that aim to maximize the boundary thickness? I did not see any specific algorithm from this motivation. 4. The practical importance of this new notion has not been demonstrated in the empirical experiments. The authors devised the noisy-mixup training method and showed the improved robustness. But I think there is a weak connection with this new notion of boundary thickness, at most the authors showed boundary thickness has been enlarged empirically and the improved robustness can somehow attribute to the enlarged thickness.

Review Point: 1. The definition of boundary thickness is rather complex and might hinder the understanding of this notion and the practical use. How to choose \alpha and \beta? How to select the distribution p? It might happen that boundary thickness of classifier A is better than that of classifier B under one set of parameters, but changing another set of parameters this case will not hold.
Review Point: 2. I think the theory about boudary thickness is lacking in this paper right now. How about the generalization bound with respect to boundary thickness? Is there anything like the generalization bound of margin?
Review Point: 3. Can we develop new algorithms that aim to maximize the boundary thickness? I did not see any specific algorithm from this motivation.
Review Point: 4. The practical importance of this new notion has not been demonstrated in the empirical experiments. The authors devised the noisy-mixup training method and showed the improved robustness. But I think there is a weak connection with this new notion of boundary thickness, at most the authors showed boundary thickness has been enlarged empirically and the improved robustness can somehow attribute to the enlarged thickness.
==================================================

Focused review:

Weaknesses:
• Empirical results can be improved by
o Including more competing methods beyond continuous optimization approaches. The PC and GES algorithms are non-identifiable. Many identifiable alternative approaches exist such as ANM.
o Considering more real data validations. Currently, only one real data result is presented. And that dataset is not in the federated setting. To show the real usage of the proposed method, at least one real distributed dataset is needed in my opinion.
o Simulations show the proposed method is superior when data are generated under the proposed model. It will be more interesting to see how the model performs under model misspecification.
• Hyperparameter tuning and sensitivity analysis. Since causal structural learning can be seen as an unsupervised learning task, tuning hyperparameter is particularly important. From Section A.5, it looks like some hyperparameters are fixed to certain values and others are tuned by simulations. For the former, I believe a sensitivity analysis is in order. For the latter, how does this strategy generalize to real data or simulation settings that are substantially different from the scenarios used for tuning? In fact, it is not clear to me what the authors meant by “tune these two parameters via the same scale of experiment with seeds 1 ∼ 10”. Can the authors provide more details on that?

Review Point: • Empirical results can be improved by o Including more competing methods beyond continuous optimization approaches. The PC and GES algorithms are non-identifiable. Many identifiable alternative approaches exist such as ANM. o Considering more real data validations. Currently, only one real data result is presented. And that dataset is not in the federated setting. To show the real usage of the proposed method, at least one real distributed dataset is needed in my opinion. o Simulations show the proposed method is superior when data are generated under the proposed model. It will be more interesting to see how the model performs under model misspecification.
Review Point: • Hyperparameter tuning and sensitivity analysis. Since causal structural learning can be seen as an unsupervised learning task, tuning hyperparameter is particularly important. From Section A.5, it looks like some hyperparameters are fixed to certain values and others are tuned by simulations. For the former, I believe a sensitivity analysis is in order. For the latter, how does this strategy generalize to real data or simulation settings that are substantially different from the scenarios used for tuning? In fact, it is not clear to me what the authors meant by “tune these two parameters via the same scale of experiment with seeds 1 ∼ 10”. Can the authors provide more details on that?
==================================================

Focused review:

1. The paper relies on pre-defined rules (Section 5.2) to divide the full knowledge graph to sub-graphs, which make the paper less interesting. 2. Although the paper has an ablation study, it does not contain enough empirical study and discussion about sub-graphs. For example, in Section 4.2, the authors mentioned that there are two type of sub-graphs, one with historical information and one does not. Which one contributes the most to the final performance? There are two awareness mentioned in Section 4.2 (temporal-awareness and relational awareness), which one contributes the most to the final performance, and why? What exactly is missing from full knowledge graph that sub-graph captures? There should be more discussion about why sub-graphs and low-level attention helps.

Review Point: 1. The paper relies on pre-defined rules (Section 5.2) to divide the full knowledge graph to sub-graphs, which make the paper less interesting.
Review Point: 2. Although the paper has an ablation study, it does not contain enough empirical study and discussion about sub-graphs. For example, in Section 4.2, the authors mentioned that there are two type of sub-graphs, one with historical information and one does not. Which one contributes the most to the final performance? There are two awareness mentioned in Section 4.2 (temporal-awareness and relational awareness), which one contributes the most to the final performance, and why? What exactly is missing from full knowledge graph that sub-graph captures? There should be more discussion about why sub-graphs and low-level attention helps.
==================================================

Focused review:

Weaknesses: See below for some questions. - General Discussion: In the Gaussian mixture models, the number of gaussian components (k) is usually an important parameter. In the experiments of this paper, k is set to 2. What is your criteria to select k? Does the increase of k hurt the performance of this model? What does the learned distribution look like for a word that only has one popular meaning?
I notice that you use the spherical case in all the experiments (the covariance matrix reduces to a single number). Is this purely for computation efficiency?
I wonder what's the performance of using a general diagonal covariance matrix.
Since in this more general case, the gaussian mixture defines different degrees of uncertainty along different directions in the semantic space, which seems more interesting.
Minor comments: Table 4 is not referred to in the text.
In reference, Luong et al. lacks the publication year.
I have read the response.

Review Point: -General Discussion: In the Gaussian mixture models, the number of gaussian components (k) is usually an important parameter. In the experiments of this paper, k is set to 2. What is your criteria to select k? Does the increase of k hurt the performance of this model? What does the learned distribution look like for a word that only has one popular meaning? I notice that you use the spherical case in all the experiments (the covariance matrix reduces to a single number). Is this purely for computation efficiency? I wonder what's the performance of using a general diagonal covariance matrix. Since in this more general case, the gaussian mixture defines different degrees of uncertainty along different directions in the semantic space, which seems more interesting. Minor comments: Table 4 is not referred to in the text. In reference, Luong et al. lacks the publication year. I have read the response.
==================================================

Focused review:

Weaknesses The empirical results should also be given in tabular form with numeric values, not only in the form of plots. This is important for accurate comparison of algorithms in the future references. 5. Originality: 5.1 Strengths The algorithm and the theoretical results are original. 6. Significance: 6.1 Strengths The work is significant as it will push the research in the desired direction of establishing the framework for the trade-off between predictive and computation performance. 7. Minor remarks: References: Reference 1 has been doubled, please check also other references as they do not contain all relevant information. 8. After rebuttal I thank the authors for their rebuttal. I do not change my evaluation, but give one additional comment that could be useful for the authors. As the paper compares W-LTLS to LomTree and FastXML, it could also include the comparison to a label tree approach being either represented by HSM, PLT, or Parabel. All these algorithms are very similar to each other. PLT and Parabel are multi-label generalizations of HSM. Parabel additionally uses a specific tree building strategy that gets very good results for multi-label problems. To compress the model size in label tree approaches one can use a simple shallow network as it is done in the FastText implementation.

Review Point: 5. Originality: 5.1 Strengths The algorithm and the theoretical results are original.
Review Point: 6. Significance: 6.1 Strengths The work is significant as it will push the research in the desired direction of establishing the framework for the trade-off between predictive and computation performance.
Review Point: 7. Minor remarks: References: Reference 1 has been doubled, please check also other references as they do not contain all relevant information.
Review Point: 8. After rebuttal I thank the authors for their rebuttal. I do not change my evaluation, but give one additional comment that could be useful for the authors. As the paper compares W-LTLS to LomTree and FastXML, it could also include the comparison to a label tree approach being either represented by HSM, PLT, or Parabel. All these algorithms are very similar to each other. PLT and Parabel are multi-label generalizations of HSM. Parabel additionally uses a specific tree building strategy that gets very good results for multi-label problems. To compress the model size in label tree approaches one can use a simple shallow network as it is done in the FastText implementation.
==================================================

Focused review:

weakness of this paper is the presentation and discussion of the experimental results.
First, the experimental results compare the mean computed from 5 runs per algorithm. 5 samples is usually not enough to draw meaningful conclusions about the differences between algorithms. 10 runs would be better, and 30 would be ideal.
The shaded region in the plots is stated to be the standard deviation. Is this assuming the distribution of the learning curves is normal? Is this actually the case?
With only 5 runs it might actually be clearer just to plot all the runs individually instead of the mean. This would give the reader a better idea of the distribution and relative performance.
In the section “Is planning necessary to make a better decision in continuous control?” a new parameter N_p is introduced without any reference in the main text. Is this a parameter of MAAC? What does it mean? This seems to be central to understanding this section.
The section “How the learned model quality affect decision-making?” talks about how the amount of training data used to train the model affects planning utility. What is this model? How is it trained?
The text is missing some information to make Figure 4 more understandable. First plots 4.a and 4.b show “Improvements” on the y-axis. What is this? How is it computed. In 4.a what is policy quality? How is it measured? In 4.b what is model quality? How is it measured? Are they averages of something? A single sample?
I think there is some potentially interesting content in the ablation experiments, but in its current form it is hard to understand. It deserves a more thorough explanation and improved clarity.

Review Point: 5 samples is usually not enough to draw meaningful conclusions about the differences between algorithms.
==================================================

Focused review:

Weakness: 1.Commonsense Knowledge 1.1 Commonsense KG not provided, not analyzed The novelty of this paper lies in the commonsense knowledge incorporated in problem generation. However, there's little introduction to the commonsense knowledge graph in the paper. Where does the external knowledge graph come from? Is it collected by the authors or derived from other sources? What is the format of the commonsense knowledge graph? What are some examples of the knowledge graphs of different topics?
1.2 Confusion about the number in commonsense knowledge According to the authors, the equation is transferred to a "template" by number mapping. For example, 2x+4y=60 can be transferred to Ax+By=M. However, there are also numbers in commonsense knowledge. For example, in the chicken and rabbit problem, each chicken has 2 feet and each rabbit has 4 feet. So the commonsense numbers are 2 and 4. However, the numbers are all input by the equation. And since it's transferred to Ax+By=M, it's the same as the model whether it's 2x+4y or 3x+6y. In this sense, it does not incorporate commonsense numbers at all. So what's the use of the commonsense knowledge graph?
2.Data 2.1 Insufficient analysis of the collected dataset The paper proposes a dataset. However, it does not provide any analysis of the dataset, such as what topics are covered by the dataset, how many problems in a topic, problem lengths, number of templates, etc.
2.2 Dataset too small The dataset only contains 1k+ problems, which is far smaller than the existing math word problems dataset (e.g., Math23k). Since the model is rather complicated, it's doubtful that such a small dataset can be trained to get an ideal result.
2.3 No data file in the code appendix
3.Experiment 3.1 Performance only slightly better than the template The performances are only slightly better than the template.
3.2 Human Evaluation The paper does not give the standard deviation of the human evaluation, which is not convincing.
3.3 Qualitative study shows poor performance 3.3.1 In Table 3, MaKE generates a bad problem. How do we know that the small boat has 2 people and the larger boat has 4 people? The equation 2x-4y=35 is not used at all. 3.3.2 In Table 4, the model generates several examples. But it only changes "equal" to "0 less than" and "0 more than", which does not show diversity.

Review Point: 1.Commonsense Knowledge 1.1 Commonsense KG not provided, not analyzed The novelty of this paper lies in the commonsense knowledge incorporated in problem generation. However, there's little introduction to the commonsense knowledge graph in the paper. Where does the external knowledge graph come from? Is it collected by the authors or derived from other sources? What is the format of the commonsense knowledge graph? What are some examples of the knowledge graphs of different topics? 1.2 Confusion about the number in commonsense knowledge According to the authors, the equation is transferred to a "template" by number mapping. For example, 2x+4y=60 can be transferred to Ax+By=M. However, there are also numbers in commonsense knowledge. For example, in the chicken and rabbit problem, each chicken has 2 feet and each rabbit has 4 feet. So the commonsense numbers are 2 and 4. However, the numbers are all input by the equation. And since it's transferred to Ax+By=M, it's the same as the model whether it's 2x+4y or 3x+6y. In this sense, it does not incorporate commonsense numbers at all. So what's the use of the commonsense knowledge graph?
Review Point: 2.Data 2.1 Insufficient analysis of the collected dataset The paper proposes a dataset. However, it does not provide any analysis of the dataset, such as what topics are covered by the dataset, how many problems in a topic, problem lengths, number of templates, etc. 2.2 Dataset too small The dataset only contains 1k+ problems, which is far smaller than the existing math word problems dataset (e.g., Math23k). Since the model is rather complicated, it's doubtful that such a small dataset can be trained to get an ideal result. 2.3 No data file in the code appendix 3.Experiment 3.1 Performance only slightly better than the template The performances are only slightly better than the template. 3.2 Human Evaluation The paper does not give the standard deviation of the human evaluation, which is not convincing. 3.3 Qualitative study shows poor performance 3.3.1 In Table 3, MaKE generates a bad problem. How do we know that the small boat has 2 people and the larger boat has 4 people? The equation 2x-4y=35 is not used at all. 3.3.2 In Table 4, the model generates several examples. But it only changes "equal" to "0 less than" and "0 more than", which does not show diversity.
==================================================

Focused review:

1. notations can be improved to be more accurate and simplified. For example, in line 163 eq1, here e_h and e_t represent an entity in general, however in line 161, the authors define e_h and e_t are seen entities. Too many notations and inconsistency can confuse the reader to some extent. And *simulated* unseen and *real* unseen sometimes are mixed up. An alternative way is to use meta-train and meta-test to denote the dataset during training and evaluation while during meta-training/meta-test the authors build support set and query set for every episode. This also raises my confusion in the meta-learning framework section: is the meta-training set includes unseen entities or not? For now I just assume it's not, otherwise, i believe the setting does not make any sense. Some notations miss explanation, such as \phi_i, f_{\theta} in eq2. Also, the figure1 includes too many lines and can be simplified for better readability. 2. how do you divide the support set and the query set in meta-training set? 3. Novelty of the model. I-GEN is pretty similar to meta-R and the T-GEN is a variant of graph-VAE. 4. What is Seen to Seen, …, Seen to Unseen in the most left col of table2? 5. It is better to denote clearly in the comparison that the T-GEN is under the transductive setting, which utilizes more data during evaluation. 6. why in Figure 5, more shots do not get improvements?

Review Point: 1. notations can be improved to be more accurate and simplified. For example, in line 163 eq1, here e_h and e_t represent an entity in general, however in line 161, the authors define e_h and e_t are seen entities. Too many notations and inconsistency can confuse the reader to some extent. And *simulated* unseen and *real* unseen sometimes are mixed up. An alternative way is to use meta-train and meta-test to denote the dataset during training and evaluation while during meta-training/meta-test the authors build support set and query set for every episode. This also raises my confusion in the meta-learning framework section: is the meta-training set includes unseen entities or not? For now I just assume it's not, otherwise, i believe the setting does not make any sense. Some notations miss explanation, such as \phi_i, f_{\theta} in eq2. Also, the figure1 includes too many lines and can be simplified for better readability.
Review Point: 2. how do you divide the support set and the query set in meta-training set?
Review Point: 3. Novelty of the model. I-GEN is pretty similar to meta-R and the T-GEN is a variant of graph-VAE.
Review Point: 4. What is Seen to Seen, …, Seen to Unseen in the most left col of table2?
Review Point: 5. It is better to denote clearly in the comparison that the T-GEN is under the transductive setting, which utilizes more data during evaluation.
Review Point: 6. why in Figure 5, more shots do not get improvements?
==================================================

Focused review:

Weakness: The main weaknesses of this paper lie in experimental comparison and technical contribution: 1）There seem to be some existing methods in the field of nonlinear physical systems modeling, such as LED[1] and Solver-in-the-loop[2] It is suggested to compare the proposed method with these existing methods. 2) There are some methods to fuse the representation of different views, such as Cross-Attention and CCA. Do these information-fusing methods work well in physical systems modeling? 3) The whole method contains three components. Which one is most important? It is suggested to add a detailed ablation study. 4) Using a neural network to learn particle-continuum coupling is not difficult. Did existing methods use the neural network for coupling? If yes, which is the advantage of this method? If not, it is better to analyze the effect of different deep-learning-based coupling methods.

Review Point: 2) There are some methods to fuse the representation of different views, such as Cross-Attention and CCA. Do these information-fusing methods work well in physical systems modeling?
Review Point: 3) The whole method contains three components. Which one is most important? It is suggested to add a detailed ablation study.
Review Point: 4) Using a neural network to learn particle-continuum coupling is not difficult. Did existing methods use the neural network for coupling? If yes, which is the advantage of this method? If not, it is better to analyze the effect of different deep-learning-based coupling methods.
==================================================

Focused review:

Weaknesses of the paper
The presentation and organization of this paper should be improved. Typos and language issues can be easily found, see the minor comments below. The contributions are not well highlighted in both the abstract and introduction section.
The authors take many efforts to conduct experiments for the position embedding of BERTs, and provide an empirical study of the four existing position embedding models (fully learnable APEs (Gehring et al., 2017), (2) fixed sinusoidal APEs (Vaswani et al., 2017), (3) fully learnable RPEs (Shaw et al., 2018), and (4) fixed sinusoidal RPEs (Wei et al., 2019).). The author tested these models and their combinations over three benchmarking datasets, however, as an empirical study paper, the analysis is too weak.
(1) For the qualitative result shown in Table 2 and Table 3, neither insight nor connection to three properties is provided for the result. In fact, the result shown in Section 4 is intuitive and not something new, since these are the basic motivations of the RPEs and learnable PEs. Moreover, the effectiveness of using both RPE and APE has already been validated in "Self-Attention with Structural Position Representations, arXiv:1909.00383. 2019".
(2) I would like to see, in the experiment part, some new experimental results and conclusions in terms of the four summarised properties, which are key contributions the authors claimed. However, what I see is just some position vector embedding similarities (i.e. Fig. 2 and Fig.3) in terms of the four position embedding models, where the results are also somehow expected and intuitive. The conclusions of the experimental results are too subjective. For example, from the analysis of Fig (2), the conclusion that : “Lastly, note that fully-learnable RPEs also do not significantly distinguish far-distant RPEs (from -64 to -20 and from 20 to 64), suggesting that truncating RPEs into a distance of 64, like (Shaw et al., 2018), is reasonable.” is a bit farfetched. In this figure the white part is as narrow as (-5,+5), a further quantitative evidence in other forms for this conclusion will be more preferable. The same issue also exists in the conclusion of Figure (3): “This may be an advantage of RPEs over APEs to perceive forward and backward words, especially in span prediction tasks where capturing this matters.”
This paper lacks the discussion with the latest position embedding models such as,
[1] "Learning to Encode Position for Transformer with Continuous Dynamical Model". ICML. 2020
[2] "Encoding Word Order in Complex Embeddings", ICLR. 2019, where the 'Translation' property is also discussed.
Other minor comments:
In the introduction section, "distances in N and R D ", N and R D
should be explained when they appear in the first place.
The references should be formatted in a unified manner.
Table 2, the bold indicators for the best performances are put on the wrong numbers, e.g. in QNLI, the bold should be 89.5, in WNLI task it should be 51.3, and in STS-B it should be 87.5.
Overall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT. But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper.
Comments after the discussion
Thank you for your detailed response. I think most of my concerns were addressed so I updated the score to 6.

Review Point: 2020 [2] "Encoding Word Order in Complex Embeddings", ICLR. 2019, where the 'Translation' property is also discussed. Other minor comments: In the introduction section, "distances in N and R D ", N and R D should be explained when they appear in the first place. The references should be formatted in a unified manner. Table 2, the bold indicators for the best performances are put on the wrong numbers, e.g. in QNLI, the bold should be 89.5, in WNLI task it should be 51.3, and in STS-B it should be 87.5. Overall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT. But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper. Comments after the discussion Thank you for your detailed response. I think most of my concerns were addressed so I updated the score to 6.
==================================================

Focused review:

1. I don't think the experiments (Table 3) in Section 5.2 are comparing with the strongest baseline for certifying l_infty robustness. For example, for large perturbation (e.g. 8/255 or 16/255), IBP based methods (see "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models") are known to significantly outperform random-smoothing based methods (e.g., ~32% certified accuracy on CIFAR-10 for 8/255 radius even though the paper was published 2 years ago). Even for certifying small perturbation radius like 2/255, the paper still does not compare with the strongest baseline: a better certified accuracy for 2/255 is around 63% in the same setup (e.g., [31]), while you report 60%. 2. [30,31,32] have shown that Gaussian noise is optimal to certify l_p robustness for p>2 up to constant factor, while the paper claims that they can improve the certified robustness for p>2 by looking at a better noise distribution than Gaussian. To me, the only explanation for this claim is that the improvement is tiny (i.e., they only improve a small constant factor). 3. The design in Section 3.2 (i.e., the involvement of dual form) and the design of new noise distribution in Section 4 seem to be two totally different elements to improve the robustness. It is unclear to me which element play a more important role. In other words, the ablation study about the two elements is missing.

Review Point: 1. I don't think the experiments (Table 3) in Section 5.2 are comparing with the strongest baseline for certifying l_infty robustness. For example, for large perturbation (e.g. 8/255 or 16/255), IBP based methods (see "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models") are known to significantly outperform random-smoothing based methods (e.g., ~32% certified accuracy on CIFAR-10 for 8/255 radius even though the paper was published 2 years ago). Even for certifying small perturbation radius like 2/255, the paper still does not compare with the strongest baseline: a better certified accuracy for 2/255 is around 63% in the same setup (e.g., [31]), while you report 60%.
Review Point: 2. [30,31,32] have shown that Gaussian noise is optimal to certify l_p robustness for p>2 up to constant factor, while the paper claims that they can improve the certified robustness for p>2 by looking at a better noise distribution than Gaussian. To me, the only explanation for this claim is that the improvement is tiny (i.e., they only improve a small constant factor).
Review Point: 3. The design in Section 3.2 (i.e., the involvement of dual form) and the design of new noise distribution in Section 4 seem to be two totally different elements to improve the robustness. It is unclear to me which element play a more important role. In other words, the ablation study about the two elements is missing.
==================================================

Focused review:

Weaknesses:
In the paper the motivation of using meta-gradient to solve the formulated Lagrangian optimization is only explained once at the beginning of Page 4 "Our intuition is that a learning rate gradient that takes into account the overall task objective and constraint thresholds will lead to improved overall performance." However it is clear to me what explicitly do you want to achieve? Are you trying to find the "ground truth" λ ¯
(i.e., 1000 in the experiments)? Does not seem to be the case; Do you want to somehow learn a "robust" policy that works well for all λ ¯
values? If so it is expected that the authors would show the empirical results on different λ ¯
values, and the proposed method works well in all of them.
Following 1), the paper evaluates different forms of outer loss and show that using a critic-only outer loss yields the best performance. What is the intuition behind this? It would be good that the authors can have a more in-depth discussion on this.
In the empirical evaluations, the paper defines a "penalized return". This is very un-intuitive -- in reality there is not always such quantized trade-off between reward and penalty of constraint violation. If there is, then one could directly add that to the objective. It would be more interesting to see, for example, given the same reward value, the proposed method always outperforms the baselines in penalty; or the other way round. The "penalized return" metric is therefore un-convincing to me.
Assuming the penalized return metric makes sense. From Figure 3 it appears that the performances of the baselines sometimes are very close to the proposed method. What puzzles me is that the performances of the baselines vary dramatically across domains. Can the authors elaborate more on why this happens?
Questions: See weaknesses points 1) 2) and 4)
Less important points:
In table 1 it seems that overall performance of RS-D4PG monotonically increases w.r.t. λ
values. I am curious to see what happens when λ
is even smaller.
Page 3, Line 2, J o b j π ( θ ) -> τ and η
are missing in the bracket
Line 4 at paragraph D4PG: Q T ( s , . . . )
-> s'

Review Point: 1), the paper evaluates different forms of outer loss and show that using a critic-only outer loss yields the best performance. What is the intuition behind this? It would be good that the authors can have a more in-depth discussion on this. In the empirical evaluations, the paper defines a "penalized return". This is very un-intuitive -- in reality there is not always such quantized trade-off between reward and penalty of constraint violation. If there is, then one could directly add that to the objective. It would be more interesting to see, for example, given the same reward value, the proposed method always outperforms the baselines in penalty; or the other way round. The "penalized return" metric is therefore un-convincing to me. Assuming the penalized return metric makes sense. From Figure 3 it appears that the performances of the baselines sometimes are very close to the proposed method. What puzzles me is that the performances of the baselines vary dramatically across domains. Can the authors elaborate more on why this happens? Questions: See weaknesses points
Review Point: 1)2) and4) Less important points: In table 1 it seems that overall performance of RS-D4PG monotonically increases w.r.t. λ values. I am curious to see what happens when λ is even smaller. Page 3, Line 2, J o b j π ( θ ) -> τ and η are missing in the bracket Line 4 at paragraph D4PG: Q T ( s , . . . ) -> s'
==================================================

Focused review:

Weaknesses
Miscellaneous Issues
The presented paper uses a different font / font size or line distance. As the font / line distance is increased, I will not consider it for the review score, but it has be fixed for the final version. Typos
35: there have been remained certain problems
85: we make some conclusion. Suggestions: we provide a conclusion.
86: preposition: defer … in -> defer … to
174: measure -> measures
223: leads to three corresponding amortized sliced Wasserstein [a word seems to be missing here], …
The method requires additional memory and model evaluations but this is extensively discussed in the presented draft.
In practice with large models, i.e., in the demonstrated experiments, the computational cost of the Wasserstein loss is often small, which means that the utility of the method is limited in some applications. However, due to the favorable asymptotical complexity of the method, it allows learning in new settings which were previously impossible or hardly feasible. Further, even in the demonstrated cases the method achieve superior performance.
What I would have liked to see would be using SW and Max-SW with even larger L / T. It would be interesting to see at which point the methods break even, even if the baselines take a very long time. It would strengthen the paper even further, and with such an evaluation, I would consider raising my score.

Review Point: 86: preposition: defer … in -> defer … to 174: measure -> measures 223: leads to three corresponding amortized sliced Wasserstein [a word seems to be missing here], … The method requires additional memory and model evaluations but this is extensively discussed in the presented draft. In practice with large models, i.e., in the demonstrated experiments, the computational cost of the Wasserstein loss is often small, which means that the utility of the method is limited in some applications. However, due to the favorable asymptotical complexity of the method, it allows learning in new settings which were previously impossible or hardly feasible. Further, even in the demonstrated cases the method achieve superior performance. What I would have liked to see would be using SW and Max-SW with even larger L / T. It would be interesting to see at which point the methods break even, even if the baselines take a very long time. It would strengthen the paper even further, and with such an evaluation, I would consider raising my score.
==================================================

Focused review:

Weakness] 1 Theoretical analysis in Sec. 3.1 is quite standard and does not relate to adversarial examples.
2 On page 5 (below eq 8), there is an argument that "the above method with LS focuses more on the smoothness of CE loss surface, which may ignore the attack of target adversarial example." Could I interpret it in this way that LS can lead to significant issue gradient obfuscation that degrades the true robustness? If so, the main argument in this paper is that "robustness via adaptive LS" is not supported well.
3 In Figure 2 (b). It is really hard to infer the stated messages that illustrate the superiority of the proposal.
[Questions] 1 In Figure 6, why PGD is up-trend, but CW is down-trend over LS degree?
2 In Lemma 1, I get confused about why constraining the margin between any logits could induce better robustness. Usually, constraining the margin between any logits could obfuscate gradients, which leads to an illusion of higher robustness.
3 Besides, I doubt the message from Theorem 1. Let us think in an extreme way: the loss surface is entirely flat. The flat loss will also harm the generalization, which puts no use of robustness.

Review Point: 1 Theoretical analysis in Sec. 3.1 is quite standard and does not relate to adversarial examples.
Review Point: 2 On page 5 (below eq 8), there is an argument that "the above method with LS focuses more on the smoothness of CE loss surface, which may ignore the attack of target adversarial example." Could I interpret it in this way that LS can lead to significant issue gradient obfuscation that degrades the true robustness? If so, the main argument in this paper is that "robustness via adaptive LS" is not supported well.
Review Point: 3 In Figure 2 (b). It is really hard to infer the stated messages that illustrate the superiority of the proposal. [Questions] 1 In Figure 6, why PGD is up-trend, but CW is down-trend over LS degree?
Review Point: 2 In Lemma 1, I get confused about why constraining the margin between any logits could induce better robustness. Usually, constraining the margin between any logits could obfuscate gradients, which leads to an illusion of higher robustness.
Review Point: 3 Besides, I doubt the message from Theorem 1. Let us think in an extreme way: the loss surface is entirely flat. The flat loss will also harm the generalization, which puts no use of robustness.
==================================================

Focused review:

Weaknesses: 1. It could be better to analyze the properties and behaviors of the proposed algorithms. 2. Introduction section, the motivation for the proposed algorithm is ambiguous. 3. There are some writing problems in the current manuscript.

Review Point: 1. It could be better to analyze the properties and behaviors of the proposed algorithms.
Review Point: 2. Introduction section, the motivation for the proposed algorithm is ambiguous.
Review Point: 3. There are some writing problems in the current manuscript.
==================================================

Focused review:

Weaknesses ---
The proposal in the paper is very interesting. However, unfortunately there are points that aren't explained properly and that are difficult to follow.
W1. The optimization procedure---arguably a key advantage of the method---is difficult to understand.
Concepts like "natural parameters", "flats" and "natural gradient" are never properly defined. Since most of the ICLR audience are unlikely to be familiar with these concepts, they should be explained. If there's not enough room in the main paper, this could be put in the appendix.
The vectors θ B and η B
are, as far as I can tell, length- B
vectors that contain the entries from θ and η
corresponding to indices in B
, i.e., indices for which θ
is zero: θ b = 0
for all b ∈ B
. With this in mind, it seems like the stepping scheme in Eq (6) only updates entries in θ
that are already zero. For a distribution Q 1
to lie in the e-flat as shown in Fig 1 (a), doesn't all entries θ b for b ∈ B
have to be fixed to zero? In that case, why are those entries updated in Eq (6)? It seems like all this optimization procedure does is update parameters we know should be zero, while leaving the other parameters unchanged...
The complexity analysis in Sec 2.6 is hard to follow since it doesn't refer back to the particular computational steps discussed earlier.
What termination criteria is used in the gradient descent method? This is important when interpreting the experiment results.
W2. The practical connection between the θ
parameters (e.g., in Eq (7)) and the H
parameters (e.g., Eq (9)) isn't explained well. The optimization is done over the parameters in θ
, but the conditions (e.g., set n
-body interactions to zero for n ≥ 3
) are imposed on the H
parameters, which are sums of θ
parameters. It is not clear to me how such conditions on the H
parameters translate to conditions on the θ
parameters. For example, if H i k , i m ( k , m )
is zero, then what indices should be added to the zero-index set B
? The parameter count in Eq (15) counts the number of parameters in the H
factors---how does this relate to the number of θ
parameters that are actually being optimized?
W3. The comparison between specific n
-body approximations and conventional tensor decompositions is vague. For example, in Sec 3 you say "if we impose that decomposed factors can be represented as products with hyper-diagonal tensors Ω
, this decomposition is equivalent to a cyclic two-body approximation." It's not clear to me what this means. How should the symbol ≃
in the figures be interpreted? Since fitting a tensor ring is a non-convex problem, it's clear that they can't be equivalent.
W4. Examples of things throughout the text that aren't well-defined/hard to follow:
Sec 2.1: "dual flatness and orthogonality" of coordinate systems. What does this mean?
Sec 2.3: "element-wise cyclic product of matrices".
While terms like "partition function" may be standard in physics, they will be unfamiliar to most members of the ICLR audience.

Review Point: --- The proposal in the paper is very interesting. However, unfortunately there are points that aren't explained properly and that are difficult to follow.
==================================================

Focused review:

1. In all the illustration and the equations, there is a clear definition of two hierarchies in the latent space: latent variable y and hyperlatents z. It would be more convincing if the author could provide some reasoning behind the choice of the latent structure. I am not sure why the latent structure is restricted to a 2-layer structure. Recent progress in VAE community including ladderVAE and structuredVAE shows that either a ladder structure with many hierarchies or a BN as the latent structure leads to better modelling performance. I would like the authors to explain if the proposed method can be applies to these innovations as well. 2. The model combines several existing techniques together, including iterative inference, Gumbel annealing, etc. Although the combination of these techniques seem to be working for the compression task, the overall novelty seems to be incremental.

Review Point: 1. In all the illustration and the equations, there is a clear definition of two hierarchies in the latent space: latent variable y and hyperlatents z. It would be more convincing if the author could provide some reasoning behind the choice of the latent structure. I am not sure why the latent structure is restricted to a 2-layer structure. Recent progress in VAE community including ladderVAE and structuredVAE shows that either a ladder structure with many hierarchies or a BN as the latent structure leads to better modelling performance. I would like the authors to explain if the proposed method can be applies to these innovations as well.
Review Point: 2. The model combines several existing techniques together, including iterative inference, Gumbel annealing, etc. Although the combination of these techniques seem to be working for the compression task, the overall novelty seems to be incremental.
==================================================

Focused review:

- The experiments conducted in this paper are largely inspired by [1] and compare PRW to the Subspace Robust Wasserstein (SRW) distance. The authors argue that their empirical results show that PRW outperforms the Subspace Robust Wasserstein (SRW) distance [1] "in both statistical and computational aspects" (l.90-91). This conclusion is a bit confusing to me for the following reasons. 1) Figures 1, 2, 3 verify that the behavior of PRW is consistent. Specifically, PRW and SRW seem equivalent regarding their empirical performance: see Figure 1 vs. Figure 2 in [1], Figure 2 vs. Figures 3 and 4 in [1], Figure 3 left vs. Figure 3 right. 2) Figure 4: PRW indeed offers the best performance, but is actually followed very closely by SRW and even yields the same error as SRW for 3 noise levels (out of 7). 3) The experiments on real data (Tables 1 and 2 in the main document, and Table 3 in the supplementary doc.) show that the results given by PRW agree with the intuitive truth (e.g., the most similar movie script to Kill Bill 1 is Kill Bill 2) and are "consistently smaller" than SRW (l.273). Therefore, similar to Figures 1, 2 and 3, these results mainly show that the approximate solution for PRW performs nicely, but doesn't outperform SRW. Besides, the fact that PRW is always smaller than SRW in these experiments, might even suggest that PRW has less discriminative power than SRW. 4) Figure 4 (right) compares the execution times of PRW (approximated with RGAS or RAGAS) and SRW. We observe that PRW is faster than SRW, especially when the dimension is high. This indeed illustrates that PRW is more computationally efficient than SRW. 5) Finally, it is not clear to me which of the empirical results illustrates the statistical advantages of PRW compared with SRW (mentioned in l.90). I am actually wondering whether the efficient computation of PRW proposed in this paper should induce a nice sample complexity, or if the observations in [2] might apply in this setting as well. [1] Subspace Robust Wasserstein Distances. Paty and Cuturi, 2019. [2] Estimation of Wasserstein distances in the Spiked Transport Model. Niles-Weed and Rigollet, 2019. - The hyperparameter tuning (for example, \alpha, \beta, \gamma) and initialization steps (l.2 in Algorithms 1 and 2) are not explained clearly enough. Besides, an analysis of the empirical performance with varying \epsilon values is missing (for example, with the same setting as in Figure 4).

Review Point: - The experiments conducted in this paper are largely inspired by [1] and compare PRW to the Subspace Robust Wasserstein (SRW) distance. The authors argue that their empirical results show that PRW outperforms the Subspace Robust Wasserstein (SRW) distance [1] "in both statistical and computational aspects" (l.90-91). This conclusion is a bit confusing to me for the following reasons.
Review Point: 1) Figures 1, 2, 3 verify that the behavior of PRW is consistent. Specifically, PRW and SRW seem equivalent regarding their empirical performance: see Figure 1 vs. Figure 2 in [1], Figure 2 vs. Figures 3 and 4 in [1], Figure 3 left vs. Figure 3 right.
Review Point: 2) Figure 4: PRW indeed offers the best performance, but is actually followed very closely by SRW and even yields the same error as SRW for 3 noise levels (out of 7).
Review Point: 3) The experiments on real data (Tables 1 and 2 in the main document, and Table 3 in the supplementary doc.) show that the results given by PRW agree with the intuitive truth (e.g., the most similar movie script to Kill Bill 1 is Kill Bill 2) and are "consistently smaller" than SRW (l.273). Therefore, similar to Figures 1, 2 and 3, these results mainly show that the approximate solution for PRW performs nicely, but doesn't outperform SRW. Besides, the fact that PRW is always smaller than SRW in these experiments, might even suggest that PRW has less discriminative power than SRW.
Review Point: 4) Figure 4 (right) compares the execution times of PRW (approximated with RGAS or RAGAS) and SRW. We observe that PRW is faster than SRW, especially when the dimension is high. This indeed illustrates that PRW is more computationally efficient than SRW.
Review Point: 5) Finally, it is not clear to me which of the empirical results illustrates the statistical advantages of PRW compared with SRW (mentioned in l.90). I am actually wondering whether the efficient computation of PRW proposed in this paper should induce a nice sample complexity, or if the observations in [2] might apply in this setting as well. [1] Subspace Robust Wasserstein Distances. Paty and Cuturi, 2019. [2] Estimation of Wasserstein distances in the Spiked Transport Model. Niles-Weed and Rigollet, 2019.
Review Point: - The hyperparameter tuning (for example, \alpha, \beta, \gamma) and initialization steps (l.2 in Algorithms 1 and 2) are not explained clearly enough. Besides, an analysis of the empirical performance with varying \epsilon values is missing (for example, with the same setting as in Figure 4).
==================================================

Focused review:

- Although the algorithm was designed for large-scale FL, the paper only reports experiments with custom splits of MNIST or CIFAR10 with only 10 nodes. The more realistic LEAF benchmark dataset would have been better in this case. - As the algorithm is motivated by communication and computation costs, a clearer analysis of the advantages of the proposed algorithm over other algorithms in these aspects would have been appreciated.

Review Point: - Although the algorithm was designed for large-scale FL, the paper only reports experiments with custom splits of MNIST or CIFAR10 with only 10 nodes. The more realistic LEAF benchmark dataset would have been better in this case.
Review Point: - As the algorithm is motivated by communication and computation costs, a clearer analysis of the advantages of the proposed algorithm over other algorithms in these aspects would have been appreciated.
==================================================

Focused review:

1. The $\gamma$-separability requirement seems odd in the sense that it also requires samples from the same class to be separated by a margin. Earlier work by Gao et al.(2019) does not seem to have this requirement. This seems artificial and probably the analysis can be tightened to fix this. Would replacing such close samples by just one 'representative' sample from the same class help? 2. The previous work by Gao et al.(2019) and this work, both only guarantee robustness against the adversary used during training. Can something be said about the robustness against the most intelligent adversary $\mathcal{A}^*$, for a network trained using a polynomial time adversary $\mathcal{A}$? This seems to be much more important question than whether a network trained using a particular adversary can defend against the same adversary. ------ Post author feedback comments ------- My concerns have been satisfactorily answered in the feedback.

Review Point: 1. The $\gamma$-separability requirement seems odd in the sense that it also requires samples from the same class to be separated by a margin. Earlier work by Gao et al.(2019) does not seem to have this requirement. This seems artificial and probably the analysis can be tightened to fix this. Would replacing such close samples by just one 'representative' sample from the same class help?
Review Point: 2. The previous work by Gao et al.(2019) and this work, both only guarantee robustness against the adversary used during training. Can something be said about the robustness against the most intelligent adversary $\mathcal{A}^*$, for a network trained using a polynomial time adversary $\mathcal{A}$? This seems to be much more important question than whether a network trained using a particular adversary can defend against the same adversary. ------ Post author feedback comments ------- My concerns have been satisfactorily answered in the feedback.
==================================================

Focused review:

Weakness:
Standard deviations are not shown everywhere which makes some of the conclusions drawn by authors unreliable, detailed below.
In A.4, figure 13 (g) - pre-SE at r=16 has slightly lower params than the proposed SE and has better clean performance, the difference in robustness is marginal. One could also argue that there is no difference in robustness if the pre-SE model were to have more params (perhaps slightly larger dimensions in SE). Standard deviations in this setting hence become imperative, which were not shown. Hence, the conclusions drawn here seem a stretch.
The argument w.r.t kernel size seems inconsistent - there is clearly (albeit small) increase in rob.acc in fig 6 (e) at resolutions 64, 128 from kernel-size = 3 -> 5. Also trivially upscaling the images can lead to loss of coherence in cifar-like (low resolution) data - this could affect clean accuracy as well (numbers not shown). I am not sure if this set of experiments is grounded enough to draw the conclusions drawn. So a statement like “kernel size of 3x3 remains the preferred choice” is not justified.
In section 3.1.3 - activation, the experiments are thorough as they take into account different parameters. But Gowal et al., also came to the same conclusion that relu might be enough. Quoting from Gowal et al. “However, in contrast to Xie et al., we found that other “smooth” activation functions do not necessarily improve robustness” and “Overall, ReLU remains a good choice”. This section and contained experiments seem unnecessary.
LayerNorm does not converge - maybe the TRADES setting used is very strict - I would suggest trying standard AT or a low Divergence term coefficient in TRADES here. Again standard deviations are missing in A.6 to draw a conclusive inference about BN, GN etc. This section adds to the inconsistencies in the paper.
In Section 3.2.1, the ratios obtained are similar to Huang et al. who showed a 10:10:4 setting works best and this can be scaled for \alpha in {0.25, 0.5, 1.0,1.5, 2.0}. This should be highlighted - as there is not much novelty here. References
Huang H, Wang Y, Erfani S, Gu Q, Bailey J, Ma X. Exploring architectural ingredients of adversarially robust deep neural networks. Advances in Neural Information Processing Systems. 2021 Dec 6;34:5545-59.
Gowal S, Qin C, Uesato J, Mann T, Kohli P. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593. 2020 Oct 7.

Review Point: 2021 Dec 6;34:5545-59. Gowal S, Qin C, Uesato J, Mann T, Kohli P. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593.
==================================================

Focused review:

1. My major concern is the novelty. The work seems to simply combine Sparse Transformer and GAN to make predictions. The contribution is somewhat incremental but okay for an acceptance. 2. Another concern is the baselines for comparison. To the best of my knowledge, many papers based on attention mechanisms that are published in top venues (e.g., NeurIPS, SIGIR) in the recent three years shown more promising results than DeepAR, such as [1] and [2]. Moreover, there are also several attempts on introducing extra loss function to regularize the forecasting models. For example, [3] introduces a shape loss to preserve the trend of time series. I prefer more discussion (or experiments) on the difference between the proposed method and these existing arts. Reference: [1] Qin, Yao, et al. "A dual-stage attention-based recurrent neural network for time series prediction." IJCAI 2018. [2] Lai, Guokun, et al. “Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks.” SIGIR 2018. [3] Vincent, L. E., and Nicolas Thome. "Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models." NeurIPS 2019.

Review Point: 1. My major concern is the novelty. The work seems to simply combine Sparse Transformer and GAN to make predictions. The contribution is somewhat incremental but okay for an acceptance.
Review Point: 2. Another concern is the baselines for comparison. To the best of my knowledge, many papers based on attention mechanisms that are published in top venues (e.g., NeurIPS, SIGIR) in the recent three years shown more promising results than DeepAR, such as [1] and [2]. Moreover, there are also several attempts on introducing extra loss function to regularize the forecasting models. For example, [3] introduces a shape loss to preserve the trend of time series. I prefer more discussion (or experiments) on the difference between the proposed method and these existing arts. Reference: [1] Qin, Yao, et al. "A dual-stage attention-based recurrent neural network for time series prediction." IJCAI 2018. [2] Lai, Guokun, et al. “Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks.” SIGIR 2018. [3] Vincent, L. E., and Nicolas Thome. "Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models." NeurIPS 2019.
==================================================

Focused review:

- There are some technical aspects of the paper that weren't clear to me: * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
* L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer?
* It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
* Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
* Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow.
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?)

Review Point: - There are some technical aspects of the paper that weren't clear to me:
Review Point: * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
Review Point: * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer?
Review Point: * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
Review Point: * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
==================================================

Focused review:

The work is already quite extensive and more cannot be reasonably expected. Still, perhaps the authors can comment on these issues: 1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1. 2) While Manchev and Spratling 2020 claim that their difference target prop for RNNs outperforms BPTT in (simpler) 4 of 5 tasks, why does DRL which is an improvement on DTP not come closer to backprop in these tasks? 3) Would a correction different from GN targets enable to come closer to backprop? 4) In lines 177-178, the authors write 'This damping interpolates between the pseudo-inverse and the transpose of Jf ̄ i,L , so for large λ, GN targets resemble gradient targets.' I would assume that the authors would optimize over lambda as a hyperparameter, but I did not find it in the list of hyperparams in Tables S1-S4. WHy not? In any case, it would be good to explore the dependence of performance on lambda explicitly in a (supplementary) plot and how this impacts performance expecially given that for 'large λ, GN targets resemble gradient targets'.

Review Point: 1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1.
Review Point: 2) While Manchev and Spratling 2020 claim that their difference target prop for RNNs outperforms BPTT in (simpler) 4 of 5 tasks, why does DRL which is an improvement on DTP not come closer to backprop in these tasks?
Review Point: 3) Would a correction different from GN targets enable to come closer to backprop?
Review Point: 4) In lines 177-178, the authors write 'This damping interpolates between the pseudo-inverse and the transpose of Jf ̄ i,L , so for large λ, GN targets resemble gradient targets.' I would assume that the authors would optimize over lambda as a hyperparameter, but I did not find it in the list of hyperparams in Tables S1-S4. WHy not? In any case, it would be good to explore the dependence of performance on lambda explicitly in a (supplementary) plot and how this impacts performance expecially given that for 'large λ, GN targets resemble gradient targets'.
==================================================

Focused review:

Weaknesses --
-- The authors claim that their model is interpretable, but do not support this claim by any studies/analyses/qualitative examples. I agree that the ablation studies show the significance of each of the modules used in the proposed model, but it is not clear if the model is actually using the visual features from the correct module for a given question, e.g. for the question âDoes the woman look happy?â, is it really the face analysis module which provides the correct signal to accurately predict the answer? This can be studied by analysing (e.g. making a pie chart of) the questions whose predictions were incorrect when the face analysis module was absent but are correct after adding the face analysis module. Similar studies can be done for all the modules. Without any such analysis, I am not convinced if the model is actually doing what the motivation of the paper says. If the author can report any studies/analyses supporting this, I am happy to increase my rating.
-- The paper does not talk about the weaknesses of the proposed model. Discussions, ideally using failure cases, about limitations of the proposed model and what is needed to improve it are very important for continuous research in this area and should be an integral part of any paper.
-- I am confused about the mathematics of some equations. In Eq 1, how is the matrix multiplication of 2 matrices P and W resulting in a vector c? In Eq 3, what dimension is the max pooling happening over? In Eq 4, should it be transpose of r_{k} which is being premultiplied with tensor W_{k}?
-- In Table 2, the MCB numbers reported are test-dev numbers instead of test-standard. MCB only reports test-standard numbers for their ensemble model with data augmentation. Please fix.
-- For classification modules, have the authors tried using soft probability scores instead of picking top k class labels?
-- Minor:
- A lot of cited papers have been published. It might be better to cite the conference version instead of arXiv.
- r_{q} and r^{l} has been used interchangeably. At many places, subscripts and superscripts have been used interchangeably. Please fix.
- Line 9: âdoes not limitâ â âlimitsâ
After author rebuttal: I thank the authors for the additional analyses. The rebuttal addresses all my concerns. I think this paper introduces an interesting and novel approach for VQA, which would be useful to the VQA research community. Therefore, I recommend the paper for acceptance. I have changed my rating to 7.

Review Point: - A lot of cited papers have been published. It might be better to cite the conference version instead of arXiv.
Review Point: - r_{q} and r^{l} has been used interchangeably. At many places, subscripts and superscripts have been used interchangeably. Please fix.
Review Point: - Line 9: âdoes not limitâ â âlimitsâ After author rebuttal: I thank the authors for the additional analyses. The rebuttal addresses all my concerns. I think this paper introduces an interesting and novel approach for VQA, which would be useful to the VQA research community. Therefore, I recommend the paper for acceptance. I have changed my rating to 7.
==================================================

Focused review:

Weakness
--- The overall technical contribution is limited. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2].
-- The use of the sine activation function is not well supported by the experiments. In table 1, sine function shows marginal improvement over tanh, and in table 5 of the appendix, using tanh shows much better performance over sine. Furthermore, tanh tends to become saturated during backpropagation of a long sequence. I would suggest the author also compares with ReLU.
-- Presentation in the paper can be improved:
The modified Adam optimizer is listed as one of the technical contributions. However, it is not stated clearly what is the modification and how does it compare to the original Adam optimizer.
The name of the robots are presented in Fig. 7 of Sec. 4.5 but are referenced multiple times in previous sections and table 1. It would be helpful to clarify the name of the agents early on.
Reference of Fig. 5 at the end of Sec. 4.2 seems to be incorrect, as no comparison of SGD and Adam is shown in Fig. 5.
[1] Lee, Joonho, et al. "Learning quadrupedal locomotion over challenging terrain." Science robotics 5.47 (2020). [2] Zhao, Allan, et al. "RoboGrammar: graph grammar for terrain-optimized robot design." ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.

Review Point: --- The overall technical contribution is limited. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2]. -- The use of the sine activation function is not well supported by the experiments. In table 1, sine function shows marginal improvement over tanh, and in table 5 of the appendix, using tanh shows much better performance over sine. Furthermore, tanh tends to become saturated during backpropagation of a long sequence. I would suggest the author also compares with ReLU. -- Presentation in the paper can be improved: The modified Adam optimizer is listed as one of the technical contributions. However, it is not stated clearly what is the modification and how does it compare to the original Adam optimizer. The name of the robots are presented in Fig. 7 of Sec. 4.5 but are referenced multiple times in previous sections and table 1. It would be helpful to clarify the name of the agents early on. Reference of Fig. 5 at the end of Sec. 4.2 seems to be incorrect, as no comparison of SGD and Adam is shown in Fig.
Review Point: 5. [1] Lee, Joonho, et al. "Learning quadrupedal locomotion over challenging terrain." Science robotics 5.47 (2020). [2] Zhao, Allan, et al. "RoboGrammar: graph grammar for terrain-optimized robot design." ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.
==================================================

Focused review:

While there is lots of relevant work cited, the paper does not contextualize and differentiate its contribution clearly. While they reference the appendix for proofs, having a bit more intuition in the body of the paper would be helpful for some results (ex: Theorem 1.) Suggestions to improve score: - Spend more space contextualizing this work in the literature, and earlier in the paper. - Give intuition in body for *why* Theorems might be true. ================== AFTER RESPONSE ================== Thank you for the clarification on contextualization in the literature.

Review Point: - Spend more space contextualizing this work in the literature, and earlier in the paper.
Review Point: - Give intuition in body for *why* Theorems might be true. ================== AFTER RESPONSE ================== Thank you for the clarification on contextualization in the literature.
==================================================

Focused review:

Weaknesses: • The paper is missing an integration of the main algorithmic steps (Fill, Propagate, Decode) with the overarching flow diagram in Fig 1 which creates a gap in the presentation.
• The abstract and main text make inconsistent claims about the transmission capacity: o Abstract: “.. covertly transmit over 10000 real-world data samples within a carrier model which has 220× less parameters than the total size of the stolen data,” o Introduction: “… covertly transmit over 10000 real-world data samples within a carrier model which has 100× less parameters than the total size of the stolen data (§4.1),”
• Definitions of metrics and illustrations of qualitative results are insufficiently described and included. o For example, the equation for a earning objective in section 3.3 should be clearly described.
o Page 7: define performance difference and hiding capacity in equations. o Fig 3 is too small for the information to be conveyed (At the 200% digital magnification of Fig 3, I can see some differences in image qualities).
• The choices and constructions of a secret key and noisy vectors are insufficiently described i.e., Are the secret keys similar to the public-private keys used in the current cryptography applications? What are the requirements on creating the noisy vectors?
• How is the information redundancy built into the Fill, Propagate, Decode algorithms? o In reference to the sentence “ Finally, by comparing the performance of the secret model with or without fusion, we conclude that the robustness of Cans largely comes from the information redundancy implemented in our design of the weight pool”

Review Point: • The paper is missing an integration of the main algorithmic steps (Fill, Propagate, Decode) with the overarching flow diagram in Fig 1 which creates a gap in the presentation.
Review Point: • The abstract and main text make inconsistent claims about the transmission capacity: o Abstract: “.. covertly transmit over 10000 real-world data samples within a carrier model which has 220× less parameters than the total size of the stolen data,” o Introduction: “… covertly transmit over 10000 real-world data samples within a carrier model which has 100× less parameters than the total size of the stolen data (§4.1),” • Definitions of metrics and illustrations of qualitative results are insufficiently described and included. o For example, the equation for a earning objective in section 3.3 should be clearly described. o Page 7: define performance difference and hiding capacity in equations. o Fig 3 is too small for the information to be conveyed (At the 200% digital magnification of Fig 3, I can see some differences in image qualities).
Review Point: • The choices and constructions of a secret key and noisy vectors are insufficiently described i.e., Are the secret keys similar to the public-private keys used in the current cryptography applications? What are the requirements on creating the noisy vectors?
Review Point: • How is the information redundancy built into the Fill, Propagate, Decode algorithms? o In reference to the sentence “ Finally, by comparing the performance of the secret model with or without fusion, we conclude that the robustness of Cans largely comes from the information redundancy implemented in our design of the weight pool”
==================================================

Focused review:

Weaknesses:
While the experimental results look positive, I believe they are not surprising because, as far as I can tell, LoRe uses the same feature map and kernel as the core evaluation metrics (LCE, MLCE). What happens empirically when different feature maps and kernels are used for applying LoRe and evaluating with LCE, MLCE?
Further, it seems as though computing the proposed metric (LCE, MLCE) depends on 1) a specific feature map, 2) a specific method for further reducing dimensions, 3) a specific kernel function, and 4) specific settings of the hyperparameters for the kernel. It is hard to be convinced that this is a robust metric when calculation depends on so many choices, i.e. I can imagine different rankings for the methods if the evaluation metric was computed with different choices of the above.
Continuing from the above point, is there a reason why the authors chose a Laplacian kernel? Have the authors experimented with other kernels? How about different feature maps?
Choosing a suitable γ
seems quite arbitrary. Last line of Section 3.2, the authors simply state, "0.2 is a good intermediate point" without any justification. In practice, how should γ
be chosen?
How much does the LCE metric diverge from classwise-ECE? If I understand correctly, LCE is meant to measure ECE in localities over the input feature space. Classwise-ECE, in a certain way, provides a proxy of this, because ECE is measured for each true class label, and inputs features should be well clustered within each class label (assuming meaningful features). In fact, if the features were separable by class, I believe LCE should be identical to classwise-ECE with an appropriate kernel and hyperparameter.
Continuing from the point above, I believe it would have been more meaningful to 1) compare against methods that target classwise-ECE (i.e. perform recalibration by class), and 2) to additionally report classwise-ECE values in the experiments.
Is LoRe an efficient algorithm during test-time? It seems as though Equation 6 needs to be computed for each test point x, where x_i indexes all points in the recalibration dataset D
. This would be in contrast with other post-hoc methods which, e.g. learn a lookup table or tune a scalar during the recalibration phase, and can predict during test-time with a single forward pass of the classifier.
Minor comments:
What is d
in the denominator of the Laplacian kernel definition (Section 3.2)?
In Figure 2, is the vertical dashed line simply ECE? It is referred to as "average calibration error" in text in Section 3.3.
Are the experiments with ImageNet run with just 1 seed?

Review Point: 4) specific settings of the hyperparameters for the kernel. It is hard to be convinced that this is a robust metric when calculation depends on so many choices, i.e. I can imagine different rankings for the methods if the evaluation metric was computed with different choices of the above. Continuing from the above point, is there a reason why the authors chose a Laplacian kernel? Have the authors experimented with other kernels? How about different feature maps? Choosing a suitable γ seems quite arbitrary. Last line of Section 3.2, the authors simply state, "0.2 is a good intermediate point" without any justification. In practice, how should γ be chosen? How much does the LCE metric diverge from classwise-ECE? If I understand correctly, LCE is meant to measure ECE in localities over the input feature space. Classwise-ECE, in a certain way, provides a proxy of this, because ECE is measured for each true class label, and inputs features should be well clustered within each class label (assuming meaningful features). In fact, if the features were separable by class, I believe LCE should be identical to classwise-ECE with an appropriate kernel and hyperparameter. Continuing from the point above, I believe it would have been more meaningful to
Review Point: 1) compare against methods that target classwise-ECE (i.e. perform recalibration by class), and
Review Point: 2) to additionally report classwise-ECE values in the experiments. Is LoRe an efficient algorithm during test-time? It seems as though Equation 6 needs to be computed for each test point x, where x_i indexes all points in the recalibration dataset D . This would be in contrast with other post-hoc methods which, e.g. learn a lookup table or tune a scalar during the recalibration phase, and can predict during test-time with a single forward pass of the classifier. Minor comments: What is d in the denominator of the Laplacian kernel definition (Section 3.2)? In Figure 2, is the vertical dashed line simply ECE? It is referred to as "average calibration error" in text in Section 3.3. Are the experiments with ImageNet run with just 1 seed?
==================================================

Focused review:

Con * The conducted experiment as well as its discussion appear a bit early and superficial. I would have liked more extensive experimentation, in-depth analysis of the proposed approach, and discussion of the results. * Minor: Figure 2+3 do not properly show the measured metric (mean error)

Review Point: * Minor: Figure 2+3 do not properly show the measured metric (mean error)
==================================================

Focused review:

Weaknesses
W1: The setting seems to be limited and not well justified. 1) It only consider ONE truck and ONE drone. Would it be easy to extend to multiple trucks and drones? This seems to be a more interesting and practical setting. 2) What is the difference of this setting versus settings where there are multiple trucks? Are there methods solving this setting, and why are they not working in TSP-D? 3) In the second paragraph of section 2.1, the two assumptions that "we allow the drone to fly for an unlimited distance" and that, "Only customer nodes and the depot can be used to launch, recharge, and load the drone." seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.
W2: It is not clear why exactly an LSTM-decoder is better than an attention-based decoder. The paper justifies that "AM loses its strong competency in routing multiple vehicles in coordination". However, AM decoder still conditions "on the current location of the vehicle and the current state of nodes". Thus, I don't think it overlooks the interaction between different vehicles. It depends more on how you design the decoder. Compared to attention, an LSTM essentially adds to the historical decisions to the policy, not the interactions between vehicles. Therefore, it is not clear why exactly LSTM-decoder is better, and the justification is quite vague in the paper.
W3: Except for AM, NM by Nazari et al. (2018) has also been an important counterpart of the proposed HM. However, it is not compared as a baseline. Whereas I understand that not every baseline should be compared, but NM is mentioned a few times throughout. If historical information is important in decoding an action, why is it not important in encoding a state? Because of this, the empirical evaluation is not totally convincing to me.

Review Point: 1) It only consider ONE truck and ONE drone. Would it be easy to extend to multiple trucks and drones? This seems to be a more interesting and practical setting.
Review Point: 2) What is the difference of this setting versus settings where there are multiple trucks? Are there methods solving this setting, and why are they not working in TSP-D?
Review Point: 3) In the second paragraph of section 2.1, the two assumptions that "we allow the drone to fly for an unlimited distance" and that, "Only customer nodes and the depot can be used to launch, recharge, and load the drone." seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.
==================================================

Focused review:

- The proposed method is complicated, and it actually is the combination of a modified version of the masked language model and contrastive learning. So the contribution should be the application of these methods to implicit relationship learning but not a totally new framework. - Line 175, the authors say that both positive and negative image-sentence pairs are sampled. Since the image-text matching loss is applied at the same time with reconstruction loss in Stage 1, I think the authors should give a clearer explanation of how to sample single images and image pairs at the same time. - The SSRP method is complicated and contains various of losses or components for self-supervised learning. The author should provide more ablation results such as removing image-text matching loss. - In Table 4 and Table 5, the authors use different datasets or different settings compared to other methods. I am curious about what the performance is if training SSRP with the larger corpora like VL-BERT* in Table 4.

Review Point: - The proposed method is complicated, and it actually is the combination of a modified version of the masked language model and contrastive learning. So the contribution should be the application of these methods to implicit relationship learning but not a totally new framework.
Review Point: - Line 175, the authors say that both positive and negative image-sentence pairs are sampled. Since the image-text matching loss is applied at the same time with reconstruction loss in Stage 1, I think the authors should give a clearer explanation of how to sample single images and image pairs at the same time.
Review Point: - The SSRP method is complicated and contains various of losses or components for self-supervised learning. The author should provide more ablation results such as removing image-text matching loss.
Review Point: - In Table 4 and Table 5, the authors use different datasets or different settings compared to other methods. I am curious about what the performance is if training SSRP with the larger corpora like VL-BERT* in Table 4.
==================================================

Focused review:

- Weaknesses and limitations of the contributions: A ) Contribution 1: Empirical evidence of the observed convex parabolic shape is only presented for classification tasks on one dataset CIFAR-10. Since this is the main contribution of the paper, other deep learning tasks and loss functions should be explored. Other datasets should also be studied to support the claim. (The author's feedback has adequately addressed this issue.) B ) Contribution 2: The method introduces new hyperparameters (update step adaptation, measuring step size, maximal step size). The sensitivity study in Figure 14 is not convincing for the following reasons: - it's unclear to me how different combinations of these parameters would perform based on that figure. - Although the gradient is normalized, I suspect the measuring step size value will still depend on the scale of the problem and I am concerned that the proposed range in Figure 14 is only adapted to ResNet32 trained on the CIFAR-10 problem. The method is claimed to be generalizable to any step direction, but no empirical evidence is presented to back up this up. It would be interesting to see how the proposed step size procedure would perform on SGD, with or without momentum and Adam directions. (The author's feedback has addressed this issue.) B ) Contribution 3: The authors used a so-called conjugate gradient method to pick the search direction, making it difficult to access whether PAL picks good learning rates based on the figures. It would be more convincing to compare the learning rates obtained by PAL using SGD-with-momentum (or any other algorithm) directions to the optimal learning rate schedule for the same algorithm. - Validation accuracy is consistently lower than SGD across the presented problems. - No plots comparing CPU times are included in the experiments. Computing an additional evaluation of the loss function on every step requires an additional forward pass and how this effects the total run time should be presented. - Comparison with PLS is not included. (The author's feedback has adequately addressed this issue.) - Comparison with second-order optimizers is not included. B ) Contribution 4: The convergence proof is provided under strong assumptions (parabolic shape + same Q matrix for individual-loss) which are, as mentioned by the authors, not valid for general deep learning scenarios.

Review Point: - Weaknesses and limitations of the contributions: A ) Contribution 1: Empirical evidence of the observed convex parabolic shape is only presented for classification tasks on one dataset CIFAR-10. Since this is the main contribution of the paper, other deep learning tasks and loss functions should be explored. Other datasets should also be studied to support the claim. (The author's feedback has adequately addressed this issue.) B ) Contribution 2: The method introduces new hyperparameters (update step adaptation, measuring step size, maximal step size). The sensitivity study in Figure 14 is not convincing for the following reasons:
Review Point: - it's unclear to me how different combinations of these parameters would perform based on that figure.
Review Point: - Although the gradient is normalized, I suspect the measuring step size value will still depend on the scale of the problem and I am concerned that the proposed range in Figure 14 is only adapted to ResNet32 trained on the CIFAR-10 problem. The method is claimed to be generalizable to any step direction, but no empirical evidence is presented to back up this up. It would be interesting to see how the proposed step size procedure would perform on SGD, with or without momentum and Adam directions. (The author's feedback has addressed this issue.) B ) Contribution 3: The authors used a so-called conjugate gradient method to pick the search direction, making it difficult to access whether PAL picks good learning rates based on the figures. It would be more convincing to compare the learning rates obtained by PAL using SGD-with-momentum (or any other algorithm) directions to the optimal learning rate schedule for the same algorithm.
Review Point: - Validation accuracy is consistently lower than SGD across the presented problems.
Review Point: - No plots comparing CPU times are included in the experiments. Computing an additional evaluation of the loss function on every step requires an additional forward pass and how this effects the total run time should be presented.
Review Point: - Comparison with PLS is not included. (The author's feedback has adequately addressed this issue.) - Comparison with second-order optimizers is not included. B ) Contribution 4: The convergence proof is provided under strong assumptions (parabolic shape + same Q matrix for individual-loss) which are, as mentioned by the authors, not valid for general deep learning scenarios.
==================================================

Focused review:

-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here. - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved.
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.

Review Point: -The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
Review Point: - The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here.
Review Point: - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance. -The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
Review Point: - The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.
==================================================

Focused review:

weakness of the experiments. Originality: 5/10 This paper seems mostly to be about transferring the more general result of [34] to the specific setting of constrained MDPs. So I wish the authors gave more attention to [34], specifically: - reviewing the contribution of [34] in more detail - clarifying the novelty of this work (Is it in the specific design choices? The actor-critic algorithm? The set-up in lines 532-542 (Appendix)? Is it just in noticing the suitability of [34]'s results for CMDPs?) Without this help from the authors, it's difficult for me assess the originality and significance of their work. At the moment, it looks to me like a pretty straightforward application of [34]s results to CMDPs. Quality: 4/10 Overall the paper seems technically sound and well-done. But the experiments seem like an after-thought and/or purely illustrative, since: 1) they aren't included in the main paper and 2) they don't include multiple trials for significance. I also didn't find the content in section 5 particularly valuable; I think experiments would have been better. I'm also not familiar with the setting (LQR), or previous work using RL for LQR (with or without constraints), and the authors' summary didn't give me enough context to interpret the strength or meaning of their results. The abstract and introduction also over-claim, stating that they "apply" the algorithm to multi-agent RL; but there are no experiments for that setting, only description of how it could be applied. Clarity: 7/10 The paper is mostly quite clearly written, with a few typos. The following should be clarified: - the novelty compared with [34] - the significance of the experimental results - the main baseline being methods with Lagrange-multipliers - the motivations from safety: i.e. this doesn't tackle safe exploration, right? It's for safe execution? How exactly would this kind of method be used to make a safe real-world system? What problems does(n't) it solve? For instance, is it possible to verify that constraints will be satisfied? In practice, do CMDP approaches out-perform incorporating constraints as large negative rewards (when)? Also, *some* intuition for the proof should be given. This should include both the intuition underlying the proof strategy from [34], and an explanation highlighting the differences and the explaining what the issues are the require modifications to their proof. Significance: 7/10 The introduction presents a strong case that (in some theoretical aspects) this work represents a significant step up from the standard approach to CMDPs, based on Lagrangian multipliers. A table summarizing different approaches to CMDPs and highlighting the advantages of this work over previous approaches might be a nice addition. However, without stronger experiments, I don't think the paper presents a strong case that this method will be superior to existing methods in practice. Some detailed suggestions: - Line 85: "minimize (1)" --> "maximize (1)" - Line 109-110 and line 83 are redundant - Algorithm 2: beta_w/v are not defined; initial values of delta^J/D are not defined - line 201: "and their are" --> "and their derivatives are" (I think?) - line 261 (right hand side): PF_k --> P_{F_k} - I suggest renaming Q1, R1, Q2, R2 as Q_J, R_J, Q_D, R_D Some Questions: - Appendix: what's the difference between figure 1a/b and figure 2a/b? - Line 152-153: why can a solution be written in closed form? (explain or reference). - Line 186: how/why does actor-critic "improve the performance further"? And is the improvement in terms of performance, rate of convergence, both, neither? And how is the claim (of improved performance) supported? - is finding a stationary point good enough? why can't we do better? (e.g. local minima?) - What is the state of RL research on LQR? How does it compare with other approaches? Is studying RL algorithms in LQR interesting practically, scientifically, or both? Some high level questions: - what are the key contributions of this work compared to [34]? - what are the key challenges for the proof? - what is the point of section 5? [34] An Liu, Vincent Lau, and Borna Kananian. Stochastic successive convex approximation for non-convex constrained stochastic optimization. arXiv preprint arXiv:1801.08266, 2018

Review Point: - reviewing the contribution of [34] in more detail - clarifying the novelty of this work (Is it in the specific design choices? The actor-critic algorithm? The set-up in lines 532-542 (Appendix)? Is it just in noticing the suitability of [34]'s results for CMDPs?) Without this help from the authors, it's difficult for me assess the originality and significance of their work. At the moment, it looks to me like a pretty straightforward application of [34]s results to CMDPs. Quality: 4/10 Overall the paper seems technically sound and well-done. But the experiments seem like an after-thought and/or purely illustrative, since:
Review Point: - the novelty compared with [34] - the significance of the experimental results - the main baseline being methods with Lagrange-multipliers - the motivations from safety: i.e. this doesn't tackle safe exploration, right? It's for safe execution? How exactly would this kind of method be used to make a safe real-world system? What problems does(n't) it solve? For instance, is it possible to verify that constraints will be satisfied? In practice, do CMDP approaches out-perform incorporating constraints as large negative rewards (when)? Also, *some* intuition for the proof should be given. This should include both the intuition underlying the proof strategy from [34], and an explanation highlighting the differences and the explaining what the issues are the require modifications to their proof. Significance: 7/10 The introduction presents a strong case that (in some theoretical aspects) this work represents a significant step up from the standard approach to CMDPs, based on Lagrangian multipliers. A table summarizing different approaches to CMDPs and highlighting the advantages of this work over previous approaches might be a nice addition. However, without stronger experiments, I don't think the paper presents a strong case that this method will be superior to existing methods in practice. Some detailed suggestions:
Review Point: - Appendix: what's the difference between figure 1a/b and figure 2a/b?
Review Point: - Line 152-153: why can a solution be written in closed form? (explain or reference).
Review Point: - Line 186: how/why does actor-critic "improve the performance further"? And is the improvement in terms of performance, rate of convergence, both, neither? And how is the claim (of improved performance) supported?
Review Point: - is finding a stationary point good enough? why can't we do better? (e.g. local minima?) - What is the state of RL research on LQR? How does it compare with other approaches? Is studying RL algorithms in LQR interesting practically, scientifically, or both? Some high level questions:
Review Point: - what are the key contributions of this work compared to [34]?
Review Point: - what is the point of section 5? [34] An Liu, Vincent Lau, and Borna Kananian. Stochastic successive convex approximation for non-convex constrained stochastic optimization. arXiv preprint arXiv:1801.08266, 2018
==================================================

Focused review:

Weaknesses: 1. Providing more comparisons with the existed CDTB will be better.
2. The “primary-secondary” relationship is mentioned a lot in this paper, however, its difference with the nuclearity is unclear and not precisely defined.
3. The experiment method is not clearly described in the paper.
- General Discussion:

Review Point: 1. Providing more comparisons with the existed CDTB will be better.
Review Point: 2. The “primary-secondary” relationship is mentioned a lot in this paper, however, its difference with the nuclearity is unclear and not precisely defined.
Review Point: 3. The experiment method is not clearly described in the paper.
==================================================

Focused review:

The two weaknesses that I can see are: * Insufficient comparison with the literature. In particular, novelty with respect to existing work is not sufficiently discussed. To my knowledge, the reference Cho et al. proposes a similar algorithm, though not in the consensus setting. More generally, it is not clear to me how novel the proposed algorithm is, and in light of the existing literature, the contributions can possibly be marginal. * It is not clear why the authors chose to limit themselves to rank-1 approximations. I understand that the convergence results are general enough to cover the rank-k scenario, but it would have been nice to see the empirical effect of the approximation rank too. In many settings, one may not need to compress at such an extreme rate, and the additional computational and communication cost of, say a rank-10 approximation, might well be worth the improved convergence rate.

Review Point: * Insufficient comparison with the literature. In particular, novelty with respect to existing work is not sufficiently discussed. To my knowledge, the reference Cho et al. proposes a similar algorithm, though not in the consensus setting. More generally, it is not clear to me how novel the proposed algorithm is, and in light of the existing literature, the contributions can possibly be marginal.
Review Point: * It is not clear why the authors chose to limit themselves to rank-1 approximations. I understand that the convergence results are general enough to cover the rank-k scenario, but it would have been nice to see the empirical effect of the approximation rank too. In many settings, one may not need to compress at such an extreme rate, and the additional computational and communication cost of, say a rank-10 approximation, might well be worth the improved convergence rate.
==================================================

Focused review:

- Motivation and insight is unclear. 1) As the title of this paper says, the main contribution may be introducing routing uncertainty in capsule networks with VI. But VB-routing [1] also studied uncertainty in their method with VI. Please explain the connection and difference with VB-routing. 2) Section 4 ‘Related Works’ is insufficient. There are many works which focus on the speeding up the routing procedure in capsule networks, such as [2]. Please discuss more detailly about the different motivations/understandings/perspectives of these methods, and provide the main insights of your method for this field. - The paper is hard to understand. 1) There many equations in this paper. I think an Algorithm cell will help readers understand the overall algorithm, as in [1] and [3]. 2) The tables and figures are poorly interpreted. For example, in Table 1, what’s the difference between ‘EM-Routing’, ‘Our EM-Routing’ and ‘{32, 8, 8, 8, 5}’. Groups of figures in Figure 3 and Figure 4 also need to be explained separately. 3) In Eq. (12), the subscripts need to be better organized. \sum_{i \in l} z_{l, l+1}, I think this summation is over multiple ‘i’s in the ‘l’, but there is no ‘i’ subscript in ‘z’ term. This summation may be confusing. 4) In L169, does the ‘N(M | \mu, \sigma)’ mean the posterior likelihood of M, if so, it is a scalar, how do you compute the entropy. - Ablation analysis. There are many components, such as different VIs, different priors. I think an ablation analysis would be better. [1] Capsule Routing via Variational Bayes. AAAI 2020. [2] Improving the Robustness of Capsule Networks to Image Affine Transformations. CVPR 2020. [3] Matrix Capsules with EM Routing. ICLR 2018.

Review Point: 1) As the title of this paper says, the main contribution may be introducing routing uncertainty in capsule networks with VI. But VB-routing [1] also studied uncertainty in their method with VI. Please explain the connection and difference with VB-routing.
Review Point: 2) Section 4 ‘Related Works’ is insufficient. There are many works which focus on the speeding up the routing procedure in capsule networks, such as [2]. Please discuss more detailly about the different motivations/understandings/perspectives of these methods, and provide the main insights of your method for this field.
Review Point: 1) There many equations in this paper. I think an Algorithm cell will help readers understand the overall algorithm, as in [1] and [3].
Review Point: 2) The tables and figures are poorly interpreted. For example, in Table 1, what’s the difference between ‘EM-Routing’, ‘Our EM-Routing’ and ‘{32, 8, 8, 8, 5}’. Groups of figures in Figure 3 and Figure 4 also need to be explained separately.
Review Point: 3) In Eq. (12), the subscripts need to be better organized. \sum_{i \in l} z_{l, l+1}, I think this summation is over multiple ‘i’s in the ‘l’, but there is no ‘i’ subscript in ‘z’ term. This summation may be confusing.
Review Point: 4) In L169, does the ‘N(M | \mu, \sigma)’ mean the posterior likelihood of M, if so, it is a scalar, how do you compute the entropy.
Review Point: - Ablation analysis. There are many components, such as different VIs, different priors. I think an ablation analysis would be better. [1] Capsule Routing via Variational Bayes. AAAI 2020. [2] Improving the Robustness of Capsule Networks to Image Affine Transformations. CVPR 2020. [3] Matrix Capsules with EM Routing. ICLR 2018.
==================================================

Focused review:

Weaknesses:
Algorithm design. The presented algorithm 1 has to collect data episode by episode. This turns the algorithm into an offline algorithm, restricting its utility. But it seems the sampling method can be done in an online manner, why not propose that? This is important because 1) an online method is a closer competitor to PER which runs online (update parameters at each environment time step) and 2) it is clearer how the two algorithms are compared.
Figure 3 is not persuasive. The TD errors can change as the parameters get updated. I do not see a clear correlation between reward magnitude and TD error magnitude. Also, if this correlation is true and is beneficial, shouldn't PER perform very well in the sparse reward setting?
Concerns about experiments. The empirical results are extensive but not persuasive.
Many figures (Fig 4-6) in the experiments section include learning curves with very large variances/standard errors, where one cannot really identify the proposed method to be better than others. Furthermore, it is better to study the hyper-parameter sensitivity of the proposed algorithm. The algorithm seems to have a large reliance on the size of the ER buffer.
Missing at least two intuitive baselines to make the proposed method more persuasive: 1). Uniformly sample the pivot points, and the rest is the same as the proposed method: this can verify the usefulness of the claimed "reverse replay." 2). Prioritized sampling of the pivot points and then uniform sampling of the rest of datapoints in each batch: this can further validate the reverse sampling is important
One critical question about the experiments. The PER/UER can update parameters at each time step, while RER++ needs to wait until the end of an episode. How do you conduct the comparison? Do you use the same amount of real environment data or the same computation power for all algorithms?
Missing details of the PER. PER has a mechanism to anneal the sampling bias. Since I see the proposed method used a "mixed replay" method to mitigate bias, it is important to report if there any effort (tuning the hyper-parameter) of the PER baseline has been made to anneal the bias.
Missing related work. The paper belongs to the broad subarea of the sampling distribution of experiences, and there are many more papers in this category that should be discussed. I name a few highly relevant works here: [1] An equivalence between loss functions and non-uniform sampling in experience replay by Scott Fujimoto et al. [2] Remember and forget for experience replay by Guido Novati et al. [3] Understanding and mitigating the limitations of PER by Yangchen Pan et al. [4] regret minimization ER in off-policy RL et al.
All these papers discuss the pros and cons of PER/ER methods, and some of them shed light on the theoretical mechanisms behind why a sampling method should be beneficial or what a good sampling distribution should be.
Presentation issue (I consider this not critical, but it can be significantly improved). The proposed sampling approach is not well-motivated. In the abstract, it says PER and UER may suffer from large bias and sub-optimal convergence, respectively. However, there is no evidence in the paper showing the proposed IER method is optimal or has a small bias. In fact, PER does address the biased sampling issue by using an important ratio, as introduced in the original paper (section 3.4). In contrast, the proposed method does not even have a sound method to anneal the sampling bias. Furthermore, the paper attempts to use RER to motivate their approach too. However, the RER theory (from a system identification setting or linear MDP setting) cited by the authors does not really apply to general RL settings. I do not mean the authors need to provide a new/strong theory to motivate their method. Still, it would be a plus if the authors specify which theorems from existing work motivate their algorithmic design.

Review Point: 1) an online method is a closer competitor to PER which runs online (update parameters at each environment time step) and
Review Point: 2) it is clearer how the two algorithms are compared. Figure 3 is not persuasive. The TD errors can change as the parameters get updated. I do not see a clear correlation between reward magnitude and TD error magnitude. Also, if this correlation is true and is beneficial, shouldn't PER perform very well in the sparse reward setting? Concerns about experiments. The empirical results are extensive but not persuasive. Many figures (Fig 4-6) in the experiments section include learning curves with very large variances/standard errors, where one cannot really identify the proposed method to be better than others. Furthermore, it is better to study the hyper-parameter sensitivity of the proposed algorithm. The algorithm seems to have a large reliance on the size of the ER buffer. Missing at least two intuitive baselines to make the proposed method more persuasive:
Review Point: 1). Uniformly sample the pivot points, and the rest is the same as the proposed method: this can verify the usefulness of the claimed "reverse replay."
==================================================

Focused review:

Weaknesses: In general, to me, the experiments are a bit weak/limited. Please find the detailed comments as follows.
• Experiments are only conducted to compare the proposed Batch BORE method with existing baselines, and very limited experiments are conducted to compare the performance of BORE++ over baselines (e.g., BORE or GP-UCB).
• For the experiments that compare the proposed Batch BORE method with existing baselines, only synthetic objective functions are used, there are no real-world problems.

Review Point: • Experiments are only conducted to compare the proposed Batch BORE method with existing baselines, and very limited experiments are conducted to compare the performance of BORE++ over baselines (e.g., BORE or GP-UCB).
Review Point: • For the experiments that compare the proposed Batch BORE method with existing baselines, only synthetic objective functions are used, there are no real-world problems.
==================================================

Focused review:

Weaknesses:
Experimental setup could be made clearer as to which tasks are used for training and eval in the transfer setting (i.e., which tasks are used to originally train the meta-extractor, and on which tasks is it transferred). Minor:
p. 3: In the fourth line from the bottom, the reference "2020" needs to be fixed.

Review Point: 3: In the fourth line from the bottom, the reference "2020" needs to be fixed.
==================================================

Focused review:

- The novelty seems limited. The idea of building correlation in low-res and then refine for or facilitate the high-res results might be new in the literature for correspondence search, but it is quite common and has been widely adopted in previous work doing stereo match, where the main job is also to find correspondence but along epipolar line. - The main contribution of this paper, IMO, is to run 4D convolution on low-res correlation volume, which saves computation and possibly achieve comparable performance. If so, the experiment showing the saving of computational resources, e.g. gpu runtime, flip-flop, memory, must be given. - Similar multi-scale approach in stereo matching often runs fast at the cost of losing accuracy, since the correlation volume in low-res is not as informative as the high-res one, and it is not easy to fix if some mistakes are made in low-res. However the experiments show that the result is even better than SOTA. It would be good to add more explanation and analysis. - It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.

Review Point: - The novelty seems limited. The idea of building correlation in low-res and then refine for or facilitate the high-res results might be new in the literature for correspondence search, but it is quite common and has been widely adopted in previous work doing stereo match, where the main job is also to find correspondence but along epipolar line.
Review Point: - The main contribution of this paper, IMO, is to run 4D convolution on low-res correlation volume, which saves computation and possibly achieve comparable performance. If so, the experiment showing the saving of computational resources, e.g. gpu runtime, flip-flop, memory, must be given.
Review Point: - Similar multi-scale approach in stereo matching often runs fast at the cost of losing accuracy, since the correlation volume in low-res is not as informative as the high-res one, and it is not easy to fix if some mistakes are made in low-res. However the experiments show that the result is even better than SOTA. It would be good to add more explanation and analysis.
Review Point: - It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.
==================================================

Focused review:

There are several experiments that, if added, would substantially improve the paper. I would be willing to improve my score if some or all of the following were added: - Regression experiments. The authors only evaluate classification experiments. To claim broad improvements over MAML, the authors should also investigate their approach on a broad set of experiments as in the MAML paper. While the RL experiments are not necessary, even basic regression experiments would give further insight into the approach. - The authors should investigate 20-way classification, not just 5-way. - An ablation of performance providing only weights or only gradients to the hyperparameter selection network. To summarize, to convince a reader that the presented approach is a broadly applicable tool to improve MAML, the authors should investigate the method on a wide variety of problems. Finally, section 4.4.2 claims that there is a consistent performance improvement throughout the steps, whereas that does not appear to be a statistically significant result.

Review Point: - Regression experiments. The authors only evaluate classification experiments. To claim broad improvements over MAML, the authors should also investigate their approach on a broad set of experiments as in the MAML paper. While the RL experiments are not necessary, even basic regression experiments would give further insight into the approach.
Review Point: - The authors should investigate 20-way classification, not just 5-way.
Review Point: - An ablation of performance providing only weights or only gradients to the hyperparameter selection network. To summarize, to convince a reader that the presented approach is a broadly applicable tool to improve MAML, the authors should investigate the method on a wide variety of problems. Finally, section 4.4.2 claims that there is a consistent performance improvement throughout the steps, whereas that does not appear to be a statistically significant result.
==================================================

Focused review:

weaknesses of the paper are that it claims universality and optimality fairly strongly in the beginning, but requires upper bounds on the gradient and the domain size up front. While this is not necessarily a major detriment the writing should make this abundantly clear in the beginning. Also, the experiments of the paper could be strengthened. Why the particular experimental settings are chosen and what they really say about the practicality of AcceleGrad could be improved. Furthermore, it isnât clear there is something particular novel in the analysis or how well known the open problem being addressed is. Nevertheless, this paper provides a very interesting algorithm that advances the state of the art for convex optimization and unifies the analysis of a number of techniques in a clever way. Consequently, it could make for a nice addition to the program. Specific comments: - 65: âare exploreâ --> âare exploredâ - 78: âwe assume to be givenâ --> âwe assume we are givenâ - 83: âNote that we allow to choose point outside Kâ â I would clarify this further. - 83: âWe also assume that the objective function I Gâ â throughout the paper or just here, I would be clear. - Figure 1 â what is rho and epsilon in the charts, would make this clearer. EDIT: Thank you for your thoughtful response. Thank you for clarifying and for agreeing to make certain changes. However, I believe novelty in analysis requires more than being the first to analyze a particular algorithm, it requires an argument of why previous analysis techniques might be stuck, a simple approach might fail, etc. I still think this paper could be a nice addition to the program, but a more concrete case would be need to raise my score much more.

Review Point: - 65: âare exploreâ --> âare exploredâ - 78: âwe assume to be givenâ --> âwe assume we are givenâ - 83: âNote that we allow to choose point outside Kâ â I would clarify this further.
Review Point: - 83: âWe also assume that the objective function I Gâ â throughout the paper or just here, I would be clear.
Review Point: - Figure 1 â what is rho and epsilon in the charts, would make this clearer. EDIT: Thank you for your thoughtful response. Thank you for clarifying and for agreeing to make certain changes. However, I believe novelty in analysis requires more than being the first to analyze a particular algorithm, it requires an argument of why previous analysis techniques might be stuck, a simple approach might fail, etc. I still think this paper could be a nice addition to the program, but a more concrete case would be need to raise my score much more.
==================================================

Focused review:

Weakness:
Even with the sophisticated design, the model performance still lacks behind a non ad-hoc transformer model (Transformer model in Table 2). While the added inductive-bias does add more interpretability, this seems to suggest that training universal large LM is still the best performant choice, as demonstrated by various big LM papers.
The proposed phrasal alignment only works when the dataset are well curated and don't use synonyms, I wonder how much it will be applicable in the wild. Specifically, 1) How well does sentence BERT produced embeddings deal with synonyms and different phrasing of the same content? Is there any analysis on this. 2) Real world NL texts are not as concise as in SNLI, there can be added redudant adjectives, disfluencies, reversed sentence structure. I wonder if alignment will fail for some of these cases.

Review Point: 1) How well does sentence BERT produced embeddings deal with synonyms and different phrasing of the same content? Is there any analysis on this.
Review Point: 2) Real world NL texts are not as concise as in SNLI, there can be added redudant adjectives, disfluencies, reversed sentence structure. I wonder if alignment will fail for some of these cases.
==================================================

Focused review:

Besides the concerns about the assumptions (discussed in Strengths), my other major comment is related to experiments. 1. It seems that the proposed approach was not compared against any existing imputation method. Without doing so, it is very difficult to see the real value of the work. 2. In Section 6.1, the authors "artificially delete intervals completely at random with probability 0.2.". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice. 3. I find the results in Section 6.2 a bit weak. Do the results echo findings reported in the literature? Such comparisons could be useful.

Review Point: 1. It seems that the proposed approach was not compared against any existing imputation method. Without doing so, it is very difficult to see the real value of the work.
Review Point: 2. In Section 6.1, the authors "artificially delete intervals completely at random with probability 0.2.". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice.
Review Point: 3. I find the results in Section 6.2 a bit weak. Do the results echo findings reported in the literature? Such comparisons could be useful.
==================================================

Focused review:

weaknesses: 1. The paper in general does not read well, and more careful proofreading is needed. 2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.

Review Point: 1. The paper in general does not read well, and more careful proofreading is needed.
Review Point: 2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.
==================================================

Focused review:

weakness section.
- Weaknesses: Regarding writing =============== No doubt the paper is well-written. But the major issue with the paper is its lucidness. Indeed, poetic language, elegance is applaud-able, but clarity in scientific writing is very much needed.
I hope you will agree with most of the stuff being articulated here: https://chairs-blog.acl2017.org/2017/02/02/last-minute-writing-advice/ Let me put my objections on writing here: - "while providing a method which is effectively zero-shot"..left readers in the blank. The notion of zero-shot has not been introduced yet!
- Figure 2: most, neutral, least - metaphoric. How did you arrive at such differentiations?
- Talk more about data. Otherwise, the method is less intuitive.
- I enjoyed reading the analysis section. But it is not clear why the proposed simple (as claimed) method can over-perform than other existing techniques?
Putting some examples would be better, I believe.
Technicality ============ "A strength of this model is its simplicity" - indeed, but the implication is not vivid from the writing. Mathematical and technical definition of a problem is one aspect, but the implication from the definition is quite hard to be understood. When that's the note-able contribution of the paper. Comparing to previous research this paper shows only marginal accuracy gain.
- Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.
- General Discussion:

Review Point: - "while providing a method which is effectively zero-shot"..left readers in the blank. The notion of zero-shot has not been introduced yet!
Review Point: - Figure 2: most, neutral, least - metaphoric. How did you arrive at such differentiations?
Review Point: - Talk more about data. Otherwise, the method is less intuitive.
Review Point: - I enjoyed reading the analysis section. But it is not clear why the proposed simple (as claimed) method can over-perform than other existing techniques? Putting some examples would be better, I believe. Technicality ============ "A strength of this model is its simplicity" - indeed, but the implication is not vivid from the writing. Mathematical and technical definition of a problem is one aspect, but the implication from the definition is quite hard to be understood. When that's the note-able contribution of the paper. Comparing to previous research this paper shows only marginal accuracy gain.
Review Point: - Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.
==================================================

Focused review:

- My main concern is that, I don’t see the benefits of modeling the data as a union of subspaces, where each subspace corresponds to a class, when the representation space is *learned*. In particular, since these subspaces won’t be orthogonal in practice, on real data. In an unsupervised setting, to recover the subspaces, one needs to perform subspace clustering, which is a hard problem and computationally expensive to perform. In a supervised setting, where estimation of the subspaces is easy, one needs to do nearest-subspace-classification which is more intricate than linear classification. In stark contrast, a linear head trained with a cross-entropy loss learns a representation space with approximately linearly separable regions for each class. As a consequence, classification is simple (linear) and Lp distances in representation space are meaningful (which is not necessarily the case when the classes lie on a union of subspaces). - I acknowledge the encouraging results regarding robustness of the representations learned with the proposed method. However, there are many other methods which can make neural networks with linear classification head more robust, for example [c]. Therefore I believe a union of subspace structure is not fundamentally required to achieve this. - While the theoretical analysis reveals interesting properties of the learned representation, it completely ignores the relationship between the individual data points and their representation, defined through the feature extractor. It is well-known that the structure and properties of the extractor crucially impact the learned representation, possibly even more than the loss, see e.g. [ZF14]. [c] Elsayed, Gamaleldin, et al. "Large margin deep networks for classification." Advances in neural information processing systems. 2018. --- Update after rebuttal: Thanks to the authors for their response. I now better see the benefits of encouraging orthogonality between class regions in the feature space, which is why I increased my rating. However, I'm still not sure whether the theoretical result is useful to explain what is going on, as I still believe the network architecture is crucial for the structure in the feature space. Furthermore, as pointed out by the other reviewers, the method seems to have many similarities with previous methods, which should be discussed more precisely.

Review Point: - My main concern is that, I don’t see the benefits of modeling the data as a union of subspaces, where each subspace corresponds to a class, when the representation space is *learned*. In particular, since these subspaces won’t be orthogonal in practice, on real data. In an unsupervised setting, to recover the subspaces, one needs to perform subspace clustering, which is a hard problem and computationally expensive to perform. In a supervised setting, where estimation of the subspaces is easy, one needs to do nearest-subspace-classification which is more intricate than linear classification. In stark contrast, a linear head trained with a cross-entropy loss learns a representation space with approximately linearly separable regions for each class. As a consequence, classification is simple (linear) and Lp distances in representation space are meaningful (which is not necessarily the case when the classes lie on a union of subspaces).
Review Point: - I acknowledge the encouraging results regarding robustness of the representations learned with the proposed method. However, there are many other methods which can make neural networks with linear classification head more robust, for example [c]. Therefore I believe a union of subspace structure is not fundamentally required to achieve this.
Review Point: - While the theoretical analysis reveals interesting properties of the learned representation, it completely ignores the relationship between the individual data points and their representation, defined through the feature extractor. It is well-known that the structure and properties of the extractor crucially impact the learned representation, possibly even more than the loss, see e.g. [ZF14]. [c] Elsayed, Gamaleldin, et al. "Large margin deep networks for classification." Advances in neural information processing systems. 2018. --- Update after rebuttal: Thanks to the authors for their response. I now better see the benefits of encouraging orthogonality between class regions in the feature space, which is why I increased my rating. However, I'm still not sure whether the theoretical result is useful to explain what is going on, as I still believe the network architecture is crucial for the structure in the feature space. Furthermore, as pointed out by the other reviewers, the method seems to have many similarities with previous methods, which should be discussed more precisely.
==================================================

Focused review:

Weaknesses: - Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether "mention" means a string type, or a particular mention in a particular document. The phrase "mention embedding" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion:

Review Point: - Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task.
Review Point: - It is sometimes difficult to follow whether "mention" means a string type, or a particular mention in a particular document. The phrase "mention embedding" is used, but it appears that embeddings are only learned for mention senses.
Review Point: - It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.
==================================================

Focused review:

Weakness:
First, a theoretical analysis of the proposed approach is lacking. For example, can the proposed approach identify and consistently estimate the conditional average treatment effect mu(x, t) = E[Y X = x, T = t]?
Second, the proposed method ruled out unobserved confounders (in Assumption a) and Figure 1). If so, why do the authors propose to balance latent representation Z, but not observed covariates X? More justification will be helpful.

Review Point: 1). If so, why do the authors propose to balance latent representation Z, but not observed covariates X? More justification will be helpful.
==================================================

Focused review:

Weakness 1. I find the paper hard to read, and lots of its parts seem to be wordy and lack clarity. I suggest the authors thoroughly reorganize the lines of writing, especially for the introduction and method section. 2. As suggested in the introduction part, the novelty of this works partly comes from the global feature of datasets instead of a single instance, which is not new in the literature, e.g., [1]. 3. Regardless of the variety of models in the experiments, more competitive baselines are necessary to justify the superiority of MAGNEX.
4. I think more clarifications for the experimental conclusions are needed. For example, why would the sparsity values be the same for all the methods? Plus, it would be nice to provide a more in-depth study about the performance gain of MAGNEX.
[1] Parameterized Explainer for Graph Neural Network. Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang.

Review Point: 1. I find the paper hard to read, and lots of its parts seem to be wordy and lack clarity. I suggest the authors thoroughly reorganize the lines of writing, especially for the introduction and method section.
Review Point: 2. As suggested in the introduction part, the novelty of this works partly comes from the global feature of datasets instead of a single instance, which is not new in the literature, e.g., [1].
Review Point: 3. Regardless of the variety of models in the experiments, more competitive baselines are necessary to justify the superiority of MAGNEX.
Review Point: 4. I think more clarifications for the experimental conclusions are needed. For example, why would the sparsity values be the same for all the methods? Plus, it would be nice to provide a more in-depth study about the performance gain of MAGNEX. [1] Parameterized Explainer for Graph Neural Network. Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang.
==================================================

Focused review:

Weaknesses: 1. The empirical results presented in support to the first “contribution”, disentangling spacial and channel pooling, are not fully convincing. Fig 3 shows the changes in the sensitivity to shuffling the channel connections. The scale is linear in the left panel (AlexNet and LeNet) and log in the right panel (VGG) creating some confusion. The change is claimed to be small in the first layers, but this change looks approximately 30 % also for very deep layers in VGG. Only in LeNet the change is clearly small (~ 5 %). Is 30 % small or large? Maybe a baseline should be defined. Moreover, the qualitative differences between the different architectures are not explained. Do they derive from the depth? 2. Fig. 4 illustrates the frequency content of the filters of the layers of a CNN at different depth. I do not understand why the curves do not all start from 1. Moreover, I do not understand what we learn from the time dependence of these coefficient. The main message conveyed by this figure should be that deep layers (1-5) “become low-frequency” (while, I assume, late layers become high frequency). A better observable would possibly be the average frequency of the filter\ sum_{kl} gamma_kl omega. I also have a problem with the interpretation of the frequency content as defined by eq. 3. The receptive field becomes larger and large with depth. Therefore, the same frequency in different layers corresponds to different physical lengths in the input image. Shouldn’t the frequency be mapped in common units across the layers? I also do not fully understand why the observation that early kernels are "low-frequency" supports the claim that channel pooling happens late in the network.

Review Point: 1. The empirical results presented in support to the first “contribution”, disentangling spacial and channel pooling, are not fully convincing. Fig 3 shows the changes in the sensitivity to shuffling the channel connections. The scale is linear in the left panel (AlexNet and LeNet) and log in the right panel (VGG) creating some confusion. The change is claimed to be small in the first layers, but this change looks approximately 30 % also for very deep layers in VGG. Only in LeNet the change is clearly small (~ 5 %). Is 30 % small or large? Maybe a baseline should be defined. Moreover, the qualitative differences between the different architectures are not explained. Do they derive from the depth?
Review Point: 2. Fig. 4 illustrates the frequency content of the filters of the layers of a CNN at different depth. I do not understand why the curves do not all start from 1. Moreover, I do not understand what we learn from the time dependence of these coefficient. The main message conveyed by this figure should be that deep layers (1-5) “become low-frequency” (while, I assume, late layers become high frequency). A better observable would possibly be the average frequency of the filter\ sum_{kl} gamma_kl omega. I also have a problem with the interpretation of the frequency content as defined by eq.
Review Point: 3. The receptive field becomes larger and large with depth. Therefore, the same frequency in different layers corresponds to different physical lengths in the input image. Shouldn’t the frequency be mapped in common units across the layers? I also do not fully understand why the observation that early kernels are "low-frequency" supports the claim that channel pooling happens late in the network.
==================================================

Focused review:

The checklist claims that this work discusses its limitations, but the referenced limitation is “mechanisms in the RNNs should be interpreted as a hypothesis for biological mechanisms”. I wouldn’t consider this a discussion of limitations of this work. I’m not even sure how it’s a limitation at all, but if the idea is that RNNs aren’t a perfect model for biological intelligence and are primarily theorized as one, then that’s a critique of the whole field (in fact, one that this paper is trying to rectify). The one limitation that this work does discuss in the main text is that experiments should extend to more complex tasks, but this is given half a sentence and is posed as a future work. I totally agree and think the paper is complete enough to leave that for future work, but that’s not a sufficient discussion of limitations as per the NeurIPS checklist guidelines. I think the paper should discuss the limited space of tasks (even in the range of complexity of RDM and (N-)CDI) and learning rules, or biological implausibility of some of the learning rules. It’s certainly possible that I misinterpreted the note in the checklist, but I don’t think this paper really discusses the limitations. I also don’t think it would be hurt by a discussion - it’s a good paper that does enough to survive an honest discussion of limitations.
I agree that this paper is theoretical enough to not need a discussion of societal impacts. Since the work “involves understanding deep neural networks”, I’m not sure that it “inherits the broader positive and negative societal impacts of deep learning”. It does very indirectly (which is what I think the authors are saying), but (also indirectly, but less so) it inherits more of the impacts of neural network interpretability than neural network application. One thing that isn’t touched is the societal impact of having good models of biological intelligence, which are huge and rarely discussed. I don’t think this warrants a flag for ethical review, since 1) there is so much ground between the state of the field and a societally impactful model and 2) I rarely see such discussion in comp neuro papers, but I do believe it warrants consideration if it is the ultimate goal.

Review Point: 1) there is so much ground between the state of the field and a societally impactful model and
Review Point: 2) I rarely see such discussion in comp neuro papers, but I do believe it warrants consideration if it is the ultimate goal.
==================================================

Focused review:

weaknesses compared to [1], which are advantages of the IBP. 1) Unlike a factorized model with an IBP prior, the proposed method lacks a sparsity constraint in the number of factors used by subsequent tasks. As such, the model will not be incentivized to use less factors, leading to increasing number of factors and increased computation with more tasks. 2) The IBP prior allows the data to dictate the number of factors to add for each task. The proposed method has no such mechanism, requiring setting the growth rate by hand using heuristics or a pre-determined schedule. Either is liable to over- or under-utilization of model capacity. Table 4 in the Experiments show that this does indeed have a significant impact on performance.
Overall, I think this is an example of convergent ideas rather than plagiarism, but a discussion of the connections is warranted.
Task incremental learning: This method requires knowing the task ID at test time to pick which factor selector weights to use. Without it, the proposed method doesn’t know which subnetwork to use, and would likely have to resort to trying all of them, which isn’t guaranteed to produce the right results. Recent continual learning methods are often evaluated in the more challenging class incremental setting, where task ID is not known.
Experiments 1. (+) Experiments are conducted on a good set of datasets 2. (+) Error bars are shown 3. (+) The proposed method mostly outperforms the baselines, especially on the more complex datasets. 4. (-) More baselines should be compared against, particularly dynamic architecture approaches, as that’s the category that this method falls under. Many of the compared methods don’t operate on the same set of continual learning assumptions as this paper; in particular, the replay-based methods are often using replay because they consider class incremental learning. 5. (-) Why are the results of Multitask learning so bad for S-CIFAR-100 and S-miniImageNet? My understanding is that it trains on all the data jointly, which should actually be the upper bound for a single model. 6. It would have been nice to visualize the factor selection matrices S for each task in order to visualize knowledge transfer.
Miscellaneous: 1. \citep should be used for parenthetical citations. 2. Initial double quote “ is backwards (Related Works). 3. “the first task,rk,1” 4. Figure 3 caption: “abd”
Questions: 1. How would you apply the weight factorization to 4D convolutional kernels?
[1] “Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors”, AISTATS 2021

Review Point: 1) Unlike a factorized model with an IBP prior, the proposed method lacks a sparsity constraint in the number of factors used by subsequent tasks. As such, the model will not be incentivized to use less factors, leading to increasing number of factors and increased computation with more tasks.
Review Point: 2) The IBP prior allows the data to dictate the number of factors to add for each task. The proposed method has no such mechanism, requiring setting the growth rate by hand using heuristics or a pre-determined schedule. Either is liable to over- or under-utilization of model capacity. Table 4 in the Experiments show that this does indeed have a significant impact on performance. Overall, I think this is an example of convergent ideas rather than plagiarism, but a discussion of the connections is warranted. Task incremental learning: This method requires knowing the task ID at test time to pick which factor selector weights to use. Without it, the proposed method doesn’t know which subnetwork to use, and would likely have to resort to trying all of them, which isn’t guaranteed to produce the right results. Recent continual learning methods are often evaluated in the more challenging class incremental setting, where task ID is not known. Experiments 1. (+) Experiments are conducted on a good set of datasets 2. (+) Error bars are shown 3. (+) The proposed method mostly outperforms the baselines, especially on the more complex datasets.
Review Point: 4. (-) More baselines should be compared against, particularly dynamic architecture approaches, as that’s the category that this method falls under. Many of the compared methods don’t operate on the same set of continual learning assumptions as this paper; in particular, the replay-based methods are often using replay because they consider class incremental learning.
Review Point: 5. (-) Why are the results of Multitask learning so bad for S-CIFAR-100 and S-miniImageNet? My understanding is that it trains on all the data jointly, which should actually be the upper bound for a single model.
Review Point: 6. It would have been nice to visualize the factor selection matrices S for each task in order to visualize knowledge transfer. Miscellaneous:
Review Point: 3. “the first task,rk,1” 4. Figure 3 caption: “abd” Questions:
Review Point: 1. How would you apply the weight factorization to 4D convolutional kernels? [1] “Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors”, AISTATS 2021
==================================================

Focused review:

Weaknesses:
One of the authors main contributions seems to be the notion of (Z1, Z2, Z3)-stable. However, it is still unclear to me 1) if (Z1, Z2, Z3)-stable actually is a novel concept that the authors came up with, 2) what the concept can and cannot be used for, and 3) how the concept relates the 'stability' in PDEs (see Qestuions 5-6).
It is challenging to fully evaluate originality of the work, because the related works section is very sparse. I have inluded some questions to evaluate originality and am willing to raise my score.
The empirical evaluation does not fully support the theorems, nor does it fully answer the research question. I have included some questions to evaluate the quality of the empirical section.
There is no discussion of limitations nor of negative societal impacts.
It would be very helpful if the limitation section could list and analyze the assumptions in Definitions/Theorems 4.1-4.4 and discuss what the extra steps would be to adapt the Definitions to other classes of PDEs beyond HJB.
Are the theorems truly applicable to all variations of the HJB equations?
What does (Z1,Z2,Z3)-stable mean and what does it not mean?
It would be helpful if the authors mention that, while the work is very theoretical, it could be used to improve numerical modeling of applications that violate the NeurIPS ethical guidelines.

Review Point: 1) if (Z1, Z2, Z3)-stable actually is a novel concept that the authors came up with,
Review Point: 2) what the concept can and cannot be used for, and
Review Point: 3) how the concept relates the 'stability' in PDEs (see Qestuions 5-6). It is challenging to fully evaluate originality of the work, because the related works section is very sparse. I have inluded some questions to evaluate originality and am willing to raise my score. The empirical evaluation does not fully support the theorems, nor does it fully answer the research question. I have included some questions to evaluate the quality of the empirical section. There is no discussion of limitations nor of negative societal impacts. It would be very helpful if the limitation section could list and analyze the assumptions in Definitions/Theorems 4.1-4.4 and discuss what the extra steps would be to adapt the Definitions to other classes of PDEs beyond HJB. Are the theorems truly applicable to all variations of the HJB equations? What does (Z1,Z2,Z3)-stable mean and what does it not mean? It would be helpful if the authors mention that, while the work is very theoretical, it could be used to improve numerical modeling of applications that violate the NeurIPS ethical guidelines.
==================================================

Focused review:

Weaknesses: 1)The proposed method is slightly incremental, and brings extra computational cost. The only contribution is that it uses the ratio of gradient norms of the loss and the regularization as a metric to trade-off the updating, which is not novel. Moreover, the gradient norm occurs in the loss, which means that it needs to further compute hessian matrix for backpropagation. But in practice, it is really hard to compute Hessian matrix, and even be inhibitively expensive to compute for large networks. So this method cannot be used in the modern networks.
2)In the experimental section, the authors only compare few baselines which cannot actually reflect the superiority of the proposed method. Moreover, the experiments also lack of large experiments, such as some experiments on ImageNet which is a standard dataset to test the performance nowadays. Finally, as discussed above, this method needs to compute Hessian which is very expensive. So it is better to compare the training time of all baselines.

Review Point: 1)The proposed method is slightly incremental, and brings extra computational cost. The only contribution is that it uses the ratio of gradient norms of the loss and the regularization as a metric to trade-off the updating, which is not novel. Moreover, the gradient norm occurs in the loss, which means that it needs to further compute hessian matrix for backpropagation. But in practice, it is really hard to compute Hessian matrix, and even be inhibitively expensive to compute for large networks. So this method cannot be used in the modern networks.
Review Point: 2)In the experimental section, the authors only compare few baselines which cannot actually reflect the superiority of the proposed method. Moreover, the experiments also lack of large experiments, such as some experiments on ImageNet which is a standard dataset to test the performance nowadays. Finally, as discussed above, this method needs to compute Hessian which is very expensive. So it is better to compare the training time of all baselines.
==================================================

Focused review:

Weaknesses:
The novelty is somewhat thin: Until the second half of page 5, the paper is mostly presenting existing backgrounds. The novelty mainly falls in Sec. 4. But the LUQ itself is rather straightforward to design, once the goal of designing logarithmic and unbiased quantizer is clear. The approaches in Sec. 5 are also rather standard and to some extent explored in previous literature. I'd say the main contribution of this paper is showing that such a simple combination of existing techniques is sufficient to achieve (surpringly good) accuracy, rather than proposing novel techniques.

Review Point: 4. But the LUQ itself is rather straightforward to design, once the goal of designing logarithmic and unbiased quantizer is clear. The approaches in Sec. 5 are also rather standard and to some extent explored in previous literature. I'd say the main contribution of this paper is showing that such a simple combination of existing techniques is sufficient to achieve (surpringly good) accuracy, rather than proposing novel techniques.
==================================================

Focused review:

Weaknesses:
Most findings from this paper seem to be well-known facts that have been explored by prior works. So the novelty of the proposed method is limited. For example, why is it a new observation that "quantizing and dequantizing the model parameters lead to roundoff errors" and "some weights are more sensitive than others which should be reflected on their quantization bi-width"? The former phenomenon is pretty much observed by all quantization-based studies, and the latter one is what motivates all prior studies on mixed-precision quantization. The main idea is to quantize the model layer-by-layer by solving a minimization problem of quantization error for each individual layer. Such a scheme has been explored by prior works such as [1] and [2]. The idea that different weights may be replaced with values of varying bit-width is also not new, as it has been studied by [3] and [4]. Several of these prior works are not references, which also indicates that the authors did not do a thorough literature search.
The proposed method only considers mixed precision for weights whereas the activations remain to be 32. As such, there is no actual performance improvement from the proposed method because the actual computation still happens in FP32.
The evaluation scope is quite limited, focusing primarily on image classification tasks and particularly ResNet variations. The authors are encouraged to evaluate its approach on a wider range of tasks/datasets, such as including NLP models and Transformer-based models such as vision transformers.
Quantizing bits to INT2, INT3, INT5, INT6, and INT7 cannot lead to any performance gains, to the best of the reviewer's knowledge, as there is no real hardware that can benefit from this irregular bit-width.
[1] Hassibi et. al. "Optimal Brain Surgeon and general network pruning", 1993 [2] Frantar et. al. "Optimal Brain Compression", 2022 [3] Shen et al. "Q-BERT: Hessian Based Ultra Low Precision Quantization of BER". 2020 [4] Zhao et. al. "Automatic Mixed-Precision Quantization Search of BERT", 2021

Review Point: 2020 [4] Zhao et. al. "Automatic Mixed-Precision Quantization Search of BERT", 2021
==================================================

Focused review:

- The numerical results only compare the test accuracy of the learned model over different distribution shifts and perturbations. While this verifies the robustness of the model and validates the problem formulation, it could be improved by adding figures that represent the computation and communication complexity and training time of the FedRobust, as well as its scalability to the network size. As it is mentioned in the paper, “a typical federated learning consists of a network of hundreds to millions of devices”. However, in the numerical simulations, the number of nodes is selected as n=10. - The convergence guarantee provided in Thm 2 is not too different (albeit a generalization) from earlier results on distributed minimax optimization. - The generalization guarantees follow earlier work; there is not substantial innovation in the result presented in Thm. 3.

Review Point: - The numerical results only compare the test accuracy of the learned model over different distribution shifts and perturbations. While this verifies the robustness of the model and validates the problem formulation, it could be improved by adding figures that represent the computation and communication complexity and training time of the FedRobust, as well as its scalability to the network size. As it is mentioned in the paper, “a typical federated learning consists of a network of hundreds to millions of devices”. However, in the numerical simulations, the number of nodes is selected as n=10.
Review Point: - The convergence guarantee provided in Thm 2 is not too different (albeit a generalization) from earlier results on distributed minimax optimization.
Review Point: - The generalization guarantees follow earlier work; there is not substantial innovation in the result presented in Thm.
==================================================

Focused review:

1. There is limited discussion of what components of the analysis are bespoke to over parameterized linear regression, and how the present work might inspire similar techniques for other, possibly more complex, problems. I think this may impact the longer term impact of this work. I think the work would benefit from further consideration and discussion of what the novel and ``portable'' insights were. 2. The conditions required in Thm.~4.5 (the main result) for consistency are somewhat obfuscated. The conditions involving $E[\kappa_X(\Sigma) (\norm{\hat w_{MR}} - \norm{\hat w_{MN}}) ]$ should be expressible as conditions on the (sequence of) covariance matrices. In the way they are currently expressed, it is difficult to compare with the ``benign'' condition of BLLT'20 [3] or the ``weakly benign'' condition of NDR'20 [23]. It would be useful if there was a more direct comparison with existing work here. BLLT'20 [3] show that their ``benign'' condition is *almost* necessary for expected risk consistency. Essentially, looking at the combination of BLLT'20's lower bound and NDR'20's upper bound the ``weakly benign'' condition of NDR'20 appears to be necessary and sufficient for risk consistency. How do the results of the present paper, at least for $\hat w_{MN}}$ compare to those results? 3. The role of 1-sided v.s. 2-sided bounds remains unclear, though discussion of 1-sided bounds possibly holding while 2-sided bounds fail makes up a fairly significant portion of Sec.~3.2 . The results of Sec.~4, which involve ``restricted'' uniform bounds (the sup is constrained to predictors which interpolate) are stated as 1-sided results, but clearly hold as 2-sided results since $L_S=0$ by the constraint and $L_D\ge 0$. It seems that ``restricted'' uniform bounds will always have this property. It may be useful to understand the role of 1-sided vs 2-sided bounds in relation to Sec.~4, or to briefly explain why the methods of Sec.~4 circumvent this issue. POST REBUTTAL COMMENTS: *Portable Insights*: Further discussion of optimistic rates, and the possibility of applying duality in subsequent work would be helpful. It is not obvious to me how the duality argument would work for problems other than linear regression. *Comparison to [3]/[23]*: Maybe it would be helpful to show that bounds on the restricted eigenvalue that lead to consistency imply the "bening" condition would hold, and intuitive relationships between them would be helpful. Probably in an appendix would be good. I agree that the restricted eigenvalue and the norm of X are more easily interpreted than the quantities appearing in the "benign" condition.

Review Point: 1. There is limited discussion of what components of the analysis are bespoke to over parameterized linear regression, and how the present work might inspire similar techniques for other, possibly more complex, problems. I think this may impact the longer term impact of this work. I think the work would benefit from further consideration and discussion of what the novel and ``portable'' insights were.
Review Point: 2. The conditions required in Thm.~4.5 (the main result) for consistency are somewhat obfuscated. The conditions involving $E[\kappa_X(\Sigma) (\norm{\hat w_{MR}} - \norm{\hat w_{MN}}) ]$ should be expressible as conditions on the (sequence of) covariance matrices. In the way they are currently expressed, it is difficult to compare with the ``benign'' condition of BLLT'20 [3] or the ``weakly benign'' condition of NDR'20 [23]. It would be useful if there was a more direct comparison with existing work here. BLLT'20 [3] show that their ``benign'' condition is *almost* necessary for expected risk consistency. Essentially, looking at the combination of BLLT'20's lower bound and NDR'20's upper bound the ``weakly benign'' condition of NDR'20 appears to be necessary and sufficient for risk consistency. How do the results of the present paper, at least for $\hat w_{MN}}$ compare to those results?
Review Point: 3. The role of 1-sided v.s. 2-sided bounds remains unclear, though discussion of 1-sided bounds possibly holding while 2-sided bounds fail makes up a fairly significant portion of Sec.~3.2 . The results of Sec.~4, which involve ``restricted'' uniform bounds (the sup is constrained to predictors which interpolate) are stated as 1-sided results, but clearly hold as 2-sided results since $L_S=0$ by the constraint and $L_D\ge 0$. It seems that ``restricted'' uniform bounds will always have this property. It may be useful to understand the role of 1-sided vs 2-sided bounds in relation to Sec.~4, or to briefly explain why the methods of Sec.~4 circumvent this issue. POST REBUTTAL COMMENTS: *Portable Insights*: Further discussion of optimistic rates, and the possibility of applying duality in subsequent work would be helpful. It is not obvious to me how the duality argument would work for problems other than linear regression. *Comparison to [3]/[23]*: Maybe it would be helpful to show that bounds on the restricted eigenvalue that lead to consistency imply the "bening" condition would hold, and intuitive relationships between them would be helpful. Probably in an appendix would be good. I agree that the restricted eigenvalue and the norm of X are more easily interpreted than the quantities appearing in the "benign" condition.
==================================================

Focused review:

1. I would have liked to see more discussion and empirical analysis of how typological differences between languages affect the relative performance of subword- and character-level models. The claims of the paper are quite broad, but there is not enough evidence to convince me that the conclusions hold generally across languages. More specifically: * I assume that character-level MT would be more beneficial for languages with logographic writing systems (e.g. Chinese) than the ones that use alphabets. My understanding is that the WMT shared task language pairs are skewed towards European languages: if that is the case, I think the shared task analysis could have considered more venues, e.g. CWMT, The Workshop on Asian Translation, or IWSLT 2020 Open Domain Translation task (ja-zh, 20M training sentences). In any case, I think the paper should have listed the WMT languages and discussed if the use/mention of character-level methods varies by language, script, or morphological patterns.
* The empirical evaluation on the WMT dataset is also only performed on European languages (English, German, and Czech). In the smaller experiments, which were used for model selection, character-level models outperform subword-level ones on English-Arabic translation; this aligns with the findings of Shaham and Levy (2020), who attribute it to non-concatenative morphology of Semitic languages. So I am left wondering if the conclusions would have been different if the larger-data evaluation included languages with non-concatenative morphologies.
2. This might be beyond the scope of the paper, but I am also wondering if the same conclusions extend to unsupervised and semi-supervised cases (for details, see the following section).
- I am wondering if the conclusions about the performance gap between character-level and subword-level models apply to unsupervised MT. For example, character-level FSTs for unsupervised translation between closely related languages (e.g. Serbian and Bosnian) perform very well [1]. An unsupervised subword-level neural MT model for the same task tested demonstrates very poor results [2], but training the same model with character-level tokenization yields a great improvement [3].
- I think that the process of selecting the best-performing model in a smaller experiment and then evaluating it extensively is reasonable, but I am wondering if evaluating the two rejected models, CANINE and Charformer, in terms of robustness could have given other interesting insights. I understand that evaluating all types of models on the larger dataset might not be feasible, but maybe the robustness testing could somehow be included in the smaller-data experiments too?
- I might be missing something, but I could not figure out if the results for the proposed two-step decoding method are reported anywhere in the paper. Are the results in Table 1 with decoder downsampling obtained with or without the two-step method? If without, then how much worse does the two-step method perform?
- Table 2 is not colorblind-friendly (red-green is the most common type of colorblindness). I would suggest changing the colors and/or specifying in the caption that the top number in each cell corresponds to chrF and the bottom one to COMET.
- In Table 3, en-cs BPE to character results are missing the recall on lemmas/forms seen in training.
[1] Pourdamghani, Nima, and Kevin Knight. " Deciphering related languages." EMNLP 2017.
[2] He, Junxian, et al. "A Probabilistic Formulation of Unsupervised Text Style Transfer." ICLR 2019.
[3] Ryskina, Maria, et al. "Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction." SIGMORPHON 2021.

Review Point: * I assume that character-level MT would be more beneficial for languages with logographic writing systems (e.g. Chinese) than the ones that use alphabets. My understanding is that the WMT shared task language pairs are skewed towards European languages: if that is the case, I think the shared task analysis could have considered more venues, e.g. CWMT, The Workshop on Asian Translation, or IWSLT 2020 Open Domain Translation task (ja-zh, 20M training sentences). In any case, I think the paper should have listed the WMT languages and discussed if the use/mention of character-level methods varies by language, script, or morphological patterns.
Review Point: * The empirical evaluation on the WMT dataset is also only performed on European languages (English, German, and Czech). In the smaller experiments, which were used for model selection, character-level models outperform subword-level ones on English-Arabic translation; this aligns with the findings of Shaham and Levy (2020), who attribute it to non-concatenative morphology of Semitic languages. So I am left wondering if the conclusions would have been different if the larger-data evaluation included languages with non-concatenative morphologies.
Review Point: 2. This might be beyond the scope of the paper, but I am also wondering if the same conclusions extend to unsupervised and semi-supervised cases (for details, see the following section).
Review Point: - I am wondering if the conclusions about the performance gap between character-level and subword-level models apply to unsupervised MT. For example, character-level FSTs for unsupervised translation between closely related languages (e.g. Serbian and Bosnian) perform very well [1]. An unsupervised subword-level neural MT model for the same task tested demonstrates very poor results [2], but training the same model with character-level tokenization yields a great improvement [3].
Review Point: - I think that the process of selecting the best-performing model in a smaller experiment and then evaluating it extensively is reasonable, but I am wondering if evaluating the two rejected models, CANINE and Charformer, in terms of robustness could have given other interesting insights. I understand that evaluating all types of models on the larger dataset might not be feasible, but maybe the robustness testing could somehow be included in the smaller-data experiments too?
Review Point: - I might be missing something, but I could not figure out if the results for the proposed two-step decoding method are reported anywhere in the paper. Are the results in Table 1 with decoder downsampling obtained with or without the two-step method? If without, then how much worse does the two-step method perform?
Review Point: - Table 2 is not colorblind-friendly (red-green is the most common type of colorblindness). I would suggest changing the colors and/or specifying in the caption that the top number in each cell corresponds to chrF and the bottom one to COMET.
Review Point: - In Table 3, en-cs BPE to character results are missing the recall on lemmas/forms seen in training. [1] Pourdamghani, Nima, and Kevin Knight. " Deciphering related languages." EMNLP 2017. [2] He, Junxian, et al. "A Probabilistic Formulation of Unsupervised Text Style Transfer." ICLR 2019. [3] Ryskina, Maria, et al. "Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction." SIGMORPHON 2021.
==================================================

Focused review:

- Some of the theoretical findings are not so surprising (early stopping, fewer parameters). - The link between SMILE and the theoretical bound could be stronger in the sense that both of them could exist independently of each other. In particular, the exact bound corresponding to SMILE is never stated (given the exact Lipschitz constants nor the number of parameters for example).

Review Point: - Some of the theoretical findings are not so surprising (early stopping, fewer parameters).
Review Point: - The link between SMILE and the theoretical bound could be stronger in the sense that both of them could exist independently of each other. In particular, the exact bound corresponding to SMILE is never stated (given the exact Lipschitz constants nor the number of parameters for example).
==================================================

Focused review:

Weaknesses: 1. The first question is that the evidence of the motivation is not direct. Since the problem to be solved is that “a predictor suffers from the accuracy decline due to long-term and continuous usage”, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim. 2. Another question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match. 3. Is there any related work about the mixed-batch method?

Review Point: 1. The first question is that the evidence of the motivation is not direct. Since the problem to be solved is that “a predictor suffers from the accuracy decline due to long-term and continuous usage”, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim.
Review Point: 2. Another question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match.
Review Point: 3. Is there any related work about the mixed-batch method?
==================================================

Focused review:

Weaknesses:
The authors claim there are no studies beyond network classification and a lack of theoretical support for the available methods. In my opinion, the claim is not correct. Many papers include different graph-level tasks in their experiments and several of them provide also theoretical support about the proposed architecture; I can refer the authors to some of them below. The authors also claim "the efficacy of these methods [on graph matching] in learning from network-valued data remains unexplored". The authors should be more precise about what remains unexplored. Nevertheless, I may agree that extra studies and theoretical analyses can positively contribute to the research in this field, and I recognize the value of considering the small sample size setting. https://ogb.stanford.edu/
Dwivedi, Vijay Prakash, et al. 2020. Benchmarking graph neural networks
Bouritsas, Giorgos, et al. 2020. Improving graph neural network expressivity via subgraph isomorphism counting.
Zambon, D., Alippi, C., & Livi, L. 2020. Graph Random Neural Features for Distance-Preserving Graph Representations.
Sato, Ryoma. 2020. A survey on the expressive power of graph neural networks.
Maron, Haggai, Heli Ben-Hamu, and Yaron Lipman. 2019. Open problems: Approximation power of invariant graph networks.
Loukas, Andreas. 2019. What graph neural networks cannot learn: depth vs width.
Kriege, Nils M., et al. 2018. A Property Testing Framework for the Theoretical Expressivity of Graph Kernels.
Chen, Hao, and Jerome H. Friedman. 2017. A new graph-based two-sample test for multivariate and object data.
The graph transformation of G to G^\sigma seems to be ill-defined. Although the graphon can have unique node degrees, graphs sampled from it can result in nodes with the same degree, so 1) there might not exist a monotonically increasing permutation but only non-decreasing, and 2) the permutation is not unique, in general.
The claim "NCLM is the only known clustering strategy for graphs of different sizes" is incorrect. Basically, any kernel-based and distance-based clustering method that does not require explicit vector embeddings can be applied to graphs. It is customary to choose the most appropriate distance or kernel that better fits the problem at hand; a simple example is k-means. The claims seem unjustified to me also because, as far as I can tell, NCLM constructs vector representations of graphs.
The methods chosen for the comparison can give only limited insights. The considered methods appear to be more or less arbitrarily chosen combinations of graph functions with clustering methods. For example, this approach does not allow us to understand whether it is the proposed distance or the considering clustering method that brings the most advantages. I might be wrong, but it seems to me that there is no limitation in performing experiments with the proposed graphon-based distance with other clustering methods, and the considered clustering methods with other distances/kernels.
Regarding the two-sample hypothesis test, it is unclear how a critical region of a given significance level can be constructed without having knowledge about the underlying graphons. From what I see, in the paper, the critical region is computed by sampling from the original graphons. I would to have some clarifications in this respect.
Questions and unclear parts:
The sorting-and-smoothing estimator is mentioned even in the abstract, but never introduced in the paper. Why is it the only one that can meet the above requirements? Moreover, the above requirements (I guess the authors refer to Assumptions 1-3) are about the graphons, not their estimators.
Theorem 1. It is counterintuitive for me that for a bounded number of nodes, the more graphs we have, the larger the rate of erroneous clustered graph increases. I would expect that for m -> infinity, the rate of misclustered graphs converges to the optimal value (which is not necessarily zero), and the variance of such observed rate decreases. Could you comment on that?
It is not clear to me to what extent the discussed clustering methods had to be adapted to work with the proposed distance. It is also unclear whether or not these clustering methods would be consistent regardless of the specific graph distance, provided that the graph distance is sufficiently expressive (eg, it is metric).
Because the considered method is based on edge histograms, I wonder if there are known limitations or important differences when considering sparse graphs instead of denser ones. Are there different best practices to operate in the two regimes?
Other comments and suggestions:
Say explicitly that graphs are symmetric and without node or edge attributes (if this is the case).
Expand the comment about indistinguishable inhomogeneous random graph models. What is the argument to show that although different, they are indistinguishable? What are we losing by ignoring such graphs, in what cases are we interested in considering also them?
Prop 1. Please, expand the discussion about the variable n which seems to be undefined.
Cor 1. Shouldn't it be: given m, n_0 and w-w' , we need to find a small enough constant C?

Review Point: 1) there might not exist a monotonically increasing permutation but only non-decreasing, and
==================================================

Focused review:

1. Questions about the structural causal model 1) I feel that the confounder set C can be interpreted as “object shapes and where to place them”. But I still do not have an intuitive way to interpret the image-specific context representation M. 2) Why is X -> M instead of M -> X? From my understanding, we sample object shapes and their locations to get M. And then later we sample object appearance (e.g., texture, lighting, etc.) to get X. 2. Implementation 1) Since the images in both VOC and COCO have different sizes and ratios, I wonder how the authors construct the confounder set C. 2) Is the segmentation mask X_m (L195) logits or probabilities? 3) I feel a bit confused about Eqn. (3). It seems that W_1 and W_2 are used as projection matrices, reducing the dimension from original spatial size (hw) to the number of class (n). I wonder if this is reasonable. And I think the projected embedding space can be any dimension, not necessarily to be n? 4) Why do the authors choose P(c) to be uniform? Using the actual object frequencies in the dataset to represent P(c) might be better? 3. Experiments 1) For Q1 in Table 1, more details are required. How exactly the segmentation mask is used in the network? What’s the dimension? Is it a soft mask with probability/logit, or a binary mask with one-hot label? What if the author constructed a self-attention mask similar to Eqn. (3)? 2) Since the proposed method requires iterative refinement, I think it should also compare with the Noisy Student training [A1]. For example, after the first time training of the segmentation model, the authors can then use it to generate pseudo labels. And then, use the pseudo labels to re-train the segmentation model. By comparing with this baseline, we can then know if the performance gain comes from causal intervention or simply from the iterative refinement of the segmentation model itself. 3) In Table, it seems that the proposed method has smaller gain when using stronger feature backbone. Does it mean that, stronger network can better handle the context (e.g., effectively exploit its advantage while discard its negative impact)? Reference [A1] Xie et al. CVPR 2020. Self-training with Noisy Student improves ImageNet classification

Review Point: 1) I feel that the confounder set C can be interpreted as “object shapes and where to place them”. But I still do not have an intuitive way to interpret the image-specific context representation M.
Review Point: 2) Why is X -> M instead of M -> X? From my understanding, we sample object shapes and their locations to get M. And then later we sample object appearance (e.g., texture, lighting, etc.) to get X.
Review Point: 2. Implementation1) Since the images in both VOC and COCO have different sizes and ratios, I wonder how the authors construct the confounder set C.
Review Point: 2) Is the segmentation mask X_m (L195) logits or probabilities?
Review Point: 3) I feel a bit confused about Eqn. (3). It seems that W_1 and W_2 are used as projection matrices, reducing the dimension from original spatial size (hw) to the number of class (n). I wonder if this is reasonable. And I think the projected embedding space can be any dimension, not necessarily to be n?
Review Point: 4) Why do the authors choose P(c) to be uniform? Using the actual object frequencies in the dataset to represent P(c) might be better?
Review Point: 3. Experiments1) For Q1 in Table 1, more details are required. How exactly the segmentation mask is used in the network? What’s the dimension? Is it a soft mask with probability/logit, or a binary mask with one-hot label? What if the author constructed a self-attention mask similar to Eqn. (3)?
Review Point: 2) Since the proposed method requires iterative refinement, I think it should also compare with the Noisy Student training [A1]. For example, after the first time training of the segmentation model, the authors can then use it to generate pseudo labels. And then, use the pseudo labels to re-train the segmentation model. By comparing with this baseline, we can then know if the performance gain comes from causal intervention or simply from the iterative refinement of the segmentation model itself.
Review Point: 3) In Table, it seems that the proposed method has smaller gain when using stronger feature backbone. Does it mean that, stronger network can better handle the context (e.g., effectively exploit its advantage while discard its negative impact)? Reference [A1] Xie et al. CVPR 2020. Self-training with Noisy Student improves ImageNet classification
==================================================

Focused review:

Weaknesses
1. In the paper, the authors used true $Q$ function and joint $Q$ value function, but reviewer doesn't find the definition of joint $Q$ value function. What is the difference between joint $Q$ value function and true $Q$ function?.
2. In the equation (3), the authors expressed the utility function as true $Q$ function. If joint $Q$-value function is used for equation (3) instead of true $Q$ function, then the equation is true. However, the author use true $Q$ function and reviewer doesn't understand how to derive the equation (3). Some more explanations are needed
3. In Appendix D and E, the authors prove their claim, but I couldn't follow the process of proof. How can the author express the utility function as equation (13) in appendix D? The reviewer doesn't understand how mapping $f$ occurs. Therefore, more explanation is needed to understand equations 13 and 18
4. Finally, the authors madeThe author proposes a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the author can solve the above mentioned questions, the reviewer will raise the score. target of critic $V(s)$ as equation (9), but there is no explanation of why that can happen. You need explanation of reason for target of critic.

Review Point: 1. In the paper, the authors used true $Q$ function and joint $Q$ value function, but reviewer doesn't find the definition of joint $Q$ value function. What is the difference between joint $Q$ value function and true $Q$ function?.
Review Point: 2. In the equation (3), the authors expressed the utility function as true $Q$ function. If joint $Q$-value function is used for equation (3) instead of true $Q$ function, then the equation is true. However, the author use true $Q$ function and reviewer doesn't understand how to derive the equation (3). Some more explanations are needed 3. In Appendix D and E, the authors prove their claim, but I couldn't follow the process of proof. How can the author express the utility function as equation (13) in appendix D? The reviewer doesn't understand how mapping $f$ occurs. Therefore, more explanation is needed to understand equations 13 and 18 4. Finally, the authors madeThe author proposes a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the author can solve the above mentioned questions, the reviewer will raise the score. target of critic $V(s)$ as equation (9), but there is no explanation of why that can happen. You need explanation of reason for target of critic.
==================================================

Focused review:

weaknesses are lack of related work and some of the claims. Strengths
The paper is overall well written and easy to follow.
The authors use a transformer architecture in a multi-agent setting and creates a connection to the multi-agent advantage decomposition theorem.
Experimental results show improvements. Weaknesses
The main contribution of the paper is the transformer architecture but related work lacks any prior work on using sequential models in a multi-agent setting. It is not easy to position the paper without knowing if the encoder-decoder structure, transformers, or advantage decomposition has been studied in the past. Especially, UPDeT*, which also uses a transformer architecture and a similar encoder-decoder structure, should be compared.
You mention that "the transformer architecture allows sequential policies to be trained in parallel" but it is not clear what this means. While you are using a different objective with all actions of all agents are available, you still need to run the decoder on a ^ t i 1 : m − 1
to generate the action distribution for agent i m
. Main difference from the autoregressive variant seems to be batching of input and gradient accumulation but not clear if this is what you meant by parallelism.
It is not clear if all of transformer components are useful in this setting. For example, since you are shuffling agents at every iteration, position encoding might not be very relevant and can be discarded. I think an ablation study is needed to understand the effect of different components.
* UPDET: UNIVERSAL MULTI-AGENT REINFORCEMENT LEARNING VIA POLICY DECOUPLING WITH TRANSFORMERS. Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang.
The authors addressed the limitations and included a source code with their submission.

Review Point: * UPDET: UNIVERSAL MULTI-AGENT REINFORCEMENT LEARNING VIA POLICY DECOUPLING WITH TRANSFORMERS. Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang. The authors addressed the limitations and included a source code with their submission.
==================================================

Focused review:

Weaknesses: - As this work has the perspective of task-oriented recommendation, it seems that works such as [] Li, Xiujun, et al. "End-to-end task-completion neural dialogue systems." arXiv preprint arXiv:1703.01008 (2017). are important to include, and compare to, at least conceptually. Also, discussion in general on how their work differs from other chatbox research works e.g. [] He, Ji, et al. "Deep reinforcement learning with a natural language action space." arXiv preprint arXiv:1511.04636(2015). would be very useful. - It is important that the authors highlight the strengths as well as the weaknesses of their released dataset: e.g. what are scenarios under which such a dataset would not work well? are 10,000 conversations enough for proper training? Similarly, a discussion on their approaches, in terms of things to further improve would be useful for the research community to extend -- e.g. a discussion on how the domain of movie recommendation can differ from other tasks, or a discussion on the exploration-exploitation trade-off. Particularly, it seems that this paper envisions conversational recommendations as a goal oriented chat dialogue. However, conversational recommendations could be more ambiguous.. - Although it is great that the authors have included these different modules capturing recommendations, sentiment analysis and natural language, more clear motivation on why each component is needed would help the reader. For example, the cold-start setting, and the sampling aspect of it, is not really explained. The specific choices for the models for each module are not explained in detail (why were they chosen? Why is a sentiment analysis model even needed -- can't we translate the like/dislike as ratings for the recommender?) - Evaluation -- since one of the contributions argued in the paper is "deep conversational recommenders", evaluation-wise, a quantitative analysis is needed, apart from user study results provided (currently the other quantitative results evaluate the different sub-modules independently). Also, the authors should make clearer the setup of how exactly the dataset is used to train/evaluate on the Amazon Turk conversations -- is beam-search used as in other neural language models? Overall, although I think that this paper is a nice contribution in the domain of movie conversational recommendation, I believe that the authors should better position their paper, highlighting also the weaknesses/ things to improve in their work, relating it to work on neural dialogue systems, and expanding on the motivation and details of their sub-modules and overall architecture. Some discussion also on how quantitative evaluation of the overall dialogue quality should happen would be very useful. == I've read the authors' rebuttal. It would be great if the authors add some of their comments from the rebuttal in the revised paper regarding the size of the dataset, comparison with goal-oriented chatbots and potential for quantitative evaluation.

Review Point: - As this work has the perspective of task-oriented recommendation, it seems that works such as [] Li, Xiujun, et al. "End-to-end task-completion neural dialogue systems." arXiv preprint arXiv:1703.01008 (2017). are important to include, and compare to, at least conceptually. Also, discussion in general on how their work differs from other chatbox research works e.g. [] He, Ji, et al. "Deep reinforcement learning with a natural language action space." arXiv preprint arXiv:1511.04636(2015). would be very useful.
Review Point: - It is important that the authors highlight the strengths as well as the weaknesses of their released dataset: e.g. what are scenarios under which such a dataset would not work well? are 10,000 conversations enough for proper training? Similarly, a discussion on their approaches, in terms of things to further improve would be useful for the research community to extend -- e.g. a discussion on how the domain of movie recommendation can differ from other tasks, or a discussion on the exploration-exploitation trade-off. Particularly, it seems that this paper envisions conversational recommendations as a goal oriented chat dialogue. However, conversational recommendations could be more ambiguous..
==================================================

Focused review:

- The paper is a little bit hard to follow. There are plenty of contents discussing the validity of ODEtoODE but have little paragraphs on how to apply the proposed method into usage. Since parameters become dynamical, it is expected to train ODEtoODE with additional procedures and may be different from the training of conventional ones. So including an algorithmic description is a good option. - Although the paper gives certain analysis on how ODEtoODE can help alleviate gradient vanishing/explosion problem, there is little experimental result to support such claim, especially for supervised learning in section 5.2. Without results such as convergence curves, etc, It is less convincing that the proposed method could really improve the stability and effectiveness.

Review Point: - The paper is a little bit hard to follow. There are plenty of contents discussing the validity of ODEtoODE but have little paragraphs on how to apply the proposed method into usage. Since parameters become dynamical, it is expected to train ODEtoODE with additional procedures and may be different from the training of conventional ones. So including an algorithmic description is a good option.
Review Point: - Although the paper gives certain analysis on how ODEtoODE can help alleviate gradient vanishing/explosion problem, there is little experimental result to support such claim, especially for supervised learning in section 5.2. Without results such as convergence curves, etc, It is less convincing that the proposed method could really improve the stability and effectiveness.
==================================================

Focused review:

1. The novelty of Theorem 1 is not entirely clear. The authors argue that the classical another parametrization B, Theta = (C Omega^{1/2}, Omega^{1/2}), which also enables joint convexity. Therefore, Theorem 1 doesn't appear to be a significant result in itself. This may be a lemma instead. 2. It is not clear why the classical parametrization does not allow penalty functions. One can include penalties on C by using the classical parametrization and levering C=B Theta^{-1}, e.g., || B Theta^{-1}||_{1,2} where 1,2 is the group L1 penalty. 3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage. 4. This work needs a more detailed comparative analysis in order to prove the superiority of the proposed approach. In particular, paper is lacking an adequate computational complexity and run-time analysis with respect to the existing methods, e.g. the standard parametrization and penalty approach. A similar weakness also exists in the numerical results section, where comprehensive empirical comparisons are lacking. The supplementary material has a numerical table, which shows a very incremental improvement and is not conclusive. 5. In proving Theorem 3, the authors employ standard methods such as restricted eigenvalues. These methods are already known to extend to non-convex objective functions and does not specifically hold for the claimed convex formulation.

Review Point: 1. The novelty of Theorem 1 is not entirely clear. The authors argue that the classical another parametrization B, Theta = (C Omega^{1/2}, Omega^{1/2}), which also enables joint convexity. Therefore, Theorem 1 doesn't appear to be a significant result in itself. This may be a lemma instead.
Review Point: 2. It is not clear why the classical parametrization does not allow penalty functions. One can include penalties on C by using the classical parametrization and levering C=B Theta^{-1}, e.g., || B Theta^{-1}||_{1,2} where 1,2 is the group L1 penalty.
Review Point: 3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage.
Review Point: 4. This work needs a more detailed comparative analysis in order to prove the superiority of the proposed approach. In particular, paper is lacking an adequate computational complexity and run-time analysis with respect to the existing methods, e.g. the standard parametrization and penalty approach. A similar weakness also exists in the numerical results section, where comprehensive empirical comparisons are lacking. The supplementary material has a numerical table, which shows a very incremental improvement and is not conclusive.
Review Point: 5. In proving Theorem 3, the authors employ standard methods such as restricted eigenvalues. These methods are already known to extend to non-convex objective functions and does not specifically hold for the claimed convex formulation.
==================================================

Focused review:

Weaknesses. About method and experiments: 1, In Table 4, the baseline method achieves 36.42% accuracy, whereas the backbone in Table 1 (ERM) shows 13.82%. The backbone of the authors’ method is higher than most existing methods. It is not clear about the effectiveness of the proposed method. Is it due to the design of the methodology or a strong baseline? 2, CD2NN aims to find the most trustworthy samples from cross domains as a positive sample to enlarge the intra-class connectivity. It is a bit tricky to decide when to find cross-domain nearest neighbors because the accurate positive pairs depend on a good network weight. How \lambda is changed according to the training epochs? About the setting: 3, The authors aim to solve the unsupervised DG, where a pre-trained model and class label are not required, but a domain label is needed. I noticed the results obtained by the authors’ method are slightly higher than ImageNet initialization. 4, I wonder if it is necessary to design an unsupervised method for DG without a pre-trained model. Many methods reported the pre-trained model for DG. If the authors use the pre-trained model, will the proposed methods still work well? I feel this paper is only meaningful if the proposed method still works well with pre-trained model. 5, Recently, a lot of research has been done on pre-trained models for DG. The differences and advantages of the proposed setting should be highlighted. • About writing: 6, The paper is not easy to read. The authors did not formulate the main idea in a good way.

Review Point: • About writing: 6, The paper is not easy to read. The authors did not formulate the main idea in a good way.
==================================================

Focused review:

Weaknesses of the paper: - no theoretical guarantees for convergence/pruning - though experiments on the small networks (LeNet300 and LeNet5) are very promising: similar to DNS [16] on LeNet300, significantly better than DNS [16] on LeNet5, the ultimate goal of pruning is to reduce the compute needed for large networks. - on the large models authors only compare GSM to L-OBS. No motivation given for the choice of the competing algorithm. Based on the smaller experiments it should be DNS [16], the closest competitor, rather than L-OBS, showed quite poor performance compared to others. - Authors state that GSM can be used for automated pruning sensitivity estimation. 1) While graphs (Fig 2) show that GSM indeed correlates with layer sensitivity, it was not shown how to actually predict sensitivity, i.e. no algorithm that inputs model, runs GSM, processes GSM result and output sensitivity for each layer. 2) Authors don't explain the detail on how the ground truth of sensitivity is achieved, lines 238-239 just say "we first estimate a layer's sensitivity by pruning ...", but no details on how actual pruning was done. comments: 1) Table 1, Table 2, Table 3 - "origin/remain params|compression ratio| non-zero ratio" --- all these columns duplicate the information, only one of the is enough. 2) Figure 1 - plot 3, 4 - two lines are indistinguishable (not even sure if there are two, just a guess), would be better to plot relative error of approximation, rather than actual values; why plot 3, 4 are only for one value of beta while plot 1 and 2 are for three values? 3) All figures - unreadable in black and white 4) Pruning majorly works with large networks, which are usually trained in distributed settings, authors do not mention anything about potential necessity to find global top Q values of the metric over the average of gradients. This will potentially break big portion of acceleration techniques, such as quantization and sparsification.

Review Point: - on the large models authors only compare GSM to L-OBS. No motivation given for the choice of the competing algorithm. Based on the smaller experiments it should be DNS [16], the closest competitor, rather than L-OBS, showed quite poor performance compared to others.
Review Point: - Authors state that GSM can be used for automated pruning sensitivity estimation.
Review Point: 1) While graphs (Fig 2) show that GSM indeed correlates with layer sensitivity, it was not shown how to actually predict sensitivity, i.e. no algorithm that inputs model, runs GSM, processes GSM result and output sensitivity for each layer.
Review Point: 2) Authors don't explain the detail on how the ground truth of sensitivity is achieved, lines 238-239 just say "we first estimate a layer's sensitivity by pruning ...", but no details on how actual pruning was done. comments:
Review Point: 1) Table 1, Table 2, Table 3 - "origin/remain params|compression ratio| non-zero ratio" --- all these columns duplicate the information, only one of the is enough.
Review Point: 2) Figure 1 - plot 3, 4 - two lines are indistinguishable (not even sure if there are two, just a guess), would be better to plot relative error of approximation, rather than actual values; why plot 3, 4 are only for one value of beta while plot 1 and 2 are for three values?
Review Point: 4) Pruning majorly works with large networks, which are usually trained in distributed settings, authors do not mention anything about potential necessity to find global top Q values of the metric over the average of gradients. This will potentially break big portion of acceleration techniques, such as quantization and sparsification.
==================================================

Focused review:

Weaknesses: 1. The presentation quality of this paper is low. As a result, it is difficult to understand what the actual novel contribution of this paper is. Many technical details were not presented clearly enough to understand. For example, although the conditional probability format is given for generating nodes based on the higher-level super-nodes, the detailed model architecture is not given. No math formulation is given to understand the generation process. 2. The authors claimed that the proposed method can help generate graphs in parallel. However, it is unclear whether the proposed method can achieve a theoretical/empirical speedup over existing graph generative methods, which should also mostly be able to execute in parallel. 3. Some writing parts were very sloppy, and mistakes happened everywhere in the paper. For example, in the caption of figure 1 the author wrote “Hierarchical A sample hierarchical graph with 3 levels …”, which I completely can not understand what does this sentence mean; At the end of section 1, the author wrote “(1) community structures of generic graphs are encoded from training data and respected in a generation”, what does “respected” mean? For the proof of theorem 2, the author claimed that proof can be found in the appendix but there is NO proof in the appendix. In table 1 there is no explanation for many missing values in the tables.

Review Point: 1. The presentation quality of this paper is low. As a result, it is difficult to understand what the actual novel contribution of this paper is. Many technical details were not presented clearly enough to understand. For example, although the conditional probability format is given for generating nodes based on the higher-level super-nodes, the detailed model architecture is not given. No math formulation is given to understand the generation process.
Review Point: 2. The authors claimed that the proposed method can help generate graphs in parallel. However, it is unclear whether the proposed method can achieve a theoretical/empirical speedup over existing graph generative methods, which should also mostly be able to execute in parallel.
Review Point: 3. Some writing parts were very sloppy, and mistakes happened everywhere in the paper. For example, in the caption of figure 1 the author wrote “Hierarchical A sample hierarchical graph with 3 levels …”, which I completely can not understand what does this sentence mean; At the end of section 1, the author wrote “(1) community structures of generic graphs are encoded from training data and respected in a generation”, what does “respected” mean? For the proof of theorem 2, the author claimed that proof can be found in the appendix but there is NO proof in the appendix. In table 1 there is no explanation for many missing values in the tables.
==================================================

Focused review:

1. Some important details of dataset are missing. For the COCO dataset, I think the authors either use some specific version from others or crop pathces containing objects and resize them to a fixed size. If it is the case, what is the input resolution? 2. It will be nice to show the sensitivy of those important hyper-parameters mentioned in L200. 3. There seem to exist a limitation of the method. Since the method relies on reconstruction, it may be difficult to deal with larger images and more diverse objects. Can the authors show results on ImageNet (e.g., mini-ImageNet)? 4. Can the authors explain L281-282 with more details? It seems non-intuitive that supervised methods are worser than semi-supervised methods under the same condition. 5. (Minor) Some details of baselines are missing. Although I agree that unsupervised baselines, like MoNet and IODINE, can not deal with complex backgrounds (even with modifications I will mention later), I would like to discuss about a fairer comparison. Those unsupervised, VAE-based methods leverage Spatial Broadcast Network as decoders to perform well on toy datasets. The capacity of this decoder favors certain complexity, and unsurprisingly fails on complex backgrounds. Thus, do the authors use ResNet as the decoder of MoNet or IODINE and tune thier hyper-parameters? 6. (Minor) Most equations relevant to classification in Sec 3 look strange to me. It seems that the authors express the negative log likelihood in a way not familiar to me.

Review Point: 1. Some important details of dataset are missing. For the COCO dataset, I think the authors either use some specific version from others or crop pathces containing objects and resize them to a fixed size. If it is the case, what is the input resolution?
Review Point: 2. It will be nice to show the sensitivy of those important hyper-parameters mentioned in L200.
Review Point: 3. There seem to exist a limitation of the method. Since the method relies on reconstruction, it may be difficult to deal with larger images and more diverse objects. Can the authors show results on ImageNet (e.g., mini-ImageNet)?
Review Point: 4. Can the authors explain L281-282 with more details? It seems non-intuitive that supervised methods are worser than semi-supervised methods under the same condition.
Review Point: 5. (Minor) Some details of baselines are missing. Although I agree that unsupervised baselines, like MoNet and IODINE, can not deal with complex backgrounds (even with modifications I will mention later), I would like to discuss about a fairer comparison. Those unsupervised, VAE-based methods leverage Spatial Broadcast Network as decoders to perform well on toy datasets. The capacity of this decoder favors certain complexity, and unsurprisingly fails on complex backgrounds. Thus, do the authors use ResNet as the decoder of MoNet or IODINE and tune thier hyper-parameters?
Review Point: 6. (Minor) Most equations relevant to classification in Sec 3 look strange to me. It seems that the authors express the negative log likelihood in a way not familiar to me.
==================================================

Focused review:

Weaknesses: 1. My feeling is that the conclusion is somewhat overclaimed. In both abstract and conclusion, it is emphasized that this work proves the pessimistic result that reweighting algorithms always overfit. However, the paper only proves that this conclusion might be true for some specific situations. For example, the reweighting algorithms need to satisfy Assumption 1 and Assumption 2, which means not all reweighting algorithms are considered. The overparameterized models need to be linear models, linearized neural networks or wide fully-connected neural networks, which are not commonly used in practice. Besides, the squared loss needs to be used to confirm the update rule is linear. All those assumptions are not quite mild for me. 2. The analysis of neural networks contributes less. With the existing NTK theorem, the extension from linear models to wide fully-connected neural networks is trivial (Section 3.2, 3.3). The work bypasses the core problem of overparametrized neural networks and only considers the easy wide fully-connected neural networks. 3. The theoretical results and experiments do not match. The theoretical proof considers wide fully-connected neural networks, while the experiments utilize a ResNet18 as the model, which is quite different. 4. Some key steps are empirical, although the paper claims that it provides a theoretical backing in the abstract. For example, this paper only proves that reweighting algorithms will converge to the same level as ERM, but the conclusion that ERM has a poor worst-group test performance is summarized through observation in practice. Besides, the paper can only empirically demonstrate that commonly used algorithms satisfy Assumption 2.

Review Point: 1. My feeling is that the conclusion is somewhat overclaimed. In both abstract and conclusion, it is emphasized that this work proves the pessimistic result that reweighting algorithms always overfit. However, the paper only proves that this conclusion might be true for some specific situations. For example, the reweighting algorithms need to satisfy Assumption 1 and Assumption 2, which means not all reweighting algorithms are considered. The overparameterized models need to be linear models, linearized neural networks or wide fully-connected neural networks, which are not commonly used in practice. Besides, the squared loss needs to be used to confirm the update rule is linear. All those assumptions are not quite mild for me.
Review Point: 2. The analysis of neural networks contributes less. With the existing NTK theorem, the extension from linear models to wide fully-connected neural networks is trivial (Section 3.2, 3.3). The work bypasses the core problem of overparametrized neural networks and only considers the easy wide fully-connected neural networks.
Review Point: 3. The theoretical results and experiments do not match. The theoretical proof considers wide fully-connected neural networks, while the experiments utilize a ResNet18 as the model, which is quite different.
Review Point: 4. Some key steps are empirical, although the paper claims that it provides a theoretical backing in the abstract. For example, this paper only proves that reweighting algorithms will converge to the same level as ERM, but the conclusion that ERM has a poor worst-group test performance is summarized through observation in practice. Besides, the paper can only empirically demonstrate that commonly used algorithms satisfy Assumption 2.
==================================================

Focused review:

weakness of the current work :
The current work focusses on a simplified setup of two classes and uses Contextual Stochastic Block Model as the generative model which does not provide an accurate representation of real-world graphs.
The authors do not demonstrate how to measure over-smoothing as well as the two effects in practice. Without being able to 1) adequately measure and quantify the two effects and 2) demonstrate the authors can adequately explain over-smoothing in a general setting, the presented approach seems hypothetical.
The authors do not demonstrate the practicality of the current approach to the research community. Theoretical analysis is great however without being able to provide utility, the scope of the work is significantly reduced.
As the authors themselves point out, the current work's analysis is based on oracle classifier, however in practice, we work in a semi-supervised setting. Thus the utility associated with the current work is pretty limited.

Review Point: 2) demonstrate the authors can adequately explain over-smoothing in a general setting, the presented approach seems hypothetical. The authors do not demonstrate the practicality of the current approach to the research community. Theoretical analysis is great however without being able to provide utility, the scope of the work is significantly reduced. As the authors themselves point out, the current work's analysis is based on oracle classifier, however in practice, we work in a semi-supervised setting. Thus the utility associated with the current work is pretty limited.
==================================================

Focused review:

Weaknesses. The authors state that the VCAUSE was developed for causal inference, however, they also state that they assumed that the causal graph is known in the current contribution. If it is the case, why there is the need for the causal inference?
Although the paper is quite clearly written, there are some flows. First, the role of U (exogenous) variables is not clear: is is assumed that there is 1 latent variable per 1 observed variable, as shown on Figure 1. Then, these U variables are not mentioned further in the paper. The VCAUSE (Definition 4.1) is defined without them. It is stated that "the latent variables Z play a similar role to the exogenous variables U" but it is very vague, and I do not see a clear correspondence.
Propositions 1 and 2 are important but I guess no novel, it should be a known result in deep learning.
I would appreciate more explanations on the design condition 2. I see that it follows from Prop. 2 but it is unclear why to use neural networks without hidden layers.

Review Point: 2 but it is unclear why to use neural networks without hidden layers.
==================================================

Focused review:

Weaknesses:
1: The paper evaluated the model empirically. However, a proper theoretical justification can be provided for the ensemble kind of model. Is it possible to provide some bound/relation between the mixup and ensemble?
2: Most of the study, like generalization ability, OOD robustness, calibration, uncertainty estimates adversarial robustness are exist in the literature. So, in terms of these study the paper is not novel, but the proposed mixup is novel.
3: The results are evaluated only over a small dataset i.e. CIFAR10/CIFAR10-Neg. Which are not sufficient to evaluate the model efficacy. I request to the author to please provide the result for the ensemble and TT-mixup result for the CIFAR100 dataset. It contains a larger class and more similar classes.
4: In the section-4 Eq:2, term “u” is not clear. How it is used in the model? Please provide the details.
5: Reproducibility may be a key concern, mixup itself has a lot of randomness and here parameters and implementation details are not complete. Also, datasets split details are missing. Overall, reproducibility is hard. I request to the author to please provide the code with the details descriptions. What are OOD samples?.
6: In the Figure:4, on the X-axis there are number 1,2 what are that numbers?
7: Ex-mixup-v1 and Ex-mixup-v2 showing consistently poor result, when it will be useful? Did the author tried over some extrapolated data? In the Figure-3 (Extra-mixup-v2) it shows very impressive result but same pattern are not evident in the Table-2 why?

Review Point: 1: The paper evaluated the model empirically. However, a proper theoretical justification can be provided for the ensemble kind of model. Is it possible to provide some bound/relation between the mixup and ensemble?
Review Point: 2: Most of the study, like generalization ability, OOD robustness, calibration, uncertainty estimates adversarial robustness are exist in the literature. So, in terms of these study the paper is not novel, but the proposed mixup is novel.
Review Point: 3: The results are evaluated only over a small dataset i.e. CIFAR10/CIFAR10-Neg. Which are not sufficient to evaluate the model efficacy. I request to the author to please provide the result for the ensemble and TT-mixup result for the CIFAR100 dataset. It contains a larger class and more similar classes.
Review Point: 4: In the section-4 Eq:2, term “u” is not clear. How it is used in the model? Please provide the details.
Review Point: 5: Reproducibility may be a key concern, mixup itself has a lot of randomness and here parameters and implementation details are not complete. Also, datasets split details are missing. Overall, reproducibility is hard. I request to the author to please provide the code with the details descriptions. What are OOD samples?.
Review Point: 6: In the Figure:4, on the X-axis there are number 1,2 what are that numbers?
Review Point: 7: Ex-mixup-v1 and Ex-mixup-v2 showing consistently poor result, when it will be useful? Did the author tried over some extrapolated data? In the Figure-3 (Extra-mixup-v2) it shows very impressive result but same pattern are not evident in the Table-2 why?
==================================================

Focused review:

I have some concerns about the paper: 1. Why focal loss is used in regression tasks? Focal loss is famous for doing class imbalance problem. It has lower gradients on easy samples, which is a good property for classification. But for regressing the IoU, lower weight for easy samples may cause inaccurate problem. This paper gives me a feeling that the authors only want to have a unified form, but didn't consider the difference between the classification and regression tasks. 2. In [1], the predicted variance of bbox parameters is used for NMS. The algorithm in this paper also produce bbox confidence (sum of two neighbour probabilities). Could it benefit the NMS? 3. The DFL is very similar with softargmax which is widely used in keypoint detection. However, the citation of this research topic is lacking. Please give some credit to authors of keypoint detection papers such as [2] and more. A problem of the softargmax is that the gradient imbalance problem. The form of softargmax is \sum p(x)x. The gradient for p(x) with x=10 is 10 times bigger than the gradient at x=1. This means that the DFL puts more weight on big objects, while the difficult problem of detection is usually the small objects. Overall, the idea of this paper is good but some of the details are still coarse. [1] He Y, Zhang X, Savvides M, et al. Softer-nms: Rethinking bounding box regression for accurate object detection[J]. arXiv preprint arXiv:1809.08545, 2018, 2. [2] Nibali A, He Z, Morgan S, et al. Numerical coordinate regression with convolutional neural networks[J]. arXiv preprint arXiv:1801.07372, 2018.

Review Point: 1. Why focal loss is used in regression tasks? Focal loss is famous for doing class imbalance problem. It has lower gradients on easy samples, which is a good property for classification. But for regressing the IoU, lower weight for easy samples may cause inaccurate problem. This paper gives me a feeling that the authors only want to have a unified form, but didn't consider the difference between the classification and regression tasks.
Review Point: 2. In [1], the predicted variance of bbox parameters is used for NMS. The algorithm in this paper also produce bbox confidence (sum of two neighbour probabilities). Could it benefit the NMS?
Review Point: 3. The DFL is very similar with softargmax which is widely used in keypoint detection. However, the citation of this research topic is lacking. Please give some credit to authors of keypoint detection papers such as [2] and more. A problem of the softargmax is that the gradient imbalance problem. The form of softargmax is \sum p(x)x. The gradient for p(x) with x=10 is 10 times bigger than the gradient at x=1. This means that the DFL puts more weight on big objects, while the difficult problem of detection is usually the small objects. Overall, the idea of this paper is good but some of the details are still coarse. [1] He Y, Zhang X, Savvides M, et al. Softer-nms: Rethinking bounding box regression for accurate object detection[J]. arXiv preprint arXiv:1809.08545, 2018, 2. [2] Nibali A, He Z, Morgan S, et al. Numerical coordinate regression with convolutional neural networks[J]. arXiv preprint arXiv:1801.07372, 2018.
==================================================

Focused review:

Weaknesses
The reason for asynchrony should be better explained, possibly with some real-world use cases. Actually, it has been observed that asynchrony may not be the best choice in distributed training, because it uses out-of-date (i.e., stale) gradients to update models. This staleness often results in degraded performance. Therefore, several state-of-the-art methods still use synchronous methods to achieve strong empirical performance, with some simple tricks such as large batch size, warmup, layerwise adaptive learning rates, and gradient compression (see reference [1-5] from below). I think this tradeoff between synchrony and asynchrony should be properly discussed.
Following the first point, the authors is encouraged to consider how to resolve the staleness issue due to asynchrony. This could further enhance this paper.
The experiments are conducted on two applications of bi-level optimization: hyper-cleaning and regularization coefficients optimization. I think bi-level optimization has received great attention these days mainly because of some emerging applications such as meta-learning, neural architecture search and etc. The authors are encouraged to consider these applications (e.g., meta-learning, neural architecture search) in the experiments. Reference:
[1] Goyal, Priya, et al. "Accurate, large minibatch sgd: Training imagenet in 1 hour." arXiv preprint arXiv:1706.02677 (2017).
[2] You, Yang, et al. "Large batch optimization for deep learning: Training bert in 76 minutes." arXiv preprint arXiv:1904.00962 (2019).
[3] Huo, Zhouyuan, Bin Gu, and Heng Huang. "Large batch optimization for deep learning using new complete layer-wise adaptive rate scaling." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.
[4] Liu, Rui, and Barzan Mozafari. "Communication-efficient Distributed Learning for Large Batch Optimization." International Conference on Machine Learning. PMLR, 2022.
[5] Wang, Tong, et al. "Large Batch Optimization for Object Detection: Training COCO in 12 minutes." European Conference on Computer Vision. Springer, Cham, 2020.

Review Point: 35. No.9. 2021. [4] Liu, Rui, and Barzan Mozafari. "Communication-efficient Distributed Learning for Large Batch Optimization." International Conference on Machine Learning. PMLR, 2022. [5] Wang, Tong, et al. "Large Batch Optimization for Object Detection: Training COCO in 12 minutes." European Conference on Computer Vision. Springer, Cham, 2020.
==================================================

Focused review:

Weaknesses 1. The parameterizations considered of the value functions at the end of the day belong to discrete time, due to the need to discretize the SDEs and sample the state-action-reward triples. Given this discrete implementa- tion, and the fact that experimentally the authors run into the conven- tional di_x000e_culties of discrete time algorithms with continuous state-action function approximation, I am a little bewildered as to what the actual bene_x000c_t is of this problem formulation, especially since it requires a re- de_x000c_nition of the value function as one that is compatible with SDEs (eqn. (4) ). That is, the intrinsic theoretical bene_x000c_ts of this perspective are not clear, especially since the main theorem is expressed in terms of RKHS only. 2. In the experiments, the authors mention kernel adaptive _x000c_filters (aka kernel LMS) or Gaussian processes as potential avenues of pursuit for addressing the function estimation in continuous domains. However, these methods are fundamentally limited by their sample complexity bottleneck, i.e., the quadratic complexity in the sample size. There's some experimental ref- erence to forgetting factors, but this issue can be addressed in a rigorous manner that preserves convergence while breaking the bottleneck, see, e.g., A. Koppel, G. Warnell, E. Stump, and A. Ribeiro, ``Parsimonious on- line learning with kernels via sparse projections in function space," arXiv preprint arXiv:1612.04111, 2016. Simply applying these methods without consideration for the fact that the sample size conceptually is approaching in_x000c_nity, makes an update of the form (16) inapplicable to RL in general. Evaluating the Bellman operator requires computing an expected value. 3. Moreover, the limited complexity of the numerical evaluation is reflective of this complexity bottleneck, in my opinion. There are far more effective RKHS value function estimation methods than GPTD in terms of value function estimation quality and memory efficiency: A. Koppel, G. Warnell, E. Stump, P. Stone, and A. Ribeiro. ``Policy Evaluation in Continuous MDPs with E_x000e_cient Kernelized Gradient Tem- poral Di_x000b_fference," in IEEE Trans. Automatic Control (submitted), Dec. 2017." It's strange that the authors only compare against a mediocre benchmark rather than the state of the art. 4. The discussion at the beginning of section 3 doesn't make sense or is written in a somewhat self-contradictory manner. The authors should take greater care to explain the di_x000b_erence between value function estimation challenges due to unobservability, and value function estimation problems that come up directly from trying to solve Bellman's evaluation equation. I'm not sure what is meant in this discussion. 5. Also, regarding L87-88: value function estimation is NOT akin supervised learning unless one does Monte Carlo rollouts to do empirical approxima- tions of one of the expectations, due to the double sampling problem, as discussed in R. S. Sutton, H. R. Maei, and C. Szepesvari, \A convergent o(n) temporal- di_x000b_erence algorithm for o_x000b_-policy learning with linear function approxi- mation," in Advances in neural information processing systems, 2009, pp. 1609?1616. and analyzed in great detail in : V. R. Konda and J. N. Tsitsiklis, ``Convergence rate of linear two-timescale stochastic approximation," Annals of applied probability, pp. 796- 819, 2004. 6. The Algorithm 1 pseudo-code is strangely broad so as to be hand-waving. There's no speci_x000c_cs of a method that could actually be implemented, or even computed in the abstract. Algorithm 1 could just as well say "train a deep network" in the inner loop of an algorithm, which is unacceptable, and not how pseudo-code works. Specifically, one can't simply "choose" at random" an RKHS function estimation algorithm and plug it in and assume it works, since the lion-share of methods for doing so either re- quire in_x000c_nite memory in the limit or employ memory-reduction that cause divergence. 7. L107-114 seems speculative or overly opinionated. This should be stated as a remark, or an aside in a Discussion section, or removed. 8. A general comment: there are no transitions between sections, which is not good for readability. 9. Again, the experiments are overly limited so as to not be convincing. GPTD is a very simplistic algorithm which is not even guaranteed to pre- serve posterior consistency, aka it is a divergent Bayesian method. There- fore, it seems like a straw man comparison. And this comparison is con- ducted on a synthetic example, whereas most RL works at least consider a rudimentary OpenAI problem such as Mountain car, if not a real robotics, power systems, or _x000c_financial application.

Review Point: 1. The parameterizations considered of the value functions at the end of the day belong to discrete time, due to the need to discretize the SDEs and sample the state-action-reward triples. Given this discrete implementa- tion, and the fact that experimentally the authors run into the conven- tional di_x000e_culties of discrete time algorithms with continuous state-action function approximation, I am a little bewildered as to what the actual bene_x000c_t is of this problem formulation, especially since it requires a re- de_x000c_nition of the value function as one that is compatible with SDEs (eqn. (4) ). That is, the intrinsic theoretical bene_x000c_ts of this perspective are not clear, especially since the main theorem is expressed in terms of RKHS only.
Review Point: 2. In the experiments, the authors mention kernel adaptive _x000c_filters (aka kernel LMS) or Gaussian processes as potential avenues of pursuit for addressing the function estimation in continuous domains. However, these methods are fundamentally limited by their sample complexity bottleneck, i.e., the quadratic complexity in the sample size. There's some experimental ref- erence to forgetting factors, but this issue can be addressed in a rigorous manner that preserves convergence while breaking the bottleneck, see, e.g., A. Koppel, G. Warnell, E. Stump, and A. Ribeiro, ``Parsimonious on- line learning with kernels via sparse projections in function space," arXiv preprint arXiv:1612.04111, 2016. Simply applying these methods without consideration for the fact that the sample size conceptually is approaching in_x000c_nity, makes an update of the form (16) inapplicable to RL in general. Evaluating the Bellman operator requires computing an expected value.
Review Point: 3. Moreover, the limited complexity of the numerical evaluation is reflective of this complexity bottleneck, in my opinion. There are far more effective RKHS value function estimation methods than GPTD in terms of value function estimation quality and memory efficiency: A. Koppel, G. Warnell, E. Stump, P. Stone, and A. Ribeiro. ``Policy Evaluation in Continuous MDPs with E_x000e_cient Kernelized Gradient Tem- poral Di_x000b_fference," in IEEE Trans. Automatic Control (submitted), Dec. 2017." It's strange that the authors only compare against a mediocre benchmark rather than the state of the art.
Review Point: 4. The discussion at the beginning of section 3 doesn't make sense or is written in a somewhat self-contradictory manner. The authors should take greater care to explain the di_x000b_erence between value function estimation challenges due to unobservability, and value function estimation problems that come up directly from trying to solve Bellman's evaluation equation. I'm not sure what is meant in this discussion.
Review Point: 5. Also, regarding L87-88: value function estimation is NOT akin supervised learning unless one does Monte Carlo rollouts to do empirical approxima- tions of one of the expectations, due to the double sampling problem, as discussed in R. S. Sutton, H. R. Maei, and C. Szepesvari, \A convergent o(n) temporal- di_x000b_erence algorithm for o_x000b_-policy learning with linear function approxi- mation," in Advances in neural information processing systems, 2009, pp. 1609?1616. and analyzed in great detail in : V. R. Konda and J. N. Tsitsiklis, ``Convergence rate of linear two-timescale stochastic approximation," Annals of applied probability, pp. 796- 819, 2004.
Review Point: 6. The Algorithm 1 pseudo-code is strangely broad so as to be hand-waving. There's no speci_x000c_cs of a method that could actually be implemented, or even computed in the abstract. Algorithm 1 could just as well say "train a deep network" in the inner loop of an algorithm, which is unacceptable, and not how pseudo-code works. Specifically, one can't simply "choose" at random" an RKHS function estimation algorithm and plug it in and assume it works, since the lion-share of methods for doing so either re- quire in_x000c_nite memory in the limit or employ memory-reduction that cause divergence.
Review Point: 7. L107-114 seems speculative or overly opinionated. This should be stated as a remark, or an aside in a Discussion section, or removed.
Review Point: 8. A general comment: there are no transitions between sections, which is not good for readability.
Review Point: 9. Again, the experiments are overly limited so as to not be convincing. GPTD is a very simplistic algorithm which is not even guaranteed to pre- serve posterior consistency, aka it is a divergent Bayesian method. There- fore, it seems like a straw man comparison. And this comparison is con- ducted on a synthetic example, whereas most RL works at least consider a rudimentary OpenAI problem such as Mountain car, if not a real robotics, power systems, or _x000c_financial application.
==================================================

Focused review:

First of all, the idea of Dynamic Instance Segmentation is very similar to [1,2]. Both this paper and [1] utilize dynamic convolutions to generate mask kernels for further segmentation, while [2] adopt the AdaIN as an alternative. All of them use coordinate maps as a condition. Second, both this paper and [1] use a unified maks feature representation for all FPN levels. Could the author please clarifry the difference ? - The coordinate information seems to be important according to Tab. 2(b). However, the details of two additional coordinate channels in both mask kernel G and mask feature F are not very clear. - Inference of the propsoed methods is very efficient. However, in Tab. 2(c), the author only discussed the efficiency of Matrix NMS. How about the contribution of dynamic mask representation in terms of speed? Typos: - L229, espically->especially - L287, emphasis ->emphasize - L323, could becomes -> could become [1] Conditional Convolutions for Instance Segmentation. ECCV2020 [2] AdaptIS: Adaptive Instance Selection Network. ICCV 2019

Review Point: - The coordinate information seems to be important according to Tab. 2(b). However, the details of two additional coordinate channels in both mask kernel G and mask feature F are not very clear.
Review Point: - L229, espically->especially - L287, emphasis ->emphasize - L323, could becomes -> could become [1] Conditional Convolutions for Instance Segmentation. ECCV2020 [2] AdaptIS: Adaptive Instance Selection Network. ICCV 2019
==================================================

Focused review:

1. SAC uses exponential moving average on the target network weights, instead of on the predicted values as analyzed in the paper. This isn’t a major difference and could be argued away using Lipschitz constants, but it’s worth mentioning. 2. The goal of SAC and similar algorithms is to find the maximum entropy policy, not the optimal (unregularized) value V^* as analyzed in Theorem 1 and alike. This doesn’t impact the contributions of this paper, but is worth mentioning and analyzing. 3. For continuous actions, finding the Boltzmann policy (eqn (10) here, or eqn (10) in the SAC paper) a very challenging problem. Given that much attention in SAC and similar papers is devoted to solving it, I think it’s worth some discussion.

Review Point: 1. SAC uses exponential moving average on the target network weights, instead of on the predicted values as analyzed in the paper. This isn’t a major difference and could be argued away using Lipschitz constants, but it’s worth mentioning.
Review Point: 2. The goal of SAC and similar algorithms is to find the maximum entropy policy, not the optimal (unregularized) value V^* as analyzed in Theorem 1 and alike. This doesn’t impact the contributions of this paper, but is worth mentioning and analyzing.
Review Point: 3. For continuous actions, finding the Boltzmann policy (eqn (10) here, or eqn (10) in the SAC paper) a very challenging problem. Given that much attention in SAC and similar papers is devoted to solving it, I think it’s worth some discussion.
==================================================

Focused review:

1. The improvement compared to deterministic attention seems marginal on some tasks such as VQA and machine translation. 2. It is not very clear what benefits modeling attention uncertainties give us. Unlike prior works on discrete latent variables that can make interpretability claims, I'm not sure why we want to model soft attention as a latent variable. Are they better under low resource scenarios? Also, can you do more quantification of the benefits of modeling attention uncertainties? Or even qualitatively showing a few examples of samples from the attention distribution and see if they truly reflect the underlying uncertainties. 3. Related to 2, what's the benefit of latent continuous attention over latent discrete attention? The hard attention baseline looks too weak to me, probably a Gumbel-Softmax type discrete attention is a more reasonable baseline. 4. The inference network does not get the true outputs, so the "approximate posterior" can never reach true posteriors. I'm not sure why you need a separate prior in this formulation. IMHO the "approximate posterior" here shall be used as the prior (it is how it's used at test time), which immediately sets the KL term to zero hence the ELBO is better than optimizing with a different prior. This would be the continuous version of Xu et al 2015's "hard attention". Why would using a different prior help (to me it gets the same information as the "approximate posterior" here)? Can you do an ablation study where you get rid of the KL term in Eq. 4? 5. While I can understand that the true KL between attention distributions is not computable, the approximation here is essentially shifting the latent variable modeling from attentions to the unnormalized weights. However, using the "approximate posterior" as the prior gets rid of the KL hence this is no longer a problem. ===post-rebuttal=== My concern about setting prior to the approximate posterior has been addressed in the authors' response. It might be useful to incorporate it into the next version.

Review Point: 1. The improvement compared to deterministic attention seems marginal on some tasks such as VQA and machine translation.
Review Point: 2. It is not very clear what benefits modeling attention uncertainties give us. Unlike prior works on discrete latent variables that can make interpretability claims, I'm not sure why we want to model soft attention as a latent variable. Are they better under low resource scenarios? Also, can you do more quantification of the benefits of modeling attention uncertainties? Or even qualitatively showing a few examples of samples from the attention distribution and see if they truly reflect the underlying uncertainties.
Review Point: 3. Related to 2, what's the benefit of latent continuous attention over latent discrete attention? The hard attention baseline looks too weak to me, probably a Gumbel-Softmax type discrete attention is a more reasonable baseline.
Review Point: 4. The inference network does not get the true outputs, so the "approximate posterior" can never reach true posteriors. I'm not sure why you need a separate prior in this formulation. IMHO the "approximate posterior" here shall be used as the prior (it is how it's used at test time), which immediately sets the KL term to zero hence the ELBO is better than optimizing with a different prior. This would be the continuous version of Xu et al 2015's "hard attention". Why would using a different prior help (to me it gets the same information as the "approximate posterior" here)? Can you do an ablation study where you get rid of the KL term in Eq. 4?
Review Point: 5. While I can understand that the true KL between attention distributions is not computable, the approximation here is essentially shifting the latent variable modeling from attentions to the unnormalized weights. However, using the "approximate posterior" as the prior gets rid of the KL hence this is no longer a problem. ===
==================================================

Focused review:

I think one limitation of this work is the relatively low accuracy reported in Table 1 compared to previous work. For example, 1. In [Ref1, Ref2], the authors reported an accuracy of 84-97% for EWC and 80-93% for GEM on the MNIST Permutation benchmark. The number reported in this paper is much lower than those, only at 62% for EWC and 55% for GEM. 2. For MER, according to [Ref3], the RA is 85.50% on MNIST Permutation, which is much higher than the 73.46% reported here. 3. For GEM, according to [Ref4], the accuracy on MNIST Rotations is at 86% even when we restrict to only 1 epoch per task. The number reported in Table 1 here is only at 67.38%. There seems to be systematic differences between previously reported results and the results in this paper. The authors should explain why there are such differences. [Ref1] Nguyen et al. Variational Continual Learning. 2018. [Ref2] Swaroop et al. Improving and Understanding Variational Continual Learning. 2018. [Ref3] Riemer et al. Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference. 2019. [Ref4] Lopez-Paz et al. Gradient Episodic Memory for Continual Learning. 2017.

Review Point: 2. For MER, according to [Ref3], the RA is 85.50% on MNIST Permutation, which is much higher than the 73.46% reported here.
Review Point: 3. For GEM, according to [Ref4], the accuracy on MNIST Rotations is at 86% even when we restrict to only 1 epoch per task. The number reported in Table 1 here is only at 67.38%. There seems to be systematic differences between previously reported results and the results in this paper. The authors should explain why there are such differences. [Ref1] Nguyen et al. Variational Continual Learning. 2018. [Ref2] Swaroop et al. Improving and Understanding Variational Continual Learning. 2018. [Ref3] Riemer et al. Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference. 2019. [Ref4] Lopez-Paz et al. Gradient Episodic Memory for Continual Learning. 2017.
==================================================

Focused review:

- Little insight into the actual problem domain. How does the new dataset and its labels compare to actual cases of out-of-context captions?
- Lack of a manual inspection of genuine deceptive captions in the dataset, neither of the random or hard samples that were generated nor of model output - Application of a single model with and without fine-tuning
- The introduction could benefit from a clear example of the problem domain - 'falsified' is an ambiguous term to use, as it also refers to the research practice where one tries to falsify a hypothesis.
- Line 111 - 113: 'by retrieving the image of the sample with the greatest textual similarity for a given caption' - it is not clear to me based on this description how these hard negatives were generated

Review Point: - Little insight into the actual problem domain. How does the new dataset and its labels compare to actual cases of out-of-context captions?
Review Point: - Lack of a manual inspection of genuine deceptive captions in the dataset, neither of the random or hard samples that were generated nor of model output - Application of a single model with and without fine-tuning - The introduction could benefit from a clear example of the problem domain - 'falsified' is an ambiguous term to use, as it also refers to the research practice where one tries to falsify a hypothesis.
Review Point: - Line 111 - 113: 'by retrieving the image of the sample with the greatest textual similarity for a given caption' - it is not clear to me based on this description how these hard negatives were generated
==================================================

Focused review:

* See section (Clarity). * 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this). * For better or for worse, there is a lot going on -- and it feel like the pages 5 and 8 were crammed up. I wonder if maybe the theory of OT (pages 3-4)can be reduced to 1 page, so that there can be a more thorough discussion, and explanation of the results and metrics rather than having the reader refer to the Supplementary Material consistently for some minor details.

Review Point: * See section (Clarity).* 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this).
Review Point: * For better or for worse, there is a lot going on -- and it feel like the pages 5 and 8 were crammed up. I wonder if maybe the theory of OT (pages 3-4)can be reduced to 1 page, so that there can be a more thorough discussion, and explanation of the results and metrics rather than having the reader refer to the Supplementary Material consistently for some minor details.
==================================================

Focused review:

weakness: Q1) The authors claim that “Most of the research is focused on adversarial attacks targeting neural network models because of the nature of their continuous learning space…” It is inaccurate statement. We study adversarial example against NN because 1) NN has been outperformed all other methods on many datasets, 2) Its performance is very high, in some cases, even outperforming human experts, 3) DNN has a lot of application and commercial values. However, decision trees are not comparable DNN in all these aspects.
Q2) The authors claim that “Tree-based models continue to be very popular (Nielsen 2016)”. 1) the reference is 6 years ago, and it is a master thesis. Please give better justification on this statement.
Q3) The authors mention that “Of the eleven datasets tested, seven shown a decrease in accuracy”. Decrease in accuracy is not necessary a problem. The question is how much.
Q4) Section 3,2 N1 is not defined although it is understandable.
Q5) Similar idea, extracting information from classifier to train an adversarial example detector has been proposed for DNN. They also have been broken. There is no theory to support that the proposed method is secure.
Q6) Since a lot of methods and tools have been developed for NN for classification, explanation and secure them, the necessity for using decision tree is weakened.

Review Point: 1) NN has been outperformed all other methods on many datasets,
Review Point: 2) Its performance is very high, in some cases, even outperforming human experts,
Review Point: 3) DNN has a lot of application and commercial values. However, decision trees are not comparable DNN in all these aspects.
Review Point: 1) the reference is 6 years ago, and it is a master thesis. Please give better justification on this statement.
==================================================

Focused review:

- While the theoretical bounds are nice, no algorithmic results for efficiently estimating the CRS model is given. I suppose that this all relies directly on the machinery developed for CDM in [48]. - I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix. - One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution. - Given the parameter scaling of the model (quadratic in the number of items) it might not be easily applicable in practice. In fact I find the simulation results confusing. It is not clear why CRS has such nonlinear performance. I do not find the explanation on line 522-527 satisfactory.

Review Point: - While the theoretical bounds are nice, no algorithmic results for efficiently estimating the CRS model is given. I suppose that this all relies directly on the machinery developed for CDM in [48].
Review Point: - I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix.
Review Point: - One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution.
Review Point: - Given the parameter scaling of the model (quadratic in the number of items) it might not be easily applicable in practice. In fact I find the simulation results confusing. It is not clear why CRS has such nonlinear performance. I do not find the explanation on line 522-527 satisfactory.
==================================================

Focused review:

Weaknesses:
Justification for the policy loss function (Equation 9) is unclear.
Comparison with prior art is lacking.
Discussion of related work is sparse and can be more detailed.
Based on the above-mentioned strengths, I vote for accepting. My concerns (further detailed below) potentially can be addressed during the rebuttal phase. ++++++++++++++++++++++++++++++++++
Major Comments
(page 2) The requirement of ‘ability to modify the environment’ is listed as a limitation of prior art (Scobee & Sastry 2020). However, like the current approach, the prior art adds the constraints / modifies the environments only conceptually (and not physically). Further, both the current and prior work focus on the case of hard constraints. Please clarify this limitation of the prior art vis-à-vis proposed approach.
(page 2) The rationale behind the objective (Equation 7) of the prior art and the proposed approach is identical. Please clarify, then, if the current algorithm is also greedy.
(Equation 9) Please provide additional details for the inclusion of the entropy term in the policy loss function.
The principle of maximum entropy is used to arrive at Eq. 4, the loss function of theta (since Eq. 4 uses the term derived in Eq. 2, which in turn is obtained from the maximum entropy principle). Given this, it is unclear why the entropy term is also included in Eq. 9. Is it used as a regularizer?
Alternatively put, consider the unconstrained version of Equation 9. In this unconstrained case, the problem is analogous to MaxEnt IRL (Ziebart et al.). In MaxEnt IRL, given the reward θ
, the policy ϕ
is computed by value / policy iteration and without the extra entropy term.
Further, adding both J and H
in the loss seem counterintuitive as they have different ‘units’. J is cumulative reward, while H is dimensionless entropy. Why is the entropy term normalized by β
? How is the normalization constant chosen?
(Section 4) While not all domains considered in the Experiments can be captured by the prior art (Scobee & Sastry 2020), the first three can be (as they have discrete state, action spaces). Please benchmark the proposed approach with prior art for these three domains. Time permitting, also consider utilizing one of the recent high-dimensional techniques (see below) as another baseline.
(Section 6) Space permitting, please include a discussion of following related works.
Constrained IRL for high-dimensional problems:
Chou, Glen, Necmiye Ozay, and Dmitry Berenson. "Learning parametric constraints in high dimensions from demonstrations." Conference on Robot Learning. PMLR, 2020.
Park, Daehyung, et al. "Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning." Conference on Robot Learning. PMLR, 2020. Notes: Extends beyond the proposed approach to consider constraints which may not be global (i.e., locally active constraints).
Chou, Glen, Necmiye Ozay, and Dmitry Berenson. "Learning constraints from locally-optimal demonstrations under cost function uncertainty." IEEE Robotics and Automation Letters 5.2 (2020): 3682-3690.
Inverse reward / policy learning frameworks that incorporate prior knowledge of reward / policy:
Ramachandran, Deepak, and Eyal Amir. "Bayesian Inverse Reinforcement Learning." IJCAI. Vol. 7. 2007.
Michini, Bernard, and Jonathan P. How. "Bayesian nonparametric inverse reinforcement learning." Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.
Unhelkar, Vaibhav V., and Julie A. Shah. "Learning models of sequential decision-making with partial specification of agent behavior." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.
Jeon, Wonseok, Seokin Seo, and Kee-Eung Kim. "A bayesian approach to generative adversarial imitation learning." Advances in Neural Information Processing Systems. 2018.
Learning features (which can be in the form of logical constraints) for IRL:
Choi, Jaedeug, and Kee-Eung Kim. "Bayesian nonparametric feature construction for inverse reinforcement learning." Twenty-Third International Joint Conference on Artificial Intelligence. 2013. ++++++++++++++++++++++++++++++++++
Questions for Rebuttal Phase
Please address comments 1-4. ++++++++++++++++++++++++++++++++++
Minor Comments
(typo) In the Introduction, Scobee & Sastry is used as singular noun, where in fact it is plural.
(Equation 5) Beta is missing in the log exponential term.
(Page 4, below Equation 7) The statement ‘Notice that … essentially tries to match’ is ambiguous, since the gradient by itself does not try to match the two values. Please consider rephrasing to say that this matching occurs at the minima (where the gradient is zero).
(Page 5, Section 3.3) Please denote the range [ 0 , 1 ] as ( 0 , 1 )
, since 0 and 1 are not in the range of ζ .
(Equation 9) Consider distinguishing the loss functions in Equation 5 and 9 (say through superscript or subscript). Due to L
being overloaded, at first glance, I misunderstood the loss function in Eq 9 as a continued derivation of Eq 5.
(Section 7, typo) (2) -> Eq. (2) ++++++++++++++++++++++++++++++++++

Review Point: 7. 2007. Michini, Bernard, and Jonathan P. How. "Bayesian nonparametric inverse reinforcement learning." Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012. Unhelkar, Vaibhav V., and Julie A. Shah. "Learning models of sequential decision-making with partial specification of agent behavior." Proceedings of the AAAI Conference on Artificial Intelligence. Vol.
==================================================

Focused review:

weaknesses are:
The contribution of the streaming-snapshot model is not critically novel. The essential idea of the streaming-snapshot model is to combine the discrete-time dynamic graph (DTDG) and continue the time dynamic graph (CTDG) model together (see more details in [1]). The authors use both two models but may have different time granularity.
The technical contributions are limited. The whole framework may look interesting and new but it is based on Finn et. al., 2017. The time attention layers are commonly used in temporal graphs which are also proposed in previous work.
The experiments are limited. First of all, the run time analysis is missing from both theoretical and empirical perspectives. More explanations are needed on the results of GL2Vec + ProtoNet. The experimental setup and parameter settings are unclear. Minors:
It is a little bit unfair to directly use CAW and TGAT under the graph metric learning setting. What is the specific loss used for these two?
Table 3, MetaTag → \textsc{MetaTag}
Why directly adding ProtoNet onto CAW and TGAT be a reasonable thing?
I expect detailed experimental setups like parameter tuning in the appendix. Could you list the experimental details in the appendix?
What is the scalability of this method compared with two strong baselines: 1) GL2Vec + ProtoNet and 2) tdGraphEmbed + ProtoNet? It seems that GL2Vec + ProtoNet is competitive on the social network datasets but bad on the bio-based datasets. Why is this case?
[1] Kazemi SM, Goel R, Jain K, Kobyzev I, Sethi A, Forsyth P, Poupart P. Representation Learning for Dynamic Graphs: A Survey. J. Mach. Learn. Res.. 2020 Jan 1;21(70):1-73.

Review Point: 2) tdGraphEmbed + ProtoNet? It seems that GL2Vec + ProtoNet is competitive on the social network datasets but bad on the bio-based datasets. Why is this case? [1] Kazemi SM, Goel R, Jain K, Kobyzev I, Sethi A, Forsyth P, Poupart P. Representation Learning for Dynamic Graphs: A Survey. J. Mach. Learn. Res..
==================================================

Focused review:

- It achieves some improvements over the C51 baseline, but not groundbreaking ones, which in combination with the prior work could turn this into an incremental improvement. The theoretical insight helps against this, but is only a small part of the paper. - It draws some parallels to the DIM paper without (as it seems to me from reading the paper and the supplementary) using a key component of the DIM which is the local-MI maximisation *in combination* with the global MI and a prior term. If I misunderstood the authors this should be made clearer in equation 4,5 and 7 - - In the same vein, the MI maximisation is performed between layers 3 and 4, not between the input and the higher layers as in DIM. While I can think of justifications for this given we are doing RL here and not unsupervised representation learning, I'd prefer the authors do so and make them clear in the paper (see also my bullet points below) - I am missing a comparison with the closest work I know to this (DeepMDP and CURL), especially in terms of what biases their loss terms express - the broader impact is less about this work than about the field of deep RL in general Edit:The most important (comparison with CURL,DeepMDP; deterministic noise experiment) have been addressed

Review Point: - It achieves some improvements over the C51 baseline, but not groundbreaking ones, which in combination with the prior work could turn this into an incremental improvement. The theoretical insight helps against this, but is only a small part of the paper.
Review Point: - It draws some parallels to the DIM paper without (as it seems to me from reading the paper and the supplementary) using a key component of the DIM which is the local-MI maximisation *in combination* with the global MI and a prior term. If I misunderstood the authors this should be made clearer in equation 4,5 and 7 - - In the same vein, the MI maximisation is performed between layers 3 and 4, not between the input and the higher layers as in DIM. While I can think of justifications for this given we are doing RL here and not unsupervised representation learning, I'd prefer the authors do so and make them clear in the paper (see also my bullet points below) - I am missing a comparison with the closest work I know to this (DeepMDP and CURL), especially in terms of what biases their loss terms express - the broader impact is less about this work than about the field of deep RL in general Edit:The most important (comparison with CURL,DeepMDP; deterministic noise experiment) have been addressed
==================================================

Focused review:

In decreasing order of importance: 1. My biggest concern is with the way that $\epsilon_1$ behaves depending on n. Firstly, it seems the choice of the value $-n$ for $\rho$ is arbitrary (with the choice being repercuted in the definition of $\epsilon$), and this should be discussed more clearly in the text. Next, it is not clear to me why the choice $\rho=-n$ is the best. Does it optimize $\epsilon_1$ in some way? Furthermore, as n tends to $\infty$, it seems that $\epsilon_1$ does NOT tend to infinity. This would imply that the bounds in the cases where no good bound on $\epsilon_2$ is available (i.e., the cases which are not covered by the work [8]), the final bound, although not trivial, is not a "high probability" bound in the strict sense of a vanishingly small failure probability. The closest thing to an explanation for this seems to be in the beginning of Section 4, but I could not find the answer to my question there. I assume I misunderstood something here, and I might lower my score in the less likely case that this turns out to be as serious an issue as it superficially appears to me. 2. The idea that fairness improves the bounds is not counter intuitive as claimed in the introduction: it is clear that the fairness assumption which applies to both the ground truth and the function search space reduces the complexity of the problem. (And without a good answer to point one, the improvement would be incremental from the theoretical point of view, although I agree that the experiments section shows improved results in this case). Furthermore, there is no attempt at extending the results to a slightly different setting or applying the results to any well defined machine learning problem, which undermines the relevance of the work to the community. It would also be intresting to try to tackle the case where the ground truth is not fair but the method requires fairness (this is closer to the general paradigm studied in the fairness literature). One could for instance consider the case where the ground is close to satisfying fairness (but does not exactly), as a result of being drawn from a high dimensional distribution whose expectation satisfies the fairness assumption: for instance, suppose the vertices of the graph are people and the attribute is "male/female", with the labels representing failure or success. For a finite graph, it is reasonable to assume that the ground truth labels are drawn from a distribution with each label being independent of gender. This will not necessarily translate to an exactly fair set of ground truth labels such as the ones considered in this work. It is a little underwhelming that such natural situations are not treatable with the results provided in this work. (2. bis. Still no solution for square grids...) And less importantly: 3. Although many of the proofs are impressive and rigorous, I don't feel they are very reader friendly (though it could be explained by a lack of familiarity with the literature on my part). In the Review Section "Additional feedback", I list some aspects which could be explained in greater detail. 4. There are some other very minor issues I list in the Section "Additional feedback".

Review Point: 1. My biggest concern is with the way that $\epsilon_1$ behaves depending on n. Firstly, it seems the choice of the value $-n$ for $\rho$ is arbitrary (with the choice being repercuted in the definition of $\epsilon$), and this should be discussed more clearly in the text. Next, it is not clear to me why the choice $\rho=-n$ is the best. Does it optimize $\epsilon_1$ in some way? Furthermore, as n tends to $\infty$, it seems that $\epsilon_1$ does NOT tend to infinity. This would imply that the bounds in the cases where no good bound on $\epsilon_2$ is available (i.e., the cases which are not covered by the work [8]), the final bound, although not trivial, is not a "high probability" bound in the strict sense of a vanishingly small failure probability. The closest thing to an explanation for this seems to be in the beginning of Section 4, but I could not find the answer to my question there. I assume I misunderstood something here, and I might lower my score in the less likely case that this turns out to be as serious an issue as it superficially appears to me.
Review Point: 3. Although many of the proofs are impressive and rigorous, I don't feel they are very reader friendly (though it could be explained by a lack of familiarity with the literature on my part). In the Review Section "Additional feedback", I list some aspects which could be explained in greater detail.
Review Point: 4. There are some other very minor issues I list in the Section "Additional feedback".
==================================================

Focused review:

weaknesses:
1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification
2) the optimization procedure used to solve the multi-objective optimization problem is not discussed in adequate detail
Detailed comments below:
Methods and Evaluation: The proposed objective is interesting and utilizes ideas from two well studied lines of research, namely, privileged learning and distribution matching to build classifiers that can incorporate multiple notions of fairness. The authors also demonstrate how some of the existing methods for learning fair classifiers are special cases of their framework. It would have been good to discuss the goal of each of the terms in the objective in more detail in Section 3.3. The part that is probably the most weakest in the entire discussion of the approach is the discussion of the optimization procedure. The authors state that there are different ways to optimize the multi-objective optimization problem they formulate without mentioning clearly which is the procedure they employ and why (in Section 3). There seems to be some discussion about the same in experiments section (first paragraph) and I think what was done is that the objective was first converted into unconstrained optimization problem and then an optimal solution from the pareto set was found using BFGS. This discussion is still quite rudimentary and it would be good to explain the pros and cons of this procedure w.r.t. other possible optimization procedures that could have been employed to optimize the objective.
The baselines used to compare the proposed approach and the evaluation in general seems a bit weak to me. Ideally, it would be good to employ baselines that learn fair classifiers based on different notions (E.g., Hardt et. al. and Zafar et. al.) and compare how well the proposed approach performs on each notion of fairness in comparison with the corresponding baseline that is designed to optimize for that notion. Furthermore, I am curious as to why k-fold cross validation was not used in generating the results. Also, was the split between train and test set done randomly? And, why are the proportions of train and test different for different datasets?
Clarity of Presentation:
The presentation is clear in general and the paper is readable. However, there are certain cases where the writing gets a bit choppy. Comments:
1. Lines 145-147 provide the reason behind x*_n being the concatenation of x_n and z_n. This is not very clear.
2. In Section 3.3, it would be good to discuss the goal of including each of the terms in the objective in the text clearly.
3. In Section 4, more details about the choice of train/test splits need to be provided (see above).
While this paper proposes a useful framework that can handle multiple notions of fairness, there is scope for improving it quite a bit in terms of its experimental evaluation and discussion of some of the technical details.

Review Point: 1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification
Review Point: 1. Lines 145-147 provide the reason behind x*_n being the concatenation of x_n and z_n. This is not very clear.
Review Point: 2. In Section 3.3, it would be good to discuss the goal of including each of the terms in the objective in the text clearly.
Review Point: 3. In Section 4, more details about the choice of train/test splits need to be provided (see above). While this paper proposes a useful framework that can handle multiple notions of fairness, there is scope for improving it quite a bit in terms of its experimental evaluation and discussion of some of the technical details.
==================================================

Focused review:

Weakness:
The proposed method requires training T GraphCVAE and another NodeVAE module. It could pose an extra burden in optimization and training costs.
Reproducibility: Perhaps I overlooked this, but I don't seem to find a reference to the code. The proposed method introduces several new modules, and there is certainly some complexity. Since there aren't enough references to the exact implementation details (and hyperparameters) of these modules, I would encourage the authors to consider releasing the code. Question:
Please correct me if I misunderstood, 1). The GraphCVAE only reconstructs node embedding rather than connectivity 2). Given an isolated node during the inference, the model relies solely on NodeVAE for prediction.

Review Point: 1). The GraphCVAE only reconstructs node embedding rather than connectivity
Review Point: 2). Given an isolated node during the inference, the model relies solely on NodeVAE for prediction.
==================================================

Focused review:

Weaknesses: There isn't anything novel in the paper. It consist of an application of an existing technology to a known problem.
The approach described in the paper is not autonomous -- it still needs a human to do the actual scoring. The paper lacks any quantitative or qualitative evaluation of how useful such system is. That is, is it making the job of the scorer easier? Is the scorer more effective as compared to not having automatic score?
The system contains multiple components and it is unclear how the quality of each one of them contributes to the overall experience.
The paper needs more work with the writing. Language and style is rough in several places.
The paper also contains several detailed examples, which don't necessarily add a lot of value to the discussion.
For the evaluation of classification, what is the baseline of predicting the most frequent class?
- General Discussion: I find this paper not very inspiring. I don't see the message in the paper apart from announcing having build such a system

Review Point: -General Discussion: I find this paper not very inspiring. I don't see the message in the paper apart from announcing having build such a system
==================================================

Focused review:

- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
- For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
- It would be better to define some notations and give a clear definition of the "information axis", "word concreteness" and also "Markov chain information content".
- Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE.

Review Point: - The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
Review Point: - For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
Review Point: - It would be better to define some notations and give a clear definition of the "information axis", "word concreteness" and also "Markov chain information content".
Review Point: - Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE.
==================================================

Focused review:

Weaknesses: - In my opinion, the paper is a bit hard to follow. Although this is expected when discussing more involved concepts, I think it would be beneficial for the exposition of the manuscript and in order to reach a larger audience, to try to make it more didactic. Some suggestions: - A visualization showing a counting of homomorphisms vs subgraph isomorphism counting. - It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper. - The authors define rooted patterns (in a similar way to the orbit counting in GSN), but do not elaborate on why it is important for the patterns to be rooted, neither how they choose the roots. A brief discussion is expected, or if non-rooted patterns are sufficient, it might be better for the sake of exposition to discuss this case only in the supplementary material. - The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 “Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets”), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes. - Comparison with GSN: The authors mention in section 2 that F-MPNNs are a unifying framework that includes GSNs. In my perspective, given that GSN is a quite similar framework to this work, this is an important claim that should be more formally stated. In particular, as shown by Curticapean et al., 2017, in order to obtain isomorphism counts of a pattern P, one needs not only to compute P-homomorphisms, but also those of the graphs that arise when doing “non-edge contractions” (the spasm of P). Hence a spasm(P)-MPNN would require one extra layer to simulate a P-GSN. I think formally stating this will give the interested reader intuition on the expressive power of GSNs, albeit not an exact characterisation (we can only say that P-GSN is at most as powerful as a spasm(P)-MPNN but we cannot exactly characterise it; is that correct?) - Also, since the concept of homomorphisms is not entirely new in graph ML, a more elaborate comparison with the paper by NT and Maehara, “Graph Homomorphism Convolution”, ICML’20 would be beneficial. This paper can be perceived as the kernel analogue to F-MPNNs. Moreover, in this paper, a universality result is provided, which might turn out to be beneficial for the authors as well.
Additional comments:
I think that something is missing from Proposition 3. In particular, if I understood correctly the proof is based on the fact that we can always construct a counterexample such that F-MPNNs will not be equally strong to 2-WL (which by the way is a stronger claim). However, if the graphs are of bounded size, a counterexample is not guaranteed to exist (this would imply that the reconstruction conjecture is false). Maybe it would help to mention in Proposition 3 that graphs are of unbounded size?
Moreover, there is a detail in the proof of Proposition 3 that I am not sure that it’s that obvious. I understand why the subgraph counts of C m + 1
are unequal between the two compared graphs, but I am not sure why this is also true for homomorphism counts.
Theorem 3: The definition of the core of a graph is unclear to me (e.g., what if P contains cliques of multiple sizes?)
In the appendix, the authors mention they used 16 layers for their dataset. That is an unusually large number of layers for GNNs. Could the authors comment on this choice?
In the same context as above, the experiments on the ZINC benchmark are usually performed with either ~100K or 500K parameters. Although I doubt that changing the number of parameters will lead to a dramatic change in performance, I suggest that the authors repeat their experiments, simply for consistency with the baselines.
The method of Bouritsas et al., arxiv’20 is called “Graph Substructure Networks” (instead of “Structure”). I encourage the authors to correct this.
After rebuttal
The authors have adequately addressed all my concerns. Enhancing MPNNs with structural features is a family of well-performing techniques that have recently gained traction. This paper introduces a unifying framework, in the context of which many open theoretical questions can be answered, hence significantly improving our understanding. Therefore, I will keep my initial recommendation and vote for acceptance. Please see my comment below for my final suggestions which, along with some improvements on the presentation, I hope will increase the impact of the paper.
Limitations: The limitations are clearly stated in section 1, by mainly referring to the fact that the patterns need to be selected by hand. I would also add a discussion on the computational complexity of homomorphism counting.
Negative societal impact: A satisfactory discussion is included in the end of the experimental section.

Review Point: - In my opinion, the paper is a bit hard to follow. Although this is expected when discussing more involved concepts, I think it would be beneficial for the exposition of the manuscript and in order to reach a larger audience, to try to make it more didactic. Some suggestions:
Review Point: - A visualization showing a counting of homomorphisms vs subgraph isomorphism counting.
Review Point: - It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper.
Review Point: - The authors define rooted patterns (in a similar way to the orbit counting in GSN), but do not elaborate on why it is important for the patterns to be rooted, neither how they choose the roots. A brief discussion is expected, or if non-rooted patterns are sufficient, it might be better for the sake of exposition to discuss this case only in the supplementary material.
Review Point: - The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 “Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets”), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes.
==================================================

Focused review:

Weakness:
I am surprised that the hyperparameter dimension plays no roll in either the assumptions or the results of Section4. This is not the case for several of the cited references, and requires some clarification. For example, when hyperparameter dimension grows, Assumption~2 may not be valid without proper control over d
, or additional assumptions (like sparsity of θ
). If the authors are working in the fixed- d
case, that should be clarified.
The algorithm lacks clarity. In particular, I am confused about exactly how the \textit{spread} of ρ π
and its tails and extremes are being captured. Considerable additional discussions and clarifications are needed about this.
How is m
chosen? Is this optimized in any way? It seems that Algorithm~1 of page 16 can be very sensitive to the choice of m
. The algorithm also requires (at least) M
iid draws from π
, so what is M
and how is this chosen?
The algorithm/theory seems to require that the randomly-weighted log-likelihood L w
should have sufficient smoothness properties near each centroid. Since centroids can be anywhere in the support of the hyperparameter, this requires strong conditions on L w
. One important property that seems to be needed is a locally quadratic behavior, ie, equation (5) needs to hold at every centroid. This seems to be a very strong condition. This is different from the traditional bootstrap conditions, where similar properties are needed only near the true value of the hyperparameter.
Bootstrap is computationally expensive but also embarrassingly parallel. So, while still challenging in applications where there is limited computational resource, the computational burden of bootstrap may be expected to become less of an issue with time.
2 lines after equation (4): "We emphasize that the optimal solution to two-stage learning is guaranteed to be the global minimizer of the loss in (2)." Where are you getting this from?
I would guess that different centroids "stabilize" at different rates, and some centroids are much easier to locate than others. The algorithm seems to treat all centroids equally. Some discussion would be helpful about this.

Review Point: 2 lines after equation (4): "We emphasize that the optimal solution to two-stage learning is guaranteed to be the global minimizer of the loss in (2)." Where are you getting this from? I would guess that different centroids "stabilize" at different rates, and some centroids are much easier to locate than others. The algorithm seems to treat all centroids equally. Some discussion would be helpful about this.
==================================================

Focused review:

The design choices of the temporal set model is not very well explained. 1. It’s not clear to me why you would want to model p(x_t|z_t) again with a neural ODE. Intuitively it seems sufficient to model p(z_t) using a neural ODE, then p(x_t|z_t) with a (conditional) permutation equivariant decoder (e.g. DeepSets or Set Transformer). Using a neural ODE for p(x_t|z_t) seems to pose an unnecessary burden on the time/memory required by the implementation, and to justify this an ablation study that compares different choices of p(x_t|z_t) would be helpful. 2. Using an RNN encoder will only make sense if the data comes at fixed time points [t_0,...,t_N]. Hence this approach would be unable to exploit the benefit of the ODE that can model dynamics at any t. Have you tried using a neural ODE for the encoder as well? 3. How does the VAE model for temporal set modelling compare to just using a single CNF to model x_t for all times t? i.e. you would optimise \sum_{i=1}^T log p(x_t_i) where log p(x_t_i) = log p(x_t_{i-1}) - \int_{t_i}^{t_{i-1}} Tr(df/dx(t)) dt. This would avoid having to use a decoder, and seems to be a much more natural approach to modelling temporal dynamics with a neural ODE. Also the results for the temporal set model are not very convincing: 4. The claim that the ExNODE can extrapolate to unseen time steps for the temporal model seems unjustified. In Figure 5 it looks like it’s struggling to capture the rotation for t > 1 5. The samples don’t look rotation invariant (i.e. the relative positions of the points appear to change with time) When comparing to other baselines, there are no mentions of training times. I believe one weakness of neural ODEs is that they are slow, and so it would be helpful to show how much slower they are compared to DeepSets and Set Transformer, and what would be the implications for scaling up to bigger datasets e.g. Do you expect the temporal model to be feasible to train for rotating ModelNet40 point clouds? Also how does the method compare to PointNet(++), which is a strong baseline for point cloud classification?

Review Point: 1. It’s not clear to me why you would want to model p(x_t|z_t) again with a neural ODE. Intuitively it seems sufficient to model p(z_t) using a neural ODE, then p(x_t|z_t) with a (conditional) permutation equivariant decoder (e.g. DeepSets or Set Transformer). Using a neural ODE for p(x_t|z_t) seems to pose an unnecessary burden on the time/memory required by the implementation, and to justify this an ablation study that compares different choices of p(x_t|z_t) would be helpful.
Review Point: 2. Using an RNN encoder will only make sense if the data comes at fixed time points [t_0,...,t_N]. Hence this approach would be unable to exploit the benefit of the ODE that can model dynamics at any t. Have you tried using a neural ODE for the encoder as well?
Review Point: 3. How does the VAE model for temporal set modelling compare to just using a single CNF to model x_t for all times t? i.e. you would optimise \sum_{i=1}^T log p(x_t_i) where log p(x_t_i) = log p(x_t_{i-1}) - \int_{t_i}^{t_{i-1}} Tr(df/dx(t)) dt. This would avoid having to use a decoder, and seems to be a much more natural approach to modelling temporal dynamics with a neural ODE. Also the results for the temporal set model are not very convincing:
Review Point: 4. The claim that the ExNODE can extrapolate to unseen time steps for the temporal model seems unjustified. In Figure 5 it looks like it’s struggling to capture the rotation for t > 1 5. The samples don’t look rotation invariant (i.e. the relative positions of the points appear to change with time) When comparing to other baselines, there are no mentions of training times. I believe one weakness of neural ODEs is that they are slow, and so it would be helpful to show how much slower they are compared to DeepSets and Set Transformer, and what would be the implications for scaling up to bigger datasets e.g. Do you expect the temporal model to be feasible to train for rotating ModelNet40 point clouds? Also how does the method compare to PointNet(++), which is a strong baseline for point cloud classification?
==================================================

Focused review:

The overall writing is poor. The whole paper lacks logic when organizing their content and is poorly structured, making it difficult to read, especially when they describe their experimental results. It is hard to learn something useful because of the writing.
Suggestions: 1. Add more figures to clarify the ideas.
2. Leave the training details (like how many hours it takes to train) in a single section like "implementation details".
3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result.

Review Point: 2. Leave the training details (like how many hours it takes to train) in a single section like "implementation details".
Review Point: 3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result.
==================================================

Focused review:

weakness of the existing works. 3. In Table 1 we can see that, besides the CAMELYON16 dataset, the baseline MIL-based methods showed much lower performances than max-pooling. Please give some discussions about the reason. 4. For ablation study, The Table 2 and Fig. 5 were not mentioned in the manuscript. What do the values stand for in Table 2 and Fig. 5? Why giving the detailed discussion in Appendix? I suggest move the discussion to the main manuscript. 5. For Fig.6, What is the purpose to show the zoom-in view of heatmap? I cannot see anything special in this area. 6. For Fig. 7, the initial accuracy of MIL-based baseline model were higher than the converged models, especially for the NSCLC dataset, why?

Review Point: 4. For ablation study, The Table 2 and Fig. 5 were not mentioned in the manuscript. What do the values stand for in Table 2 and Fig. 5? Why giving the detailed discussion in Appendix? I suggest move the discussion to the main manuscript.
Review Point: 5. For Fig.6, What is the purpose to show the zoom-in view of heatmap? I cannot see anything special in this area.
Review Point: 6. For Fig. 7, the initial accuracy of MIL-based baseline model were higher than the converged models, especially for the NSCLC dataset, why?
==================================================

Focused review:

weaknesses of the method. Clarity: The paper has been written in a manner that is straightforward to read and follow. Significance: There are two factors which dent the significance of this work. 1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too. 2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain.

Review Point: 1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too.
Review Point: 2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain.
==================================================

Focused review:

Weakness:
I have some concerns on identification mechanism based on identity bank. 1) Scalability. As shown in Table 3 (a), the performance is getting worse with growth of the maximum number of identities. It means that the capacity should be preset to some small number (e.g., 10). In real-world scenario, we can have more than 10 objects and most of the time we don't know how many objects we will need to handle in the future. Have the authors thought about how to scale up without compromising performance? 2) Randomness. Identities are randomly assigned one embedding from the identity bank. How the results are robust against this randomness? It would be undesirable for the result to change with each inference. It would be great to have some analysis on this aspect.
Overall Evaluation:
The paper present a novel approach for multi-object video object segmentation and the proposed method outperfrom previous state-of-the-arts on several benchmarks.
Now, I would recommend to accept this paper. I will finalize the score after seeing how authors address my concerns in Weakness.
While future works are discussed in Supplementary Materials, I encourage the authors to include more discussions on limitations and societal impacts.

Review Point: 1) Scalability. As shown in Table 3 (a), the performance is getting worse with growth of the maximum number of identities. It means that the capacity should be preset to some small number (e.g., 10). In real-world scenario, we can have more than 10 objects and most of the time we don't know how many objects we will need to handle in the future. Have the authors thought about how to scale up without compromising performance?
Review Point: 2) Randomness. Identities are randomly assigned one embedding from the identity bank. How the results are robust against this randomness? It would be undesirable for the result to change with each inference. It would be great to have some analysis on this aspect. Overall Evaluation: The paper present a novel approach for multi-object video object segmentation and the proposed method outperfrom previous state-of-the-arts on several benchmarks. Now, I would recommend to accept this paper. I will finalize the score after seeing how authors address my concerns in Weakness. While future works are discussed in Supplementary Materials, I encourage the authors to include more discussions on limitations and societal impacts.
==================================================

Focused review:

The algorithms are inefficient, and the techniques are not very novel in my view: -Analyzing the log likelihood around the optimum by bounding its strong convexity and its gradient isn't too novel an approach - Using the Yatracos class to learn Ising models has been employed in a recent paper: https://arxiv.org/abs/1806.06887 - In fact, all the technical staff in Section 3 can be circumvented as follows: 1. It is easy to construct an epsilon net in TV distance of all Ising models with at most k edges of size at most (p^2 choose k) (k/eps^2)^k (the way to do this is to use the exact expression for Symmetric KL distance between two Ising models, Pinsker's inequality, and gridding of all edge parameters) 2. Whenever a eps-net of size N exists for a family of distribution, it is known that logN / eps^2 samples from a target distribution in the family suffice to select a distribution from the net that is eps close to the target the error rate of sqrt{k log(p)/n} for learning in TV distance follow from the above observations.. In any event, obtaining the TV to parameter bound and doing robust sparse regression at each node hasn't been done before as far as I know, and it employs some heavy machinery recently shown for the Ising model.

Review Point: 1. It is easy to construct an epsilon net in TV distance of all Ising models with at most k edges of size at most (p^2 choose k) (k/eps^2)^k (the way to do this is to use the exact expression for Symmetric KL distance between two Ising models, Pinsker's inequality, and gridding of all edge parameters) 2. Whenever a eps-net of size N exists for a family of distribution, it is known that logN / eps^2 samples from a target distribution in the family suffice to select a distribution from the net that is eps close to the target the error rate of sqrt{k log(p)/n} for learning in TV distance follow from the above observations.. In any event, obtaining the TV to parameter bound and doing robust sparse regression at each node hasn't been done before as far as I know, and it employs some heavy machinery recently shown for the Ising model.
==================================================

Focused review:

1. It's surprising that no large-scale results (i.e. ImageNet) are shown in the paper. Some previous works were not able to show large-scale results with the excuse of inefficiency of their pruning methods (although there are lots of efficient pruning methods out there), however, I believe the method proposed in this paper is efficient and should be able to run in ImageNet easily. I suspect the conclusion doesn't hold for large-scale problems. 2. The smart-ratio random tickets are concluded in a limited number of neural architectures/datasets, and tested in the same settings. The conclusion may not generalize to other settings. Such as, the sparsity can decrease/remain as the layer goes deeper as shown in Figure 4 [A]. More architectures/datasets should be used as meta-validation setting to validate the generalizability of the method. 3. The hybrid tickets only improve for Cifar-100 not for Cifar-10 in Table 4. [A] Jongsoo, et al. "Faster cnns with direct sparse convolutions and guided pruning." arXiv preprint arXiv:1608.01409 (2016).

Review Point: 1. It's surprising that no large-scale results (i.e. ImageNet) are shown in the paper. Some previous works were not able to show large-scale results with the excuse of inefficiency of their pruning methods (although there are lots of efficient pruning methods out there), however, I believe the method proposed in this paper is efficient and should be able to run in ImageNet easily. I suspect the conclusion doesn't hold for large-scale problems.
Review Point: 2. The smart-ratio random tickets are concluded in a limited number of neural architectures/datasets, and tested in the same settings. The conclusion may not generalize to other settings. Such as, the sparsity can decrease/remain as the layer goes deeper as shown in Figure 4 [A]. More architectures/datasets should be used as meta-validation setting to validate the generalizability of the method.
Review Point: 3. The hybrid tickets only improve for Cifar-100 not for Cifar-10 in Table 4. [A] Jongsoo, et al. "Faster cnns with direct sparse convolutions and guided pruning." arXiv preprint arXiv:1608.01409 (2016).
==================================================

Focused review:

1. The authors unnecessarily frame the method into causal inference. This can be clearly seen in Fig1(b), where the hidden variable of interest: \alpha and \beta are all root nodes, and there are no mediation nodes. Therefore, this renders almost all techniques of causal inference such as do-operation and counterfactuals trivial. Though the authors claim that the unawareness of causal knowledge is a benefit, I suggest the authors change a storyline. 2. Though the discovered \alpha shows the effective interventional effect on the classifier output, it is still un-nameable and coarse-grained, that is to say, any user is still hard to explain the decision of the classifier. It seems that the authors over-claim that \alpha is a more "expressive vocabulary than feature selection" in Line 41 to 45. 3. The mechanism for imposing \beta to cause zero-effect on Y is proved in Proposition 3, which is based on the assumption of Gaussian distribution. However, this assumption is too strong. In fact, for more realistic classifiers, the data is not Gaussian. Therefore, in general, there is no guarantee for the zero-effect of \beta on Y. Correct me if I were wrong. 4. Another downside of \alpha is that it requires the use of the training data. I think it is not practical as other feature selection method, who only requires the trained feature extractor (backbone model). As you know, in a practical scenario, training data is confidential. 5. This paper is essentially about "disentanglement". I suggest the authors compare and discuss the following related work (but not limited to): R1: Counterfactuals uncover the modular structure of deep generative models. R2: Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations R3: Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness R4: Towards a Definition of Disentangled Representations

Review Point: 1. The authors unnecessarily frame the method into causal inference. This can be clearly seen in Fig1(b), where the hidden variable of interest: \alpha and \beta are all root nodes, and there are no mediation nodes. Therefore, this renders almost all techniques of causal inference such as do-operation and counterfactuals trivial. Though the authors claim that the unawareness of causal knowledge is a benefit, I suggest the authors change a storyline.
Review Point: 2. Though the discovered \alpha shows the effective interventional effect on the classifier output, it is still un-nameable and coarse-grained, that is to say, any user is still hard to explain the decision of the classifier. It seems that the authors over-claim that \alpha is a more "expressive vocabulary than feature selection" in Line 41 to 45.
Review Point: 3. The mechanism for imposing \beta to cause zero-effect on Y is proved in Proposition 3, which is based on the assumption of Gaussian distribution. However, this assumption is too strong. In fact, for more realistic classifiers, the data is not Gaussian. Therefore, in general, there is no guarantee for the zero-effect of \beta on Y. Correct me if I were wrong.
Review Point: 4. Another downside of \alpha is that it requires the use of the training data. I think it is not practical as other feature selection method, who only requires the trained feature extractor (backbone model). As you know, in a practical scenario, training data is confidential.
Review Point: 5. This paper is essentially about "disentanglement". I suggest the authors compare and discuss the following related work (but not limited to):
==================================================

Focused review:

- Contributions that are claimed are somewhat weak. In fact, I think that main contribution is in the gradient analysis (as I mentioned in Strengths) and all of the three claimed contributions can be bundled into one "incremental improvements of previous works". Namely: C1: L2-regularization is interesting, but ablation study shows it has the smallest effect on the performance; C2: hybrid similarity measure is a simple combination of two established similarity measures, and it additionally adds another hyper-parameter (alpha) that seems to be very sensitive to setup (Fig5(a) shows that setting lower alpha reduces performance by 1mAP, and setting higher reduces by 0.5 mAP); C3: novel architecture is actually almost identical architecture as [21,22] with addition of FRN block from [36] (ablation study shows this gives the most increase to performance), so it more of a practical combination of previous work than actual contribution. Minor: - Color code Tab 2 with 1st, 2nd and 3rd best result for each column, to be easier for reader to follow, it is a pretty big and unreadable table

Review Point: - Contributions that are claimed are somewhat weak. In fact, I think that main contribution is in the gradient analysis (as I mentioned in Strengths) and all of the three claimed contributions can be bundled into one "incremental improvements of previous works". Namely:
Review Point: - Color code Tab 2 with 1st, 2nd and 3rd best result for each column, to be easier for reader to follow, it is a pretty big and unreadable table
==================================================

Focused review:

The biggest limitation of this work for me, is the experimental setup, specifically (1) the lack of comparison to existing models (2) poor results on text classification and speech act classification when compared to existing work and (3) the choice of benchmarks. - The only baseline approach compared against is BERT. I would recommend reporting results presented in previous work on POS tagging, speech act classification and text classification. This is particularly important since you run your own BERT baselines, it would be for the reader to know how these baselines compare with numbers reported in other papers. For example, [1] reports results on 20Newsgroups and [2,3] on the switchboard dialog act classification dataset and [4,5] on POS tagging. - I am curious why your classification accuracies on the 20newsgroups and switchboard datasets are much lower than what is reported in the literature? For example [1] reports 86.8% accuracy on 20newsgroups while you report only 32.21% for BERT and 51.01 for BERT + Prism. Similarly, [2] reports an accuracy of ~79% for BERT on the switchboard dataset while you report only ~47%. [3] reports a pretty strong non-BERT baseline of 82.9%. These differences are quite large and I wouldn't feel comfortable vouching for this paper unless your reported numbers are in this ballpark. As a sanity check, you could try to see what happens if you don't finetune the initial BERT model on wikitext-103? - Accuracy is not the typical evaluation metric used to evaluate POS-taggers in the literature - micro or macro averaged F1 is typically used. For example, see [4]. - Since Figure 5 demonstrates good performance on long range masked language modeling, LAMBADA might be a good benchmark to validate this. - It would be nice to see ablations where you use high filters on POS tagging and low filters on paragraph/document classification to see the gains that come from choosing the right set of filters for each task. [1] "Neural Attentive Bag-of-Entities Model for Text Classification" [2] "Deep Dialog Act Recognition using Multiple Token, Segment, and Context Information Representations" [3] "Dialogue Act Classification with Context-Aware Self-Attention" [4] "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"

Review Point: - The only baseline approach compared against is BERT. I would recommend reporting results presented in previous work on POS tagging, speech act classification and text classification. This is particularly important since you run your own BERT baselines, it would be for the reader to know how these baselines compare with numbers reported in other papers. For example, [1] reports results on 20Newsgroups and [2,3] on the switchboard dialog act classification dataset and [4,5] on POS tagging.
Review Point: - I am curious why your classification accuracies on the 20newsgroups and switchboard datasets are much lower than what is reported in the literature? For example [1] reports 86.8% accuracy on 20newsgroups while you report only 32.21% for BERT and 51.01 for BERT + Prism. Similarly, [2] reports an accuracy of ~79% for BERT on the switchboard dataset while you report only ~47%. [3] reports a pretty strong non-BERT baseline of 82.9%. These differences are quite large and I wouldn't feel comfortable vouching for this paper unless your reported numbers are in this ballpark. As a sanity check, you could try to see what happens if you don't finetune the initial BERT model on wikitext-103?
Review Point: - Accuracy is not the typical evaluation metric used to evaluate POS-taggers in the literature - micro or macro averaged F1 is typically used. For example, see [4].
Review Point: - Since Figure 5 demonstrates good performance on long range masked language modeling, LAMBADA might be a good benchmark to validate this.
Review Point: - It would be nice to see ablations where you use high filters on POS tagging and low filters on paragraph/document classification to see the gains that come from choosing the right set of filters for each task. [1] "Neural Attentive Bag-of-Entities Model for Text Classification" [2] "Deep Dialog Act Recognition using Multiple Token, Segment, and Context Information Representations" [3] "Dialogue Act Classification with Context-Aware Self-Attention" [4] "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"
==================================================

Focused review:

- I would like to see a bit more experiments, especially regarding its performance, as the performance of the proposed method is not beating other approaches by a wide margin (from looking at pure std, it seems to be overlapping mostly, although the mean is lower, and seems to be more stable on the synthetic example). - I would like to see the result of the existing covariate shift methods with some tuning on the hyperparameters, as the authors proposed the cross val is wasteful of data. Here, they were fixed during experiment, perhaps it will be interesting to see with some tuning, how does the existing baselines compare, especially when authors claim their method is more stable and robust practically, compared to other methods. Even an oracle is ok. - The method proposed only works on SVM and boosting (for classification), though the authors says this can be extended.

Review Point: - I would like to see a bit more experiments, especially regarding its performance, as the performance of the proposed method is not beating other approaches by a wide margin (from looking at pure std, it seems to be overlapping mostly, although the mean is lower, and seems to be more stable on the synthetic example).
Review Point: - I would like to see the result of the existing covariate shift methods with some tuning on the hyperparameters, as the authors proposed the cross val is wasteful of data. Here, they were fixed during experiment, perhaps it will be interesting to see with some tuning, how does the existing baselines compare, especially when authors claim their method is more stable and robust practically, compared to other methods. Even an oracle is ok.
Review Point: - The method proposed only works on SVM and boosting (for classification), though the authors says this can be extended.
==================================================

Focused review:

weaknesses).
Weaknesses - Some parts of the paper are difficult to follow, see also Typos etc below. - Ideally other baselines would also be included, such as the other works discussed in related work [29, 5, 6].
After the Authors' Response My weakness points after been addressed in the authors' response. Consequently I raised my score.
All unclear parts have been answered
The authors' explained why the chosen baseline makes the most sense. It would be great if this is added to the final version of the paper.
Questions - Do you think there is a way to test beforehand whether I(X_1, Y_1) would be lowered more than I(X_2, Y_1) beforehand? - Out of curiosity, did you consider first using Aug and then CF.CDA? Especially for the correlated palate result it could be interesting to see if now CF.CDA can improve. - Did both CDA and MMI have the same lambda_RL (Eq 9) value? From Figure 6 it seems the biggest difference between CDA and MMI is that MMI has more discontinuous phrase/tokens.
Typos, representation etc. - Line 69: Is X_2 defined as all features of X not in X_1? Stating this explicitly would be great. - Line 88: What ideas exactly do you take from [19] and how does your approach differ? - Eq 2: Does this mean Y is a value in [0, 1] for two possible labels? Can this be extended to more labels? This should be clarified. - 262: What are the possible Y values for TripAdvisor’s location aspect? - The definitions and usage of the various variables is sometimes difficult to follow. E.g. What exactly is the definition of X_2? (see also first point above). When does X_M become X_1? Sometimes the augmented data has a superscript, sometimes it does not. In line 131 the meaning of x_1 and x_2 are reverse, which can get confusing - maybe x’_1 and x’_2 would make it easier to follow together with a table that explains the meaning of different variables? - Section 2.3: Before line 116 mentioned the change when adding the counterfactual example, it would be helpful to first state what I(X_2, Y_1) and I(X_1, Y_1) are without it.
Minor points - Line 29: How is desired relationship between input text and target labels defined? - Line 44: What is meant by the initial rationale selector is perfect? It seems if it were perfect no additional work needs to be done. - Line 14, 47: A brief explanation of “multi-aspect” would be helpful - Figure 1: Subscripts s and t should be 1 and 2? - 184: Delete “the”
There is a broader impact section which discusses the limitations and dangers adequately.

Review Point: - Ideally other baselines would also be included, such as the other works discussed in related work [29, 5, 6]. After the Authors' Response My weakness points after been addressed in the authors' response. Consequently I raised my score. All unclear parts have been answered The authors' explained why the chosen baseline makes the most sense. It would be great if this is added to the final version of the paper. Questions - Do you think there is a way to test beforehand whether I(X_1, Y_1) would be lowered more than I(X_2, Y_1) beforehand?
Review Point: - Out of curiosity, did you consider first using Aug and then CF.CDA? Especially for the correlated palate result it could be interesting to see if now CF.CDA can improve.
Review Point: - Line 69: Is X_2 defined as all features of X not in X_1? Stating this explicitly would be great.
Review Point: - Line 88: What ideas exactly do you take from [19] and how does your approach differ?
Review Point: - Eq 2: Does this mean Y is a value in [0, 1] for two possible labels? Can this be extended to more labels? This should be clarified.
Review Point: - 262: What are the possible Y values for TripAdvisor’s location aspect?
Review Point: - The definitions and usage of the various variables is sometimes difficult to follow. E.g. What exactly is the definition of X_2? (see also first point above). When does X_M become X_1? Sometimes the augmented data has a superscript, sometimes it does not. In line 131 the meaning of x_1 and x_2 are reverse, which can get confusing - maybe x’_1 and x’_2 would make it easier to follow together with a table that explains the meaning of different variables?
Review Point: - Section 2.3: Before line 116 mentioned the change when adding the counterfactual example, it would be helpful to first state what I(X_2, Y_1) and I(X_1, Y_1) are without it. Minor points - Line 29: How is desired relationship between input text and target labels defined?
Review Point: - Line 44: What is meant by the initial rationale selector is perfect? It seems if it were perfect no additional work needs to be done.
Review Point: - Line 14, 47: A brief explanation of “multi-aspect” would be helpful - Figure 1: Subscripts s and t should be 1 and 2?
Review Point: - 184: Delete “the” There is a broader impact section which discusses the limitations and dangers adequately.
==================================================

Focused review:

Weaknesses:
A broad issue I have is I'm not convinced "memorization" is the appropriate term for the behavior discussed.
It's not clear to me that every time the correct word is predicted as maximum probability, it should be counted as memorization rather than generalised learning. Also, how is this affected by conditions where the same context appears multiple times in the training set with different labels? If every time a certain sequence occurs in training, it always occurs with a particular word, it seems that it's not memorization to indicate that that's the highest probability word.
What does it mean for a part of speech to be memorized? I don't agree that a part of speech is memorized just because it is predicted correctly in context. There aren't that many parts of speech, so unlike when trying to predict the missing word in a unique sentence, I don't think that it's appropriate to claim that a sequence is memorized just because the predictions made are the correct part of speech. 207 highlights this problem, as it refers to the behavior first as learning and then as memorizing.
209: If the model is just learning to predict that the token is a numeral, and not learning to predict the specific numeral, it’s not clear to me that this is actually going to have privacy implications.
In terms of the uniqueness experiments, I would want to see what the effect of word frequency is in general before considering the role of completely unique words.
174: "classical ML concepts cannot even explain such a memorization trend." Not sure what this means, you should probably talk about the concepts that you feel fail to explain the trend.
I'm not convinced by the experiments that consider memorization relationship with overfitting as though these are two separate phenomena. What if the model is overfitting on a subset of the data, but generalization is improving overall, just not on examples that might be related to that subset? Seems like overfitting by this metric might be an emergent property of having enough memorization occur.
Problems with sparse literature review:
This work is not contextualized in the existing literature on training dynamics in language models, outside of recent literature on general scaling laws. I recommend looking through a variety of work on the area: https://www.semanticscholar.org/search?q=training%20dynamics%20language%20models&sort=relevance
In general, I take issue with the lack of citations to existing work. Is label memorization extending an existing concept? You wouldn't think so, from the paper. There is a mention of spaced repetition from the cognitive science literature, but the authors do not acknowledge that it has also been applied in natural language processing training: https://aclanthology.org/D17-1255/
Many concepts are introduced without any kind of citation to the existing literature: catastrophic forgetting, machine unlearning. Terms like "basin" and "phase transition" are introduced without background, so it's not clear what type of phenomenon each of these phrases is intended to refer to in this context. Minor:
132: "generally monotonically decreasing" just say that it's generally decreasing. It's not monotonically decreasing, and appending "generally" in this context just means "not monotonic".
51: \citep should be \citet here
I didn't feel that the authors were precise enough in their definitions, leading them to use some words, like memorization, in ways that might not reflect what we usually mean when we talk about memorization. I think being more precise in their language would help this paper greatly, as the results that they describe might not influence other notions of memorization.

Review Point: 51: \citep should be \citet here I didn't feel that the authors were precise enough in their definitions, leading them to use some words, like memorization, in ways that might not reflect what we usually mean when we talk about memorization. I think being more precise in their language would help this paper greatly, as the results that they describe might not influence other notions of memorization.
==================================================

Focused review:

I think a lot of comparisons / qualitative analysis is missing: 1. I would like to see a T-SNE plot that shows that the claim that entities with similar (e1,r) contexts are indeed being assigned similar representations is true. 2. There have been a few new KG completion models in the past few years such as KBAT / GAATs that achieve competitive performance on WN18RR/FB15k-237. Would DURA help on these methods as well? I think it's ok if the answer is no, but it would make the paper stronger if a discussion on limitations is added.

Review Point: 1. I would like to see a T-SNE plot that shows that the claim that entities with similar (e1,r) contexts are indeed being assigned similar representations is true.
Review Point: 2. There have been a few new KG completion models in the past few years such as KBAT / GAATs that achieve competitive performance on WN18RR/FB15k-237. Would DURA help on these methods as well? I think it's ok if the answer is no, but it would make the paper stronger if a discussion on limitations is added.
==================================================

Focused review:

Weakness/Questions:
In section 3.1, the authors try to motivate RND by the use case in RL. However, 1) I don't understand why random noise can serve as an exploration bonus; 2) I don't see a clear connection between RL and the diversity of datasets.
The point of RND is to evaluate the generalization gap of features given by a random network. How about we just estimate the entropy of the random features?
Why does RND take the average over predictor networks of different epochs Eq(3)? Why not just take the last predictor?
My biggest concern lies in the experiment part: 1. no experiments show that RND is better than FID; 2. no experiments show the benefit of the random network over the pretrained network (e.g. the one used in FID or IS); 3. no experiments show the necessity of several designs: the normalization and the averaging over different checkpoints.

Review Point: 1) I don't understand why random noise can serve as an exploration bonus;
Review Point: 2) I don't see a clear connection between RL and the diversity of datasets. The point of RND is to evaluate the generalization gap of features given by a random network. How about we just estimate the entropy of the random features? Why does RND take the average over predictor networks of different epochs Eq(3)? Why not just take the last predictor? My biggest concern lies in the experiment part:
Review Point: 1. no experiments show that RND is better than FID;
Review Point: 2. no experiments show the benefit of the random network over the pretrained network (e.g. the one used in FID or IS);
Review Point: 3. no experiments show the necessity of several designs: the normalization and the averaging over different checkpoints.
==================================================

Focused review:

Currently I am giving a score 8, mainly because the idea, motivation and storyline are exciting. But the draft’s Sections 3 & 4 remain unclear in several ways. My final score will depend on how the authors clarify the main questions below: -Section 3 appears to be too “high level” (it shouldn’t be, for the many new things discussed). For example, I was expecting to see how backpropagation was done for the two new layers, but they were unexplained (not even in the supplementary). Also, it is surprising that “fixing shift” as an important extension towards the authors’ claimed “coarse/fine flexibility” only takes five lines in Section 3. A true gem may be overlooked! -Section 4: it is totally unclear what are the dimensions of shift and add layers? For example, when you compare “ShiftAddNet” with ResNet-20, shall I imagine either shift or add layer to have the same dimension as the conv layer, for each layer? Or else? How about DeepShift/AdderNet? Are they fair-comparable to ShiftAddNets in layer/model sizes? - Section 4: The two IoT datasets (FlatCam Face [26], Head-pose detection [11]) are unpopular, weird choices. The former is relatively recent but not substantially followed yet. The latter was published in 2004 and was no longer used much recently. I feel strange why the authors choose the two uncommon datasets, that makes their benchmarking results a bit hard to sense and evaluate. There should have been better options for IoT benchmarking, such as some wearable health or mobile activity recognition data, or even some sets in UCI.

Review Point: - Section 4: The two IoT datasets (FlatCam Face [26], Head-pose detection [11]) are unpopular, weird choices. The former is relatively recent but not substantially followed yet. The latter was published in 2004 and was no longer used much recently. I feel strange why the authors choose the two uncommon datasets, that makes their benchmarking results a bit hard to sense and evaluate. There should have been better options for IoT benchmarking, such as some wearable health or mobile activity recognition data, or even some sets in UCI.
==================================================

Focused review:

Weaknesses
The empirical investigation relies on only one environment (Hopper).
Further suggestions for improvement
The statement "Our algorithm achieves better performance than MOPO in all datasets" should be softened, e.g. "Our algorithm achieves equal or better performance than MOPO in all datasets", because in medium_replay the performance is not significantly better.
In Table 2 and Table 3 standard deviations are given after ±
. Correctly, the ±
sign is used to indicate the uncertainty of the measurement. So a confidence interval or a standard error. This serves to ensure that the statistical significance of differences in the measured values can be easily grasped.
Uncertainties should be stated with one or at most two valid digits.
The number of decimal places in A ±
B must match. E.g. "31.34 ± 0.5" -> "31.3 ± 0.5".
Using only three repetitions (seeds) leads to unreliable results. If it is somehow possible, there should be more, e.g. 10 or 50.
In "Other methods include behavior regularized policy optimization", also (Fujimoto et al., 2019) should be cited.
Similar to the present paper, (Depeweg et al., Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning 2018) estimates uncertainty without ensemble and uses the uncertainty for conservatives in offline RL, it should, therefore, be cited. However, long roll-outs are used for the return estimation and no Q-function is used, so that the approach is structurally clearly different from MOPO.
It is claimed "Conservatism in MBRL is achieved by uncertainty-based penalization of the model predictions." however, this is only one possible way, another possibility is also in model-based the behavior regularized policy optimization, e.g. (Swazinna et al, Overcoming model bias for robust offline deep reinforcement learning, 2021). Presumably other techniques exist, so it is probably better to write, e.g., "Conservatism in MBRL is frequently achieved by uncertainty-based penalization of the model predictions."
It would be interesting to study the behavior in a stochastic environment, because only in stochastic environments the problem exists to separate aleatory and epistemic uncertainty (future work).

Review Point: 10 or 50. In "Other methods include behavior regularized policy optimization", also (Fujimoto et al., 2019) should be cited. Similar to the present paper, (Depeweg et al., Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning 2018) estimates uncertainty without ensemble and uses the uncertainty for conservatives in offline RL, it should, therefore, be cited. However, long roll-outs are used for the return estimation and no Q-function is used, so that the approach is structurally clearly different from MOPO. It is claimed "Conservatism in MBRL is achieved by uncertainty-based penalization of the model predictions." however, this is only one possible way, another possibility is also in model-based the behavior regularized policy optimization, e.g. (Swazinna et al, Overcoming model bias for robust offline deep reinforcement learning, 2021). Presumably other techniques exist, so it is probably better to write, e.g., "Conservatism in MBRL is frequently achieved by uncertainty-based penalization of the model predictions." It would be interesting to study the behavior in a stochastic environment, because only in stochastic environments the problem exists to separate aleatory and epistemic uncertainty (future work).
==================================================

Focused review:

Weaknesses:
Not many, none serious.
I'm not sure I fully understand the contrastive loss function; its description is definitely too terse. Unlike earlier, N
does not appear to be the segment length but some constant. The anchor is apparently chosen from the current segment, but this is not stated. What does it mean for s j p
to be chosen "according to a segment-truncated normal distribution"? I guess it is its rank within the segment that is chosen from a normal distribution centered at the rank of the anchor point. This should also be reflected in the mean of the normal distribution; a
does not make sense. And shouldn't s j a
be excluded from this choice?
"We show in Figure 5 that this parameter is relatively easy to set": If all runs shown used the same parameter value then the results are suggestively reassuring, but they do not show how performance would differ if (say) specially-tuned values had been used.
Some Details:
The fact that the milestone dependency structure must be pre-specified becomes clear pretty late into the paper. This should be noted in the introduction, if not in the abstract.
The elements b g t
(4th row of Sec. 4) are the same as the elements b g , t
near the bottom of p. 3 but the latter uses a lower instead of an upper index. Reading the paper, it is quite obvious that this is in fact the same b
, but it would be nice to make this connection explicit since it is key to understanding the method.
Two crippled sentences on p. 5:
"subtask the others"
"failed to collected"
p. 6, "Context learning": f ∗ ( s i ) = f ∗ ( s j )
should read f ∗ ( s t ) = f ∗ ( s t + 1 )
p. 6, "set used in the KNN procedure": I found this wording misleading and suggest its replacement by " k
closest positive points in D g +
", or perhaps simply " d q k ".
Fig. 5 left: It would be interesting to know why the HAL curve drops sharply upon removal of the 5th milestone.
Fig. 5 center and right: it would be interesting to know how policies trained without edge stochasticity would perform in environments with various levels of edge stochasticity.
Fig. 6: Quantify the depth of each milestone within the task hierarchy.

Review Point: 3 but the latter uses a lower instead of an upper index. Reading the paper, it is quite obvious that this is in fact the same b , but it would be nice to make this connection explicit since it is key to understanding the method. Two crippled sentences on p.
Review Point: 5: "subtask the others" "failed to collected" p. 6, "Context learning": f ∗ ( s i ) = f ∗ ( s j ) should read f ∗ ( s t ) = f ∗ ( s t + 1 ) p. 6, "set used in the KNN procedure": I found this wording misleading and suggest its replacement by " k closest positive points in D g + ", or perhaps simply " d q k ". Fig. 5 left: It would be interesting to know why the HAL curve drops sharply upon removal of the 5th milestone. Fig. 5 center and right: it would be interesting to know how policies trained without edge stochasticity would perform in environments with various levels of edge stochasticity. Fig.
Review Point: 6: Quantify the depth of each milestone within the task hierarchy.
==================================================

Focused review:

1. I'm not convinced by the method of the low-rank constraint, especially setting the rank to a number (C), because I think it could potentially suppress the representation ability of the model. Also, how to select the rank is another problem. 2.More details about the experiments for the ablation study will help us to understand the advantage of the proposed method, especially the one about the rank (for example, how you conduct the comparison experiment with traditional low rank constraint). 3. How do you select lambda_1 and lambda_2? Will you release the code later?

Review Point: 1. I'm not convinced by the method of the low-rank constraint, especially setting the rank to a number (C), because I think it could potentially suppress the representation ability of the model. Also, how to select the rank is another problem.
Review Point: 2.More details about the experiments for the ablation study will help us to understand the advantage of the proposed method, especially the one about the rank (for example, how you conduct the comparison experiment with traditional low rank constraint).
Review Point: 3. How do you select lambda_1 and lambda_2? Will you release the code later?
==================================================

Focused review:

Weaknesses: ========================= Maybe the weakness of the paper is that the originality lies mainly in the targeted representations (UCCA), not really in the proposed parser.
========================= - More detailed comments and clarification questions: ========================= Introduction Lines 46-49: Note that "discontinuous nodes" could be linked to non-projectivity in the dependency framework. So maybe rather state that the difference is with phrase-structure syntax not dependency syntax.
Section 2: In the UCCA scheme's description, the alternative "a node (or unit) corresponds to a terminal or to several sub-units" is not very clear. Do you mean something else than a node is either a terminal or a non terminal? Can't a non terminal node have one child only (and thus neither be a terminal nor have several sub-units) ?
Note that "movement, action or state" is not totally appropriate, since there are processes which are neither movements nor actions (e.g. agentless transformations).
(the UCCA guidelines use these three terms, but later state the state/process dichotomy, with processes being an "action, movement or some other relation that evolves in time").
lines 178-181: Note that the contrast between "John and Mary's trip" and "John and Mary's children" is not very felicitous. The relational noun "children" evokes an underlying relation between two participants (the children and John+Mary), that has to be accounted for in UCCA too.
Section 4: Concerning the conversion procedures: - While it is very complete to provide the precise description of the conversion procedure in the supplementary material, it would ease reading to describe it informally in the paper (as a variant of the constituent-to-dependency conversion procedure à la Manning, 95). Also, it would be interesting to motivate the priority order used to define the head of an edge.
- How l(u) is chosen in case of several children with same label should be made explicit (leftmost ?).
- In the converted graphs in figure 4, some edges seem inverted (e.g. the direction between "John" and "moved" and between "John" and "gave" should be the same).
- Further, I am confused as to why the upper bound for remote edges in bilexical approximations is so low. The current description of the conversions do not allow to get an quick idea of which kind of remote edges cannot be handled.
Concerning the comparison to other parsers: It does not seem completely fair to tune the proposed parser, but to use default settings for the other parsers.
Section 5 Line 595: please better motivate the claim "using better input encoding" Section 6 I am not convinced by the alledged superiority of representations with non-terminal nodes. Although it can be considered more elegant not to choose a head for some constructions, it can be noted that formally co-head labels can be used in bilexical dependencies to recover the same information.

Review Point: - While it is very complete to provide the precise description of the conversion procedure in the supplementary material, it would ease reading to describe it informally in the paper (as a variant of the constituent-to-dependency conversion procedure à la Manning, 95). Also, it would be interesting to motivate the priority order used to define the head of an edge.
Review Point: - How l(u) is chosen in case of several children with same label should be made explicit (leftmost ?).
Review Point: - In the converted graphs in figure 4, some edges seem inverted (e.g. the direction between "John" and "moved" and between "John" and "gave" should be the same).
Review Point: - Further, I am confused as to why the upper bound for remote edges in bilexical approximations is so low. The current description of the conversions do not allow to get an quick idea of which kind of remote edges cannot be handled. Concerning the comparison to other parsers: It does not seem completely fair to tune the proposed parser, but to use default settings for the other parsers. Section 5 Line 595: please better motivate the claim "using better input encoding" Section 6 I am not convinced by the alledged superiority of representations with non-terminal nodes. Although it can be considered more elegant not to choose a head for some constructions, it can be noted that formally co-head labels can be used in bilexical dependencies to recover the same information.
==================================================

Focused review:

1) One question I have is on the computation cost of the proposed model. Since CircleGAN has additional training steps, and needs to update center, disc, and pivot, what is its training speed when comparing to, say SphereGAN? 2) In table 1(b), it seems without great circle learning (model 4), the model can still achieve good performance, even better than model 1 (IS). I'm wondering why is that?

Review Point: 1) One question I have is on the computation cost of the proposed model. Since CircleGAN has additional training steps, and needs to update center, disc, and pivot, what is its training speed when comparing to, say SphereGAN?
Review Point: 2) In table 1(b), it seems without great circle learning (model 4), the model can still achieve good performance, even better than model 1 (IS). I'm wondering why is that?
==================================================

Focused review:

While I like the topic and the contributions in the paper (both modeling and theory) -- the paper itself seems like a work in progress rather than a finished paper, and requires significant reorganization and compression. The main problem is that the paper takes on too much -- and leaves critical information in the appendix (e.g. all the figures for the main experimental results in the body of the paper are left to the appendix). Hence the paper is not self contained (it's published w.o. the appendix, which is mainly for review). Also the paper would be significantly stronger if at least some evidence with more realistic data (beyond the ideal noiseless y=Ax scenario) were considered. Finally, relation to other notions of diversity, and related works in structured sparsity, and related formulations that come to mind is barely discussed. Some concrete suggestions are below: 1) The paper attempts to cover too much content in the space of a NeurIPS 8 pages -- and leaves essential parts to the appendix (experimental results, figures). You discuss: various lemmas about diversity, a general nonlinear formulation, group-based formulation, and finally the k-mer measurement and similarity relaxation. I would suggest to focus on the latter (which is the most interesting), and compress the discussion of the first two to a summary of results. E.g. I was less interested in the details of the general non-convex case and matlab's fmincon optimization. 2) You analyze the diversity measure mathematically, but some additional insight into what it's actually doing would be useful (effectively reducing the counts for repeated organisms), and how does it compare to other diversity measures (e.g. det of similarity matrix, like determinantal point processes, e.t.c). 3) For plain compressed sensing l1-norm is often the most interesting case of the lp, p<=1 family. Here it's excluded. Why? Is the reason that you're looking for non-negative concentration vectors which sum to 1, hence l1-norm is by definition fixed to 1, so optimizing l1-norm is futile? (but p < 1 should still help). This discussion would be helpful, and not present in the paper. This would add the motivation for the reweighted-l1 formulation by Candes et al. 4) Does it always make sense to "merge" similar organisms? For example if the ultimate goal of metagenomic profiling is to discriminate two nearby species, one of which is toxic, and the other is innocuous -- then this would be the worst prior one can use, and make ultimate classification much more difficult. When does the similarity prior make sense? 5) For your final approach (k-mer similarity). How realistic is the prior that similar species are more likely to co-occur? It sounds natural -- but not always true -- for example in an unrelated context -- pepsi and coke are similar products, but are rarely seen together (substitutes vs. complementary). Does it make sense for metagenomic data?

Review Point: 1) The paper attempts to cover too much content in the space of a NeurIPS 8 pages -- and leaves essential parts to the appendix (experimental results, figures). You discuss: various lemmas about diversity, a general nonlinear formulation, group-based formulation, and finally the k-mer measurement and similarity relaxation. I would suggest to focus on the latter (which is the most interesting), and compress the discussion of the first two to a summary of results. E.g. I was less interested in the details of the general non-convex case and matlab's fmincon optimization.
Review Point: 2) You analyze the diversity measure mathematically, but some additional insight into what it's actually doing would be useful (effectively reducing the counts for repeated organisms), and how does it compare to other diversity measures (e.g. det of similarity matrix, like determinantal point processes, e.t.c).
Review Point: 3) For plain compressed sensing l1-norm is often the most interesting case of the lp, p<=1 family. Here it's excluded. Why? Is the reason that you're looking for non-negative concentration vectors which sum to 1, hence l1-norm is by definition fixed to 1, so optimizing l1-norm is futile? (but p < 1 should still help). This discussion would be helpful, and not present in the paper. This would add the motivation for the reweighted-l1 formulation by Candes et al.
Review Point: 4) Does it always make sense to "merge" similar organisms? For example if the ultimate goal of metagenomic profiling is to discriminate two nearby species, one of which is toxic, and the other is innocuous -- then this would be the worst prior one can use, and make ultimate classification much more difficult. When does the similarity prior make sense?
Review Point: 5) For your final approach (k-mer similarity). How realistic is the prior that similar species are more likely to co-occur? It sounds natural -- but not always true -- for example in an unrelated context -- pepsi and coke are similar products, but are rarely seen together (substitutes vs. complementary). Does it make sense for metagenomic data?
==================================================

Focused review:

Weakness 1). There is one key assumption in the sparse coding that the latent code between backbone network and output is sparse when the output image is synthesised at the best performance. However, it may not be necessarily this case. The authors are suggested to prove first that the latent code layer must be a sparse vector (or tensor) 2). Usually, PSNR or SSIM are also used for evaluating the image restoration tasks. It would be great to provide the scores of these two traditional metrics in the paper in order to have an all-around evalutions. 3). The paper does not clearly disclose the detailed parameters of the proposed method, for example, what is the selected threshold λ
value in L0 norm? And the reason to use these threshold ? What are the weighting terms of the three different normalization constraints?

Review Point: 1). There is one key assumption in the sparse coding that the latent code between backbone network and output is sparse when the output image is synthesised at the best performance. However, it may not be necessarily this case. The authors are suggested to prove first that the latent code layer must be a sparse vector (or tensor)
Review Point: 2). Usually, PSNR or SSIM are also used for evaluating the image restoration tasks. It would be great to provide the scores of these two traditional metrics in the paper in order to have an all-around evalutions.
Review Point: 3). The paper does not clearly disclose the detailed parameters of the proposed method, for example, what is the selected threshold λ value in L0 norm? And the reason to use these threshold ? What are the weighting terms of the three different normalization constraints?
==================================================

Focused review:

weaknesses:
a) The novelty in this paper is quite limited. The results are built on the large body of bi-level optimization literature and super-ADAM. Noticeable, there are already many works for bi-level optimization can achieve the same complexities.
b) The adaptive framework is not very meaningful here. Note that all the adaptivity is absorbed in the matrices A t and B t
in the algorithms. The stepsize can be considered to be inversely related with eigenvalues of matrices. In Assumption 7, it assumes A t ⪰ ρ I d and B t = b I p ( b u ≥ b ≥ b l > 0 )
, which basically indicates stepsize for x
is upper bounded and stepsize for y
is lower and upper bounded. Then by controlling the other parameters in the stepsizes, e.g., λ and γ
, the stepsizes can be treated just like non-adaptive stepsizes. Therefore, the analysis does not need too much novelty to accommodate adaptive stepsize, e.g., Lemma 10. This leads to some drawbacks: 1) upper and lower bounds for adaptive matrices A t and B t
need to be know in order to pick other hyperparameters, 2) usually, ρ
in Adam is chosen to be very small just for stability. But when ρ
is small, Theorem 1 needs a very small γ
which may reduce the gain from Adam stepsize.

Review Point: 1) upper and lower bounds for adaptive matrices A t and B t need to be know in order to pick other hyperparameters,
Review Point: 2) usually, ρ in Adam is chosen to be very small just for stability. But when ρ is small, Theorem 1 needs a very small γ which may reduce the gain from Adam stepsize.
==================================================

Focused review:

Weakness: 1 In general, this paper lacks novelty. Filter pruning, as well as the regularization term and schedule, are common techniques in high-level vision tasks, and this paper applies them to the SR task. The difference might be the pruning criterion. Indeed, the extensive residual connections cause the main difficulty to directly apply these techniques to the SR task. However, the authors do not include deep investigations to this problem, e.g., experiments of direct application, insightful analysis about the intrinsic reasons, etc. In addition, to solve this problem, the authors randomly select a set of filters to be pruned and fix them during pruning. This may affect the stability of training, where no repetitive experiments are provided. 2 In Table 3, the SRPN is pruned from an extended version of RCAN (with 96 channels) and achieves state-of-the-art performance. This is logical since the extended RCAN is more powerful and contains redundant parameters. It is better to, the performance of the extended RCAN should also be provided. 3 The authors claim a general idea to prune SR networks, especially for large networks. However, only the pruning of RCAN (without channel attention) is validated. To demonstrate the generalization capacity, more networks with different topologies should be included.

Review Point: 1 In general, this paper lacks novelty. Filter pruning, as well as the regularization term and schedule, are common techniques in high-level vision tasks, and this paper applies them to the SR task. The difference might be the pruning criterion. Indeed, the extensive residual connections cause the main difficulty to directly apply these techniques to the SR task. However, the authors do not include deep investigations to this problem, e.g., experiments of direct application, insightful analysis about the intrinsic reasons, etc. In addition, to solve this problem, the authors randomly select a set of filters to be pruned and fix them during pruning. This may affect the stability of training, where no repetitive experiments are provided.
Review Point: 2 In Table 3, the SRPN is pruned from an extended version of RCAN (with 96 channels) and achieves state-of-the-art performance. This is logical since the extended RCAN is more powerful and contains redundant parameters. It is better to, the performance of the extended RCAN should also be provided.
Review Point: 3 The authors claim a general idea to prune SR networks, especially for large networks. However, only the pruning of RCAN (without channel attention) is validated. To demonstrate the generalization capacity, more networks with different topologies should be included.
==================================================

Focused review:

1) Novelty: At the end, there is very little in the way of a fundamentally new idea. The use of SVD in deep learning, and the observation that it can be represented efficiently via Householder reflections, was already developed in [18]. WY factorization was already developed in 1987 also, I think in order to solve similar problems. The authors simply adjust the tool to the job. 2) Limited applicability: seems that the technique applies only to layers whose #input neurons is equal to #output neurons. 3) Experiments: very weak. They do not explore deep learning at all, but rather focus on the time to do a single step and the cost of various matrix operations. I am curious to see how end-to-end learning pans out. 4) Discussion of memory complexity is a bit neglected. There are some discussions throughout, but I do not feel they are complete. Is there a memory-parallelization tradeoff? The start of section 3.3 discusses only time complexity and parallelism tradeoff.

Review Point: 1) Novelty: At the end, there is very little in the way of a fundamentally new idea. The use of SVD in deep learning, and the observation that it can be represented efficiently via Householder reflections, was already developed in [18]. WY factorization was already developed in 1987 also, I think in order to solve similar problems. The authors simply adjust the tool to the job.
Review Point: 2) Limited applicability: seems that the technique applies only to layers whose #input neurons is equal to #output neurons.
Review Point: 3) Experiments: very weak. They do not explore deep learning at all, but rather focus on the time to do a single step and the cost of various matrix operations. I am curious to see how end-to-end learning pans out.
Review Point: 4) Discussion of memory complexity is a bit neglected. There are some discussions throughout, but I do not feel they are complete. Is there a memory-parallelization tradeoff? The start of section 3.3 discusses only time complexity and parallelism tradeoff.
==================================================

Focused review:

After reading the rebuttal, may concern is as follows. 1. The observation lacks of detailed explanations. I agree the observation is interesting, but it is pity that there is no theoretical discussion on it. In fact, theoretical discussions will make the paper from an average OK submission to an excellent one. 2. The proposed method is simple. If the paper has an extension, I would like to see the comparison with other trad dictional simple methods in image feature description, e,g,，BoF, LBP, etc.

Review Point: 1. The observation lacks of detailed explanations. I agree the observation is interesting, but it is pity that there is no theoretical discussion on it. In fact, theoretical discussions will make the paper from an average OK submission to an excellent one.
Review Point: 2. The proposed method is simple. If the paper has an extension, I would like to see the comparison with other trad dictional simple methods in image feature description, e,g,，BoF, LBP, etc.
==================================================

Focused review:

The empirical evaluations in this work are difficult to interpret, and I am not sure what the key takeaways are. The metrics used in this work (AUC score, recall at K) are non-standard for the tasks under consideration, so it is difficult to interpret the results in the context of word-level QE evaluation. Although it is interesting that sentence-level QE models contain some information about probable word-level translation errors, it is unsurprising that sentence level models would learn to focus on target tokens that are likely to be wrong and I think the paper at least needs more analysis or insight into the details of what is going on to be useful for other researchers in the context of QE (see comments for some ideas).
As presented, the work cannot be framed as a new approach to word-level QE because it does not use the same evaluation metrics, and the approach cannot label tokens in a MT hypothesis as Good/Bad, so any comparison with supervised word-level QE approaches is not informative.
Possible Analyses / experiments: - what types of errors do the feature attribution models find/miss, what do these models tell us about what sentence-level models are learning? - what are the dynamics between source and target token attribution? do some inputs attribute more responsibility to the source, while others attribute more to the target hypotheses? why? ( the notion of an "attribution budget" could be useful here) - do feature attribution approaches completely miss some types of word-level errors? does this show that sentence level models aren't learning that information? or does it indicate a problem with the data generation method of the word-level QE task?

Review Point: - what types of errors do the feature attribution models find/miss, what do these models tell us about what sentence-level models are learning?
Review Point: - what are the dynamics between source and target token attribution? do some inputs attribute more responsibility to the source, while others attribute more to the target hypotheses? why? ( the notion of an "attribution budget" could be useful here) - do feature attribution approaches completely miss some types of word-level errors? does this show that sentence level models aren't learning that information? or does it indicate a problem with the data generation method of the word-level QE task?
==================================================

Focused review:

- Results with an unsupervised clustering algorithm (oblivious to which of the four learning algorithms an example belongs to) would have been a nice plus. - Ideal observer approach not immediately translatable to a 'non-virtual' experiment. - Generalization experiments are appropriate, but could be improved. In particular all models are trained on the same dataset (admittedly, lifting this constraint might not prove easy) and all network models considered are rather similar.

Review Point: - Results with an unsupervised clustering algorithm (oblivious to which of the four learning algorithms an example belongs to) would have been a nice plus.
Review Point: - Ideal observer approach not immediately translatable to a 'non-virtual' experiment.
Review Point: - Generalization experiments are appropriate, but could be improved. In particular all models are trained on the same dataset (admittedly, lifting this constraint might not prove easy) and all network models considered are rather similar.
==================================================

Focused review:

Weakness:
Major Concern #1 Novelty of the theoretical result.
The paper mentioned several times (e.g. first paragraph of related work) that existing theory of domain adaptation e.g. [1] is limited to the case of binary labels. While the results shown in [1] are indeed for the binary classification setting, I do not necesarily agree this limitation exists, in [1] this was done for simplicity as it is typical, the extension to the multiclass setting just follows. Moreover, the multi-class setting using a novel discrepancy measure that extends [1] was analyzed in [2]. In [3] the authors further extend the results from [1] to general f-divergences and also derived rigorous generalization bounds. The family of f-divergences include KL and r KL as particular cases. A lot of work has also be done since [1] (2010) for example [1,2,3,4] and comparison, improvement wrt to them is not discussed .
Moreover, if the goal is to show a bound where the discrepancy between marginals is estimated using the KL divergence rather than L1 (as in [1]) that could be obtained by directly applying Pinsker's inequality on top of [1] Theorem 1. How would that compare to the current setting?
Major Concern #2 Significance of the derived bound.
One of the major motivation for the H-divergence introduced in [1] vs L1 and then several authors using hypot-based divergences such as [2,3,4]. It is that (quoting [1]) "L1 is an overly strict measure that unnecessarily inflates the bound". Pinsker's inequality shows KL upper bounds L1. Wouldnt the direct use of the KL then inflates more the bound? . How does writing a generalization bound using the KL divergence which is further ubounded solves this problem?
Major Concern #3 Comparison of the algorithm with modern baselines. The latest baseline WD is from 2018. Why there is no comparison to recent work in domain-adaptation? I would also recommend the authors to do experiments in the office-31 dataset as it has become standard practice. This will also make the comparison vs existing work easier.
Major Concern #4 No experimental analysis is provided.
[1] A theory of learning from different domains. Springer 2010
[2] Bridging theory and algorithm for domain adaptation. ICML 2019
[3] f-Domain Adversarial Learning: Theory and Algorithms. ICML 2021
[4] On learning invariant representations for domain adaptation,ICML 2020
[5] Office-31 Dataset: https://www.cc.gatech.edu/~judy/domainadapt/

Review Point: . How does writing a generalization bound using the KL divergence which is further ubounded solves this problem? Major Concern #3 Comparison of the algorithm with modern baselines. The latest baseline WD is from 2018. Why there is no comparison to recent work in domain-adaptation? I would also recommend the authors to do experiments in the office-31 dataset as it has become standard practice. This will also make the comparison vs existing work easier. Major Concern #4 No experimental analysis is provided. [1] A theory of learning from different domains. Springer 2010 [2] Bridging theory and algorithm for domain adaptation. ICML 2019 [3] f-Domain Adversarial Learning: Theory and Algorithms. ICML 2021 [4] On learning invariant representations for domain adaptation,ICML 2020 [5] Office-31 Dataset: https://www.cc.gatech.edu/~judy/domainadapt/
==================================================

Focused review:

1. The approach is derivative, meaning it takes two existing approaches and combines them. It's not bad. I'm fine with derivatives if they work.
2. I have a feeling that the authors also took two precautions against immediate derivatives from their approach, and that gave them a couple of pages to add to their paper: paragraphs 3.3.1 Softmax-Based Mechanism and 3.3.2 Entropy-Based Mechanism explain why they did not use the softmax score and the entropy score, and that gave them one more paragraph: 4.1.1 Ablation Studies. But I'm also fine with this strategy.
Consequently, earlier exists require -> exits Our method is easily.. orthogonal to many other existing methods (and later: To investigate the orthogonality of E-LANG with others)- I don't really get the meaning of 'orthogonal' and 'orthogonality' in this context. Seems that it is in the same direction with the general trends.
and are also applicable for encoder-decoder -> is also

Review Point: 1. The approach is derivative, meaning it takes two existing approaches and combines them. It's not bad. I'm fine with derivatives if they work.
==================================================

Focused review:

Weakness
The impt mask defined in Equ. 2 and used in Equ. 3 is a little weird. It seems that the elements in impt mask are not 0/1 and could be extremely larger than 1. Therefore, multiplying this mask to the original weight matrix may strongly affect the functionality of the original modular (i.e., the adapter), and the gradients (which will be used to calculate the importance score) can also be influenced.
Lack of further analysis of the mechanism behind TST. I suppose the most important and interesting part in this work is the designs in Equ. 2-7. I agree that these designs sounds good, but I feel it is too amazing for a neural model to perform in a so interpretable way. This is my major concerns to this work. I list several concrete questions as follows and will change my score according to your response. Questions
Q1: Did you cherry pick the visualization in Figure 3? Your analysis in 'Effectiveness of task similarity detection' tried to demonstrate that the actions (or called the choices of masks) in TST are in line with the intuition that the similar tasks share the mask while dissimilar tasks choose different masks. Therefore, I'm wondering if these analysis are from the cherry picked model or general ones. For instance, for the two tasks 'icsi' and 'ami' in SUM, when the random seed and task sequence order are changed, can TST still find them similar?
Q2: Does Equ. 7 work correctly at the beginning of the task sequences? Since masks are randomly initialized, the importance scores calculated in the first several tasks could be hugely affected by this randomness, and the comparison in Equ. 7 might also be influenced. Moreover, I notice that for (a) (b) (e) in Figure 3, TST found most similar tasks at the beginning of the task sequences. Therefore, I'm wondering whether these 'similar tasks' are superficial correlations according to their order in task sequences.
Minor Questions
How are the adapter used in TST trained? Since the paper claims that only the mask is trained, I wonder if the adapter is just randomly initialized without further training?

Review Point: 3 is a little weird. It seems that the elements in impt mask are not 0/1 and could be extremely larger than 1. Therefore, multiplying this mask to the original weight matrix may strongly affect the functionality of the original modular (i.e., the adapter), and the gradients (which will be used to calculate the importance score) can also be influenced. Lack of further analysis of the mechanism behind TST. I suppose the most important and interesting part in this work is the designs in Equ. 2-7. I agree that these designs sounds good, but I feel it is too amazing for a neural model to perform in a so interpretable way. This is my major concerns to this work. I list several concrete questions as follows and will change my score according to your response. Questions Q1: Did you cherry pick the visualization in Figure 3? Your analysis in 'Effectiveness of task similarity detection' tried to demonstrate that the actions (or called the choices of masks) in TST are in line with the intuition that the similar tasks share the mask while dissimilar tasks choose different masks. Therefore, I'm wondering if these analysis are from the cherry picked model or general ones. For instance, for the two tasks 'icsi' and 'ami' in SUM, when the random seed and task sequence order are changed, can TST still find them similar?
Review Point: 7 might also be influenced. Moreover, I notice that for (a) (b) (e) in Figure 3, TST found most similar tasks at the beginning of the task sequences. Therefore, I'm wondering whether these 'similar tasks' are superficial correlations according to their order in task sequences. Minor Questions How are the adapter used in TST trained? Since the paper claims that only the mask is trained, I wonder if the adapter is just randomly initialized without further training?
==================================================

Focused review:

* Although the general ML for communications area is relevant for neurips, this particular paper seems too specific to be of general interest, and may fit better in a venue like IEEE Trans. Commun. * There is some confusion between multi-armed bandit and reinforcement learning in this work * No explanation/intuition of why this dynamic scheduling helps is given: some greater insight from [3] would have set the stage better for this work

Review Point: * Although the general ML for communications area is relevant for neurips, this particular paper seems too specific to be of general interest, and may fit better in a venue like IEEE Trans. Commun.
Review Point: * There is some confusion between multi-armed bandit and reinforcement learning in this work * No explanation/intuition of why this dynamic scheduling helps is given: some greater insight from [3] would have set the stage better for this work
==================================================

Focused review:

Weaknesses
Unfortunately, this paper has many weaknesses. First, and importantly, the main contribution of this paper (section 5.3) is not well motivated and the explanation of the contribution itself is really handwavy. Second, explanation of the different baseline approaches is also too handwavy, making the overall paper difficult to understand and not self-contained.
On the motivations: the authors argue that previous approaches cannot be applied to structured prediction. I strongly disagree with this.
l 227, "the output distributiion of global prediction is often intractable": in the non-projective dependency parsing case, the distribution can be computed using the Matrix Tree Theorem [1, 2, 3]. If we restrict the problem to projective trees, this can be computed via dynamic programming [4, 5]. For the transition based model, approximated approaches have been explored in the litterature [6]
l. 222, "the instantiations of KD are usually based on a loss function computing the cross-entropy of output distributions [...] however existing methods are no directly applicable to structure prediction tasks": I don't understand why. First, the next sentence is false (see previous point), but also KL divergence between structured distributions has been studied in the litterature. For non-projective trees, see [7, Section 6.3], for methods based on dynamic programming see [8]
l. 263, "furthermore, it is unclear how to adapt existing sampling methods to solutions that are not formalized as sequence generation": Recent work has considered sampling from the non-projective tree distributions, including sampling without replacement [9, 10]. Moreover, previous work has also considered perturb-and-MAP approaches [11, 12, 13]. Finally, in the case of dynamic programming algorithms, it is well known that it is possible to sample from the associated exponential familly distributions, see e.g. [14]
Related work
As suggested by the comment above, the literature is not properly explored or cited by the authors. There are similar problems in the introduction and related work section. For example:
l. 54: authors cite Dozan and Manning (2017) for graph-based parsers, whereas the correct citations is more than 10 years older [15]
l. 21: for transition-based parsers, they cite Ma et al. (2018), better citations would be [16, 17]
[1] Structured Prediction Models via the Matrix-Tree Theorem (Koo et al.)
[2] Probabilistic Models of Nonprojective Dependency Trees (Smith and Smith)
[3] On the complexity of non-projective data-driven dependency parsing (McDonald and Satta)
[4] Semiring parsing (Goodman)
[5] Differentiable Dynamic Programming for Structured Prediction and Attention (Mensch and Blondel)
[6] Globally Normalized Transition-Based Neural Networks (Andor et al.)
[7] Efficient Computation of Expectations under Spanning Tree Distributions (Zmigrod et al.)
[8] First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests (Li and Eisner)
[9] Efficient Sampling of Dependency Structures (Zmigrod et al.)
[10] Unbiased and Efficient Sampling of Dependency Trees (Stanojevi)
[11] Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models (Papandreou and Yuille)
[12] Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder (Corro and Titov)
[13] Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming (Corro and Titov)
[14] See section 17.4.5 of Machine Learning: A Probabilistic Perspective (Muprhy) for the idea and Latent Template Induction with Gumbel-CRFs (Fu et al.) for application to CRF-like distribution
[15] Non-projective Dependency Parsing using Spanning Tree Algorithms (McDonald et al.)
[16] A Classifier-Based Parser with Linear Run-Time Complexity (Sagae, Lavie)
[17] Algorithms for Deterministic Incremental Dependency Parsing (Nivre)

Review Point: 54: authors cite Dozan and Manning (2017) for graph-based parsers, whereas the correct citations is more than 10 years older [15] l.
Review Point: 21: for transition-based parsers, they cite Ma et al. (2018), better citations would be [16, 17] [1] Structured Prediction Models via the Matrix-Tree Theorem (Koo et al.) [2] Probabilistic Models of Nonprojective Dependency Trees (Smith and Smith) [3] On the complexity of non-projective data-driven dependency parsing (McDonald and Satta) [4] Semiring parsing (Goodman) [5] Differentiable Dynamic Programming for Structured Prediction and Attention (Mensch and Blondel) [6] Globally Normalized Transition-Based Neural Networks (Andor et al.) [7] Efficient Computation of Expectations under Spanning Tree Distributions (Zmigrod et al.) [8] First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests (Li and Eisner) [9] Efficient Sampling of Dependency Structures (Zmigrod et al.) [10] Unbiased and Efficient Sampling of Dependency Trees (Stanojevi) [11] Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models (Papandreou and Yuille) [12] Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder (Corro and Titov) [13] Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming (Corro and Titov) [14] See section 17.4.5 of Machine Learning: A Probabilistic Perspective (Muprhy) for the idea and Latent Template Induction with Gumbel-CRFs (Fu et al.) for application to CRF-like distribution [15] Non-projective Dependency Parsing using Spanning Tree Algorithms (McDonald et al.) [16] A Classifier-Based Parser with Linear Run-Time Complexity (Sagae, Lavie) [17] Algorithms for Deterministic Incremental Dependency Parsing (Nivre)
==================================================

Focused review:

1. The proposed method seems only works for digit or text images, such as MNIST and SVHN. Can it be used on natural images, such as CIFAR10, which has wider applications in the real world then digit/text. 2. Are the results obtained on Synbols dataset generalizable to large-scale datasets? For example, if you find algorithm A is better than B on Synbols dataset, will the conclusion hold on large images (e.g., ImageNet scale) in real-world applications? This need to be discussed in the paper.

Review Point: 1. The proposed method seems only works for digit or text images, such as MNIST and SVHN. Can it be used on natural images, such as CIFAR10, which has wider applications in the real world then digit/text.
Review Point: 2. Are the results obtained on Synbols dataset generalizable to large-scale datasets? For example, if you find algorithm A is better than B on Synbols dataset, will the conclusion hold on large images (e.g., ImageNet scale) in real-world applications? This need to be discussed in the paper.
==================================================

Focused review:

Weaknesses (this summarizes the overall weaknesses, please see comments below for details) Clarity:
The terms independence, invariance, disentanglement, alignment have been used (sometimes interchangeably) without really defining / describing them. For this paper this needs to be defined and discussed.
The related work reads more like a background section. It would be clearer if you can compare against past work. How are you different, how are you similar?
Motivation and significance:
The model builds on three existing approaches that should be correctly acknowledged: 1) The weakly-supervised disentanglement by Locatello et al. [8] is enhanced by the detecting & swapping approach. It is not clear how much the detecting & swapping approach advances the disentanglement performance. 2) A supervised contrastive approach similar to [7] and 3) using an average over class-specific representations [1, 8]. I think an extension by combining these approaches is totally okay w.r.t. novelty, if you can show that this combination enhances overall performance. However, the authors should clarify which model their model is based on, e.g., by comparing their model to the existing approaches in the related work and mentioning it in the main methodology section.
The overall picture is missing (and it also reflects in the evaluation): Why is it essential to separate class from content (I assumed this is the goal; however, the paper is not clear on this)? Which applications would benefit from it this?
It is unclear whether the goal is to separate/disentangle predictive and nuisance representation: The predictive representation is averaged for having a common class-specific representation, but does that mean that the nuisance representation is independent of it?
Empirical evaluation:
The empirical evaluation does not necessarily show the advantages of training both disentanglement and predictive part. Can you think about applications where it is important to have this separation (e.g., fairness, robustness)?
The training uses common disentanglement datasets, which are good for evaluating disentanglement performance. However, they are synthetic, and it is not necessarily granted that the performance will generalize for real-world datasets. Can you show that this method can generalize to real-world datasets, e.g., CIFAR-10/100, ImageNet, CelebA, etc.?
The comparison with existing disentanglement is not fair as all comparisons are unsupervised or weakly-supervised, whereas the proposed model is supervised.
It is not clear how well the predictive part z p
is disentangled from the nuisance part z n
. Can you quantify this? As this is the overall goal, however, not shown in the evaluation, this decreases the impact of this paper.
Detailed comments
Concerning this work, can you explain what disentangled and invariant representations are? As there are different definitions or notions of both terms, it would be good to clarify that. What is the difference between these two?
[Sec. 1, 1st paragraph] "Typically, a DNN learns to encode representations which contain all factors [...]. Disentangled representation learning and invariant representation learning are often used to address these challenges." Can you add the references to these works you are referring to? Further, this statement is not quite correct. Disentangled representation also learns to encode all factors of variations. However, disentanglement refers to the factors of variation being disentangled in the representation space.
[Sec. 1, 2nd paragraph] "invariant representation learning [...] in which they split representation z
into two parts [...]": Some references are missing here [1, 2, 3, 4]
[Sec. 1, 4th paragraph] "[...] supervised disentangled representation methods cannot handle unknown nuisance factors [...]": Can you back this claim or add references to past work?
[Figure 1]: I don't quite understand this Figure. What does the image on the left and the colors represent? Can you explain all subplots a), b), and c)?
[Sec. 2, Related work] The related work reads more like a background section. It would be clearer and more helpful to the user to know how your work is related/similar (similar tasks, similar approach/methodology) to these past works and how they are different (what are you contributing)?
[Sec. 2, 1st paragraph] typos / errors: "evidence lower bound of likelihood [...]" -> "evidence lower bound of the log-likelihood [...]", "between distribution q ϕ ( z x )
" -> "between approximate distribution q ϕ ( z x )
", "the negative of ELBO" -> "the negative ELBO"
[Sec. 3.1] What are "known" and "unknown" latent factors? In what way are they "known"?
[Sec. 3.1] What exactly do you mean under weakly supervised disentangled representation learning? What are the assumptions made here?
[Figure 2] The Figure says "Concat"; however, Section 3.2 describes a "Detect and Swap" approach. What is being done here?
[Sec. 3.2, Detecting and Swapping the distinct latent factors] "we can directly measure the mutual information between the corresponding the dimensions [...] by measuring [...] either KL divergence or Jenson-Shannon divergence": This is not correct. The mutual information is calculated by using the KL divergence between the product of the variables and the joint of the variables. The way the authors present it's just the KL divergence between two Gaussian latent variables, not the mutual information.
[Sec. 3] latent variable corresponding to the unknown nuisance factors z n u
: This has been introduced in Section 3.1. However, it has not been used any further. What is the purpose of introducing it?
[Sec. 3.2, Disentangled representation loss function]: What is L V A E
? Is this the ELBO?
[Sec. 3.3, 1st paragraph] "After we obtain the disentangled representation z n k
, the predictive factors z p
may still be entangled with z n u
": Why can't z p
be entangled with z n k ?
[Sec. 3.3, Making z p
independent of z n
] "we generate new latent representation z ¯ p
": From which input is z ¯ p
"generated"? Do you mean "calculated"?
Table 1: What is used for classification, only z p
? Are the models fair comparisons as some of them are unsupervised?
Can you quantify how well the representations are disentangled from each other, e.g., using mutual information?
Can you describe the training procedure? E.g., dataset split, optimization, evaluation settings. References
[1] Hosoya, H., 2018. Group-based learning of disentangled representations with generalizability for novel contents. arXiv preprint arXiv:1809.02383.
[2] Bouchacourt, D., Tomioka, R. and Nowozin, S., 2018, April. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. In Thirty-Second AAAI Conference on Artificial Intelligence.
[3] Creager, E., Madras, D., Jacobsen, J.H., Weis, M., Swersky, K., Pitassi, T. and Zemel, R., 2019, May. Flexibly fair representation learning by disentanglement. In International conference on machine learning (pp. 1436-1445). PMLR.
[4] Klys, J., Snell, J. and Zemel, R., 2018. Learning latent subspaces in variational autoencoders. arXiv preprint arXiv:1812.06190.
[5] Khemakhem, I., Kingma, D., Monti, R. and Hyvarinen, A., 2020, June. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics (pp. 2207-2217). PMLR.
[6] Mitrovic, J., McWilliams, B., Walker, J., Buesing, L. and Blundell, C., 2020. Representation learning via invariant causal mechanisms. arXiv preprint arXiv:2010.07922.
[7] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C. and Krishnan, D., 2020. Supervised contrastive learning. arXiv preprint arXiv:2004.11362.
[8] Locatello, F., Poole, B., Rätsch, G., Schölkopf, B., Bachem, O. and Tschannen, M., 2020, November. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning (pp. 6348-6359). PMLR.

Review Point: 1) The weakly-supervised disentanglement by Locatello et al. [8] is enhanced by the detecting & swapping approach. It is not clear how much the detecting & swapping approach advances the disentanglement performance.
==================================================

Focused review:

Weaknesses:
The experiment tasks, while common in this type of paper, are rather simple. The locomotion problems are not complex enough to show improvement, and for the robotic hand experiments, the optmimal length L of the latent trajectory segments seems rather small (3). This makes it a bit unclear how much the learned latent representation helps with planning compared to just 1) reducing the time discretization of the TT by a factor of 1/L (i.e. executing the same action for L=3 steps in a row), and 2) manually discretizing the action space (e.g. clustering it using k-means), although for a fair comparison one would need to sample from a learned conditional sequence prior (or ignore it for both) 3) both of these in combination. I do not think this was covered by the existing baselines? Would 1) be feasible to add?
It would have been interesting with some introspection and qualitative examples of the learned latent action representation (trajectory segments). This approach should be more suitable for tasks that have a natural discrete structure. It would have been interesting to see how well it could recover that.

Review Point: 1) reducing the time discretization of the TT by a factor of 1/L (i.e. executing the same action for L=3 steps in a row), and
Review Point: 2) manually discretizing the action space (e.g. clustering it using k-means), although for a fair comparison one would need to sample from a learned conditional sequence prior (or ignore it for both)
Review Point: 3) both of these in combination. I do not think this was covered by the existing baselines? Would
Review Point: 1) be feasible to add? It would have been interesting with some introspection and qualitative examples of the learned latent action representation (trajectory segments). This approach should be more suitable for tasks that have a natural discrete structure. It would have been interesting to see how well it could recover that.
==================================================

Focused review:

1. Some explanations are needed. In Line 151, I concern about why "implicit regularizer" leading to "generalized models". There is no analysis on it. 2. In Table 4. It would be better to state the difference between Conv-LSTM[4] and Conv-LSTM(baseline) clearly.

Review Point: 1. Some explanations are needed. In Line 151, I concern about why "implicit regularizer" leading to "generalized models". There is no analysis on it.
Review Point: 2. In Table 4. It would be better to state the difference between Conv-LSTM[4] and Conv-LSTM(baseline) clearly.
==================================================

Focused review:

- The problem modelled is essentially the cold-start recommendation problem, where items are shown to a new user to learn the user’s preferences. Similarly to this work, each item for which preferences are obtained is only presented once. A key difference is the ‘purity’ of the problem here (no side information) and deep theoretical analysis. It would be helpful for the related work section to make this connection. - The the first two settings studied, recommendation utility does not depend on the user (as all users are statistically indistinguishable). This is a highly simplified setting. While the simplification still requires complex analysis, it does not represent a classic recommendation application (where personalization plays a key role). - The authors argue that the techniques developed to allow the analysis are a key contribution. However, almost all of this is in the supplemental work, rather than in the core of the paper. Similarly, the authors present all “the pseudo-codes of our algorithms, all proofs, numerical experiments, as well as some insightful discussions” in the appendix, suggesting there is just too much attempted content here: The paper cannot really be appreciated without the appendix, so the appendix is not really “supplemental”.

Review Point: - The problem modelled is essentially the cold-start recommendation problem, where items are shown to a new user to learn the user’s preferences. Similarly to this work, each item for which preferences are obtained is only presented once. A key difference is the ‘purity’ of the problem here (no side information) and deep theoretical analysis. It would be helpful for the related work section to make this connection.
Review Point: - The the first two settings studied, recommendation utility does not depend on the user (as all users are statistically indistinguishable). This is a highly simplified setting. While the simplification still requires complex analysis, it does not represent a classic recommendation application (where personalization plays a key role).
Review Point: - The authors argue that the techniques developed to allow the analysis are a key contribution. However, almost all of this is in the supplemental work, rather than in the core of the paper. Similarly, the authors present all “the pseudo-codes of our algorithms, all proofs, numerical experiments, as well as some insightful discussions” in the appendix, suggesting there is just too much attempted content here: The paper cannot really be appreciated without the appendix, so the appendix is not really “supplemental”.
==================================================

Focused review:

1. Not clear how this method can be applied outside of fully cooperative settings, as the authors claim. The authors should justify this claim theoretically or empirically, or else remove it. 2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019). 3. The analysis is somewhat "circumstantial", need more detailed experiments to be a convincing argument in this section. For example the claim in lines 235 - 236 seems to require further evidence to be completely convincing. 4. The link with self-play could be more clearly drawn out. As far as I can tell, the advantage of this over self-play is precisely the different initialization of the separate agents. It is surprising and important that this has such a significant effect, and could potentially spur a meta-learning investigation into optimal initialization for SEAC in future work.

Review Point: 1. Not clear how this method can be applied outside of fully cooperative settings, as the authors claim. The authors should justify this claim theoretically or empirically, or else remove it.
Review Point: 2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019).
Review Point: 3. The analysis is somewhat "circumstantial", need more detailed experiments to be a convincing argument in this section. For example the claim in lines 235 - 236 seems to require further evidence to be completely convincing.
Review Point: 4. The link with self-play could be more clearly drawn out. As far as I can tell, the advantage of this over self-play is precisely the different initialization of the separate agents. It is surprising and important that this has such a significant effect, and could potentially spur a meta-learning investigation into optimal initialization for SEAC in future work.
==================================================

Focused review:

1) I do not think that the authors are able to motivate or demonstrate the usefulness of their contributions. The proposed tracker performs far from current state-of-the-art trackers, such as SiamRPN++ and DiMP. I do not believe the proposed component to be effective in such more advanced trackers, since they integrate the advantages of both template and detection based method in a more unified manner. In order to demonstrate the usefulness, the authors should attempt to improve modern SOTA trackers, which is not done here. 2) The experiments are lacking in several aspects. Only two datasets are used, OTB and LaSOT (both OTB-2013 and OTB-50 are subsets of OTB-100 and should therefore not be considered). OTB is small and considered obsolete since results are saturated. In addition to LaSOT, the authors should experiment with large-scale datasets, such as TrackingNet, UAV123, or GOT10k. Moreover, the authors only compare with outdated trackers. The claim of state-of-the-art is wrong. 3) No deeper analysis or ablation of the method itself is performed. Only the fusion of different trackers is performed. 4) The motivation and method description is not that clear. Design choices are not motivated for the most part. 5) I could not find significant novelty in the proposed detection tracker FCT. It essentially seems to correspond to a fully-convolutional version of MDNet.

Review Point: 1) I do not think that the authors are able to motivate or demonstrate the usefulness of their contributions. The proposed tracker performs far from current state-of-the-art trackers, such as SiamRPN++ and DiMP. I do not believe the proposed component to be effective in such more advanced trackers, since they integrate the advantages of both template and detection based method in a more unified manner. In order to demonstrate the usefulness, the authors should attempt to improve modern SOTA trackers, which is not done here.
Review Point: 2) The experiments are lacking in several aspects. Only two datasets are used, OTB and LaSOT (both OTB-2013 and OTB-50 are subsets of OTB-100 and should therefore not be considered). OTB is small and considered obsolete since results are saturated. In addition to LaSOT, the authors should experiment with large-scale datasets, such as TrackingNet, UAV123, or GOT10k. Moreover, the authors only compare with outdated trackers. The claim of state-of-the-art is wrong.
Review Point: 3) No deeper analysis or ablation of the method itself is performed. Only the fusion of different trackers is performed.
Review Point: 4) The motivation and method description is not that clear. Design choices are not motivated for the most part.
Review Point: 5) I could not find significant novelty in the proposed detection tracker FCT. It essentially seems to correspond to a fully-convolutional version of MDNet.
==================================================

Focused review:

Weakness: 1. The only difference between using PTQ in LIC models and conventional neural networks is the format of task loss. This paper simply applies the PTQ method for the task of LIC, thus makes somewhat trivial technical contributions. 2. The theoretical derivations and the relevant conclusions in this paper have been given in the paper of AdaRound [1]. 3. The setting of this paper is to transform the neural network from FP32 to INT8. But actually this setting is defined by the author without any explanations. Some other choices can be adopted, e.g., INT4 or unsigned INT16. 4. There are some vague claims without evidence, such as, ‘Such inappropriate processing of bias may lead to catastrophic results. For example, having the bias in INT32 precision may cause data overflow of the INT32 accumulator; while setting bias as zero would degrade the model performance significantly.’ References: [1] Up or down? adaptive rounding for post-training quantization. Nagel et al., ICML 2020.

Review Point: 1. The only difference between using PTQ in LIC models and conventional neural networks is the format of task loss. This paper simply applies the PTQ method for the task of LIC, thus makes somewhat trivial technical contributions.
Review Point: 2. The theoretical derivations and the relevant conclusions in this paper have been given in the paper of AdaRound [1].
Review Point: 3. The setting of this paper is to transform the neural network from FP32 to INT8. But actually this setting is defined by the author without any explanations. Some other choices can be adopted, e.g., INT4 or unsigned INT16.
Review Point: 4. There are some vague claims without evidence, such as, ‘Such inappropriate processing of bias may lead to catastrophic results. For example, having the bias in INT32 precision may cause data overflow of the INT32 accumulator; while setting bias as zero would degrade the model performance significantly.’ References: [1] Up or down? adaptive rounding for post-training quantization. Nagel et al., ICML 2020.
==================================================

Focused review:

After reading the paper, it is hard to understand this paper in a high level, except its key topic is to accelerate learning using reward shaping (GCN is applied here). In the high level, does this paper only focus on inference? or both inference and learning? If this paper only use GCN in the inference stage, then its contribution is limited. Specifically, we know best P(O|S) (which is given) in Eq. 6, and then we can train GCN by minimizing Eq. 6. If we need to infer P(O|S) from RL, and in turn learn GCN parameters, then it is a challenge, the algorithm should not be easy as it stated in the paper. Moreover, the time complexity is much high because we need to count both GCN and RL steps.

Review Point: 6. If we need to infer P(O|S) from RL, and in turn learn GCN parameters, then it is a challenge, the algorithm should not be easy as it stated in the paper. Moreover, the time complexity is much high because we need to count both GCN and RL steps.
==================================================

Focused review:

Novelty: 1. One of the main contribution of the paper is the algorithm proposed in Section 5 which achieves the minimax optimal regret bound. However, the main technique used here is the mini-batch paradigm which has been considered before, e.g., in [Dekel et al., 2011; Arora et al., 2012], and it seems that the proof technique is rather straightforward. 2. It seems that the adversary’s strategy and the minimax analysis in Section 4.1 are largely based on those of [Abernethy et al., 2008]. It would be better if the authors could add more discussions on what are the difficulties to adapt the minimax analysis for classic OCO into the switch-constrained setting. Significance: 2. The fugal game proposed in this paper is novel and very interesting, but I am unsure about the significance of this contribution since it is considered in the 1-d situation and it only improves the minimax bound by constant factors. --------------------------------------Post Rebuttal--------------------------- The authors cleared my concerns in the rebuttal. I am happy to raise my score.

Review Point: 2. It seems that the adversary’s strategy and the minimax analysis in Section 4.1 are largely based on those of [Abernethy et al., 2008]. It would be better if the authors could add more discussions on what are the difficulties to adapt the minimax analysis for classic OCO into the switch-constrained setting. Significance:
Review Point: 2. The fugal game proposed in this paper is novel and very interesting, but I am unsure about the significance of this contribution since it is considered in the 1-d situation and it only improves the minimax bound by constant factors. --------------------------------------Post Rebuttal--------------------------- The authors cleared my concerns in the rebuttal. I am happy to raise my score.
==================================================

Focused review:

Weaknesses
Theorems 3.5 and 3.6 appear to be missing something. Clearly setting β ∗ = 0 and T ( X ) = 0
achieves zero error (and satisfies the unbiasedness assumption). Perhaps β ∗
needs to be bound, e.g., by taking the supremum over a certain range. Similarly, it is not clear what the expectation is over (I read it to be over X
but this could be made explicit).
There is no discussion about the existence and uniqueness of the estimator, i.e., of the stationary distribution of (3). This needs to be formalized more rigorously. Under what conditions on X does the spectral estimate exist? How do Algorithms 1 & 2 handle cases where this condition is not satisfied?
The similarities and differences to existing pairwise & spectral approaches for the Rasch model should be emphasized in the main text. Reading through Appendix D, it is apparent that the differences are major, but this was not clear in the main text (beyond the brief comment in the introduction about dense vs. sparse matrices). I strongly suggest making use of any additional space to move parts of Appendix D to the main text. Finally, statements like that of line 135, "our algorithm instantiates the general spectral approach [...]" are slightly misleading, given prior work.
These weaknesses are relatively minor and I believe they can be addressed in a camera-ready version.
Small comments:
I am surprised that Agarwal et al. [2] is not cited in Section 4, "accelerating the spectral algorithm".
Paragraph starting on line 50: an explicit expression for the CMLE would be helpful; it is hard to understand what the "likelihood conditioned on s l
" looks like.
l. 60: later -> latter
l.116 and 204: practicioners -> practitioners

Review Point: 60: later -> latter l.116 and 204: practicioners -> practitioners
==================================================

Focused review:

Weaknesses: 1. The proposed method is only studied with the logloss. It is not clear whether the proposed method can help other ranking-based recommendation loss, e.g., Bayesian personalized ranking loss. 2. The backbone models, i.e., GMF and NeuMF, are a little weak. The authors are recommended to consider other advanced recommendation models as backbone models, e.g., LightGCN and other sequential recommendation models. 3. Some claims in this paper are not supported by experimental analysis.

Review Point: 1. The proposed method is only studied with the logloss. It is not clear whether the proposed method can help other ranking-based recommendation loss, e.g., Bayesian personalized ranking loss.
Review Point: 2. The backbone models, i.e., GMF and NeuMF, are a little weak. The authors are recommended to consider other advanced recommendation models as backbone models, e.g., LightGCN and other sequential recommendation models.
Review Point: 3. Some claims in this paper are not supported by experimental analysis.
==================================================

Focused review:

1. In general, one can hardly play synchronous V-trace algorithm because the state distribution of the off-policy data can be really bad. It is also not proper to claim that the sample bounds is logarithmic in S because in each round the proposed algorithm needs at least S trajectories. 2. As far as I understand, in IMPALA, one needs to compute V_{\pi_{\bar{\rho}}} to approximate V_{\pi}. To reduce the approximation error, one should set \bar{\rho} sufficiently large. However, the sample bound in Theorem 3.1 depends on \bar{\rho} polynomially. So one can hardly say that the approximation is efficient. So I think the result about V-trace is not significant.

Review Point: 1. In general, one can hardly play synchronous V-trace algorithm because the state distribution of the off-policy data can be really bad. It is also not proper to claim that the sample bounds is logarithmic in S because in each round the proposed algorithm needs at least S trajectories.
Review Point: 2. As far as I understand, in IMPALA, one needs to compute V_{\pi_{\bar{\rho}}} to approximate V_{\pi}. To reduce the approximation error, one should set \bar{\rho} sufficiently large. However, the sample bound in Theorem 3.1 depends on \bar{\rho} polynomially. So one can hardly say that the approximation is efficient. So I think the result about V-trace is not significant.
==================================================

Focused review:

Weaknesses:
While the transformations described in the paper are intuitive, some of them are hard to scale. In particular, one has to design such transformations manually for a given programming language setup. Further, authors have not provided an easy to use list of transformations (or dataset version) which other researchers can reuse for future experiments.
Prior work [1] show that problem description, context are crucial to execution accuracy. This paper's experiments also underline that execution accuracy highly depends upon the prompt (context) fed to the model and minor changes in the context can lead to big drop in execution accuracy. I am not sure that this paper offers new insights to the community.
Authors crafted these prompts manually and show that all of these transformations lead to significant accuracy drop. It's not clear 1) how many different prompts they tried for a given experiment. 2) Have they reported results for all transformations or only for those transformations which lead to accuracy drop.
Paper is missing crucial details related to how the code was generated. In particular, it's not clear which decoding method (greedy, sampling with temperature, etc.) is used to generate the data. Without such crucial details, it's hard to replicate the results presented in the paper.
I don't see any major concerns related to negative societal impact.

Review Point: 1) how many different prompts they tried for a given experiment.
Review Point: 2) Have they reported results for all transformations or only for those transformations which lead to accuracy drop. Paper is missing crucial details related to how the code was generated. In particular, it's not clear which decoding method (greedy, sampling with temperature, etc.) is used to generate the data. Without such crucial details, it's hard to replicate the results presented in the paper. I don't see any major concerns related to negative societal impact.
==================================================

Focused review:

Weaknesses: 1. The writing is not clear. The descriptions of the techniqual part can not be easily followed by the reviewer, which make it very hard to reimplement the techniques. 2. An incomplete sequence is represented by a finite state automaton. In this paper, only a two out of three finite state automation is used. Is it possible/computational feasible to use more complicated finite state automaton to select more image labels? As there are 12 image labels per image on average, only selecting two labels seems insufficient. 3. The authors describe an online version of the algorithm because it is impractical to train multiple iterations/epochs with large models and datasets. Is it true that the proposed method requires much more computation than other methods? Please compare the computational complexity with other methods. 4. How many estimated complete captions could be obtained for one image? Is it possible to generate over C(12, 3) * C(3, 2) = 660 different captions for one image?

Review Point: 1. The writing is not clear. The descriptions of the techniqual part can not be easily followed by the reviewer, which make it very hard to reimplement the techniques.
Review Point: 2. An incomplete sequence is represented by a finite state automaton. In this paper, only a two out of three finite state automation is used. Is it possible/computational feasible to use more complicated finite state automaton to select more image labels? As there are 12 image labels per image on average, only selecting two labels seems insufficient.
Review Point: 3. The authors describe an online version of the algorithm because it is impractical to train multiple iterations/epochs with large models and datasets. Is it true that the proposed method requires much more computation than other methods? Please compare the computational complexity with other methods.
Review Point: 4. How many estimated complete captions could be obtained for one image? Is it possible to generate over C(12, 3) * C(3, 2) = 660 different captions for one image?
==================================================

Focused review:

The paper is technically sound and I have no major comments on the weaknesses. Some minor points & discussion: * L109: ``We derive a general form of STE under a very clear approximation extending the linearization construction [31] '' The authors tend to summarize the contributions of this paper as extending the approach of Tokui & Sato (2017). However, a notable difference is that Tokui & Sato apply Gumbel-max reparameterization to all discrete latents to resolve the dependencies, which is not the approach taken here. I believe using Tokui & Sato's method we can also train deep binary stochastic networks with a manageable cost (linear to the number of bernoulli latents). Therefore, it is important to discuss their differences and advantages of the linear approximation used in this work (it also adds bias). Experiments on comparing them are very welcome, too. * L100: Finally, many experimentally oriented works successfully apply straight-through estimators (STE), originally considered by Hinton for variational auto-encoders. The statement here is imprecise, it is proposed for deep autoencoders with binary hidden codes. No variational bounds were used.

Review Point: * L109: ``We derive a general form of STE under a very clear approximation extending the linearization construction [31] '' The authors tend to summarize the contributions of this paper as extending the approach of Tokui & Sato (2017). However, a notable difference is that Tokui & Sato apply Gumbel-max reparameterization to all discrete latents to resolve the dependencies, which is not the approach taken here. I believe using Tokui & Sato's method we can also train deep binary stochastic networks with a manageable cost (linear to the number of bernoulli latents). Therefore, it is important to discuss their differences and advantages of the linear approximation used in this work (it also adds bias). Experiments on comparing them are very welcome, too.
Review Point: * L100: Finally, many experimentally oriented works successfully apply straight-through estimators (STE), originally considered by Hinton for variational auto-encoders. The statement here is imprecise, it is proposed for deep autoencoders with binary hidden codes. No variational bounds were used.
==================================================

Focused review:

Weaknesses
1 Fidelity of the explanation. The knowledge graph is treated as a ground truth during the explanation, while the model may not use any object or concept in the ConceptNet. For instance, the scene classification model may use the local color, texture, the whole illumination, or even the background (non-object) as core logic to make the decision. The proposed method could not cover the explanation of these cases.
2 Missing the relationship between objects. A model may not use the existence of objects but their spatial relationship or correlation of them with the background. The proposed method could not be used to explain reasons outside of the predefined knowledge graph. Please consider comparing your method with "A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts, CVPR 2021" which consider the relationship between objects (concepts) during the explanation.
3 Generalization. The author claim it is a method of image classification (title) while only showing experiments and method of scene classification (a little bit overclaiming). The proposed method needs a predefined knowledge graph, which may be hard to apply to the general image classification task.
4 Time and effort cost. If the user wants to modify one image error, how long will it take? It looks like the method needs a lot of extra training or effort to explain even a single image.

Review Point: 1 Fidelity of the explanation. The knowledge graph is treated as a ground truth during the explanation, while the model may not use any object or concept in the ConceptNet. For instance, the scene classification model may use the local color, texture, the whole illumination, or even the background (non-object) as core logic to make the decision. The proposed method could not cover the explanation of these cases.
Review Point: 2 Missing the relationship between objects. A model may not use the existence of objects but their spatial relationship or correlation of them with the background. The proposed method could not be used to explain reasons outside of the predefined knowledge graph. Please consider comparing your method with "A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts, CVPR 2021" which consider the relationship between objects (concepts) during the explanation.
Review Point: 3 Generalization. The author claim it is a method of image classification (title) while only showing experiments and method of scene classification (a little bit overclaiming). The proposed method needs a predefined knowledge graph, which may be hard to apply to the general image classification task.
Review Point: 4 Time and effort cost. If the user wants to modify one image error, how long will it take? It looks like the method needs a lot of extra training or effort to explain even a single image.
==================================================

Focused review:

weakness of the work is that no lower bounds are provided. It is hard to judge how much revenue is left on the table without knowing how much a seller should expect to make in such a setting. - For the case with many bidders per round, it seems kind of strange that the number of copies m needs to grow when aiming for lower regret. Is there some inherent limitation or intuition for why it is hard to obtain results for a single item (m=1)? - A branch of work that deals specifically with repeated interactions with bidders is the field of dynamic mechanism design e.g. the work of Mirrokni et al. "Non-clairvoyant Dynamic Mechanism Design". It would be nice to compare the setting and results to this literature. Evaluation Overall, I like the proposed algorithm as it is quite simple and is able to capture complicated settings with many bidders participating in many rounds. The paper manages to show sublinear regret bounds but there is still room for improvement to match the sqrt(T) regret lower bound. The connection to joint-differential privacy is quite interesting but seems to have limitations for many bidders. It is unclear whether an alternative approach can yield non-trivial regret bounds for m=1.

Review Point: - For the case with many bidders per round, it seems kind of strange that the number of copies m needs to grow when aiming for lower regret. Is there some inherent limitation or intuition for why it is hard to obtain results for a single item (m=1)?
Review Point: - A branch of work that deals specifically with repeated interactions with bidders is the field of dynamic mechanism design e.g. the work of Mirrokni et al. "Non-clairvoyant Dynamic Mechanism Design". It would be nice to compare the setting and results to this literature. Evaluation Overall, I like the proposed algorithm as it is quite simple and is able to capture complicated settings with many bidders participating in many rounds. The paper manages to show sublinear regret bounds but there is still room for improvement to match the sqrt(T) regret lower bound. The connection to joint-differential privacy is quite interesting but seems to have limitations for many bidders. It is unclear whether an alternative approach can yield non-trivial regret bounds for m=1.
==================================================

Focused review:

See some questions about correctness, and missing baselines below. There is a lot of room for improvement when it comes to the writing; * Mainly in terms of clarity (see below) - the model architecture is very hard to understand. * writing seems very rushed, with unfinished sentences (eg L332), * strange turns of phrases (eg "largest dataset that we have ever met", "largely-expressive models", "dark clouds") * There are sometimes long-winded introductory sentences without real information, which gets in the way of clear exposition (eg L171-173). I encourage the authors to rewrite the paper in a more direct style, removing all unnecessary adjectives.

Review Point: * Mainly in terms of clarity (see below) - the model architecture is very hard to understand.
Review Point: * writing seems very rushed, with unfinished sentences (eg L332), * strange turns of phrases (eg "largest dataset that we have ever met", "largely-expressive models", "dark clouds") * There are sometimes long-winded introductory sentences without real information, which gets in the way of clear exposition (eg L171-173). I encourage the authors to rewrite the paper in a more direct style, removing all unnecessary adjectives.
==================================================

Focused review:

Weaknesses:
The main weakness of this paper to me is the evaluation section. The proposed approach is only validated on simple datasets like MNIST and SVHN. It would be more convincing to show the effectiveness of the proposed approach on natural images and a large number of classes, like CIFAR and ImageNet, as used in the previous work such as IG.
Following the comment in 1), the prior works, such as IG and DeepLIFT, have been used to analyze other types of models and been evaluated on other types of data, such as genomics and neural machine translation. In addition to images, would the proposed approach also apply to these domains?
As illustrated in Figure 1, the key assumption of the proposed approach is that a GAN model (StarGAN) is able to generate examples that are much closer to the input examples than the training examples (i.e., minimum distance training sample). Under what conditions would such an assumption hold?
Following the comment in 3), it would be interesting to show and analyze some failure cases.
In the proposed approach, the StarGAN directly uses the already trained model classifier as its discriminator. How if the StarGAN trains its own discriminator without using the model classifier?
It would be interesting to show the hyper-parameter (different trade-off lambdas) sensitivity.
I understood that the authors focused on one-vs-one explanations. But I am interested to hear the authors’ thoughts on how to extend the proposed approach to one-vs-all explanations.
After Rebuttal:
I thank the authors for the rebuttal. I have also read the other reviewers’ comments. Unfortunately, the rebuttal is unconvincing and sometimes vague. I keep my original rating.

Review Point: 1), the prior works, such as IG and DeepLIFT, have been used to analyze other types of models and been evaluated on other types of data, such as genomics and neural machine translation. In addition to images, would the proposed approach also apply to these domains? As illustrated in Figure 1, the key assumption of the proposed approach is that a GAN model (StarGAN) is able to generate examples that are much closer to the input examples than the training examples (i.e., minimum distance training sample). Under what conditions would such an assumption hold? Following the comment in
Review Point: 3), it would be interesting to show and analyze some failure cases. In the proposed approach, the StarGAN directly uses the already trained model classifier as its discriminator. How if the StarGAN trains its own discriminator without using the model classifier? It would be interesting to show the hyper-parameter (different trade-off lambdas) sensitivity. I understood that the authors focused on one-vs-one explanations. But I am interested to hear the authors’ thoughts on how to extend the proposed approach to one-vs-all explanations. After Rebuttal: I thank the authors for the rebuttal. I have also read the other reviewers’ comments. Unfortunately, the rebuttal is unconvincing and sometimes vague. I keep my original rating.
==================================================


review_id,source,focused_review,review_point,human_actionability,human_specificity,human_verifiability,human_politeness,Notes,actionability_tim,specificity_tim,verifiability_tim,politness_tim,Notes Tim,llm_actionability,llm_feedback_actionability,llm_politeness,llm_feedback_politeness,llm_verifiability,llm_feedback_verifiability,llm_specificity,llm_feedback_specificity
905,ICLR_2023_2312,"Weaknesses
1. Literature Review
The paper regrettably fails to acknowledge a vast body of related literature, on (i) intention-conditioned trajectory prediction, (ii) variational graph methods for trajectory prediction, and (iii) models that explicitly model social interactions for forecasting. At the very least, these references ought to be mentioned and discussed for a diligent representation of the research space, even if the methods are not directly compared against.
(i) Intention-Conditioned Trajectory Prediction:
[R1, R2, R3] talk about intention-conditioned trajectory prediction for autonomous vehicles. Apart from the data the methods are applied to, the architectures can be applicable to, and are relevant for, the problem being addressed here. Crucially, the DROGON paper defines intention explicitly (more on this in Weakness 2. below).
(ii) Variational Graph Methods:
[R4] from the Neurips I Can't Believe It's Not Better Workshop explicitly deals with graph conditional variational methods for multi-agent trajectory prediction. The results in that paper are very relevant for this research area and should be included.
(iii) Encoding Social Interactions:
Graph and other stochastic methods that encode social interactions between agents have been long applied to trajectory and behavior forecasating problems. [R5] explicitly incorporates a spatiotemporal graph for incorporating social interactions between agents. [R6] more recently explicitly takes a meta-learning approach for modeling the dynamics unique to a group for probabilistic forecasting. A sports team is a group, and if each team is viewed as having unique social dynamics resulting from the team's strategy then [R6]'s core modeling idea is directly applicable. The cue in [R6] terms is simply player location here. Their modeling of social influence of other agents is also permutation invariant, a limitation this paper claims about existing methods. References:
[R1] DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning - Choi et al.
[R2] Intention-Driven Trajectory Prediction for Autonomous Driving - Fan et al.
[R3] LOKI: Long Term and Key Intentions for Trajectory Prediction - Girase et al.
[R4] Graph Conditional Variational Models: Too Complex for Multiagent Trajectories? - Rudolph et al.
[R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al.
[R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.
2. Unsupported claims and definitions
The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.
3. Notation
There are a few notational errors. For instance, the variable used for the sequence cannot be the same as the individual elements: x < t = [ x 1 , . . . ]
. See [R4] for this. In many places there exist grammatical errors and incomplete sentences. Please do a pass to fix these.","2. Unsupported claims and definitions The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.",0.0,0.0,1.0,0.0,Do we need to include the full review as well?,1.0,1.0,-1.0,0.0,nan,-1,"The review point does not provide any direct or explicit actionable comments that the authors should make. It merely points out the lack of definitions and the absence of explicit causal factors in the paper. The reviewer does not suggest any specific steps or experiments that could be done to address these issues. Instead, the reviewer leaves the authors to deduce what needs to be done, which is not helpful in guiding the authors to improve their paper. The review point does not fulfill the criteria of providing actionable comments, either explicit or implicit. Therefore, the score is -1.",0,"The review point provided is critical and constructive, focusing on the lack of definitions and support for the claims made in the paper. It points out the absence of explicit causal factors and the ambiguity of the term ""intention"" in the context of team sports. The reviewer's tone is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not use any aggressive or disrespectful language, and the feedback is delivered in a manner that is neither rude nor overly polite. The reviewer's focus on the specific aspects of the paper, such as the lack of explicit causal factors and the ambiguity of the term ""intention,"" demonstrates a clear understanding of the paper's content and a constructive approach to providing feedback. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes several claims, but it fails to provide any form of verification or justification for these claims. The reviewer points out that the paper does not define agent intentions and causality in the specific setting, which is a critical aspect of the paper's claims. The reviewer also questions the paper's claim to model causal relationships, stating that there are no explicit causal factors modeled or learned in the graph structure. However, the reviewer does not provide any logical deductions or references to external or internal sources that could support these claims.

The reviewer's comments on the intention-conditioned trajectory and the role of communication in team sports are also unsupported. The reviewer questions what intention means in this context, but does not offer any explanation or reference to support this claim.

In summary, the review point contains several unsupported claims, and it does not provide any logical deductions or references to external or internal sources that could verify these claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability.",1,"The review point provided is quite specific and directly addresses the paper's treatment of agent intentions and causality. It clearly identifies the lack of definition and explicit modeling of causal relationships in the paper. The review point also points out the ambiguity in the paper's discussion of intention in the context of team sports. This specificity in the review point allows the authors to pinpoint exactly which parts of the paper are being critiqued. The review point's focus on the absence of explicit causal factors and the lack of clarity in the paper's discussion of intention demonstrates a clear understanding of the paper's content and the issues it raises. Therefore, the authors can be confident that the review point is addressing specific aspects of the paper. 

In contrast, a review point that lacks specificity would not allow the authors to identify which parts of the paper are being addressed. It would leave the authors unsure about the exact issues being raised. The review point's lack of specificity would not provide the authors with a clear direction for improvement or clarification. 

The review point's specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to confidently identify the parts of the paper being addressed by the review point. The review point's focus on the paper's treatment of agent intentions and causality, and its clear identification of the issues, demonstrates a high level of specificity. 

Therefore, based on the score rubric's criteria, the review point is highly specific and directly addresses the paper's content, making it clear which parts of the paper are being critiqued."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- I'm missing a discussion why values for p different from 2 are not interesting to consider.,0.0,1.0,0.0,0.0,nan,1.0,1.0,-1.0,0.0,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that values for p different from 2 are not interesting to consider, without offering any reasoning or justification for this claim. The authors are left without any guidance on what to do next or how to address the issue raised in the review point. The lack of actionable comments in the review point means that the authors cannot deduce what needs to be done, which is why the review point does not meet the criteria for actionability. Therefore, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the facts about the paper, such as the lack of discussion on values for p different from 2, without adding any personal opinion or judgment. The reviewer's tone is passive, and the feedback is delivered in a straightforward manner without any attempt to be helpful or constructive. The reviewer does not provide any suggestions or recommendations for improvement, which could have been helpful to the authors. Therefore, the review point is neutral and does not fit well with the other two scores.",0,"The review point in question does not contain any claims that need to be verified. It merely states a lack of interest in considering values for p different from 2. This statement is not a claim that requires justification or verification, as it does not make any assertions about the data or the methodology. It is a subjective opinion that does not need to be substantiated with logical deductions or references to external or internal sources. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains a claim nor does it require verification. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It lacks specificity and does not provide any concrete examples or suggestions for improvement. The authors are left with no clear direction on how to address the review point, which could lead to confusion or misinterpretation of the feedback. The review point does not make it obvious which part of the paper is being addressed, nor does it allow the authors to confidently identify the related parts of the paper. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and clarity. 

In contrast, a review point that is specific and clear would directly address a particular section or aspect of the paper, providing a clear direction for the authors to follow. It would also offer specific examples or suggestions for improvement, which would help the authors understand exactly what needs to be addressed. This would enable the authors to confidently identify the related parts of the paper and take appropriate action. 

So the overall score is -1, as the review point does not provide the necessary specificity and clarity to guide the authors in addressing the feedback."
612,ICLR_2021_971,"Weaknesses
The proposed algorithm is not parameter-free (unlike SNIP, which is virtually parameter-free), is quite complicated (and I imagine difficult to implement), and there is little justification for certain components of the method, e.g., the dynamic scaling function (and choices of lambda), whether the simplification of m_{t-1} = … = m_{T} is mild enough. It is not clear to me how a practitioner can run the proposed algorithm in a parameter-free way without having to conduct ablation studies of their own first, especially since, as the authors note, “We observed that the penalty parameter was difficult to tune properly, either being too aggressive at pruning, or too passive” as the justification for the dynamic scaling function
Parts of the paper are too dense and notation-heavy, and this hurts readability and understanding significantly, e.g., Lemma 1, paragraph regarding the introduction of the saliency function on pg. 2.
The presented experimental results are not very compelling. For example, in Table 3, we see that BEP 1e-4 achieves a ~.4% improvement over SNIP and GRASP, at the cost of ~7-8.4 more hours of training time. This calls into question the effectiveness of the proposed approach -- which is, at the end of the day, meant to speed up training + pruning by removing unnecessary components of the network early on. Clarity
The paper is reasonably well-written and organized overall. It was clear that the authors compressed some of the mathematical expressions/lemmas (e.g., statement of Lemma 1), which is somewhat understandable given the page limit, but this hurt readability and understandability.","2. The presented experimental results are not very compelling. For example, in Table 3, we see that BEP 1e-4 achieves a ~.4% improvement over SNIP and GRASP, at the cost of ~7-8.4 more hours of training time. This calls into question the effectiveness of the proposed approach -- which is, at the end of the day, meant to speed up training + pruning by removing unnecessary components of the network early on. Clarity The paper is reasonably well-written and organized overall. It was clear that the authors compressed some of the mathematical expressions/lemmas (e.g., statement of Lemma 1), which is somewhat understandable given the page limit, but this hurt readability and understandability.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,There are two points in here,0,"The review point does not provide any direct or explicit actionable comments that the authors should make. It merely points out the limitations of the proposed approach without suggesting any specific steps or experiments that could be done to address these limitations. The review point also does not include any implicit actionable comments that the authors can infer after reading the review point. It simply states the limitations without providing any guidance on how to improve the approach. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or supportive. The feedback is focused on the content of the paper, specifically the experimental results and the clarity of the mathematical expressions. The reviewer's tone is neutral, and the feedback is delivered in a manner that is neither rude nor overly polite. The reviewer's comments are factual and do not contain any personal opinions or judgments, which is in line with the score rubric's criteria for a score of 0. The reviewer's feedback is constructive, providing suggestions for improvement, but it does not go beyond the facts presented in the paper. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states the results of the experiments and the implications of these results. The review point does not make any claims about the validity of the results or the effectiveness of the proposed approach. It also does not provide any logical deductions or references to external or internal sources that could verify the claims. The review point is focused on the clarity of the paper and the readability of the mathematical expressions, which are not related to the verifiability of the claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifying claims. So the score is 0.",-1,"The review point provided is not very specific, which is a key aspect of the score rubric. The review point mentions the experimental results in Table 3 and the training time, but it does not clearly identify which parts of the paper are being addressed. The authors might be unsure about which parts of the paper are being discussed in the review point. The review point also mentions the clarity of the paper, but it does not provide specific examples of where the clarity is lacking. This lack of specificity makes it difficult for the authors to identify the exact parts of the paper that are being addressed by the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that meets the criteria for specificity would clearly identify which parts of the paper are being addressed and provide specific examples of where the clarity is lacking. This would allow the authors to confidently identify the parts of the paper that are being discussed in the review point. 

So the overall score is -1, as the review point does not provide the necessary specificity to allow the authors to confidently identify the parts of the paper that are being addressed."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other?,0.0,1.0,0.0,0.0,nan,0.0,1.0,1.0,0.0,"[A] Leaning to give a 1 here.
[V] I am assuming that ""in this case"" makes it clear to the author to which part the review is referring to, even though it is missing here. ",0,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the relationship between the CDF and percentile rank without suggesting any specific actions or improvements that could be made. The review point also does not include any actionable comments that can be implicitly inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the facts about the paper, such as the lack of empirical evaluation and the potential misinterpretation of the BPE constraints. However, the reviewer could have been more constructive by offering suggestions for improvement or by acknowledging the strengths of the paper. The reviewer's tone is passive, which is the default score if the review point does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It is a question about the relationship between the CDF and percentile rank, which is a factual statement that does not require any justification or reference. The review point is simply asking for an explanation of how one can be expressed in terms of the other, which is a logical question that does not require any external or internal sources to answer. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. So the score is 0.",1,"The review point provided is quite specific and directly addresses the relationship between the CDF and percentile rank. It asks for a clear expression of one in terms of the other, which is a direct and specific question. This specificity allows the authors to identify which parts of the paper are related to the review point. The authors can be confident that the review point is addressing the statistical aspects of the paper, which is a clear and specific part of the paper.

The review point does not leave room for ambiguity or uncertainty. It directly asks for a clear expression of the relationship between the CDF and percentile rank, which is a specific aspect of the paper. This specificity ensures that the authors can confidently identify which parts of the paper are being addressed by the review point.

In contrast, a review point that lacks specificity would not allow the authors to confidently identify which parts of the paper are being addressed. It would leave room for ambiguity and uncertainty, as it would not clearly specify which part of the paper is being addressed.

Therefore, based on the score rubric, the review point is specific and directly addresses a clear part of the paper, which allows the authors to confidently identify which parts of the paper are being addressed. This specificity aligns with the criteria for a score of 1."
4288,NIPS_2020_251,"* The NF assumption was not discussed as compared to a standard SSM which uses additive measurement noise. Placing the emission noise *before* the nonlinearity is a crucial move; otherwise filtering is not tractable. I would have appreciated further discussion of the impact of this. It's possible that this technique can be applied as a drop-in replacement in many models to avoid awkward approximations such as EKF, UKF and PF; however this conclusion is not immediate from the work presented in this paper. * As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning. * The qualitative experiments seemed particularly artificial; I did not learn much here beyond the fact that the implementation broadly seems to work. If these are indicative of a real-world problem, it would be helpful to make this clearer. * NKF does not show markedly better performance than the GP-Copula model in the main experiments.",* NKF does not show markedly better performance than the GP-Copula model in the main experiments.,-1.0,1.0,0.0,0.0,nan,-1.0,1.0,-1.0,0.0,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the NKF model does not show markedly better performance than the GP-Copula model in the main experiments. This statement does not offer any guidance or suggestions for improvement. It also lacks any implicit actionable comments that could be inferred from the review point. The authors are left without any clear direction on what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided does not exhibit a tone that is particularly polite or encouraging. It is more neutral and factual, which aligns with a score of 0 according to the rubric. The reviewer does not use any aggressive or disrespectful language, but the tone is not particularly helpful or encouraging either. The reviewer simply states the facts about the performance of the NKF model compared to the GP-Copula model, without offering any constructive feedback or suggestions for improvement. The reviewer also does not use any language that could be perceived as rude or disrespectful. Therefore, based on the rubric, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not make any claims, nor does it require verification. It simply states that the NKF model does not show markedly better performance than the GP-Copula model in the main experiments. This is a factual statement that does not need any justification or reference to external or internal sources. The review point is straightforward and does not contain any claims that could be verified or refuted. Therefore, it does not meet the criteria for a score of 1, as it does not contain a claim that is verified. It also does not meet the criteria for a score of -1, as it does not contain any claims that are not verified. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly specify which part of the paper is being addressed. It merely states that the NKF model does not show markedly better performance than the GP-Copula model, without providing any concrete examples or details that would help the authors identify the specific sections of the paper that are being critiqued. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. As a result, the authors would likely be unsure about the paper part that is being addressed by the review point comment. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and thus, it would receive a score of -1."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.","- The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile.",0.0,1.0,1.0,0.0,nan,1.0,0.0,1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states a limitation without suggesting any specific actions that should be taken. The reviewer's interpretation of the AUP stopping movement entirely is not actionable and does not provide any insight into how the authors could improve their work. The review point does not give any hint or direct actionable comments, leaving the authors without a clear direction on what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the expected tone of a review. The language used is unclear and lacks the necessary politeness expected in a review. The phrase ""AUP stops moving entirely"" is vague and does not provide constructive feedback. It does not offer any suggestions for improvement or clarify the point being made. The tone of the review point is not helpful or encouraging, and it does not contribute to a constructive dialogue. The reviewer's interpretation of the phrase is also not clear, which could lead to confusion. The review point does not meet the criteria for politeness, as it does not provide any positive feedback or suggestions for improvement. Therefore, the review point is not helpful and does not contribute to a constructive review process. So the overall score is -1.",-1,"The review point in question makes a claim about the AUP's behavior, suggesting that it stops moving entirely. However, this claim is not supported by any logical deductions or references to external or internal sources. The reviewer's interpretation of the AUP's behavior is based on a single line of text, which lacks the necessary context or explanation to be considered a verified claim. The reviewer's belief that the AUP is immobile is not substantiated, and the claim is not justified or explained in any way. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it contains a claim that is not verified. 

In contrast, a review point that meets the criteria for verifiability would contain a claim that is supported by logical deductions and references to external or internal sources. For example, a review point might claim that the statistical analysis in the paper is incorrect due to the p-values not matching the standard thresholds for significance. This claim would be supported by a reference to the guidelines in 'Statistics for Biologists' (Smith et al., 2020), which would provide a logical explanation and a source to verify the claim. 

So the overall score is -1, as the review point contains a claim that is not verified, and it misses a justification, explanation, or reference.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which part of the paper is being addressed. The statement ""AUP stops moving entirely"" is ambiguous and does not provide a clear direction for the authors to improve their work. The reviewer's interpretation of the statement is not supported by the text, and it is unclear how the authors should respond to this comment. The lack of specificity in the review point does not allow the authors to confidently identify which parts of the paper are being addressed. Therefore, the score is -1."
3368,NIPS_2020_1285,- Only one constraint value is selected for each environment (50% of the speed attained by an unconstrained PPO agent - how and why was 50% chosen?). I would be interested in seeing whether FOCOPS consistently exceeds performance and satisfies constraints compared to other methods for different constraint levels. - Only one constraint is used for the experiments. How would FOCOPS perform empirically when there are multiple constraints?,- Only one constraint is used for the experiments. How would FOCOPS perform empirically when there are multiple constraints?,0.0,1.0,0.0,0.0,nan,1.0,1.0,-1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the existence of a single constraint in the experiments and asks how FOCOPS would perform under multiple constraints. This question is open-ended and does not provide any guidance or direction for the authors to follow. The review point does not suggest any specific actions or experiments that could be performed to address the issue of multiple constraints. It also does not include any actionable comments that can be implicitly inferred from the review point. Instead, it only mentions the limitation of using only one constraint without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer asks questions and points out potential issues without being overly critical or overly positive. The feedback is focused on the content of the paper and does not make any personal judgments about the authors or their work. The reviewer's tone is consistent throughout the feedback, maintaining a neutral stance. This is in line with the score rubric's description of a score of 0, which indicates a neutral, factual, or passive tone. Therefore, the review point is appropriately scored as 0.",0,"The review point in question does not contain any claims or make any assertions that require verification. It simply poses a question about the performance of FOCOPS under multiple constraints, which is a factual statement that does not need to be verified. The review point does not make any claims about the results of the experiments or the validity of the methodology used. It also does not provide any logical deductions or references to external or internal sources that could be used to verify any claims. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the performance of FOCOPS under multiple constraints. It directly addresses the paper's methodology and its potential limitations. The authors can be confident that the review point is discussing the empirical performance of the model under different conditions. The review point is well-defined and directly relates to the paper's content, making it clear which part of the paper is being addressed.

The review point does not leave room for ambiguity, as it specifically asks about the model's performance under multiple constraints. This directness ensures that the authors can identify the relevant sections of the paper and understand the review point's focus. The review point's specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about the part of the paper being addressed.

In contrast, a review point with a score of -1 would be vague and unspecific, leaving the authors unsure about which parts of the paper are being addressed. The lack of specificity in such a review point would not allow the authors to confidently identify the relevant sections of the paper.

The review point's specificity is crucial for the authors to understand and address the feedback. It is essential for the authors to be able to identify the specific aspects of their paper that need improvement or clarification. The review point's specificity ensures that the authors can focus their efforts on the areas highlighted in the review point.

In conclusion, the review point's specificity makes it clear which part of the paper is being addressed, allowing the authors to be confident about the paper part that is being addressed by the review point comment. This specificity aligns with the score rubric's criteria for a score of 1."
2693,NIPS_2019_436,"Weaknesses 1. Proposed method is relatively simple extension - involves using typical prototype for class in addition to transformation of class word-embeddings. 2. The benefit of incorporating semantic information is largely in the 5-class, 1-shot learning case (3.5% Mini-Imagenet and 2.75% Tiered-Imagenet accuracy gain compared to state-of-the-art LEO applied to regular few-shot learning scenario) and there seems to be very little gain beyond that number of shots. Comments The proposed setting and method requires that word embeddings are known for all train and test classes. Is it a more realistic scenario for few-shot learning that word-embeddings are only available for train classes, as this removes requirement that model that can only be used to learn about concepts that we already have word-embeddings for? Is semantic information as defined in paper applicable to few-shot settings beyond image-classification?","2. The benefit of incorporating semantic information is largely in the 5-class, 1-shot learning case (3.5% Mini-Imagenet and 2.75% Tiered-Imagenet accuracy gain compared to state-of-the-art LEO applied to regular few-shot learning scenario) and there seems to be very little gain beyond that number of shots. Comments The proposed setting and method requires that word embeddings are known for all train and test classes. Is it a more realistic scenario for few-shot learning that word-embeddings are only available for train classes, as this removes requirement that model that can only be used to learn about concepts that we already have word-embeddings for? Is semantic information as defined in paper applicable to few-shot settings beyond image-classification?",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,Multiple points in here,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely discusses the potential limitations of the proposed method without suggesting any specific actions or experiments that could be done to address these limitations. The review point also fails to provide any implicit actionable comments that could be inferred from the review point. It simply states the limitations without offering any solutions or further exploration of the topic. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point is delivered in a neutral tone, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer's comments are factual and do not contain any personal attacks or disrespectful remarks. The reviewer's questions and observations are presented in a straightforward manner, without any attempt to be overly polite or to provide constructive feedback. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point makes a claim about the benefit of incorporating semantic information in few-shot learning scenarios. However, it does not provide any logical deductions or references to external or internal sources to support this claim. The review point also questions the applicability of semantic information in few-shot settings beyond image-classification, but again, it does not offer any evidence or reasoning to back up this claim. The lack of verification and justification in the review point means that it does not meet the criteria for a score of 1. Therefore, based on the score rubric, the review point is not verified and does not provide logical explanations or references to support its claims. So the overall score is -1.",0,"The review point addresses the specificity of the paper's content, focusing on the use of semantic information in few-shot learning scenarios. It correctly identifies the 5-class, 1-shot learning case as the primary context where the benefits of semantic information are significant. However, it also raises a question about the applicability of semantic information in other few-shot learning settings, which is a valid point but not directly related to the specificity of the paper's content.

The review point does not provide a clear answer to the question of whether word-embeddings are available for train and test classes, which is a key aspect of the paper's methodology. This lack of specificity in the review point could lead to confusion among the authors about which parts of the paper are being addressed.

Moreover, the review point does not offer a clear direction for the authors to improve their paper. It only raises questions without providing any concrete suggestions for improvement. This lack of specificity in the review point could result in the authors being unsure about how to address the issues raised.

In conclusion, the review point does not provide a clear and specific answer to the question of which parts of the paper are being addressed, and it does not offer any concrete suggestions for improvement. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric."
4026,NIPS_2020_159,"UPDATE: Thank you for your response, which addressed most of my concerns. The only issue is that only controlling the number of leaves can still be problematic since the depth of the also matters [1]. [1] Reyzin L, Schapire RE. How boosting the margin can also boost classifier complexity, ICML 2006. ============================= I have several concerns and questions: 1. Line 97: what does “same size” mean? We know that in order to make a fair comparison, we must make sure that the model complexity of base learners is the same. For decision trees, does it mean the same number of leaves? To me, the best way could be just using a decision stump as a base learner. 2. The empirical results are also not convincing to me: 1) the results are only averaged over three runs, which is insufficient to me. 2) I would also like to see the standard deviations of the average accuracies. 3) The experiments are only evaluated on one data set. To make the conclusion more convicting, the author should make a comparison on more data sets. 4) I was also wondering if the conclusion still holds with other base learners. 3. We know that AdaBoost is just a special case of gradient boosting with the exponential loss. Therefore, my feeling is that the analysis is just about the gradient boosting with different loss functions, not the behavior of gradient boosting itself.","1. Line 97: what does “same size” mean? We know that in order to make a fair comparison, we must make sure that the model complexity of base learners is the same. For decision trees, does it mean the same number of leaves? To me, the best way could be just using a decision stump as a base learner.",1.0,1.0,1.0,0.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely poses a question about the meaning of ""same size"" in the context of decision trees, without offering any suggestions or experiments that could be done to clarify the meaning. The review point also does not provide any implicit actionable comments that could be inferred from the text. Instead, it only mentions a limitation in the model complexity of base learners without suggesting any specific actions that should be taken to address this limitation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The reviewer's question about the meaning of ""same size"" in the context of decision trees is a straightforward inquiry, lacking any aggressive or disrespectful tone. The suggestion to use a decision stump as a base learner is a constructive idea, but it is presented in a neutral manner without any encouragement or discouragement. The reviewer's tone is neither rude nor overly polite, and it does not contain any elements of rudeness or disrespect. The reviewer's language is factual and does not contain any personal opinions or judgments. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point in question does not make any claims, but rather asks a question about the meaning of ""same size"" in the context of model complexity. The question is not answered, and no attempt is made to verify the claim or provide a logical explanation. The review point does not include any references to external or internal sources, nor does it provide any justification for the claim. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor verifies it, and thus, it does not fulfill the requirements for a score of 1. The review point is more aligned with a score of 0, as it does not contain any claims or factual statements that do not need verifications. So the overall score is 0.",1,"The review point in question is quite specific, focusing on the concept of ""same size"" in relation to model complexity for base learners. It directly addresses the question of whether this means the same number of leaves for decision trees, and even suggests an alternative base learner, a decision stump. This level of specificity allows the authors to pinpoint exactly which part of the paper is being discussed, and they can be confident about the review point's focus. The review point is not only clear but also directly related to the paper's content, which is crucial for the authors to understand and address the feedback. Therefore, based on the score rubric, the review point is highly specific and directly addresses a part of the paper, making it obvious which part is being addressed. So the score is 1."
1193,ICLR_2023_1294,"weaknesses:
1: The best feature of CLIP is the generality, that is, being able to recognize any image without pre-defined/fixed classes. MUST adapt the CLIP model to a specific dataset (which is the main purpose of this paper). A simple solution is claimed by authors in the limitation section: ""There exists a simple way to address this concern: gather unlabeled image from all the domains of interest, and perform MUST to learn a single model that can generalize to multiple domains."" It would be really great to have 1-2 such experiments to verify this hypothesis.
Minor weaknesses:
2: The paper does not reach out to theoretical backup to explain why MUST works.
3: Adding results on ImageNet-Sketch will further strengthen this paper.
4: A related work [1] is worth discussing.
[1]: Test-time training with masked autoencoders, NeurIPS 2022","4: A related work [1] is worth discussing. [1]: Test-time training with masked autoencoders, NeurIPS 2022",0.0,0.0,-1.0,0.0,nan,1.0,0.0,0.0,0.0,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely mentions the limitations of the related work without suggesting any specific actions that should be taken. The review point does not give any hint or direct actionable comments that the authors should make. It lacks the necessary detail to guide the authors on how to improve their work or what specific changes they could make. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor encouraging, but rather a straightforward critique of the work. The reviewer does not express any personal opinions or emotions, and instead focuses on the technical aspects of the paper. The feedback is delivered in a neutral manner, without any overtly polite or impolite language. The reviewer's comments are factual and do not contain any personal judgments or biases. The feedback is also constructive, as it provides specific suggestions for improvement. However, the feedback could be more engaging and less passive to better align with the score of 1. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that a related work is worth discussing, without making any assertions or claims that could be verified or refuted. The review point does not include any logical deductions, nor does it reference any external or internal sources to support any claims. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a general statement rather than a claim that could be verified or refuted. As such, it does not fulfill the requirements for a score of 1, which would involve making a claim and verifying it with logical deductions and sources. So the overall score is 0.",1,"The review point provided is quite specific, directly addressing the concept of test-time training with masked autoencoders as presented in the paper. It clearly identifies the part of the paper that is being discussed, which is the methodology and its potential impact on the performance of the model. The authors of the paper would be able to pinpoint the exact sections of their work that are being critiqued, which is a key aspect of the score rubric. The review point is also concise and to the point, which is beneficial for the authors to understand the critique without unnecessary information. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it clear for the authors to identify the parts of the paper that are being addressed. So the overall score is 1."
2578,NIPS_2019_1348,"Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent. ",1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used.,-1.0,1.0,-1.0,0.0,nan,-1.0,1.0,1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the implications of using EHRM and the skewness of the human risk distribution without suggesting any specific actions or improvements that could be made. The review point also lacks any implicit actionable comments that could be inferred from the text. It simply points out the limitations of the paper without providing any guidance on how to address these limitations or improve the paper. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The feedback is based on the information provided in the paper, and it does not make any assumptions or judgments about the authors or their work. The reviewer also does not offer any suggestions or recommendations for improvement, which is a characteristic of a neutral review. Therefore, the review point is consistent with the score of 0, as it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the fairness implications of using EHRM, which is a side effect of different hyperparameter settings for the skewness of the human risk distribution. However, the claim lacks verification. There is no logical deduction or reference to external or internal sources that would support the claim. The review point simply states that there is no direct relationship between fairness consideration and the risk metric used, without providing any evidence or justification for this claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It is important to note that the claim made in the review point is not supported by logical deductions or references, which is a key aspect of the evaluation criteria. So the overall score is -1.",1,"The review point provided is quite specific, addressing the implications of using EHRM and the relationship between hyperparameter settings and fairness considerations. It clearly identifies the part of the paper that is being addressed, which is the discussion on the skewness of the human risk distribution and its impact on fairness. The authors can be confident that the review point is directly related to the specific section of the paper, as it directly references the use of different hyperparameter settings and their effect on fairness. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to pinpoint the exact part of the paper that is being discussed. 

In contrast, a review point with a score of -1 would not provide the authors with a clear understanding of which parts of the paper are being addressed. It would leave the authors uncertain about the specific content that is being critiqued. The lack of specificity in such a review point would not allow the authors to confidently identify the paper sections related to the review point. 

The review point provided is also noteworthy for its directness and relevance to the paper's content. It does not merely suggest that the paper has issues, but rather identifies a specific aspect of the paper that could be improved. This directness and relevance further contribute to the score of 1, as it allows the authors to understand exactly what needs to be addressed in their paper. 

In conclusion, the review point is highly specific and directly addresses a particular aspect of the paper, which is the fairness implications of using EHRM and the relationship between hyperparameter settings and fairness considerations. This specificity and directness make it clear to the authors which part of the paper is being addressed, thus fulfilling the criteria for a score of 1."
2606,NIPS_2019_1408,"Weaknesses: - The paper is not that original given the amount of work in learning multimodal generative models:   â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior.   â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths: - The experimental results are nice. The paper claims that their MMVAE modal fulfills all four criteria including (1) latent variables that decompose into shared and private subspaces, (2) be able to generate data across all modalities, (3) be able to generate data across individual modalities, and (4) improve discriminative performance in each modality by leveraging related data from other modalities. Let's look at each of these 4 in detail:   â (1) Yes, their model does indeed learn factorized variables which can be shown by good conditional generation on MNIST+SVHN dataset.   â (2) Yes, joint generation (which I assume to mean generation from a single modality) is performed on vision -> vision and language -> language for CUB,   â (3) Yes, conditional generation can be performed on CUB via language -> vision and vice versa.  Weaknesses: - (continuing on whether the model does indeed achieve the 4 properties that the authors describe)   â (3 continued) However, it is unclear how significant the performance is for both 2) and 3) since the authors report no comparisons with existing generative models, even simple ones such as a conditional VAE from language to vision. In other words, what if I forgo with the complicated MoE VAE, and all the components of the proposed model, and simply use a conditional VAE from language to vision. There are many ablation studies that are missing from the paper especially since the model is so complicated.   â (4) The authors have not seemed to perform extensive experiments for this criteria since they only report the performance of a simple linear classifier on top of the latent variables. There has been much work in learning discriminative models for multimodal data involving aligning or fusing language and vision spaces. Just to name a few involving language and vision:     - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016     - DeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013 Therefore, it is important to justify why I should use this MMVAE model when there is a lot of existing work on fusing multimodal data for prediction. ****************************Clarity**************************** Strengths: - The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information. - The table in Figure 2 nicely summarizes some of the existing works in multimodal learning and whether they fulfill the 4 criteria that the authors have pointed out to be important. Weaknesses: - Given the authors' great job in setting up the paper via Figure 1, Figure 2, and the introduction, I was rather disappointed that section 2 did not continue on this clear flow. To begin, a model diagram/schematic at the beginning of section 2 would have helped a lot. Ideally, such a model diagram could closely resemble Figure 2 where you have already set up a nice 'Venn Diagram' of multimodal information. Given this, your model basically assigns latent variables to each of the information overlapping spaces as well as arrows (neural network layers) as the inference and generation path from the variables to observed data. Showing such a detailed model diagram in an 'expanded' or 'more detailed' version of Figure 2 would be extremely helpful in understanding the notation (which there are a lot), how MMVAE accomplishes all 4 properties, as well as the inference and generation paths in MMVAE. - Unfortunately, the table in Figure 2 it is not super complete given the amount of work that has been done in latent factorization (e.g. Learning Factorized Multimodal Representations, ICLR 2019) and purely discriminative multimodal fusion (i.e. point d on synergy) - There are a few typos and stylistic issues: 1. line 18: ""Given the lack explicit labels availableâ -> âGiven the lack of explicit labels availableâ 2. line 19: âcan provided importantâ -> âcan provide importantâ 3. line 25: âbetween (Yildirim, 2014) themâ -> âbetween them (Yildirim, 2014)â 4. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite interesting. - The paper did a commendable job in attempting to perform experiments to justify the 4 properties they outlined in the introduction. - I can see future practitioners using the variational MoE layers for encoding multimodal data, especially when there is missing multimodal data. Weaknesses: - That being said, there are some important concerns especially regarding the utility of the model as compared to existing work. In particular, there are some statements in the model description where it would be nice to have some experimental results in order to convince the reader that this model compares favorably with existing work: 1. line 113: You set \alpha_m uniformly to be 1/M which implies that the contributions from all modalities are the same. However, works in multimodal fusion have shown that dynamically weighting the modalities is quite important because 1) modalities might contain noise or uncertain information, 2) different modalities contribute differently to the prediction (e.g. in a video when a speaker is not saying anything then their visual behaviors are more indicative than their speech or language behaviors). Recent works therefore study, for example, gated attentions (e.g. Gated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 or Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning, ICMI 2017) to learn these weights. How does your model compare to this line of related work, and can your model be modified to take advantage of these fusion methods? 2. line 145-146: ""We prefer the IWAE objective over the standard ELBO objective not just for the fact that it estimates a tighter bound, but also for the properties of the posterior when computing the multi-sample estimate."" -> Do you have experimental results that back this up? How significant is the difference? 3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.",- The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information.,-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,Is this really a weakness?,0,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely states that the introduction and motivation figures are clear and informative, without suggesting any specific actions or improvements that could be made. The review point also does not include any actionable comments that can be implicitly inferred, nor does it mention any limitations without providing any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer praises the introduction and the motivation figures, indicating a positive reception of the paper. The reviewer also provides a constructive suggestion about the informative nature of the figures, which is a helpful and encouraging tone. The reviewer's tone is not aggressive or disrespectful, and it is clear and constructive, which is why it fits the criteria for a score of 1. 

The reviewer's feedback is also specific and relevant to the paper, which is a key aspect of a constructive review. The reviewer's comments are focused on the content of the paper and offer suggestions for improvement, which is a positive and helpful approach. 

In contrast, a review point that would receive a score of -1 would be characterized by a rude or disrespectful tone. Such a review point would likely include negative comments about the paper's quality, the author's abilities, or the conference's standards. This type of review is not helpful or encouraging and would not meet the criteria for a score of 1. 

Overall, the review point provided is polite, constructive, and aligns well with the score rubric's criteria for a score of 1. It is clear that the reviewer has taken a positive and helpful approach to the review, which is why it is a good example of a review point that would receive a score of 1.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the paper is clear and that the introduction and motivation figures are informative. There is no claim made that requires justification or reference to external or internal sources. The review point is focused on the clarity of the paper and the informative nature of the figures, which are factual statements that do not require verification. Therefore, based on the score rubric, the review point does not contain any claims that need to be verified, and it does not miss any justification, explanation, or reference. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the introduction of the paper, particularly the motivation figures. It highlights the informative nature of figure 2, which is a clear indication of the reviewer's understanding of the paper's content. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the parts of the paper that are being addressed by the review point, as it directly references the introduction and the motivation figures. Therefore, the review point demonstrates a high level of specificity, which is in line with the score rubric's criteria for a score of 1. 

In contrast, a review point that lacks specificity would not provide clear guidance to the authors about which parts of the paper are being addressed. The authors would be left uncertain about which parts of the paper are being critiqued or praised. This lack of specificity would not meet the criteria for a score of 1, as it would not allow the authors to confidently identify the parts of the paper that are being addressed by the review point. 

Therefore, based on the score rubric, the review point provided is specific and directly addresses the introduction and motivation figures of the paper, making it clear which parts of the paper are being addressed. This specificity is what sets the review point apart from a review point that lacks specificity, and it is why the review point provided would receive a score of 1."
44,ICLR_2022_1016,"Weaknesses: 1. In this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time. 2. For previous SVD-based and NS-based methods, computation processes for forward and backward propagations are consistent. However, this work adopts Matrix Taylor Polynomial or Matrix Pade Approximation for forward propagations and uses approximate Lyapunov equation for backward propagation, leading variance in forward and backward propagations. The authors would better make some discussions about this issue. 3. The authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it.
[r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021.
Other comments: 1. It is clear that MPA involves matrix inverse, which is very GPU-unfriendly. As stated in the paper: ""Moreover, we note that the matrix inverse can be avoided, as Eq. (13) can be more efficiently and numerically stably computed by solving the linear system"". The authors would better provide more detailed computation and analysis. 2. How to compute the coefficients of p m and q n
for the Matrix Pade Approximation in equation (12)? Do forward operations of MPA in Table 1 contain computation of coefficients p m and q n
? 3. Does Equation (2) lack a (·){sym} operation for / f r a c l U
? 4. Is equation (11) missing a term z^{k} in the left side? 5. I am not sure why sign(B) in equation (21) can be calculated as identity matrix? 6. P{M} and Q_{N} are used to approximate the Taylor series. If I am not misunderstanding, does I- Q_{N}^{-1}P_{M} replace Q_{N}^{-1}P_{M} in Eqn. (13)? and do the terms I-X replace X in Eqn. (12)?","3. The authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it. [r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021. Other comments:",1.0,1.0,1.0,1.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the efficiency of the BP of MPA without suggesting any improvements or further research. The review point also lacks any implicit actionable comments that the authors can infer from the review. It does not provide any guidance on how to address the limitations mentioned or how to improve the method. The review point is more of a commentary on the existing work rather than a constructive critique with actionable insights. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does it show any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating the facts and asking a question about the method used. The reviewer does not make any personal judgments or comments about the quality of the paper, which is a key aspect of the score rubric. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and does not fit well with the other two scores.",-1,"The review point in question makes a claim about the efficiency of the BP of MPA, which is not directly supported by the provided information. The claim is that the BP of MPA is efficient, as shown in Table 6. However, the review point does not provide any logical deductions or references to external or internal sources that would verify this claim. The review point also fails to discuss why the BP of MPA is efficient, which is a key aspect of the claim.

In contrast, the review point does not contain any claims or factual statements that do not need verifications. It is focused on the efficiency of the BP of MPA and the comparison between approximate matrix square root and accurate SVD in global covariance pooling. However, it does not provide any justification or explanation for the claim made.

Therefore, based on the score rubric, the review point contains a claim that is not verified, as it misses a justification, explanation, or reference. The review point does not meet the criteria for a score of 1, as it does not support the claim with logical deductions or references. It also does not meet the criteria for a score of 0, as it does not contain any claims or factual statements that do not need verifications.

So the overall score is -1.",1,"The review point provided in r1 is specific and directly addresses the efficiency of the BP of MPA, which is a key aspect of the paper. It asks a direct question about the efficiency of the BP of MPA, which is a central theme of the paper. The review point also provides a clear connection to the paper's content, specifically referencing Table 6, which shows the efficiency of the BP of MPA. This specificity allows the authors to confidently identify which parts of the paper are being addressed by the review point. The review point is focused and relevant, which is why it would receive a score of 1 according to the score rubric. 

In contrast, the review point in r2 is vague and does not provide a clear connection to the paper's content. It does not ask a direct question about the efficiency of the BP of MPA, nor does it reference any specific part of the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The review point in r2 is not focused on the efficiency of the BP of MPA, which is a central theme of the paper. This lack of focus and specificity is why the review point in r2 would receive a score of -1 according to the score rubric. 

Therefore, the review point in r1 is more effective in addressing the paper's content and is more likely to lead to a constructive discussion about the efficiency of the BP of MPA. The review point in r2, on the other hand, is less effective in addressing the paper's content and is less likely to lead to a constructive discussion. 

So the overall score is 1."
4854,NIPS_2020_653,"The key issue is in Eq. 6, which appears to indicate that the normalising flow transformation is applied at each timepoint independently. This renders the model somewhat trivial (a Kalman filter with nonlinear outputs), extensively studied (e.g. in the EKF literature). It is well known that linear dynamics in a Kalman filter is a discretisation of an underlying continuous dynamical system, and you could use unevenly spaced observations if desired. Here are some possible claims that would render the work more interesting: 1.) Inclusion of the Jacobean in the output transformation renders the ML solution a better characterisation of the Bayesian solution. 2.) Having a normalising flow that depends on the value of the Weiner process at all past time steps (which allows much richer temporal dependencies). 3.) Arguing that the presently described process is surprisingly effective, by looking at a more empirical examples, and comparing to SOTA performance from referenced papers. Eq 12 might do some of this, but it is extremely unclear.",1.) Inclusion of the Jacobean in the output transformation renders the ML solution a better characterisation of the Bayesian solution.,-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the work without suggesting any specific actions that should be taken. The review point does not give any hint or direct actionable comments that the authors should make. It lacks the necessary detail to guide the authors on how to improve their work. The review point does not fulfill the criteria of providing actionable comments that can be implicitly inferred, nor does it suggest any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the requirements for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor encouraging, but rather a straightforward critique of the output transformation. The reviewer does not express any form of politeness or rudeness, maintaining a neutral stance throughout. The feedback is focused on the technical aspects of the paper, without any personal or emotional undertones. The reviewer's comments are factual and do not contain any language that could be construed as polite or impolite. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question makes a claim about the inclusion of the Jacobean in the output transformation, suggesting that it renders the ML solution a better characterization of the Bayesian solution. However, this claim is not supported by any logical deductions or references to external or internal sources. The review point does not provide any justification or explanation as to why the inclusion of the Jacobean would lead to a better characterization of the Bayesian solution. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The claim is not verified, and the review point misses a justification, explanation, or reference. Therefore, the review point is not aligned with the score rubric's requirement for verifiability.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the inclusion of the Jacobean in the output transformation. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed, and they can be confident that the review point is focused on this particular aspect. The review point does not leave room for ambiguity, and it is clear that the authors can identify the relevant sections of the paper. The review point is also well-structured, making it easy for the authors to understand and address the feedback. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1, as it provides a clear and specific direction for the authors to improve their paper."
4349,NIPS_2020_1636,"While the general idea of the paper is appealing and has been evaluated extensively, the presentation of the methodology is lacking in clarity at times. After reading section 3, some issues could have been addressed more clearly: • Regarding line 171/172: what do the authors mean by “regret reaches the plateau”? Can this be quantified? • If the tree is constructed as described, it is questionable that the leftmost leave is actually the ‘best’ leave. Can this be shown? • How is the problem treated that SVM can lead to many distinct areas in the described methodology? Consider the case of the 1D sine function and we have data points only at increments of pi. K-means would result in two clusters, i.e., the points with values +1 and -1, respectively. Then, SVM would potentially cluster the domain in the two classes resulting in alternating regions for each class. What would be the resulting domain for TuRBO then? In the very beginning, the authors mention that only deterministic objective functions are considered. It is not clear how this statement fits to the the main result: the optimization of policies on the MuJoCo tasks which are known to be inherently stochastic objectives. Does LA-MCTS depend on the deterministic assumption? If yes: why does it work well in practice on stochastic functions, and if not: Why assume it then? The empirical performance of LA-MCTS is impressive. However, the method combines many different building blocks and as such introduces many additional hyperparameters. Though an ablation study was performed, the performance of the method depends drastically on the choice of hyperparameters. As such, the practicability of the approach is limited as an additional layer of parameters needs to be tuned in addition to the BO parameters. It is not exactly clear, why the authors call their method ‘latent actions’ as these are just the decision boundaries from the SVM classifier. No theoretical work is presented. ------------------------------ After reading the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments. Tree construction: Being the best node in expectation is something different then being the best node. This should be made more clear in the main paper. Further, Figure 10 does not really help to make this issue more clear as for example the evaluated points are missing in the plot. How's the initial purple region selected when no data is available? Also, using a contour-plot to visualize the objective function would help to understand the figure better. Deterministic assumption: if no component depends on being deterministic, than I'd highly recommend removing this from the main paper in the beginning. Also, just using a sample mean of 5 rollouts does not lead to a deterministic function but just reduces the variance by a factor of 5, which can still be relatively high for RL tasks especially as the outcome does not necessarily follow a uni-modal distribution. Also, please make the use of multiple rollouts more transparent as this simplifies the RL problem drastically. Minor: Appendix A.1: Hit-and-Run and Gibbs sampling do not require the region to be a convex polytope. Overall: The approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference.","• If the tree is constructed as described, it is questionable that the leftmost leave is actually the ‘best’ leave. Can this be shown?",0.0,0.0,-1.0,0.0,nan,1.0,1.0,-1.0,0.0,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or experiments that can be done. It merely questions the validity of the leftmost leaf being the 'best' leaf without offering any concrete suggestions or methods to verify or disprove this claim. The review point does not give any hint or direct actionable comments that the authors should make, nor does it include any implicit actionable comments that the authors can infer. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply questions the validity of the leftmost leaf being the best leaf in the tree, without any personal opinion or judgment. It is a straightforward question that seeks clarification, which is a neutral approach to reviewing. The review point does not contain any elements of politeness or rudeness, and it does not provide any constructive feedback or suggestions for improvement. It is a neutral observation that does not fit well with the other two scores, which are more aligned with the tone of the review point. Therefore, the score of 0 is appropriate for this review point.",0,"The review point in question does not contain any claims, nor does it require verification. It is a neutral observation about the structure of a tree, which does not make any assertions or claims that could be verified or refuted. The review point simply questions the positioning of the leftmost leaf in the tree, without providing any evidence or reasoning to support this claim. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a rhetorical question rather than a claim that needs to be verified. As such, it does not fall into the categories of a score of 1, -1, or 0, as it does not contain any claims that could be verified or refuted. 

So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It merely questions the validity of the leftmost leaf being the 'best' leaf without providing any specific details or examples from the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors would likely be unsure about which parts of the paper are being addressed by the review point comment. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors would not be confident about the paper part that is being addressed by the review point. So the overall score is -1."
4495,NIPS_2020_1367,"1) In my experience, it is commonly collapsed when training the network with BN directly normalized by moving statistics. Batch renormalization technique [1] can address this problem. If you have used batch renormalization in the experiments, it is better to cite it in the experimental part. If you used a new technique to avoid training from collapse, please give the details. [1] Ioffe,S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In Advances in neural information processing systems, pp. 1945–1953, 2017. 2) I think it might need more comparison/ablation experiments to show the characteristics of StochNorm. There are no experiments of comparing stochastic normalization to each branch batch normalization with the current mini-batch statistics or the moving statistics. The parameter selection probability $p$ is significant, and it is better to offer the parameter sensitivity analysis in the experiments. 3) StochNorm is orthogonal to other fine-tuning methods, which is claimed to be an advantage. However, when observing Table 3 and Table 4, I find the performances of L^2_SP + StochNorm and DETAL + StochNorm on dataset Stanford Cars is even inferior to that of single StochNorm, which might be inconsistent with the claim. 4) A typical NeurIPS paper commonly offers us insights with theoretically grounded analysis. This work is relatively weak in this aspect.",4) A typical NeurIPS paper commonly offers us insights with theoretically grounded analysis. This work is relatively weak in this aspect.,-1.0,-1.0,-1.0,0.0,nan,1.0,-1.0,-1.0,0.0,"[A]: Can kind of be argued in both ways. Reviewer wants more theoretically grounded analysis. However, this is very abstract.",-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It lacks specificity and does not provide any guidance on how the authors could improve their work. The review point merely points out the limitations of the paper without suggesting any concrete actions that could be taken to address these limitations. This lack of actionable comments makes it difficult for the authors to deduce what needs to be done to improve their paper. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the expected tone of a review. It lacks the politeness and constructive criticism that is expected in a review. The tone is more critical and dismissive, which is not conducive to a constructive review. The reviewer does not offer any constructive feedback or suggestions for improvement. Instead, the reviewer simply states the shortcomings of the paper without providing any guidance or direction for the authors to improve their work. This approach is not helpful and does not encourage the authors to improve their work. The reviewer's tone is more aggressive and disrespectful, which is not appropriate for a review. Therefore, based on the score rubric, the review point is not helpful, encouraging, or polite, and it does not meet the criteria for a constructive review. So the overall score is -1.",0,"The review point provided does not contain any claims that need to be verified. It is a general statement about the quality of the paper in terms of theoretical grounding, which is not a claim that requires justification or verification. The review point does not make any assertions that could be supported by logical deductions or references to external or internal sources. It is a factual statement that does not require verification. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer specific insights into the paper. It fails to identify any particular part of the paper that is being addressed, which is crucial for the authors to understand and improve their work. The review point does not provide any concrete examples or suggestions that would help the authors to pinpoint the areas that need improvement. Instead, it offers general comments that do not contribute to a constructive critique. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point. The review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. It also does not allow the authors to guess which parts are being addressed in the review point, as it lacks the necessary detail and specificity. So the overall score is -1."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.","- It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the claim about safe reinforcement learning is inaccurate, without suggesting any specific actions or improvements that could be made. The review point also does not include any actionable comments that can be implicitly inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionability makes it difficult for the authors to understand what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply states the claim made in the paper and then provides a counter-argument based on the abstract of a different paper. It does not offer any constructive feedback or suggestions for improvement. The review point is also not overly critical or positive, which is why it fits the score of 0. It is a straightforward statement of fact, which is the default score if the review point does not fit well with the other two scores. 

The review point does not contain any elements of politeness or rudeness, which is why it does not fit the criteria for a score of 1 or -1. It is a neutral review point that does not engage with the authors in a way that would be considered polite or rude. 

In conclusion, the review point is neutral and factual, and it does not fit the criteria for a score of 1 or -1. It is a straightforward statement of fact, which is why it fits the score of 0.",0,"The review point in question makes a claim about the nature of safe reinforcement learning, stating that it is not just about avoiding negative side effects during training, but also about safety at deployment. This claim is supported by a citation from a paper by García et al. (2015). However, the review point does not provide a logical deduction or an explanation of why the claim is verified. It also lacks a reference to the specific part of the paper that supports the claim. The review point does not meet the criteria for verifying a claim through logical explanations or external sources. Therefore, the review point does not fulfill the requirements of the score rubric for verifiability. 

In contrast, a review point that meets the criteria for verifiability would provide a logical deduction or an explanation of why the claim is verified, and it would include a reference to the specific part of the paper that supports the claim. For example, a review point that discusses the statistical analysis in a paper and points out that the p-values reported for the t-tests do not match the standard thresholds for significance, would provide a logical deduction and reference to the guidelines in 'Statistics for Biologists' (Smith et al., 2020). 

Based on the score rubric, the review point in question does not contain any claims, or only includes factual statements and claims that do not need verifications. Therefore, the review point does not meet the criteria for verifiability. 

So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a key aspect of the paper in question. It points out a discrepancy in the claim made by the authors regarding the focus of safe reinforcement learning. The review point clearly identifies the part of the paper that is being addressed, which is the claim about the focus of safe RL. This specificity allows the authors to pinpoint the exact section of the paper that needs to be reevaluated or clarified. The review point also provides a clear direction for the authors to follow in order to address the issue raised. Therefore, the authors can be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of which part of the paper is being addressed. It would leave the authors uncertain about which sections of the paper are relevant to the review point. This lack of specificity would not help the authors in identifying the exact part of the paper that needs to be revised or clarified. 

The review point provided is well-aligned with the score rubric, as it demonstrates a high level of specificity, allowing the authors to confidently identify the part of the paper that is being addressed. This is in line with the score rubric's criteria for a score of 1, which indicates that the authors can be confident about which part of the paper is being addressed by the review point. 

So the overall score is 1."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","- Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper.",1.0,1.0,1.0,1.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely asks a question about the use of DBpedia properties as features, without suggesting any specific actions or experiments that could be done. The review point also does not provide any implicit actionable comments that could be inferred from the text. It simply states that it is crucial to list all the features in details, but does not offer any guidance on how to achieve this. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and thus would receive a score of -1.",1,"The review point in question is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer's tone is helpful and encouraging, as they express a desire for clarity and replicability in the paper. The reviewer's request for a list of all the features used in the paper is a clear and polite way to seek additional information, which is essential for understanding the paper's methodology. The reviewer's language is respectful and their feedback is focused on improving the paper, rather than criticizing it. This approach is likely to be well-received by the authors and can contribute to a more constructive dialogue. Therefore, based on the score rubric, the review point is indeed polite and constructive.",-1,"The review point in question is about the usage of DBpedia properties as features in the paper. The claim made in the review point is that the authors may not have used all the properties, and if not, a list of the used properties is requested. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The review point also does not provide any explanation as to why listing all the features is crucial for clarity and replicability of the paper. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It contains a claim that is not verified, and it misses a justification, explanation, or reference. So the overall score is -1.",0,"The review point in question is about the specificity of the DBpedia properties used as features in the paper. The review point asks whether all the properties have been used or if there is a subset, and if the latter, it requests a list of these properties. This question is directly related to the paper's content and methodology, and it is crucial for understanding the scope and limitations of the study.

The authors' response to this review point is not clear and does not provide the necessary details. The authors mention that they have used a subset of the DBpedia properties but do not specify which ones. This lack of specificity makes it difficult for the reader to understand the exact features used in the study.

The review point is well-defined and directly addresses a specific aspect of the paper. It is clear that the authors should be able to identify which parts of the paper are related to the review point. However, the authors' response does not provide the necessary clarity and detail, which is essential for the replicability and understanding of the paper.

In conclusion, the review point is well-defined and directly addresses a specific aspect of the paper. The authors' response, however, lacks the necessary specificity and detail, making it difficult for the reader to understand the exact features used in the study. Therefore, the authors are not fully confident about which part of the paper is being addressed by the review point."
3735,NIPS_2020_1660,"- The work heavily builds on the previous work of Cutkosky and Orabona [10]. In particular, the power of the proposed BCO method really relies on the paramter-free nature of the scale learning interface in ""Interface 3"". So this diminishes its significance from the side of technical contributions. - In line 72, the authors argue two new ideas ""appropriate surrogate loss function"" and ""a new one-point gradient estimator with time-varying parameters"". I admit that the design of surrogate loss is clever, while the gradient estimator seems standard, with only a slight and nature twist for the problem. So it is not appropriate to emphasize much on that.","- In line 72, the authors argue two new ideas ""appropriate surrogate loss function"" and ""a new one-point gradient estimator with time-varying parameters"". I admit that the design of surrogate loss is clever, while the gradient estimator seems standard, with only a slight and nature twist for the problem. So it is not appropriate to emphasize much on that.",0.0,1.0,-1.0,0.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments that the authors should make. It merely acknowledges the clever design of the surrogate loss function and the slight twist in the gradient estimator without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors can deduce what needs to be done. It also does not mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer acknowledges the clever design of the surrogate loss function and the slight twist in the gradient estimator, but does not emphasize these aspects. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states their observations without any personal opinion or bias. The feedback is focused on the content of the paper, rather than the tone or the reviewer's personal feelings. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point in question does not make any claims, nor does it provide any justification or reference to support the statements made. The reviewer acknowledges the clever design of the surrogate loss function and the slight twist in the gradient estimator, but does not offer any verification or logical deductions to support these claims. The reviewer also does not provide any external or internal sources to substantiate the claims. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. The reviewer's statements are based on personal opinion and lack the necessary evidence or reasoning to be considered verified. So the overall score is 0.",1,"The review point provided is quite specific, addressing two distinct ideas presented in the paper. The first idea, ""appropriate surrogate loss function,"" is clearly identified and the reviewer's opinion on its cleverness is stated. The second idea, ""a new one-point gradient estimator with time-varying parameters,"" is also identified and the reviewer's view on its standard nature with a slight twist is expressed. This specificity in the review point allows the authors to pinpoint exactly which parts of the paper are being addressed, which is in line with the score rubric's criteria for a score of 1. The authors can be confident that the review point is directly related to the two ideas mentioned in the paper. Therefore, the review point is well-aligned with the score rubric's requirements for a score of 1."
1353,ICLR_2023_3031,"Weaknesses: - The technique seems to be in its early stages and it seems as if tuning needs to be performed for every new dynamical system and setting. First, the different training phases are difficult to follow and makes me wondering if the pipeline is robust enough. Also, please explain how was chosen to use a 4-layer U-Net for the Lorenz-63 and only a 3-layer U-Net for the Lorenz-96. - Network structure: it is not clear which losses are minimized at the different phases, in particular, the perturbator+flow operator is not clear. From eq. 11, it looks as if the dynamics and the data fidelity losses were applied on the same current state x_hat, while my understanding was that L_rec was calculated on the perturbator's output while L_dyn was calculated on the flow operator's output. If the hybrid loss is calculated on the final output of the 2 blocks, I don't se how we can enforce the decoupling of the two goals as stated. - It is difficult to understand the size of the input data and latent data. In particular, please give axis labels on Figure 1 images, as it first looks like a 2D spatial problem, while later it is explained that the input is of size time and location. Please also clarify what does T represent in Figure 2: I guess the time dimension, and in this case where is the location dimension? - Please explain best the following sentence: 'For the case in which the prior dynamics are unbiased': in a real setting, how can we know if the prior dynamics are biased or not? - 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive? - Why stopping at 8 and 10 blocks for the experiments, while it looks as if the results are always improving with more blocks?
Typos: Specifically, The perturbator
The perturbator uses the observations and labels it has learned to perturb the reconstructed states to make it deviate from the original flow --> please rephrase
Figure 3: the color do not match Figure 1, as here the colors are also linked to training/no training. Maybe use another sign to indicate training/no training, as a red line surrounding the box.",- 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive?,0.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the representation of 'espilon(t)' is reductive, without suggesting any specific actions or improvements that could be made to address this issue. The review point does not provide any implicit actionable comments that could be inferred by the authors, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not give any hint, or direct actionable comments. After reading the review point, the authors still do not know or can not deduce what needs to be done. This lack of actionable content is why the review point would receive a score of -1 according to the score rubric. 

So the overall score is -1.",0,"The review point in question is not particularly polite or constructive. It uses a somewhat dismissive tone by stating that the representation of white Gaussian noise is ""reductive."" This choice of words could be perceived as a lack of respect for the original work. The reviewer does not provide any constructive feedback or suggestions for improvement, which is a key aspect of a polite and helpful review. Instead, the reviewer simply states a problem without offering any solutions or alternative approaches. This lack of constructive feedback and the somewhat dismissive tone make the review point less helpful and more critical than it needs to be. Therefore, based on the score rubric, the tone of the review point is not helpful or encouraging, and it does not meet the criteria for a score of 1. So the overall score is 0.",-1,"The review point in question makes a claim about the nature of 'espilon(t)', suggesting it represents white Gaussian noise. However, it fails to provide any form of verification or justification for this claim. The reviewer does not offer any logical deductions or references to external or internal sources that could substantiate the claim. This lack of verification is a significant shortcoming, as it does not meet the criteria for a score of 1, which requires the claim to be supported by logical deductions and internal or external sources. The review point also does not contain any factual statements or claims that do not need verifications, which would have been a requirement for a score of 0. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a higher score. So the overall score is -1.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The statement ""espilon(t) represents the white Gaussian noise"" is a general comment that could apply to many different sections of the paper. It does not provide any specific details or examples that would help the authors to understand which part of the paper is being discussed. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors would likely be unsure about which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
1451,ICLR_2023_2368,"Weaknesses: 1. There is no theoretical guarantee that the discoveries resulting from the sparse network architecture are unique. 2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here. 3. It lacks model complexity analysis and comparison. Given different levels of biological entities' intra and inter interactions, I am worried about the model's real applicability.","2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here.",0.0,1.0,-1.0,0.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a missed description in the paper, which is the quantification of the top-k frequent interaction pairs. The review point does not suggest any specific actions or improvements that could be made to the paper. It also does not provide any implicit actionable comments that the authors could infer from the review point. The review point only mentions a limitation in the paper without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts about the paper's content and its shortcomings, without adding any personal opinion or judgment. The reviewer does not make any negative or positive comments about the paper's quality or the authors' efforts. Instead, the reviewer focuses on the specific aspects of the paper that were not addressed, such as the lack of a method to quantify the top-k frequent interaction pairs. The reviewer also points out the limited results and discussions presented in the paper, which is a factual observation rather than a critique. Therefore, the review point is neutral and does not fit well with the other two scores, which are more focused on the tone of the review. So the overall score is 0.",-1,"The review point in question makes a claim about the lack of detail in the paper regarding the quantification of top-k frequent interaction pairs. However, it fails to provide a logical explanation or reference to external or internal sources that would verify this claim. The review point merely states that the paper missed this aspect without offering any justification or explanation. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The review point does not contain any claims that do not need verifications, nor does it provide any factual statements that could be verified. Therefore, based on the score rubric, the review point is not verifiable and does not meet the criteria for a score of 1.",1,"The review point provided is quite specific, pointing out a clear deficiency in the paper's description of how to quantify the top-k frequent interaction pairs. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the section in question, which is section 5.4. This level of specificity is crucial for the authors to understand the exact area where the paper falls short.

Moreover, the review point directly addresses the paper's claim about novel discovery, highlighting the lack of results and discussions in this area. This specificity not only identifies the problem but also relates it to the paper's main advantage, which is a key aspect of the paper's evaluation.

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of which part of the paper is being addressed. It would leave the authors guessing or unsure about the exact area of concern.

Therefore, based on the score rubric's criteria for specificity, the review point provided is well-crafted and directly addresses the paper's shortcomings. It is clear, specific, and directly related to the paper's main claims, making it an effective review point."
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.","1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness.",0.0,1.0,1.0,0.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the absence of a study among different adversarially trained models, which is a limitation of the paper. However, it does not suggest any specific actions that the authors could take to address this limitation. The review point also does not provide any implicit actionable comments that the authors can infer. It simply states the lack of a study without offering any guidance on how to proceed with such a study. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it offer implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply states the absence of a study among different adversarially trained models, which is a factual observation. It does not provide any constructive feedback or suggestions for improvement. The review point is also not overly critical or dismissive, which is why it does not fall into the category of a score of -1. It is a straightforward observation that the trade-off among robust trained models is unclear. Therefore, based on the rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the lack of a study among different adversarially trained models, which is a critical aspect of the evaluation criteria. However, the review point fails to provide any verification or justification for this claim. It does not offer any logical deductions or references to external or internal sources that could support the claim. The absence of any verification or justification for the claim makes it difficult to assess the validity of the review point. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",1,"The review point provided is quite specific and directly addresses a key aspect of the paper, which is the trade-off among different adversarially trained models. It specifically mentions the TRADES model and its potential to improve both robustness and back-door robustness. This level of specificity allows the authors to clearly identify which part of the paper is being addressed by the review point. The authors can be confident that the review point is focused on the comparison of different models and their respective robustness and back-door robustness. This is in line with the score rubric, which indicates that a score of 1 is given when the review point makes it obvious which part of the paper is being addressed. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
5339,NIPS_2020_1039,"1) It could be better if a more comprehensive comparison of the asymptotic performances among on/off-policy methods. Off policy methods enjoy better sample efficiency at the cost of higher computation burden. Maybe an additional table could be provided in the appendix. Personally, I don’t actually expect a gap between the on/off-policy methods. 2) Although the usage of current f-function in the f-divergence is justified, it will still be interesting to see a comparison if the alternative f-functions were adopted. An empirical comparison would further justify the usage of the author’s current choice. Minor: 1) In eq. 12 (the proof of Lemma 2), in the third from the last line, the integration should be over \mathcal{S} \times \mathcal{S} instead of \mathcal{S} \times \mathcal{A} \times \mathcal{S}. 2) Notations in sec. 8.6 are inconsistent: sampling from \pi is switching back and forth between $a \sim \pi(s)$ and $a \sim \pi(\cdot | s)$. 3) At the end of Algorithm box 1, does J_{\nabla \theta} J_{reg} (\pi_{\theta}) intends to mean that the gradient of \theta on J_{reg}? 4) Can the authors slightly justify the first equation in sec. 3.2? Why is that an equality instead of an inequality?","1) In eq. 12 (the proof of Lemma 2), in the third from the last line, the integration should be over \mathcal{S} \times \mathcal{S} instead of \mathcal{S} \times \mathcal{A} \times \mathcal{S}.",1.0,1.0,-1.0,0.0,nan,1.0,1.0,-1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out a mistake in the integration domain of the proof, without suggesting any corrections or improvements. The authors are left without any guidance on how to rectify the error or what steps to take to ensure the correctness of their proof. The review point does not provide any implicit actionable comments that could be inferred by the authors, nor does it mention any limitations without suggesting any explicit or implicit actions to be taken. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of politeness or rudeness, but rather states the correct integration domain in the proof of Lemma 2. The tone is purely informative, without any encouragement or criticism. The reviewer simply points out the error in the integration domain, which is a factual observation. This is in line with the score of 0, which is given to a review point that is neutral, factual, or passive. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a characteristic of a neutral review. Therefore, the review point is not aggressive, disrespectful, or encouraging, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a specific claim regarding the integration domain in the proof of Lemma 2. The claim is that the integration should be over the set of all possible states, denoted by \mathcal{S} \times \mathcal{S}, rather than over the set of all possible states and actions, denoted by \mathcal{S} \times \mathcal{A} \times \mathcal{S}. This claim is not supported by any logical deductions or references to external or internal sources. The reviewer does not provide any justification or explanation as to why the integration should be over the set of all possible states, which is a critical aspect of the proof. Therefore, the review point fails to meet the criteria for verifiability as outlined in the score rubric. The claim is not verified, and the reviewer does not provide any logical explanations or references to support the claim. As a result, the review point does not meet the standards for performance in terms of verifiability. So the score is -1.",1,"The review point provided is specific and directly addresses a particular part of the paper, which is the integration in the proof of Lemma 2. The review point clearly states that the integration should be over \mathcal{S} \times \mathcal{S} instead of the current integration over \mathcal{S} \times \mathcal{A} \times \mathcal{S}. This specificity allows the authors to identify exactly which part of the paper is being addressed, and they can be confident that the review point is pointing out a mistake in the integration. The review point is not only specific but also directly related to the mathematical proof, which is a crucial part of the paper. Therefore, the authors can be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not allow the authors to identify which parts of the paper are being addressed. For example, a review point that simply states that the paper is hard to follow or that some sections need to be rewritten would not provide the authors with any clear direction on how to improve their paper. The lack of specificity in such a review point would leave the authors unsure about which parts of the paper are being addressed. 

Therefore, based on the score rubric, the review point provided is specific and directly addresses a particular part of the paper, which is why it would receive a score of 1. The authors can be confident about which part of the paper is being addressed by the review point, and they can take action to correct the mistake."
4364,NIPS_2020_285,"- In Section 2, it is assumed that the state and action spaces are finite. Is this assumption really necessary? It might be quite limiting since policy gradient methods are typically employed when dealing with continuous state-action spaces. Moreover, the transition model is defined as deterministic. Is this assumption necessary? - Proposition 6: I am a little confused about the notation. Equation (13) employs the improvement operator for the value-based case, but the remark in the subsequent line is stated for the improvement operator for the trajectory-based case. - Proposition 5: This is more curiosity than an issue. Are there some sufficient conditions to enforce that Var(R) > 0 along the whole learning process? It seems to me that as we get close to the optimum we are going to prefer less stochastic policies, thus we slow down convergence. Do you think we can still converge asymptotically if deterministic policies are allowed? Anyway, in policy search, we could even limit to stochastic policies. Maybe in such a case, we can have a non-zero guaranteed improvement and, consequently, converge in a finite number of iterations. I think the paper would greatly benefit from a discussion on these points. - Proposition 2: This is also a curiosity. The optimal policy, in the considered policy space, is a fixed point of the operator. There can be other fixed points? If so, do the authors think that is possible to characterize the space of fixed-points? Are there some conditions under which the fixed point is unique? ***Minor*** - lines 50 and 62: s_{t_1} -> s_{t+1} - Equation (3) goes beyond margins - line 64: reporting the formal definition of d^\pi might help - Equation (24) there should be a \propto instead of = - Proposition 5: there should be a statement, not just a formula - The notation of Proposition 9 does not match that used in the proof (z vs f(R)) - Figure 2: not very readable in grayscale, I suggest using different linestyles or markers","- Proposition 6: I am a little confused about the notation. Equation (13) employs the improvement operator for the value-based case, but the remark in the subsequent line is stated for the improvement operator for the trajectory-based case.",0.0,1.0,0.0,0.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a discrepancy in the notation used in the paper, without suggesting any specific actions or improvements that could be made. The review point does not offer any insight into how the authors could resolve the confusion or improve the clarity of their work. It also does not provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and thus, it would receive a score of -1.",1,"The review point in question is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses confusion about the notation, which is a neutral observation. However, the reviewer also provides a helpful suggestion by pointing out the discrepancy between the notation used in the value-based case and the trajectory-based case. This indicates a constructive approach to the review, as the reviewer is not only identifying a potential issue but also offering a solution to improve the clarity of the paper. The tone of the review point is respectful and encouraging, which is in line with the score rubric's definition of politeness. Therefore, the review point is helpful and encouraging, which is why it would receive a score of 1 according to the score rubric.",0,"The review point in question does not make any claims, nor does it require verification. It merely points out a discrepancy in the notation used in the paper, which is a factual statement and does not require any justification or reference. The reviewer is simply confused about the notation and does not attempt to verify any claims or provide any logical deductions or references. Therefore, according to the score rubric, the review point does not contain any claims, and it does not require verification, which aligns with the score of 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The reviewer's confusion about the notation and the improvement operator is not clearly linked to a specific section or idea in the paper. The reviewer's uncertainty about the paper part being addressed is not resolved by the review point, which leaves the authors unsure about the specifics of the reviewer's concerns. The lack of specificity in the review point does not allow the authors to confidently identify the paper part being addressed, which is a key criterion in the score rubric. Therefore, the review point does not meet the requirements of the score rubric, and the authors would not be confident about which part of the paper is being addressed by the review point. So the score is -1."
4874,NIPS_2020_62,"- The paper's narrative is based around POMDPs, but the experimental evaluation does not really stress the capability of the method in that respect. Evaluation is done on pixel-based control, which is PO of course, but we have know that a lagged observation of a few time-steps can make the state fully observable quickly. (See the appendix of [1]). Hence, we do not know how the method fares in environments where the state uncertainty has to be actively reduced by the agent. Therefore I think the paper overstates the results. It is easy to get out of this, however, since one can just drop the POMDP claim. - The justification of the overall approach could have been improved. For me personally (and the optimal control community) it is obvious that we want some kind of state estimation when we use control, as most–if not all–practical problems are PO. But the paper could have done a much better job at its justification. E.g. a very noisy sensor that requires a few time steps waiting to correctly estimate a quantity makes such approaches necessary. The authors suffer from the fact that the RL community is somewhat focused on Mujoco-like benchmarks, which are representative of only a very small fraction of practical optimal control problems. But the authors could have chosen to use a different suite of environments, such as EscapeRoomba or MountainHike, which would illustrate this. If the authors had chosen to conduct experiments that tackle much more relevant POMDP problems, I'd have given an increased score. - I would have enjoyed an ablation whether AISOC/MaxEnt is necessary. [1] **CURL: Contrastive Unsupervised Representations for Reinforcement Learning** Michael Laskin*, Aravind Srinivas*, Pieter Abbeel. Thirty-seventh International Conference Machine Learning (ICML), 2020.","- The justification of the overall approach could have been improved. For me personally (and the optimal control community) it is obvious that we want some kind of state estimation when we use control, as most–if not all–practical problems are PO. But the paper could have done a much better job at its justification. E.g. a very noisy sensor that requires a few time steps waiting to correctly estimate a quantity makes such approaches necessary. The authors suffer from the fact that the RL community is somewhat focused on Mujoco-like benchmarks, which are representative of only a very small fraction of practical optimal control problems. But the authors could have chosen to use a different suite of environments, such as EscapeRoomba or MountainHike, which would illustrate this. If the authors had chosen to conduct experiments that tackle much more relevant POMDP problems, I'd have given an increased score.",1.0,1.0,1.0,1.0,nan,1.0,1.0,1.0,0.0,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the limitations of the paper and suggests that the authors could have chosen different environments for their experiments. However, it does not provide any specific guidance or recommendations on how to improve the paper or address the issues raised. The review point also lacks any implicit actionable comments that could be inferred by the authors. It does not suggest any particular experiments or changes that could be made to enhance the paper's quality or relevance. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is somewhat critical and lacks the constructive tone that would be expected for a score of 1. The reviewer does not offer any positive feedback or suggestions for improvement, which is a key aspect of a helpful and encouraging review. Instead, the reviewer focuses on the shortcomings of the paper and the community's focus on Mujoco-like benchmarks. While the reviewer does mention the potential for increased relevance by using different environments, this is not presented in a way that would be seen as helpful or encouraging. The tone of the review point is more neutral and factual, which aligns with a score of 0. The reviewer does not come across as aggressive or disrespectful, but the lack of positive reinforcement and constructive criticism makes the review point less effective in providing guidance for the authors. So the overall score is 0.",-1,"The review point provided contains a claim that is not verified, which is a significant shortcoming according to the score rubric. The claim is that the authors' approach to optimal control is not justified, and the reviewer suggests that the authors could have chosen different environments to better illustrate the necessity of state estimation. However, the reviewer does not provide any logical deductions or references to external or internal sources that would verify this claim. The reviewer also fails to provide any justification for why the chosen environments are not representative of practical optimal control problems. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified. 

The reviewer's claim that the authors' approach is not justified is not supported by any logical deductions or references to external or internal sources. The reviewer's suggestion that the authors could have chosen different environments is also not supported by any logical deductions or references to external or internal sources. The reviewer's claim that the authors' approach is not justified because the RL community is focused on Mujoco-like benchmarks is not verified, and the reviewer does not provide any justification for this claim. 

In conclusion, the review point does not meet the criteria for a score of 1 because it contains a claim that is not verified, and it lacks logical deductions and references to external or internal sources that would verify the claim. The reviewer's claims are not supported by any logical deductions or references to external or internal sources, and the reviewer does not provide any justification for the claims made. Therefore, the review point is not verifiable and does not meet the criteria for a score of 1.",1,"The review point provided is quite specific and directly addresses the approach taken in the paper. It points out the lack of justification for the overall approach and suggests that the authors could have chosen different environments to better illustrate the practicality of their method. This specificity allows the authors to identify which parts of the paper are being addressed by the review point. The review point also provides a clear direction for improvement, which is to conduct experiments on more relevant POMDP problems. This specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. The review point does not leave any ambiguity about the paper part that is being addressed, and it directly addresses the approach taken in the paper. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations.,0.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a limitation in the computations of the L2 Wasserstein distance without suggesting any specific actions to address this issue. The review point does not provide any implicit actionable comments that can be inferred, which leaves the authors without a clear direction on how to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The reviewer simply states the issue with the stability of the model and the lack of discussion on the numerical aspects of the L2 Wasserstein distance computations. The reviewer's tone is neutral, and the feedback is delivered in a straightforward manner without any personal attacks or overly positive comments. The reviewer's focus is on the technical aspects of the paper, which is a factual and neutral approach to reviewing. Therefore, the review point is in line with the score rubric's criteria for a score of 0.",-1,"The review point in question makes a claim about the stability problems of the Extended Projection (EP) method when moment updates are not numerically accurate. However, it fails to provide any verification or justification for this claim. The reviewer mentions the issue of quadrature approximations but does not delve into the specifics of how these approximations affect the stability of the EP method. Furthermore, the reviewer does not provide any logical deductions or references to external or internal sources that could support the claim. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified. 

In contrast, a review point that would receive a score of 1 would contain a claim that is supported by logical deductions and internal or external sources. For example, a reviewer might claim that the statistical analysis in a paper is incorrect because the p-values reported for the t-tests do not match the standard thresholds for significance. This claim could be verified by referencing the guidelines in 'Statistics for Biologists' (Smith et al., 2020), which would provide a logical explanation and an external source to support the claim. 

Therefore, based on the score rubric, the review point in question does not meet the criteria for a score of 1 because it lacks verification and justification for its claims.",1,"The review point provided is quite specific, focusing on the numerical accuracy of the L2 Wasserstein distance computations. It directly addresses the stability problems that arise when the moment updates are not numerically accurate, which is a critical aspect of the paper. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are related to the review point. The authors can be confident that the review point is addressing a specific part of the paper, which is the computational aspect of the L2 Wasserstein distance. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to confidently identify the part of the paper being addressed. 

In contrast, a review point that lacks specificity would not provide clear guidance to the authors. It would be vague and could apply to multiple parts of the paper, making it difficult for the authors to pinpoint the exact section that needs improvement. This lack of specificity would result in a score of -1, as the authors would not be able to confidently identify the part of the paper being addressed. 

Therefore, based on the score rubric, the review point provided is specific and directly addresses a critical aspect of the paper, making it easy for the authors to identify and address the issue. This specificity is what sets the review point apart from a review point that lacks specificity, and it is why the review point provided would receive a score of 1."
3549,NIPS_2020_1360,"The idea of the paper is simple and the motivation of the approach is well justified. However i have some concerns on the novelty of the work and the comparison with existing results. In particular I have the following concerns: Main Issues: I am concern regarding two of the main contributions of this work. I believe that some of the claims on the main contributions section are not novel. 1) The fact that the stochastic EG did not converge was shown with a counterexample in Chavdarova et al. [2]. However, in line 82, the authors mentioned that their approach on counterexample is an improvement because they show that the non-convergence persists for any error distribution with positive variance. In my opinion the proposal counterexample is not really a main contribution of this work. This work builds upon the counterexample of Chavdarova et al. [2] and use it to propose a convergent variant of the EG. 2) In line 55 the authors claim: ""Prior to our work, last-iterate convergence rate for bilinear min-max games had only been studied in the deterministic setting."" This is not true. In ICML 2020 Loizou et. al proposed the analysis of stochastic Hamiltonian methods showing last-iterate convergence for stochastic bilinear games and some classes of non-convex non-concave games. They also proposed a variance reduced method showing linear convergence (which is much faster then the O(1/t) rate of the DSEG in this setting). Other Issues: 3) The authors mentioned that the error bound condition is satisfied for two large classes of problems: Strongly monotone operators and Affine operators. However the definitions of the above two problems were never explicitly given. In addition for the affine operators it is mentioned that $\tau$ is the minimum non-zero singular value of the matrix. Which matrix? the authors need to be more rigorous. 4) In line 116 the authors start using capital letters for the notation (see $X_t$). until this point everything was lower case. 5) In experiments the only method presented is the DSEG with different values for $r_\gamma$ and $r_eta$. This is not adequate. The method should compare with other methods that guarantee convergence for the classes of games under study, like the stochastic Hamiltonian methods mentioned above. Even if the DSEG will be slower it will be really helpful for the reader. In addition the indicators of the lines in the Figure 3 are not distinguished (for colour-blind readers)",4) In line 116 the authors start using capital letters for the notation (see $X_t$). until this point everything was lower case.,0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a change in notation style without suggesting any specific actions or improvements that could be made. The review point does not offer any insight into how the authors could improve their work or what they could do differently. It also does not provide any implicit actionable comments that could be inferred from the review point. The lack of actionable content in the review point means that the authors are left without any clear direction on how to proceed with their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of politeness or rudeness, but rather provides a straightforward critique of the authors' use of capital letters in their notation. The reviewer's tone is neither aggressive nor encouraging, but rather a simple observation of the inconsistency in the notation style. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. Therefore, based on the score rubric, the review point is neutral and factual, and does not fit well with the other two scores.",0,"The review point in question does not contain any claims or assertions that require verification. It merely points out a change in notation style from lowercase to capital letters in the text. This observation is a factual statement and does not require any justification or reference to support it. The review point does not make any claims about the content of the paper or its implications, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not contribute to the overall evaluation of the paper. It is simply a note on the formatting of the text. So the overall score is 0.",1,"The review point in question is quite specific, as it directly addresses the change in notation from lower case to capital letters in line 116. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed. The review point is clear and concise, making it easy for the authors to identify the relevant section of their paper. The authors can be confident that the review point is addressing a specific aspect of their work, which is the shift in notation. This level of specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not allow the authors to confidently identify the part of the paper being addressed. For example, a review point that simply states that the paper is hard to follow or that certain sections need to be made clearer would not provide the authors with any concrete information about which parts of the paper are problematic. This lack of specificity would result in the authors being unsure about which parts of the paper are being addressed by the review point. 

Therefore, based on the score rubric's criteria, the review point in question is highly specific and directly addresses a particular aspect of the paper, which is the change in notation. This specificity allows the authors to be confident about which part of the paper is being addressed by the review point."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization.,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a lack of discussion on further derivatives of the site update for marginal likelihood hyperparameter optimization. The authors are left without any clear direction on what needs to be done or how to proceed. The review point does not provide any implicit actionable comments that can be inferred, nor does it suggest any limitations that could be addressed with specific actions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating a lack of discussion on a specific topic. This is in line with the rubric's description of a score of 0, which indicates a neutral tone. The reviewer does not make any personal judgments or comments on the quality of the work, which is a key aspect of politeness in a review. The reviewer's tone is consistent throughout, maintaining a neutral stance without any form of bias or personal opinion. Therefore, based on the rubric's criteria for politeness, the review point is neutral and factual, and thus, it would be scored as 0.",-1,"The review point in question is missing a discussion on the computation of further derivatives of the site update for marginal likelihood hyperparameter optimization. This is a significant omission as it directly impacts the verifiability of the claim. The claim is that the method used for the site update is suitable for hyperparameter optimization, but without a discussion on how to compute further derivatives, the claim is not fully supported. The lack of verification in this review point is evident, as it does not provide any logical deductions or references to external or internal sources that could substantiate the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. 

In contrast, a review point that contains a claim and verifies it would provide logical deductions and references to external or internal sources that support the claim. For example, a review point that discusses the statistical analysis of a paper and compares the p-values reported with the standard thresholds for significance would be verifiable. The reviewer would provide a logical explanation and reference to a source like 'Statistics for Biologists' to support the claim that the statistical analysis appears to be incorrect. 

In conclusion, the review point in question does not meet the criteria for verifiability as it lacks a discussion on the computation of further derivatives and does not provide any logical deductions or references to support the claim. Therefore, the score is -1.",1,"The review point provided is quite specific and directly addresses a key aspect of the paper, which is the computation of further derivatives for marginal likelihood hyperparameter optimization. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed, and they can be confident that the review point is focused on this particular aspect. The review point is clear and concise, making it easy for the authors to understand and respond to. It also provides a clear direction for the authors to improve their paper, by suggesting that they should include a discussion on the computation of further derivatives. This specificity in the review point aligns well with the score rubric, which emphasizes the importance of clarity and specificity in the review point. Therefore, based on the score rubric, the review point is of high quality and effectively addresses the paper's content."
1261,ICLR_2023_4259,"Weaknesses
Linear latent dynamics have been used in object-centric models before
No quantitative evaluation with respect to object tracking
Several baselines missing
No ablations whether all the complex components are necessary
1. Novelty
I have some concerns about novelty here. Linearizing the latent dynamics is not a new idea. It's one of the main ideas behind Contrastive Predictive Coding (van den Oord 2018) and has been used in the context of object-centric video models in VideoMONet (Weis et al., JMLR 2021) for predicting the next frame. Although these works don't frame it in the language of Koopman operator theory, the main idea seems very similar. Thus, the novel idea in this work seems to me the interpretation and manipulation of the eigenvalues of the linear transition matrix. Shouldn't this also be possible with previous methods like VideoMONet?
2. Evaluation
The authors evaluate only with respect to reconstruction and all further demonstrations of manipulating the dynamics are only qualitative based on individual examples. However, object-centric models have problems tracking objects properly through occlusions (Weis et al. 2021). Thus, we don't know whether and how well the approach actually works except for individual hand-picked examples.
3. Ablations missing
The authors framework is a mix and match of components proposed in earlier works, such as object masks, occlusion modeling, separating appearance from pose, tracking objects and modeling their interactions. However, to what extent each of these components actually do what they promise and whether they are necessary for the problems studied by the authors remains unclear. Wouldn't a substantially simpler approach accomplish the same on such simple datasets? I think it would make much more sense to modify existing approaches by replacing only the component that models the latent dynamics and showing that performance can be retained while gaining interpretability or -- if this is not possible -- showing which other components are necessary to make the linear latent dyanmics possible.
4. Baselines
Several strong baselines for object-centric dynamic models like OP3 (Veerapaneni et al. 2020), VideoMONet (Weis et al. 2021) or SAVi (Kipf et al. 2022) seem to be missing.","4. Baselines Several strong baselines for object-centric dynamic models like OP3 (Veerapaneni et al. 2020), VideoMONet (Weis et al. 2021) or SAVi (Kipf et al. 2022) seem to be missing.",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that several strong baselines for object-centric dynamic models are missing, without suggesting any specific actions or improvements that could be made. The review point does not give any hint or direct actionable comments, and after reading it, the authors still do not know what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not provide any encouragement or constructive feedback. The reviewer simply states the absence of strong baselines for object-centric dynamic models, which is a factual observation. However, the reviewer could have been more polite by using softer language or providing more detailed feedback on how to improve the baselines. The reviewer's tone is more passive than neutral, which is why it does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not contain any claims or assertions that require verification. It merely states that several strong baselines for object-centric dynamic models are missing, which is a factual statement that does not need any justification or reference. The review point does not make any claims about the quality or performance of the models mentioned, nor does it provide any logical deductions or comparisons. It simply points out the absence of certain models, which is a straightforward observation. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not contain any factual statements that do not require verification. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions several strong baselines for object-centric dynamic models but fails to specify which parts of the paper are related to these baselines. The authors would likely be unsure about which parts of the paper are being addressed by the review point, as it does not provide any concrete examples or suggestions for improvement. This lack of specificity makes it difficult for the authors to identify and address the issues raised in the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","-The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. -README file for the dataset [Authors committed to add README file] - General Discussion:",0.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of detail in the explanation of methods and the absence of a README file. The review point also mentions the general discussion but does not provide any actionable comments or suggestions that could be inferred from the review point. The lack of explicit or implicit actionable comments in the review point means that the authors are left without clear guidance on what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or rude, nor is it overly encouraging or helpful. The feedback is delivered in a straightforward manner, focusing on the content of the paper rather than the tone or style of the writing. The reviewer's comments are factual and do not contain any personal judgments or opinions. The feedback is also constructive, offering suggestions for improvement without being overly critical or dismissive. The reviewer's comments are also consistent with the other review points, maintaining a neutral tone throughout. Therefore, based on the score rubric, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the explanation of methods is too detailed and that there is no mention of other work, which is a factual statement and does not require any form of verification. The review point also mentions the commitment of the authors to address the issue in the final version, which is a statement of intent rather than a claim that needs to be verified.

The review point does not make any claims that could be verified or disproved. It does not provide any logical deductions or references to external or internal sources that could support or refute any claims. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified or disproved.

In conclusion, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. It is a factual statement that does not require any form of verification. Therefore, the score is 0.",1,"The review point provided is quite specific and directly addresses the issues in the paper. It points out the excessive detail in some paragraphs and the lack of mention of other work, which is a clear indication of the areas that need improvement. The authors are also informed about the commitment to address these issues in the final version, which shows a clear understanding of the review point.

The review point also mentions the need for a README file for the dataset, which is a specific requirement that the authors can easily identify and address. Furthermore, the general discussion section is also pointed out, which is a clear indication of the part of the paper that is being addressed.

The authors are likely to be confident about which parts of the paper are being addressed by the review point, as the issues are clearly outlined and the authors are informed about the commitment to address them. The specificity of the review point makes it obvious which part of the paper is being addressed, and the authors can easily identify and address the issues.

In conclusion, the review point is specific and directly addresses the issues in the paper, making it clear for the authors to identify and address the areas that need improvement. The authors are likely to be confident about which parts of the paper are being addressed by the review point. Therefore, the score is 1."
5776,NIPS_2018_600,"weakness of the non-local (NL) module [31] that the correlations across channels are less taken into account, and then formulate the compact generalized non-local (CGNL) module to remedy the issue through summarizing the previous methods of NL and bilinear pooling [14] in a unified manner. The CGNL is evaluated on thorough experiments for action and fine-grained classification tasks, exhibiting promising performance competitive to the state-of-the-arts. Positives: + The paper is well organized and easy to follow. + The generalized formulation (8,9) to unify bilinear pooling and non-local module is theoretically sound. + Good performance. Negatives: - Less discussion on the linear version of CGNL using dot product for f. - Missing fundamental comparison to the simple ResBlock. The authors nicely present the generalized formulation toward CGNL by unifying the two previous works of bilinear pooling and non-local module. Though the kernelized (non-linear) correlation function f is well theoretically motivated, the actual form of f that achieves the better empirical performance is a âlinearâ form (dot product). In this regard, the reviewer has the following concerns. - Less discussion about the linear form. If the reviewer correctly understands the CGNL formulation, the linear function f of dot product f (line 204) can greatly simplify the CGNL into Y = X * W_theta * tr[(X*W_phi)â * (X*W_g)] = X * W_theta * tr[(XâX)* W_g* W_phiâ]  = s * X * W_theta, where s = tr[(XâX) * W_g * W_phiâ]= tr[(XâX)* W] is just a scalar and W = W_g*W_phiâ. This reformulation would be beneficial from the following viewpoints. > It reduces the parameters from {W_theta, W_phi, W_g} to {W_theta, W}, which facilitates the implementation. > It is closely related to squeeze-and-excitation (SE) module [9]. The above formulation can be regarded as a bilinear extension of SE from âsqueezeâ viewpoint since it âsqueezesâ the feature map X into the bilinear form of XâX while SE simply employs an average-pooling.  Such discussions as above would help the readers to further understand the methods and to further extend the method. - Missing comparison. Based on the above discussion, one can think that the baseline for the linear CGNL is a simple ResBlock of Z = BatchNorm( X * W_z ) + X, while the linear CGNL is Z = BatchNorm( s * X * W_theta * W_z ) + X  = BatchNorm( s * X * W_tz ) + X. The only difference is the scaling factor s that is also build on X. Through batch normalization, such a scaling might be less effective (during the training) and thus by comparing these closely-related methods, the authors have to clarify its effectiveness of CGNL empirically. Due to this concern, the reviewer can not fairly evaluate the impact of the method on classification performance. [After Rebuttal] The reviewer appreciates the authorsâ efforts to perform the comparison experiments in such a short rebuttal period. The comparison with the standard ResBlock clarifies the effectiveness of the proposed method as well as helps us to further understand how it works. ",+ The paper is well organized and easy to follow.,-1.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out some limitations of the paper without suggesting any specific actions that could be taken to address these limitations. For example, the review point mentions that the related work section is severely lacking, but it does not provide any guidance on how to improve it. Similarly, it notes that the proposed method has only a little performance improvement, but it does not suggest any particular improvements that could be made. The review point also fails to provide any implicit actionable comments that the authors could infer from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is well-structured and easy to follow, which aligns with the score rubric's requirement for a helpful and encouraging tone. The reviewer offers constructive feedback, suggesting improvements and acknowledging the strengths of the paper. The language used is polite and respectful, which is crucial in a review context. The reviewer also maintains a neutral stance, avoiding any aggressive or disrespectful remarks. The feedback is focused on the content of the paper, providing specific suggestions for improvement, which is a key aspect of a constructive review. The reviewer's tone is consistently helpful and encouraging, which is why the score is 1.",0,"The review point provided does not contain any claims that need to be verified. It is a general statement about the organization and readability of the paper, which is not a claim that requires justification or reference to external or internal sources. The review point does not make any assertions about the content of the paper, nor does it provide any logical deductions or comparisons to other works. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is simply a comment on the structure of the paper, which is not a claim that needs to be verified. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer specific guidance on how to improve the paper. It lacks the specificity required to effectively address the paper's content. The review point does not clearly identify which parts of the paper are being addressed, nor does it provide clear direction for the authors to follow. This lack of specificity makes it difficult for the authors to understand what aspects of the paper need to be improved. The review point does not offer any concrete suggestions or examples that would help the authors to identify and address the issues in their paper. As a result, the authors are left with a sense of uncertainty about which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Instead, it leaves the authors with no idea about the paper part that is being addressed by the review point comment. So the overall score is -1."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","- In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely states the difference between supervised and unsupervised approaches without suggesting any specific actions or improvements that could be made to the proposed method. The review point also does not include any implicit actionable comments that the authors can infer from the review. Instead, it only mentions some limitations of the proposed method without offering any solutions or further steps that could be taken. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not contain any encouraging or helpful language. The review point simply states the facts about the authors' work and the differences between their approach and the one they are comparing to. It does not offer any constructive criticism or suggestions for improvement, which would have been helpful and encouraging. The review point also does not contain any negative or disrespectful language, which would have been inappropriate and would have scored a -1. Therefore, based on the rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the supervised nature of the proposed approach, which is a valid point. However, it fails to provide any form of verification or justification for this claim. The review point merely states that the approach is still supervised, but it does not delve into the specifics of how this is the case or why it is significant. Furthermore, the review point does not mention any external or internal sources that could be used to support the claim. This lack of verification and justification is a clear deviation from the score rubric, which requires claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It clearly identifies the difference between supervised and unsupervised approaches, and how the proposed method still falls under the supervised category, but with a unique twist in the generation of training data. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed, and they can be confident about the review point's focus. The review point is not only specific but also directly related to the paper's content, which is crucial for the authors to understand and address the feedback. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
4170,NIPS_2020_490,"I think more details and discussion are needed to justify and evaluate some claims in the paper. 1. In Figure 5, it is said that ""two children spawned at an early time t_s in the chaotic training regime arrive at two different loss basins"", while ""two children spawned at a later time t_s in the stable training regime arrive at the same loss basin"". While I do agree with this claim, a simple explanation for it may just be that as t_s becomes larger, the distance between a child and its spawning parent becomes smaller and smaller. To see this, note that as the risk decreases, the gradient norm also becomes smaller, and the child is trained for T-t_s epochs, which also shrinks as t_s becomes larger since T seems to be fixed. Therefore we can expect the child and parent to eventually lie in the same basin as t_s becomes larger, and I wonder what is the new conclusion we can draw from this experiment. 2. In Figure 6, at the last points of the curves, is \tilde{t} equal to T? If so the green curves should meet the red curves since there is no additional training, while if not I wonder the details. Regarding the claim that the learned NTK can outperform full deep learning, it seems that the kernel can indeed drives the training error smaller, but the improvement in test error is very small and may be negative as in Figure 12. I also wonder if the curves in Figures 6, 11 and 12 represent the average of multiple parent and child runs?","2. In Figure 6, at the last points of the curves, is \tilde{t} equal to T? If so the green curves should meet the red curves since there is no additional training, while if not I wonder the details. Regarding the claim that the learned NTK can outperform full deep learning, it seems that the kernel can indeed drives the training error smaller, but the improvement in test error is very small and may be negative as in Figure 12. I also wonder if the curves in Figures 6, 11 and 12 represent the average of multiple parent and child runs?",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the observations and questions without suggesting any specific actions or experiments that could be taken to address the issues raised. The review point also does not include any implicit actionable comments that could be inferred from the review point. It simply states the limitations of the paper without offering any solutions or further investigation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point is delivered in a neutral tone, which is in line with the score rubric's criteria for a score of 0. The reviewer's language is factual and does not convey any particular emotion or stance. The reviewer's questions and observations are presented in a straightforward manner, without any aggressive or disrespectful language. The reviewer's tone is neither encouraging nor discouraging, but rather neutral and focused on the content of the paper. The reviewer's comments are constructive, aiming to understand the details of the paper and its implications. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance throughout. Therefore, based on the score rubric, the review point is neutral and factual, fitting well with the other two scores.",-1,"The review point in question contains several claims that are not verified. The first claim is about the equality of the variables \tilde{t} and T, and the second claim is about the performance of the learned NTK compared to full deep learning. The third claim is about the representation of the curves in Figures 6, 11, and 12. However, the review point does not provide any logical deductions, explanations, or references to support these claims. The lack of verification in the review point means that the claims are not substantiated, and the reader is left without a clear understanding of the validity of the claims. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability, and it is not supported by logical deductions or references. So the overall score is -1.",0,"The review point provided is quite specific and directly addresses the last points of the curves in the paper, which is the main focus of the reviewer. The reviewer is confident that the part of the paper being addressed is the comparison between the learned NTK and full deep learning. The reviewer also questions the representation of the curves in the paper, which shows a clear understanding of the paper's content.

However, the reviewer's question about whether the curves represent the average of multiple parent and child runs is not directly related to the main focus of the review point. This question, while relevant to the paper, is not as specific as the other points raised in the review.

The reviewer's comments on the improvement in test error being very small and potentially negative are also specific and directly address the paper's claim about the learned NTK outperforming full deep learning. This shows that the reviewer has a good grasp of the paper's content and is able to provide a detailed analysis of the paper's claims.

In conclusion, the review point is specific and directly addresses the main focus of the paper, which is the comparison between the learned NTK and full deep learning. The reviewer's questions and comments are relevant and demonstrate a clear understanding of the paper's content. However, the question about the representation of the curves is not as specific as the other points raised in the review. So the overall score is 0."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature).,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out a potential correlation between the percentile rank and the perceived relative lightness without suggesting any specific actions to verify or explore this correlation further. The review point also does not provide any implicit actionable comments that the authors could infer from the review point. It simply states a limitation without offering any guidance on how to address it or any suggestions for future research. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply states the lack of correlation between the percentile rank and the perceived relative lightness, without making any personal judgments or comments on the quality of the paper. It also does not provide any constructive feedback or suggestions for improvement. The review point is purely factual and does not engage with the content of the paper in a way that would be considered polite or encouraging. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question makes a claim about the correlation between the percentile rank and the perceived relative lightness. However, it does not provide any verification or justification for this claim. The review point merely states that the authors do not show if the perceived change in lightness is at the same scale of perception, without offering any logical deductions or references to external or internal sources that could support or refute this claim. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified. 

The review point also fails to provide any factual statements or claims that do not need verifications, which would have been necessary for a score of 0. Therefore, based on the score rubric, the review point does not meet the criteria for any of the scores. 

In conclusion, the review point does not provide any logical deductions, external or internal sources, or factual statements that could support or refute the claim made. This lack of verification and justification is why the review point does not meet the criteria for a score of 1.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It questions the correlation between the percentile rank and the perceived relative lightness, and whether this is at the same scale of perception. This specificity allows the authors to identify the exact part of the paper that is being addressed, which is the correlation between the percentile rank and the perceived relative lightness. The authors can be confident that the review point is focused on this particular aspect of the paper. Therefore, the review point meets the criteria for a score of 1, as it clearly identifies the part of the paper that is being addressed. 

In contrast, a review point that lacks specificity would not allow the authors to confidently identify the part of the paper being addressed. For example, a review point that simply states that the paper is hard to follow or that certain sections need to be made clearer would not provide the authors with a clear direction on what specific parts of the paper need to be improved. This lack of specificity would result in the authors being unsure about which parts of the paper are being addressed by the review point. 

Therefore, the review point provided is well-structured and directly addresses a specific aspect of the paper, which is why it meets the criteria for a score of 1."
3556,NIPS_2020_936,"Weaknesses: - the paper could gain of having a thorough discussion on the generalization properties of the proposed learning procedure (and if no gain, that should be stated as well); - is is not clear how (not) stringent is Assumption 3 regarding the hypothesis classes, whereas it is key for the results provided; in addition the finite class claim related to floating point arithmetic could be developed a bit more; - it is not clear to me what the main strength of MixBoost is: -- is it related to the computational savings? -- is it related to the generalization performances of the method? -- is it related to the use of Random Fourier Features? - regarding Random Fourier Feature: wouldn't an even accelerated method such as Fastfood (Le et al. 2013) preferable to the sampling proposed here?","- the paper could gain of having a thorough discussion on the generalization properties of the proposed learning procedure (and if no gain, that should be stated as well); - is is not clear how (not) stringent is Assumption 3 regarding the hypothesis classes, whereas it is key for the results provided; in addition the finite class claim related to floating point arithmetic could be developed a bit more; - it is not clear to me what the main strength of MixBoost is: -- is it related to the computational savings? -- is it related to the generalization performances of the method? -- is it related to the use of Random Fourier Features?",1.0,1.0,-1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out areas where the paper could be improved, such as the generalization properties of the proposed learning procedure, the stringency of Assumption 3, and the development of the finite class claim related to floating point arithmetic. However, it does not provide any specific guidance on how to address these issues. The review point also questions the main strength of MixBoost, but it does not offer any concrete suggestions on how to clarify this point. Therefore, based on the score rubric, the review point does not give any hint, or direct actionable comments, and the authors still do not know or can not deduce what needs to be done. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any aggression or rudeness, nor do they provide any encouragement or helpful suggestions. Instead, they focus on the aspects of the paper that could be improved or clarified. The reviewer's tone is neutral, and they do not make any personal judgments about the paper or its authors. They simply state the points that need to be addressed, which is a passive approach to reviewing. The reviewer's comments are factual and do not contain any subjective opinions, which is why the score is 0. 

The reviewer's comments are also constructive, as they provide specific areas where the paper could be improved. However, the reviewer does not offer any positive feedback or encouragement, which would have been helpful and would have resulted in a higher score. The reviewer's tone is consistent throughout the review point, and they maintain a neutral stance throughout. 

In conclusion, the review point is neutral and factual, and it does not contain any aggressive or disrespectful language. The reviewer's tone is consistent, and they provide specific feedback without making any personal judgments. This is why the score is 0.",-1,"The review point provided contains several claims that are not verified. The first claim is about the generalization properties of the proposed learning procedure. The reviewer suggests that a thorough discussion on this topic is needed, but does not provide any logical deductions or references to support this claim. The second claim is about the stringency of Assumption 3 regarding the hypothesis classes. The reviewer questions the stringency of the assumption but does not offer any justification or explanation to support this claim. The third claim is about the finite class claim related to floating point arithmetic. The reviewer suggests that this claim could be developed more, but again, does not provide any logical deductions or references to support this claim. The fourth claim is about the main strength of MixBoost. The reviewer asks several questions about the strength of the method but does not provide any logical deductions or references to support these questions.

In summary, the review point contains several claims that are not verified, and the reviewer does not provide any logical deductions or references to support these claims. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The reviewer's claims are not supported by logical explanations or references, which is essential for verifying claims in a review. Therefore, the review point is not aligned with the score rubric's criteria for verifiability.",1,"The review point provided is quite specific and directly addresses several key aspects of the paper. It clearly identifies the need for a thorough discussion on the generalization properties of the proposed learning procedure, which is a central theme of the paper. The review point also raises concerns about the stringency of Assumption 3 and the finite class claim related to floating point arithmetic, both of which are critical to the paper's results. Furthermore, the review point questions the main strength of MixBoost, which is a crucial aspect of the paper's contribution.

The authors of the paper would be able to identify which parts of the paper are being addressed by the review point. They would be confident about the specific aspects of the paper that need to be addressed, such as the generalization properties, the stringency of Assumption 3, the finite class claim, and the main strength of MixBoost. This level of specificity in the review point ensures that the authors are fully aware of the areas that require further clarification or development.

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of which parts of the paper are being addressed. The authors would be unsure about the specific aspects of the paper that need to be improved or clarified. This lack of specificity would make it difficult for the authors to identify and address the issues raised in the review point.

Therefore, based on the score rubric, the review point is highly specific and directly addresses the key aspects of the paper, making it clear to the authors which parts of the paper are being addressed. This level of specificity is in line with the score rubric's criteria for a score of 1."
1707,ICLR_2023_1914,"Weaknesses (1) The framing of synergies and their neuroscientific context is somewhat lacking. The premise of the paper is that muscle synergies can be predicted from the cortical inputs, e.g. We applied our method to the corticomuscular system, which is made up of corticospinal pathways between the primary motor cortex and muscles in the body and creates muscle synergies that enable efficient connections between the brain and muscles.”, however synergies are thought to be generated in the spinal cord and some of their first characterization was in frogs, a species without a motor cortex. One reason this is problematic is that the paper is framed as revealing new synergies, e.g. “However, the conventional approach uses only muscle activities (observed phenomena) to capture the muscle synergies, and there may still be unexplored muscle synergies that remain hidden” However, based on the model design it seems like it should be detecting a subset of the muscle-only synergies. Moreover, synergies is largely defined in a muscle-centric way. It is certainly the case that discovering cortico-muscular shared synergies is interesting, but the framing is very different. (2) There are no details provided on the TCN training, and importantly how data was split up for train and test splits. This is especially important for the SCI experiments. (3) There are multiple alternative models that could be considered, for instance performing NNMF on a linear or nonlinear prediction of EMG from ecog activity. The specific motivation for the increased number of parameters and model structure of the TCN is not provided. One of the appeals of NNMF is its simplicity, allowing it to be used across paradigms and contexts, and this TCN introduces a lot of added complexity, with only minimal gains in VAF at high numbers of syneries. (4) For the SCI experiments – there is no ground truth present and so it is impossible to evaluate which technique is ‘correct’. As noted above, without knowledge of how much data is required for model training, it is hard to know if the increase in number of synergies observed is a result of
Nits: - ‘connectivity’ is misleading, as it isn’t using the structural connections between the brain and body. - Figure 6: would help to have an estimate of variance for the number of synergies, e.g. from using different subsets of data to train/test.","- ‘connectivity’ is misleading, as it isn’t using the structural connections between the brain and body.",-1.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a misconception about the term 'connectivity' without suggesting any specific actions or improvements that could be made. The review point does not provide any implicit actionable comments that could be inferred by the authors. It simply states that the term 'connectivity' is misleading without offering any guidance on how to address this issue. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor does it provide implicit actionable comments. So the overall score is -1.",0,"The review point provided is not aligned with the score rubric's criteria for politeness. The tone of the review point is not only neutral but also slightly critical, which is not in line with the rubric's requirement for a helpful or encouraging tone. The review point does not offer any constructive feedback or suggestions for improvement, which is a key aspect of a polite review. Instead, it simply states that the term 'connectivity' is misleading without providing any context or explanation as to why this is the case. This lack of detail and explanation does not contribute to a helpful or encouraging review. Therefore, the review point does not meet the criteria for a score of 1, as it does not provide a helpful or encouraging tone. So the overall score is 0.",-1,"The review point in question makes a claim that the term 'connectivity' is misleading, as it does not refer to the structural connections between the brain and body. However, this claim is not supported by any logical deductions or references to external or internal sources. The reviewer does not provide any justification or explanation as to why the term 'connectivity' is misleading, nor does it offer any evidence or reasoning to back up the claim. This lack of verification and justification is in direct contrast to the score rubric's requirement for claims to be verified through logical explanations or references. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The term 'connectivity' is used without any context or explanation, which makes it difficult for the authors to understand what specific aspect of the paper is being critiqued. The review point does not provide any specific details or examples that would help the authors to identify the relevant sections of the paper. This lack of specificity in the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Instead, it leaves the authors uncertain and unsure about the paper part that is being addressed by the review point comment. Therefore, based on the score rubric, the review point is not effective in addressing the specificity of the paper. So the overall score is -1."
2606,NIPS_2019_1408,"Weaknesses: - The paper is not that original given the amount of work in learning multimodal generative models:   â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior.   â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths: - The experimental results are nice. The paper claims that their MMVAE modal fulfills all four criteria including (1) latent variables that decompose into shared and private subspaces, (2) be able to generate data across all modalities, (3) be able to generate data across individual modalities, and (4) improve discriminative performance in each modality by leveraging related data from other modalities. Let's look at each of these 4 in detail:   â (1) Yes, their model does indeed learn factorized variables which can be shown by good conditional generation on MNIST+SVHN dataset.   â (2) Yes, joint generation (which I assume to mean generation from a single modality) is performed on vision -> vision and language -> language for CUB,   â (3) Yes, conditional generation can be performed on CUB via language -> vision and vice versa.  Weaknesses: - (continuing on whether the model does indeed achieve the 4 properties that the authors describe)   â (3 continued) However, it is unclear how significant the performance is for both 2) and 3) since the authors report no comparisons with existing generative models, even simple ones such as a conditional VAE from language to vision. In other words, what if I forgo with the complicated MoE VAE, and all the components of the proposed model, and simply use a conditional VAE from language to vision. There are many ablation studies that are missing from the paper especially since the model is so complicated.   â (4) The authors have not seemed to perform extensive experiments for this criteria since they only report the performance of a simple linear classifier on top of the latent variables. There has been much work in learning discriminative models for multimodal data involving aligning or fusing language and vision spaces. Just to name a few involving language and vision:     - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016     - DeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013 Therefore, it is important to justify why I should use this MMVAE model when there is a lot of existing work on fusing multimodal data for prediction. ****************************Clarity**************************** Strengths: - The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information. - The table in Figure 2 nicely summarizes some of the existing works in multimodal learning and whether they fulfill the 4 criteria that the authors have pointed out to be important. Weaknesses: - Given the authors' great job in setting up the paper via Figure 1, Figure 2, and the introduction, I was rather disappointed that section 2 did not continue on this clear flow. To begin, a model diagram/schematic at the beginning of section 2 would have helped a lot. Ideally, such a model diagram could closely resemble Figure 2 where you have already set up a nice 'Venn Diagram' of multimodal information. Given this, your model basically assigns latent variables to each of the information overlapping spaces as well as arrows (neural network layers) as the inference and generation path from the variables to observed data. Showing such a detailed model diagram in an 'expanded' or 'more detailed' version of Figure 2 would be extremely helpful in understanding the notation (which there are a lot), how MMVAE accomplishes all 4 properties, as well as the inference and generation paths in MMVAE. - Unfortunately, the table in Figure 2 it is not super complete given the amount of work that has been done in latent factorization (e.g. Learning Factorized Multimodal Representations, ICLR 2019) and purely discriminative multimodal fusion (i.e. point d on synergy) - There are a few typos and stylistic issues: 1. line 18: ""Given the lack explicit labels availableâ -> âGiven the lack of explicit labels availableâ 2. line 19: âcan provided importantâ -> âcan provide importantâ 3. line 25: âbetween (Yildirim, 2014) themâ -> âbetween them (Yildirim, 2014)â 4. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite interesting. - The paper did a commendable job in attempting to perform experiments to justify the 4 properties they outlined in the introduction. - I can see future practitioners using the variational MoE layers for encoding multimodal data, especially when there is missing multimodal data. Weaknesses: - That being said, there are some important concerns especially regarding the utility of the model as compared to existing work. In particular, there are some statements in the model description where it would be nice to have some experimental results in order to convince the reader that this model compares favorably with existing work: 1. line 113: You set \alpha_m uniformly to be 1/M which implies that the contributions from all modalities are the same. However, works in multimodal fusion have shown that dynamically weighting the modalities is quite important because 1) modalities might contain noise or uncertain information, 2) different modalities contribute differently to the prediction (e.g. in a video when a speaker is not saying anything then their visual behaviors are more indicative than their speech or language behaviors). Recent works therefore study, for example, gated attentions (e.g. Gated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 or Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning, ICMI 2017) to learn these weights. How does your model compare to this line of related work, and can your model be modified to take advantage of these fusion methods? 2. line 145-146: ""We prefer the IWAE objective over the standard ELBO objective not just for the fact that it estimates a tighter bound, but also for the properties of the posterior when computing the multi-sample estimate."" -> Do you have experimental results that back this up? How significant is the difference? 3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.","3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a potential issue with the computational overhead of the proposed method, without suggesting any specific actions to address this concern. The review point also lacks any implicit actionable comments that the authors could infer from the text. It simply states the problem without providing any guidance on how to improve the situation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and as such, it would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer's tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer's concerns about the experimental runtimes and the comparison with previous works are stated in a straightforward manner, without any personal attacks or overly positive comments. The reviewer's feedback on the quality of generation and the need for improvement in writing is also presented in a neutral tone. The reviewer's comments are focused on the content of the paper and do not include any personal opinions or biases. Therefore, the review point is consistent with the score rubric's criteria for a score of 0.",-1,"The review point in question makes a claim about the computational overhead of the proposed method. However, it fails to provide any form of verification or justification for this claim. The reviewer does not offer any logical deductions or references to external or internal sources that could substantiate the claim. Instead, the reviewer simply states that the method could be costly without providing any evidence or reasoning to support this assertion.

The review point also lacks any mention of experimental runtimes, which is a critical aspect of verifying the claim about computational overhead. Without this information, the claim remains unverified and unsubstantiated.

Furthermore, the reviewer does not address the potential impact of the number of modalities on the computational cost, which is a significant factor in the context of the claim. The reviewer's failure to consider this aspect further undermines the credibility of the claim.

In conclusion, the review point contains a claim that is not verified, and it misses a justification, explanation, or reference. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Therefore, the score is -1.",1,"The review point in question is quite specific, addressing a particular aspect of the paper related to the computational overhead of the proposed method. It directly questions the efficiency of the method, particularly in relation to the number of modalities and the size of the decoders. This specificity allows the authors to pinpoint the exact part of the paper that is being scrutinized, which is the line 157-158. The review point is clear and concise, making it easy for the authors to understand and respond to. It also provides a clear direction for the authors to address in their rebuttal, which is to provide experimental runtimes to demonstrate the efficiency of their method. This specificity aligns well with the score rubric, as it allows the authors to be confident about which part of the paper is being addressed by the review point. Therefore, based on the score rubric, the review point is well-crafted and effectively communicates the concerns raised."
4026,NIPS_2020_159,"UPDATE: Thank you for your response, which addressed most of my concerns. The only issue is that only controlling the number of leaves can still be problematic since the depth of the also matters [1]. [1] Reyzin L, Schapire RE. How boosting the margin can also boost classifier complexity, ICML 2006. ============================= I have several concerns and questions: 1. Line 97: what does “same size” mean? We know that in order to make a fair comparison, we must make sure that the model complexity of base learners is the same. For decision trees, does it mean the same number of leaves? To me, the best way could be just using a decision stump as a base learner. 2. The empirical results are also not convincing to me: 1) the results are only averaged over three runs, which is insufficient to me. 2) I would also like to see the standard deviations of the average accuracies. 3) The experiments are only evaluated on one data set. To make the conclusion more convicting, the author should make a comparison on more data sets. 4) I was also wondering if the conclusion still holds with other base learners. 3. We know that AdaBoost is just a special case of gradient boosting with the exponential loss. Therefore, my feeling is that the analysis is just about the gradient boosting with different loss functions, not the behavior of gradient boosting itself.","3) The experiments are only evaluated on one data set. To make the conclusion more convicting, the author should make a comparison on more data sets.",1.0,-1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,0,"The review point suggests that the author should make a comparison on more data sets to make the conclusion more convicting. However, it does not provide any direct or explicit actionable steps or experiments that the authors should make. It also lacks any implicit actionable comments that the authors can infer after reading the review point. The review point only mentions the limitation of evaluating the experiments on one data set without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, which is in line with the criteria for a score of 0. The feedback is focused on the content of the paper and does not include any personal attacks or disrespectful language. The reviewer also does not provide any encouragement or positive reinforcement, which would have been indicative of a higher score. The feedback is purely factual and does not attempt to be helpful or constructive. Therefore, based on the score rubric, the review point is neutral and factual, and the tone is neither aggressive nor overly polite.",-1,"The review point in question makes a claim about the need for more data sets to support the conclusions of the experiments. However, it does not provide any logical deductions or references to external or internal sources to verify this claim. The review point simply states that the conclusion would be more convincing with additional data sets, but it does not offer any justification or explanation as to why this is the case. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The review point does not contain any claims that are not verified, nor does it include factual statements that do not need verifications. Instead, it makes a claim that is not supported by any logical deductions or references. Therefore, based on the score rubric, the review point is not convincing and does not meet the criteria for a score of 1.",1,"The review point provided is quite specific, addressing the limitation of the paper that it only evaluates the experiments on one data set. This review point is clear and directly points out the need for a comparison on more data sets to strengthen the conclusion of the paper. The authors can easily identify which part of the paper is being addressed by the review point, which is the evaluation of the experiments. They can also be confident about the specific aspect of the paper that needs improvement, which is the scope of the experiments. The review point is well-aligned with the score rubric, as it makes it obvious which part of the paper is being addressed and the authors can be confident about it. Therefore, the score is 1."
4568,NIPS_2020_812,"Significance and novelty of the contribution. Some of the assumptions that the authors then chose to make made the paper a less interestsing than I had hoped. making the investigation very deep, but narrow: * ...that mapping means creating some form of quantized top-down view of the environment. I don't see why a human readable map should be in any way optimal for a learnt system. Other work indeed shows you can decode a such a map from a hidden representation, but the choice of representation / architecture is very important for agent performance. * using MapNet as the one example of fully learn system. I appreciate that this choice might be motivated by the fact that the agent uses a mapping represenation compatible with the others leveraged in the paper. At the same time I think it is fair to say thatt the conclusions the authors derive from the experiments are not warranted. * the unusual reward structure using r_closer - what happens without this signal, in a setting that is less artificial? Overall I think the overall impact of the paper is limited by the artifically rescricted scope of the experiments. In particular, the conclusions about the learnt system are not convincing, because of the choice of agent design and representation, and lack of comparisons with baselines in the literature - that is, I think the results we see are like this because of the choice of agent, rather that something fundamental you have found. I would love if the authors could convince me otherwise in the rebutttal.",* using MapNet as the one example of fully learn system. I appreciate that this choice might be motivated by the fact that the agent uses a mapping represenation compatible with the others leveraged in the paper. At the same time I think it is fair to say thatt the conclusions the authors derive from the experiments are not warranted.,-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments. It merely states that the conclusions drawn from the experiments are not warranted, without suggesting any specific steps or experiments that could be taken to address this issue. The review point also does not include any implicit actionable comments that could be inferred from the review point. It simply points out the limitations of the paper without offering any solutions or improvements that could be made. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither provides explicit actionable comments nor does it suggest any implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer simply states their opinion on the use of MapNet and the conclusions drawn from the experiments. The feedback is focused on the choice of MapNet and the validity of the conclusions, without any personal attacks or disrespectful remarks. The reviewer's tone is neutral and does not attempt to be overly polite or to provide constructive feedback. Therefore, the review point is in line with the score of 0, as it does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states a personal opinion about the choice of MapNet as an example of a fully learned system and questions the validity of the conclusions drawn from the experiments. There is no attempt to verify any claims or to provide logical explanations or references to support the reviewer's viewpoint. The review point is more of a commentary rather than a critical analysis that requires verification. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. 

The review point does not make any claims that need to be verified, nor does it provide any logical deductions or references to support its statements. It is a subjective opinion that does not require any form of verification. The reviewer's concerns about the choice of MapNet and the conclusions drawn from the experiments are not substantiated with any evidence or reasoning. 

In conclusion, the review point does not contain any claims that need to be verified, nor does it provide any logical deductions or references to support its statements. It is a subjective opinion that does not require any form of verification. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. 

So the score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer mentions the use of MapNet as an example of a fully learn system, but does not specify which aspects of the paper are being critiqued. The reviewer also expresses doubt about the conclusions drawn from the experiments, but again, does not specify which conclusions are in question. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The reviewer's comments are too general and do not provide clear guidance on how to improve the paper. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point with specificity would clearly identify which part of the paper is being addressed, such as a specific section or a particular claim made in the paper. The reviewer would provide clear and specific feedback on what aspects of the paper need improvement, and how the authors can address these issues. This would allow the authors to confidently identify which parts of the paper are being addressed by the review point. 

Based on the score rubric, the review point provided does not meet the criteria for specificity. It lacks the necessary detail to guide the authors on how to improve their paper. Therefore, the score is -1."
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art.,-1.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided is well-structured and offers a clear understanding of the paper's main idea and proposed approach. It effectively positions the paper in relation to existing work and demonstrates the effectiveness of the proposed approach. However, the review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It also lacks any implicit actionable comments that the authors can infer from the review point. Instead, it only mentions some limitations without suggesting any specific actions that should be taken. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it offer implicit actionable comments. Hence, the score is -1.",1,"The review point provided is well-structured and maintains a professional tone throughout. It offers constructive feedback, highlighting the strengths of the paper and suggesting areas for improvement. The reviewer's comments are respectful and encouraging, which aligns with the score rubric's criteria for a score of 1. The reviewer's suggestions are framed in a way that is helpful and supportive, which is indicative of a polite and constructive tone. The reviewer's language is polite and the feedback is delivered in a manner that is likely to be well-received by the authors. The reviewer's comments are focused on the content of the paper and provide specific examples of how the paper could be improved. This demonstrates a clear understanding of the paper's content and a thoughtful approach to providing feedback. The reviewer's tone is consistently polite and constructive, which is why the review point is scored as a 1.",0,"The review point provided does not contain any claims that need to be verified. It is a general statement about the quality of the paper, focusing on the organization and clarity of the main idea. The review point does not make any specific claims about the paper's content or methodology, nor does it provide any logical deductions or references to external or internal sources that would be necessary for verification. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability and would receive a score of 0.",1,"The review point provided is well-structured and clearly identifies the main idea and proposed approach of the paper. It also compares the effectiveness of the proposed approach with the state-of-the-art, which is a crucial aspect of the review process. The review point is specific in addressing the paper's content, making it clear which parts of the paper are being discussed. The authors can confidently identify the sections of the paper that are being addressed by the review point. The review point is also well-organized, which contributes to its clarity and effectiveness. Therefore, based on the score rubric, the review point is of high quality and meets the criteria for a score of 1."
493,ICLR_2022_1823,"Weaknesses
The main weakness is that the relationship between ADU and constituency tree is not clearly described. Is it true that ADU are often phrases that occur in the constituency tree? How often does this happen? Does the new BERT-based model adhere to constituency constraints? Is the BENEPAR parser appropriate for this data? Based on Trautmann et al.’s comments on grammaticality and clauses, my intuition is that an ADU is almost always a phrase in the tree, in which case it is somewhat less surprising that it helps this task (and maybe it should be helping even more). If this is the case, it’s worth considering related work in distant supervision to include.
The treatment of constituency trees is haphazard. 1) At times it is not clear if dependency or constituency trees are being used; 2) Table 1 should be made more clear, what do the percentage values indicate, and why not use the full tree at all; 3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree.
(low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.","3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree. (low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out some limitations of the work without suggesting any specific actions that could be taken to address these limitations. The review point also does not include any implicit actionable comments that the authors can infer from the review. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or helpful. The reviewer simply states the facts about the work and provides a critique that is not overly harsh or positive. The reviewer also acknowledges the challenges mentioned in the work but does not delve into them in a way that would be considered rude or disrespectful. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, which is why it would receive a score of 0.",0,"The review point in question does not contain any claims that need to be verified. It merely discusses the potential challenges of batching heterogeneous graphs, which is a factual statement that does not require any form of verification. The reviewer does not make any claims about the work or its implications, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor verifies one, and thus, it does not warrant a score of 1, 0, or -1. 

The reviewer's concern about the difficulty of batching heterogeneous graphs is a valid point, but it is not a claim that needs to be verified. The reviewer's skepticism about the claim that batching heterogeneous graphs is a major difficulty is also a valid point, but it is not a claim that needs to be verified. The reviewer's suggestion to discuss tradeoffs in architecture selection is a valid point, but it is not a claim that needs to be verified. 

In conclusion, the review point does not contain any claims that need to be verified, and it does not meet the criteria for verifiability. Therefore, it does not warrant a score of 1, 0, or -1.",-1,"The review point provided is somewhat vague and does not clearly identify which part of the paper is being addressed. The reviewer mentions the use of GNN in representing constituency trees, but does not specify which sections of the paper are related to this aspect. The reviewer also expresses doubt about the significance of batching heterogeneous graphs, but fails to provide a clear explanation of why this is a major difficulty or how it impacts the overall work. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The reviewer's comments are more general and do not provide a clear direction for the authors to improve their work. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that meets the criteria for specificity would clearly identify which part of the paper is being addressed and provide a detailed explanation of the issues or concerns. The reviewer would also offer specific suggestions for improvement, which would help the authors to understand exactly what needs to be addressed in their work. 

Based on the score rubric, the review point provided does not meet the criteria for specificity, and therefore, the authors would not be confident about which part of the paper is being addressed by the review point. 

So the overall score is -1."
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.","- the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets.",-1.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the effectiveness of the proposed method without suggesting any specific actions or improvements that could be made. The review point also does not include any implicit actionable comments that could be inferred from the text. It simply mentions the effectiveness of the method and its limitations without providing any guidance on how to address these limitations or improve the method. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the paper and providing a critique based on the information presented. The feedback is constructive, offering suggestions for improvement without being overly critical or dismissive. The reviewer also refrains from using any language that could be perceived as disrespectful or rude. Therefore, the review point is in line with the criteria for a score of 0, as it is neutral and factual, and does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the proposed method is effective for a significant majority of the studied datasets, which is a factual statement that does not require any justification or reference. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the score is 0.",1,"The review point provided is quite specific and directly addresses the effectiveness of the proposed method in terms of accuracy and interpretability. It clearly identifies the key aspects of the paper that are being evaluated, such as the method's performance on different datasets and the clarity of the method's explanation. This specificity allows the authors to understand exactly which parts of the paper are being addressed by the review point. The authors can be confident that the review point is focused on the method's effectiveness and the clarity of its explanation. Therefore, based on the score rubric, the review point demonstrates a high level of specificity, which is why it would receive a score of 1."
1212,ICLR_2023_4249,"Weaknesses: • The novelty of this paper might be slightly insufficient. My understanding is that the proposed model can be considered as an extension of previous GraphCL. Despite the introduction of cross-view reconstruction, it seems that cross-view reconstruction exists in previous graph representation learning studies. • The performance might be still somewhat limited. The authors compared many methods under different settings with GraphCV on multiple datasets. However, it can be found that GraphCV is comparable to or even underperforms the best baseline in many cases. While GraphCV has better performance than baselines in some cases, the advantages are not significant enough. Other questions: • Why are two data augmentation graphs needed? • The proposed model is involved in multiple losses. How to determine the weight values of different losses? Could the fixed weight values be generalized to different datasets?","• The performance might be still somewhat limited. The authors compared many methods under different settings with GraphCV on multiple datasets. However, it can be found that GraphCV is comparable to or even underperforms the best baseline in many cases. While GraphCV has better performance than baselines in some cases, the advantages are not significant enough. Other questions:",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely states the limitations of the GraphCV method without suggesting any improvements or further research. The review point does not offer any implicit actionable comments that the authors can infer, nor does it provide any guidance on how to address the issues raised. The lack of actionable comments in the review point means that the authors are left without any clear direction on what needs to be done to improve the performance of the GraphCV method. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper and its comparison to other methods. The feedback is factual and does not contain any negative or positive connotations, which is in line with the score of 0. The reviewer does not make any personal judgments about the quality of the paper or the authors' work, but rather provides a critique based on the information presented in the paper. The feedback is also constructive, as it suggests areas where the paper could be improved, such as the technical novelty and the inclusion of certain sections. This approach is helpful for the authors to understand the strengths and weaknesses of their work. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that GraphCV has better performance than baselines in some cases, but the advantages are not significant enough. This is a factual statement that does not require any form of verification. The review point does not make any claims that could be verified or refuted, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or refuted. So the overall score is 0.",-1,"The review point provided is somewhat vague and does not clearly identify which parts of the paper are being addressed. The authors are left with a sense of uncertainty about the specific aspects of the paper that are being critiqued. The review point does not provide clear examples or specific points that would allow the authors to confidently identify the areas of concern. Instead, it offers a generalized critique that lacks the specificity required to effectively address the paper's content. This lack of specificity makes it difficult for the authors to understand and respond to the review point. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide the necessary specificity for the authors to identify the paper parts being addressed."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- I'm missing a discussion on the marginal likelihood and its accuracy.,0.0,0.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of discussion on the marginal likelihood and its accuracy, without suggesting any specific actions or improvements that could be made. The review point does not give any hint or direct actionable comments that the authors should make, and after reading the review point, the authors still do not know or can not deduce what needs to be done. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating the absence of a discussion on the marginal likelihood and its accuracy. This is a factual observation and does not carry any emotional or motivational undertones. The reviewer does not make any personal judgments or criticisms, which further contributes to the neutrality of the tone. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question is missing a discussion on the marginal likelihood and its accuracy. This is a significant omission as the marginal likelihood is a crucial aspect of Bayesian inference and its accuracy is essential for the validity of the conclusions drawn from the data. The review point fails to make any claims or provide any justification, explanation, or reference to support the claim that the marginal likelihood and its accuracy are missing from the review. This lack of verification and justification is a clear deviation from the score rubric, which requires claims to be verified through logical deductions or references to internal or external sources. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims or verify them. Instead, it falls short of the criteria for a score of 0, as it does not include factual statements or claims that do not need verifications. Consequently, the review point is not relevant for verifiability in review points. So the overall score is -1.",1,"The review point in question is about the marginal likelihood and its accuracy. This review point is specific and directly addresses a particular aspect of the paper. It is clear that the authors can identify which parts of the paper are related to the review point, as it specifically mentions the marginal likelihood and its accuracy. The authors would be confident about which part of the paper is being addressed by the review point, as it is a focused and precise critique. The review point does not leave any ambiguity about the paper part that is being addressed, which is in line with the score rubric's criteria for a score of 1. Therefore, the review point is well-articulated and directly addresses the paper's content, making it easy for the authors to understand and respond to."
2578,NIPS_2019_1348,"Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent. ",- doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent.,1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a sentence that seems independent and does not provide any guidance on what needs to be done. The review point does not suggest any changes or improvements that could be made to the paper. It also does not infer any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it infer implicit actionable comments. Hence, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts about the paper, such as the lack of empirical evaluation and the absence of a clear context for the research question. The reviewer does not offer any constructive criticism or suggestions for improvement, which is why the tone is considered neutral. The reviewer's comments are focused on the content of the paper rather than the tone or manner in which it is presented. Therefore, the review point does not fit well with the other two scores, as it does not exhibit the helpful or encouraging tone of a score of 1, nor does it display the rudeness or aggression of a score of -1. 

So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It merely points out a potential issue with the phrasing of a sentence in the text. The reviewer suggests removing the word ""hence"" from the last line of the text, as it seems independent from the preceding sentence. This is a matter of style and clarity, rather than a substantive claim that requires verification. The reviewer does not provide any logical deductions, external or internal sources, or justifications to support their claim. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The phrase ""doomed -> deemed"" is not specific enough to guide the authors on what needs to be improved or clarified. Additionally, the instruction to remove ""hence"" is not justified as it does not contribute to the clarity or specificity of the review point. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are related to the review point. This could lead to confusion and uncertainty about the paper's content and the reviewer's expectations. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

The review point does not provide any concrete examples or suggestions for improvement, which would have been helpful in guiding the authors. Instead, it leaves the authors with a general sense of what might be wrong, without any clear direction. This lack of specificity is a significant drawback in the review point, as it does not allow the authors to confidently address the issues raised. 

In conclusion, the review point is not specific enough to guide the authors on what needs to be improved or clarified. It lacks the necessary detail and clarity to effectively address the paper's content. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

So the overall score is -1."
44,ICLR_2022_1016,"Weaknesses: 1. In this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time. 2. For previous SVD-based and NS-based methods, computation processes for forward and backward propagations are consistent. However, this work adopts Matrix Taylor Polynomial or Matrix Pade Approximation for forward propagations and uses approximate Lyapunov equation for backward propagation, leading variance in forward and backward propagations. The authors would better make some discussions about this issue. 3. The authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it.
[r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021.
Other comments: 1. It is clear that MPA involves matrix inverse, which is very GPU-unfriendly. As stated in the paper: ""Moreover, we note that the matrix inverse can be avoided, as Eq. (13) can be more efficiently and numerically stably computed by solving the linear system"". The authors would better provide more detailed computation and analysis. 2. How to compute the coefficients of p m and q n
for the Matrix Pade Approximation in equation (12)? Do forward operations of MPA in Table 1 contain computation of coefficients p m and q n
? 3. Does Equation (2) lack a (·){sym} operation for / f r a c l U
? 4. Is equation (11) missing a term z^{k} in the left side? 5. I am not sure why sign(B) in equation (21) can be calculated as identity matrix? 6. P{M} and Q_{N} are used to approximate the Taylor series. If I am not misunderstanding, does I- Q_{N}^{-1}P_{M} replace Q_{N}^{-1}P_{M} in Eqn. (13)? and do the terms I-X replace X in Eqn. (12)?","1. In this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point suggests a comparison between MTP/MPA and NS iteration based on accuracy and running time. However, it does not provide explicit actionable steps or experiments that the authors should undertake. The review point does not give any direct actionable comments, and it does not suggest any implicit actions that the authors can infer. It only mentions the need for comparison without providing any specific guidance on how to conduct the comparison. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point does not suggest any direct or explicit actionable comments that the authors should make. It also does not provide any implicit actionable comments that the authors can infer. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. 

In conclusion, the review point does not provide any actionable steps or experiments that the authors should undertake, and it does not suggest any implicit or explicit actionable comments. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

So the score is -1.",0,"The review point is presented in a neutral tone, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The reviewer simply states the facts and suggests a comparison between different methods, which is a neutral approach to reviewing. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a characteristic of a neutral tone. The reviewer also does not provide any specific suggestions for improvement, which is another aspect of a neutral tone. Therefore, based on the score rubric, the review point is neutral and factual, which is why it would receive a score of 0.",-1,"The review point makes a claim about the importance of Matrix Taylor Polynomial (MTP) or Matrix Pade Approximation (MPA) for fast matrix square root, and suggests comparing MTP/MPA+NS based BP and NS baed FP + Lya in terms of accuracy and running time. However, the review point does not provide any logical deductions or references to support this claim. It lacks the necessary justification and explanation to verify the claim. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability. It contains a claim that is not verified, as it misses a justification, explanation, or reference. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the paper's use of Matrix Taylor Polynomial (MTP) and Matrix Pade Approximation (MPA) for forward propagations, and the approximate Lyapunov equation for backward propagation. It also suggests a comparison between MTP/MPA+NS based BP and NS baed FP + Lya in terms of accuracy and running time. This specificity allows the authors to clearly identify which parts of the paper are being addressed by the review point. The review point is focused on the mathematical methods used in the paper and their implications for matrix square root calculations. It does not leave any ambiguity about the paper's content or the reviewer's intent. Therefore, the authors can be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not provide clear guidance to the authors. It would not help them identify which parts of the paper are being addressed by the review point. The authors would be left guessing which parts of the paper are being discussed, which would not be helpful for them to improve their work. 

The review point provided is well-structured and directly addresses the paper's content, making it easy for the authors to understand and respond to the reviewer's comments. It is clear, concise, and directly related to the mathematical methods used in the paper. This specificity is crucial for the authors to understand the reviewer's feedback and to make improvements to their work. 

Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it easy for the authors to understand and respond to the reviewer's comments. It is clear, concise, and directly related to the mathematical methods used in the paper. This specificity is crucial for the authors to understand the reviewer's feedback and to make improvements to their work. 

So the overall score is 1."
2040,ARR_2022_143_review,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","10. Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”).",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the difficulties of cross-domain translation and the potential of cluster-based methods without suggesting any specific actions or improvements. Furthermore, the review point does not provide any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. The review point is more of a critique than a constructive feedback, which does not align with the score rubric's requirement for actionability. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is not particularly polite or constructive. It does not offer any encouragement or helpful suggestions to the authors. Instead, it presents a critique of the paper's content, which is not delivered in a polite manner. The tone of the review point is more critical than constructive, which is not in line with the score rubric's criteria for politeness. The review point does not provide any positive feedback or suggestions for improvement, which is a key aspect of a polite and constructive review. Therefore, based on the score rubric, the review point is not helpful or encouraging, and it does not meet the criteria for a score of 1. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a request for a simple example of the difficulties of cross-domain translation and a brief summary of the major directions of dealing with ""domain adaption for NMT"". The review point does not make any claims that could be verified or disproved. It is a request for information and does not present any assertions that could be validated or refuted. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor does it provide a logical explanation or reference to support any claim. Hence, the score is 0.",1,"The review point provided is quite specific and directly addresses the difficulties of cross-domain translation and the usefulness of cluster-based methods. It also asks for a brief summary of the major directions of dealing with ""domain adaption for NMT,"" which is a clear and focused request. The review point is well-structured and directly relates to the content of the paper, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is targeting the specific aspects of the paper that are related to the difficulties of cross-domain translation and the effectiveness of cluster-based methods. The request for a comparison among the major directions is also a clear and specific request that would help in ranking the paper. Therefore, the review point is highly specific and directly addresses the content of the paper, which aligns with the score rubric's criteria for a score of 1."
914,ICLR_2023_1657,"Weaknesses:
The paper starts with grandiose claims of tackling ""open-ended learning"". However, open-ended learning involves learning to perform across diverse environments. But the definition of open-ended learning in this work seems restricted only to learning different skills in a given environment. In experiments, it is mostly restricted to goal conditioned environments and learning a goal conditioned policy.
""But where do goals come from? Almost always, they are sampled from a fixed distribution over a predefined goal space; i.e. they come from an engineer."" There are numerous works where the goals are NOT generated from a fixed distribution (listed in the references below)
The previous statement makes us believe that in this work, the goals are not generated from a fixed distribution. However, a few paragraphs later, the authors note that ""In this second challenge — the one we focus on — agents must learn to organize their own learning trajectories by prioritizing goals with the objective of maximizing long-term skill mastery."" i.e, this work focuses on learning a goal conditioned policy from pre-defined goals.
""In social episodes, a social partner suggests a novel goal to the agent and decomposes it into two consecutive sub-goals: 1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities."" The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments.
One of the contributions listed is: ""an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion"". This seems like a standard active learning setting and not a novel contribution.
References: [1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020 [2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017 [3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021 [4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021 [5] Automatic curriculum learning through value disagreement. Zhang et al, 2020 [6] Exploration via hindsight goal generation. Ren et al, 2019 [7] Automatic goal generation for reinforcement learning agents. Florensa et al, 2018","1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities."" The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments. One of the contributions listed is: ""an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion"". This seems like a standard active learning setting and not a novel contribution. References: [1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020 [2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017 [3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021 [4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021 [5] Automatic curriculum learning through value disagreement. Zhang et al, 2020 [6] Exploration via hindsight goal generation. Ren et al, 2019 [7] Automatic goal generation for reinforcement learning agents. Florensa et al, 2018",-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the existence of a frontier goal and a beyond goal without suggesting any specific actions or methods to achieve these goals. The review point also lacks any implicit actionable comments that could be inferred from the text. Instead, it only mentions some limitations without providing any explicit or implicit actions that should be done. This lack of actionability makes it difficult for the authors to deduce what needs to be done based on the review point. Therefore, according to the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided does not exhibit the level of politeness expected in a constructive critique. The tone is somewhat dismissive, with phrases like ""this seems like a standard active learning setting and not a novel contribution,"" which could be perceived as belittling the work of the authors. Additionally, the review point lacks the constructive element of providing specific suggestions for improvement, which is crucial for a helpful critique. Instead, it merely states that the contribution is not novel, without offering any constructive feedback or guidance on how to enhance the work. This approach does not align with the goal of fostering a positive and supportive environment for the authors to improve their work. Therefore, based on the score rubric, the tone of the review point is not helpful or encouraging, and it does not meet the criteria for a score of 1. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely describes a mechanism for an agent to self-monitor its learning progress and query a social partner for goal suggestions. This is a factual statement and does not require any justification or reference to verify its accuracy. The review point does not make any claims about the performance of the agent or the effectiveness of the learning mechanism, which are the types of claims that would need to be verified. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the contribution of an active learning mechanism in the paper. It clearly identifies the mechanism and its role in self-monitoring and querying for goal suggestions. The review point also provides a direct link to the paper's contribution, which is the active learning mechanism. This specificity allows the authors to confidently identify which part of the paper is being addressed by the review point. The review point is not only specific but also directly related to the contribution, making it easy for the authors to understand and respond to the feedback. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the model without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point does not suggest any specific changes or improvements that could be made to the model. It also does not provide any insights into how the model could be better suited for the task at hand. Instead, it simply points out the limitations of the model without offering any solutions or directions for future work. 

In conclusion, the review point does not provide any actionable comments or suggestions, which is why it does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the paper and providing a critique that is based on the information presented. The reviewer's comments are factual and do not contain any disrespectful or rude language. The feedback is constructive, offering suggestions for improvement without being overly critical or dismissive. The reviewer's tone is consistent throughout the review, maintaining a neutral stance. Therefore, the review point meets the criteria for a score of 0, as it is neither aggressive nor overly polite, but rather neutral and factual.",0,"The review point in question does not contain any claims or assertions that require verification. It merely states that only one generative model is tested, which is a factual statement that does not need any justification or reference. The review point does not make any claims about the results of the model or how they might differ with other models. It also does not provide any logical deductions or references to external or internal sources that could verify any claims. Therefore, according to the score rubric, the review point does not contain any claims that need verification, and it does not make any claims that could be verified. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It asks a direct question about the generative model used in the paper and whether the results would change with other models. This specificity allows the authors to identify which part of the paper is being addressed, and they can be confident about the exact area of focus. The review point is clear and concise, making it easy for the authors to understand and respond to. It also encourages further exploration of the topic, which is beneficial for the paper's development. Therefore, based on the score rubric, the review point is well-crafted and meets the criteria for a score of 1."
3390,NIPS_2020_875,"- Some of the claims made by the authors seem imprecise (see below), and the presentation could be more clear/streamlined. - Parts of the proposed approach seem somewhat ad-hoc, and would benefit from better empirical or theoretical motivation (see below). - Some of the baselines in the experiments seem weak or missing.",- Some of the baselines in the experiments seem weak or missing.,-1.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the weaknesses or missing baselines in the experiments without suggesting any specific actions to be taken. The review point does not provide any implicit actionable comments that can be inferred, nor does it offer any explicit or implicit actions that should be done. It simply states the limitations without providing any guidance on how to address them. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer's comments are focused on the content of the paper, and they do not express any personal opinions or judgments about the quality of the work. The reviewer's questions and observations are presented in a straightforward manner, without any additional commentary that could be perceived as polite or impolite. The reviewer's approach is consistent with the expected tone for a neutral review, which is to provide a factual assessment of the paper without any personal bias or emotional undertones. Therefore, the review point is neutral and does not fit well with the other two scores, which are helpful or encouraging, or rude, disrespectful, or aggressive. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It merely states that some of the baselines in the experiments seem weak or missing, without providing any specific examples or evidence to support this claim. The review point does not make any claims that could be verified, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It also does not meet the criteria for a score of -1, as it does not contain any claims that are not verified. The review point is neutral and does not contain any claims that require verification. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The authors are left with a sense of uncertainty about the specific aspects of the paper that are being critiqued. The review point does not provide any concrete examples or suggestions for improvement, which would have helped the authors to understand and address the issues raised. Furthermore, the review point does not offer any constructive feedback or guidance on how to improve the paper. Instead, it leaves the authors with a general sense of confusion and uncertainty about the paper's shortcomings. This lack of specificity and clarity in the review point does not meet the criteria for a score of 1, as it does not allow the authors to confidently identify the parts of the paper that are being addressed. Therefore, based on the score rubric, the review point is not effective in addressing the specificity of the paper."
4072,NIPS_2020_1186,"Theoretical Grounding: - Since the regret bound relies on bounding the state reconstruction error at each point in a finite-length sequence, it seems clear to me that the bound only applies to MBRL algorithms that rely exclusively on full-episode rollouts for planning (e.g. PILCO). This criticism in particular seems to apply to the practical algorithm used in the submission, since it relies on a parametric value function approximator to make the planning horizon more tractable. - In practice we don't know \beta _a priori_. How should \beta be chosen? Empirical Evaluation: - The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch. - No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling. - No ablations of β (presumably a crucial design choice). - No demonstration that their dynamics models as implemented satisfy their calibration assumption. Significance/Impact: - Information on the specific implementation details is fairly sparse (e.g. what learning rate and batch size did you use for your dynamics models? Did you reuse a previous implementation of MPO?). Reproducing the authors' findings would likely prove very difficult. MBRL is notorious in the machine learning field for reproducibility issues. If you (the authors) had to reimplement your method from scratch tomorrow, what details would you need?",- No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling.,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the proposed method without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors should make. It lacks any explicit or implicit actions that should be done, which is why it does not meet the criteria for actionability. The authors are left without any clear direction on how to proceed with their work. Therefore, the review point does not fulfill the requirements of the score rubric.",-1,"The review point provided does not meet the criteria for politeness as outlined in the score rubric. The tone of the review point is aggressive and disrespectful, which is evident in the choice of words such as ""No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling."" This statement is not only factually incorrect but also dismissive of the paper's contributions. The reviewer fails to provide constructive feedback or suggestions for improvement, instead choosing to criticize the paper's methodology without offering any alternative solutions or insights. This approach is not conducive to a respectful and helpful review process. Therefore, based on the score rubric, the review point is not helpful, encouraging, or polite, and thus would receive a score of -1.",0,"The review point provided does not contain any claims or make any comparisons to competing methods. It merely describes the proposed exploration strategy and its comparison to greedy improvement and approximate Thompson Sampling. The lack of claims and comparisons means that there is no need for verification, as the review point does not make any assertions that could be verified or refuted. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need verification. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims or comparisons that could be verified. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer a clear direction for the authors to follow. It lacks specificity and does not address any particular part of the paper. The review point does not provide any concrete examples or suggestions for improvement, which would help the authors understand what aspects of their work need attention. Furthermore, the review point does not make it obvious which part of the paper is being addressed, leaving the authors unsure about the paper part that is being addressed by the review point comment. This lack of specificity and clarity in the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Therefore, based on the score rubric, the review point is not effective in guiding the authors to improve their paper."
2482,NIPS_2021_277,"Weaknesses:
The first two rows in Table 1 show that the proposed ITC loss significantly improves the performance. I wish to hear discussions and comments on where the ITC could be useful. Is it specified to the proposed vision-transformer-related VLP models, or it could be extended to the Vilt-like structure [21] with linear projection only, or even to the conventional VLP methods [13,2,3] with detector features.
The ablation of the model architecture might be necessary to understand the source of improvements, i.e., from the proposed losses and distillation, or the change in model architecture and patch size. Specifically, it might be necessary to ablate 1) different model sizes, e.g., small, base, large; 2) different patch sizes and numbers, e.g., ViT/32, /16, and input image size; 3) the influence of model initialization in Sec 3.1.
A minor concern is the comparison to SOTA in Table 4. The compared methods are the BASE version of the SOTA [3,8]. The LARGE version of [3,8] yields similar performance to the reported numbers.
A comment: The contribution of the momentum distillation and the analyses in Section 4 limit at applying the related techniques (moving average teacher in semi-supervised learning [33], the mutual information maximization perspective in NLP [A]) onto the VLP methods. Nonetheless, the proposed techniques and analyses do help the VLP study.
[A] A mutual information maximization perspective of language representation learning
The limitations and societal impacts are discussed in the conclusion section.","2) different patch sizes and numbers, e.g., ViT/32, /16, and input image size;",-1.0,1.0,0.0,0.0, This needs the rest of the review to annotate,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely mentions the existence of different patch sizes and numbers, and the input image size, without suggesting any specific actions or improvements that could be made. The review point also does not include any actionable comments that can be implicitly inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionable content is why the review point does not meet the criteria for a higher score. Therefore, based on the score rubric, the review point is not actionable and does not provide any guidance for the authors to follow.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The feedback is based on the facts presented in the paper, and the reviewer does not make any subjective judgments or comments on the quality of the paper. The reviewer also does not provide any encouragement or constructive criticism, which would have been necessary for a higher score. The feedback is purely factual and does not engage with the paper in a way that would be considered polite or encouraging. Therefore, the review point is neutral and factual, and the tone is neither aggressive nor overly polite, which is why it receives a score of 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states the different patch sizes and input image sizes used in the study. There are no assertions made that require justification or reference to external or internal sources. The review point is purely factual and does not make any claims that could be verified or refuted. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the patch sizes and numbers used in the paper, as well as the input image size. This specificity allows the authors to pinpoint exactly which parts of the paper are being addressed by the review point. The authors can confidently identify that the review point is discussing the different patch sizes and numbers, as well as the input image size, and how these factors might affect the performance of the model. This level of specificity is crucial for the authors to understand and address the feedback provided.

In contrast, a review point that lacks specificity would not allow the authors to confidently identify which parts of the paper are being addressed. The authors would be left guessing which parts of the paper are related to the review point, which could lead to confusion and a lack of direction for their revisions.

The review point provided is also clear and concise, making it easy for the authors to understand and respond to. It directly addresses the specific aspects of the paper that need to be improved or clarified. This clarity and conciseness further contribute to the authors' confidence in understanding and addressing the feedback.

In summary, the review point provided is specific, clear, and concise, which allows the authors to confidently identify and address the feedback. This level of specificity is essential for effective communication between the reviewer and the authors. Therefore, based on the score rubric, the review point is of high quality and would receive a score of 1."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.",- What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision.,0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It does not suggest any specific actions that the authors should take, nor does it provide any implicit actionable comments that can be inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionability makes it difficult for the authors to deduce what needs to be done based on the review point. Therefore, according to the score rubric, the review point does not meet the criteria for actionability and would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer asks questions about the impact of environment ordering during training and the necessity of the curriculum, which are factual and neutral. However, the reviewer does not provide any constructive feedback or suggestions for improvement, which could have made the tone more helpful and encouraging. The reviewer also does not express any opinion on the quality of the paper or the authors' work, which is a characteristic of a neutral tone. Therefore, based on the rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not contain any claims that need to be verified. It is a series of questions that are more speculative in nature, rather than making definitive statements that could be verified or refuted. The questions are more about the potential implications of environment ordering during training, and the reasons behind choosing such a curriculum. They do not present any factual claims that could be validated or disproven. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains claims that need verification nor does it provide any logical deductions or references to external or internal sources that could support or refute the claims. Hence, the score is 0.",1,"The review point provided is quite specific and directly addresses the impact of environment ordering during training, the curriculum effects, and the rationale behind using such a curriculum. It clearly identifies the parts of the paper that are relevant to the review point, which would allow the authors to confidently identify the sections of their paper that need to be addressed. The review point also suggests an ablation to demonstrate the necessity of the approach, which would provide a clear context for the decision to use this curriculum. This specificity in the review point aligns well with the score rubric, which emphasizes the importance of clarity and directness in addressing the paper's content. Therefore, based on the score rubric, the review point is well-crafted and would effectively guide the authors in revising their paper."
4072,NIPS_2020_1186,"Theoretical Grounding: - Since the regret bound relies on bounding the state reconstruction error at each point in a finite-length sequence, it seems clear to me that the bound only applies to MBRL algorithms that rely exclusively on full-episode rollouts for planning (e.g. PILCO). This criticism in particular seems to apply to the practical algorithm used in the submission, since it relies on a parametric value function approximator to make the planning horizon more tractable. - In practice we don't know \beta _a priori_. How should \beta be chosen? Empirical Evaluation: - The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch. - No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling. - No ablations of β (presumably a crucial design choice). - No demonstration that their dynamics models as implemented satisfy their calibration assumption. Significance/Impact: - Information on the specific implementation details is fairly sparse (e.g. what learning rate and batch size did you use for your dynamics models? Did you reuse a previous implementation of MPO?). Reproducing the authors' findings would likely prove very difficult. MBRL is notorious in the machine learning field for reproducibility issues. If you (the authors) had to reimplement your method from scratch tomorrow, what details would you need?","- The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the method without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that the authors can infer, nor does it give any explicit or implicit actions that should be done. It is clear that the authors are left without any guidance on how to proceed with their research. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the score rubric's criteria for politeness. The tone of the review point is aggressive and disrespectful, which is evident from the phrases such as ""The method is not evaluated in a stochastic environment"" and ""the experiments give the impression of a bait-and-switch."" These statements are not only factually incorrect but also convey a negative and confrontational attitude towards the paper. The reviewer's approach is not constructive or helpful, but rather critical and dismissive. This is in stark contrast to the polite and encouraging tone that is expected according to the score rubric. Therefore, the review point does not meet the criteria for politeness and is not helpful or encouraging. So the overall score is -1.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely states that the method should be evaluated in a stochastic environment, which is a factual statement that does not need any justification or reference. The review point does not make any claims about the method's performance, the validity of the experiments, or the accuracy of the results. It also does not provide any logical deductions or references to external or internal sources that could support or refute the claim. Therefore, according to the score rubric, the review point does not contain any claims that need verification, and it does not make any claims that could be verified or refuted. So the overall score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the need to evaluate the method in a stochastic environment, but it does not specify which sections of the paper are related to this aspect. The authors are left with uncertainty about which parts of the paper are being addressed by the review point. The lack of specificity in the review point does not allow the authors to confidently identify the relevant sections of the paper. Therefore, the score is -1."
2969,NIPS_2022_1846,"Weaknesses and Questions 1. For the distribution estimation, this paper uses three Gradient flow networks to learn different parameters. According to Section 4.3, the networks learn specific parameters for each class using different inputs (i.e., x ¯ j
). That is to say, the network F 1
will output c j j = 1 n for n
classes. However, in Line 142, the authors point that the parameter c
is shared between all classes. How to unify this c
? 2. How to update F 2
, and F 3
via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1
can be trained. 3. Some experimental details are missing. 3.1. What is the ratio of training data D t
to validation data D v
in the training stage. 3.2. What is the value of initial c
. 4. In Table 2, the metric-based baseline FEAT performs similar accuracy to the proposed method. It’s better to discuss the superiority of the method in terms of time consumption. According to Algorithm 2, the upper bound Eq. (14) simplifies the training of classifiers, but Eq. (16) is still difficult to compute. Typo: In Table 2, the description does not match the content, e.g., ""Euclidean Metric"" (or ""Hyperbolic Metric"") and ""Model"".",3. Some experimental details are missing. 3.1. What is the ratio of training data D t to validation data D v in the training stage. 3.2. What is the value of initial c .,0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the missing experimental details without suggesting any specific actions or improvements that could be made. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point only mentions some limitations without giving any explicit or implicit actions that should be done. It fails to provide any actionable insights or suggestions that could help the authors improve their work. The lack of actionable comments in the review point makes it difficult for the authors to deduce what needs to be done to address the issues raised. 

In conclusion, the review point does not provide any actionable steps or comments, which is why it receives a score of -1 according to the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer asks for clarification on the ratio of training data to validation data and the value of initial c, which are factual inquiries. However, the reviewer does not provide any constructive feedback or suggestions for improvement, which could have made the tone more helpful and encouraging. The reviewer's tone is more passive, which is why it fits the score of 0. The reviewer's approach is more focused on seeking clarification rather than providing feedback or suggestions for improvement. Therefore, the review point is neutral and factual, but it lacks the constructive tone that would have made it more helpful and encouraging.",-1,"The review point in question contains two claims: 

1. The ratio of training data D t to validation data D v in the training stage. 
2. The value of initial c .

However, the review point does not provide any verification or justification for these claims. It merely states the claims without any supporting evidence or logical deductions. This lack of verification and justification is a clear deviation from the score rubric, which requires claims to be supported by logical deductions and internal or external sources.

The absence of any verification or justification for the claims in the review point makes it difficult to assess the validity of the claims. Without this information, it is impossible to determine whether the claims are accurate or not.

Therefore, based on the score rubric, the review point does not meet the criteria for verifiability. It contains claims that are not verified, and it misses the justification, explanation, or reference that would be necessary to support the claims. As a result, the review point is not aligned with the score rubric's requirements for verifiability. 

So the overall score is -1.",1,"The review point provided is quite specific, addressing two distinct aspects of the paper: the ratio of training data to validation data and the value of the initial c. This specificity allows the authors to pinpoint exactly which parts of the paper are being addressed, which is crucial for them to understand and respond to the review point. The authors can be confident that the review point is focused on these two aspects, and they can also be confident that these are the only aspects being addressed. This level of specificity is in line with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed. Therefore, the authors can be confident about which part of the paper is being addressed by the review point."
3661,NIPS_2020_1466,"- Regarding the comparisons in Table 1., I would have liked to see comparisons with [9], [10] and/or [11]. SMPL and ASAP are good, but they are blended with an initialization from the authors' method and a comparison with DHBC alone is not completely convincing. - The method requires the ground truth correspondences as supervision which is a strong assumption. Many related methods ([11, 13, 40] in the paper) investigated the unsupervised setting at least to some degree and therefore allow for training on non-synthetic data. - The authors decided to limit themselves to human shapes in this work, although there are no modelling assumption that clearly require this. While I can to some extent understand this decision, it would still be interesting to see some qualitative generalization results to other classes of shapes. I understand that your method requires the human template for training, but similar methods [11] also show at least some examples that are not human or only close to human shape.","- The method requires the ground truth correspondences as supervision which is a strong assumption. Many related methods ([11, 13, 40] in the paper) investigated the unsupervised setting at least to some degree and therefore allow for training on non-synthetic data.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the method without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, nor does it give any explicit or implicit actions that should be done. It simply states the assumptions and limitations of the method without offering any constructive feedback or suggestions for improvement. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and is not helpful in guiding the authors on what to do next. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not offer any encouragement or constructive feedback. The review point simply states the method's reliance on ground truth correspondences and mentions that other methods have investigated the unsupervised setting. It does not provide any specific suggestions or criticisms, which makes it a passive review point. The reviewer does not engage with the content of the paper in a way that would be considered helpful or encouraging. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question makes a claim about the necessity of ground truth correspondences for the method, which is a strong assumption. However, it fails to provide any verification or justification for this claim. The review point does not offer any logical deductions or references to external or internal sources that could support the claim. It merely states that many related methods have investigated the unsupervised setting, but it does not elaborate on how this relates to the claim or how it verifies the claim.

The review point also does not address the aspect of verifiability in a meaningful way. It does not provide any examples or explanations that would demonstrate the verifiability of the claim. Instead, it simply states that the claim is not verified, which is not helpful in understanding the review point's position on the matter.

In conclusion, the review point contains a claim that is not verified, and it does not provide any logical explanations or references to support the claim. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Therefore, the review point is not effective in demonstrating the verifiability of the claim.",1,"The review point provided is quite specific and directly addresses the method's reliance on ground truth correspondences for supervision. It clearly identifies the assumption that the method requires and contrasts it with other related methods that have investigated the unsupervised setting. This specificity allows the authors to pinpoint the exact part of the paper that is being addressed, which is the method's supervision requirement. The review point also provides a clear direction for the authors to consider, which is to investigate the unsupervised setting further. This specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, the review point lacks specificity when it comes to the general theme or idea that is spread across many paper sections. It does not provide clear guidance on how to improve the paper or which specific sections need attention. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point, which is why it would receive a score of -1 according to the score rubric. 

Therefore, based on the score rubric's criteria, the review point is specific and directly addresses a key aspect of the paper, making it clear for the authors to understand and act upon. This specificity is what sets it apart from a review point that lacks specificity and does not provide clear guidance for the authors. 

So the overall score is 1."
315,ICLR_2022_2903,"Weaknesses / Questions for authors ===
""Coherence"" seems to be an important topic of the paper. It is mentioned often, and Section 3.1 provide a formal definition of epsilon-coherence. However, in the empirical results this didn't seem to be mentioned anymore. It made me wonder: what is the use of the formal definitions in Section 3.1? I was expected this to be used to evaluate the models. Maybe it did, but then it wasn't clear to me, so in any case I think the relation between the experiments and Section 3.1 can be made clearer.
I found the explanation of the newly proposed relation decoder the ""Dynamic Comparator"" too short. It was difficult for me to understand what it did. More importantly: it seems to outperform all other models very convincingly, but I could not find any discussion on why you believe this is the case. So I felt the empirical results deserve more detailed reflection. Perhaps an ablation study would be interesting?
The empirical studies by itself are also somewhat one-sided (MNIST --> BlockStack). Did you also try the other direction? I.e., train on the BlockStack domain and then evaluate on MNIST? More generally, I was wondering whether the empirical setup is actually similar to how we humans learn. Can we really learn a relation such as ""larger than"" from one domain, and can we directly apply it to another? This is actually not directly clear to me. I can imagine we learn a relation in multiple domains, after which we are able to generalize to new domains.
All appendices seem to be missing from the PDF.
=== Detailed comments for improving the paper ===
Section 2:
I liked the formalisation and I think it was well presented, also the example was helpful.
Section 3:
""Z is a latent variable, itself draw from marginal p_z"" --> ""draw"" should be ""drawn"". Also, I believe p_z is a prior instead of a marginal.
In Definition 3: ψ S ( S )
is undefined, I supposed it means ψ S ( S ) = ψ S ( s ) s ∈ S
, i.e. all encoding of S?
It could be helpful to add a sentence explaining the measure on substructures, e.g., saying that this value will be close to 1 if the soft-structure is similar to the structure.
""It is straightforward to show that ∑ S σ . . . = 1
"" --> what are we summing over here? All possible interpretations of \sigma use S? Please clarify this.
""If we have a theory \Tau over \sigma"" --> I don't get this, isn't a theory defined over the language which contains the relations?
Section 4:
Overall I found this section quite difficult to follow and I wasn't sure the details were relevant (but I could be wrong here). You could consider removing some technical details here (and moving them to the appendix), so that you have more space for possible additional experiments.
The grounding s i j k
is used but only in the next paragraph it is explained what it means, so please define it beforehand.
It is not explained what an ""atomic subformula"" is.
Section 5:
""given a domain images"" --> remove ""a""
""latent space ."" --> remove space
The explanation on the DC is extremely dense, and I'd like to understand better why you developed it this way and why you think it works well.
Section 8:
""ordinaliety"" --> ordinality",".. = 1 "" --> what are we summing over here? All possible interpretations of \sigma use S? Please clarify this. ""If we have a theory \Tau over \sigma"" --> I don't get this, isn't a theory defined over the language which contains the relations? Section 4: Overall I found this section quite difficult to follow and I wasn't sure the details were relevant (but I could be wrong here). You could consider removing some technical details here (and moving them to the appendix), so that you have more space for possible additional experiments. The grounding s i j k is used but only in the next paragraph it is explained what it means, so please define it beforehand. It is not explained what an ""atomic subformula"" is. Section 5: ""given a domain images"" --> remove ""a"" ""latent space ."" --> remove space The explanation on the DC is extremely dense, and I'd like to understand better why you developed it this way and why you think it works well. Section 8: ""ordinaliety"" --> ordinality",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out areas of confusion and lack of clarity without suggesting any specific actions to be taken. For instance, the reviewer questions the relevance of certain technical details and the use of ""atomic subformula"" without providing any guidance on how to address these issues. Similarly, the reviewer expresses a desire to understand the rationale behind the development of the DC but does not offer any suggestions on how to achieve this understanding. The review point also fails to provide any implicit actionable comments that the authors could infer from the review. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not particularly polite. It contains several instances of direct criticism and questions that could be perceived as confrontational. For example, the reviewer asks, ""what are we summing over here?"" and ""isn't a theory defined over the language which contains the relations?"" These questions, while valid, are posed in a way that could be interpreted as challenging the author's understanding of the subject matter. Additionally, the reviewer suggests removing technical details and moving them to the appendix, which could be seen as dismissive of the author's efforts to include comprehensive information. The reviewer also points out that the explanation of the grounding s i j k is not provided beforehand, which is a valid point but is presented in a way that could be perceived as critical. The reviewer's comments on the DC and the term ""ordinality"" are also presented in a way that could be seen as overly critical. Overall, the tone of the review point is more critical than constructive, which is not in line with the score rubric's criteria for politeness. So the overall score is -1.",-1,"The review point provided contains several claims that are not verified. For instance, the reviewer questions the relevance of certain technical details and the use of the term ""atomic subformula"" without providing any justification or explanation. Additionally, the reviewer suggests removing some details and moving them to the appendix, but does not provide any logical reasoning or external sources to support this claim. The reviewer also expresses confusion about the theory being defined over the language containing the relations, but fails to provide any logical deductions or internal or external sources to verify this claim. Furthermore, the reviewer questions the explanation of the grounding s i j k and the development of the DC, but does not offer any verification or justification for these claims. Overall, the review point lacks the necessary verification of claims, which is essential according to the score rubric. Therefore, the review point does not meet the criteria for verifiability and is not supported by logical deductions or internal or external sources. So the score is -1.",1,"The review point provided is quite specific and directly addresses certain aspects of the paper. It points out the lack of clarity in the explanation of the grounding s i j k and the need for a more detailed explanation of the atomic subformula. The reviewer also questions the relevance of certain technical details in Section 4, suggesting that they could be moved to an appendix. Furthermore, the reviewer requests more information on the development of the DC and its effectiveness. These specific points of critique demonstrate a clear understanding of the paper's content and a focused approach to the review.

However, the review point could have been more specific in some areas. For instance, it could have provided more context on why the authors chose to develop the DC in the way they did. Additionally, the reviewer's request for clarification on the use of S in the context of the theory Tau is somewhat vague and could have been more precisely articulated.

Despite these minor shortcomings, the review point is generally well-structured and directly addresses the paper's content, making it clear which parts of the paper are being addressed. The authors would likely be confident about which parts of the paper are being addressed by the review point, as the reviewer's comments are specific and directly related to the paper's content.

In conclusion, the review point is well-crafted and provides a clear and specific critique of the paper. It demonstrates a good understanding of the paper's content and directly addresses the areas that need improvement. The reviewer's comments are focused and relevant, which would help the authors to understand and address the issues raised."
1305,ICLR_2023_1603,"Weaknesses - 1. The main drawback of the design is that it relies heavily on a centrally available dataset. One of the two primary goals of the system is to handle non-IIDness in the data, which raises the question - how does the performance of F2L depend on the quality of the root dataset at the server. How well does the root dataset represent the non-IIDness present among the clients? How is scalability affected if the root dataset is not updated to well represent the newly joined clients? More experiments are required to convince the reader that the system can do well even when the root dataset does not exactly represent the data distribution among the clients. 2. Table 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance? 3. Figure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature? 4. F2L relies on switching between LKD and FedAvg after sufficient convergence has happened. How is this threshold chosen? What can be a general way to choose this value for any dataset? 5. Figure 3 shows that a student can outperform a teacher in F2L. This experiment was performed on EMNIST. Does this observation hold in general, independent of the dataset? If not, what conditions does this depend on?",3. Figure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It does not suggest any specific changes or improvements that could be made to the F2L model or the training process. The review point also does not include any actionable comments that can be implicitly inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. The review point does not guide the authors on how to address the limitations or how to improve the performance of the F2L model. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point is delivered in a neutral tone, which aligns with the score rubric's criteria for a score of 0. The reviewer's language is factual and does not convey any particular emotion or stance. The reviewer asks questions about the F2L performance and the sensitivity of the observations to the knowledge distillation parameters, which is a constructive approach to reviewing. However, the reviewer does not provide any specific examples of politeness or lack thereof, which would have been helpful in assessing the tone of the review point. The reviewer's tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer's tone is neutral, which is the default score if the review point does not fit well with the other two scores. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores. So the score is 0.",-1,"The review point in question makes several claims, such as the performance of F2L being better than vanilla FL, the potential role of knowledge distillation, and the sensitivity of observations to the parameters of knowledge distillation. However, the review point does not provide any logical deductions or references to external or internal sources to support these claims. The lack of verification for the claims made in the review point means that the review point does not meet the criteria for verifiability as outlined in the score rubric. Therefore, the review point is not satisfactory in terms of verifying its claims. 

In contrast, a review point that contains a claim and verifies it would provide logical explanations or references to external or internal sources to support the claim. For example, a review point might cite a specific statistical guideline to support a claim about p-values, or it might reference a previous study to support a claim about the performance of a machine learning model. 

The review point in question also fails to address the comparison with Fed-Distill, which is a key aspect of the review point. Without this comparison, the review point is incomplete and does not provide a comprehensive analysis of the topic. 

In conclusion, the review point does not meet the criteria for verifiability as it lacks logical deductions and references to support its claims. It also fails to address all aspects of the review point, such as the comparison with Fed-Distill. Therefore, the review point is not satisfactory in terms of verifying its claims.",1,"The review point provided is quite specific and directly addresses the performance of F2L when a client is injected midway during the training. It also asks about the potential role of knowledge distillation in this performance improvement. The review point further compares F2L with Fed-Distill and questions the sensitivity of the observations with respect to the knowledge distillation parameters - lambda and temperature. This specificity in the review point allows the authors to identify which parts of the paper are being addressed, and they can be confident about the paper part that is being addressed by the review point comment. The review point is well-structured and directly relates to the paper's content, making it clear and easy to understand. Therefore, the review point meets the criteria for a score of 1, as it is specific and directly addresses the paper's content."
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.",- A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the potential impact of re-evaluating modes less often and the relationship between the frequency of mode selection and relation re-prediction. The review point does not suggest any specific actions or experiments that the authors could undertake to improve the final performance. It also does not provide any implicit actionable comments that the authors can infer from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point is focused on the technical aspects of the paper, such as the prediction modes and their frequency, without making any personal judgments or comments on the quality of the paper. It is a straightforward analysis of the paper's content, which is in line with the expected tone for a score of 0. The review point does not contain any elements of politeness or rudeness, and it maintains a neutral stance throughout. Therefore, based on the score rubric, the review point is appropriately scored as 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a hypothetical scenario that is not grounded in any specific data or methodology. The review point is more of a thought experiment, exploring the potential impact of varying the frequency of mode selection and relation re-prediction on the final performance of a model. It does not make any assertions that need to be verified, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims or require any form of verification.",1,"The review point provided is quite specific and directly addresses the impact of the frequency of mode selection and relation re-prediction on the final performance of the model. It clearly identifies the part of the paper that is being addressed, which is the prediction mode for each variable at every time step. The review point also provides a clear connection between the frequency of mode selection and the final performance of the model, which is a crucial aspect of the paper.

The review point is well-structured and directly engages with the paper's content, making it easy for the authors to identify which parts of the paper are being addressed. The specificity of the review point is evident in the way it directly relates the frequency of mode selection to the final performance of the model. This specificity is in line with the score rubric, which emphasizes the importance of the review point being clear and directly addressing the paper's content.

In contrast, the review point does not provide any specific examples or details that would help the authors to understand the impact of the frequency of mode selection and relation re-prediction on the final performance of the model. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point.

Therefore, based on the score rubric, the review point is well-structured and directly addresses the paper's content, making it easy for the authors to identify which parts of the paper are being addressed. The specificity of the review point is evident in the way it directly relates the frequency of mode selection to the final performance of the model. This specificity is in line with the score rubric, which emphasizes the importance of the review point being clear and directly addressing the paper's content. So the overall score is 1."
4665,NIPS_2020_789,"- My main concern is that, I don’t see the benefits of modeling the data as a union of subspaces, where each subspace corresponds to a class, when the representation space is *learned*. In particular, since these subspaces won’t be orthogonal in practice, on real data. In an unsupervised setting, to recover the subspaces, one needs to perform subspace clustering, which is a hard problem and computationally expensive to perform. In a supervised setting, where estimation of the subspaces is easy, one needs to do nearest-subspace-classification which is more intricate than linear classification. In stark contrast, a linear head trained with a cross-entropy loss learns a representation space with approximately linearly separable regions for each class. As a consequence, classification is simple (linear) and Lp distances in representation space are meaningful (which is not necessarily the case when the classes lie on a union of subspaces). - I acknowledge the encouraging results regarding robustness of the representations learned with the proposed method. However, there are many other methods which can make neural networks with linear classification head more robust, for example [c]. Therefore I believe a union of subspace structure is not fundamentally required to achieve this. - While the theoretical analysis reveals interesting properties of the learned representation, it completely ignores the relationship between the individual data points and their representation, defined through the feature extractor. It is well-known that the structure and properties of the extractor crucially impact the learned representation, possibly even more than the loss, see e.g. [ZF14]. [c] Elsayed, Gamaleldin, et al. ""Large margin deep networks for classification."" Advances in neural information processing systems. 2018. --- Update after rebuttal: Thanks to the authors for their response. I now better see the benefits of encouraging orthogonality between class regions in the feature space, which is why I increased my rating. However, I'm still not sure whether the theoretical result is useful to explain what is going on, as I still believe the network architecture is crucial for the structure in the feature space. Furthermore, as pointed out by the other reviewers, the method seems to have many similarities with previous methods, which should be discussed more precisely.","- I acknowledge the encouraging results regarding robustness of the representations learned with the proposed method. However, there are many other methods which can make neural networks with linear classification head more robust, for example [c]. Therefore I believe a union of subspace structure is not fundamentally required to achieve this.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that there are other methods that could potentially make neural networks more robust, without suggesting any specific actions or improvements that could be made to the proposed method. The review point also does not include any implicit actionable comments that could be inferred from the review point. It simply mentions the existence of other methods without providing any guidance or direction for the authors to follow. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer acknowledges the positive results of the proposed method but also points out that there are other methods that could potentially make neural networks more robust. This is a neutral observation that does not show any aggression or disrespect. The reviewer does not provide any specific examples or criticisms, which further contributes to the neutral tone of the review point. The reviewer's tone is not overly critical or overly positive, which is why it fits the score of 0. The reviewer's language is also polite and professional, which is important in a review context. Therefore, based on the rubric, the review point is neutral and factual, and it does not exhibit any aggressive or disrespectful tone.",0,"The review point in question does not contain any claims that need to be verified. It merely states that there are other methods that could potentially make neural networks more robust, without making any specific claims about the proposed method. The review point does not provide any logical deductions or references to external or internal sources that would support or verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains any claims nor does it provide any justification, explanation, or reference to support or verify any claims. Hence, the score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The reviewer acknowledges the robustness of the representations but then immediately dismisses the need for a union of subspace structure without providing any specific reasoning or evidence from the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The reviewer's statement could apply to many different aspects of the paper, and without further clarification, the authors would be left guessing which parts are being addressed. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors would not be confident about which part of the paper is being addressed by the review point. So the score is -1."
3498,NIPS_2020_944,"1. Weighted retraining is not new. The cross-entropy method (De Boer et al., 2005; Neil et al., 2018) maximizes the expectation E_p(x)[f(x)] of the objective function f(x) when sampling from a policy p(x) by periodically retraining p(x) on the samples with the highest reward, e.g. those with a reward above a quantile cutoff (i.e. using a stepwise weighting function). Instantiations of the cross-entropy method include DbAs (Brooks et al) and FBGAN (Gupta et al). Reward weighted regression (RWR) (Hachiya et al) is another existing optimization technique that employs weighted retraining. Angermueller et al. (http://arxiv.org/abs/2006.03227) recently employed these techniques as baselines for high-dimensional discrete optimization. 2. The described rank-based weighting function is not new. See RankGAN (Lin et al. 2017) or LeakGAN (Guo et al. 2017) for an example. 3. The evaluation is missing important baselines such a DbAs, FBGAN, RWR, and model-based optimization. 4. Chemical design task: It is unclear how the optimization trajectory of ‘original’ was obtained. How were new data points sampled from JT-VAE? Why does the trajectory stop at 250? 5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning. 6. What do error bars represent? How often were experiments repeated with different random seeds?",6. What do error bars represent? How often were experiments repeated with different random seeds?,0.0,0.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the existence of error bars and the repetition of experiments with different random seeds without suggesting any specific actions or improvements that could be made. The review point does not offer any insight into how the authors could improve their work or what they could do differently. It also does not provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the paper and providing a critique based on the information presented. The reviewer's comments are factual and do not contain any disrespectful or rude language. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. The reviewer's comments are constructive, offering suggestions for improvement without being overly critical or dismissive. The reviewer's tone is neither encouraging nor discouraging, but rather neutral and informative. Therefore, the review point is in line with the score rubric's criteria for a score of 0.",0,"The review point in question does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to external or internal sources. The review point simply asks what error bars represent and how often experiments were repeated with different random seeds. It does not make any claims about the validity of the statistical analysis, the effectiveness of the method, or the accuracy of the results. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not provide any information that could be verified or refuted. It is a straightforward question that does not require any further elaboration or justification. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the two main aspects of the paper: the representation of error bars and the frequency of experiment repetition with different random seeds. The authors can be confident that the review point is discussing the error bars and the experiment repetition, as it clearly states what these terms represent and how often the experiments were repeated. The review point is well-structured and directly relates to the content of the paper, making it easy for the authors to identify which parts of the paper are being addressed. The specificity of the review point ensures that the authors are not left guessing which parts of the paper are being discussed. Therefore, based on the score rubric, the review point is of high quality and demonstrates a clear understanding of the paper's content."
5646,NIPS_2018_809,"Weakness: - The uniqueness of connecting curves between two weights would be unclear, and there might be a gap between the curve and FGE. A natural question would be, for example, if we run the curve findings several times, we will see many different curves? Or, those curves would be nearly unique?  - The evidences are basically empirical, and it would be nice if we have some supportive explanations on why this curve happens (and whether it always happens). - The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding)  Comment: - Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research. - To be honest, I'm not sure the first curve-finding part explains well why the FGE work. The cyclical learning rate scheduling would perturb the weight around the initial converged weight, but it cannot guarantee that weight is changing along the curve described in the first part.","- The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding) Comment:",-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the connections between the curve finding and FGE are weak and that the method described is computationally demanding. However, it does not suggest any specific improvements or changes that could be made to address these issues. The review point also lacks any implicit actionable comments that could be inferred from the text. It does not provide any guidance on how the authors could improve their work or what specific aspects of their method could be further explored or refined. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or constructive criticism. The tone is purely informative, with the reviewer simply stating their observations about the paper. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a key aspect of the score rubric. The reviewer's comments are focused on the content of the paper, rather than the tone or style of the paper. Therefore, the review point is neutral and does not fit well with the other two scores. 

The reviewer's comments are also factual, with no personal opinions or biases expressed. The reviewer simply states what they see in the paper, without any additional commentary. This is in line with the score rubric's requirement for a neutral tone. The reviewer's comments are also passive, as they do not actively engage with the paper or its content. Instead, they simply report on what they observe. 

In conclusion, the review point is neutral, factual, and passive, which is why it receives a score of 0 according to the score rubric. The reviewer's tone is not aggressive, nor is it encouraging or constructive, and their comments are purely informative.",0,"The review point provided does not contain any claims that need to be verified. It merely states a personal opinion about the connections between the curve finding and FGE, without making any assertions that could be verified. The reviewer's comment about the computational demand of the method is a factual statement that does not require any form of verification. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it neither contains any claims nor does it provide any logical deductions or references to support any claims. So the overall score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer's comment about the connections between the curve finding and FGE being weak is vague and does not provide a clear direction for the authors to improve their paper. The reviewer's statement about the computational demand of the method is also not directly related to the specific aspects of the paper that the reviewer is addressing. The lack of specificity in the review point makes it difficult for the authors to understand which parts of the paper are being criticized or praised. This lack of specificity is why the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. 

The review point does not provide any concrete examples or suggestions for improvement, which would have been helpful in guiding the authors to address the issues raised. Instead, the reviewer's comments are general and do not offer any actionable insights. This lack of specificity is why the review point does not meet the criteria for a score of 0, as it does not allow the authors to guess which parts are being addressed in the review point. 

In conclusion, the review point is not specific enough to clearly identify which part of the paper is being addressed, and it does not provide any concrete examples or suggestions for improvement. This lack of specificity is why the review point does not meet the criteria for a score of 1 or 0."
2557,NIPS_2019_220,"Weaknesses: 1. Unclear experimental methodology. The paper states that 300W-LP is used to train the model, but later it is claimed same procedure is used as was used for baselines. Most baselines do not use 300W-LP dataset in their training. Is 300W-LP used in all experiments or just some? If it is used in all this would provide an unfair advantage to the proposed method. 2. Missing link to similar work on Continuous Conditional Random Fields [Ristovski 2013] and Continuous Conditional Neural Fields [Baltrusaitis 2014] that has a similar structure of the CRF and ability to perform exact inference. 3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer) ","3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer)",1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a lack of mention of Gaussian NLL in the paper, and suggests a minor change in terminology. The review point does not provide any insight into what the authors could do to improve their work or address the issues raised. It also fails to suggest any implicit actions that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the expected tone of a review. It is presented in a manner that is accusatory and confrontational, which is not conducive to a constructive review process. The use of phrases such as ""seems to come out of nowhere"" and ""is not mentioned anywhere in the paper, besides the ablation study?"" are indicative of a negative and critical tone. This approach does not foster a positive dialogue or provide constructive feedback. The reviewer's suggestion to replace ""difference mean"" with ""expected difference"" is presented in a way that could be perceived as condescending, rather than helpful. The overall tone of the review point is not in line with the expected politeness and constructiveness. Therefore, the review point does not meet the criteria for a helpful, encouraging, or neutral tone. It is more aligned with a negative and critical tone, which is not appropriate for a review. So the score is -1.",0,"The review point in question does not make any claims, nor does it require verification. It merely suggests a change in terminology, which is a factual statement and does not require any justification or reference. The review point is focused on the clarity of the paper, which is not a matter of verifiability. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score of the paper. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The mention of ""Gaussian NLL"" is vague and does not provide any context or reference to a specific section or idea in the paper. Additionally, the suggestion to replace ""difference mean"" with ""expected difference"" is not directly related to the paper's content and does not contribute to a clear understanding of the paper's focus. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are being addressed, leading to uncertainty and a lack of confidence in their understanding of the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
5336,NIPS_2020_814,"- Only applies to overdetermined least-squares (LS) problems, which is a bit uninteresting at this point. We can solve these quite well already. - Practical performance improvement by using orthogonal transforms is slight (only apparent as sketch becomes larger, where it is less useful; and in the regimes studied by numerical simulations, the decay factor rho was very small for all methods, so convergence was super fast for everything and the differences were not that great) - Everything is in the asymptotic regime, which is not obviously useful, and not very standard for sketching results - The language of the paper oversells things, since it keeps referring to things as ""optimal"" without qualifying that everything is asymptotic.","- Practical performance improvement by using orthogonal transforms is slight (only apparent as sketch becomes larger, where it is less useful; and in the regimes studied by numerical simulations, the decay factor rho was very small for all methods, so convergence was super fast for everything and the differences were not that great) - Everything is in the asymptotic regime, which is not obviously useful, and not very standard for sketching results - The language of the paper oversells things, since it keeps referring to things as ""optimal"" without qualifying that everything is asymptotic.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the paper without suggesting any improvements or further research. The review point also lacks any implicit actionable comments that could be inferred from the text. It does not offer any insights into how the authors could improve their work or what specific aspects of the paper could be addressed. The review point is more focused on the shortcomings of the paper rather than providing constructive feedback or suggestions for future work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not particularly polite. It uses a somewhat critical tone, which is evident in phrases such as ""slight,"" ""not very useful,"" and ""oversells things."" These phrases can be perceived as dismissive and lacking in constructive feedback. The reviewer also fails to provide any positive reinforcement or suggestions for improvement, which is a key aspect of a polite review. Instead, the reviewer focuses on the shortcomings of the paper without offering any constructive criticism or guidance. This approach can be seen as disrespectful and not conducive to a positive review experience. Therefore, based on the score rubric, the tone of the review point is not helpful or encouraging, and it does not meet the criteria for a polite review. So the overall score is -1.",0,"The review point provided does not contain any claims that need to be verified. It is a critique of the paper's methodology and results, but it does not make any assertions that could be verified. The reviewer discusses the practical performance improvement, the asymptotic regime, and the language used in the paper, but these are not claims that require justification or reference to external or internal sources. The reviewer's comments are more of an analysis of the paper's content rather than a verification of specific claims. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any assertions that could be verified. So the score is 0.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which parts of the paper are being addressed. The review point mentions ""orthogonal transforms"" and ""asymptotic regime,"" but it does not explain how these concepts are relevant to the paper or how they impact the practical performance improvement. Furthermore, the review point criticizes the language of the paper for overselling things, but it does not provide any concrete examples or suggestions for improvement. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point. So the overall score is -1."
5043,NIPS_2020_443,"- Apart from ridge regression, there are some regression models like lasso regression the authors might try. - For Table 1, the notation should be aligned with its description. - Overall, I am quite curious why 2v2 accuracies for all hypotheses are just slightly better than the random chance. Why is that? Correct me if I am wrong, thanks.","- For Table 1, the notation should be aligned with its description.",1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point for Table 1 does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the notation should be aligned with its description, which is a general statement and does not offer any specific guidance or suggestions for improvement. The review point does not include any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided for Table 1 is not aligned with its description. The tone of the review point is neutral and factual, which is not in line with the politeness aspect of the score rubric. The review point does not contain any encouraging or helpful language, nor does it contain any aggressive or disrespectful language. It is simply a statement of fact, which is why it would receive a score of 0. The review point does not provide any constructive feedback or suggestions for improvement, which is a key component of a review point. It also lacks any form of politeness or encouragement, which is essential for a review point to be effective and helpful. Therefore, the review point does not meet the criteria for a score of 1 or 2, as it does not contain any elements of politeness or encouragement. 

So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that the notation in Table 1 should be aligned with its description. This statement is not a claim that requires justification or reference to external or internal sources. It is a straightforward requirement that is not open to interpretation or debate. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. The review point does not contain any claims that need to be verified, nor does it make any claims that could be verified. It is a simple statement of fact that does not require any form of validation. So the overall score is 0.",-1,"The review point provided for Table 1 is not specific enough to clearly identify which part of the paper is being addressed. The review point is vague and does not provide any concrete examples or suggestions for improvement. It lacks the specificity required to guide the authors in addressing the issues raised. The authors would likely be unsure about which parts of the paper are related to the review point, as the review point does not provide any clear direction or actionable insights. This lack of specificity makes it difficult for the authors to confidently identify and address the issues raised in the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
3366,NIPS_2020_639,"The relevance of this paper is entirely unclear, for multiple reasons: 1. The author themselves state ""This work does not present any foreseeable societal consequence."", raising the question why we should we care about this work in the first place. 2. They don't make any detectable effort towards arguing for why their work is relevant in the paper either, rendering it a purely theoretical exercise. 3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.","3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.",0.0,-1.0,-1.0,-1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It lacks any specific guidance or suggestions that would help the authors improve their work. The review point merely points out the absence of empirical evaluation and the lack of comparison with other methods, without providing any concrete recommendations or insights. It does not offer any implicit actionable comments that the authors could infer, nor does it suggest any limitations that could be addressed with specific actions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and it does not provide any value to the authors in terms of guiding them towards potential improvements or clarifications. So the overall score is -1.",-1,"The review point provided is quite blunt and lacks the necessary politeness expected in a review process. The tone is aggressive and disrespectful, which is not conducive to a constructive review. The reviewer does not attempt to provide any constructive feedback or suggestions for improvement. Instead, the reviewer simply states that the paper is not suitable for a publication at NeurIPS without providing any specific reasons or examples. This approach does not help the authors to understand what needs to be improved or how to improve their work. Furthermore, the reviewer does not acknowledge any potential value in the theoretical contributions, which is a crucial aspect of a review. The lack of politeness and constructive feedback in the review point is in stark contrast to the examples provided in the score rubric, which demonstrate a helpful and encouraging tone. Therefore, based on the score rubric, the review point is not suitable for a publication at NeurIPS due to its aggressive and disrespectful tone.",0,"The review point provided does not contain any claims or make any assertions that could be verified. It merely states that the paper lacks empirical evaluation and does not compare with other methods. The review point does not provide any logical deductions, nor does it reference any external or internal sources to support its claims. It also fails to justify why the lack of empirical evaluation and comparison with other methods is significant. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it does not contain any claims that could be verified. So the overall score is 0.",-1,"The review point provided lacks specificity and does not clearly identify which parts of the paper are being addressed. The authors are left with no idea about the paper part that is being addressed by the review point comment. The review point does not provide any concrete examples or suggestions for improvement, which makes it difficult for the authors to understand what aspects of their paper need to be revised. Furthermore, the review point does not offer any constructive feedback or guidance on how to improve the paper. Instead, it merely states that the paper is not suitable for a publication at NeurIPS without providing any specific reasons or examples. This lack of specificity and constructive feedback is why the review point does not meet the criteria for a higher score. So the overall score is -1."
5592,NIPS_2018_55,"weakness of this paper lies in the evaluation. Although it is a great thing that this paper uses more datasets than MNIST, the evaluation can be much improved. 1) The statements in the MNIST experiment such as ""While results without an CAE are quite convincing, the CAE clearly improves the pertinent positives and negatives in many cases. Regarding pertinent positives, the cyan highlighted pixels in the column with CAE (CAE CEM PP) are a superset to the cyan-highlighted pixels in column without (CEM PP). While these explanations are at the same level of confidence regarding the classifier, explanations using an AE are visually more interpretable."" are problematic. These are quite subjective statements, and some form of quantitative evaluation across subjects is required for such claims. 2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.  3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is ""With the help of domain experts"". 4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions:   I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence.   line 218: an CAE -> a CAE   line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.","4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions: I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence. line 218: an CAE -> a CAE line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments that the authors should make. It only mentions some limitations without giving any explicit or implicit actions that should be done. The review point does not suggest any specific changes or improvements that could be made to the paper. It also does not provide any guidance on how to address the issues raised in the review point. The review point is more focused on the content of the paper rather than providing actionable feedback. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is generally polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the potential of the paper and offers suggestions for improvement, such as discussing the choice of parameters and refining the experiments. However, there are instances where the tone could be perceived as slightly less polite, such as the comment about the introduction being ""a bit out of nowhere"" and the suggestion to drop one experiment. These comments, while constructive, could be phrased more delicately to maintain a consistently polite tone. Additionally, the reviewer's request for clarification on set operations in the formulation is a valid point, but it could be framed in a more polite manner. Overall, the review point is helpful and encouraging, but there is room for improvement in terms of maintaining a consistently polite tone throughout. So the score is 1.",0,"The review point provided does not contain any claims that need to be verified. It is more of a general commentary on the paper, with suggestions for improvement and minor presentation issues. The reviewer does not make any claims about the paper's content or methodology, nor do they provide any logical deductions or references to external or internal sources that would support any claims. The feedback is more focused on the presentation and structure of the paper, rather than the content itself. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it does not contain any claims that need to be verified. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the parameters c, \beta, and \gamma, which are crucial to the proposed approach. It also suggests a potential improvement to the paper by dropping one experiment and enhancing the other two. This shows a clear understanding of the paper's content and a thoughtful approach to its critique. The review point also provides specific suggestions for presentation improvements, which further demonstrates its specificity. Additionally, the review point asks for clarification on how certain set operations apply to continuous variables, indicating a clear focus on the technical aspects of the paper. This level of specificity aligns well with the score rubric's criteria for a score of 1, as it allows the authors to confidently identify which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
811,ICLR_2021_1181,"Weaknesses
1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?","3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019. However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments that the authors should make. It merely states the lack of detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation. The review point does not suggest any specific actions or experiments that could be done to improve the paper or address the limitations mentioned. It also does not provide any implicit actionable comments that the authors can infer after reading the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or helpful. The feedback is focused on the lack of detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation, which is a factual critique of the paper. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a key aspect of maintaining a neutral tone. The feedback is also constructive, as it points out a specific area where the paper could be improved. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point makes a claim about the novelty of the paper's approach to discriminative learning and domain adaptation. However, it fails to provide a justification or reference to support this claim. The review point merely lists existing literature without comparing or contrasting the paper's method with these existing methods. This lack of verification and comparison with existing literature is a significant shortcoming in the review point. The review point does not meet the criteria for verifiability as it does not provide logical deductions or references to support its claims. Therefore, the review point is not verified and does not meet the standards set for performance.",0,"The review point provided is quite specific, focusing on the motivation of learning a large margin between different classes and its relation to discriminative learning. It also references specific literature, such as Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017, and Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed.

However, the review point falls short in providing a detailed discussion and comparison with existing discriminative feature learning methods for domain adaptation. This lack of detail and comparison is a critical aspect of the review point that is not addressed, which is why it does not fully meet the criteria for a score of 1.

The authors of the paper would likely be confident about which part of the paper is being addressed by the review point, as it directly references the motivation of learning a large margin between different classes and its relation to discriminative learning. However, they would not be fully confident due to the lack of detailed discussions and comparisons with existing methods.

In conclusion, the review point is specific and addresses a key aspect of the paper, but it lacks the depth and comparative analysis that would make it fully clear and actionable for the authors. Therefore, the score is 0."
71,ICLR_2022_1155,"Weaknesses: 1. This work is motivated by “the common misconception that adversarial examples are high-frequency noise”; however, such an understanding has already been questioned in the literature. For example, in (Tsuzuku & Sato, 2019), it shows “adversarial perturbations do not necessarily lie in high-frequency spots” by experiments; in (Yin et al. 2020), it says “adversarial examples are not strictly a high frequency phenomenon”; particularly in Bernhard et al. (2021), it questions “some preconceived hypothesis” that “adversarial perturbations as a pure HSF phenomenon with data-agnostic spatial frequency characteristics”. Therefore, the motivation should be better justified. The contribution is more like additional evidence of the ongoing debate, but rather a new frequency-based understanding of the “common misconception”. This should not be overclaimed. 2. This work reveals that adversarial examples are dataset dependent; however, this is a commonly accepted point in the community, and therefore the idea and the conclusion do not seem new. In addition, the demonstration of this point with only CIFAR-10 and ImageNet-derived datasets does not seem sufficient. More datasets should be considered including the simplest one (MNIST), and others such as CIFAR-100, SVHN, Fashion-MNIST. 3. Only PGD attacks are investigated – it is unclear if the observations also occur with other attacks, e.g., C&W, auto-attack. 4. It claims the “observations overlap with insights from the concurrent work by Bernhard et al. (2021)”; however Bernhard et al. (2021) appeared on arXiv in April 2021. The authors may want to highlight the differences and justify this point. According to my understanding, the main messages of two papers are the same. This will dwarf the contribution of this paper. 5. When measuring the importance of different frequency components and attacking low-frequency components, how to guarantee the imperceptibility? Should not the threat models be re-defined? How to make the trade-off between imperceptibility and attack successfulness? 6. The measure of average noise gradient over the entire dataset should be further justified. Why is not the average noise gradient of a specific class, given the intuition that the frequency properties of images from different classes may be so different that the averaging may be misleading. 7. When designing adversarial training with frequency-based perturbation, how to find the frequency subspace? How is PGD attack restricted to the same frequency bands?","5. When measuring the importance of different frequency components and attacking low-frequency components, how to guarantee the imperceptibility? Should not the threat models be re-defined? How to make the trade-off between imperceptibility and attack successfulness?",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It lacks specificity and does not suggest any changes or improvements that could be made to the work. The review point merely points out the limitations of the work without offering any solutions or alternatives. It does not provide any actionable comments that can be implicitly inferred, nor does it suggest any direct actions that should be taken. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not give any hint, or direct actionable comments. After reading the review point, the authors still do not know or can not deduce what needs to be done. The review point is vague and does not provide any clear direction for the authors to follow. It fails to suggest any specific improvements or changes that could be made to the work. 

In conclusion, the review point does not provide any actionable steps or comments, and it does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, which is in line with the neutral tone expected in a review. The reviewer also refrains from making any personal judgments or criticisms, which further contributes to the neutrality of the tone. The reviewer's focus is on the content of the paper and the technical aspects, rather than on the authors or the paper's presentation. This approach is consistent with the expected tone for a review, as it allows the reviewer to provide a fair and unbiased assessment of the paper. Therefore, the review point is neutral and factual, which is why it receives a score of 0 according to the rubric.",0,"The review point in question does not contain any claims, nor does it make any assertions that require verification. It is a series of questions and considerations that are more in the nature of a discussion rather than a claim-based review. The review point does not make any claims about the importance of different frequency components, the imperceptibility of attacks, or the trade-off between imperceptibility and attack success. It also does not suggest any re-definition of threat models. Instead, it poses questions and considerations that are open-ended and require further exploration and verification.

The review point does not contain any logical deductions or references to external or internal sources that could be used to verify any claims. It is a review point that is more focused on the aspects of the review rather than making claims that need to be verified.

Therefore, based on the score rubric, the review point does not contain any claims that need to be verified, nor does it make any assertions that could be verified. It is a review point that is more focused on the aspects of the review rather than making claims that need to be verified. Hence, the score is 0.",1,"The review point provided is quite specific and directly addresses the importance of different frequency components and the trade-off between imperceptibility and attack success. It clearly identifies the need to re-define threat models and the importance of ensuring the imperceptibility of the system. The authors can confidently identify which parts of the paper are related to the review point, as it directly addresses the key aspects of the paper. The review point is well-structured and provides a clear direction for the authors to follow in order to improve their paper. It also encourages the authors to consider the implications of their work on the broader field of cybersecurity. Therefore, based on the score rubric, the review point is highly specific and directly addresses the key aspects of the paper, making it clear to the authors which parts of the paper are being addressed. So the score is 1."
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks:",-1.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the gap between the lower bound and the upper-bound, and suggests that the existing bound with the independence number may be better. However, it does not provide any specific guidance on how to address this gap or improve the existing bound. Furthermore, the review point mentions the results on the adaptive adversary and the counterfactual feedback but does not offer any actionable comments or suggestions on how to improve these results. The review point also lacks any implicit actionable comments that could be inferred by the authors. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",NO_LABEL,Failed to generate feedback,0,"The review point provided does not contain any claims that need to be verified. It merely points out a gap between the lower bound and the upper-bound, and suggests that the existing bound with the independence number may be better. It also mentions that the results on the adaptive adversary and the counterfactual feedback could be better. However, these are not claims that need to be verified, but rather observations that could be further explored. The review point does not make any claims that could be verified or disproved, and it does not provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or disproved. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The reviewer mentions a gap between the lower bound and the upper-bound, but does not specify which part of the paper this gap is related to. The reviewer also mentions the independence number, adaptive adversary, and counterfactual feedback, but again, does not specify which parts of the paper these concepts are related to. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The authors would likely be unsure about which parts of the paper are being discussed, which is not ideal for a review process. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the overall score is -1."
597,ICLR_2021_2678,"Weakness –
W0 – There is no comparison to existing metrics, e.g., IS and FID to clearly show the advantages as the authors claimed in the paper, e.g., fewer samples and low variance. I think people now are using the combination of IS and FID in their experiments to measure quality and diversity. How are these two new metrics better than the existing combination? There is a bit disconnected between the concepts of the three proposed metrics, and I think it's still fair to compare the proposed metrics (visual quality and mode diversity) with the combination of IS and FID.
W1 – The visual quality and mode diversity metrics use the face detection/verification frameworks which are quite specific to face datasets that have ground-truth labels. Can they apply to other domains rather than faces? If so, does it cause any bias?
W3 – What is X − 1
in Eq. 6. Is it the inversion of matrix determination or number the division of the number of samples?
W4 – Mistakes in Eqs. 3 and 10 and Table 3, should be gradient of ∇ x ^
instead of ∇ η .
W5 – It looks like the paper is a bit rushed in this submission, there are sufficient missing details of implementations, e.g., hyper-parameters, batch size, detail of architecture, … are not provided. These factors may also have effects on the training of GAN. It would be interesting to have studies on these in future work.
W6 – The paper's experiments are limited to one low-resolution dataset with standard GAN architectures. It's important to evaluate various GAN architectures and datasets for GAN assessment papers. It would be interesting to conduct the experiments also on state-of-the-art GAN models, e.g., SN-GAN, BigGAN, ProGAN, and StyleGAN, and with high-resolution datasets as well.
W7 - Some metrics are interesting and likely to be valuable in the future, but can be more polished on experiments and comparison. It would be great if the current measures can be developed to extend for other datasets, e.g., CIFAR, ImageNet.
W8 – The paper has derived some mathematics of turning points, but what is its meaning and how is it useful to derive this equation?","3 and 10 and Table 3, should be gradient of ∇ x ^ instead of ∇ η . W5 – It looks like the paper is a bit rushed in this submission, there are sufficient missing details of implementations, e.g., hyper-parameters, batch size, detail of architecture, … are not provided. These factors may also have effects on the training of GAN. It would be interesting to have studies on these in future work. W6 – The paper's experiments are limited to one low-resolution dataset with standard GAN architectures. It's important to evaluate various GAN architectures and datasets for GAN assessment papers. It would be interesting to conduct the experiments also on state-of-the-art GAN models, e.g., SN-GAN, BigGAN, ProGAN, and StyleGAN, and with high-resolution datasets as well. W7 - Some metrics are interesting and likely to be valuable in the future, but can be more polished on experiments and comparison. It would be great if the current measures can be developed to extend for other datasets, e.g., CIFAR, ImageNet. W8 – The paper has derived some mathematics of turning points, but what is its meaning and how is it useful to derive this equation?",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the paper without suggesting any specific improvements or further research directions. The review point does not provide any implicit actionable comments that the authors can infer, which is a key aspect of the score rubric. It also lacks any explicit or implicit actions that should be done, which is a critical requirement for a score of 1. The review point does not give any hint or direct actionable comments, leaving the authors without a clear direction for their future work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement, such as providing more details on implementations and considering various GAN architectures and datasets. The tone is encouraging and helpful, which is evident in the reviewer's use of phrases like ""it would be interesting"" and ""it would be great if."" These phrases indicate a positive and constructive approach to the review. The reviewer also asks questions about the paper's content, which shows a genuine interest in understanding the paper better. This approach is likely to be well-received by the authors, as it provides clear guidance on how to improve their work. The reviewer's tone is consistently polite and constructive throughout the review point, which is why it receives a score of 1.",-1,"The review point provided contains several claims that are not verified. For instance, the claim about the gradient of ∇ x ^ instead of ∇ η is not supported by any logical deductions or references. Similarly, the claim about the rushed nature of the paper and the lack of details on implementations, such as hyper-parameters and batch size, is not substantiated with any evidence or justification. The review point also makes a claim about the limited scope of the paper's experiments, but it does not provide any external or internal sources to verify this claim. Furthermore, the review point mentions the need for more polished metrics and the derivation of some mathematics, but it fails to explain why these are important or how they can be verified. Overall, the review point contains several unverified claims, which does not meet the criteria for verifiability as outlined in the score rubric. Therefore, the review point does not meet the standards for performance in terms of verifiability.",0,"The review point provided is quite specific and directly addresses certain aspects of the paper. It points out the lack of detail in the implementation of the GAN model, such as hyper-parameters and batch size, which are crucial for the training of GANs. The review also highlights the need for a broader range of datasets and GAN architectures to be evaluated, which is a valid point considering the current state of GAN research. Furthermore, the review suggests that the metrics used in the paper could be more polished and extended to other datasets. Lastly, it questions the relevance and usefulness of the mathematical equation derived in the paper.

The authors of the paper can identify which parts of the paper are being addressed by the review point. They can be confident about the specific aspects of the paper that are being critiqued, such as the lack of detail in the implementation and the need for a broader range of datasets and GAN architectures. However, they may not be fully confident about the specificity of the review point, especially regarding the mathematical equation and its relevance.

In conclusion, the review point is specific and directly addresses certain aspects of the paper, which allows the authors to identify the parts being addressed. However, the authors may not be fully confident about the specificity of the review point, especially regarding the mathematical equation and its relevance. Therefore, the score is 0."
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments. It merely states the existence of a domination number in feedback graphs without switching costs, and it does not suggest any specific actions or experiments that could be done. The review point also does not provide any implicit actionable comments that could be inferred from the review point. It only mentions the existence of the domination number and the lack of citation of existing results, without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The reviewer simply states the facts about the domination number and its absence in the analysis, without any personal opinion or judgment. The reviewer also refrains from making any negative or positive comments about the quality of the paper or the authors' work. The tone is neutral and the reviewer sticks to the facts, which is in line with the score rubric's description of a score of 0. Therefore, the review point is not aggressive, disrespectful, or rude, nor is it overly polite or encouraging. It is simply neutral and factual.",0,"The review point in question does not contain any claims, nor does it require verification. It is a straightforward question about the introduction of the domination number in feedback graphs without switching costs. The response to this question would be factual and would not require any justification or reference to external or internal sources. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. The response to the review point would be a simple statement about the presence or absence of the domination number in the feedback graphs without switching costs. This is why the score is 0.",1,"The review point provided is quite specific and directly addresses the introduction of the domination number in the context of feedback graphs without switching costs. It asks for a citation of existing results if the domination number has been introduced, and if not, it requests a statement about the kind of results the analysis would provide without using mini-batches. This specificity aligns with the score rubric's requirement for the review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the part of the paper that is being addressed by the review point, which is the introduction of the domination number and its implications for feedback graphs. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
2228,ACL_2017_130_review.json,"Weaknesses: The paper suffers from several drawbacks 1. The paper is hard to read due to incorrect usage of English. The current manuscript would benefit a lot from a review grammar and spellings. 
2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100? 
3. What is the motivation behind enriching the graph? Why not represent each word by a node in the graph and connect them by the similarity between their vectors, irrespective of co-occurrence? 
4. The datsets are from a biomedical domain. No domain specific tools have been leveraged. 
5. Since dataset class distribution is unclear, it is unclear to determine if accuracy is a good measure for evaluation. In either case, since it is a binary classification task, F1 would have been a desirable metric. 
6. Results are reported unto 4 decimal places on very small datasets (43 transcripts) without statistical tests over increments. Therefore, it is unclear if the gains are significant. ","2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100?",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the shortcomings of the dataset descriptions and the inconsistency in the number of participants mentioned. The review point does not suggest any improvements or changes that could be made to the dataset or the study design. It also fails to provide any implicit actionable comments that could be inferred from the review point. The authors are left without any clear direction on what needs to be done to improve the study. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not polite and comes across as aggressive and disrespectful. The reviewer's tone is critical and dismissive, particularly when discussing the dataset descriptions and the inconsistency in the number of participants. The reviewer's comments about the age of the patients being irrelevant and the excessive text consumed in describing the datasets are also unhelpful and could be perceived as rude. The reviewer's language is confrontational, and the tone is not constructive or encouraging. The reviewer's approach to the review point does not align with the score rubric's criteria for politeness. Therefore, the review point is not helpful or encouraging, and it does not meet the standards of politeness. So the score is -1.",-1,"The review point in question makes several claims, but it fails to provide any form of verification or justification for these claims. The reviewer points out that the dataset descriptions are not at a transcript level, which is a valid concern, but does not provide any evidence or logical reasoning to support this claim. Additionally, the reviewer questions the accuracy of the number of participants and the total number of males and females, but again, there is no verification or justification provided. The reviewer also criticizes the inclusion of patient age in the classification task, but does not offer any logical explanation or reference to support this claim. The reviewer's concerns about the classification task and the dataset descriptions are not substantiated with any form of evidence or logical reasoning, which is a key aspect of the score rubric. Therefore, the review point does not meet the criteria for verifiability, as it contains claims that are not verified. So the overall score is -1.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer's concerns about the classification task and the dataset descriptions are not clearly linked to specific sections of the paper. The reviewer's confusion about the numbers in the paper is not directly related to any particular part of the paper. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The authors are left with uncertainty about the paper part that is being addressed by the review point comment. Therefore, the score is -1."
5538,NIPS_2020_1623,"1. Lacking insight on setting the hyper-params: Table 2 (b) shows E_s = 60 will have the best performance but in the experiments in Section 2.4, E_s is set to 70. Is there any insight or explanation on the reason why to do such changes? 2. Not enough model structures in the experiments to support the conclusion: As mentioned by one of the baselines [24], previous works utilize a various set of different architectures, which hinders a fair comparison. And this paper only uses ResNet34@CIFAR10/100, and ResNet50@ImageNet. Although this paper reproduces some baselines in the setting above, it would be better to provide more comparison on other model structures and use the reported numbers in the baselines.","2. Not enough model structures in the experiments to support the conclusion: As mentioned by one of the baselines [24], previous works utilize a various set of different architectures, which hinders a fair comparison. And this paper only uses ResNet34@CIFAR10/100, and ResNet50@ImageNet. Although this paper reproduces some baselines in the setting above, it would be better to provide more comparison on other model structures and use the reported numbers in the baselines.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of model structures in the experiments, which is a limitation of the paper. However, it does not suggest any specific actions that could be taken to address this limitation. The review point also does not provide any implicit actionable comments that the authors can infer. It simply states the problem without offering any solutions or further steps to be taken. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it offer implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or helpful. The reviewer simply states the facts about the lack of model structures in the experiments, which is a neutral observation. The reviewer does not offer any constructive criticism or suggestions for improvement, which would have been helpful and encouraging. Instead, the reviewer merely points out the lack of comparison with other model structures, which is a factual observation. Therefore, the tone of the review point is neutral, factual, or passive, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question makes a claim about the lack of model structures in the experiments, which is a valid point. However, it fails to provide any verification or justification for this claim. The reviewer simply states that the paper only uses ResNet34 and ResNet50, and suggests that more comparison on other model structures would be beneficial. This is a clear example of a claim that is not verified, as it lacks any logical deductions or references to external or internal sources that could support the claim.

The review point does not provide any logical explanations or references to external or internal sources that could verify the claim. It merely states that the paper's use of only two model structures is insufficient for a fair comparison, without offering any evidence or reasoning to support this claim.

In contrast, a review point that contains a claim and verifies it would provide logical deductions and references to external or internal sources that justify the claim. For example, a review point might point out that the statistical analysis in the paper is incorrect because the p-values reported do not match the standard thresholds for significance, and then cite the guidelines in 'Statistics for Biologists' to support this claim.

Therefore, based on the score rubric, the review point in question does not meet the criteria for verifiability. It contains a claim that is not verified, and it misses a justification, explanation, or reference that could support the claim. As a result, the review point does not fulfill the requirements for a score of 1, which is the highest score on the rubric. 

So the overall score is -1.",1,"The review point provided is quite specific, focusing on the lack of model structures in the experiments. It directly addresses the paper's reliance on only two model structures, ResNet34 and ResNet50, and suggests that a broader range of architectures should have been considered for a fair comparison. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the part of the paper that is being addressed by the review point, which is the model structures used in the experiments. The review point is clear and concise, leaving no room for ambiguity. Therefore, based on the score rubric, the review point is well-crafted and meets the criteria for a score of 1."
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.","4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the paper without suggesting any specific actions that the authors could take to improve their work. The review point does not provide any implicit actionable comments that the authors can infer, nor does it give any explicit or implicit actions that should be done. It simply states the shortcomings of the paper without offering any constructive feedback or guidance. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer's comments are focused on the content of the paper, and they do not express any personal opinions or judgments about the quality of the work. The reviewer's questions and observations are presented in a straightforward manner, which is appropriate for a review process. The reviewer's comments are also constructive, as they provide suggestions for improvement without being overly critical or dismissive. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely states that the number of steps in the adversarial attack is limited, which is a factual statement and does not require any justification or reference. The review point does not make any claims about the quality of the work or the validity of the results, nor does it provide any logical deductions or references to external or internal sources. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains a claim nor verifies it, and thus, it does not fulfill the requirements for a score of 1. The review point is also not relevant for verifiability, as it does not discuss any specific claims or results that could be verified. Therefore, the review point is not suitable for a score of 1, and it does not meet the criteria for a score of 0 either. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the number of steps in the adversarial attack. It directly addresses the paper's methodology and the robustness of the model, which is a critical aspect of the paper. The authors can easily identify which parts of the paper are related to the review point, as it specifically mentions the number of steps in the attack. This level of specificity allows the authors to be confident about which part of the paper is being addressed by the review point. The review point is also well-structured, providing a clear and concise critique that directly relates to the paper's content. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
2040,ARR_2022_143_review,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","7. Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not.",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states the BLEU score as ""0.18"" without any context or explanation as to why this score is significant or what it implies for the paper's performance. The review point does not suggest any improvements or changes that could be made to the paper, nor does it provide any insights into how the score could be interpreted or used to inform future work. As such, the review point does not meet the criteria for actionability as outlined in the score rubric. It fails to give any hint or direct actionable comments that the authors could deduce or act upon. Therefore, based on the score rubric, the review point is not actionable and does not provide any value to the authors in terms of guiding their work or understanding the implications of their results. So the overall score is -1.",0,"The review point in question is neutral and factual, which aligns with a score of 0 on the politeness scale. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or praise. The tone is purely informative, with the reviewer simply stating the facts about the paper's content and quality. The reviewer does not make any subjective judgments or personal opinions about the paper, which is a key characteristic of a neutral tone. The reviewer's language is straightforward and devoid of any emotional undertones, which further supports the score of 0. The reviewer's comments are focused on the paper's content and do not include any personal attacks or disrespectful remarks. Therefore, based on the score rubric, the review point is neutral and factual, and does not fit well with the other two scores.",0,"The review point in question does not contain any claims or make any assertions. It merely states that the reviewer does not understand whether the result of ""0.18 BLEU"" is an ""impressive result"" or not. This statement does not require any form of verification, as it does not make any claims that could be verified or refuted. The reviewer's confusion is a personal opinion and does not contribute to the scientific discourse or the evaluation of the paper. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains a claim nor does it provide any form of justification or reference to support or refute the claim. Hence, the score is 0.",0,"The review point in question is ""0.18 BLEU,"" which is a specific metric used in natural language processing to evaluate the quality of machine-generated translations. However, the review point does not provide any context or explanation as to why this metric is significant or how it relates to the paper's content. This lack of specificity makes it difficult for the authors to understand the relevance of the review point to their work. The authors may be left guessing which parts of the paper the review point is addressing, which is not ideal for a constructive review process. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors are not fully confident about the paper part that is being addressed by the review point comment. So the score is 0."
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does a good job of explaining the proposed idea, but it falls short in providing actionable steps or experiments that can be done. The review point mentions the lack of description of how the idea was evaluated and what was the outcome, which is a critical aspect of the review process. However, it does not offer any direct or explicit actionable comments that the authors should make. The review point also lacks any implicit actionable comments that the authors can infer after reading the review point. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionable content is why the review point does not meet the criteria for a higher score. So the overall score is -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement, such as providing a simpler expression for the bounds and strengthening the introduction with a clearer context. These comments are delivered in a respectful and encouraging tone, which is helpful for the authors. The reviewer also acknowledges the potential of the proposed method, which is a positive aspect.

In contrast, the review point that would receive a score of -1 is characterized by a rude and disrespectful tone. The reviewer uses harsh language, such as ""terrible"" and ""poor quality,"" which is not constructive. The reviewer also makes negative judgments about the paper's suitability for publication, which is not helpful for the authors.

The review point that would receive a score of 0 is neutral and factual, which is the default score if the review point does not fit well with the other two scores. The reviewer asks questions and provides observations without being overly critical or overly positive.

In conclusion, the review point provided is polite and constructive, which is in line with the score rubric's criteria for a score of 1. The reviewer offers helpful suggestions and maintains a respectful tone throughout the review.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. The language issues mentioned are minor and do not affect the verifiability of the claims. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that are not verified. The review point is factual and does not require any justification or explanation. So the overall score is 0.",-1,"The review point provided does not clearly specify which part of the paper is being addressed. It mentions that the abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. However, it does not provide any concrete examples or specific details that would help the authors identify which parts of the paper are related to the review point. The language used is also somewhat vague, which could lead to confusion among the authors.

The review point does not make it obvious which part of the paper is being addressed, and it does not provide enough information for the authors to confidently identify the relevant sections of the paper. The lack of specificity in the review point makes it difficult for the authors to understand what aspects of their work are being critiqued.

In conclusion, the review point does not meet the criteria for specificity, as it does not provide clear and specific feedback on the paper. The authors would likely be unsure about which parts of the paper are being addressed by the review point. Therefore, the score is -1."
2040,ARR_2022_143_review,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","2. Figure 1, what are the labels for horizontal and vertical axis?",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the paper without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point does not suggest any specific changes or improvements that could be made to the paper. It also does not provide any insight into how the authors could address the limitations mentioned. The lack of actionable comments in the review point means that the authors are left without any clear direction on how to improve their work. 

In conclusion, the review point fails to provide any actionable steps or comments, which is a critical aspect of the score rubric. It does not help the authors to understand what needs to be done to improve their paper. Therefore, the review point is not helpful and does not meet the criteria for actionability. 

So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of politeness or rudeness, but rather presents a straightforward analysis of the paper. The reviewer's tone is neutral, and the feedback is delivered in a factual manner without any aggressive or disrespectful language. The reviewer's comments are focused on the content of the paper, and there is no indication of any personal bias or emotional tone. The reviewer's feedback is constructive, providing suggestions for improvement, but it does not go beyond the facts presented in the paper. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It is a straightforward question about the labels for the horizontal and vertical axis in a figure. The response to this question is factual and does not require any form of verification. It is clear that the review point does not make any claims that could be verified or not, and it does not contain any elements that could be considered as a claim. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, 0, or -1. It is neither a claim nor does it require verification. The response is simply a statement of fact, which is not relevant to the concept of verifiability. So the overall score is 0.",1,"The review point in question is quite specific, as it directly addresses the labels for the horizontal and vertical axis in Figure 1. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed, which is the core of the review point. The authors can be confident that the review point is focused on the labels of the axes in the figure, and they can also be sure that this is a critical aspect of the paper that needs to be addressed.

The review point does not leave any ambiguity about the paper part that is being addressed. It is clear that the review point is focused on the labels of the axes in the figure, and this is a specific aspect of the paper that needs to be addressed. The authors can be confident that the review point is focused on the labels of the axes in the figure, and they can also be sure that this is a critical aspect of the paper that needs to be addressed.

The review point is not only specific but also directly related to the content of the paper. It is not a general comment about the paper, but rather a targeted critique of the labels used in the figure. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point.

In conclusion, the review point is highly specific and directly addresses a critical aspect of the paper, which is the labels for the horizontal and vertical axis in the figure. This specificity allows the authors to be confident about which part of the paper is being addressed by the review point, and it is clear that the review point is focused on this specific aspect of the paper. Therefore, the review point meets the criteria for a score of 1 in the score rubric."
4364,NIPS_2020_285,"- In Section 2, it is assumed that the state and action spaces are finite. Is this assumption really necessary? It might be quite limiting since policy gradient methods are typically employed when dealing with continuous state-action spaces. Moreover, the transition model is defined as deterministic. Is this assumption necessary? - Proposition 6: I am a little confused about the notation. Equation (13) employs the improvement operator for the value-based case, but the remark in the subsequent line is stated for the improvement operator for the trajectory-based case. - Proposition 5: This is more curiosity than an issue. Are there some sufficient conditions to enforce that Var(R) > 0 along the whole learning process? It seems to me that as we get close to the optimum we are going to prefer less stochastic policies, thus we slow down convergence. Do you think we can still converge asymptotically if deterministic policies are allowed? Anyway, in policy search, we could even limit to stochastic policies. Maybe in such a case, we can have a non-zero guaranteed improvement and, consequently, converge in a finite number of iterations. I think the paper would greatly benefit from a discussion on these points. - Proposition 2: This is also a curiosity. The optimal policy, in the considered policy space, is a fixed point of the operator. There can be other fixed points? If so, do the authors think that is possible to characterize the space of fixed-points? Are there some conditions under which the fixed point is unique? ***Minor*** - lines 50 and 62: s_{t_1} -> s_{t+1} - Equation (3) goes beyond margins - line 64: reporting the formal definition of d^\pi might help - Equation (24) there should be a \propto instead of = - Proposition 5: there should be a statement, not just a formula - The notation of Proposition 9 does not match that used in the proof (z vs f(R)) - Figure 2: not very readable in grayscale, I suggest using different linestyles or markers","- Proposition 2: This is also a curiosity. The optimal policy, in the considered policy space, is a fixed point of the operator. There can be other fixed points? If so, do the authors think that is possible to characterize the space of fixed-points? Are there some conditions under which the fixed point is unique? ***Minor*** - lines 50 and 62: s_{t_1} -> s_{t+1} - Equation (3) goes beyond margins - line 64: reporting the formal definition of d^\pi might help - Equation (24) there should be a \propto instead of = - Proposition 5: there should be a statement, not just a formula - The notation of Proposition 9 does not match that used in the proof (z vs f(R)) - Figure 2: not very readable in grayscale, I suggest using different linestyles or markers",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out potential issues or areas for improvement without suggesting any specific actions to be taken. For instance, the review point mentions that the optimal policy might not be unique and asks if the authors think it is possible to characterize the space of fixed-points. However, it does not provide any guidance on how to approach this question or what steps to take to investigate this further. Similarly, the review point points out that the notation of Proposition 9 does not match that used in the proof, but it does not suggest any specific changes to be made. The review point also mentions that the formal definition of d^\pi is not reported, but it does not provide any insight into why this is important or how it could be incorporated into the paper. Overall, the review point lacks actionable comments and does not help the authors to understand what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for providing direct and explicit actionable comments.",1,"The review point provided is a mix of constructive criticism and neutral observation. The reviewer has pointed out several areas where the paper could be improved, such as the notation in Proposition 9 and the readability of the figure. However, the tone of the review point is not overly critical or aggressive. The reviewer has also suggested improvements, such as providing a simpler expression for the bounds and using different linestyles or markers for the figure. These suggestions are helpful and encourage the authors to refine their work. The reviewer's tone is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer's feedback is focused on the content of the paper and offers specific suggestions for improvement, which is a positive aspect of the review. So the overall score is 1.",-1,"The review point provided contains several claims that are not verified. For instance, the claim about the existence of other fixed points in the policy space is not supported by any logical deductions or references. Similarly, the claim about the readability of the figure in grayscale is not substantiated with any justification or explanation. The review point also includes a claim about the notation in Proposition 9 not matching that used in the proof, but it does not provide any evidence or reasoning to support this claim. Furthermore, the review point suggests that the formal definition of d^\pi should be reported, but it does not explain why this is necessary or how it would contribute to the understanding of the paper. The review point also points out that there should be a statement, not just a formula, in Proposition 5, but it does not clarify what this statement should be or why it is important. Overall, the review point contains several claims that are not verified, and it lacks the necessary justification, explanation, or references to support these claims. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability.",1,"The review point provided is quite specific, addressing several key points in the paper. It identifies specific sections of the paper, such as the discussion of fixed points in the policy space, and provides clear and direct feedback on these sections. The review point also suggests improvements to the paper, such as the use of different linestyles or markers for the figure, and the need for a more detailed explanation of certain concepts. This level of specificity allows the authors to understand exactly which parts of the paper are being addressed by the review point. The feedback is also constructive, offering suggestions for how to improve the paper. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it clear to the authors which parts of the paper are being discussed. So the overall score is 1."
1822,ARR_2022_288_review,"-	While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. 
-	Although ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. 
-	The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case. The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. 
The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. 
The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. 
Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ","- While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the model without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that the authors can infer, nor does it give any explicit or implicit actions that should be done. It simply states the shortcomings of the model without offering any constructive feedback or guidance on how to address these issues. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The feedback is focused on the scope of the language model and its comparison with other models, which is a factual critique. However, the review point could have been more constructive by suggesting ways to improve the model or by providing more specific feedback on the paper's content. The review point does not contain any rude or disrespectful language, which is a positive aspect. It is also not overly aggressive or passive, which is why it fits the score of 0. The feedback is delivered in a neutral manner, which is appropriate for a review point. So the overall score is 0.",-1,"The review point provided makes a claim about the narrow scope of the language model in question. However, it fails to verify this claim with logical deductions or references to external or internal sources. The review point merely states that other similar models have a wider coverage, but it does not provide any evidence or reasoning to support this claim. Furthermore, the review point does not discuss the importance of the language model in addressing a wide variety of problems in the domain, which is a key aspect of the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It lacks the necessary justification, explanation, or reference to support the claim made. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the scope of the language model in question. It clearly identifies the limitations of the model and suggests that the authors should expand on the importance of the model in addressing a wide variety of problems in the domain. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the parts of the paper that are being addressed by the review point, as it directly points out the narrow scope of the model and the need for a wider coverage. The review point also provides a clear direction for the authors to improve their paper, which is a crucial aspect of a review point. Therefore, based on the score rubric, the review point is well-crafted and effectively addresses the specificity of the paper."
1822,ARR_2022_288_review,"-	While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. 
-	Although ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. 
-	The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case. The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. 
The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. 
The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. 
Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ","- The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case. The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases?",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provides a clear and detailed analysis of the paper, highlighting the strengths and weaknesses of the proposed language model. It suggests that the authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The review point also raises questions about the importance of certain features of BERT in the context of the Cont version of ConfliBERT. However, it falls short in providing direct and explicit actionable steps or experiments that the authors should undertake. The review point does not suggest any specific experiments or methods that could be used to address the questions raised. It also does not provide any implicit actionable comments that the authors can infer from the review point. Therefore, based on the score rubric, the review point does not give any hint, or direct actionable comments, and the authors still do not know or can not deduce what needs to be done. So the overall score is -1.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement and acknowledges the strengths of the paper. The tone is encouraging and the feedback is delivered in a respectful manner. The reviewer also asks relevant questions that could help the authors understand the limitations of their work and how they could be addressed. This approach is helpful and promotes a positive dialogue between the reviewer and the authors. The reviewer's comments are focused on the content of the paper and do not include any personal attacks or disrespectful language. The feedback is also well-structured, with each point clearly addressing a specific aspect of the paper. This makes it easy for the authors to understand and respond to the feedback. The reviewer's tone is consistently polite and constructive throughout the review point. Therefore, based on the score rubric, the review point is of high quality and meets the criteria for a score of 1.",-1,"The review point provided contains several claims that are not verified. The reviewer raises questions about the conditions under which the Cont version of ConfliBERT performs better and the importance of BERT features in this case. However, the reviewer does not provide any logical deductions or references to external or internal sources to support these claims. Additionally, the reviewer points out the need for a domain-specific language model but does not verify the claim by discussing the advantages and disadvantages of using domain-specific datasets. The reviewer also questions the percentage of vocabulary present in the general dataset but does not provide any justification or explanation for this claim. The reviewer's comments on the p-values and the comparison of datasets are also not verified, as they do not provide any logical deductions or references to support their claims. Therefore, the review point does not meet the criteria for verifiability as it contains several claims that are not verified.",1,"The review point provided is quite specific and directly addresses the paper's discussion on the performance of ConfliBERT in the context of political conflicts and violence. It identifies the lack of clarity in the paper regarding the conditions under which the Cont version of ConfliBERT performs better and the importance of BERT features in this context. The review point also raises questions about the advantages and disadvantages of using domain-specific datasets and the potential impact of the vocabulary present in the general dataset but not in the domain-specific data. This specificity in the review point allows the authors to identify exactly which parts of the paper are being addressed, and they can be confident about the areas that need further clarification or discussion. The review point is well-structured and directly engages with the paper's content, making it clear which aspects of the paper are being critiqued. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it obvious which part of the paper is being addressed. So the score is 1."
5776,NIPS_2018_600,"weakness of the non-local (NL) module [31] that the correlations across channels are less taken into account, and then formulate the compact generalized non-local (CGNL) module to remedy the issue through summarizing the previous methods of NL and bilinear pooling [14] in a unified manner. The CGNL is evaluated on thorough experiments for action and fine-grained classification tasks, exhibiting promising performance competitive to the state-of-the-arts. Positives: + The paper is well organized and easy to follow. + The generalized formulation (8,9) to unify bilinear pooling and non-local module is theoretically sound. + Good performance. Negatives: - Less discussion on the linear version of CGNL using dot product for f. - Missing fundamental comparison to the simple ResBlock. The authors nicely present the generalized formulation toward CGNL by unifying the two previous works of bilinear pooling and non-local module. Though the kernelized (non-linear) correlation function f is well theoretically motivated, the actual form of f that achieves the better empirical performance is a âlinearâ form (dot product). In this regard, the reviewer has the following concerns. - Less discussion about the linear form. If the reviewer correctly understands the CGNL formulation, the linear function f of dot product f (line 204) can greatly simplify the CGNL into Y = X * W_theta * tr[(X*W_phi)â * (X*W_g)] = X * W_theta * tr[(XâX)* W_g* W_phiâ]  = s * X * W_theta, where s = tr[(XâX) * W_g * W_phiâ]= tr[(XâX)* W] is just a scalar and W = W_g*W_phiâ. This reformulation would be beneficial from the following viewpoints. > It reduces the parameters from {W_theta, W_phi, W_g} to {W_theta, W}, which facilitates the implementation. > It is closely related to squeeze-and-excitation (SE) module [9]. The above formulation can be regarded as a bilinear extension of SE from âsqueezeâ viewpoint since it âsqueezesâ the feature map X into the bilinear form of XâX while SE simply employs an average-pooling.  Such discussions as above would help the readers to further understand the methods and to further extend the method. - Missing comparison. Based on the above discussion, one can think that the baseline for the linear CGNL is a simple ResBlock of Z = BatchNorm( X * W_z ) + X, while the linear CGNL is Z = BatchNorm( s * X * W_theta * W_z ) + X  = BatchNorm( s * X * W_tz ) + X. The only difference is the scaling factor s that is also build on X. Through batch normalization, such a scaling might be less effective (during the training) and thus by comparing these closely-related methods, the authors have to clarify its effectiveness of CGNL empirically. Due to this concern, the reviewer can not fairly evaluate the impact of the method on classification performance. [After Rebuttal] The reviewer appreciates the authorsâ efforts to perform the comparison experiments in such a short rebuttal period. The comparison with the standard ResBlock clarifies the effectiveness of the proposed method as well as helps us to further understand how it works. ",+ Good performance. Negatives:- Less discussion on the linear version of CGNL using dot product for f.,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of discussion on the linear version of CGNL using dot product for f. The authors are left without any clear direction on what needs to be done or how to improve their work. The review point does not suggest any specific changes or improvements that could be made to the paper. It also does not provide any implicit actionable comments that the authors can infer from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the good performance of the paper but also points out a lack of discussion on a specific aspect, which is a constructive critique. The tone is respectful and encouraging, which is essential for a review point. The reviewer's feedback is helpful and provides a clear direction for improvement. The reviewer's language is polite and does not contain any aggressive or disrespectful remarks. Therefore, based on the score rubric, the review point is of high quality and meets the criteria for a score of 1.",0,"The review point provided does not contain any claims that need to be verified. It merely states that there is less discussion on the linear version of CGNL using dot product for f. This is a factual statement and does not require any justification or reference to support it. The review point does not make any claims that could be verified or disproved, which is why it does not meet the criteria for a score of 1 or 0. It also does not contain any claims that are not verified, which is why it does not meet the criteria for a score of -1. Therefore, the review point is neutral and does not contribute to the score rubric.",1,"The review point provided is quite specific, focusing on the linear version of CGNL using dot product for f. This specificity allows the authors to identify which part of the paper is being addressed, which is the linear version of CGNL. The review point is clear and concise, making it easy for the authors to understand what aspect of their paper is being critiqued. The authors can be confident that the review point is directly related to their work, as it specifically mentions the linear version of CGNL and the use of dot product for f. This level of specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not allow the authors to identify which parts of the paper are being addressed. For example, a review point that simply states that the paper is hard to follow or that certain sections need to be made clearer would not provide the authors with any specific guidance on how to improve their paper. This lack of specificity would result in the authors being unsure about which parts of the paper are being addressed by the review point. 

Therefore, based on the score rubric's criteria, the review point provided is specific and directly addresses a particular aspect of the paper, which is the linear version of CGNL using dot product for f. This specificity allows the authors to be confident about which part of the paper is being addressed by the review point."
1193,ICLR_2023_1294,"weaknesses:
1: The best feature of CLIP is the generality, that is, being able to recognize any image without pre-defined/fixed classes. MUST adapt the CLIP model to a specific dataset (which is the main purpose of this paper). A simple solution is claimed by authors in the limitation section: ""There exists a simple way to address this concern: gather unlabeled image from all the domains of interest, and perform MUST to learn a single model that can generalize to multiple domains."" It would be really great to have 1-2 such experiments to verify this hypothesis.
Minor weaknesses:
2: The paper does not reach out to theoretical backup to explain why MUST works.
3: Adding results on ImageNet-Sketch will further strengthen this paper.
4: A related work [1] is worth discussing.
[1]: Test-time training with masked autoencoders, NeurIPS 2022",2: The paper does not reach out to theoretical backup to explain why MUST works.,0.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of theoretical backup in the paper, without suggesting any specific actions that the authors should take to improve the paper. The review point does not give any hint or direct actionable comments, and after reading the review point, the authors still do not know or can not deduce what needs to be done. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",-1,"The review point provided does not exhibit the politeness expected in a constructive critique. The tone is more critical and lacks the supportive language that would be helpful to the authors. The reviewer's comments are direct and somewhat harsh, which could be perceived as disrespectful. For example, the phrase ""This introduction is terrible and makes no sense"" is quite aggressive and does not provide any constructive feedback. Additionally, the statement ""No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods"" is factual but lacks the polite tone that would be more appropriate in a review. The reviewer's comments do not encourage the authors to improve their work, but rather criticize it without offering any constructive suggestions. Therefore, the review point does not meet the criteria for politeness, as it is not helpful or encouraging.",0,"The review point in question does not provide any claims that could be verified. It merely states that the paper does not provide a theoretical backup for why MUST works. This statement is not a claim that could be verified or disproved, as it does not present any specific information or assertions that could be tested or supported by evidence. The review point does not make any claims that could be verified, nor does it provide any logical deductions or references to external or internal sources that could support or refute the claim. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor verifies it, and thus, it does not fulfill the requirements for a score of 1. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer a clear direction for the authors to address. It lacks specificity and does not provide a clear indication of which part of the paper is being addressed. The authors are left with a sense of uncertainty about what exactly needs to be improved or clarified. The review point does not guide the authors to identify the specific sections of the paper that need attention. It also fails to provide a clear hypothesis or question that the authors could use to focus their efforts. The lack of specificity in the review point makes it difficult for the authors to confidently identify the areas that need improvement. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that is specific would clearly indicate which part of the paper is being addressed and would provide a clear direction for the authors to follow. It would guide the authors to identify the specific sections of the paper that need attention and would help them to focus their efforts on improving those sections. The review point would also provide a clear hypothesis or question that the authors could use to focus their efforts. 

Based on the score rubric, the review point provided does not meet the criteria for specificity. It fails to guide the authors to identify the specific sections of the paper that need attention and does not provide a clear direction for improvement. Therefore, the score is -1."
4288,NIPS_2020_251,"* The NF assumption was not discussed as compared to a standard SSM which uses additive measurement noise. Placing the emission noise *before* the nonlinearity is a crucial move; otherwise filtering is not tractable. I would have appreciated further discussion of the impact of this. It's possible that this technique can be applied as a drop-in replacement in many models to avoid awkward approximations such as EKF, UKF and PF; however this conclusion is not immediate from the work presented in this paper. * As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning. * The qualitative experiments seemed particularly artificial; I did not learn much here beyond the fact that the implementation broadly seems to work. If these are indicative of a real-world problem, it would be helpful to make this clearer. * NKF does not show markedly better performance than the GP-Copula model in the main experiments.","* As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a potential problem with the inferred value of $z$ without suggesting any solutions or further investigations. The review point does not include any actionable comments that can be implicitly inferred, nor does it provide any explicit or implicit actions that should be done. It simply states the issue without offering any guidance on how to address it. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor encouraging, but rather a straightforward statement of the observed discrepancy between the true value of $z$ and the inferred value. The review point does not contain any elements of politeness or rudeness, and it is delivered in a neutral manner. It is a factual observation that the inferred value of $z$ is not in line with the true value, which could potentially cause problems for inference and learning. The review point does not make any judgments or comments on the quality of the paper or the work of the authors, which is why it is considered neutral. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point provided contains a claim about the inferred value of $z$ being substantially different from the true value of $z = -5$. However, the claim is not verified. There is no logical deduction or reference to external or internal sources that would support the claim. The review point simply states the problem without providing any justification or explanation. This lack of verification is in direct contrast to the score rubric, which requires claims to be supported by logical deductions and internal or external sources. Therefore, the review point does not meet the criteria for a score of 1, as it fails to verify the claim made. 

In conclusion, the review point does not contain any claims that are verified, nor does it include factual statements that do not require verification. It is clear that the review point does not meet the criteria for a score of 1, as it lacks the necessary verification of the claim made. 

So the overall score is -1.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It clearly identifies the issue with the inferred value of $z$ and how it could lead to problems in inference and learning. This specificity allows the authors to pinpoint exactly which part of the paper is being addressed, and they can be confident about the nature of the problem. The review point is focused on a specific example and does not generalize the issue to other parts of the paper. It also does not leave room for ambiguity, which is crucial for the authors to understand and address the issue. Therefore, based on the score rubric, the review point is highly specific and directly addresses a particular aspect of the paper, making it clear for the authors to identify and address the issue. So the score is 1."
352,ICLR_2022_2232,"Weaknesses
The writing should be improved overall. Issues throughout include grammar, misuse of commas, capitalization (as in reference to algorithm 1 on page 6), technical details left out, poor formatting.
There seems to be a lack of novelty in the modeling approach: the same training schemes, DNN model, etc have been developed before, in papers that are cited by this one.
The results are rather limited and do not seem to show a clear advantage over standard techniques.
Some sentences have unclear meaning, e.g.: Page 1 - ‘a manual adaptation and business knowledge are needed…” - ‘its dynamic dimension reflects directly the demand change over the learning steps…” Page 2 - ‘Before we deep dive in the model architecture and present its main components, we should briefly highlight some problem-related concepts’ - the narration is too casual Page 5 - Equation 13 label is cut off - means that violation is given -infinity in what sense? A large floating point number? Page 6 - Training: Sample rollout and greedy rollout - are these ever defined? What is the baseline function used in the REINFORCE algorithm? Page 7 - Google OR-Tools baseline - what are the details of the implementation? Is it the CP-Sat solver? A specialized solver? - What is the meaning of the numbers in Table 1? Is lower better? - Why does RNN-RL appear twice in Table 2 with different results? - Is OR-tools called with a solver timeout? Or it is allowed to run to completion? - The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method?
General questions:
Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case?
Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper?
What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.","- The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method? General questions: Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case? Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper? What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable comments. It merely poses questions without suggesting any specific actions or experiments that could be done. The review point does not guide the authors on what to do next or how to improve their work. It lacks the necessary detail and direction that would help the authors to make informed decisions about their research. The review point does not fulfill the criteria of providing actionable comments, either explicit or implicit. Therefore, based on the score rubric, the review point does not meet the requirements for a score of 1, 0, or -1.",0,"The review point is delivered in a neutral tone, which aligns with the score rubric's criteria for a score of 0. The reviewer's questions are factual and do not contain any aggressive or disrespectful language. However, the tone could be perceived as slightly passive, as it does not actively encourage or provide constructive feedback. The reviewer's questions are focused on the technical aspects of the paper, which is appropriate for a scientific review. The reviewer's inquiries about the feasibility of solutions and the novelty of the approach are relevant and demonstrate a clear understanding of the paper's content. However, the reviewer does not offer any specific suggestions for improvement or highlight any strengths of the paper. This lack of engagement and encouragement is what differentiates a score of 0 from a score of 1. So the overall score is 0.",-1,"The review point contains several claims, but it fails to provide any verification or justification for these claims. For instance, the claim that the results for RNN-RL are very similar to OR-tools is not supported by any logical deductions or references to external or internal sources. Similarly, the claim that the solution is built incrementally and the possibility of taking an action that leads to no further feasible actions is not substantiated. The review point also asks questions about the masking scheme and the novelty of the approach, but it does not provide any answers or explanations. This lack of verification and justification for the claims made in the review point is why it does not meet the criteria for a score of 1. The review point does not contain any claims that are verified, nor does it include factual statements that do not need verifications. Therefore, based on the score rubric, the review point is not verifiable and does not meet the criteria for a score of 1.",-1,"The review point provided is quite vague and does not clearly identify which parts of the paper are being addressed. It asks general questions about the method and its feasibility, but it does not specify which sections of the paper are related to these questions. The authors would likely be unsure about which parts of the paper are being addressed by the review point, as it does not provide clear guidance or specific points of discussion. The lack of specificity in the review point makes it difficult for the authors to confidently identify the parts of the paper that are being addressed. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which parts of the paper are being addressed by the review point."
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.",1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out some issues and limitations without suggesting any specific actions that the authors should take. For example, the review point mentions that the number of switches can be upper-bounded by twice the number of times the revealing action is played, but it does not provide any guidance on how to achieve this or why it is important. Similarly, the review point questions why Thm 3.1 implies the condition of Thm C.5 with alpha=1, but it does not offer any insight into how to resolve this issue or what the implications might be. The review point also fails to provide any implicit actionable comments that the authors can infer after reading the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is not particularly polite or constructive. It contains several negative comments, such as ""too many ))"" and ""some notations seem to be undefined (w_t, W_t)."" These comments could be perceived as aggressive or disrespectful. Additionally, the reviewer's confusion about the implications of Thm 3.1 is expressed in a way that could be seen as confrontational, rather than a request for clarification. The reviewer also makes a critical remark about the non-decreasing nature of (rho_t), which could be interpreted as a personal opinion rather than a constructive critique. While the reviewer does make an attempt to provide feedback, the overall tone is not as helpful or encouraging as it could be. Therefore, based on the score rubric, the review point is not as polite or constructive as it could be, and it does not meet the criteria for a score of 1.",-1,"The review point contains several claims that are not verified. For instance, the claim about the number of switches being upper-bounded by twice the number of times the revealing action is played is not supported by any logical deductions or references. Similarly, the claim about the condition of Thm C.5 with alpha=1/2 is not justified, and the claim about the non-decreasing nature of (rho_t) is not explained. The review point also contains undefined notations (w_t, W_t), which are not verified or explained. Therefore, the review point does not meet the criteria for verifiability as it lacks logical explanations, references, and justifications for its claims.",0,"The review point provided is quite specific and directly addresses certain aspects of the paper. It points out the lack of a constant 2 in the number of switches, which is a clear and specific issue. The review point also identifies the undefined notations (w_t, W_t) and suggests adding a remark, which is a direct and specific comment on the paper. Furthermore, the review point provides a specific upper-bound for the number of switches, which is a clear and specific suggestion for improvement. The review point also questions the implication of Thm 3.1 with Thm C.5, which is a direct and specific question about the paper's content.

However, the review point does not provide a clear and specific comment on p14, which is a general suggestion to add a remark without specifying what the remark should be about. This lack of specificity in the review point for p14 is why it does not score a 1. The review point also does not provide a clear and specific comment on p15, l458, which is a general statement about the number of switches without specifying the exact upper-bound.

In conclusion, the review point is mostly specific and directly addresses certain parts of the paper, but it lacks specificity in a few areas, which is why it does not score a 1. The authors can guess which parts are being addressed in the review point, but are not fully confident."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.",- Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the concept of using a VAE to encode exploratory frames and the impact of exploration on auxiliary rewards. There is no suggestion of what the authors should do or what experiments they could conduct. The review point does not give any hint or direct actionable comments, leaving the authors without a clear direction on how to proceed. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply asks questions about the methodology and the impact of exploration on the auxiliary rewards, without making any judgments or providing any constructive feedback. It is a straightforward inquiry about the details of the research, which is a neutral approach to reviewing. The review point does not contain any elements of politeness or rudeness, and it does not provide any specific suggestions for improvement or praise for the work. It is a passive review point that does not engage with the content of the paper in a way that would be considered helpful or encouraging. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not contain any claims, nor does it require verification. It is a question that seeks to understand the relationship between a VAE used for encoding exploratory frames and the effect of auxiliary rewards on the environment. The review point is purely theoretical and does not make any assertions that need to be verified. It is a question that is open to interpretation and does not require any external or internal sources to be justified. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It asks about the representation of rewards as a function of a VAE used to encode exploratory frames in the environment, and how this impacts the effect of auxiliary rewards learned this way. This specificity allows the authors to identify which parts of the paper are related to the review point, and they can be confident about which part is being addressed. The review point is focused on a particular theme and idea, which is the use of a VAE for encoding exploratory frames and its impact on auxiliary rewards. This focus on a specific aspect of the paper makes it clear to the authors which part of the paper is being addressed by the review point. Therefore, the review point meets the criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point."
5136,NIPS_2020_44,"The main weaknesses which make me a little less excited about this work are mostly related to the scalability of the algorithm(s) and the applications of their techniques. - Scalability: Barrier-Greedy++ is not scalable but achieves a slightly better approximation (by a factor of 2) compared to the relatively more scalable Barrier-Greedy. However, Barrier-Greeedy itself though is linear in n, is cubic in r (which is the size of the largest feasible set). Even if r is a fraction of n (say, 10% of n), the algorithm is still cublc in n which could be very slow for n in range of a million instances. - Motivating applications: Given that greedy algorithm already achieves a factor of k for multiple matroid constraints (which occur mostly in practice), I do not see this algorithm being useful in only the matroid case. Moreover, I do not see any guarantees in the *only* knapsack case (i.e. if there is no matroid constraint). As far as I have seen even in the related work of Mirzasoleiman et al and Feldman et al, they either have only matroid constraints or only knapsack constraints. I would really like the authors to clarify this. In my opining, this paper would be significantly strenghted if this algorithm applies to only multiple knapsack constraints. It seems the algorithm also has an additional assumption of (l \leq k) whcih the authors later relax (but this part was not super clear in the paper).","- Scalability: Barrier-Greedy++ is not scalable but achieves a slightly better approximation (by a factor of 2) compared to the relatively more scalable Barrier-Greedy. However, Barrier-Greeedy itself though is linear in n, is cubic in r (which is the size of the largest feasible set). Even if r is a fraction of n (say, 10% of n), the algorithm is still cublc in n which could be very slow for n in range of a million instances.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the Barrier-Greedy++ algorithm without suggesting any improvements or modifications that could be made to address these limitations. The review point does not provide any implicit actionable comments that could be inferred by the authors, nor does it give any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific changes or improvements that could be made to the algorithm, nor does it provide any insights into how the algorithm could be optimized or made more scalable. It also does not offer any suggestions on how to better utilize the dataset or how to improve the performance of the algorithm. 

In conclusion, the review point does not provide any actionable comments or suggestions, which is why it does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts about the scalability of the Barrier-Greedy++ algorithm and its comparison to the Barrier-Greedy algorithm. The reviewer does not offer any constructive criticism or suggestions for improvement, which is a characteristic of a neutral tone. The reviewer also does not express any personal opinions or judgments, which further contributes to the neutrality of the review point. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point provided discusses the scalability of the Barrier-Greedy++ algorithm, which is not scalable but offers a slightly better approximation than the Barrier-Greedy algorithm. The claim made in the review point is that the Barrier-Greedy++ algorithm is cublc in n, which is a significant drawback. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The review point also fails to mention any specific instances or scenarios where the scalability issue would be problematic. Therefore, the review point does not meet the criteria for verifiability as it contains a claim that is not verified. 

In contrast, the review point could have been improved by providing a logical explanation or referencing external or internal sources to support the claim about the scalability of the Barrier-Greedy++ algorithm. Additionally, the review point could have been more specific about the instances or scenarios where the scalability issue would be problematic. 

So the overall score is -1 because the review point contains a claim that is not verified, and it misses a justification, explanation, or reference.",1,"The review point provided is quite specific and directly addresses the scalability issue of the Barrier-Greedy++ algorithm. It clearly states that the algorithm is not scalable and is cubic in n, which is a crucial detail for the authors to understand the limitations of their method. The review point also provides a comparison with the Barrier-Greedy algorithm, highlighting the slight improvement in approximation. This comparison is important as it gives the authors a benchmark to assess the performance of their algorithm.

The review point is well-structured and directly addresses the paper's content, making it easy for the authors to identify which parts of the paper are being discussed. The specificity of the review point is evident in the way it points out the cubic complexity of the algorithm and the comparison with the Barrier-Greedy algorithm. This level of detail and specificity is crucial for the authors to understand the implications of their work and to improve their method.

In conclusion, the review point is highly specific and directly addresses the scalability issue of the Barrier-Greedy++ algorithm, making it clear for the authors to understand the limitations of their method. The review point also provides a comparison with the Barrier-Greedy algorithm, which is important for the authors to assess the performance of their algorithm. Therefore, the review point is well-structured and directly addresses the paper's content, making it easy for the authors to identify which parts of the paper are being discussed. So the score is 1."
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.","- The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner?",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the static nature of the observation model's attentional coefficients and their potential impact on model performance. The review point does not suggest any specific actions or experiments that the authors could undertake to address these issues. It also does not provide any implicit actionable comments that the authors can infer from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer's questions are straightforward and do not contain any personal opinions or judgments. The reviewer's comments are focused on the technical aspects of the paper, which is a neutral approach to reviewing. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, the review point is factual and does not fit well with the other two scores, which are more focused on the tone of the review. So the overall score is 0.",-1,"The review point in question makes a claim about the static nature of the attentional coefficients in the observation model and their potential impact on model performance. However, it fails to provide any form of verification or justification for this claim. The review point does not offer any logical deductions or references to external or internal sources that could substantiate the claim. This lack of verification and justification is a clear deviation from the score rubric's requirement for claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",1,"The review point provided is quite specific and directly addresses the model's attentional coefficients and their static nature. It questions the contribution of these coefficients to the model's performance and whether their static nature predetermines the impact of variables on each other. This specificity allows the authors to identify the exact part of the paper that is being addressed, which is the model's performance and the nature of its attentional coefficients. The authors can be confident that the review point is discussing the model's performance and the nature of its attentional coefficients, as it directly addresses these aspects. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
196,ICLR_2022_1224,"Weakness] 1. One of the experimental results can be interpreted differently so that one of the contributions can be just a misreading of the experimental results. 2. Some parts of the paper require more explanations and formal descriptions about the concepts.
[Comments] 1. About Figure 2b, as far as I understand, the authors tested two attack methods (single-point attack and multi-point attack) against two ResNet50 models (regular and robust) over the same set of target data points. (If they are tested on different sets of target data points, it could be an unfair comparison.) Then, against the regular ResNet50 model, the single-point attack can find much better perturbations (than the multi-point attack) for the same set of target data points. Doesn’t this just mean that “the multi-point attack is not effective against the regular ResNet50 model” rather than “adversarially trained model is more vulnerable (than the regular ResNet50 model) against the multi-point attack”? In other words, the main contribution that “certain black-box attack can perform better against adversarially trained model” is just a misreading of the experimental result and there is another possible interpretation that “the multi-point attack is not effective against the regular model”. (The reason for this another interpretation could be the reason that is provided by the authors; a regular model has a less smooth boundary.) Also, the comparison with DeepFool (Figure 3b) says that the robustness gain is bigger for white-box attacks (constantly 17) compared to other black-box attacks. However, since the robustness gain is defined as a ratio, this value can be small just because black-box attacks cannot find a small enough perturbation for regular models. Again, we still don’t know whether this is because of the vulnerability of adversarially trained models or because of the poor performance of the black-box attacks. I suggest the authors check the average perturbation sizes to show that the black-box attacks do not perform poorly against the adversarially trained model. 2. First, robustness gain is only briefly described in the introduction and I cannot see any formal definition of it. Even though experiments are the only way to compute the robustness gain, it is better to describe it formally (an ideal & theoretical definition. If there is any reference that supports the concept, cite it.) and explain the intuition behind the concept. Second, more details are needed for the robustness gain experiment. The main motivation for this experiment is unclear. η
(which must be the robustness gain) was not defined before it is mentioned. 3. Lastly, I don’t think that robustness gain is a good measure to make comparisons. This is because the robustness gain is a ratio so it can be decreased by the poor performances of black-box attacks against a regular model. In other words, it is unclear whether the results come from the vulnerability of an adversarially trained model against black-box attacks or the results just come from the poor performance of the attacks against a regular model.","2. First, robustness gain is only briefly described in the introduction and I cannot see any formal definition of it. Even though experiments are the only way to compute the robustness gain, it is better to describe it formally (an ideal & theoretical definition. If there is any reference that supports the concept, cite it.) and explain the intuition behind the concept. Second, more details are needed for the robustness gain experiment. The main motivation for this experiment is unclear. η (which must be the robustness gain) was not defined before it is mentioned.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of formal definition and the need for more details in the experiment. The review point does not suggest any specific actions that the authors should take to improve the robustness gain or the clarity of the experiment's motivation. It also fails to provide any implicit actionable comments that the authors can infer from the review point. The lack of actionable comments in the review point makes it difficult for the authors to deduce what needs to be done to address the issues raised. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The feedback is focused on the content of the paper, specifically the lack of a formal definition of the robustness gain and the need for more details in the experiment. The reviewer's tone is neutral, and the feedback is delivered in a straightforward manner without any personal attacks or disrespectful language. The reviewer's comments are constructive, aiming to improve the paper by suggesting the inclusion of a formal definition and more detailed explanations. Therefore, the review point is neutral and factual, fitting the score of 0 in the rubric.",0,"The review point in question does not make any claims, which is in line with the score rubric's requirement for a score of 0. The review point focuses on the lack of formal definition and the need for more details regarding the robustness gain experiment. It does not attempt to verify any claims, nor does it provide any logical deductions or references to support its points. The review point is purely descriptive and does not engage in any form of verification or justification. Therefore, it does not meet the criteria for a score of 1 or -1, as it neither contains a claim nor verifies one. It is also not relevant to the aspect of verifiability, as it does not attempt to verify any claims. So the overall score is 0.",-1,"The review point provided is lacking in specificity, which is crucial for the authors to understand which part of the paper is being addressed. The review point mentions the need for a formal definition of robustness gain and more details for the robustness gain experiment, but it does not specify where in the paper these elements are located or how they relate to the rest of the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors are left with uncertainty about which parts of the paper are being addressed by the review point. Therefore, the score is -1, as the review point does not make it obvious which part of the paper is being addressed."
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret?",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the analysis and suggests that the authors might consider extending their analysis to high-probability bounds on the regret. However, it does not provide any specific guidance or recommendations on how to achieve this. The review point also fails to suggest any implicit actionable comments that the authors can infer. It simply states that the choice of gamma seems to be optimal without providing any reasoning or evidence to support this claim. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and does not provide any actionable comments or suggestions. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer's comments are focused on the technical aspects of the paper, specifically the exploration parameter gamma and the bounds on the regret. The reviewer's question about extending the analysis to high-probability bounds is a valid point, but it is presented in a neutral manner without any additional commentary that could be construed as polite or encouraging. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance on the paper's content and quality. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the utility of the exploration parameter gamma in the context of Alg 1 and Thm 3.1. It suggests that the choice of gamma=0 might be optimal and questions the role of gamma in the analysis. However, the review point does not provide any logical deductions or references to support this claim. It also fails to address the aspect of verifiability, which is crucial for the evaluation of the review point. The claim made in the review point is not verified, and there is no justification or explanation provided to support the claim. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains a claim that is not verified, which is why it would receive a score of -1.",1,"The review point provided is quite specific and directly addresses the exploration parameter gamma in the context of Alg 1 and Thm 3.1. It questions the utility of gamma and suggests that the choice gamma=0 might be optimal. This review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. The review point also invites further discussion on high-probability upper-bounds and the role of gamma, which shows an understanding of the paper's content and a willingness to engage with the authors on specific aspects.

In contrast, the review point lacks specificity and does not clearly identify which parts of the paper are being addressed. It is vague and does not provide any concrete suggestions or questions that would help the authors understand what needs to be improved or clarified. The review point does not demonstrate a clear understanding of the paper's content or a willingness to engage with the authors on specific aspects.

Therefore, based on the score rubric, the review point provided is specific and directly addresses the paper's content, making it easy for the authors to identify which parts of the paper are being addressed. It also invites further discussion, which shows an understanding of the paper's content and a willingness to engage with the authors. The review point lacks specificity and does not clearly identify which parts of the paper are being addressed, making it difficult for the authors to understand what needs to be improved or clarified. So the overall score is 1."
280,ICLR_2022_586,"weakness I see is the way of presenting the results in Fig. 3. All the percentages of optimally solved problems are relatively low, so the plots for some algorithms are not clearly visible (however, it is clear that the introduced algorithm outperforms other approaches). I recommend acceptance of this article.","3. All the percentages of optimally solved problems are relatively low, so the plots for some algorithms are not clearly visible (however, it is clear that the introduced algorithm outperforms other approaches). I recommend acceptance of this article.",0.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the percentages of optimally solved problems are relatively low and that the plots for some algorithms are not clearly visible. While it does suggest that the authors should accept the article, it does not offer any specific guidance on how to improve the article or address the limitations mentioned. The review point also fails to provide any implicit actionable comments that the authors can infer after reading the review point. It simply states that the introduced algorithm outperforms other approaches without providing any insight into how this could be achieved or what specific improvements could be made. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or helpful. The feedback is delivered in a straightforward manner, focusing on the content of the paper rather than the tone or style of the writing. The reviewer's comments are factual and do not contain any personal opinions or judgments, which is in line with the neutral tone expected in a review. The feedback is also constructive, providing suggestions for improvement without being overly critical or dismissive. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the percentages of optimally solved problems are relatively low, which is a factual statement. The review point also makes a general statement about the visibility of the plots for some algorithms, which is not a claim that requires verification. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified or disproved. So the score is 0.",-1,"The review point provided is quite vague and does not clearly specify which parts of the paper are being addressed. It mentions that the percentages of optimally solved problems are low and that the plots for some algorithms are not clearly visible, but it does not provide any concrete examples or suggestions for improvement. The review point also states that the introduced algorithm outperforms other approaches, but it does not explain why or how this is the case. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors may be left uncertain about which aspects of their work are being critiqued. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors would not be confident about which part of the paper is being addressed by the review point. So the overall score is -1."
1646,ICLR_2023_1255,"Weakness:
The description of the developed methodology is unclear. It is hard to follow the work. The notation and equation formulation are complicated. Together with the corresponding explanation, they should be largely improved.
The long range information is the main advantage of the proposed method, however, there are no corresponding experimental discuss or theoretical analyses in the manuscript.
The computation complexity analysis is a big concern. The proposed method uses more parameters and computation steps during message passing, this would bring lots of computation burden. However, from the real dataset experimental results in table 4, the performance promotion is slight. 4 The novelty isn't particularly high. Most techniques used were proposed and demonstrated before.
How about experimental results on the PDBbind dataset.",4 The novelty isn't particularly high. Most techniques used were proposed and demonstrated before. How about experimental results on the PDBbind dataset.,-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the novelty isn't particularly high and that most techniques used were proposed and demonstrated before. The review point does not suggest any specific actions or improvements that could be made to the work. It also does not provide any implicit actionable comments that could be inferred from the review point. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor particularly helpful or encouraging. The reviewer's comments are focused on the content of the paper, such as the novelty of the techniques used and the experimental results on the PDBbind dataset. However, the reviewer does not offer any constructive feedback or suggestions for improvement, which could have made the tone more helpful. The reviewer's comments are also quite direct and lack the politeness that could have been present in the feedback. For example, the reviewer could have phrased their comments in a more encouraging manner, such as suggesting that the authors could have provided a simpler expression alongside the current one. This would have made the tone more helpful and constructive. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It merely states that the novelty of the techniques used is not particularly high, which is a factual statement that does not require any form of verification. The review point does not make any claims about experimental results on the PDBbind dataset, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the experimental results on the PDBbind dataset. It directly addresses the novelty of the techniques used and the specificity of the results. The authors can be confident that the review point is addressing the experimental results and their novelty, as it clearly states that the techniques used were proposed and demonstrated before. This specificity in the review point allows the authors to identify which parts of the paper are related to the review point, namely the experimental results and their novelty. The authors can also be confident that the review point is not addressing any other part of the paper, as it is focused on the experimental results. Therefore, the review point is well-aligned with the score rubric, which requires the review point to make it obvious which part of the paper is being addressed. So the overall score is 1."
4450,NIPS_2020_1451,"1. Unlike the works HaoChen and Sra and Nagaraj et.al, this work uses the fact that all component functions f_i are mu strongly convex. 2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples. 3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.",2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples.,1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the paper without suggesting any specific actions that could be taken to address these limitations. The review point does not offer any implicit actionable comments that the authors can infer, nor does it provide any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any direct and explicit actionable comments that the authors should make. It does not provide any actionable steps or experiments that the authors can undertake to improve the paper. Instead, it merely points out the limitations of the paper without offering any solutions or improvements. 

The review point does not include any actionable comments that can be implicitly inferred. It does not provide any guidance on how the authors could improve their paper or address the limitations mentioned. The review point does not suggest any specific actions that the authors could take to enhance the paper's quality or relevance. 

The review point only mentions some limitations without giving any explicit or implicit actions that should be done. It does not offer any suggestions on how the authors could improve their paper or address the limitations mentioned. 

In conclusion, the review point does not meet the criteria for actionability as it does not provide any direct or explicit actionable comments, steps, or experiments that the authors should undertake. It also does not include any actionable comments that can be implicitly inferred. Therefore, the review point is not actionable and does not contribute to the improvement of the paper.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The feedback is based on the facts presented in the paper, and the reviewer does not make any subjective judgments or comments on the quality of the paper. The reviewer also does not provide any constructive suggestions or encouragement, which would have been indicative of a higher score. The feedback is purely factual and does not engage with the authors in a way that would be considered polite or encouraging. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to external or internal sources. The review point simply states that the authors need to explain why removing some of the assumptions is an important contribution, which is a general requirement for the paper rather than a specific claim that needs to be verified. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. 

The review point does not contain any logical deductions or references to external or internal sources that would support a claim. It also does not provide any examples that would help to verify the claim. The review point is more of a general commentary on the paper rather than a specific claim that needs to be verified. 

In conclusion, the review point does not contain any claims that need to be verified, and it does not meet the criteria for verifiability as outlined in the score rubric. Therefore, the score is 0.",1,"The review point provided is quite specific and directly addresses the removal of assumptions like bounded variance and bounded gradients. It asks for solid examples to explain why this is an important contribution, which is a clear and direct question. The authors can confidently identify which parts of the paper are related to the review point, as it specifically asks for examples related to the assumptions mentioned. This level of specificity ensures that the authors are fully aware of what is being asked for in the review point. Therefore, the authors can be confident about which part of the paper is being addressed by the review point. 

In contrast, the review points that lack specificity do not provide clear guidance on what parts of the paper are being addressed. The authors are left guessing which parts of the paper are being discussed, which can lead to confusion and uncertainty. This lack of specificity does not help the authors to identify the relevant sections of the paper, and thus, they are not confident about which part of the paper is being addressed by the review point. 

Therefore, based on the score rubric, the review point provided is specific and directly addresses the removal of assumptions, making it clear which part of the paper is being addressed. This is in line with the score criteria for a score of 1, where the authors can be confident about which part of the paper is being addressed by the review point."
4812,NIPS_2020_1627,"- While the theoretical results seem correct, it is not clear to me the advantages of this approach compared to previous work, in particular, gradient Q-learning (GQ). On line 110, it is written that the assumptions are not as stringent but I am not convinced that this is the case. Could the authors clarify this point? - Assumption 1 does not seem very natural to me. If I am interpreting it correctly, it assumes that we have a fixed replay buffer of data on which we are doing updates, as in the offline batch RL setting. It is not specified which policy is used to collect this data and I would expect certain assumptions on this behavior policy. - I do not think assumption 4 would be extremely realistic in practice, but I think this is acceptable in this case since it seems like this assumption is mainly made to provide more insight on the performance of the algorithm (in a special case). As such, I would be more comfortable if it was presented as such, instead of trying to justify it in practice. For example, line 196: ""We note that Assumption (IV) does not impose any additional constraint on the features considered, since we can make them orthogonal and scale them to ensure that the latter assumption holds."" I do not find this argument convincing since we may be given features that do not satisfy this condition in practice or features may be changing over time if they are being learned. - I would have appreciated some more explanation concerning the intuition of the two-timescale updates. Line 121: ""...but instead match the projection of the output along the feature space, much like the “pre-conditioning” step from [1]."" seems to hint at the idea but it is not clear to me exactly what this means. - One of the contributions is listed as ""A better theoretical understanding for the use of the target network in DQN."" (line 112). I think the paper could have included more explanations about this point. Currently, I cannot find much discussion about how CQL informs us about DQN. - For the empirical experiments, it is not clear how the hyperparamters were selected. On line 222, it is simply written what the step sizes are but the algorithms can be sensitive to this choice. It would be much more informative if sweeps over the two step sizes were done and the algorithms' sensitivity to them was assessed. Full parameter sweeps seems feasible in this case due to the simplicity of the environments. - For the mountain car experiments, it is surprising that only 3 training runs were done (line 255) for each algorithm. I would expect many more for such a simple environment, say, at least 10. Currently, the standard errors are very large and it is difficult to make meaningful comparisons between the algorithms.","- I would have appreciated some more explanation concerning the intuition of the two-timescale updates. Line 121: ""...but instead match the projection of the output along the feature space, much like the “pre-conditioning” step from [1]."" seems to hint at the idea but it is not clear to me exactly what this means.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely hints at the idea of matching the projection of the output along the feature space, which is not clear to the reader. The review point does not suggest any specific actions or experiments that the authors should undertake, nor does it provide any implicit actionable comments that the authors can infer. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses a desire for more explanation on a specific aspect of the paper, which is a helpful and encouraging approach. The tone is respectful and the feedback is focused on improving the paper, rather than criticizing it. The reviewer's request for clarification is a positive way to engage with the paper's content and to contribute to its development. This approach is in line with the rubric's emphasis on helpfulness and encouragement. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1.",0,"The review point in question does not make any claims, nor does it require verification. It is a statement of confusion regarding the concept of two-timescale updates, which is not a claim but rather a request for clarification. The reviewer is seeking to understand the intuition behind the two-timescale updates, which is a common practice in machine learning and deep learning. The reviewer's confusion is not a claim that needs to be verified, but rather a statement of lack of understanding. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. 

The reviewer's request for more explanation is not a claim, but rather a request for additional information. This is a factual statement that does not require verification. The reviewer is not making any claims about the accuracy of the method or the validity of the results, but rather expressing a need for further clarification. 

In conclusion, the review point does not contain any claims, nor does it require verification. It is a statement of confusion and a request for additional information, which is not relevant for verifiability in review points. Therefore, the score is 0.",-1,"The review point provided is somewhat vague and does not clearly identify which part of the paper is being addressed. The reviewer mentions the ""two-timescale updates"" and the ""pre-conditioning"" step from [1], but fails to explain what these terms mean or how they relate to the paper. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being discussed. The reviewer's comment about the ""intuition"" of the updates is also unclear, as it does not provide any concrete information or examples that would help the authors to understand the reviewer's concerns.

In contrast, a review point that is more specific would clearly identify which part of the paper is being addressed and provide a clear explanation of the issues. For example, a review point that mentions a specific section of the paper and provides a detailed critique of the content would be more helpful to the authors. Additionally, a review point that asks a specific question about the paper would help the authors to understand what the reviewer is looking for.

Overall, the review point provided is not sufficiently specific to help the authors understand which parts of the paper are being addressed. The lack of clarity and detail in the review point makes it difficult for the authors to identify the areas of the paper that need improvement. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which parts of the paper are being addressed by the review point."
5493,NIPS_2020_393,"Even though the paper is well written, simple to follow and provides an extensive set of experiments. However, the paper is relatively dense and a lot of further insight can be found in the appendix (which could almost already make another paper). Nevertheless, certain aspects of the analysis seem to be brushed over or left out: - Section 4 starts with the statement that it has been shown that deep ensembles indeed perform Bayesian Model Averaging, however no part of Section 3 directly addresses this point. It is clear that ensembles perform some sort of model averaging, however it would be interesting to further discuss the form q(w|D) takes in deep ensembles and which implicit assumptions about the prior are made and whether deep ensembles recover the true p(w|D) in the limit of infinite models in the ensemble (Sampling perspective?) or whether they are closer to a variational approximation and which implicit assumptions are made about the form of q(w|D). - Fig 3 offers a comparison of different predictive distributions and it is claimed that an increase in samples does not improve the estimate of the predictive distribution for SVI. However, I believe that this is an unfair comparison as this estimate is strongly influenced by the form of variational approximation used. Therefore, an approximation with little expressive power can only gain little improvements by further samples, whereas more complicated variational distributions might cover multiple modes in the weight distribution and could benefit from more samples. This should me more thoroughly examined.","- Section 4 starts with the statement that it has been shown that deep ensembles indeed perform Bayesian Model Averaging, however no part of Section 3 directly addresses this point. It is clear that ensembles perform some sort of model averaging, however it would be interesting to further discuss the form q(w|D) takes in deep ensembles and which implicit assumptions about the prior are made and whether deep ensembles recover the true p(w|D) in the limit of infinite models in the ensemble (Sampling perspective?) or whether they are closer to a variational approximation and which implicit assumptions are made about the form of q(w|D).",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a lack of discussion on certain aspects of deep ensembles, such as the form of the posterior distribution and the assumptions made about the prior. While it does suggest that the authors could further discuss these points, it does not provide any specific guidance on how to do so. The review point also does not include any implicit actionable comments that could be inferred by the authors. It simply states that the authors could have included more information on certain topics. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does it show any form of encouragement or helpfulness. The tone is purely informative, discussing the implications of deep ensembles and their relation to Bayesian Model Averaging. The reviewer does not make any personal judgments or criticisms, instead focusing on the content of the paper and its potential implications. The reviewer's tone is consistent throughout, maintaining a neutral stance. Therefore, based on the rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the nature of deep ensembles and their relationship to Bayesian Model Averaging. However, it fails to provide any form of verification or justification for this claim. The review point does not offer any logical deductions or references to external or internal sources that could substantiate the claim. It merely states that deep ensembles perform some form of model averaging without delving into the specifics of how this is achieved or what the implications of this are.

The review point also raises questions about the form of the posterior distribution in deep ensembles and the assumptions made about the prior. However, it does not attempt to answer these questions or provide any evidence to support the claims made. This lack of verification and justification is a significant shortcoming in the review point.

In contrast, a review point that would score a 1 would contain a claim and would provide logical deductions and references to external or internal sources to verify the claim. The review point would also address the specifics of the claim, such as the form of the posterior distribution and the assumptions made about the prior.

Therefore, based on the score rubric, the review point in question does not meet the criteria for verifiability. It contains a claim without providing any form of verification or justification, which is a critical aspect of the review process. As a result, the review point is not effective in addressing the claims made and does not contribute to a comprehensive understanding of the topic.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It points out that Section 4 discusses Bayesian Model Averaging in deep ensembles, but it does not mention any specifics from Section 3. This review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is focused on the relationship between deep ensembles and Bayesian Model Averaging, and they can also infer that the review point is not related to any other part of the paper. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
2578,NIPS_2019_1348,"Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent. ","0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the human risk measure without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific changes or improvements that could be made to the model or the methodology. It also does not provide any insights into how the model could be better aligned with the goal of fairness. The lack of actionable comments in the review point means that the authors are left without any clear direction on how to proceed with their work. 

In conclusion, the review point fails to provide any actionable steps or comments, which is a critical aspect of the score rubric. As such, the review point does not meet the criteria for actionability and would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer presents a concern about the assumption that a human risk measure is the gold standard for fairness, which is a valid point of discussion. However, the reviewer does not provide any constructive feedback or suggestions for improvement, which would have been helpful in guiding the authors to enhance their work. The reviewer simply states the concern without offering any additional insights or guidance. This makes the tone of the review point neutral and factual, but not particularly helpful or encouraging. Therefore, based on the score rubric, the review point is neutral and factual, and does not fit well with the other two scores.",0,"The review point in question does not contain any claims that need to be verified. It merely states a concern about the assumption that a human risk measure is the gold standard for fairness, without making any claims or assertions that require justification or verification. The review point is focused on questioning the assumption, rather than making claims that need to be supported by logical deductions or external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims or assertions that require verification. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The reviewer's concern about the assumption that a human risk measure is the gold standard is not directly tied to any specific section of the paper. The reviewer's argument about humans being poor random number generators is not substantiated with any evidence or reference to the paper. Furthermore, the reviewer's claim that minimizing risk in the tails of a distribution does not necessarily make the model fair is not supported by any analysis or discussion of the paper's content.

The review point does not provide any specific feedback or suggestions for improvement, which would help the authors to understand and address the concerns raised. Instead, the reviewer's comments are general and do not offer any concrete guidance on how to improve the paper.

In conclusion, the review point lacks specificity and does not effectively address any particular part of the paper. The authors would likely be unsure about which parts of the paper are being addressed by the review point, and they would not be confident in their understanding of the reviewer's concerns. Therefore, the score is -1."
5748,NIPS_2018_394,"Weakness: - The experiments are insufficient to validate the claim. Only CIFAR10/100 are used, but many of studied techniques that were effective on CIFAR10/100 and MNIST turned out ineffective on other larger datasets/tasks. I would be happy to raise my score if the authors could provide ImageNet improvement (at least for SO and SRIP). - As the authors also implied, MC is not enforced in the âright wayâ (columns not normalized). I would like the authors to report their MC performance with column normalization for completeness. ","- The experiments are insufficient to validate the claim. Only CIFAR10/100 are used, but many of studied techniques that were effective on CIFAR10/100 and MNIST turned out ineffective on other larger datasets/tasks. I would be happy to raise my score if the authors could provide ImageNet improvement (at least for SO and SRIP).",-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out the limitations of the experiments and the datasets used, without suggesting any improvements or further research directions. The reviewer's request for ImageNet improvement is not accompanied by any specific recommendations or methods that could be employed to achieve this. The lack of actionable comments in the review point means that the authors are left without a clear understanding of what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the limitations of the experiments and suggests a potential improvement, which is a helpful and encouraging approach. The tone is respectful and the feedback is focused on the specific aspects of the paper that could be improved, rather than being aggressive or disrespectful. The reviewer also expresses a willingness to raise their score if the authors address the concerns raised. This demonstrates a constructive and polite approach to the review process. Therefore, based on the score rubric, the review point is of high quality and aligns well with the criteria for a score of 1.",-1,"The review point in question makes a claim about the insufficiency of the experiments to validate the claim, specifically pointing out the use of only CIFAR10/100 datasets and the ineffectiveness of certain techniques on larger datasets. However, the review point does not provide any logical deductions or references to external or internal sources to support this claim. It merely states the issue without offering any justification or explanation. This lack of verification and support for the claim is why the review point does not meet the criteria for a score of 1. The review point also fails to provide any factual statements or claims that do not need verifications, which would have been necessary for a score of 0. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a higher score. So the overall score is -1.",1,"The review point provided is specific and directly addresses the claim made in the paper. It points out the insufficiency of the experiments used to validate the claim, focusing on the use of only CIFAR10/100 datasets and the ineffectiveness of the studied techniques on other larger datasets. This specificity allows the authors to identify which parts of the paper are being addressed by the review point. The review point also provides a clear direction for improvement, asking for ImageNet improvement, particularly for SO and SRIP. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, the review points that lack specificity do not provide clear guidance for the authors. They are vague and do not directly address any specific part of the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The review points that are too general or do not provide clear directions for improvement do not meet the criteria for a score of 1. 

Therefore, based on the score rubric, the review point provided is specific and directly addresses the claim made in the paper, allowing the authors to be confident about which part of the paper is being addressed by the review point."
2969,NIPS_2022_1846,"Weaknesses and Questions 1. For the distribution estimation, this paper uses three Gradient flow networks to learn different parameters. According to Section 4.3, the networks learn specific parameters for each class using different inputs (i.e., x ¯ j
). That is to say, the network F 1
will output c j j = 1 n for n
classes. However, in Line 142, the authors point that the parameter c
is shared between all classes. How to unify this c
? 2. How to update F 2
, and F 3
via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1
can be trained. 3. Some experimental details are missing. 3.1. What is the ratio of training data D t
to validation data D v
in the training stage. 3.2. What is the value of initial c
. 4. In Table 2, the metric-based baseline FEAT performs similar accuracy to the proposed method. It’s better to discuss the superiority of the method in terms of time consumption. According to Algorithm 2, the upper bound Eq. (14) simplifies the training of classifiers, but Eq. (16) is still difficult to compute. Typo: In Table 2, the description does not match the content, e.g., ""Euclidean Metric"" (or ""Hyperbolic Metric"") and ""Model"".","2. How to update F 2 , and F 3 via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1 can be trained.",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the limitations of the paper without suggesting any specific actions that could be taken to address these limitations. The review point does not offer any insights into how the authors could improve their work or what experiments they could conduct to enhance the performance of their model. It also fails to provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not contain any encouraging or helpful language. The review point simply states the facts about the training of F 1 and the inability to update F 2 and F 3, without offering any constructive feedback or suggestions for improvement. It also does not make any assumptions or speculations about the content of the paper, which is a characteristic of a neutral review point. The review point is purely factual and does not engage with the content of the paper in a way that would be considered polite or encouraging. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not contain any claims, nor does it require verification. It is a factual statement that only the network F 1 can be trained when the classifiers are fixed. This is a straightforward assertion that does not require any justification or reference to external or internal sources. The claim is not controversial and is supported by the given context. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not contain any claims that are not verified. The review point is simply stating a fact about the training process of the networks F 1, F 2, and F 3. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the question of how to update F 2 and F 3 via minimizing Eq. (16). It also clarifies that the network F 1 can be trained when the classifiers are fixed. This specificity allows the authors to confidently identify the parts of the paper that are being addressed by the review point. The review point is focused on the mathematical optimization problem and the implications of training the network F 1. It does not stray into general themes or ideas, but rather concentrates on the technical aspects of the paper. Therefore, the authors can be confident about which part of the paper is being addressed by the review point. 

In contrast, the review point could have been more general and less specific, which would have led to a lower score. For example, the review point could have mentioned that the paper needs to be improved in terms of clarity and organization, without specifying which sections need attention. This would have made it less clear for the authors which parts of the paper are being addressed by the review point. 

In conclusion, the review point is highly specific and directly addresses the technical aspects of the paper, which allows the authors to confidently identify the parts of the paper that are being addressed by the review point. This specificity aligns with the score rubric's criteria for a score of 1."
3401,NIPS_2020_1223,"*Temporal logic as such is often useful when considering infinite traces. Signal temporal logic is quite useful in the case of finite-time traces when dealing with continuous time systems. Neither of them are under consideration here, and I think this is the biggest draw back. *Novelty is quite limited. Much of the ideas in the paper have been introduced before. I will list some out here (which hasn't been discussed in the paper): 1. Writing STL specifications in terms of DNF specifications/using logical operators has been done previous in works such as: a) https://arxiv.org/abs/1703.09563 (they use DNFs/MILPS, which are equivalent), b) https://openreview.net/forum?id=BklC2RNKDS *Projecting outputs to satisfy constraints has been considered in works such as: a)https://arxiv.org/pdf/1805.07075.pdf, b) https://arxiv.org/abs/1801.08757 c)https://arxiv.org/abs/1603.06318. In fact, if you're only looking at finite+discrete time traces, this work is quite similar to c. Without considering infinite traces, the STL specifications reduce to first-order logic specifications as considered in c already. 2. There is a rich history of consider temporal logic specifications, and even STL specifications in the CPS community. As such, the specifications introduced here are not novel/new. 3. Propositions 4.1, 4.2 and 4.3 are trivial/obvious to the best of my knowledge. 4. There's very little discussion in terms of related work regarding enforcing STL/TL specifications for learning systems -- there have been several papers attempting to do this. a) https://robotics.sciencemag.org/content/4/37/eaay6276.full b) https://dorsa.fyi/publications/sadigh2014learning.pdf c) https://dl.acm.org/doi/abs/10.5555/3306127.3331994 d) https://openreview.net/forum?id=BklC2RNKDS","*Temporal logic as such is often useful when considering infinite traces. Signal temporal logic is quite useful in the case of finite-time traces when dealing with continuous time systems. Neither of them are under consideration here, and I think this is the biggest draw back. *Novelty is quite limited. Much of the ideas in the paper have been introduced before. I will list some out here (which hasn't been discussed in the paper):",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the paper without suggesting any improvements or further research. The review point does not offer any implicit actionable comments that the authors can infer, nor does it provide any explicit or implicit actions that should be done. It simply points out the shortcomings of the paper without offering any constructive feedback or guidance. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does he offer any form of encouragement or helpfulness. The tone is purely informative, with the reviewer stating facts about the paper and its content. The reviewer also refrains from making any personal judgments or comments about the quality of the paper or the authors' work. Instead, he simply points out the limitations of the paper and the novelty of its ideas. This approach is consistent with the neutral tone expected in a review point. The reviewer's comments are also factual and do not contain any subjective opinions or biases. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the temporal logic discussed in the paper is not applicable to the scenario at hand, and that the ideas presented are not novel. This is a factual statement that does not require any form of verification. The review point also does not make any claims that could be verified or disproved. It simply states the limitations of the paper without providing any evidence or reasoning to support these claims. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified or disproved. So the overall score is 0.",-1,"The review point provided does not effectively address the specificity of the paper. It fails to identify which parts of the paper are being addressed by the review point. The review point does not provide any concrete examples or specific points from the paper that would help the authors understand which parts are being discussed. Instead, it makes general statements about the paper's content and its relation to the review point. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. As a result, the authors would not be confident about which part of the paper is being addressed by the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that meets the criteria for specificity would provide clear examples or specific points from the paper that directly relate to the review point. This would allow the authors to confidently identify which parts of the paper are being addressed by the review point. 

So the overall score is -1."
3013,NIPS_2022_1564,"Weaknesses:
1.The main part can be more concise (especially for the introduction part)and including empirical results.
2.Given the new introduced hyper-parameters, it is still not clear whether this new proposed method is empirically useful. How to choose hyper-parameters in a more practical training setting?
3.The empirical evaluations can not well supported their theoretical analysis. As the authors claim running experiments with 24 A100 GPUs, all methods should be compared in a relatively large scaled training task. Only small linear regression experiment results are reported, where communication is not really an issue.
The paper discusses a new variant on a technique in distributed training. As far as I’m concerned, there is no serious issue or limitation that would impact society.","3.The empirical evaluations can not well supported their theoretical analysis. As the authors claim running experiments with 24 A100 GPUs, all methods should be compared in a relatively large scaled training task. Only small linear regression experiment results are reported, where communication is not really an issue. The paper discusses a new variant on a technique in distributed training. As far as I’m concerned, there is no serious issue or limitation that would impact society.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the limitations of the paper without suggesting any improvements or further experiments that could be done. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point only mentions some limitations without giving any explicit or implicit actions that should be done. It fails to suggest any direct and explicit actionable comments that the authors should make, which is a key aspect of the score rubric. The review point does not provide any actionable insights or suggestions that could help the authors improve their work. 

In conclusion, the review point does not provide any actionable steps or comments, and it does not meet the criteria for actionability as outlined in the score rubric. Therefore, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The feedback is delivered in a straightforward manner, focusing on the content of the paper rather than the tone or style of the writing. The reviewer's comments are factual and do not contain any personal opinions or judgments, which is in line with the neutral tone expected in a review. The feedback is also constructive, providing suggestions for improvement without being overly critical or dismissive. The reviewer's comments are focused on the content of the paper and the potential for improvement, rather than the quality of the writing or the tone of the paper. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states the existence of a new variant on a technique in distributed training without making any claims about its validity or impact. The review point also does not discuss any potential issues or limitations that could impact society, which is not relevant to the aspect of verifiability. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not contain any factual statements or claims that do not need verifications. The review point is neutral and does not provide any logical deductions or references to external or internal sources. So the overall score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the use of 24 A100 GPUs and the discussion of a new variant on a technique in distributed training, but it does not specify which sections of the paper these elements are found in. Additionally, the review point does not provide any concrete examples or questions that would help the authors to understand which parts of the paper are being addressed. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are related to the review point. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point with specificity would clearly identify which part of the paper is being addressed, such as mentioning a specific section or paragraph where the issue is found. This would allow the authors to confidently identify the part of the paper that needs to be addressed. 

The review point also fails to provide any constructive feedback or suggestions for improvement, which would be helpful for the authors to understand how to improve their paper. Instead, it simply states that there is no serious issue or limitation that would impact society, which is not relevant to the review point's focus on the specificity of the paper. 

Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point."
5328,NIPS_2020_1036,"* I was surprised to see, after the authors touted advantages of using a consistent training and test implementations, that the results of the (UN+UQ) system were significantly worse than those of the ""inconsistent"" solution introduced by Balle et al 2017 (UN+Q). Only when the softened quantizer is added (UN+UQ+SR) do we see a relatively small improvement. Why? Are there potential ways to improve this? * This makes one wonder about how much the UQ noise matters. In particular, it would be instructive to see a comparison to (UN+SR). Given the previous comment, one might suspect this would lead to even better performance - and thus that the UQ methodology, despite its mathematical interest, is not of practical value. * Although I think it's important and interesting, this is a pretty heavy and narrowly-focused topic for the NeurIPS community.","* This makes one wonder about how much the UQ noise matters. In particular, it would be instructive to see a comparison to (UN+SR). Given the previous comment, one might suspect this would lead to even better performance - and thus that the UQ methodology, despite its mathematical interest, is not of practical value.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely suggests a comparison to (UN+SR) without providing any specific guidance on how to conduct such a comparison or what to look for. The review point also does not include any actionable comments that can be implicitly inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is not particularly polite. It starts with a question that could be interpreted as skeptical about the importance of the UQ noise, which might be seen as a subtle form of criticism. The suggestion to compare the UQ methodology with (UN+SR) is presented in a way that implies the UQ methodology might be inferior, which could be perceived as disrespectful. The tone of the review point is more neutral than aggressive, but it lacks the constructive and encouraging tone that would be expected for a score of 1. The review point does not provide any positive feedback or suggestions for improvement, which is a key aspect of a polite review. Instead, it focuses on the potential shortcomings of the UQ methodology without offering any constructive criticism or guidance. Therefore, based on the score rubric, the review point is not particularly polite and does not meet the criteria for a score of 1.",-1,"The review point in question makes a claim about the practical value of the UQ methodology, suggesting that it may not be of practical value due to its mathematical interest. However, this claim is not verified. The reviewer does not provide any logical deductions or references to external or internal sources that would support this claim. The reviewer also does not justify why the UQ methodology is not of practical value, which is a key aspect of verifying a claim.

In contrast, the review point does not contain any claims or only includes factual statements that do not need verifications. The reviewer is simply expressing a thought about the practical value of the UQ methodology, without making any claims that need to be verified.

Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it contains a claim that is not verified. It also does not meet the criteria for a score of 0, as it does not contain any claims or only includes factual statements. The review point is therefore not relevant for verifiability in review points. 

So the overall score is -1.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions a comparison to (UN+SR) but fails to specify which aspects of the paper should be compared or how this comparison would be conducted. The review point also suggests that the UQ methodology may not be of practical value, but it does not provide any concrete evidence or reasoning to support this claim. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. The authors are left guessing, which does not align with the score rubric's requirement for specificity. Therefore, the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Instead, it leaves the authors uncertain and unsure. So the overall score is -1."
5646,NIPS_2018_809,"Weakness: - The uniqueness of connecting curves between two weights would be unclear, and there might be a gap between the curve and FGE. A natural question would be, for example, if we run the curve findings several times, we will see many different curves? Or, those curves would be nearly unique?  - The evidences are basically empirical, and it would be nice if we have some supportive explanations on why this curve happens (and whether it always happens). - The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding)  Comment: - Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research. - To be honest, I'm not sure the first curve-finding part explains well why the FGE work. The cyclical learning rate scheduling would perturb the weight around the initial converged weight, but it cannot guarantee that weight is changing along the curve described in the first part.","- Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It lacks specific recommendations for the authors to follow, which is crucial for actionability. The review point also does not include any implicit actionable comments that can be inferred by the authors. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This is why the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point does not suggest any improvements or changes that could be made to the paper. It does not provide any insights into how the authors could enhance the quality of their work or address the limitations mentioned. The lack of actionable comments in the review point means that the authors are left without any clear direction on what to do next. 

In conclusion, the review point does not fulfill the requirements for actionability, as it does not offer any direct or explicit actionable comments or implicit actionable comments. This is why the review point would receive a score of -1 according to the score rubric.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses enjoyment in reading the paper and acknowledges the mysterious nature of the reported phenomena, which is a positive and encouraging tone. The reviewer also suggests that the paper could inspire further research, which is a constructive comment. The tone is not aggressive or disrespectful, and it is not neutral or passive. Instead, it is helpful and encouraging, which is why it would receive a score of 1 according to the rubric. 

The reviewer's comments are also specific and provide constructive feedback, such as suggesting that the paper could be improved by providing a simpler expression alongside the current one, or by strengthening the introduction with a clearer context. These comments are polite and show respect for the authors' work, while also offering constructive criticism that could help the authors improve their paper. 

In contrast, the review point could have been more critical and less polite, which would have resulted in a lower score. For example, the reviewer could have used more negative language or provided more direct criticism of the paper's content or quality. This would have made the tone more aggressive or disrespectful, which is not in line with the score rubric's criteria for a score of 1. 

Overall, the review point is polite, constructive, and encourages further research, which is why it would receive a score of 1 according to the rubric.",0,"The review point provided does not contain any claims that need to be verified. It is a subjective opinion about the paper being fun to read and inspiring for subsequent research. The reviewer does not make any claims about the paper's content or methodology, nor do they provide any logical deductions or references to external or internal sources. The review point is purely based on the reviewer's personal experience and does not address the verifiability of any claims made in the paper. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, 0, or -1. It neither contains a claim nor does it verify any claim. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer specific feedback on the paper. It mentions that the paper is ""fun to read"" and that the phenomena are ""quite mysterious,"" but it fails to identify any particular part of the paper that is being addressed. The review point does not provide any concrete suggestions for improvement or any specific points that could be further elaborated upon. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. As a result, the authors would likely be unsure about which parts of the paper are being discussed. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the overall score is -1."
5563,NIPS_2020_1167,"The motivation and illustration are not clear. The details are as follows, 1. Why the RANet can capture more context information than SPP and previous attentional models? The authors claim that RANet naturally provides the spatial and category relationship of pixels to construct the contextual representations, but the category information in RANet may be not accurate, which could results in error guiding information. 2. After obtaining the contextual representation $O$ as described by Eq.(8), how to get the final segmentation map? 3.In Eq.(1), $B_{i \arrow j}$ denote a set of pixels on the line, what is the direction of the line? Vertical?horizontal？or oblique？","3.In Eq.(1), $B_{i \arrow j}$ denote a set of pixels on the line, what is the direction of the line? Vertical?horizontal？or oblique？",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the direction of the line in the equation without suggesting any further actions or experiments that could be done. The review point does not prompt the authors to consider any specific limitations or to provide any additional information that could be inferred. It lacks the actionability that is required for a higher score. The authors are left without any clear direction on what needs to be done or what could be inferred from the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not contain any encouraging or helpful language. The review point simply states the direction of the line in the equation, without any additional commentary or suggestions for improvement. It is a straightforward, objective assessment of the mathematical expression presented in the equation. The review point does not engage with the content in a way that would be considered polite or constructive, nor does it criticize or praise the work in any way. It is a neutral observation that does not contribute to the overall tone or quality of the review. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not contain any claims or assertions that require verification. It is a straightforward question about the direction of a line in a mathematical equation, which is a factual statement that does not need any justification or reference. The question is about the direction of the line, which is a property of the equation itself and does not require any external or internal sources to verify. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. So the score is 0.",1,"The review point provided is quite specific and directly addresses the direction of the line in the paper. It asks for clarification on whether the line is vertical, horizontal, or oblique, which is a clear and direct question. This specificity allows the authors to identify exactly which part of the paper is being addressed, and they can be confident about the answer. The review point does not leave room for ambiguity, and it is directly related to the content of the paper. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it clear which part of the paper is being discussed. So the score is 1."
5592,NIPS_2018_55,"weakness of this paper lies in the evaluation. Although it is a great thing that this paper uses more datasets than MNIST, the evaluation can be much improved. 1) The statements in the MNIST experiment such as ""While results without an CAE are quite convincing, the CAE clearly improves the pertinent positives and negatives in many cases. Regarding pertinent positives, the cyan highlighted pixels in the column with CAE (CAE CEM PP) are a superset to the cyan-highlighted pixels in column without (CEM PP). While these explanations are at the same level of confidence regarding the classifier, explanations using an AE are visually more interpretable."" are problematic. These are quite subjective statements, and some form of quantitative evaluation across subjects is required for such claims. 2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.  3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is ""With the help of domain experts"". 4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions:   I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence.   line 218: an CAE -> a CAE   line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.","2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the limitations of the experiment and the ambiguity of the results without suggesting any specific actions that could be taken to address these issues. The review point does not offer any insights into how the authors could improve their work or what further research could be conducted. It also fails to provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer's comments are focused on the content of the paper, questioning the statistical significance of the results and the expertise of the experts involved in the fraud experiment. The reviewer's tone is more passive, with a focus on the uncertainties and ambiguities of the paper's findings. The reviewer does not make any personal judgments or criticisms, which is why the tone is neutral. The reviewer's comments are more focused on the content of the paper rather than the tone or style of the paper. Therefore, the review point does not fit well with the other two scores, as it does not exhibit any aggressive or disrespectful tone, nor does it provide any helpful or encouraging feedback.",-1,"The review point in question makes several claims, but it fails to provide any form of verification or justification for these claims. The reviewer expresses uncertainty about the expertise of the experts involved in the procurement fraud experiment and questions the statistical significance of the results. However, there is no attempt to validate these claims with logical deductions or references to external or internal sources. The reviewer also makes a claim about the sample size, but again, there is no verification or justification provided.

The review point does not contain any factual statements or claims that do not need verifications, as it is primarily focused on the lack of clarity and verification in the experiment. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains several unverified claims, which is why it would receive a score of -1. 

In conclusion, the review point fails to provide any form of verification or justification for its claims, which is why it would receive a score of -1 according to the score rubric.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which part of the paper is being addressed. The review point mentions the experts' opinions and the statistical significance of the difference in Table 1, but it does not provide any concrete examples or specific points in the paper that are being questioned. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. The authors are left guessing which parts of the paper are related to the review point, which is not ideal for a constructive review process. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the score is -1."
5632,NIPS_2018_1007,"Weakness: Which brings us to various unclear parts in the paper. First of all, there are key claims that are hard to justify. For instance: ""a key strength of neural models is their effectiveness at efficient transfer"". I am sure you'll find a lot of disagreement here especially when you are not working in the vision community where ImageNet models transfer fairly well. This is not the case with models trained with RL (DQN or policy gradients) because the gradients are a lot noisier and the representations learning is more difficult. Another example being, ""A3C algorithm [â¦], because it is simple, robust and stable"" is hard to swallow given the term ""asynchronous"" in the algorithm. The paper's treatment of ""transfer"" is quite unclear too. Transfer learning has a long history and would refer to multiple surveys on transfer in RL [1,2] to better place their objective. Moreover, we can expect that the NIPS audience wouldn't know as much about symbolic AI and RDDL description, so use of terms like fluents without defining them first, leaves things unclear to the reader. Similarly, even thought the components of the architecture are clearly explained individually, their exact combination and how exactly the losses are setup is quite unclear. I hope the authors are atleast planning on releasing their code for easier replication. There are quite a few components in the proposed method. Whether they are warranted can only be checked by experimental verification. The paper is quite unclear about the exact nature of these problem domains - what's the observation space like, what are the possible number of problems that are generated in a domain, etc. (One can look it up on IPPC but making the paper clear would be better). Moreover, since these are planning problems it's hard to say if DeepRL algorithms like A3C are right baselines to benchmark against. The paper _is_ using the model after all (transition). Wouldn't it be useful to at least show the standard methods used in IPPC and their efficiency. Do we even gain on anything at all by showing transfer abilities if the total time taken by standard planning algorithms for each problem domain is less than learning via DeepRL? Moreover it's unclear what were the parameters of A3C algorithms itself - number of workers/batch size etc. It doesn't look like Figure 2 and Figure 3 show averaged runs over multiple seeds (maybe fill color to show standard error?) nor is there any standard deviation for Table 2 results. So although the inclusion of ""SAD"" seems pretty big as an improvement, I can't make a strong claim given how much variance there can be with DeepRL experiments.  # Minor things:  - Line 85: Missing citation? - Line 329: Not sure if Table 2 suggests that. Possibly meant to be Figure 3? [1]: http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf [2]: https://hal.inria.fr/hal-00772626/document ",- Line 329: Not sure if Table 2 suggests that. Possibly meant to be Figure 3? [1]: http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf [2]: https://hal.inria.fr/hal-00772626/document,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a potential confusion regarding the use of Table 2 or Figure 3, without suggesting any specific actions or experiments that could be taken to resolve this confusion. The review point also does not include any implicit actionable comments that could be inferred from the text. Instead, it only mentions a limitation without providing any guidance on how to address it. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and thus, it would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The reviewer's tone is not aggressive or disrespectful, nor is it overly polite or encouraging. The reviewer simply expresses uncertainty about the content of the paper, which is a neutral stance. The reviewer's question about whether Table 2 is meant to be Figure 3 is a factual observation that does not carry any negative or positive connotations. The reviewer's tone is not overly critical or overly positive, which is why it fits the score of 0. The reviewer's language is straightforward and does not contain any elements of politeness or rudeness. Therefore, the review point is neutral and factual, which is why it receives a score of 0.",0,"The review point in question does not make any claims, nor does it require verification. It merely expresses uncertainty about the terminology used in the paper, suggesting a possible misinterpretation of the term ""Table"" as ""Figure"". This uncertainty is not a claim and does not require any form of verification. The review point is simply a statement of confusion, which does not align with the score rubric's criteria for verifiability. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The reviewer's uncertainty about whether Table 2 or possibly Figure 3 is being referred to does not provide a clear direction for the authors to follow. This lack of specificity could lead to confusion and does not allow the authors to confidently identify the parts of the paper that are being addressed. The review point does not provide a clear path for the authors to improve their work, nor does it offer a specific area for clarification. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that is specific would clearly identify which part of the paper is being addressed, allowing the authors to confidently identify the areas that need improvement. The review point would provide a clear direction for the authors to follow, which would help them to improve their work. 

Based on the score rubric, the review point in question does not meet the criteria for specificity. It does not provide a clear direction for the authors to follow, nor does it allow the authors to confidently identify the parts of the paper that are being addressed. Therefore, the review point is not effective in addressing the paper's content. 

So the overall score is -1."
4011,NIPS_2020_182,"1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion. 3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework. 4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description. 5. There are many typos in this paper, e.g., - Line 40: “and and reture” - Line 127 and Line 142: “Appendix ??”",3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the difficulty in following the technical details of the paper, without suggesting any specific actions or improvements that could be made. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any direct and explicit actionable comments that the authors should make, nor does it provide any implicit actionable comments that the authors can infer. It merely states that the technical details are hard to follow, without offering any solutions or improvements. This lack of actionable comments makes the review point ineffective in guiding the authors on how to improve their paper. 

In conclusion, the review point does not provide any actionable comments, either explicit or implicit, and therefore does not meet the criteria for actionability as outlined in the score rubric.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the complexity of the technical details and suggests a helpful solution by recommending the inclusion of a neural network diagram or a pseudo-code algorithm. This approach is encouraging and aims to assist the reader in understanding the proposed framework. The tone is respectful and the feedback is focused on improving the clarity of the paper, which is a positive and constructive way to provide feedback. The reviewer's language is polite and the feedback is delivered in a manner that is likely to be well-received by the authors. Therefore, based on the score rubric, the review point is helpful and encouraging, which is why it would receive a score of 1.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely suggests that the technical details of the paper could be better understood with the inclusion of a neural network diagram or a pseudo-code algorithm. This is a factual statement that does not need any form of verification. The review point does not make any claims about the accuracy of the paper's content, nor does it question the validity of the paper's findings. It simply offers a suggestion for improving the clarity of the paper's technical details. Therefore, based on the score rubric, the review point does not contain any claims that need to be verified, and it does not make any assertions that could be verified or refuted. So the overall score is 0.",1,"The review point provided is quite specific, pointing out the difficulty in following the technical details of the paper. It suggests the inclusion of a neural network diagram or a pseudo-code algorithm to aid in understanding the proposed framework. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can identify the need for clarity in the technical details, and they can be confident about the part of the paper that is being addressed by the review point. The review point is focused on a particular aspect of the paper, which is the technical details, and it provides a clear direction for improvement. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
2396,NIPS_2021_311,"Weaknesses - The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.","- What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely poses a hypothetical scenario without suggesting any specific actions or experiments that could be taken to address the issue. The review point does not offer any insight into how the corpus residual value could be used to detect differences in test examples. It also does not provide any implicit actionable comments that could be inferred from the review point. The authors are left without any clear direction on what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the author. The review point simply states a potential issue with the use of a different corpus, without any personal opinion or judgment. It is a straightforward question that could be answered with a simple yes or no, or with additional information. The review point does not contain any elements of politeness or rudeness, and it does not provide any constructive feedback or suggestions for improvement. It is a neutral observation that does not fit well with the other two scores. Therefore, the score is 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a hypothetical scenario that is not based on any specific data or findings. The question posed is purely speculative and does not make any assertions that could be verified or refuted. It is a thought experiment that is not grounded in any empirical evidence or theoretical framework. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a philosophical inquiry rather than a scientific or methodological one. It does not involve any claims that could be supported by logical deductions or external or internal sources. As such, it does not fulfill the requirements for a score of 1, which would involve a claim that is verified through logical explanations or references. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a potential issue in the paper. It asks a hypothetical question about the implications of using a different corpus for explaining a test example, which is a clear and focused concern. This specificity allows the authors to identify the exact part of the paper that is being addressed, which is the explanation of the test example. The authors can be confident that the review point is discussing the potential impact of the choice of corpus on the interpretation of the test example.

The review point does not leave any ambiguity about which part of the paper is being addressed. It directly points out the potential issue and asks a question that is directly related to the paper's content. This specificity is in line with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point.

In contrast, a review point that lacks specificity would not allow the authors to be confident about which part of the paper is being addressed. It would leave the authors guessing which parts of the paper are related to the review point. This lack of specificity is what the score rubric describes as a score of -1.

Therefore, based on the score rubric, the review point provided is specific and directly addresses a potential issue in the paper, allowing the authors to be confident about which part of the paper is being addressed. This is why the review point is given a score of 1."
4575,NIPS_2020_1719,"Just 3 concerns: 1. Equation 11 is expressed without the regularization terms ... and it is stated that extending it with the same is straightforward. Is that really so? It is not really that straightforward when projecting. SO authors must expand on this a bit. 2. Justification provided for not comparing against Pareto-frontier search methods is very loose and ambiguous. and not convincing in anyway. Authors should expand on this more. 3. This agnostic formulation in this case is related with solving a minmax problem ... in ""robust MDPs"" or even in adversarial games.... Authors should make an attempt to discuss the same. Finally, while the evaluations are acceptable they definitely have room for improvement, at least in the way the results are discussed. -- that is sub-par. Ideally the authors must define evaluation goals as in what is measured and for what reason.. what is it that they are trying to show. Then when discussed the tables/plots connect the discussion back to the goals.","3. This agnostic formulation in this case is related with solving a minmax problem ... in ""robust MDPs"" or even in adversarial games.... Authors should make an attempt to discuss the same. Finally, while the evaluations are acceptable they definitely have room for improvement, at least in the way the results are discussed. -- that is sub-par. Ideally the authors must define evaluation goals as in what is measured and for what reason.. what is it that they are trying to show. Then when discussed the tables/plots connect the discussion back to the goals.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the work without suggesting any specific improvements or actions that could be taken. The review point does not provide any implicit actionable comments that the authors can infer, which is a key aspect of the score rubric. It also lacks any explicit or implicit actions that should be done, which is a critical requirement for a score of 1. The review point does not guide the authors on what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The feedback is focused on the content of the paper, discussing the formulation of the problem and the evaluation goals. However, the feedback could be more constructive by providing specific suggestions for improvement. The review point does not offer any encouragement or positive reinforcement, which could have made the tone more helpful. The feedback is also somewhat passive, as it does not actively engage with the authors' work or suggest ways to enhance the paper. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a general commentary on the quality of the evaluations and the discussion of the results. The review point does not make any specific claims about the content of the paper or the methodology used. Instead, it offers a critique of the way the results are discussed, which is not a claim that needs to be verified. The review point also suggests that the authors should define evaluation goals more clearly, but this is a suggestion rather than a claim that needs to be verified. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not meet the criteria for a score of 1, 0, or -1.",-1,"The review point provided is somewhat vague and does not clearly identify which parts of the paper are being addressed. The authors are left with a sense of uncertainty about the specific aspects of the paper that are being critiqued. The review point does not provide a clear connection between the discussion and the evaluation goals, which is crucial for the authors to understand the feedback. The lack of specificity in the review point makes it difficult for the authors to confidently identify the parts of the paper that are being addressed. Therefore, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide the authors with a clear understanding of the feedback."
5331,NIPS_2020_790,"- While the theoretical bounds are nice, no algorithmic results for efficiently estimating the CRS model is given. I suppose that this all relies directly on the machinery developed for CDM in [48]. - I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix. - One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution. - Given the parameter scaling of the model (quadratic in the number of items) it might not be easily applicable in practice. In fact I find the simulation results confusing. It is not clear why CRS has such nonlinear performance. I do not find the explanation on line 522-527 satisfactory.",- I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the suboptimal utilization of the page limit and the placement of main discussions in the Appendix. While it does suggest that the authors should reconsider their approach to the presentation of their work, it does not provide any specific guidance on how to do so. The review point lacks the provision of any actionable comments that can be inferred, and it does not suggest any limitations that could be addressed with explicit or implicit actions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not aligned with the expected tone of a constructive review. The language used is critical and somewhat aggressive, which is not conducive to a polite and respectful review process. The reviewer's tone is not encouraging or helpful, but rather dismissive and negative. The reviewer's comments about the utilization of the page limit and the placement of main discussions in the Appendix are not delivered in a manner that would be considered polite or constructive. The reviewer's choice of words such as ""suboptimal"" and ""discrepancy"" are not used in a way that would be considered helpful or encouraging. Instead, they are used in a way that could be perceived as rude or disrespectful. Therefore, based on the score rubric, the tone of the review point is not helpful, or encouraging, and it is not neutral, factual, or passive. It is aggressive and disrespectful, which is why it would receive a score of -1.",-1,"The review point in question makes a claim about the utilization of the page limit, stating that main discussions should not appear in the Appendix. However, this claim is not verified. There is no logical deduction or reference to external or internal sources that would support this claim. The reviewer simply states their opinion without providing any justification or explanation. This lack of verification is a clear deviation from the score rubric, which requires claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and would receive a score of -1.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the utilization of the page limit. It clearly states that main discussions, such as the discrepancy of the optimization, should not appear in the Appendix. This specificity allows the authors to identify exactly which parts of the paper are being addressed by the review point. The authors can be confident that the review point is referring to the placement of certain content within the paper. Therefore, the review point meets the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
2829,NIPS_2022_158,"Weakness
Not sure what the main take away is. The goal appears to be to understand the neural encoding in the retina, but after that the analysis and results, there is no attempt to tie these back to neurobiological mechanisms. It seems one could, but the paper just ends with the statement, ""our results are in strong agreement with observed retinal data,"" which leaves you hanging.
Specific issues:
The difference of Gaussians model in eq. 8: it mentions that the center position of each kernel is different for each neuron, but is this also learned? not mentioned.
Section 3: linear model in the continuum limit - this is very unclear. what is being continuized? space? The integral is over frequency space - not following what's going on. principal vectors a_1, a_2 and reciprocal vectors b_1, b_2 - what are these?
Section 4.1: "" power spectral density can be well approximated by a product of spatial and temporal power-law densities"" - Dong & Atick is cited, but curiously the claim the exact opposite, it is not separable.
Figure 4, panel A shows striking clustering in temporal spectral centroids - they are all stacked neatly in tight columns, no scatter. is this what emerges from the learned filters, or is somehow the quantization imposed?
The mosaics are interesting to look at, but not clear what to take away from this.
Overall this seems like a very promising direction, I want to like this paper, but I find it a bit confusing and lacking a clear message.","8: it mentions that the center position of each kernel is different for each neuron, but is this also learned? not mentioned. Section 3: linear model in the continuum limit - this is very unclear. what is being continuized? space? The integral is over frequency space - not following what's going on. principal vectors a_1, a_2 and reciprocal vectors b_1, b_2 - what are these? Section 4.1: "" power spectral density can be well approximated by a product of spatial and temporal power-law densities"" - Dong & Atick is cited, but curiously the claim the exact opposite, it is not separable. Figure 4, panel A shows striking clustering in temporal spectral centroids - they are all stacked neatly in tight columns, no scatter. is this what emerges from the learned filters, or is somehow the quantization imposed? The mosaics are interesting to look at, but not clear what to take away from this. Overall this seems like a very promising direction, I want to like this paper, but I find it a bit confusing and lacking a clear message.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments that the authors should make. It merely points out the lack of clarity and understanding in the paper, without suggesting any specific steps or experiments that could be done to improve the paper. The review point also does not include any implicit actionable comments that the authors can infer after reading the review point. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is not particularly polite or constructive. It contains several instances of confusion and lack of clarity, which could be perceived as a lack of understanding or respect for the paper. The reviewer seems to struggle with the concepts presented, and their comments are more focused on the confusion they feel rather than providing constructive feedback. For example, the reviewer questions the continuity of the linear model and the nature of the principal and reciprocal vectors without offering any insight or suggestions for improvement. Additionally, the reviewer's comments on the clustering in temporal spectral centroids and the mosaics are vague and do not contribute to a clearer understanding of the paper. The tone of the review point is more neutral than constructive, and it lacks the helpfulness that would be expected from a reviewer. Therefore, based on the score rubric, the review point is not particularly polite or constructive, and it does not meet the criteria for a higher score.",-1,"The review point contains several claims that are not verified. For instance, the claim about the center position of each kernel being different for each neuron is not supported by any logical deductions or references. Similarly, the claim about the linear model in the continuum limit is unclear and lacks a justification. The review point also fails to clarify what the principal vectors a_1, a_2 and reciprocal vectors b_1, b_2 are, which is a significant omission. Furthermore, the claim about the power spectral density being well approximated by a product of spatial and temporal power-law densities is not supported by any evidence or reference. The review point also does not provide a clear explanation of the clustering in temporal spectral centroids, leaving the reader with an unclear understanding of the results. Overall, the review point is filled with unverified claims and lacks the necessary logical explanations and references to support its claims. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability.",-1,"The review point provided is quite vague and does not clearly identify which parts of the paper are being addressed. The reviewer seems to be confused about the continuum limit and the principal vectors, which are critical aspects of the paper. The reviewer also fails to understand the relationship between the learned filters and the temporal spectral centroids. Furthermore, the reviewer does not provide any specific feedback on how the paper could be improved or clarified. This lack of specificity and clarity in the review point makes it difficult for the authors to identify which parts of the paper are being addressed. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which parts of the paper are being addressed by the review point. So the score is -1."
4787,NIPS_2020_611,"I think this paper would benefit from a more comprehensive connection to existing ideas in the field of biologically-plausible learning. Specifically: 1. There is a body of literature on biologically plausible methods for training feedforward ANNs with labels, see [R1] and [R2] for example. 2. Biologically-plausible learning rules that do not require label information have been studied in [R3] for shallow networks, and in [R4] for training multiple hidden layers of representations. The latter work, although targeting similarity search as a downstream task and not classification, also extensively uses divisive normalization for images. Similarly to this work, this seems to be important for achieving high accuracy (precision). 3. The empirical evaluations (Table 1) need to be compared with previously published results in the biologically-plausible settings. For instance, [R3] reports accuracy on fully connected network for MNIST better than 98.5%. Refs [1,3] report slightly better than 50% accuracy for fully connected architectures on CIFAR-10. Refs: R1. https://arxiv.org/abs/1412.7525 R2. https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full R3. https://www.pnas.org/content/116/16/7723 R4. https://arxiv.org/abs/2001.04907","2. Biologically-plausible learning rules that do not require label information have been studied in [R3] for shallow networks, and in [R4] for training multiple hidden layers of representations. The latter work, although targeting similarity search as a downstream task and not classification, also extensively uses divisive normalization for images. Similarly to this work, this seems to be important for achieving high accuracy (precision).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the existence of certain methods and their potential benefits without suggesting any specific actions or experiments that could be undertaken. The review point also does not include any implicit actionable comments that could be inferred from the text. It simply mentions the existence of certain methods and their potential benefits without providing any guidance on how to proceed with them. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the research and its implications. The feedback is based on the information provided in the research papers and does not include any subjective or emotional language. The reviewer's comments are constructive and aim to clarify the research's scope and potential improvements. The feedback is also concise and to the point, which is a characteristic of a neutral tone. Therefore, the review point is in line with the score of 0, as it does not fit well with the other two scores.",-1,"The review point makes a claim about the importance of divisive normalization for achieving high accuracy in deep learning models. However, it does not provide any logical deductions or external or internal sources to support this claim. The claim is made without any justification or explanation, which is a key aspect of the score rubric. The review point also fails to mention any specific studies or data that could have been used to verify the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. 

In contrast, the review point does not contain any claims or only includes factual statements that do not require verification. This is in line with the score rubric's criteria for a score of 0. The review point does not make any claims about the performance of the model or the significance of the results, which are the types of claims that would need to be verified. 

Overall, the review point does not meet the criteria for verifiability as it contains a claim without any supporting evidence or logical deductions. Therefore, the score is -1.",1,"The review point provided is quite specific and directly addresses the learning rules and their application in the paper. It clearly identifies the sections of the paper that are relevant to the review point, such as the use of divisive normalization for images and the training of multiple hidden layers of representations. This specificity allows the authors to confidently identify which parts of the paper are being addressed by the review point. The review point also provides a clear understanding of the importance of these techniques for achieving high accuracy in the paper's task. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed and allows the authors to be confident about the paper part that is being addressed by the review point comment."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out the lack of explanation regarding the use of percentile rank in the paper. The reviewer's question about the use of percentile rank is valid, but it does not offer any actionable advice or suggestions for improvement. The reviewer's inquiry is more of a commentary rather than a directive for the authors to follow. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests explicit actions nor does it provide implicit actionable comments that the authors can infer. The review point is more of a critique rather than a constructive feedback that could guide the authors in their future work. So the overall score is -1.",1,"The review point is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the reasoning behind the use of percentile rank but also points out that it is not explained in the paper. This is a helpful critique that encourages the authors to provide more clarity in their work. The tone is respectful and the feedback is focused on improving the paper, which is a key aspect of a constructive review. The reviewer's approach is encouraging and aims to help the authors enhance their paper. Therefore, the review point is helpful and aligns with the score rubric's criteria for a score of 1.",0,"The review point in question does not contain any claims, nor does it require verification. It is a question about the use of percentile rank in the paper, which is a factual statement that does not need to be verified. The reviewer is simply asking for clarification on a point that was not addressed in the paper. This is in line with the score rubric, which states that a score of 0 is given when the review point does not contain any claims or only includes factual statements and claims that do not need verifications. Therefore, the review point is not relevant for verifiability, and the score is 0.",1,"The review point in question is quite specific, focusing on the use of percentile rank in the paper. It directly addresses a particular aspect of the paper, which is the absence of an explanation for the use of percentile rank. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed, which is the reasoning behind the use of percentile rank. The review point is clear and concise, making it easy for the authors to understand and respond to. It also provides a clear direction for the authors to follow in their response, which is to explain the reasoning behind the use of percentile rank. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point. Therefore, based on the score rubric, the review point is well-crafted and effectively communicates the specific aspect of the paper that needs attention."
1812,ARR_2022_174_review,"1. Authors could have investigated Vrank predictions, towards understanding where the model makes mistakes. Assuming that Vrank would be adopted to evaluate VIST models, it is crucial to know its limitations. Additionally, this could serve as guidance to collect extra annotations to mitigate those mistakes. 2. Vrank does not account for the actual images. This is somehow reflected in the correlation results of Vrank for Obj and Event error types. This seems to be a weakness of the proposed approach since by definition, it is trying to imitate human judgment, but without access to the same information that annotators had to decide. In the Appendix, ""Model Design"" section, authors state that tried vision and language models but do not provide details. What was actually tried? Did it improve in detecting Obj and Event error types?
3. Given the insights gathered by the authors, I would expect a critical discussion regarding the inclusion of Vrank in VIST evaluation protocols. Namely, provided that Vrank also has error, the best possible automatic protocol would include only Vrank or be complemented with other metrics? 
In overall, the paper is well written, but there are some minor typos so I would suggest proofreading the manuscript. Some of the typos/grammar issues found: - Line 130-131: check grammar - Line 248: ""... and is ..."" - Line 400-401: check grammar - Line 426: ""autometric"" - Line 556-557: check grammar ","- Line 130-131: check grammar - Line 248: ""... and is ..."" - Line 400-401: check grammar - Line 426: ""autometric"" - Line 556-557: check grammar",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out areas that need to be checked or improved, such as grammar and the use of certain terms. While these are important aspects of the review process, they do not provide the authors with clear guidance on how to proceed with their work.

The review point also lacks any implicit actionable comments that the authors can infer. It does not suggest any potential improvements or areas of focus that the authors could explore. Instead, it simply states that certain aspects need to be addressed.

Furthermore, the review point does not mention any limitations without providing any explicit or implicit actions that should be done. It merely points out the limitations without offering any solutions or recommendations.

In conclusion, the review point does not meet the criteria for actionability as it does not provide any direct or explicit actionable comments, nor does it include any implicit actionable comments. It also fails to suggest any potential improvements or areas of focus that the authors could explore. Therefore, based on the score rubric, the review point is not actionable and does not provide the authors with clear guidance on how to proceed with their work.",NO_LABEL,Failed to generate feedback,0,"The review point provided does not contain any claims or make any assertions that require verification. It merely suggests checking grammar in certain lines of the text. This is a factual statement that does not require any form of verification, as it does not make any claims about the content of the text. Therefore, according to the score rubric, the review point does not contain any claims and is not relevant for verifiability. It is a straightforward instruction that does not involve any form of validation or justification. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer specific guidance on how to improve the paper. The reviewer's comments are general and do not pinpoint any particular section or aspect of the paper that needs attention. For example, the reviewer mentions checking grammar in several lines but does not specify which lines or what grammatical issues need to be addressed. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point.

Moreover, the reviewer's comment about the term ""autometric"" is not accompanied by any explanation or context, which could leave the authors unsure about what exactly is wrong with the term. The same issue arises with the reviewer's comment about the phrase ""... and is ..."" in line 248, where the authors are left without any clear direction on how to improve the sentence.

In contrast, a review point that is specific and clear would provide the authors with a clear understanding of what needs to be addressed in the paper. It would give them a sense of direction and help them focus on the areas that require improvement.

Based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors would not be confident about which parts of the paper are related to the review point. Therefore, the review point lacks the specificity required to effectively guide the authors in improving their paper."
4575,NIPS_2020_1719,"Just 3 concerns: 1. Equation 11 is expressed without the regularization terms ... and it is stated that extending it with the same is straightforward. Is that really so? It is not really that straightforward when projecting. SO authors must expand on this a bit. 2. Justification provided for not comparing against Pareto-frontier search methods is very loose and ambiguous. and not convincing in anyway. Authors should expand on this more. 3. This agnostic formulation in this case is related with solving a minmax problem ... in ""robust MDPs"" or even in adversarial games.... Authors should make an attempt to discuss the same. Finally, while the evaluations are acceptable they definitely have room for improvement, at least in the way the results are discussed. -- that is sub-par. Ideally the authors must define evaluation goals as in what is measured and for what reason.. what is it that they are trying to show. Then when discussed the tables/plots connect the discussion back to the goals.",2. Justification provided for not comparing against Pareto-frontier search methods is very loose and ambiguous. and not convincing in anyway. Authors should expand on this more.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided lacks direct and explicit actionable steps or experiments that can be done. It does not suggest any specific actions that the authors should take to improve their work. Instead, it merely points out the limitations of the authors' approach without offering any concrete solutions or methods to address these limitations. The review point also fails to provide any implicit actionable comments that the authors can infer from the review. It simply states the limitations without giving any guidance on how to overcome them. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and does not provide any actionable comments or suggestions. So the overall score is -1.",0,"The review point provided is not particularly polite or constructive. It lacks the supportive tone that is expected in a review. The language used is somewhat critical and does not offer any constructive feedback or suggestions for improvement. The reviewer does not provide any specific guidance or recommendations that could help the authors improve their work. Instead, the reviewer simply states that the authors should expand on certain aspects without offering any concrete advice on how to do so. This approach is not helpful and does not encourage the authors to improve their work. The tone of the review point is more neutral than anything else, which does not align with the criteria for a score of 1. Therefore, the review point does not meet the criteria for politeness and constructiveness as outlined in the score rubric. So the overall score is 0.",-1,"The review point in question does not provide a clear claim or any form of justification. It merely states that the authors should expand on their justification for not comparing against Pareto-frontier search methods, without offering any concrete evidence or logical reasoning to support this claim. The lack of verification and the absence of any supporting arguments or references make this review point ambiguous and unconvincing. Therefore, according to the score rubric, this review point does not meet the criteria for verifiability and thus would receive a score of -1.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which part of the paper is being addressed. The authors are left with uncertainty about the exact aspects of the paper that are being critiqued. The review point does not provide any concrete examples or suggestions for improvement, which would have helped the authors to understand and address the issues raised. The lack of specificity in the review point makes it difficult for the authors to confidently identify the parts of the paper that are being addressed. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that is specific would clearly identify which part of the paper is being addressed and provide concrete examples or suggestions for improvement. This would allow the authors to confidently identify the parts of the paper that are being addressed by the review point. 

The review point provided does not meet the criteria for specificity, as it is too general and does not provide clear guidance for the authors. Therefore, the score is -1."
3471,NIPS_2020_679,"he main weaknesses of the paper are: * the lack of comparisons to other model-based exploration approaches to disentangle the impact of the model-based aspect vs the novelty measure on performance. * the lack of empirical comparisons and discussion of ICM (Pathak et al., 2017) and RND (Burda, 2018). * the lack of statistical tests to support empirical comparisons. * the lack of an ablative study with respect to the 6 different losses used by the proposed approach.","* the lack of empirical comparisons and discussion of ICM (Pathak et al., 2017) and RND (Burda, 2018).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of empirical comparisons and discussion of ICM and RND without suggesting any specific actions that the authors should take. The review point does not give any hint or direct actionable comments, and after reading the review point, the authors still do not know or can not deduce what needs to be done. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",-1,"The review point provided lacks the necessary politeness and constructive tone that is expected in a review. The language used is quite harsh and critical, which can be perceived as aggressive and disrespectful. Phrases such as ""This introduction is terrible and makes no sense"" and ""No empirical evaluation whatsoever is provided"" are examples of this. The reviewer fails to provide any constructive feedback or suggestions for improvement, which is a key aspect of a review. Instead, the reviewer simply criticizes the paper without offering any guidance or encouragement. This approach does not foster a positive and constructive dialogue between the reviewer and the authors. Therefore, based on the score rubric, the tone of the review point is not helpful, encouraging, or constructive, and it is not neutral or factual. It is, in fact, rude and disrespectful. So the overall score is -1.",0,"The review point in question does not contain any claims that need to be verified. It merely states the lack of empirical comparisons and discussion of ICM and RND, without making any claims or assertions that could be verified. The review point is focused on the absence of certain elements in the original papers, rather than providing any claims or statements that could be verified. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it does not contain any claims that could be verified. So the overall score is 0.",-1,"The review point provided lacks specificity and does not clearly identify which parts of the paper are being addressed. The review point is vague and does not provide any concrete examples or suggestions for improvement. It fails to mention any specific sections of the paper or any particular aspects that need to be addressed. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being criticized or praised. As a result, the authors are left with no clear direction on how to improve their paper based on the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that is specific would clearly identify which parts of the paper are being addressed, providing concrete examples or suggestions for improvement. This would allow the authors to understand exactly what needs to be improved and how to do so. The review point would also help the authors to focus their efforts on the specific aspects that are being criticized or praised. 

Based on the score rubric, the review point provided does not meet the criteria for specificity. It is too vague and does not provide the necessary details for the authors to understand and address the review point effectively. Therefore, the score is -1."
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.","- Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments that the authors should make. It merely points out the lack of clarity in the research method and the definition of interpretability. The review point does not suggest any specific actions or experiments that could be done to improve the paper. It also does not provide any implicit actionable comments that the authors can infer from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The feedback is focused on the content of the paper, providing specific suggestions and questions that could be addressed to improve the paper. However, the review point could have been more constructive by offering more specific guidance on how to improve the paper, rather than simply stating that certain aspects are unclear or lacking. This would have made the feedback more helpful to the authors. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a critique of the research method and the definition of interpretability, but it does not make any assertions that could be verified. The review point is focused on the lack of clarity in the research method and the definition of interpretability, which are not claims that can be verified. The review point also discusses the approach taken by the authors, but it does not present any claims that could be verified. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any assertions that could be verified. So the score is 0.",-1,"The review point provided is not very specific in addressing the paper's weaknesses. It mentions the lack of clarity in the research method and the assessment of the experimental approach, but it does not specify which parts of the paper are lacking in these areas. The review point also addresses the definition of interpretability, but it does not clearly connect this definition to the proposed approach in the paper. The authors might be unsure about which parts of the paper are being addressed by the review point, as the feedback is quite general and does not provide clear guidance on how to improve the paper. The lack of specificity in the review point makes it difficult for the authors to identify and address the issues raised. Therefore, the score is -1."
2396,NIPS_2021_311,"Weaknesses - The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- How does the method scale as the corpus size or hidden dimension size is increased?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely mentions some limitations without giving any explicit or implicit actions that should be done. For example, the review point points out that the related work section is severely lacking and that the proposed method's performance improvement is only a little, but it does not suggest any specific actions to address these issues. Similarly, the review point notes that the contribution of the paper is incremental, but it does not provide any guidance on how to improve the paper's contribution. The review point also fails to suggest any specific experiments or steps that could be taken to improve the method's scalability as the corpus size or hidden dimension size is increased. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor constructive, but rather a straightforward assessment of the method's scalability. The reviewer asks questions about the method's implementation and its potential limitations, which is a neutral approach to critique. However, the reviewer could have been more polite by using softer language and providing more constructive feedback. For example, instead of stating ""It feels Section 6 doesn't belong in this paper,"" the reviewer could have suggested that the section could be improved or restructured. This would have made the feedback more helpful and less critical. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that the method does not scale well with increased corpus size or hidden dimension size. The reviewer simply states that the method's performance degrades as the size of the dataset or the number of hidden dimensions increases. This is a straightforward observation that does not require any justification or reference to external or internal sources. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. 

The reviewer's statement is a factual observation that is not subject to verification. It is a clear and direct statement that the method's performance is not scalable, which is a well-known issue in machine learning. The reviewer does not make any claims that need to be supported by logical deductions or external sources. 

In conclusion, the review point does not contain any claims that need to be verified, and it is a factual statement that does not require any justification or reference to external or internal sources. Therefore, the review point does not meet the criteria for verifiability. 

So the score is 0.",-1,"The review point provided is quite vague and does not clearly specify which part of the paper is being addressed. It lacks specificity and does not guide the authors on what exactly needs to be improved or clarified. The review point does not provide any concrete examples or suggestions that would help the authors understand what aspects of their paper are being critiqued. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors are left with uncertainty about the paper part that is being addressed by the review point comment. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
905,ICLR_2023_2312,"Weaknesses
1. Literature Review
The paper regrettably fails to acknowledge a vast body of related literature, on (i) intention-conditioned trajectory prediction, (ii) variational graph methods for trajectory prediction, and (iii) models that explicitly model social interactions for forecasting. At the very least, these references ought to be mentioned and discussed for a diligent representation of the research space, even if the methods are not directly compared against.
(i) Intention-Conditioned Trajectory Prediction:
[R1, R2, R3] talk about intention-conditioned trajectory prediction for autonomous vehicles. Apart from the data the methods are applied to, the architectures can be applicable to, and are relevant for, the problem being addressed here. Crucially, the DROGON paper defines intention explicitly (more on this in Weakness 2. below).
(ii) Variational Graph Methods:
[R4] from the Neurips I Can't Believe It's Not Better Workshop explicitly deals with graph conditional variational methods for multi-agent trajectory prediction. The results in that paper are very relevant for this research area and should be included.
(iii) Encoding Social Interactions:
Graph and other stochastic methods that encode social interactions between agents have been long applied to trajectory and behavior forecasating problems. [R5] explicitly incorporates a spatiotemporal graph for incorporating social interactions between agents. [R6] more recently explicitly takes a meta-learning approach for modeling the dynamics unique to a group for probabilistic forecasting. A sports team is a group, and if each team is viewed as having unique social dynamics resulting from the team's strategy then [R6]'s core modeling idea is directly applicable. The cue in [R6] terms is simply player location here. Their modeling of social influence of other agents is also permutation invariant, a limitation this paper claims about existing methods. References:
[R1] DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning - Choi et al.
[R2] Intention-Driven Trajectory Prediction for Autonomous Driving - Fan et al.
[R3] LOKI: Long Term and Key Intentions for Trajectory Prediction - Girase et al.
[R4] Graph Conditional Variational Models: Too Complex for Multiagent Trajectories? - Rudolph et al.
[R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al.
[R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.
2. Unsupported claims and definitions
The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.
3. Notation
There are a few notational errors. For instance, the variable used for the sequence cannot be the same as the individual elements: x < t = [ x 1 , . . . ]
. See [R4] for this. In many places there exist grammatical errors and incomplete sentences. Please do a pass to fix these.",- Rudolph et al. [R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al. [R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point for the paper by Raman et al. does not provide any direct or explicit actionable steps or comments. It merely states the limitations of the proposed method without suggesting any improvements or further experiments that could be done. The review point does not give any hint or direct actionable comments that the authors should make, and after reading the review point, the authors still do not know or can not deduce what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review points provided are all neutral in tone, which aligns with the score of 0 in the rubric. The reviewers do not express any form of aggression or rudeness, nor do they provide any form of encouragement or helpfulness. The feedback is purely factual and does not contain any personal opinions or judgments. The reviewers simply state the facts and ask questions, which is a passive approach to reviewing.

In the first review point, the reviewer asks a question about the method used in the paper, which is a neutral way of seeking clarification. In the second review point, the reviewer points out a limitation in the paper's methodology, which is a neutral way of providing feedback. In the third review point, the reviewer suggests that a section of the paper could be restructured, which is a neutral way of providing constructive criticism.

Overall, the review points are all neutral and factual, which is why they receive a score of 0 according to the rubric. The reviewers do not express any form of politeness or lack thereof, which is why the score is not higher or lower. The feedback is purely objective and does not contain any personal opinions or judgments. 

So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a discussion of the methodologies and their implications, rather than a critique of the claims made in the original papers. The reviewer does not make any assertions that require justification or reference to external or internal sources. Instead, the reviewer provides a general overview of the methods and their potential applications. This approach aligns with the score rubric's criteria for a score of 0, as it does not contain any claims that need to be verified. Therefore, the review point does not meet the criteria for a score of 1 or -1. 

The reviewer's comments are factual and do not require any form of verification. They are based on the information provided in the original papers and do not challenge the validity of any claims. The reviewer's focus on the methodologies and their potential applications is a clear indication that the review point does not contain any claims that need to be verified. 

In conclusion, the review point does not contain any claims that need to be verified, and it does not meet the criteria for a score of 1 or -1. The reviewer's comments are factual and do not require any form of verification. Therefore, the review point is not relevant for verifiability in review point.",1,"The review point provided for the paper by Raman et al. is quite specific and directly addresses a particular aspect of the paper. It points out the lack of clarity in the explanation of the method's performance and the absence of a guarantee for the method's performance. This specificity allows the authors to identify which parts of the paper are being addressed by the review point. The authors can be confident that the review point is focusing on the method's performance and the lack of a guarantee for its performance.

On the other hand, the review point for the paper by Mohamed et al. is not as specific. It mentions the need for improvement in the writing and the need for clarity in certain sections, but it does not specify which sections need improvement or what aspects of the writing need to be clarified. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The authors may be unsure about which parts of the paper are being criticized or what aspects of the writing need to be improved.

In the case of the review point for the paper by Rudolph et al., it is also not as specific. It mentions the need for additional experiments to validate the hypothesis presented in the paper, but it does not specify which hypothesis needs to be validated or what aspects of the hypothesis need to be tested. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The authors may be unsure about which hypothesis needs to be validated or what aspects of the hypothesis need to be tested.

In conclusion, the review point for the paper by Raman et al. is more specific and directly addresses a particular aspect of the paper, making it easier for the authors to identify which parts of the paper are being addressed by the review point. The review points for the papers by Mohamed et al. and Rudolph et al. are less specific and do not clearly identify which parts of the papers are being addressed by the review points. Therefore, based on the score rubric, the review point for the paper by Raman et al. is more effective in addressing the specificity aspect of the evaluation criteria."
4011,NIPS_2020_182,"1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion. 3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework. 4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description. 5. There are many typos in this paper, e.g., - Line 40: “and and reture” - Line 127 and Line 142: “Appendix ??”","4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a discrepancy in the number of source sets used for averaging MAE, without suggesting any specific actions or experiments that could be taken to resolve this inconsistency. The review point does not offer any implicit actionable comments that could be inferred from the text, nor does it mention any limitations without providing any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point in question is not particularly polite or constructive. It points out a discrepancy in the number of source sets used to calculate MAE, which is a valid point. However, the tone used to convey this discrepancy is somewhat neutral and lacks the helpful or encouraging tone that would be expected from a review point. The reviewer does not offer any suggestions or constructive criticism to help the authors improve their work. Instead, the reviewer simply states the problem without providing any guidance or support. This approach does not foster a positive or constructive dialogue between the reviewer and the authors. Therefore, based on the score rubric, the tone of the review point is neutral, factual, or passive, which is why it would receive a score of 0.",-1,"The review point in question presents a discrepancy in the number of source sets used for the MAE calculation. The claim that 1,000 source sets are generated is contradicted by the subsequent statement that MAE is averaged over only 100 source sets. This inconsistency is a clear example of a claim that is not verified, as it lacks any justification or explanation. The review point does not provide any logical deductions or references to external or internal sources that could support the claim. Therefore, it fails to meet the criteria for verifiability as outlined in the score rubric. The lack of verification in this review point is a significant issue, as it undermines the credibility of the analysis and the conclusions drawn from it. So the overall score is -1.",-1,"The review point in question is a clear example of a lack of specificity. It points out a discrepancy in the number of source sets used in the paper, but it fails to specify which part of the paper is being addressed. The review point does not provide any context or explanation as to why the discrepancy is significant or how it affects the overall paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors are left with no idea about the paper part that is being addressed by the review point comment. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and it would be scored as -1."
475,ICLR_2022_3205,"Weaknesses
This method trades one intractible problem for another: it requires the learning of cross-values v e ′ ( x t ; e )
for all pairs of possible environments e , e ′
. It is not clear that this will be an improvement when scaling up.
At a few points the paper introduces approximations, but the gap to the true value and the implications of these approximations are not made completely clear to me. The authors should be more precise about the tradeoffs and costs of the methods they propose, both in terms of accuracy and computational cost.
On page 6, it claims that estimating v c
according to samples will lead to Thompson sampling-like behavior, which might lead to better exploration. This seems a bit facetious given that this paper attempts to find a Bayes-optimal policy and explicitly points out the weaknesses of Thompson sampling in an earlier section.
Not scaled to larger domains, but this is understandable.
Questions and minor comments
Is the belief state conditioning the policy also supposed to change with time τ
? As written it looks like the optimal Bayes-adaptive policy conditions on one sampled belief about the environment and then plays without updating that belief.
It is not intuitive to me how it is possible to estimate v f
, despite the Bellman equation written in Eq. 12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct?
I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into \paragraphs — it's currently a huge wall of text.","12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct? I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into \paragraphs — it's currently a huge wall of text.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states that the update should integrate over all possible environments and suggests that the bolded sections in page 6 should be broken out into paragraphs. However, it does not offer any specific actions or experiments that the authors should undertake. The review point also does not provide any implicit actionable comments that the authors can infer. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they provide any form of encouragement or helpfulness. The reviewer simply states the facts about the update and the formatting of the text. The reviewer also makes a suggestion about the formatting of the text, which is a constructive approach. However, the tone of the review point is not particularly engaging or encouraging, which is why it does not score higher on the politeness scale. The reviewer's tone is more passive than anything else, which is why it fits the score of 0. The reviewer's feedback is focused on the technical aspects of the update and the formatting of the text, without any personal or emotional undertones. So the overall score is 0.",0,"The review point in question does not make any claims, nor does it require verification. It is a critique of the format of the document, specifically the presentation of information in the bolded sections of page 6. The reviewer suggests that these sections should be broken out into paragraphs, which is a matter of style and readability rather than a claim that needs to be verified. The reviewer's comment about the update needing to integrate over all possible environments is a general observation about the nature of software updates, not a claim that requires justification or verification. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. 

The reviewer's feedback is focused on the presentation of information and does not address the content or claims made in the document. As such, the review point does not contain any claims that need to be verified, nor does it include any factual statements that do not require verification. The feedback is purely subjective and does not engage with the content of the document in a way that would require verification. 

In conclusion, the review point does not contain any claims that need to be verified, nor does it include any factual statements that do not require verification. The feedback is focused on the presentation of information and does not engage with the content of the document in a way that would require verification. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric.",-1,"The review point provided is somewhat vague and does not clearly specify which part of the paper is being addressed. The reviewer mentions the need for the update to integrate over all possible environments, but does not provide a clear connection to any specific section of the paper. Additionally, the reviewer suggests that the bolded sections in page 6 should be broken out into paragraphs, but does not explain why this is necessary or how it would improve the paper.

The review point also makes a general comment about the paper being difficult to follow, but does not provide specific examples or suggestions for improvement. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point.

In contrast, a review point with a higher score would provide clear and specific feedback, such as pointing out a particular section where the argument is weak or where the data is not well-explained. This would allow the authors to understand exactly what needs to be improved and how to address the issues raised.

Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors would not be confident about which parts are being addressed by the review point. The lack of specificity in the review point is a significant drawback, and it does not meet the criteria for a higher score."
2606,NIPS_2019_1408,"Weaknesses: - The paper is not that original given the amount of work in learning multimodal generative models:   â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior.   â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths: - The experimental results are nice. The paper claims that their MMVAE modal fulfills all four criteria including (1) latent variables that decompose into shared and private subspaces, (2) be able to generate data across all modalities, (3) be able to generate data across individual modalities, and (4) improve discriminative performance in each modality by leveraging related data from other modalities. Let's look at each of these 4 in detail:   â (1) Yes, their model does indeed learn factorized variables which can be shown by good conditional generation on MNIST+SVHN dataset.   â (2) Yes, joint generation (which I assume to mean generation from a single modality) is performed on vision -> vision and language -> language for CUB,   â (3) Yes, conditional generation can be performed on CUB via language -> vision and vice versa.  Weaknesses: - (continuing on whether the model does indeed achieve the 4 properties that the authors describe)   â (3 continued) However, it is unclear how significant the performance is for both 2) and 3) since the authors report no comparisons with existing generative models, even simple ones such as a conditional VAE from language to vision. In other words, what if I forgo with the complicated MoE VAE, and all the components of the proposed model, and simply use a conditional VAE from language to vision. There are many ablation studies that are missing from the paper especially since the model is so complicated.   â (4) The authors have not seemed to perform extensive experiments for this criteria since they only report the performance of a simple linear classifier on top of the latent variables. There has been much work in learning discriminative models for multimodal data involving aligning or fusing language and vision spaces. Just to name a few involving language and vision:     - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016     - DeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013 Therefore, it is important to justify why I should use this MMVAE model when there is a lot of existing work on fusing multimodal data for prediction. ****************************Clarity**************************** Strengths: - The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information. - The table in Figure 2 nicely summarizes some of the existing works in multimodal learning and whether they fulfill the 4 criteria that the authors have pointed out to be important. Weaknesses: - Given the authors' great job in setting up the paper via Figure 1, Figure 2, and the introduction, I was rather disappointed that section 2 did not continue on this clear flow. To begin, a model diagram/schematic at the beginning of section 2 would have helped a lot. Ideally, such a model diagram could closely resemble Figure 2 where you have already set up a nice 'Venn Diagram' of multimodal information. Given this, your model basically assigns latent variables to each of the information overlapping spaces as well as arrows (neural network layers) as the inference and generation path from the variables to observed data. Showing such a detailed model diagram in an 'expanded' or 'more detailed' version of Figure 2 would be extremely helpful in understanding the notation (which there are a lot), how MMVAE accomplishes all 4 properties, as well as the inference and generation paths in MMVAE. - Unfortunately, the table in Figure 2 it is not super complete given the amount of work that has been done in latent factorization (e.g. Learning Factorized Multimodal Representations, ICLR 2019) and purely discriminative multimodal fusion (i.e. point d on synergy) - There are a few typos and stylistic issues: 1. line 18: ""Given the lack explicit labels availableâ -> âGiven the lack of explicit labels availableâ 2. line 19: âcan provided importantâ -> âcan provide importantâ 3. line 25: âbetween (Yildirim, 2014) themâ -> âbetween them (Yildirim, 2014)â 4. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite interesting. - The paper did a commendable job in attempting to perform experiments to justify the 4 properties they outlined in the introduction. - I can see future practitioners using the variational MoE layers for encoding multimodal data, especially when there is missing multimodal data. Weaknesses: - That being said, there are some important concerns especially regarding the utility of the model as compared to existing work. In particular, there are some statements in the model description where it would be nice to have some experimental results in order to convince the reader that this model compares favorably with existing work: 1. line 113: You set \alpha_m uniformly to be 1/M which implies that the contributions from all modalities are the same. However, works in multimodal fusion have shown that dynamically weighting the modalities is quite important because 1) modalities might contain noise or uncertain information, 2) different modalities contribute differently to the prediction (e.g. in a video when a speaker is not saying anything then their visual behaviors are more indicative than their speech or language behaviors). Recent works therefore study, for example, gated attentions (e.g. Gated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 or Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning, ICMI 2017) to learn these weights. How does your model compare to this line of related work, and can your model be modified to take advantage of these fusion methods? 2. line 145-146: ""We prefer the IWAE objective over the standard ELBO objective not just for the fact that it estimates a tighter bound, but also for the properties of the posterior when computing the multi-sample estimate."" -> Do you have experimental results that back this up? How significant is the difference? 3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.","- The paper is not that original given the amount of work in learning multimodal generative models: â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior. â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments that the authors should make. It merely points out the similarities between the paper and other works without suggesting any specific actions or improvements that could be made. The review point also does not include any implicit actionable comments that the authors can infer after reading the review point. It simply states the limitations of the paper without offering any guidance on how to address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not polite and comes across as aggressive. The reviewer uses phrases such as ""not that original"" and ""very similar to the motivation in the paper by Tsai et al."" which can be perceived as dismissive and disrespectful. The reviewer also fails to provide constructive feedback or suggestions for improvement, which is a key aspect of a polite review. Instead, the reviewer simply states that the paper should have cited and compared with another paper, without offering any specific insights or recommendations. This approach does not contribute to the development of the paper or the improvement of the authors' work. Therefore, based on the score rubric, the tone of the review point is not helpful, encouraging, or neutral, but rather rude and disrespectful. So the overall score is -1.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the paper is not original and compares it to other works without providing any logical deductions or references to support these claims. The review point does not make any claims that could be verified, nor does it provide any justification or explanation for the claims it makes. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither supports its claims with logical deductions nor references external or internal sources to justify its claims. Hence, the score is 0.",1,"The review point provided is quite specific and directly addresses the originality of the paper in relation to existing work in the field of multimodal generative models. It clearly identifies the paper's reliance on the work of Wu and Goodman (2018) and its similarity to the paper by Tsai et al. (ICLR 2019). This specificity allows the authors to pinpoint exactly which parts of their paper are being critiqued, thus providing a clear direction for improvement. The review point also offers a direct comparison with the cited works, which can help the authors understand the context of their paper in relation to the broader field. This level of specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to confidently identify the parts of their paper that are being addressed by the review point. 

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of which parts of their paper are being addressed. It would leave the authors uncertain about the exact aspects of their work that are being critiqued. This lack of specificity would not meet the criteria for a score of 1, as it would not allow the authors to confidently identify the parts of their paper that are being addressed by the review point. 

Therefore, based on the score rubric's criteria for specificity, the review point provided is of high quality and would be rated as a score of 1."
4349,NIPS_2020_1636,"While the general idea of the paper is appealing and has been evaluated extensively, the presentation of the methodology is lacking in clarity at times. After reading section 3, some issues could have been addressed more clearly: • Regarding line 171/172: what do the authors mean by “regret reaches the plateau”? Can this be quantified? • If the tree is constructed as described, it is questionable that the leftmost leave is actually the ‘best’ leave. Can this be shown? • How is the problem treated that SVM can lead to many distinct areas in the described methodology? Consider the case of the 1D sine function and we have data points only at increments of pi. K-means would result in two clusters, i.e., the points with values +1 and -1, respectively. Then, SVM would potentially cluster the domain in the two classes resulting in alternating regions for each class. What would be the resulting domain for TuRBO then? In the very beginning, the authors mention that only deterministic objective functions are considered. It is not clear how this statement fits to the the main result: the optimization of policies on the MuJoCo tasks which are known to be inherently stochastic objectives. Does LA-MCTS depend on the deterministic assumption? If yes: why does it work well in practice on stochastic functions, and if not: Why assume it then? The empirical performance of LA-MCTS is impressive. However, the method combines many different building blocks and as such introduces many additional hyperparameters. Though an ablation study was performed, the performance of the method depends drastically on the choice of hyperparameters. As such, the practicability of the approach is limited as an additional layer of parameters needs to be tuned in addition to the BO parameters. It is not exactly clear, why the authors call their method ‘latent actions’ as these are just the decision boundaries from the SVM classifier. No theoretical work is presented. ------------------------------ After reading the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments. Tree construction: Being the best node in expectation is something different then being the best node. This should be made more clear in the main paper. Further, Figure 10 does not really help to make this issue more clear as for example the evaluated points are missing in the plot. How's the initial purple region selected when no data is available? Also, using a contour-plot to visualize the objective function would help to understand the figure better. Deterministic assumption: if no component depends on being deterministic, than I'd highly recommend removing this from the main paper in the beginning. Also, just using a sample mean of 5 rollouts does not lead to a deterministic function but just reduces the variance by a factor of 5, which can still be relatively high for RL tasks especially as the outcome does not necessarily follow a uni-modal distribution. Also, please make the use of multiple rollouts more transparent as this simplifies the RL problem drastically. Minor: Appendix A.1: Hit-and-Run and Gibbs sampling do not require the region to be a convex polytope. Overall: The approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference.","• How is the problem treated that SVM can lead to many distinct areas in the described methodology? Consider the case of the 1D sine function and we have data points only at increments of pi. K-means would result in two clusters, i.e., the points with values +1 and -1, respectively. Then, SVM would potentially cluster the domain in the two classes resulting in alternating regions for each class. What would be the resulting domain for TuRBO then? In the very beginning, the authors mention that only deterministic objective functions are considered. It is not clear how this statement fits to the the main result: the optimization of policies on the MuJoCo tasks which are known to be inherently stochastic objectives. Does LA-MCTS depend on the deterministic assumption? If yes: why does it work well in practice on stochastic functions, and if not: Why assume it then? The empirical performance of LA-MCTS is impressive. However, the method combines many different building blocks and as such introduces many additional hyperparameters. Though an ablation study was performed, the performance of the method depends drastically on the choice of hyperparameters. As such, the practicability of the approach is limited as an additional layer of parameters needs to be tuned in addition to the BO parameters. It is not exactly clear, why the authors call their method ‘latent actions’ as these are just the decision boundaries from the SVM classifier. No theoretical work is presented. ------------------------------ After reading the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments. Tree construction: Being the best node in expectation is something different then being the best node. This should be made more clear in the main paper. Further, Figure 10 does not really help to make this issue more clear as for example the evaluated points are missing in the plot. How's the initial purple region selected when no data is available? Also, using a contour-plot to visualize the objective function would help to understand the figure better. Deterministic assumption: if no component depends on being deterministic, than I'd highly recommend removing this from the main paper in the beginning. Also, just using a sample mean of 5 rollouts does not lead to a deterministic function but just reduces the variance by a factor of 5, which can still be relatively high for RL tasks especially as the outcome does not necessarily follow a uni-modal distribution. Also, please make the use of multiple rollouts more transparent as this simplifies the RL problem drastically. Minor: Appendix A.1: Hit-and-Run and Gibbs sampling do not require the region to be a convex polytope. Overall: The approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the issues and limitations of the paper without suggesting any specific actions that could be taken to address these issues. The review point also lacks any implicit actionable comments that could be inferred by the authors. It does not provide any guidance on how to improve the paper or how to better understand the issues raised. The review point is more of a critique rather than a constructive feedback that could help the authors in improving their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is quite critical and lacks the necessary politeness expected in a review. The tone is aggressive and disrespectful, with phrases such as ""the approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference"" and ""the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments."" These statements are not only unhelpful but also dismissive of the authors' efforts. The reviewer also fails to provide constructive feedback, instead focusing on the perceived shortcomings of the paper. The lack of encouragement and the absence of any positive reinforcement further contribute to the negative tone of the review. The reviewer's approach does not align with the score rubric's criteria for politeness, which emphasizes helpfulness, encouragement, and a neutral, factual, or passive tone. Therefore, the review point does not meet the criteria for a score of 1, and instead, it would be more appropriate to score it as -1.",-1,"The review point contains several claims that are not verified. For instance, the claim about the deterministic assumption in the paper is not justified. The reviewer suggests removing this assumption from the paper, but does not provide a logical explanation or reference to support this claim. Similarly, the reviewer questions the selection of the purple region in the paper, but does not offer any evidence or reasoning to back up this claim. The reviewer also criticizes the use of multiple rollouts in the paper, but fails to explain why this simplifies the RL problem or how it affects the deterministic nature of the objective function. The reviewer's comments on the tree construction and the use of a contour-plot are also not supported by any logical deductions or references. The reviewer's concerns about the ablation study and the choice of hyperparameters are valid, but they do not provide any evidence or reasoning to support their claims. Overall, the review point lacks verifiable claims and does not provide any logical explanations or references to support its claims. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric.",1,"The review point provided is quite specific and directly addresses several key points in the paper. It clearly identifies the areas where the authors' assumptions and methodologies are being questioned. The review point also provides a clear direction for the authors to improve their paper, such as clarifying the deterministic assumption and the use of multiple rollouts. It also suggests a more transparent approach to the use of hyperparameters in the LA-MCTS method. The feedback is well-structured and directly addresses the concerns raised in the review point. The authors can be confident that they understand which parts of the paper are being addressed by the review point. The review point is also well-aligned with the score rubric, as it provides a clear and specific critique of the paper. Therefore, the score is 1."
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.",3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of clarity in the experiment details and the potential implications of this lack of clarity. The reviewer does not suggest any specific actions or experiments that the authors should undertake to address the issues raised. Instead, the reviewer leaves the authors to deduce what needs to be done, which is not helpful in terms of actionability.

The review point does not provide any implicit actionable comments either. It does not give any hints or suggestions that the authors can infer from the review point. The reviewer simply states the limitations of the paper without offering any guidance on how to improve the paper or address the issues raised.

In conclusion, the review point does not meet the criteria for actionability as it neither provides explicit actionable comments nor does it offer implicit actionable comments. Therefore, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The tone is purely informative, focusing on the lack of detail in the experiment description and the potential implications of this on the robustness of the adversarial training. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a key aspect of maintaining a neutral tone. The reviewer's focus on the specifics of the experiment and its potential impact on the paper's robustness is a clear indication of a neutral and factual approach. Therefore, the review point is in line with the score of 0, as it does not exhibit any form of politeness or lack thereof.",0,"The review point in question does not make any claims, but rather poses questions about the experiment details and their implications on the adversarial robustness. It does not provide any logical deductions or references to external or internal sources that could verify the claims. The review point is more of a speculation on the potential weaknesses of the adversarial training method, rather than a claim that needs to be verified. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims or factual statements that do not need verifications. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions the possibility of poisoned data and the potential impact on adversarial training and backdoor attack robustness, but it does not specify where in the paper these issues might be found. The review point also suggests that a figure showing the trade-off under this setting might be missing, but it does not provide any concrete information about the paper's content or structure.

The authors would likely be unsure about which parts of the paper are being addressed by the review point. The lack of specificity in the review point does not allow the authors to confidently identify the relevant sections of their paper. Instead, the authors would need to guess which parts of the paper are being addressed, which is not an ideal situation for a review process.

In conclusion, the review point lacks the necessary specificity to effectively address the paper's content. It does not provide clear guidance for the authors to improve their work, and it fails to identify the specific parts of the paper that need attention. Therefore, the review point does not meet the criteria for a score of 1, as it does not allow the authors to confidently identify the relevant sections of their paper."
4450,NIPS_2020_1451,"1. Unlike the works HaoChen and Sra and Nagaraj et.al, this work uses the fact that all component functions f_i are mu strongly convex. 2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples. 3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.",3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the implications of the finite sigma value without suggesting any specific actions or experiments that could be undertaken. The review point also does not include any implicit actionable comments that could be inferred by the authors. It simply points out the limitations of the work without offering any guidance on how to address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor encouraging, but rather a straightforward statement of the implications of the finite sigma. The review point does not contain any elements of politeness or rudeness, and it is delivered in a neutral manner. It is a factual observation about the smoothness property of the functions f_i and its implication on the gradients. The review point does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the implications of a finite sigma in the context of function gradients. However, it fails to provide any form of verification or justification for this claim. The claim is made without any supporting logical deductions or references to external or internal sources. This lack of verification and justification is a clear deviation from the score rubric's requirement for claims to be supported by logical deductions and internal or external sources. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains a claim that is not verified, which is why it would receive a score of -1 according to the rubric.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It clearly states that the quantity sigma being finite implies that all the gradients are finite, which is a direct implication of the smoothness property of the functions f_i. This specificity allows the authors to pinpoint exactly which part of the paper is being addressed, and they can be confident that the review point is discussing the mathematical properties of the functions used in the paper. The review point does not leave any ambiguity about the paper part being addressed, and it is clear that the authors can identify the relevant sections of the paper. Therefore, based on the score rubric, the review point is highly specific and directly addresses a particular aspect of the paper, which aligns with a score of 1."
5331,NIPS_2020_790,"- While the theoretical bounds are nice, no algorithmic results for efficiently estimating the CRS model is given. I suppose that this all relies directly on the machinery developed for CDM in [48]. - I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix. - One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution. - Given the parameter scaling of the model (quadratic in the number of items) it might not be easily applicable in practice. In fact I find the simulation results confusing. It is not clear why CRS has such nonlinear performance. I do not find the explanation on line 522-527 satisfactory.","- One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a limitation of the theorems without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, nor does it give any explicit or implicit actions that should be done. It simply states the limitation without offering any guidance on how to address it. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the limitation of the theorems and suggests that the contribution would be strengthened by addressing this limitation. The tone is respectful and the feedback is helpful, encouraging the authors to consider the practical implications of their work. The reviewer's approach is encouraging and supportive, which is essential in a review context. The feedback is also specific, pointing out the exact limitation and suggesting a way to improve the contribution. This specificity and the constructive tone make the review point highly effective in providing feedback to the authors. So the overall score is 1.",-1,"The review point provided makes a claim about the limitations of the theorems in the paper, stating that they only provide guarantees for datasets of full rankings. This claim is not verified, as it lacks any logical deductions or references to external or internal sources that could support the claim. The review point does not provide any justification or explanation as to why the claim is made, nor does it offer any evidence or reasoning to back up the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. The claim is not supported by logical deductions or internal or external sources, and it misses a justification, explanation, or reference. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses a limitation of the paper. It points out that the theorems in the paper only guarantee performance for datasets with full rankings, and it suggests that the paper would be strengthened by providing guarantees for datasets with partial rankings. This specificity makes it clear to the authors which part of the paper is being addressed by the review point. The authors can confidently identify that the review point is discussing the limitations of the theorems in the paper and their applicability to different types of datasets. Therefore, the review point meets the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed and the authors can be confident about it."
811,ICLR_2021_1181,"Weaknesses
1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?",2.How fast does this algorithm run or train compared with other baselines?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments. It merely states the limitations of the algorithm without suggesting any specific steps or experiments that could be done to improve the algorithm. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point does not suggest any improvements or modifications to the algorithm, nor does it provide any insights into how the algorithm could be optimized. It also fails to mention any specific limitations of the algorithm that could be addressed through actionable steps. 

In conclusion, the review point does not provide any actionable comments or suggestions, which is why it receives a score of -1 according to the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer asks questions and provides observations without any personal bias or emotional undertones. The feedback is focused on the content of the paper and does not make any personal judgments about the authors or their work. The reviewer's tone is consistent throughout the feedback, maintaining a neutral stance. The feedback is also constructive, offering suggestions for improvement without being overly critical or overly positive. The reviewer's approach is professional and objective, which is the expected standard for a reviewer. Therefore, the review point is in line with the score of 0 in the rubric.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to external or internal sources. The review point simply states that the algorithm runs or trains faster compared to other baselines, which is a straightforward comparison without any need for verification. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the performance of the algorithm in comparison to other baselines. It clearly identifies the aspect of the paper that is being evaluated, which is the speed of the algorithm. The review point is well-structured and directly relates to the paper's content, making it easy for the authors to identify which parts of the paper are being addressed. The review point also provides a clear direction for the authors to improve their paper, which is to compare the algorithm's performance with other baselines. This specificity in the review point aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, the review points provided in the score rubric's examples for a score of -1 are vague and do not clearly identify the specific aspects of the paper that need improvement. The review points do not provide any concrete direction for the authors to follow, which would leave the authors unsure about which parts of the paper are being addressed. This lack of specificity in the review points is why they would receive a score of -1 according to the score rubric. 

Therefore, based on the score rubric's criteria, the review point provided is more specific and directly addresses the performance of the algorithm, making it easier for the authors to identify and improve the relevant parts of their paper."
275,ICLR_2022_1955,"weaknesses ablation of representation is missing: It could be argued that the most gains are from the new GNN architecture. It could be instructive to try out the same model with the same architecture that G2G, GLN, or NeuralSym uses. Or maybe with DRGAT, G2G, GLN, or NeuralSym would also perform better?
baselines is missing: I would suggest to add
Sacha et al - MEGAN model https://arxiv.org/abs/2006.15426
Sun et al https://arxiv.org/abs/2007.13437
Seidl et al https://arxiv.org/abs/2104.03279 to the results table, which outperforms SemiRetro in top10 accuracy.
notes: In the introduction, the authors write: Fortunately, with the rapid accumulation of chemical data, machine learning is promising to solve this problem (Szymkuc et al., 2016; Coley et al., 2018; Segler et al., 2018). - However, Szymkuc et al 2016 argue against the use of data-driven approaches, so I would suggest to not to cite them in this context.","- However, Szymkuc et al 2016 argue against the use of data-driven approaches, so I would suggest to not to cite them in this context.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states a position against the use of data-driven approaches without providing any constructive feedback or suggestions for improvement. The review point does not give any hint or direct actionable comments that the authors should make, nor does it include any implicit actionable comments that the authors can infer after reading the review point. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the expected tone of a review. It is dismissive and discourages the citation of a specific source, which is not a constructive approach. The tone is not neutral or factual, but rather aggressive and disrespectful. This is evident in the phrase ""I would suggest to not to cite them in this context,"" which is not a polite suggestion but rather a directive. The review point does not provide any constructive feedback or suggestions for improvement, which is a key aspect of a review. Instead, it focuses on a negative aspect of the paper without offering any positive feedback or constructive criticism. This approach is not helpful or encouraging, and it does not meet the criteria for a score of 1. Therefore, based on the score rubric, the tone of the review point is rude and disrespectful, and it does not fit well with the other two scores. So the overall score is -1.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely suggests not citing a particular source, which is not a claim and does not require any form of justification or explanation. The review point does not make any logical deductions or reference any external or internal sources, which are key components of a verifiable claim. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and is not relevant to the task at hand. So the overall score is 0.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The authors are left with uncertainty about the specific aspects of the paper that the review point is referring to. The review point does not provide any concrete examples or suggestions for improvement, which would have helped the authors to understand and address the issues raised. Furthermore, the review point does not engage with the content of the paper in a meaningful way, instead offering a generalized critique without any specific details. This lack of specificity makes it difficult for the authors to identify and rectify the issues mentioned in the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and clarity, and thus would receive a score of -1."
2316,ACL_2017_216_review.json,"Weaknesses: 1. Compared to Balikas COLING16's work, the paper has a weaker visualization (Fig 5), which makes us doubt about the actual segmenting and assigning results of document. It could be more convincing to give a longer exemplar and make color assignment consistent with topics listed in Figure 4. 
2. Since the model is more flexible than that of Balikas COLING16, it may be underfitting, could you please explain this more?
- General Discussion: The paper is well written and structured. The intuition introduced in the Abstract and again exemplified in the Introduction is quite convincing. The experiments are of a full range, solid, and achieves better quantitative results against previous works. If the visualization part is stronger, or explained why less powerful visualization, it will be more confident. Another concern is about computation efficiency, since the seminal LDA work proposed to use Variational Inference which is faster during training compared to MCMC, we wish to see the author’s future development. ","2. Since the model is more flexible than that of Balikas COLING16, it may be underfitting, could you please explain this more?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the model may be underfitting without suggesting any specific actions to address this issue. The review point also lacks any implicit actionable comments that could be inferred by the authors. It does not provide any guidance on how to improve the model or what specific limitations need to be addressed. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct actions nor provides implicit actions that can be inferred. So the overall score is -1.",1,"The review point in question is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the flexibility of the model but expresses concern about potential underfitting. This concern is presented in a respectful manner, inviting further explanation rather than making accusations. The reviewer's tone is helpful and encouraging, which is in line with the rubric's definition of a score of 1. The reviewer's language is polite and the feedback is constructive, offering a pathway for improvement without being aggressive or disrespectful. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1.",-1,"The review point in question makes a claim about the model's flexibility and potential underfitting. However, it fails to provide any verification or justification for this claim. The reviewer does not offer any logical deductions or references to external or internal sources that could support the claim. This lack of verification and justification is a clear deviation from the score rubric's requirement for claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The review point is vague and does not provide any concrete examples or specific details that would help the authors understand which parts of the paper are being discussed. The lack of specificity in the review point makes it difficult for the authors to identify the exact sections of the paper that are being addressed. This lack of specificity is in line with the score rubric's criteria for a score of -1, as it does not allow the authors to confidently identify the paper parts being addressed by the review point. Therefore, the review point does not meet the criteria for a higher score."
2396,NIPS_2021_311,"Weaknesses - The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- What happens if a too small corpus is chosen? Can this be detected?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the chosen corpus without suggesting any specific actions to address these limitations. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the authors are left without any clear direction on what needs to be done. This lack of actionability is why the review point receives a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, which is in line with the neutral tone expected in a review. The reviewer simply states the facts and asks questions, which is a passive approach to reviewing. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a missed opportunity to be helpful or encouraging. The reviewer also does not express any disrespect or aggression, which is a positive aspect of the review. However, the reviewer could have been more engaging and provided more detailed feedback to enhance the quality of the review. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It is a hypothetical scenario that is not backed by any specific evidence or data. The question is more of a thought experiment rather than a claim that requires justification or verification. It does not make any assertions that could be validated or disproven, nor does it reference any sources or data that could be used to support or refute the hypothetical scenario. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a general inquiry rather than a claim that needs to be verified. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer a clear direction for the authors to follow. It lacks specificity and does not address a particular part of the paper. The authors are left with no clear idea of what needs to be improved or clarified. The review point does not provide any concrete examples or suggestions that would help the authors to identify the specific parts of the paper that need attention. This lack of specificity makes it difficult for the authors to understand what aspects of their paper are being critiqued. As a result, the authors are left with a sense of uncertainty about the paper's content and the reviewer's expectations. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.","- The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the confusion in the language used and the lack of definition for certain key concepts. While it does suggest that the authors might have intended to describe ""training the Q_i functions"" instead of ""the agent learns R_AUP,"" it does not provide any concrete steps or suggestions for improvement. Furthermore, the review point does not infer any implicit actionable comments that could guide the authors in their future work. It merely states the limitations without offering any solutions or recommendations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and it does not provide any actionable comments or suggestions. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer's language is neither aggressive nor disrespectful, and it does not contain any encouraging or helpful remarks. The reviewer simply points out the confusion in the language used and the lack of definition for certain key concepts. The tone is neutral, and the reviewer does not make any personal judgments or criticisms. The feedback is focused on the content of the paper and the clarity of the language used. The reviewer's comments are factual and do not contain any subjective opinions or personal attacks. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question contains a claim that the language used in the paper is confusing, specifically the phrase ""the agent learns R_AUP."" The reviewer believes this phrase should have been used to describe ""training the Q_i functions"" corresponding to each R_i. However, the reviewer does not provide any logical deductions or references to support this claim. The reviewer also points out that the concept of ""initial state reachability"" is not defined, but again, no justification or explanation is provided to support this claim.

The review point does not meet the criteria for verifiability as it contains claims that are not verified. The reviewer fails to provide logical explanations or references to support the claims made. This lack of verification and justification is why the review point receives a score of -1. 

In contrast, a review point that meets the criteria for verifiability would contain claims that are supported by logical deductions and internal or external sources. The reviewer would provide a clear justification for why the claims are valid, and the claims would be supported by evidence or references. 

Therefore, based on the score rubric, the review point in question does not meet the criteria for verifiability, as it contains claims that are not verified.",-1,"The review point provided is somewhat vague and does not clearly identify which part of the paper is being addressed. The language used in the review point, such as ""the agent learns R_AUP,"" is confusing and does not align with the intended meaning of ""training the Q_i functions."" This lack of clarity could lead to uncertainty among the authors about which parts of the paper are being discussed. Additionally, the review point mentions the concept of ""initial state reachability"" without providing a clear definition or context, which could further confuse the authors. The review point also fails to address the specificity of the paper's content, which is a key aspect of the evaluation criteria. Therefore, the authors may not be confident about which parts of the paper are being addressed by the review point. 

In contrast, a review point that addresses a specific part of the paper, such as the hypothesis presented in section 5.2.1, would allow the authors to identify the exact section being discussed. This would enable the authors to confidently address the concerns raised in the review point. The review point should also provide a clear and specific critique of the paper's content, such as the lack of guarantee for the performance of the new method, which would help the authors understand the exact nature of the critique. 

Overall, the review point provided lacks the specificity and clarity needed to effectively address the authors' work. It does not provide a clear direction for the authors to improve their paper, and it fails to identify the specific parts of the paper that are being addressed. This lack of specificity and clarity is why the review point would receive a score of -1 according to the score rubric."
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ",- A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the definitions of independence and domination numbers are not formal enough, without suggesting any specific improvements or methods to achieve this. The review point does not provide any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not give any hint or direct actionable comments, and after reading the review point, the authors still do not know or can not deduce what needs to be done. This lack of actionable content is why the review point would receive a score of -1 according to the score rubric. 

In contrast, a review point that provides direct and explicit actionable comments would be more helpful to the authors. It would guide them on what specific changes or experiments they should conduct to improve their work. 

So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they provide any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating their observations and questions without any additional commentary. This is evident in the reviewer's straightforward questions and statements, such as ""The bounds seem really hard to compute"" and ""Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited."" These statements are neutral and do not convey any form of politeness or lack thereof. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely states that the definitions of independence and domination numbers are not formal enough, which is a subjective opinion rather than a claim that needs to be substantiated. The reviewer's difficulty in understanding the concepts does not constitute a claim that needs to be verified. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. The reviewer's statement is a personal observation and does not require any logical deductions or references to external or internal sources to support it. As such, the review point does not contain any claims that need to be verified, and it does not meet the criteria for a score of 1. So the overall score is 0.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which parts of the paper are being addressed. The review point merely states that the definitions of independence and domination numbers are not well-defined, but it does not provide any concrete examples or suggestions for improvement. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being criticized. The review point does not guide the authors on how to improve their definitions or provide any constructive feedback. As a result, the authors are left uncertain about the exact issues with their paper. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and clarity. It fails to make it obvious which part of the paper is being addressed, and the authors are not confident about the paper part that is being addressed by the review point. So the overall score is -1."
2055,ARR_2022_17_review,"- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant. - The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021) rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.  - Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models. - Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) 
Most of my comments are given above. Details : some references have no venue -> please correct this.   misc{hsu2021robust,    title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},    author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},    year={2021},    eprint={2104.01027},    archivePrefix={arXiv},    primaryClass={cs. SD} } ","- Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) Most of my comments are given above. Details : some references have no venue -> please correct this. misc{hsu2021robust, title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training}, author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli}, year={2021}, eprint={2104.01027}, archivePrefix={arXiv}, primaryClass={cs. SD} }",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of fine-tuning in the study and suggests that adaptability is not an interesting feature. However, it does not offer any specific suggestions or recommendations on how to address these issues. The review point also mentions the absence of venue information in some references, but it does not provide any actionable steps to correct this. The lack of actionable comments and the absence of any explicit or implicit actions that should be done make the review point ineffective in guiding the authors on what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is not aligned with the expected tone of a constructive review. The language used is somewhat confrontational and lacks the necessary politeness. The reviewer's comments such as ""adaptability is not an interesting feature?"" and ""some references have no venue -> please correct this"" come across as accusatory and disrespectful. This is not conducive to a helpful and encouraging review. The reviewer's tone is more aggressive than neutral or constructive, which is not in line with the score rubric's criteria for a helpful and encouraging review. Therefore, the review point does not meet the criteria for a score of 1, as it is not helpful or encouraging in tone. 

The reviewer's comments also lack the necessary detail and specificity that would be expected in a constructive review. For example, the reviewer does not provide any constructive feedback on how the paper could be improved or what specific aspects of the paper could be strengthened. Instead, the reviewer makes generalized and negative comments about the paper. This lack of specificity and constructive feedback further detracts from the quality of the review point. 

In conclusion, the review point is not helpful or encouraging in tone, and it lacks the necessary detail and specificity that would be expected in a constructive review. Therefore, the review point does not meet the criteria for a score of 1.",0,"The review point provided does not contain any claims that need to be verified. It merely states that fine-tuning is not considered in the study and questions why this is the case. The review point does not make any claims that could be verified or disproved. It also does not provide any logical deductions or references to support or refute the claims it makes. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains claims that need verification nor does it provide any logical deductions or references to support or refute the claims it makes. Hence, the score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The comment about the lack of fine-tuning in the study is not specific to any particular section or aspect of the paper. It also fails to provide any constructive feedback or suggestions for improvement. The review point does not help the authors to understand which parts of the paper are related to the comment, and it does not give them any confidence about the paper part that is being addressed. The lack of specificity in the review point makes it difficult for the authors to identify the exact issues that need to be addressed. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and clarity. So the overall score is -1."
4011,NIPS_2020_182,"1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion. 3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework. 4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description. 5. There are many typos in this paper, e.g., - Line 40: “and and reture” - Line 127 and Line 142: “Appendix ??”","1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of validation in the utilization of the approximation and the unjustified choice of baselines in different tasks. The review point does not suggest any specific actions that the authors should take to address these issues. It also does not provide any implicit actionable comments that the authors can infer from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The feedback is focused on the content of the paper, discussing the utilization of the approximation and the choice of baselines in the numerical experiments. The reviewer does not express any personal opinions or emotions, which is in line with the neutral tone expected in a review. The feedback is also constructive, providing suggestions for improvement without being overly critical or dismissive. The reviewer's tone is consistent throughout the feedback, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and the tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. So the score is 0.",-1,"The review point in question makes several claims, but it fails to provide any form of verification for these claims. The first claim is about the utilization of the approximation in (10), which is not properly validated. There is no logical explanation or reference to external or internal sources that could support this claim. The second claim is about the choice of different baselines in the numerical experiments. Again, there is no justification or explanation provided for why these baselines were chosen in the specific tasks. The lack of verification for these claims means that the review point does not meet the criteria for verifiability as outlined in the score rubric. Therefore, the review point is not validated and does not provide any logical deductions or references to support its claims. This is why the review point would receive a score of -1 according to the score rubric.",0,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the utilization of the approximation in (10) and the comparison of different baselines in numerical experiments, but it does not specify which sections of the paper these points are related to. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. The authors are left guessing which parts of the paper are being discussed, which does not align with the score rubric's requirement for specificity. Therefore, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. 

In contrast, a review point that would score a 1 would clearly identify which part of the paper is being addressed, such as a specific section or a particular hypothesis. The authors would be confident about which part of the paper is being addressed by the review point. 

The review point also fails to provide a clear rationale for the comparison of different baselines in numerical experiments, which is another aspect of the review point that lacks specificity. The authors are left wondering why certain algorithms were chosen for comparison in different tasks. 

In conclusion, the review point does not meet the criteria for a score of 1 due to its lack of specificity and clear rationale for the comparison of different baselines in numerical experiments."
3644,NIPS_2020_1156,"- Other papers have done somewhat similar things, e.g. Duncker & Sahani (2018) have condition-dependent latents as well as single-trial latents. - The paper is somewhat light on experimental results, and the conclusions are somewhat abstract (on *average* there is less overlap between noise and sensory coding dimensions, but there seems to be substantial trial-to-trial variability) - I don't have any other major concerns besides the typical complications and drawbacks of Gaussian Processes. It would be useful to consider different choices for the GP kernel function. Also more discussion on how to choose parameters like the lengthscale of the kernel would be useful. (In the original GPFA paper the authors place a prior over the lengthscale... Is that done here or is the lengthscale a user-determined constant?)","- The paper is somewhat light on experimental results, and the conclusions are somewhat abstract (on *average* there is less overlap between noise and sensory coding dimensions, but there seems to be substantial trial-to-trial variability) - I don't have any other major concerns besides the typical complications and drawbacks of Gaussian Processes. It would be useful to consider different choices for the GP kernel function. Also more discussion on how to choose parameters like the lengthscale of the kernel would be useful. (In the original GPFA paper the authors place a prior over the lengthscale... Is that done here or is the lengthscale a user-determined constant?)",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of experimental results and the abstractness of the conclusions. The reviewer also mentions the use of Gaussian Processes and the need to consider different choices for the kernel function, but does not provide any specific guidance or suggestions on how to address these issues. Furthermore, the reviewer does not ask any direct questions to the authors, which could have led to more actionable comments. The review point also lacks any implicit actionable comments that could be inferred by the authors. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the paper's strengths, such as the use of Gaussian Processes, while also offering constructive feedback on areas for improvement, such as the experimental results and the choice of kernel function. The tone is respectful and encouraging, with suggestions for enhancing the paper's quality. The reviewer also asks clarifying questions about the lengthscale of the kernel, which shows a genuine interest in understanding the paper's methodology. This approach is helpful and supportive, which is the essence of a score of 1. The reviewer's tone is not aggressive or disrespectful, and it is not neutral or passive, but rather it is constructive and polite. Therefore, based on the score rubric, the review point is of high quality.",0,"The review point provided does not contain any claims that need to be verified. It is a critique of the paper's experimental results and the abstractness of the conclusions. The reviewer also suggests considering different choices for the GP kernel function and discussing the choice of parameters like the lengthscale of the kernel. However, these are not claims that need to be verified, but rather suggestions for improvement. The reviewer does not make any claims that could be verified or disproved, and the feedback does not include any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not meet the criteria for a score of 1. So the overall score is 0.",0,"The review point provided is somewhat vague and does not clearly identify which parts of the paper are being addressed. The reviewer mentions that the paper is ""somewhat light on experimental results"" and that the conclusions are ""somewhat abstract,"" but does not specify which sections of the paper are lacking in these areas. Additionally, the reviewer suggests considering different choices for the GP kernel function and discussing how to choose parameters like the lengthscale, but again, does not specify which sections of the paper would benefit from these considerations. The reviewer also mentions that the authors placed a prior over the lengthscale in the original GPFA paper, but does not clarify whether this is done in the current paper or if the lengthscale is a user-determined constant. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric."
1455,ICLR_2023_4735,"Weaknesses: 1) The contribution of this paper is to exploit the knowledge from the multiple pretrained deep models for VQA, while the practical method is doubtful. First, I am wondering why there are different pretrained models for different datasets. It is better to have a deeper analysis or insight into the fusion mechanism of various pretrained deep models. How the different pretrained models contribute to the quality-aware feature extraction is also valuable to be studied. 2) There are many pretrained models for various tasks, more pretrained models are encouraged to study for VQA. 3) I am still unclear about the motivation of intra-consistency and inter-divisibility loss related to quality assessment.","1) The contribution of this paper is to exploit the knowledge from the multiple pretrained deep models for VQA, while the practical method is doubtful. First, I am wondering why there are different pretrained models for different datasets. It is better to have a deeper analysis or insight into the fusion mechanism of various pretrained deep models. How the different pretrained models contribute to the quality-aware feature extraction is also valuable to be studied.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely questions the practicality of the method and suggests a deeper analysis of the fusion mechanism of various pretrained deep models. However, it does not provide any specific guidance or recommendations on how to improve the method or what aspects to focus on. The review point also lacks any implicit actionable comments that the authors can infer from the review. It only mentions the limitations of the method without suggesting any concrete actions to address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer's questions and comments are focused on the technical aspects of the paper, which is a neutral approach to reviewing. However, the reviewer could have been more constructive by offering suggestions or pointing out specific strengths of the paper. The reviewer's tone is more passive than active, which is why it fits the score of 0. The reviewer's comments are not overly critical or positive, but rather neutral observations. Therefore, the reviewer's tone is neither rude nor overly helpful, but rather neutral and factual.",0,"The review point in question does not make any claims, but rather poses questions and suggests areas for further study. It does not provide any logical deductions or references to external or internal sources that could verify the claims. The review point is more of a critique of the paper's methodology and lacks the necessary elements to be considered a claim. It is also noteworthy that the review point does not contain any factual statements that could be verified. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability. 

The review point's focus on the fusion mechanism of various pretrained deep models and the contribution of different pretrained models to the quality-aware feature extraction is a valid point of critique. However, it does not provide any evidence or reasoning to support its claims. The review point's questions and suggestions for further study are valuable, but they do not constitute claims that need to be verified. 

In conclusion, the review point does not contain any claims, nor does it provide any logical deductions or references to external or internal sources that could verify the claims. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric.",1,"The review point provided is quite specific and directly addresses the contribution of the paper, which is the use of multiple pretrained deep models for VQA. It questions the practicality of the method and the rationale behind the use of different pretrained models for different datasets. The review point also seeks a deeper analysis of the fusion mechanism of these models and their contribution to the quality-aware feature extraction. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the parts of the paper that are related to the review point, as it directly addresses the core aspects of the paper. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
429,ICLR_2022_1745,"Weaknesses Major
The authors have some fundamental issues and misconceptions about the BootstrappedDQN (BootDQN) algorithm. Section 2 mentions that BootDQN addresses exploration by running multiple behavior policies in the environment; if so, then it would be true that the method would have no reason to solve sparse reward tasks a priori as described by the authors. However, the reason for BootDQN's success in exploration is because of posterior sampling [8] and the fact that the ensemble is an approximate posterior over the optimal action-value function of the MDP, conditioned on all agent interactions observed thus far. It is precisely this principle that allows the algorithm to address sparse-reward tasks [2]. The authors' description of Thompson sampling in BootDQN as an ad-hoc heuristic seems incorrect given the rigorous theoretical guarantees that accompany randomized least-squares value-iteration algorithms [3]. The authors are also confused about the use of replay buffers in BootDQN, claiming that data from each head is held separate. There is exactly one replay buffer used in BootDQN and Bernoulli masks are sampled and stored with each transition for implementing the statistical bootstrap. Overall, the connection the authors draw with BootDQN in this work seem rather disingenuous; the proposed algorithm is simply applying ensembles of actor-critic pairs with no connection to the statistical bootstrap, unlike BootDQN. Renaming the algorithm and rephrasing the contribution seem appropriate.
A more critical issue concerning BootDQN and the proposed BHER algorithm is that the former is a purely value-based RL algorithm that maintains a posterior distribution over the optimal action-value function. In contrast, the latter is an off-policy actor-critic algorithm where, naturally, the critic is meant to be an estimate of the action-value function induced by the actor policy. While the empirical results of this paper confirm empirical benefits of this ensembling heuristic (the ablation in Figure 5 shows that this is responsible for most of the BHER performance), the authors have offered no real justification for this ensemble actor-critic algorithm. What is the point of representing epistemic uncertainty over the actor and critic in this manner?
I don't find the so-called counterintuitive prioritization to be counterintuitive at all. It seems natural that hindsight transitions will serve the agent well only when there is little uncertainty in the associated optimal behavior under the relabeled goal. Can the authors explain why sampling based on higher variance seems to still maintain reasonable performance in three of seven environments shown in Figure 5? Minor
In the first paragraph of Section 4.2, the authors describe an instance of Prioritized Experience Replay (PER) [6] where the variance of the critic ensemble is used to prioritize transitions sampled from the replay buffer. They seem to confuse this technique with methods for intrinsic motivation [7] based on curiosity and Random Network Distillation. Clarity Strengths
The authors provide ample details about their experimental setup for reproducibility of their results. Weaknesses Major
Overall, the paper is not well written. There are numerous grammatical errors throughout, genuinely too many for me to sensibly list them all out here. Oftentimes, these errors are missing articles (for example, ""it uses successful trajectories generated by agent as expert demonstrates"") or incorrect phrases (""on the contrast"", ""we inference all the Q-values""). Normally, I wouldn't bother nitpicking at a small handful of these, but there are too many throughout the entire body for what may end up being a published conference paper. Minor
The authors should remove the phrase ""importance sampling"" that is used twice in the paper to, in my reading, talk about the importance of sampled goals, rather than the Monte-Carlo technique of the same name. Originality Strengths
The authors demonstrate a good instinct in examining how other techniques used in deep reinforcement learning might further improve the efficacy of HER. Weaknesses Major
Fundamentally, this paper rests on the idea of using ensembles and prioritized experience replay together, neither of which is new to (deep) reinforcement learning [1,4,5,9]. Though I do not know of any prior work that has explored this specific combination, it would not surprise me if such prior work already exists.
More importantly, there are other options for leveraging such ensembles that have not been addressed in this work [1,4]. Similarly, the authors only consider variance-based prioritization schemes, rather than the traditional prioritization based on TD-error or any recent variants of PER. Demonstrating that the authors' specific choices in the proposed approach are better than these existing approaches to ensembling and PER would dramatically improve what so far seems to be a rather incremental algorithm. Minor
While it is appropriate for the related work section to focus on HER, it should also acknowledge the two fundamental innovations of this paper (ensembles and prioritization schemes) and provide an overview of related work for these areas as well. Significance Strengths
The only positive I can glean from this paper are the empirical results which seem to support the use of ensembling in actor-critic algorithms. Figure 5 shows a marginal drop in performance when the proposed counterintuitive prioritization scheme is not used. That said, it is not clear that this paper advances our understanding of ensembling in deep RL any more than prior work. Weaknesses Major
Given the lack of comparisons mentioned above, it's difficult to assess how impactful the proposed approach will be. With the breadth of existing work on the topic, I'm unconvinced that this will add any novel insights into how practitioners use ensemble methods in reinforcement learning. The prioritization scheme, while slightly interesting on the surface, doesn't seem to be a critical ingredient to the proposed algorithm's success based on the ablation studies shown.
I don't believe any of the experiments have shown results for regular DDPG without the use of HER. Having this baseline in place is important as it communicates the extent to which HER is even necessary for achieving a reasonable level of performance in each of the examined environments. Minor References
Lee, Kimin, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. ""Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning."" In International Conference on Machine Learning, pp. 6131-6141. PMLR, 2021.
Osband, Ian, and Benjamin Van Roy. ""Why is posterior sampling better than optimism for reinforcement learning?."" In International conference on machine learning, pp. 2701-2710. PMLR, 2017.
Osband, Ian, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. ""Deep Exploration via Randomized Value Functions."" J. Mach. Learn. Res. 20, no. 124 (2019): 1-62.
Peer, Oren, Chen Tessler, Nadav Merlis, and Ron Meir. ""Ensemble Bootstrapping for Q-Learning."" arXiv preprint arXiv:2103.00445 (2021).
Saphal, Rohan, Balaraman Ravindran, Dheevatsa Mudigere, Sasikant Avancha, and Bharat Kaul. ""SEERL: Sample Efficient Ensemble Reinforcement Learning."" In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1100-1108. 2021.
Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized Experience Replay."" In ICLR, 2016.
Singh, Satinder, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. ""Intrinsically motivated reinforcement learning: An evolutionary perspective."" IEEE Transactions on Autonomous Mental Development 2, no. 2 (2010): 70-82.
Strens, Malcolm. ""A Bayesian framework for reinforcement learning."" In ICML, vol. 2000, pp. 943-950. 2000.
Wiering, Marco A., and Hado Van Hasselt. ""Ensemble algorithms in reinforcement learning."" IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 38, no. 4 (2008): 930-936.","124 (2019): 1-62. Peer, Oren, Chen Tessler, Nadav Merlis, and Ron Meir. ""Ensemble Bootstrapping for Q-Learning."" arXiv preprint arXiv:2103.00445 (2021). Saphal, Rohan, Balaraman Ravindran, Dheevatsa Mudigere, Sasikant Avancha, and Bharat Kaul. ""SEERL: Sample Efficient Ensemble Reinforcement Learning."" In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1100-1108. 2021. Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized Experience Replay."" In ICLR, 2016. Singh, Satinder, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. ""Intrinsically motivated reinforcement learning: An evolutionary perspective."" IEEE Transactions on Autonomous Mental Development 2, no.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It lacks any specific recommendations or suggestions that the authors could follow to improve their work. The review point merely points out some limitations of the work without offering any solutions or ways to address these limitations. It does not provide any implicit actionable comments that the authors can infer after reading the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any direct and explicit actionable comments that the authors should make. It does not provide any hints or direct actionable comments that would help the authors understand what needs to be done. The review point is also devoid of any implicit actionable comments that the authors can infer after reading the review point. 

In conclusion, the review point does not provide any actionable steps or comments, which is why it would receive a score of -1 according to the score rubric.",0,"The review point provided is a collection of papers and their respective reviews. The tone of the reviews is crucial in determining the score. The reviews are generally neutral, with a few instances of politeness and a few instances of a more critical tone.

For instance, the review of the paper by Saphal et al. is polite and constructive, suggesting improvements and acknowledging the potential of the method. The review of the paper by Schaul et al. is also polite, offering suggestions for improvement.

However, there are instances where the tone is more critical. For example, the review of the paper by Singh et al. is less polite, with phrases like ""This poor quality of experiments and writing doesn’t live up to the level of this conference."" This kind of language is not conducive to a constructive review and could be perceived as disrespectful.

In terms of the score rubric, the reviews are mostly neutral, with a few instances of politeness and a few instances of a more critical tone. The reviews are not aggressive or disrespectful, but they do not consistently demonstrate a helpful or encouraging tone.

Therefore, based on the score rubric, the review point is mostly neutral, with a few instances of politeness and a few instances of a more critical tone. The overall tone is not aggressive or disrespectful, but it does not consistently demonstrate a helpful or encouraging tone. So the score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a collection of references to various papers in the field of reinforcement learning. The review point does not make any claims or assertions about the content of the referenced papers. Instead, it simply lists the papers and their authors, which is a factual statement that does not require any form of verification.

The review point does not contain any logical deductions or justifications that would be necessary to verify any claims. It also does not mention any external or internal sources that could be used to verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified.

In conclusion, the review point is not relevant for verifiability, as it does not contain any claims or assertions that need to be verified. It is a simple listing of papers and authors, which is a factual statement that does not require any form of verification. So the score is 0.",1,"The review point provided is quite specific and directly addresses the content of the paper. It identifies the key aspects of the paper, such as the ensemble bootstrapping for Q-Learning, the sample efficient ensemble reinforcement learning, and the prioritized experience replay. The review point also touches upon the intrinsically motivated reinforcement learning, which is a central theme of the paper. This specificity allows the authors to clearly understand which parts of the paper are being addressed by the review point. The authors can be confident that the review point is focused on the main ideas and methods presented in the paper. Therefore, based on the score rubric, the review point demonstrates a high level of specificity, which is why it would receive a score of 1."
1944,ARR_2022_248_review,"It is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. 
Some grammar issues need to be reviewed. For example, at line 394 in section 4.2:      Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> ""we to ...."" ","3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. Some grammar issues need to be reviewed. For example, at line 394 in section 4.2: Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> ""we to ....""",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a discrepancy in the use of word error rate (WER) in the paper and in the figure, without suggesting any specific changes or improvements. Additionally, the review point identifies a grammatical error but does not provide any guidance on how to correct it. The review point also fails to suggest any implicit actionable comments that the authors can infer from the review. It simply states the limitations of the paper without offering any solutions or recommendations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and does not provide any actionable comments or suggestions. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does it offer any form of encouragement or helpfulness. The reviewer simply points out the inconsistency in the use of word error rate (WER) in the paper and the grammatical error in line 394. The tone is neutral and the feedback is purely factual, without any personal opinion or emotional undertones. The reviewer does not make any judgments about the quality of the paper or the authors' work, which is a key aspect of maintaining a neutral tone. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not make any claims, but rather points out a discrepancy in the use of word error rate (WER) in the paper. The reviewer notes that the authors used (100 - WER) as the vertical axis in Fig. 3, which is not clear and could be misleading. However, the reviewer does not provide any logical deductions or references to support this claim. The reviewer also identifies a grammatical error in the paper, but again, does not offer any justification or explanation for this observation.

The review point does not meet the criteria for verifiability as it lacks any claims that are verified. The reviewer does not provide any logical explanations or references to support the claims made. Therefore, the review point does not fulfill the requirements of the score rubric, which requires claims to be verified with logical deductions or references.

In conclusion, the review point does not contain any claims, or only includes factual statements and claims that do not need verifications. It fails to meet the criteria for verifiability as it does not provide any logical deductions or references to support the claims made. So the score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the use of word error rate (WER) and the vertical axis in Fig. 3, but it does not explain why the authors used (100 - WER) as the vertical axis. This lack of clarity could lead to confusion among the authors. Additionally, the review point points out a grammatical error in line 394, but it does not provide a clear explanation of the error or how it affects the overall paper. The authors may be unsure about the specifics of the review point and how it relates to their work. Therefore, the review point does not meet the criteria for specificity, as it does not make it obvious which part of the paper is being addressed. So the score is -1."
5339,NIPS_2020_1039,"1) It could be better if a more comprehensive comparison of the asymptotic performances among on/off-policy methods. Off policy methods enjoy better sample efficiency at the cost of higher computation burden. Maybe an additional table could be provided in the appendix. Personally, I don’t actually expect a gap between the on/off-policy methods. 2) Although the usage of current f-function in the f-divergence is justified, it will still be interesting to see a comparison if the alternative f-functions were adopted. An empirical comparison would further justify the usage of the author’s current choice. Minor: 1) In eq. 12 (the proof of Lemma 2), in the third from the last line, the integration should be over \mathcal{S} \times \mathcal{S} instead of \mathcal{S} \times \mathcal{A} \times \mathcal{S}. 2) Notations in sec. 8.6 are inconsistent: sampling from \pi is switching back and forth between $a \sim \pi(s)$ and $a \sim \pi(\cdot | s)$. 3) At the end of Algorithm box 1, does J_{\nabla \theta} J_{reg} (\pi_{\theta}) intends to mean that the gradient of \theta on J_{reg}? 4) Can the authors slightly justify the first equation in sec. 3.2? Why is that an equality instead of an inequality?",4) Can the authors slightly justify the first equation in sec. 3.2? Why is that an equality instead of an inequality?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable comments. It merely asks a question about the justification of the first equation in sec. 3.2, without suggesting any specific steps or experiments that could be taken to address the issue. The review point does not offer any guidance on how the authors could improve their work or what they could do to better justify their equation. It also does not provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The reviewer's language is clear and concise, and the feedback is based on the information provided in the paper. The reviewer does not make any assumptions or judgments about the authors or their work, which is a key aspect of maintaining a neutral tone. The feedback is also constructive, as it provides specific suggestions for improvement without being overly critical or dismissive. So the overall score is 0.",-1,"The review point in question asks for a justification for the equality in the first equation of sec. 3.2. This question is relevant to the verifiability of the claim made in the paper. However, the review point does not provide any justification or explanation for why the equation is an equality instead of an inequality. It merely states the question without offering any logical deductions or references to support the claim. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The review point does not contain any claims that are verified, nor does it include factual statements that do not need verifications. Instead, it presents a question that requires a justification, which is not provided. Therefore, based on the score rubric, the review point is not satisfactory in terms of verifiability.",1,"The review point in question is quite specific, focusing on the justification for the first equation in section 3.2. It directly addresses the authors' reasoning for the equality instead of an inequality, which is a clear and targeted aspect of the paper. The review point is well-defined and directly relates to the content of the paper, making it easy for the authors to identify which part of the paper is being addressed. The authors can be confident that the review point is about the first equation in section 3.2, and they can be sure that this is the part of the paper that needs to be justified. The specificity of the review point is in line with the score rubric, which emphasizes the importance of identifying the exact part of the paper that is being addressed. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
352,ICLR_2022_2232,"Weaknesses
The writing should be improved overall. Issues throughout include grammar, misuse of commas, capitalization (as in reference to algorithm 1 on page 6), technical details left out, poor formatting.
There seems to be a lack of novelty in the modeling approach: the same training schemes, DNN model, etc have been developed before, in papers that are cited by this one.
The results are rather limited and do not seem to show a clear advantage over standard techniques.
Some sentences have unclear meaning, e.g.: Page 1 - ‘a manual adaptation and business knowledge are needed…” - ‘its dynamic dimension reflects directly the demand change over the learning steps…” Page 2 - ‘Before we deep dive in the model architecture and present its main components, we should briefly highlight some problem-related concepts’ - the narration is too casual Page 5 - Equation 13 label is cut off - means that violation is given -infinity in what sense? A large floating point number? Page 6 - Training: Sample rollout and greedy rollout - are these ever defined? What is the baseline function used in the REINFORCE algorithm? Page 7 - Google OR-Tools baseline - what are the details of the implementation? Is it the CP-Sat solver? A specialized solver? - What is the meaning of the numbers in Table 1? Is lower better? - Why does RNN-RL appear twice in Table 2 with different results? - Is OR-tools called with a solver timeout? Or it is allowed to run to completion? - The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method?
General questions:
Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case?
Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper?
What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.",- Is OR-tools called with a solver timeout? Or it is allowed to run to completion?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments. It merely states the existence of a solver timeout without suggesting any specific actions or experiments that could be done to address this limitation. The authors are left without any clear direction on how to proceed with their work. The review point does not offer any implicit actionable comments either, which could have been inferred from the review point. It simply mentions the existence of a solver timeout without providing any insight into how this could impact the work or what steps could be taken to mitigate this issue. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the facts about the paper, such as the solver timeout and the potential for the algorithm to generalize. However, the review point could have been more constructive by offering suggestions for improvement or acknowledging the strengths of the paper. The reviewer's tone is passive, which is the default score if the review point does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a straightforward question about the use of OR-tools and their associated timeouts. The question is factual and does not require any logical deductions or references to external or internal sources. It is a simple inquiry about the operational parameters of the OR-tools, which is not a claim that needs to be verified. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain a claim that needs to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims, which is why it would receive a score of 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the use of OR-tools and the solver timeout. It is clear that the review point is focused on a specific part of the paper, and the authors can confidently identify which parts of the paper are related to the review point. The review point is well-defined and does not leave room for ambiguity, which is crucial for the authors to understand and address the feedback.

The review point is also well-structured, with a clear question that directly relates to the paper's content. This makes it easy for the authors to understand what is being asked and to provide a relevant response. The specificity of the review point ensures that the authors are not left guessing about what is being addressed, which is a key aspect of the score rubric.

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of what is being addressed. It would leave them uncertain about which parts of the paper are related to the review point, and they would not be able to confidently identify the relevant sections. This would result in a lower score, as the authors would not be able to confidently address the feedback.

Therefore, based on the score rubric, the review point provided is well-structured, specific, and directly addresses a particular aspect of the paper. It allows the authors to confidently identify the relevant sections of the paper and to provide a relevant response. This is why the review point is given a score of 1."
3459,NIPS_2020_1464,"- significance and novelty: is quite similar to closely related work [13,14] (and maybe PathNet), but does not compare to them. -- The discussion says ""the major difference between [13,14] and ours is that they treat their network depends on the task, which might not be suitable for online continual learning setting."" So the only difference is that they may underperform. This paper either has to compare to [13,14] (especially [13] is very similar and open-source), or show numbers in efficiency (FLOPs, test time, etc) to show that they cannot be used efficiently. - claims and evaluation -- fairness of number of parameters: As far as I can tell, HAT has 7.1M params, A-GEM uses ResNet18 which is 11M params. This paper uses 43M params which is not fair. It is also not fair to say all methods have similar efficiency at test time, since multiple blocks in the same layer can be on. -- The exploration trick improved performance quite a lot. It is great that this paper did an ablation study, but since it has a lot of hyperparameters and design choices (why H^1/2 not H, why sigmoid, how are gamma/kappa/epsilon chosen), it is unclear how the design choices are made and how the hyperparameter was tuned, as this would tweak the balance between learning and remembering. An analysis of sensitivity to hyperparameters is preferred. -- The paper is not *entirely* an online method, since the model is pre-trained on the first task (with multiple epochs) to make NAS training work. --- Was line 253 epochs referring to this? Or did this paper train anything else with more than 1 epoch?","- claims and evaluation -- fairness of number of parameters: As far as I can tell, HAT has 7.1M params, A-GEM uses ResNet18 which is 11M params. This paper uses 43M params which is not fair. It is also not fair to say all methods have similar efficiency at test time, since multiple blocks in the same layer can be on. -- The exploration trick improved performance quite a lot. It is great that this paper did an ablation study, but since it has a lot of hyperparameters and design choices (why H^1/2 not H, why sigmoid, how are gamma/kappa/epsilon chosen), it is unclear how the design choices are made and how the hyperparameter was tuned, as this would tweak the balance between learning and remembering. An analysis of sensitivity to hyperparameters is preferred. -- The paper is not *entirely* an online method, since the model is pre-trained on the first task (with multiple epochs) to make NAS training work. --- Was line 253 epochs referring to this? Or did this paper train anything else with more than 1 epoch?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments. It merely points out the number of parameters used in the paper and the lack of clarity in the design choices and hyperparameter tuning. The review point also questions the fairness of the comparison with other methods, but it does not suggest any specific actions or experiments that could be done to address these concerns. Furthermore, the review point does not provide any implicit actionable comments that could be inferred from the review point. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not polite and comes across as aggressive and disrespectful. The reviewer uses phrases such as ""This paper uses 43M params which is not fair,"" and ""It is also not fair to say all methods have similar efficiency at test time,"" which are confrontational and lack the constructive tone expected in a review. Additionally, the reviewer questions the validity of the paper's methods and the process of hyperparameter tuning, which can be perceived as dismissive and unhelpful. The reviewer also makes assumptions about the paper's training process, which could be seen as an attack on the paper's credibility. The tone of the review point is not helpful or encouraging, and it does not meet the standards of politeness as outlined in the score rubric. Therefore, the review point is not suitable for a constructive review.",-1,"The review point contains several claims, but it fails to provide any verification or justification for these claims. For instance, the claim about the number of parameters in the paper is not supported by any logical deductions or references to external or internal sources. Similarly, the reviewer's concerns about the exploration trick, the ablation study, and the pre-training of the model are not substantiated with any evidence or analysis. The reviewer also questions the number of epochs mentioned in the paper, but again, there is no verification or justification provided. The lack of verification and justification for the claims made in the review point means that it does not meet the criteria for a score of 1. Therefore, based on the score rubric, the review point is not verified and does not provide logical deductions or references to support its claims. So the overall score is -1.",0,"The review point provided is quite specific and directly addresses certain aspects of the paper. It mentions the number of parameters in the paper and compares it to other methods, which is a clear and direct critique. The review point also questions the fairness of the comparison and the efficiency of the methods, which is a relevant point of discussion.

However, the review point lacks specificity in some areas. For instance, it mentions the exploration trick and the ablation study but does not provide a clear explanation of what these are or how they contribute to the paper's findings. Additionally, the review point raises questions about the pre-training of the model and the number of epochs, but it does not provide a clear connection between these points and the paper's content.

The review point also fails to address the sensitivity to hyperparameters, which is a critical aspect of the paper's methodology. This lack of specificity in addressing the hyperparameter tuning process makes it difficult for the authors to identify which parts of the paper are being addressed by the review point.

In conclusion, while the review point is specific in some areas, it lacks specificity in others, making it difficult for the authors to fully understand and address the points raised. Therefore, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed."
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.","2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the existence of a trade-off without suggesting any specific actions or experiments that could be done to address the issue. The review point also lacks any implicit actionable comments that could be inferred from the text. It simply points out the existence of a trade-off without offering any guidance on how to approach it. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply states the lack of a curve to show the trade-off, which is a factual observation. It does not provide any constructive feedback or suggestions for improvement. The review point is also not overly critical or dismissive, which is why it does not fall into the category of being rude or disrespectful. It is a straightforward observation that the trade-off is not clearly demonstrated in the paper. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not make any claims, nor does it provide any logical deductions or references to support any potential claims. It merely states that a curve with some reasonable resolution is needed to show the trade-off, without offering any specific examples or evidence to back up this statement. The review point does not engage with the topic in a way that would allow for verification of any claims, and therefore does not meet the criteria for verifiability as outlined in the score rubric. As such, the review point does not contain any claims, or only includes factual statements and claims that do not need verifications. Therefore, the score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions the need for a curve to show the trade-off but fails to specify which aspects of the paper are related to this trade-off. The authors are left with uncertainty about which parts of the paper are being addressed by the review point. The lack of specificity in the review point does not allow the authors to confidently identify the parts of the paper that are being discussed. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide the authors with a clear understanding of the paper part that is being addressed by the review point comment."
207,ICLR_2022_1625,"Weakness
--- The overall technical contribution is limited. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2].
-- The use of the sine activation function is not well supported by the experiments. In table 1, sine function shows marginal improvement over tanh, and in table 5 of the appendix, using tanh shows much better performance over sine. Furthermore, tanh tends to become saturated during backpropagation of a long sequence. I would suggest the author also compares with ReLU.
-- Presentation in the paper can be improved:
The modified Adam optimizer is listed as one of the technical contributions. However, it is not stated clearly what is the modification and how does it compare to the original Adam optimizer.
The name of the robots are presented in Fig. 7 of Sec. 4.5 but are referenced multiple times in previous sections and table 1. It would be helpful to clarify the name of the agents early on.
Reference of Fig. 5 at the end of Sec. 4.2 seems to be incorrect, as no comparison of SGD and Adam is shown in Fig. 5.
[1] Lee, Joonho, et al. ""Learning quadrupedal locomotion over challenging terrain."" Science robotics 5.47 (2020). [2] Zhao, Allan, et al. ""RoboGrammar: graph grammar for terrain-optimized robot design."" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.","5. [1] Lee, Joonho, et al. ""Learning quadrupedal locomotion over challenging terrain."" Science robotics 5.47 (2020). [2] Zhao, Allan, et al. ""RoboGrammar: graph grammar for terrain-optimized robot design."" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It lacks any specific guidance or suggestions that the authors could follow to improve their work. The review point merely points out some limitations without providing any actionable comments that could be inferred. It does not suggest any changes or improvements that could be made to the work, nor does it provide any insights into how the authors could address the issues raised. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review points provided are neutral and factual, which aligns with the score of 0. The tone of the review points is neither aggressive nor overly polite. The review points are focused on the content of the papers and provide constructive feedback without being overly critical or overly positive. The feedback is delivered in a manner that is respectful and professional, which is appropriate for academic reviews. The review points do not contain any personal attacks or disrespectful language, which is essential for maintaining a professional tone. The feedback is also factual and based on the content of the papers, which is a key aspect of a neutral review. Therefore, the review points meet the criteria for a score of 0, as they are neutral, factual, and professional in tone.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that merely describes the content of two different papers. The review point does not make any claims about the validity of the research or the accuracy of the results. It also does not provide any logical deductions or references to external or internal sources that could be used to verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor verifies it. The review point is simply a summary of the content of two different papers. 

So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the content of the papers. It identifies the key aspects of the papers, such as the methodology used in the studies and the specific contributions made by the authors. The review point also provides a clear direction for the authors to improve their work, by suggesting areas where more detail or clarification is needed. This level of specificity allows the authors to understand exactly what aspects of their papers are being critiqued and how they can address these points.

In contrast, the review points that lack specificity are vague and do not provide clear guidance for the authors. They fail to identify specific parts of the papers that need improvement or clarification. This lack of specificity can leave the authors uncertain about what aspects of their work are being addressed and how they can improve their papers.

The review point that corresponds to a score of 0 is somewhat specific, but it still leaves room for ambiguity. It asks questions about the papers and suggests that the authors should consider certain aspects, but it does not provide clear guidance on how to address these questions or what specific improvements could be made.

Overall, the review point that is most effective is the one that is specific and directly addresses the content of the papers. It provides clear guidance for the authors on how to improve their work, which is essential for a constructive review process."
1305,ICLR_2023_1603,"Weaknesses - 1. The main drawback of the design is that it relies heavily on a centrally available dataset. One of the two primary goals of the system is to handle non-IIDness in the data, which raises the question - how does the performance of F2L depend on the quality of the root dataset at the server. How well does the root dataset represent the non-IIDness present among the clients? How is scalability affected if the root dataset is not updated to well represent the newly joined clients? More experiments are required to convince the reader that the system can do well even when the root dataset does not exactly represent the data distribution among the clients. 2. Table 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance? 3. Figure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature? 4. F2L relies on switching between LKD and FedAvg after sufficient convergence has happened. How is this threshold chosen? What can be a general way to choose this value for any dataset? 5. Figure 3 shows that a student can outperform a teacher in F2L. This experiment was performed on EMNIST. Does this observation hold in general, independent of the dataset? If not, what conditions does this depend on?",2. Table 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the performance differences between F2L and Fed-Distill without suggesting any specific actions or experiments that could be done to improve the performance of the other benchmarks. The review point also does not provide any implicit actionable comments that could be inferred from the review point. It only mentions the limitations of the other benchmarks without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point is neutral and factual, which aligns with the score of 0. It does not contain any aggressive or disrespectful language, nor does it exhibit any form of encouragement or helpfulness. The review point simply states the facts about the performance of the benchmarks and asks questions about the root datasets and the tuning of Fed-Distill. It does not make any judgments or provide any constructive feedback. The tone is passive, and the review point does not engage with the content in a way that would be considered polite or encouraging. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the performance of F2L versus Fed-Distill, and it also questions whether they both use the same root datasets and whether Fed-Distill is well tuned for best performance. However, the review point does not provide any logical deductions or references to external or internal sources to support these claims. It merely states the facts without any justification or explanation. This lack of verification and support for the claims made in the review point is why it does not meet the criteria for a score of 1. The review point does not contain any claims that do not need verifications, nor does it include factual statements that are not relevant for verifiability. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a score of 1.",1,"The review point provided is quite specific and directly addresses the performance of F2L and Fed-Distill. It asks for clarification on whether both methods use the same root datasets and whether Fed-Distill is well tuned for best performance. This specificity allows the authors to identify which parts of the paper are being addressed by the review point. The authors can be confident that the review point is focused on the comparison between F2L and Fed-Distill, and the potential reasons for the performance differences. The review point does not leave any ambiguity about the paper part being addressed, which aligns with the score rubric's criteria for a score of 1. Therefore, the review point is well-crafted and provides clear guidance for the authors to address."
493,ICLR_2022_1823,"Weaknesses
The main weakness is that the relationship between ADU and constituency tree is not clearly described. Is it true that ADU are often phrases that occur in the constituency tree? How often does this happen? Does the new BERT-based model adhere to constituency constraints? Is the BENEPAR parser appropriate for this data? Based on Trautmann et al.’s comments on grammaticality and clauses, my intuition is that an ADU is almost always a phrase in the tree, in which case it is somewhat less surprising that it helps this task (and maybe it should be helping even more). If this is the case, it’s worth considering related work in distant supervision to include.
The treatment of constituency trees is haphazard. 1) At times it is not clear if dependency or constituency trees are being used; 2) Table 1 should be made more clear, what do the percentage values indicate, and why not use the full tree at all; 3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree.
(low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.",1) At times it is not clear if dependency or constituency trees are being used;,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of clarity in the use of dependency or constituency trees without suggesting any specific actions to be taken. The review point also does not include any actionable comments that can be implicitly inferred. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor constructive, but rather a straightforward critique of the paper's content. The reviewer does not express any form of politeness or rudeness, instead focusing on the technical aspects of the paper. The feedback is delivered in a neutral manner, without any personal attacks or overly positive comments. The reviewer's tone is consistent throughout, maintaining a neutral stance on the paper's quality. This is in line with the score rubric, which defines a score of 0 as a neutral, factual, or passive tone. Therefore, the review point is appropriately scored as 0.",0,"The review point in question does not make any claims, nor does it require verification. It is a general observation about the ambiguity in the use of dependency or constituency trees, which is a factual statement that does not need to be verified. The review point does not contain any claims that could be verified or invalidated, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain a claim that is verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point is vague and does not provide any concrete examples or suggestions for improvement. It lacks the specificity required to guide the authors in addressing the issues raised. The authors would likely be unsure about which parts of the paper are being addressed by the review point, as the feedback does not provide clear guidance or examples. This lack of specificity makes it difficult for the authors to confidently identify and address the issues raised in the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric."
4008,NIPS_2020_404,"- What strikes me as surprising is the direct usage of the function f as the part embedding and point embedding. This needs further justifications, as ideally a BAE-like network should assign a hard per-part occupancy score of {0,1}. e.g. if the point coordinate belongs to the wing of an airplane, then the ideal f should output a one-hot vector. Clearly networks struggle in doing so, thus creating non binary outputs. However, this is more of a bug than a feature. Yet, the authors here exploit this approximation by making the network encode per-point subtleties into this approximated 1-hot. My intuition tells me this is not a principled way of doing it and perhaps a point embedding should be an intermediate prediction from which another few layers convert it into a part embedding? - The CR loss assumes all sampled points on the source have a corresponding point on the target. However, as explained by the authors, often part may appear and/or disappear. How is this being addressed? minor: line 109: a more detailed definition of f^-1 is recommended. In particular, it should be made explicit that f^-1 takes as input both the semantic embedding as well as the shape code and retrieves the coordinate. references: Probably relevant is this recent work that also uses implicit neural function to register between a partial and a full shape: ""The Whole Is Greater Than the Sum of Its Nonrigid Parts""","- The CR loss assumes all sampled points on the source have a corresponding point on the target. However, as explained by the authors, often part may appear and/or disappear. How is this being addressed? minor: line 109: a more detailed definition of f^-1 is recommended. In particular, it should be made explicit that f^-1 takes as input both the semantic embedding as well as the shape code and retrieves the coordinate. references: Probably relevant is this recent work that also uses implicit neural function to register between a partial and a full shape: ""The Whole Is Greater Than the Sum of Its Nonrigid Parts""",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a limitation in the CR loss assumption and suggests a more detailed definition of f^-1. However, it does not provide any specific guidance on how to address this limitation or improve the method. The review point also references a recent work, but it does not elaborate on how this work could be relevant or how it could be used to improve the current method. Therefore, the authors are left without clear instructions on what actions to take. The review point lacks the necessary actionable comments that would allow the authors to deduce what needs to be done. As a result, the review point does not meet the criteria for providing actionable feedback. So the overall score is -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers a suggestion for improvement, specifically recommending a more detailed definition of f^-1. This recommendation is delivered in a respectful and helpful manner, which is indicative of a polite tone. Additionally, the reviewer provides a reference to a recent work, which demonstrates an understanding of the field and offers a potential solution to the problem at hand. The tone of the review point is encouraging and supportive, which is in line with the rubric's description of a helpful and constructive tone. Therefore, based on the rubric's criteria, the review point is indeed polite and constructive.",-1,"The review point in question makes a claim about the CR loss and its assumption regarding the correspondence of points between the source and target. However, it fails to provide a clear justification or reference to support this claim. The review point suggests that the CR loss assumes all sampled points on the source have a corresponding point on the target, which is not always the case. It also recommends a more detailed definition of f^-1, but does not elaborate on what this definition should include or why it is necessary. Furthermore, the review point mentions a recent work that uses implicit neural function to register between a partial and a full shape, but it does not explain how this work relates to the CR loss or the claim being made. Therefore, the review point does not meet the criteria for verifiability as it lacks logical deductions, internal or external sources, and justification for its claims. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses a key aspect of the paper, which is the handling of partial shapes in the context of shape registration. The review point clearly identifies the issue with the CR loss assumption and suggests a more detailed definition of f^-1, which is crucial for understanding how the partial shapes are being addressed. Furthermore, the review point provides a relevant reference to a recent work that also deals with implicit neural functions, which shows a good understanding of the topic and its related literature.

The authors of the paper would be able to identify the specific part of the paper that is being addressed by the review point, as it directly relates to the CR loss and the handling of partial shapes. This level of specificity in the review point would allow the authors to confidently address the concerns raised.

In contrast, a review point that lacks specificity would not provide clear guidance on which parts of the paper are being addressed. The authors would be left guessing which parts of the paper are related to the review point, which could lead to confusion and a lack of confidence in addressing the review point.

Therefore, based on the score rubric, the review point is highly specific and directly addresses a key aspect of the paper, making it clear which part of the paper is being addressed. This level of specificity aligns with the criteria for a score of 1, as the authors would be confident about which part of the paper is being addressed by the review point."
4072,NIPS_2020_1186,"Theoretical Grounding: - Since the regret bound relies on bounding the state reconstruction error at each point in a finite-length sequence, it seems clear to me that the bound only applies to MBRL algorithms that rely exclusively on full-episode rollouts for planning (e.g. PILCO). This criticism in particular seems to apply to the practical algorithm used in the submission, since it relies on a parametric value function approximator to make the planning horizon more tractable. - In practice we don't know \beta _a priori_. How should \beta be chosen? Empirical Evaluation: - The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch. - No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling. - No ablations of β (presumably a crucial design choice). - No demonstration that their dynamics models as implemented satisfy their calibration assumption. Significance/Impact: - Information on the specific implementation details is fairly sparse (e.g. what learning rate and batch size did you use for your dynamics models? Did you reuse a previous implementation of MPO?). Reproducing the authors' findings would likely prove very difficult. MBRL is notorious in the machine learning field for reproducibility issues. If you (the authors) had to reimplement your method from scratch tomorrow, what details would you need?",- No ablations of β (presumably a crucial design choice).,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the absence of ablations of β, which is a crucial design choice, without suggesting any specific actions or improvements that could be made. The review point does not offer any implicit actionable comments that could be inferred by the authors. It simply points out a limitation without providing any guidance on how to address it or any suggestions for future work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided does not exhibit any form of politeness. It is direct and lacks the necessary tact and respect that is expected in a review setting. The reviewer's tone is aggressive and disrespectful, as seen in the phrases ""No ablations of β (presumably a crucial design choice)."" This statement is not only factually incorrect but also dismissive of the paper's efforts. The reviewer's approach is confrontational and does not provide any constructive feedback or suggestions for improvement. Instead, it criticizes the paper's design choice without any justification or explanation. This is not in line with the expected tone of a review, which should be helpful, encouraging, or at least neutral and factual. Therefore, based on the score rubric, the review point is not polite and does not meet the criteria for a helpful or encouraging tone. So the score is -1.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely states that there are no ablations of β, which is a factual statement and does not require any justification or reference. The review point does not make any claims about the design choice or its implications, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not contribute to the overall score or evaluation of the paper. So the score is 0.",-1,"The review point provided is vague and does not specify which part of the paper is being addressed. It lacks the specificity required to guide the authors in identifying the relevant sections of their paper. The review point does not mention any particular section or aspect of the paper, nor does it provide any concrete suggestions for improvement. This lack of specificity makes it difficult for the authors to understand what aspects of their paper need to be addressed. As a result, the authors are left with no clear direction on how to improve their paper based on the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that is specific would clearly identify which part of the paper is being addressed and provide clear guidance on how to improve that particular section. For example, a review point that mentions a specific section of the paper and provides a clear suggestion for improvement would be more helpful to the authors. 

The lack of specificity in the review point is a significant drawback, as it does not allow the authors to confidently identify which parts of their paper are being addressed by the review point. This lack of clarity and direction is why the review point does not meet the criteria for specificity as outlined in the score rubric. 

So the overall score is -1."
1231,ICLR_2023_3550,"Weaknesses
I am not entirely sure about the use of individual training accuracy gain as part of the metric, I think we should technically look at user-level accuracy, where we have at least a single sample held out test for a given training sample thereby measuring generalization accuracy gains. I think measuring the gain on training samples is a bit meaningless as technically the default accuracy on those samples could be 100% for that user, if we do like a KNN classifier.
The measurement of privacy expenditure is inconsistent, there should just be 4 privacy budget epsilons and all experiments done on those, as opposed to different epsilons and even different ways of reporting. Like for one method epsilon is reported,for another the standard deviation of the noise added is reported (which could be converted to epsilon and I think should be for presenting results). All this said, I find stacking all these inconsistent guarantees together like that in Figure 3 in appropriate.
I am not sure I fully understand the two lines of reasoning for why the naive definitions of IF are bad, in section 3.2: • A training algorithm is individually fair if users having similar data face similar privacy risks. This definition is inadequate because a difference in privacy risks can be large if users’ data are dissimilar --> I am not sure how this relates to the former sentence. Doesn't really make sense. • A training algorithm is individually fair if the difference in privacy risks between any pair of users is small. This definition can be satisfied by reducing privacy risks with privacy-preserving ML because the differences are small if all users’ privacy risks are small. However, strong privacy protection with DP is known to degrade classification performance --> There are two issues with this: a) the only way to get similar privacy risks is not by applying privacy preserving methods, there could be other ways out there, so to say that this is achieved by strong DP guarantees is a bit inaccurate, and b) there are recent papers that show DP can be achieved with little loss to accuracy [3-5]
I am not really sure what the conclusion “necessity of the proposed IF for NNs'' really means. If it means we need improvements, there are some group level improvements, why not try them and then test? Improvments like [1-2]. I think the fact that the only conclusion from the paper is that individual level fairness is not good in privacy preserving methods is a bit repetitive gien prior work.
[1] Tran, Cuong, Ferdinando Fioretto, and Pascal Van Hentenryck. ""Differentially private and fair deep learning: A lagrangian dual approach."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 11. 2021.
[2] Fioretto, Ferdinando, et al. ""Differential Privacy and Fairness in Decisions and Learning Tasks: A Survey."" arXiv preprint arXiv:2202.08187 (2022).
[3] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12. (ICLR 2022)
[4] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13. (ICLR 2022)
[5] Tramer, Florian, and Dan Boneh. ""Differentially Private Learning Needs Better Features (or Much More Data)."" International Conference on Learning Representations. 2020.",• A training algorithm is individually fair if users having similar data face similar privacy risks. This definition is inadequate because a difference in privacy risks can be large if users’ data are dissimilar --> I am not sure how this relates to the former sentence. Doesn't really make sense.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states a limitation of the training algorithm without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific changes or improvements that could be made to the training algorithm. It also does not provide any insights into how the algorithm could be better tailored to handle similar data and privacy risks. The lack of actionable comments in the review point makes it difficult for the authors to deduce what needs to be done to improve the algorithm. 

In conclusion, the review point does not provide any actionable steps or comments, which is why it does not meet the criteria for actionability as outlined in the score rubric.",-1,"The review point provided is not aligned with the tone expected in a constructive review. The tone is dismissive and lacks the necessary politeness. The reviewer does not offer any constructive feedback or suggestions for improvement. Instead, the reviewer simply states that the definition is inadequate without providing any reasoning or examples to support this claim. This approach is not helpful and does not encourage the authors to improve their work. The reviewer's tone is more aggressive than neutral, which is not conducive to a productive review process. Therefore, the review point does not meet the criteria for politeness and is not helpful or encouraging. So the overall score is -1.",0,"The review point provided does not contain any claims or make any assertions. It merely states a definition and then questions its relevance to the previous sentence. This does not align with the score rubric, which requires the review point to contain claims and verify them. The review point does not provide any logical deductions, explanations, or references to support its statements. Therefore, it does not meet the criteria for verifiability. The review point is also not relevant to the aspect of verifiability, as it does not attempt to verify any claims. Consequently, the review point does not fulfill the requirements of the score rubric. So the overall score is 0.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The statement ""A training algorithm is individually fair if users having similar data face similar privacy risks"" is not directly related to the review point, which is about the fairness of a training algorithm. The review point does not provide any specific details or examples from the paper that would help the authors understand which part of the paper is being addressed. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors would likely be unsure about which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed. So the score is -1."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","- Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely suggests that the authors should add performance results for domain-dependent features in the final version. This does not provide any guidance on how to improve the current version or what specific changes should be made. The review point lacks any actionable comments that can be inferred, and it does not mention any limitations without suggesting any explicit or implicit actions to be taken. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point is focused on the technical aspects of the paper, such as the domain-dependent features and their impact on performance. It does not make any personal comments about the authors or the quality of the paper, which is a key aspect of maintaining a neutral tone. The review point is also constructive, as it suggests that the authors will add performance results for the domain-dependent features in the final version. This indicates a willingness to engage with the authors and provide feedback that could help improve the paper. So the overall score is 0.",-1,"The review point in question makes a claim about the impact of domain-dependent features on the performance of the model. However, it fails to provide any form of verification or justification for this claim. The review point simply states that the authors will add performance results for these features in the final version, without offering any preliminary analysis or evidence to support the claim. This lack of verification and justification is a clear deviation from the score rubric's requirement for claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",1,"The review point provided is quite specific and directly addresses the performance of domain-dependent features in the paper. It clearly identifies the section of the paper that is being discussed, which is section 2.4. The authors are expected to be confident about which part of the paper is being addressed by the review point, as it directly relates to the performance of domain-dependent features. The review point is also well-structured, providing a clear direction for the authors to follow in their final version. It is evident that the authors will be able to identify the specific parts of the paper that need to be addressed based on the review point. Therefore, the score is 1."
811,ICLR_2021_1181,"Weaknesses
1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?","2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the complexity of the procedure and the absence of a formal complexity analysis. The review point also lacks any actionable comments that can be implicitly inferred. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable steps nor includes actionable comments that can be implicitly inferred. Hence, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does it provide any form of encouragement or helpfulness. The tone is purely informative, focusing on the technical aspects of the paper without any personal opinion or emotional undertones. The reviewer points out the complexity of the GMM and the lack of a formal complexity analysis, as well as the absence of a statistically significant test for performance improvements. However, the reviewer does not offer any constructive feedback or suggestions for improvement. The reviewer's tone is consistent throughout, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the procedure is complex and that a formal complexity analysis and a statistically significant test are absent. This is a factual statement that does not require any form of verification. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or disproved. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the complexity of the proposed SAUM method. It points out the absence of a formal complexity analysis and the lack of a statistically significant test for performance improvements. This specificity allows the authors to identify which parts of the paper are being addressed by the review point. The authors can be confident that the review point is discussing the complexity of the SAUM method and the need for a more rigorous evaluation. The review point does not leave any ambiguity about the paper part being addressed, which is in line with the score rubric's criteria for a score of 1. Therefore, the authors can be confident about which part of the paper is being addressed by the review point."
3412,NIPS_2020_1180,"Here are the following things that I think are missing from this work and should be addressed: 1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture. 2. Competitive Baselines: I carefully looked through the outputs of Self-Tuning [2] and the results are equally impressive. The quantitative analysis and the human studies also seemingly suggest that. Impressively, [2] runs on a CPU with 8 core and the proposed formulation requires a Tesla V100 GPU. I would also point to the quality of results synthesized using Texture CNN. One may, however, complain about the amount of time it takes to synthesize a new texture using this approach. 3. I have some reservation about the evaluations. Please see the next section for specific details.","1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out a limitation of the paper without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that the authors could infer, nor does it give any explicit or implicit actions that should be done. It simply states that the paper cannot address a certain type of texture, without offering any constructive feedback or guidance on how to improve the paper. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does he offer any form of encouragement or helpfulness. The tone is purely informative, pointing out the limitations of the paper without any personal bias or negative connotations. The reviewer simply states the facts about the paper's claims and the limitations of the work, which is a neutral approach to reviewing. The reviewer does not make any personal judgments or criticisms, which is why the tone is neutral. The reviewer also does not provide any constructive feedback or suggestions for improvement, which is why the tone is not helpful or encouraging. Therefore, the review point is neutral and factual, which is why it receives a score of 0.",1,"The review point provided makes a clear claim about the limitations of the paper's universal texture synthesis. It states that the work cannot address non-stationary textures due to the fundamental regularity assumption. This claim is supported by a reference to Zhou et al. [61], which demonstrates the existence of non-stationary textures that the paper's method cannot handle. The review point also provides a logical explanation for why the paper's method is limited to regular texture patterns, which is a key aspect of verifying the claim.

The review point does not make any unverified claims and instead focuses on the limitations of the paper's work. It provides a clear and concise argument that is supported by an external source, which is a strong point in terms of verifiability. The review point also avoids making any unfounded claims or statements that do not need verification, which is in line with the score rubric's criteria for a score of 1.

In contrast, the review points that would receive a score of -1 are those that contain claims that are not verified. They miss justifications, explanations, or references that would support the claims made. The review points that would receive a score of 0 are those that do not contain any claims or only include factual statements and claims that do not need verifications.

Therefore, based on the score rubric, the review point provided is well-structured and makes a clear claim that is supported by logical deductions and an external source. It does not contain any unverified claims and is in line with the criteria for a score of 1.",1,"The review point provided is quite specific, addressing the claim of universal texture synthesis in the paper. It clearly states that the paper has been demonstrated to work only with regular texture patterns, and it points out the limitation of the paper in handling non-stationary textures. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the part of the paper that is being addressed by the review point, which is the claim of universal texture synthesis. The review point does not leave any ambiguity about the paper part being addressed, and it directly challenges the paper's claim, which is a strong point in the review process. Therefore, based on the score rubric, the review point is well-articulated and specific, making it clear which part of the paper is being addressed. So the score is 1."
4916,NIPS_2020_1560,"The paper has a number of issues which are beyond the authors control. Most are caused by the extreme publication speed in the field and the limitations of the conference publishing model. Indeed all of my major concerns are of this type: 1. The use of ImageNet-V2 as primary example: The main issue I find in the paper is the use of ImageNet-V2 as the primary example for “natural distribution shift”. There is indeed a large gap between model performance on ImageNet and ImageNet-V2 but almost all of this can be attributed to subtle effects that arise in the dataset replication process as described by Engstrom et. al. 2020. Using this to criticize the paper is however unfair as Engstrom et. al. 2020 was published on 19. May 2020 only a week before the abstract deadline. ### Post-rebuttal comments: I did not know Shankar et. al., ICML ’20. Thanks for pointing out that reference. I have to look more deeply into this but judging from a quick read their results do indeed change my perception on the performance gap in ImageNet-V2. Nevertheless I think ObjectNet is the more obvious example and should be front and center. As Djolonga et. al. 2020 show it's the least correlated with other benchmarks with a very different design making it (in my current view) the most interesting of the selected benchmarks. 2. The definition of “distribution shifts arising in real data”: While the distribution shift from ImageNet to ImageNet-V2 has mostly been explained by Engstrom et. al. 2020 those to ObjetNet, ImageNet-vid-robust and YTBB-vid-robust can reasonably be expected to be real and existent. They do however only cover a subset of distribution shifts arising in real-world images. As to what is missing take for example ImageNet-R which was recently introduced by Hendrycks et. al. 2020. ImageNet-R shows real-world images not of the original objects but of different artistic renditions like paintings or sculptures. In this case some robustness interventions do have an effect. So the discussion of what constitutes a “natural distribution shift” has to be more nuanced. But as before this information was not available to the authors at time of submission because Hendrycks et. a. 2020 was published 3 weeks after the submission deadline. What turns this situation absurd is the fact that Hendrycks et. al. 2020 heavily builds upon what can readily be assumed to be an earlier version of the present article (not cited here to keep the double-blind mechanism as much intact as possible). ### Post-rebuttal comments: Thanks for including ImageNet-R even though it doesn't make the story easier. The dedication to completing the testbed is really amazing. 3. Big Transfer (BiT) models are missing from the analysis: The recently published Big Transfer model family (Kolesnikov et. al. 2019) was shown to have stunning generalization properties. The most interesting model of that family, BiT trained on the JFT300 dataset, has however not yet been publicly released. After seeing the L2-NoisyStudent model perform so well it would have been interesting to see if BiT-JFT can live up to it’s hype. Instead the authors of BiT have released their own robustness study using partly similar methodology as in the paper presented here (Djolonga et. al. 2020). This should not imply that Djolonga et. al. 2020 is biased or does something wrong but simply illustrate how fast paced the field has become. ### Post-rebuttal comments: Thanks for including BiT-M and reaching out to the authors. As I said above the commitment to completeness is great! 4. Too much information for 8 pages: It is pretty obvious that the amount of content presented in this paper is more than fits 8 pages in the NeurIPS template. I think the authors did a good job presenting their work in that format but when reading the paper it is still noticeable that there was much more content than could fit. It becomes even more obvious when reading the appendix which is full of exciting experiments that provide valuable information but have a good chance of being overlooked there. I want to repeat here that these problems are beyond the authors control. Most of it is caused by the huge amount of related work that was done in parallel and the conference submission system makes it impossible to publish longer papers or significantly update them during the review process. As a result I think it would be unfair to judge the submission based on these flaws. I would still appreciated if the authors could adapt their interpretations and related work prior to submission. Thus the following suggestions contain points regarding the above mentioned issues: 1. Use ObjectNet instead of ImageNet-V2 as the go to example (especially in Figures 1 & 5 as well as in Section 4) 2. Discuss different possibilities to select distribution shifts in real data in the introduction. State and motivate your choice. If I was asked I’d call them distribution shifts to ImageNet like images, as opposed to sketches, renditions, images with specific environmental factors like nighttime scenes or images taken in bad weather etc. which have a specific and easy to point out distribution shift. I think the second to last paragraph of the broader impact statement does a good job in justifying and contextualizing this approach and could be used here. 3. Follows thereof: Try to be a bit more specific as the chosen “natural” distribution shifts are just a subset of what is possible. The paper sometimes reads as if it covered all natural distribution shifts while it doesn’t (especially in section 1). Smaller suggestions: 4. Place “Dataset shifts” before “Consistency shifts” in section 3.1.1 or mention the video datasets in the first section 5. Change the description of “Image corruptions” in 3.1.2 removing the statement that you used corruptions from Geirhos et. al. 2019 which according to the appendix were not used and either don’t mention the number of corruptions (38) or explain why it’s 38 and not 19. I’d probably just remove that number as the nuanced discussion of “in memory” and “on disk” corruptions is only mentioned in the appendix. 6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”",6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states that the dataset shifts are not specified in section 41, without suggesting any specific actions or experiments that could be done to address this issue. The review point also does not include any implicit actionable comments that could be inferred from the text. Instead, it only mentions the lack of specification in the dataset shifts, which does not provide any guidance or direction for the authors to follow. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and thus, it would receive a score of -1.",0,"The review point provided in section 41 of the paper is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of politeness or rudeness, but rather presents a straightforward critique of the dataset used in the paper. The reviewer's tone is neutral, and the feedback is delivered in a factual manner without any aggressive or disrespectful language. The reviewer's comments are focused on the dataset and its implications for the paper's results, which is a constructive approach to reviewing. However, the reviewer could have been more polite by using phrases that are more encouraging or helpful, which would have improved the tone of the review point. So the overall score is 0.",0,"The review point in question does not contain any claims or assertions that require verification. It merely states the existence of a dataset shift in the context of the paper. The review point does not make any claims about the validity of the dataset or the statistical significance of the results. It also does not provide any logical deductions or references to external or internal sources that could verify or refute the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains a claim nor verifies it, and thus, it does not fulfill the requirements for a score of 1. The review point is factual and does not require any form of verification, which is why it would receive a score of 0.",1,"The review point in question is quite specific, as it directly addresses the dataset on which the ""Dataset shifts"" section is computed. This specificity is crucial for the authors to understand which part of the paper is being addressed. The review point is clear and concise, leaving no room for ambiguity. It directly points out the need for specifying the dataset, which is a key aspect of the paper's section on ""Dataset shifts."" This level of specificity ensures that the authors can confidently identify the relevant section of the paper. Therefore, based on the score rubric, the review point is highly effective in addressing the specificity required for the paper's section on ""Dataset shifts."" 

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of which part of the paper is being addressed. It would leave the authors uncertain about the exact section of the paper that needs attention. This lack of specificity would not meet the criteria for a score of 1, as it would not allow the authors to confidently identify the relevant section of the paper. 

Therefore, the review point in question is highly effective in addressing the specificity required for the paper's section on ""Dataset shifts."" It is clear, concise, and directly points out the need for specifying the dataset, which is a key aspect of the paper's section on ""Dataset shifts."" This level of specificity ensures that the authors can confidently identify the relevant section of the paper. 

So the overall score is 1."
1629,ICLR_2023_840,"Weaknesses:
Some important related works are missing: [1] tackles temporal causal discovery with Neural ODEs that would be able to handle inconistent sampling intervals, [2] performs joint structure learning and data imputation, [3] performs temporal causal discovery using the NOTEARS framework for continuous DAG learning. Methods based on the same framework have been applied to static data for joint causal discovery and data imputation [4]. All these weaken the novelty of this paper.
The paper mentions that it is based on Granger causality. However, the current formulation also allows for an interpretation as an additive noise model: ie x i = f ( p a i ) + e i
. Could you please comment on this? This interpretation would also allow for the identification of the temporal causal graph A 0 , τ
rather than just the summary graph A ^ = m a x t A t .
Please add some comments about the difference between Granger causality and ANMs or PCMCI that also identify the temporal causal graph.
It is unclear how τ m a x
is chosen. Is this assumed to be known? What if this isn't known?
All experiments use missing data. It would be great to see a baseline comparing to datasets with full observability. Misc:
What's the intuition of using the moving average as a training signal for the imputation network?
For the graph discovery stage - do you calculate an expectation over multiple graph samples or is this amortised over different batches? Or do you use the same graph sample for optimising this loss?
Please explain ZOH earlier in the text.
Eq 3: what is e
? What's the assumption about it? This might make or break the use of the L2 loss.
Please pay attention to the use of \citep and \citet.
Eq 5: You use inconsistent τ = 0. . . and τ = 1 , . . . .
p5 just above eq 11: I believe n 2
should be n 3 .
p2: "" to conduct causal inference and .."" - should this be ""causal discovery""? Causal inference tackles the question of inferring causal estimates (e.g. ATEs).
[1] Bellot, Alexis, Kim Branson, and Mihaela van der Schaar. ""Neural graphical modelling in continuous-time: consistency guarantees and algorithms."" International Conference on Learning Representations. 2021. [2] Morales-Alvarez, Pablo, et al. ""VICause: Simultaneous Missing Value Imputation and Causal Discovery with Groups."" arXiv preprint arXiv:2110.08223 (2021). [3] Pamfil, Roxana, et al. ""Dynotears: Structure learning from time-series data."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020. [4] Geffner, Tomas, et al. ""Deep End-to-end Causal Inference."" arXiv preprint arXiv:2202.02195 (2022).",... p5 just above eq 11: I believe n 2 should be n 3 .,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a discrepancy in the notation of the variable 'n' without suggesting any specific changes or improvements. The review point does not provide any implicit actionable comments that can be inferred from the review point. It lacks any explicit or implicit actions that should be done, which is a key requirement as per the score rubric. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer simply states a discrepancy in the notation used in the paper, without any additional commentary or tone that could be construed as polite, aggressive, or neutral. The reviewer's statement is straightforward and does not contain any elements of politeness or rudeness. It is a simple observation that does not engage with the content of the paper in a way that would be considered helpful or encouraging, nor does it come across as disrespectful or aggressive. The reviewer's tone is neutral, and the review point is delivered in a factual manner, which is consistent with the score of 0. Therefore, the review point does not fit well with the other two scores, and the score of 0 is appropriate.",-1,"The review point in question makes a claim about the value of n in equation 11. However, it does not provide any logical deductions or references to support this claim. The reviewer simply states that n 2 should be n 3, but does not explain why this is the case or provide any evidence to back up this assertion. This lack of verification and justification is in direct contrast to the score rubric's requirement for claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, this review point does not meet the criteria for verifiability and would receive a score of -1.",1,"The review point provided is quite specific, pointing out a clear error in the paper. It directly addresses the incorrect value of 'n' in equation 11, which is a crucial part of the paper. The authors can be confident that the review point is referring to a specific part of the paper, which is the mathematical representation of the model. This specificity aligns with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed. The authors can easily identify the error and rectify it, demonstrating a clear understanding of the review point. Therefore, the review point is highly effective in guiding the authors to improve their paper."
1073,ICLR_2023_642,"Weaknesses:
Unclear notations. The authors used the same notations to write vectors and scalars. Reading these notations would be challenging to follow for many readers. Please consider updating your notations and refer to the notation section in the Formatting Instructions template for ICLR 23.
The framework impact is unclear. The authors mentioned that the case of intrinsic but known bias and variance is often the case in computational neuroscience and neuromorphic engineering. This is the main motivation for their approach. However, the framework provided is limited to specific cases, namely, white noise and fixed bias. The authors argue that their assumptions are reasonable for most cases computational scientists and neuromorphic engineers face, but they don’t provide evidence for their claims. Clearly, this framework provides an important way for analyzing methods such as perturbed gradient descent methods with Gaussian noise, but it’s unclear how it can help analyze other cases. This suggests that the framework is quite limited.
The authors need to show that their choices and assumption are still useful for computational neuroscience and neuromorphic engineering. This can happen by referring to contributing and important works from these fields having known bias and variance with Gaussian noise.
In the experiments, the used bias is restricted to having the same magnitude for all weights ( b 1 →
). Can we reproduce the results if we use arbitrary biases? It would be better if the authors tried a number of arbitrary biases and averaged the results.
The paper is not well-placed in the literature. The authors didn’t describe the related works fully (e.g., stochastic gradient Langevin dynamics). This makes the work novelty unclear since the authors didn’t mention how analyzing the gradient estimator was done in earlier works and how their contribution is discernible from the earlier contributions. Mentioning earlier contributions increases the quality of your work and makes it distinguishable from other work. Please also refer to my comment in the novelty section.
Missing evidence of some claims and missing details. Here, I mention a few:
It’s not clear how increasing the width and/or depth can lower the trace of the Hessian (Section 2.1). If this comes from a known result/theory, please mention it. Otherwise, please show how it lowers the trace.
The authors mentioned that they use an analytical and empirical framework that is agnostic to the actual learning rule. However, the framework is built on top of a specific learning rule. It’s unclear what is meant by agnostic in this context (Section 1).
The authors mentioned in the abstract that the ideal amount of variance depends on the size and sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. However, the authors didn’t mention the sparsity dependence anywhere in the paper.
The authors mentioned in a note after the proof of Theorem A.5 that it is also valid for Tanh but not Sigmoid. However, the proof assumed that the second derivative is zero. It’s unclear whether a similar derivation can be developed without this assumption. However, the authors only mention the relationship with the gain of ϕ ( . ) .
More information is needed on how the empirical likelihood of descent is computed (Fig. 7).
The use of MSE should be mentioned in Theorem A.3 since it’s not proven for any loss function. In addition, the dataset notation is wrong. It should be D = { ( x 1 , y 1 ) , . . . , ( x M , y M ) }
, where M
is the number of examples since it’s a set containing input-output pairs, not just a single pair.
The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead.
A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1).
The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work.
It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary.
The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30.
It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance.
The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results.
Alternative hypotheses can be made with some experiments. The experiment in Fig. 3.A needs improvement. The authors mention that excessive amounts of variance and/or bias can hinder learning performance. In Fig. 3. A, they only show levels of variance that help decrease loss. An alternative explanation from their figure is that by increasing the variance, the performance improves. This is not the case, of course, so I think the authors need to add more variance curves that hinder performance to avoid alternative interpretations.
Minor issues that didn’t impact the score:
There are nine arXiv references. If they are published, please add this information instead of citing arXiv.
What is a norm N
vector? Can you please add the definition to the paper?
You mentioned that the step size has to be very small. However, in Fig. 1, the step size used is large (0.02). Can you please explain why? Can this be an additional reason why there is no smooth relationship between the values of the variance and performance?
No error bars are added in Fig. 4 or Fig. 7. Can you please add them?
In experiments shown in Fig. 3 and Fig. 5, the number of runs used to create the error bars is not mentioned in Appendix B.2.
A missing 2 D
in Eq. 27.
In Theorem A.3 proof, how the input x
has two indices? The input is a vector, not a matrix. Moreover, shouldn’t ∑ k ( W k ( 2 ) ) 2 = 1 / d
, not d ?",".. , ( x M , y M ) } , where M is the number of examples since it’s a set containing input-output pairs, not just a single pair. The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead. A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1). The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work. It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary. The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30. It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance. The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results. Alternative hypotheses can be made with some experiments. The experiment in Fig.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out issues and limitations without suggesting any specific actions that the authors should take. The review point does not give any hint or direct actionable comments that the authors can deduce what needs to be done. It lacks the provision of any implicit actionable comments that the authors can infer after reading the review point. The review point does not fulfill the criteria of providing actionable steps or comments, which is essential for the authors to improve their work. Therefore, the review point does not meet the requirements of the score rubric.",-1,"The review point provided is not aligned with the score rubric's criteria for politeness. The tone of the review point is aggressive and disrespectful, which is evident in phrases such as ""The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance."" This statement is not only incorrect but also delivered in a manner that is confrontational rather than constructive. Additionally, the review point includes several instances of rudeness, such as ""The experiment is misplaced being at the introduction section."" This statement is not only incorrect but also delivered in a manner that is confrontational rather than constructive. The review point also lacks the necessary politeness when suggesting improvements, such as ""I suggest increasing the number of runs to 20 or 30."" This suggestion is presented in a way that is critical rather than helpful. Overall, the review point does not meet the criteria for politeness as outlined in the score rubric.",-1,"The review point provided contains several claims that are not verified. The reviewer points out that the argument about increasing depth does not relate to the variance argument, but fails to provide a logical explanation or reference to support this claim. Additionally, the reviewer criticizes the placement of an experiment in the introduction section, but does not offer a clear justification for why this placement is problematic. The reviewer also questions the use of batch gradient descent and the consistency of the experiment results, but again, does not provide a logical explanation or reference to support these claims. The reviewer suggests increasing the number of runs and aligning the results with the rest of the paper, but does not provide a clear rationale for these suggestions. The reviewer also mentions the possibility of alternative hypotheses, but does not elaborate on what these hypotheses might be or why they are relevant. Overall, the review point lacks the necessary verification of claims, making it difficult to assess the validity of the reviewer's points. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and is not supported by logical deductions or references.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions several issues, such as the misplaced experiment, the lack of clarity in the introduction, and the inconsistency of the experiment results. However, it does not specify which sections of the paper are related to these issues. The authors are left with a vague understanding of the review point, which does not help them to identify and address the specific concerns raised. The lack of specificity in the review point makes it difficult for the authors to confidently identify which parts of the paper are being addressed. Therefore, the score is -1."
4133,NIPS_2020_73,"1. While I find the combination of single-image 3D reconstruction and GAN interesting, I am concerned about the technical contribution of the paper. It seems that each component is similar to previous works. The single-image 3D reconstruction network is almost identical to [24], and the GAN network also are standard. It feels like the contribution of the paper is just a combination of these two tasks. 2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network. 3. Why using the sinusoidal encoding in the network? How does it compare to directly using the (u, v) coordinates? Overall, I like the results of the paper. However, I am not fully convinced about the choice of the framework, particularly for the questions discussed in point 2. The technical contributions of the proposed method is also not significant to me.","2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the possibility of using a 2D GAN to generate high-quality images and then running a single-image reconstruction network to generate textured meshes. However, it does not suggest any specific actions or experiments that the authors should undertake to validate or improve the proposed method. The review point also does not provide any implicit actionable comments that can be inferred from the text. It only mentions the limitations of the proposed method without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any aggression or rudeness, nor do they offer any encouragement or helpful suggestions. Instead, they present a logical argument about the feasibility of the proposed method, comparing it to an alternative solution. The tone is neutral, and the reviewer does not make any personal judgments about the quality of the paper or the abilities of the authors. The reviewer's focus is on the technical aspects of the proposed method and its comparison to a baseline solution. This approach is consistent with the score rubric's requirement for a neutral tone. Therefore, the review point is not aggressive, disrespectful, or rude, and it does not encourage or help the authors. It is also not overly critical or dismissive, which would have been indicative of a lower score. The reviewer's tone is factual and objective, which is why the score is 0.",0,"The review point provided here does not contain any claims that need to be verified. It merely presents an alternative solution to the proposed task and questions the necessity of the proposed framework. The review point does not make any claims about the performance of the proposed method or the baseline solution. It also does not provide any logical deductions or references to external or internal sources that could verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains claims that need to be verified nor does it provide any logical explanations or references to support its statements. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It suggests an alternative solution to the task and compares it to the proposed method. The review point also questions the necessity of the proposed framework, given the potential of using a 2D GAN to generate high-quality images and then running a single-image reconstruction network. This comparison and questioning are clear and directly related to the paper's content. The authors would be able to identify which parts of the paper are being addressed by the review point, and they would be confident about the specificity of the review point. Therefore, the review point meets the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed and the authors can be confident about it."
1433,ICLR_2023_2880,"Weaknesses: 1. The first question is that the evidence of the motivation is not direct. Since the problem to be solved is that “a predictor suffers from the accuracy decline due to long-term and continuous usage”, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim. 2. Another question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match. 3. Is there any related work about the mixed-batch method?",3. Is there any related work about the mixed-batch method?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely mentions some limitations without giving any explicit or implicit actions that should be done. For example, the review point states that the related work section is severely lacking, but it does not suggest any specific actions to improve it. Similarly, it points out that the proposed method has only a little performance improvement, but it does not provide any guidance on how to enhance the method's performance. The review point also fails to suggest any improvements to the method's implementation or to propose any new experiments that could be conducted. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point is neutral and factual, which aligns with the score of 0. It does not contain any aggressive or disrespectful language, nor does it offer any encouragement or constructive feedback. The tone is purely informative, with no attempt to engage the authors in a more positive or supportive manner. The reviewer simply states the facts and asks questions, which is a passive approach to reviewing. This is evident in the reviewer's use of phrases like ""It feels Section 6 doesn't belong in this paper,"" and ""Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited."" These statements are neutral and do not convey any particular tone or attitude towards the authors' work. Therefore, the review point is not helpful or encouraging, nor is it rude or disrespectful, but it is also not constructive. It is simply a factual summary of the paper's content and a series of questions. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to external or internal sources. The review point simply asks if there is any related work about the mixed-batch method, which is a straightforward question that does not involve any claims or assertions. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims, which is why it would receive a score of 0.",1,"The review point in question is quite specific, focusing on the mixed-batch method and its related work. It directly addresses the paper's methodology and its comparison to other methods. This specificity allows the authors to pinpoint exactly which parts of the paper are being discussed, which is crucial for understanding and responding to the review point. The authors can confidently identify the sections of the paper that are relevant to the review point, such as the methodology and the comparison to other methods. This level of specificity is in line with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. Therefore, the review point is well-crafted and effectively communicates the focus of the review."
531,ICLR_2022_1119,"weaknesses, starting from the most significant ones.
Assumptions and Threat Model? This is probably the only “true” problem of the paper, which should be absolutely rectified. I was not fully able to understand the assumptions made by Tesseract. Does it work “only” against the “directed deviation attack” proposed by Fang et al.? Or does it also protect against different attacks? In general, Section 2.2, Threat Model, is not very comprehensive. The authors should better expand this section by clearly pointing out all the assumptions and requirements of the proposed method. This is especially true because the Fang et al. attack was proposed in 2020, and some of its assumptions are not yet well-known. Specifically, this statement is suspicious: “We assume a full-knowledge (white-box) attack where the attackers have access to the current benign gradients.”. Does it mean that Tesseract only works under this assumption? I.e., the attacker knows, and exploits, the current benign gradients? This is a rather “unrealistic” assumption: I understand the willingness to work against “worst case” scenarios; yet, if such “worst case” scenarios are not realistic in the first place, then what is the purpose of the proposed mechanism? What benefit is there in protecting against an attack that will never happen in the first place? I invite the authors to restructure this section by using the common taxonomies adopted in adversarial ML papers [I].
Problem or Feature Space attacks? The authors perform their experiments on four well-known datasets: MNIST, CIFAR, Shakespeare, FEMNIST; for each dataset, a different (deep) ML model is targeted. Three of these datasets are of images, whereas Shakespeare contains text data. There are different ways to create “adversarial examples”, depending on the ‘space’ where the perturbation is applied. As far as I am aware, the adversarial examples considered in this paper to perform the poisoned updates are created in the feature space. It would be a lot more interesting if at least one evaluation included adversarial examples generated in the “problem” space [A]—or, at the very least, considered samples generated by “physically realizable” adversarial perturbations [B]. I acknowledge that the method should work even in these circumstances, as the proposed Tesseract defense is agnostic of the process used to apply the perturbation. However, considering the strong relationship with (real) security that permeates the paper, I believe that a more convincing use-case would dramatically improve the quality of the paper. This is also motivated by the current state-of-the-art: after almost a decade of adversarial attacks, more recent efforts are leaning towards evaluation that consider more realistic circumstances, where the attacker is constrained by the limitations of the real world; this is even more true in “distributed system” scenarios, such as Network Intrusion Detection Systems, which bear a strong relationship with federated learning (e.g., [C, D, E, F]). As such, I invite the authors to perform an additional “proof-of-concept” experiment where they consider adversaries with constrained capabilities. This is also motivated by the fact that some perturbations may yield different effects when created in the problem space (as shown in [A]).
Tradeoff? A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients. Even if the baseline performance does not decrease, what is the overhead of the proposed method? For instance, in Table 2 the authors report some results for “Attack=None”, which I assume represent the accuracy when no attack takes place. However, all the rows of these experiments (namely, FedSGD, Tesseract, Faba, FoolsGold, FLTrust) consider hardening FL techniques; for instance, on MNIST the proposed Tesseract has an accuracy of 92.52 when no attack takes place—the best among all other defences. Despite being appreciable, I am interested in knowing the performance when NO defense is applied. Surely, the test accuracy in a “fully trusted” FL setting should be superior than 92.52. Hence, I ask: what is the ‘cost’ of Tesseract?
Lack of a concrete use-case. I believe that the paper could be further improved with a concrete use-case, where the authors explain, step-by-step, how a (single, or multiple) attacker can compromise a federated learning system, and how the proposed method can help in solving such problem. Hence, I request the description of a concrete use-case explaining the abstract scenario reported in Figure 1. Such use-case can be at the basis of the “constrained” attack that I invite the authors to perform in my ""problem space perturbations"" suggestion.
Some additional issues:
• In the Introduction, the authors state: “To counter this threat, a set of approaches has been developed for countering Byzantine clients in FL…”. I believe that “Byzantine Clients” is a wrong term: what is countered by Tesseract are not byzantine clients, but ""unloyal"" clients, that are “against” the byzantine clients (at least by referring to the well-known problem of the byzantine generals, which should agree on a method to reach consensus in the presence of unloyal generals).
• The caption of Figure 1 has a typo “c out of m clients maybe be malicious”.
• In Figure, the gradient “LM_{c-1}” is out of place.
• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?
• Figure 1 appears before Figure 2, but in the text it is referenced after Figure 2.
• Putting Figure 2 so early on is very confusing. The “flip score” is a measure introduced for the first time in this paper. As such, any reader would be thrown off by such graphs before reading the paper, meaning that the findings of Figure 2 are difficult to interpret---during the Introduction---, as the flip score has not been defined yet. As such, such graphs are ultimately meaningless: I have to trust the authors that they correspond to “interesting” observations and “fair” experiments, which is not scientific.
• The presentation and notation in the “Flip-score” (page 5) is very ugly and difficult to follow.
• Section 5 should be merged in Section 6
• W.r.t. Table 2, the authors state “We see that TESSERACT is the winner or 2nd place finisher in 7 of the 12 cells (benign + two attacks * 4 datasets)”. This should be better highlighted. I only see three bold values for Tesseract in Table 2.
• W.r.t. Table 2, the authors state “We have not shown the test loss curve for Krum aggregation because of the large loss values.”. I invite the authors to report such values in Table 2, because the different “formats” of the three subtables (None, Full-Krum, Full-Trim) make this table very hard to interpret.
EXTERNAL REFERENCES
[A]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[B]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Conaml: Constrained adversarial machine learning for cyber-physical systems."" Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021.
[F]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.
[I]: ""Wild patterns: Ten years after the rise of adversarial machine learning."" Pattern Recognition 84 (2018): 317-331.","• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states a fact about the simulation of federated learning, without suggesting any changes or improvements that could be made. The review point does not prompt the authors to consider any specific limitations or to propose any particular experiments or methods. It lacks the actionability that is necessary for the authors to understand what needs to be done in order to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. It does not contain any aggressive or disrespectful language, nor does it offer any encouragement or constructive criticism. The review point simply states a fact about the simulation of federated learning, without any additional commentary or suggestions for improvement. It is a straightforward observation about the number of clients in the simulation, which is a factual aspect of the paper. The tone is neutral, and the review point does not engage with the content of the paper in a way that would be considered polite or encouraging. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not contain any claims or make any assertions. It simply states the number of clients and the number of malicious clients in the simulation of federated learning. There is no claim made about the minimum amount of ""m"" or any other aspect of the simulation. The review point does not require any verification as it does not make any claims that need to be supported by logical deductions or external or internal sources. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. So the score is 0.",1,"The review point in question is quite specific, as it directly addresses the number of clients in the federated learning simulation, and the presence of malicious clients. This specificity is in line with the score rubric, which requires the review point to be clear and directly related to a specific part of the paper. The authors can be confident that the review point is addressing the simulation details, which is a critical aspect of the paper. The review point does not leave room for ambiguity, and it is clear that the authors can identify the part of the paper that is being addressed. Therefore, based on the score rubric, the review point is well-crafted and meets the criteria for a score of 1."
2329,ACL_2017_318_review.json,"Weaknesses: 1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted. 
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper. 
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings? 
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work. 
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs. 
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task. 
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"". 
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2? ","3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of clarity in the comparison of the proposed models with models that only consider different senses but not sememes. The review point also fails to suggest any implicit actionable comments that the authors can infer. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionable content makes it difficult for the authors to deduce what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. Instead, it presents a question about the comparison of the proposed models to models that only consider different senses but not sememes. The review point also suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not provide any specific examples of politeness or lack of politeness, which would have been helpful in assessing the tone of the review point. So the overall score is 0.",-1,"The review point in question makes a claim about the comparison of the proposed models to models that only consider different senses but not sememes. However, it fails to provide a clear justification or reference to support this claim. The review point also suggests that the MST baseline is an example of such a model, but it does not elaborate on how this baseline is relevant to the comparison being made. Furthermore, the review point does not mention any other baselines based on related work, which would have strengthened the argument.

In terms of verifiability, the review point does not contain any logical deductions or references to external or internal sources that could validate the claim. The lack of verification and justification in the review point makes it difficult to assess the validity of the claims made.

Therefore, based on the score rubric, the review point does not meet the criteria for verifiability. It contains a claim that is not verified, and it misses a justification, explanation, or reference. As a result, the review point is not sufficiently described, and it would be stronger with the inclusion of more baselines and a clearer justification for the claims made.",-1,"The review point provided is somewhat vague and does not clearly specify which part of the paper is being addressed. It mentions the comparison of the proposed models to models that only consider different senses but not sememes, and the lack of sufficient description of the MST baseline. However, it does not provide a clear direction for the authors to follow in order to improve their paper. The review point also lacks specific examples or suggestions that would help the authors understand what needs to be improved. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors may not be confident about which part of the paper is being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide clear guidance for the authors."
5163,NIPS_2020_1524,"Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer. 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense? 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion. 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this. 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues: - ""philosophical note"" paragraph seems a bit digressive. - Line 109 failed to use superscript? - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.","3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the proposed method without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, and it does not give any hints or direct actionable comments that the authors should make. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It lacks the necessary guidance for the authors to improve their work or to conduct further research. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or helpful. The reviewer simply states the facts about the paper, such as the simplification of the Hessian and the introduction of lambda. The reviewer also points out minor issues without being overly critical or dismissive. The tone is neither rude nor overly polite, which is why it fits the score of 0. The reviewer's comments are factual and do not contain any personal opinions or judgments, which is why the tone is neutral. The reviewer's comments are also not overly critical or dismissive, which is why the tone is not aggressive or disrespectful. So the overall score is 0.",-1,"The review point provided contains several claims, but it fails to verify these claims. The simplification of the Hessian by the squared gradient is mentioned, but there is no justification or explanation as to why this simplification is non-trivial or how it affects the optimizer. Similarly, the introduction of lambda replacing eta/2sigma^2 is noted, but the review point does not explain how this change extends the gap between the optimizer and the Bayesian filtering model. The review point also mentions minor issues, but it does not provide any specific examples or explanations to support these claims. Therefore, the review point does not meet the criteria for verifiability as it lacks logical deductions, internal or external sources, and justifications for its claims. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the simplification of the Hessian by the squared gradient and the introduction of lambda replacing eta/2sigma^2. It clearly identifies the parts of the paper that are being addressed, which is the main criterion for scoring. The authors can be confident that the review point is discussing the optimization techniques used in the paper and how they differ from the Bayesian filtering model. The review point also mentions minor issues, which adds to the specificity of the feedback. The authors can easily identify which parts of the paper are related to the review point, and they can be confident about the paper part that is being addressed by the review point comment. Therefore, the review point meets the criteria for a score of 1, as it is clear, specific, and directly addresses the paper's content."
1904,ARR_2022_306_review,"1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. 
1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set. ","1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely questions the necessity of including examples in the BI, without suggesting any specific actions or experiments that could be undertaken to address this concern. The review point does not offer any insight into how the authors could improve their method or what implications the absence of examples might have. It also fails to provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer simply questions the necessity of including examples in the BI, which is a valid point of discussion. The reviewer's tone is neutral and does not convey any strong emotions or opinions. The reviewer's language is straightforward and does not contain any language that could be perceived as disrespectful or aggressive. The reviewer's focus is on the content of the BI and the potential implications of its structure. Therefore, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question poses a question about the necessity of including examples in the BI when they are already extracted from the instances. This question is a claim that the inclusion of examples in the BI is redundant. However, the review point does not provide any logical deductions or references to support this claim. It lacks the necessary justification or explanation that would verify the claim. The absence of any supporting evidence or logical reasoning makes the claim unverified. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified.",1,"The review point in question is focused on the specificity of the BI in the proposed method. It questions the necessity of including examples in the BI when these examples are already extracted from the instances. The review point is clear and directly addresses the method's approach to the BI, which is a crucial aspect of the paper. The authors can confidently identify the part of the paper that is being addressed by the review point, as it specifically points out the BI and its relation to the instances. The review point is also concise and to the point, which makes it easy for the authors to understand and respond to. Therefore, based on the score rubric, the review point is well-defined and directly addresses a specific aspect of the paper, which is why it would receive a score of 1."
1972,ARR_2022_357_review,"- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. 
- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion ","- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out the limitations of the methods used and the lack of novelty in the approach. The review point does not suggest any improvements or modifications that could be made to the existing methods or resources. It also fails to provide any implicit actionable comments that the authors could infer from the review point. The review point is more of a critique rather than a constructive feedback that could guide the authors in their future work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not offer any encouragement or constructive feedback. The reviewer simply states the facts about the paper's methods and contributions, without any additional commentary or suggestions for improvement. The reviewer's tone is passive, and the feedback is purely factual, without any attempt to engage with the paper in a more constructive manner. This is in line with the default score of 0, which is given when the review point does not fit well with the other two scores. Therefore, the review point is neutral and factual, and it does not meet the criteria for a higher score.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the methods used are basic and not novel, which is a factual statement that does not require any form of verification. The review point also mentions that the main contribution of the paper is the annotated resource, which is a factual claim that does not need to be verified. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that are not verified. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the methods used for automatic linking of arguments to their implicit human value(s). It clearly identifies the methods as basic and not novel, which aligns with the score rubric's requirement for specificity. The review point also makes it clear that the main contribution of the paper is the annotated resource, which is a direct and specific critique of the paper's content.

The authors of the paper would be able to identify which parts of the paper are being addressed by the review point. They would understand that the review point is critiquing the methods used for automatic linking of arguments to their implicit human value(s), and that the main contribution of the paper is the annotated resource. This level of specificity is in line with the score rubric's criteria for a score of 1.

In contrast, a review point that lacks specificity would not provide clear guidance to the authors about which parts of the paper are being addressed. It would be vague and not directly related to the content of the paper. This would lead to confusion and uncertainty among the authors, which is not in line with the score rubric's criteria for a score of 1.

Therefore, based on the score rubric's criteria for specificity, the review point provided is clear, direct, and specific, making it obvious which part of the paper is being addressed. The authors would be confident about which part of the paper is being addressed by the review point."
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.",- A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the potential impact of re-evaluating modes less often and the relationship between the frequency of mode selection and relation re-prediction. The review point does not suggest any specific actions or experiments that the authors could undertake to improve the final performance. It also does not provide any implicit actionable comments that the authors can infer from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point is focused on the technical aspects of the paper, such as the prediction modes and their frequency, without making any personal judgments or comments on the quality of the paper. It is a straightforward analysis of the paper's content, which is in line with the expected tone for a score of 0. The review point does not contain any elements of politeness or rudeness, and it maintains a neutral stance throughout. Therefore, based on the score rubric, the review point is appropriately scored as 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a hypothetical scenario that is not grounded in any specific data or methodology. The review point is more of a thought experiment, exploring the potential impact of varying the frequency of mode selection and relation re-prediction on the final performance of a model. It does not make any assertions that need to be verified, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims or require any form of verification.",1,"The review point provided is quite specific and directly addresses the impact of the frequency of mode selection and relation re-prediction on the final performance of the model. It clearly identifies the part of the paper that is being addressed, which is the prediction mode for each variable at every time step. The review point also provides a clear connection between the frequency of mode selection and the final performance of the model, which is a crucial aspect of the paper.

The review point is well-structured and directly engages with the paper's content, making it easy for the authors to identify which parts of the paper are being addressed. The specificity of the review point is evident in the way it directly relates the frequency of mode selection to the final performance of the model. This specificity is in line with the score rubric, which emphasizes the importance of the review point being clear and directly addressing the paper's content.

In contrast, the review points that lack specificity are vague and do not provide clear guidance on how to improve the paper. They fail to address specific aspects of the paper and do not provide a clear connection between the different elements of the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point.

Therefore, based on the score rubric, the review point provided is well-structured, specific, and directly addresses the paper's content, making it clear and easy for the authors to understand and address the feedback. The review point is also in line with the score rubric's emphasis on specificity and direct engagement with the paper's content. 

So the overall score is 1."
1232,ICLR_2023_3724,"Weaknesses:
The paper has low readability. A lot of the issues are certainly fixable but in its current form, it is confusing enough to distract from evaluating the technical contributions of the paper. Certain examples are:
“DeVAE surpasses 2% for β-TCVAE and 9% for β-VAE.” In what terms exactly?
Many instances in the introduction talking about spreading the conflict of disentanglement and reconstruction over time and space is not easy to follow and understand.
“However, in this work, we get rid of calculating TC by leveraging the narrow information bottleneck (Tishby et al., 1999; Burgess et al., 2018) to find efficient codes for representing the data, which promotes disentanglement.” This is pretty confusing right where it is in the introduction and only becomes somewhat clear after reading the method section.
The authors claim that DynamicVAE suffers from Information Diffusion problems. If that is the case, wouldn’t that result in low disentanglement scores or at least high variance across different seeds for DynamicVAE? But, that’s not the case in Figure 2.
The experiment for high-dimensional latent space is weak. 1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space?
A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set?
What are the layer embeddings? They seem to be an important component of the method but are never explained.
How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper?
Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.","1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space? A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set? What are the layer embeddings? They seem to be an important component of the method but are never explained. How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper? Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the DeVAE method without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred from the text. It lacks any mention of specific experiments or steps that could be taken to address the issues raised. The review point also fails to suggest any comparisons with other methods, which could have provided a clearer direction for future research. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point is presented in a manner that is not polite. The tone is aggressive and disrespectful, as seen in phrases like ""1024 dimensional latent space for dSprites seems unrealistic"" and ""DeVAE is worse for low dimensions."" The reviewer also questions the design decisions and the selection of hyperparameters without providing any constructive feedback or suggestions. The lack of politeness is evident in the way the reviewer criticizes the paper without offering any constructive criticism or acknowledging the potential merits of the work. The reviewer also fails to provide any helpful or encouraging comments, which is a key aspect of the score rubric. Therefore, based on the score rubric, the review point is not polite and is therefore scored as -1.",-1,"The review point contains several claims that are not verified. The claim about the unrealistic dimensionality of the latent space for dSprites is not supported by any logical deductions or references. The question about why DeVAE is worse for low dimensions is not answered, and the lack of explanation for the design decisions, hyperparameters, beta values, and layer embeddings makes the review point incomplete. The computational overhead is mentioned, but there is no comparison with other methods, and the absence of comparison with FactorVAE and Cascade-VAE is a missed opportunity for a more comprehensive review. The review point does not contain any factual statements or claims that do not need verifications, and it fails to provide logical explanations or references to support its claims. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric.",1,"The review point provided is quite specific and directly addresses several key aspects of the paper. It questions the appropriateness of the 1024 dimensional latent space for dSprites, which is a clear and specific point of contention. The review point also asks about the selection of hyperparameters and beta values, which are crucial components of the DeVAE method. Furthermore, it inquires about the layer embeddings and the computational overhead, which are important aspects of the method's implementation. The review point also raises the issue of why DeVAE is not compared with FactorVAE and Cascade-VAE, which is a relevant and specific point of comparison.

The specificity of the review point is evident in the way it directly addresses the paper's content and asks for clarification on several key points. This level of specificity allows the authors to identify which parts of the paper are being addressed by the review point, and they can be confident about the areas of focus. The review point does not leave any ambiguity about the paper's content and the questions posed are directly related to the paper's sections.

In contrast, a review point that lacks specificity would not provide clear guidance on which parts of the paper are being addressed. It would leave the authors unsure about the specific aspects of the paper that need to be addressed. The lack of specificity would make it difficult for the authors to identify the areas of focus and to provide a clear and concise response to the review point.

Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it clear for the authors to identify the areas of focus. The review point's specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which parts of the paper are being addressed by the review point."
2353,NIPS_2021_1788,"Weaknesses: - The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution. - Some parts of the paper need clearer writing (more below)
Comments and questions: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching. - More explanation is needed for Eq (5). For example, what is the meaning of the cost c 0 ( s , a )
? (e.g., to quantify out-of-distribution actions) - The use of s ′ and a ′
when defining $\hat{\pi}{\beta} a t l i n e 107 m i g h t c a u s e c o n f u s i o n a s \mathcal{D} c o n t a i n s
(s,a,r,s’)$. - This paper is about deriving a conservative estimate of the quantiles of the return from offline data where the conservativeness is for penalizing out-of-distribution actions. In the paper, they define OOD actions as those are not drawn from \hat{\pi}{\beta}(.|s) (line 109) but in Assumption 3.1. they assume that \hat{\pi}_{\beta}(a|s) > 0, i.e., there is no OOD actions. Thus, what is the merit of the theoretical result presented in the paper?
The authors have adequately addressed the limitations and social impact of their work.","- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the simplicity of the approach without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not offer any encouragement or constructive feedback. The reviewer simply states the approach proposed is straightforward without much technical innovation, which is a factual observation. However, the review point could have been more engaging by providing specific examples or suggestions for improvement, which would have made it more helpful and constructive. The reviewer also does not express any opinion on the quality of the paper or the performance of the proposed method, which could have added a layer of politeness to the review. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely describes the approach proposed in the paper, which is a direct combination of CQL and QR-DQN. The review point does not make any claims about the validity or effectiveness of the approach, nor does it provide any logical deductions or references to external or internal sources that could verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor verifies it. The review point is more of a straightforward description of the approach, which is not relevant for verifiability. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the approach proposed in the paper. It clearly identifies the combination of CQL and QR-DQN as the main focus of the review point. The authors can be confident that the review point is discussing the methodology used in the paper, specifically the conservative quantiles of the return distribution. This level of specificity allows the authors to pinpoint exactly which part of the paper is being addressed by the review point. The review point does not leave any ambiguity about the paper part being discussed, which is in line with the score rubric's criteria for a score of 1. Therefore, the authors can be confident about which part of the paper is being addressed by the review point."
3805,NIPS_2020_1707,"There are some concerns: 1. In line 82, authors should provide more explanations why they assumed linear constrains. How does it compare with non-linear combination in terms of performance and optimization speed. 2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability. 3. How does the cheap upsampling method (bicubic in the paper) influence the result? What is the limitations of upscaling factor, say will it fail if the factor is 8? 4. More comparisons and results from RAISR should be presented. 5. Experiments on image denoising and deblocking is very limited, lacking quantitative comparisons on benchmarks and intuitive explanation of this generalization.",2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the limitations of the experiments and the comparison between hand-crafted filters and learned filters without suggesting any specific actions or improvements that could be made. The review point does not guide the authors on how to address the issues raised or how to improve the methodology. It lacks the actionable insights that would help the authors to enhance their work or to better understand the implications of their findings. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, which is in line with the score rubric's criteria for a neutral tone. The reviewer simply states the facts and points out the limitations of the experiments and the comparison between hand-crafted filters and learned filters. The reviewer also questions the completeness of the dictionary, which is a factual observation rather than a critique. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and the tone is neither aggressive nor overly polite.",-1,"The review point in question makes several claims, but it fails to provide any form of verification or justification for these claims. For instance, the claim that the pre-defined dictionary is over-complete is not supported by any logical deductions or references to external or internal sources. Similarly, the comparison between hand-crafted filters and learned filters lacks any form of validation or explanation. The claim that the experiments on Set5 are limited in data size and generalization ability is also not substantiated with any evidence or reasoning.

The review point does not contain any claims that are factual and do not require verification, as it is primarily focused on making unsupported claims. This lack of verification and justification for the claims made in the review point is why it does not meet the criteria for a score of 1.

In conclusion, the review point contains several unverified claims, which is why it does not meet the criteria for a score of 1. The claims made in the review point are not supported by logical deductions or references to external or internal sources, and therefore, they are not verified.",1,"The review point provided is quite specific and directly addresses the paper's methodology and its comparison with other methods. It specifically points out the over-completeness of the pre-defined dictionary and the need for a comparison between hand-crafted filters and learned filters. The review point also highlights the limitations of the experiments on Set5, which is a clear indication of the specific aspects of the paper that are being addressed.

The authors of the paper would be able to identify which parts of the paper are related to the review point. They would be confident about the specific aspects of the paper that are being addressed by the review point, such as the pre-defined dictionary, the comparison between hand-crafted filters and learned filters, and the limitations of the experiments on Set5.

The review point is well-structured and directly addresses the paper's methodology and its comparison with other methods. It also highlights the limitations of the experiments on Set5, which is a clear indication of the specific aspects of the paper that are being addressed.

In conclusion, the review point is specific and directly addresses the paper's methodology and its comparison with other methods. It also highlights the limitations of the experiments on Set5, which is a clear indication of the specific aspects of the paper that are being addressed. Therefore, the authors would be confident about which parts of the paper are being addressed by the review point."
5651,NIPS_2018_195,"Weaknesses: - What is the time comparison of VCL relative to BatchNorm and having no normalization? - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently. - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed. - Line 177 typo: ""batchsized"" - It would aid a reader if the authors summarized the loss and how it is computed at the end of Section 2.2. - How sensitive is the framework to the choice of n? - How does \beta vary over time? Could the authors include a graph for this in the Appendix? Questions: - Will the authors be open-sourcing the code for the experiments? - Have you experimented with a constant \beta? - Have you experimented with having _both_ BN and VCL?  Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal. ",- Have you experimented with having _both_ BN and VCL? Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely states the existence of both BN and VCL without suggesting any specific actions or improvements that could be made. The review point also does not include any implicit actionable comments that the authors can infer from the review point. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the authors' efforts and provides suggestions for improvement, which is a helpful and encouraging tone. The reviewer also thanks the authors for their clarification, which further demonstrates politeness. The feedback is focused on the content of the paper and offers specific advice on how to enhance the paper's quality. This approach is respectful and aims to assist the authors in refining their work. The reviewer's tone is consistently polite and constructive throughout the feedback, which is in line with the score rubric's requirements for a score of 1. 

In contrast, the review point could have been more aggressive or disrespectful to score lower on the politeness scale. For example, the reviewer could have used more negative language or made more critical comments about the paper's content or the authors' abilities. However, the reviewer chose to maintain a polite and constructive tone, which is why the score is 1. 

So the overall score is 1, as the review point is helpful, encouraging, and maintains a polite and constructive tone throughout.",0,"The review point in question does not contain any claims that need to be verified. It is a simple question about the use of both BN and VCL in the paper, which is a factual statement and does not require any justification or reference. The reviewer is asking for clarification on the use of both BN and VCL, which is a straightforward inquiry. There is no claim made in the review point, and it does not require any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it is not relevant for verifiability in review points. So the score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the use of both BN and VCL. This specificity allows the authors to identify exactly which part of the paper is being addressed, and they can be confident about the focus of the review point. The review point also encourages the authors to provide further clarification, which is a positive aspect of the feedback.

In contrast, the review points that lack specificity do not provide clear guidance to the authors. They are vague and do not specify which parts of the paper are being addressed. This lack of specificity can lead to confusion and uncertainty among the authors, as they are left to guess which parts of the paper are being discussed.

The review point that asks if the authors have experimented with having both BN and VCL is a good example of a specific review point. It directly addresses a particular aspect of the paper and encourages the authors to provide more information. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed.

Therefore, based on the score rubric, the review point provided is more effective than the review points that lack specificity. It directly addresses a specific aspect of the paper and encourages the authors to provide more information, which is why it would receive a higher score."
5726,NIPS_2018_245,"Weakness] 1: Poor writing and annotations are a little hard to follow. 2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.  3: The motivation is to solve when the question doesn't focus on the most obvious visual concept when there are synonyms and homographs. However, from the experiment, it's hard to see whether this specific problem is solved or not. Although the number is better than the previous method, it will be great if the authors could product more experiments to show more about the question/motivation raised in the introduction.  4: Following 3, applying MLP after GCN is very common, and I'm not surprised that the performance will drop without MLP. The authors should show more ablation studies on performance when varying the number of facts retrieval, what happened if we different number of layer of GCN? ","2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the technical novelty of the paper is limited without suggesting any specific actions that the authors should take. The review point does not give any hint or direct actionable comments that the authors can deduce from the review point. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not offer any encouragement or constructive criticism. The reviewer simply states the limitations of the paper without any additional commentary or suggestions for improvement. The reviewer's tone is passive, and the feedback is delivered in a straightforward manner without any embellishments or personal opinions. The reviewer's focus is on the technical aspects of the paper, and the feedback is strictly based on the content of the paper. Therefore, the review point is neutral and does not fit well with the other two scores.",0,"The review point in question does not contain any claims that require verification. It merely states that the technical novelty of the paper is limited, which is a factual statement that does not need any justification or reference. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The statement ""Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited"" is too general and does not provide any specific feedback on the paper's content or methodology. It lacks the specificity required to guide the authors on how to improve their work. The authors would likely be unsure about which parts of the paper are being addressed by the review point, as the feedback does not provide any concrete examples or suggestions for improvement. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that meets the criteria for specificity would clearly identify which part of the paper is being addressed and provide specific feedback on that part. For example, a review point might point out a specific section of the paper where the authors could have provided more detail or clarification. It would also suggest how the authors could improve that section to better align with the expectations of the review process. 

In conclusion, the review point provided does not meet the criteria for specificity, as it is too general and does not provide any specific feedback on the paper's content or methodology. The authors would likely be unsure about which parts of the paper are being addressed by the review point, which is not helpful for improving the paper."
1656,ICLR_2023_1511,"Weakness_ - The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented). - Similarly, it is lacking a ""So What"" on the bounds provided, which are often just left there as final statements, without an analysis that explains whether 1) they are (likely to be) tight and 2) what this implies for practitioners. - Although well-written, the paper felt quite dense, even compared to other pure-math ML papers. More examples such as Figure 2 would help. - As far as I understood, the assumption on the non-linearities discards the sigmoid and the softmax, which are popular non-linearities. It would be good to acknowledge this directly by name.","- The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out the lack of motivation in the paper and suggests that the authors should have provided more context or reasoning for their work. However, it does not provide any specific guidance or recommendations on how to improve the paper or what actions the authors could take to enhance the quality of their work.

The review point also lacks any implicit actionable comments that the authors could infer from the review. It does not suggest any potential improvements or areas for further exploration that the authors could consider. Instead, it simply states that the paper is lacking in motivation without offering any constructive feedback or suggestions for improvement.

In conclusion, the review point does not meet the criteria for actionability as it neither provides explicit actionable comments nor does it offer implicit actionable comments that the authors could infer. Therefore, based on the score rubric, the review point would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or help the authors. The reviewer's comments are focused on the content of the paper, rather than the tone or manner in which the paper is presented. The reviewer's questions and observations are presented in a straightforward manner, without any personal judgments or emotional undertones. This approach is consistent with the neutral tone expected in a review process. The reviewer's comments are also constructive, providing suggestions for improvement without being overly critical or dismissive. Therefore, the review point is in line with the score of 0, as it is neither rude nor overly polite, and it maintains a neutral stance throughout.",0,"The review point provided does not contain any claims that need to be verified. It is a general statement about the need for motivation in the paper, which is not a claim that requires justification or verification. The review point does not make any assertions that could be supported by logical deductions or references to external or internal sources. It is a factual statement that does not require verification, as it does not make any claims that could be verified or disproved. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly specify which part of the paper is being addressed. It lacks the specificity required to guide the authors in understanding and addressing the feedback. The review point does not provide any concrete examples or suggestions that would help the authors identify the areas of the paper that need improvement. It also fails to mention any particular sections or ideas that are unclear or need further clarification. This lack of specificity makes it difficult for the authors to confidently identify the parts of the paper that are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide any clear guidance for the authors."
3604,NIPS_2020_356,"1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community. 2. Evaluation conducted using 102 scenes from the simulated dataset. There is no mention of why or how these scenes were selected, whether they are complex enough, or whether they pose a significant challenge. Will their proposed method scale to any environment? Some statistics about the selected scenes are necessary. How big are the selected scenes? How many rooms? How many of each target object in each scene? List the scenes used in the appendix for reproducibility. --POST-REBUTTAL--- I am happy with the author's response on this. 3. Though great to have reported real world transfer, the experiments run seem to be minimal and the evaluation premature. --POST-REBUTTAL--- I am happy with the author's response on this. 4. There is overall lack of a deeper discussion about the importance and implications of this work for the wider research effort in autonomous agents and how the authors expect the proposed method to scale. In addition, the paper is overly focused on results from the datasets and challenge, with little interpretation of these results. 5. No standard deviations are reported in the quantitative results. ---POST-REBUTTAL--- The authors have not addressed this concern. I strongly believe that results without any notion of deviation or error (without a good reason for their absence) are not reliable. 6. I am not sure if I missed it, but I could not find an explanation of the “Random” baseline. Is this random at the level of the low level action (local policy) or random at the level of the long-term goal? Also, the distance to success values reported in Table 1 are not that much larger than that achieved by the proposed model. This is of course difficult to judge because of the lack of more scene information or results interpretation. --POST-REBUTTAL--- I am happy with the author's response on this. I understand now that this is indeed a challenging task - looking at the very low success rate of baselines.","1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable comments that the authors should make. It does not suggest any specific experiments or steps that the authors could take to improve their work. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. The review point does not guide the authors on how to enhance the significance of their work or how to make it more beneficial to the wider community. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses concern about the novelty of the work, but does so in a respectful manner. The reviewer acknowledges the potential value of the approach for the indoors navigation community, demonstrating an understanding of the paper's relevance. The tone is helpful and encouraging, which is in line with the rubric's definition of a score of 1. The reviewer's feedback is focused on the specific aspects of the paper, rather than being aggressive or disrespectful. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1.",0,"The review point in question does not contain any claims that need to be verified. It is a general commentary on the novelty of the work and its potential impact on the wider community. The reviewer expresses concern about the significance of the work and its potential benefits, but does not make any specific claims that could be verified. The reviewer also acknowledges the value of the specific approach for the embodied indoors navigation community, which is a factual statement that does not require verification. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified. So the overall score is 0.",1,"The review point provided is quite specific, addressing the concern about the novelty of the work and its potential impact on the wider community. It clearly identifies the issue of the algorithm being a combination of existing methods, and it questions whether this approach is a significant scientific contribution. The reviewer's discussion also shows an understanding of the specific approach and its relevance to the embodied indoors navigation community. This indicates that the authors can be confident about which part of the paper is being addressed by the review point. The review point is well-aligned with the score rubric, as it directly addresses the specificity of the paper's contribution. Therefore, the score is 1."
2313,ACL_2017_657_review.json,"Weaknesses: one of the main weaknesses of the paper lies in the fact that the goals are not clear enough. One overall, ambitious goal put forward by the authors is to use approaches from experimental psychology to interpret LSTMs. 
However, no clear methodology to do so is presented in the paper. On the other hand, if the goal is to validate sociological assumptions, then one should do so by studying the relationships between gender markers and the written justifications, independently on any model. The claim that ""expected gender differences (are) a function of theories of gendered self-construal"" is not proven in the study.
- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up.
As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed. ","- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up. As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the weaknesses of the study without suggesting any improvements or further research. The reviewer does not provide any guidance on how to enhance the study or address the issues raised. The lack of actionable comments in the review point means that the authors are left without any clear direction on what needs to be done to improve their work. This is in contrast to the score rubric, which requires the review point to suggest direct and explicit actionable comments. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not particularly polite. It uses a critical tone, which is evident in phrases such as ""these probabilities are valid ways to interpret the model"" and ""the correlation coefficients are not that high, so that the point put forward is not really backed up."" These statements are not delivered in a constructive manner, and they could be perceived as dismissive or even disrespectful. The reviewer also expresses a lack of clarity in the authors' goals and questions the appropriateness of the methodology without providing constructive feedback on how to improve it. This approach does not foster a positive dialogue or encourage the authors to refine their work. The reviewer's tone is more confrontational than supportive, which is not conducive to a helpful review process. Therefore, based on the score rubric, the review point is not polite and does not meet the criteria for a helpful or encouraging tone.",-1,"The review point provided contains several claims that are not verified. The reviewer points out that the correlation between LSTM network probabilities and SVM coefficients does not prove the validity of the probabilities. However, the reviewer does not provide any logical deductions or references to support this claim. Similarly, the reviewer criticizes the hesitation between two goals in the paper but fails to provide any justification or explanation for this claim. The reviewer also mentions the lack of detail in the explanation of the multilevel bayesian models used in the paper, but again, there is no verification of this claim. The reviewer's comments on the correlation coefficients and the methodology used are also not supported by any logical deductions or references. Therefore, the review point does not meet the criteria for verifiability as it contains claims that are not verified.",1,"The review point provided is quite specific and directly addresses several key points in the paper. It clearly identifies the correlation between LSTM network probabilities and SVM coefficients, and it critically evaluates the validity of these probabilities as interpretations of the model. The review point also points out the hesitation in the authors' goals and the lack of detail in the methodology, particularly concerning the multilevel bayesian models used. This specificity in the review point allows the authors to understand exactly which parts of the paper are being addressed, and they can confidently identify the areas that need improvement. The review point is well-structured and directly engages with the paper's content, making it clear and actionable for the authors. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, which aligns with a score of 1."
1569,ICLR_2023_303,"Weaknesses: 1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it? 2. Regarding the proposed hierarchical patch embedding, can authors give more explanation about the specifical design as 2 consecutive MLP with ratio 3, how authors reach this design. As this is a critical module proposed, can authors provide some ablation studies about it, e.g. why uses a different MLP ratio of 3 as latter stages uses 4 instead? Will allocate computations differently to 56x56, 28x28 influence the final performance? What computation percentage should be used in patch embedding v.s. latter stage (14x14) for better performance?","1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided several areas where the authors could improve their work. However, the review point did not offer direct and explicit actionable steps or experiments that the authors could undertake. Instead, it pointed out several issues without providing clear guidance on how to address them. For example, the review point mentioned that the authors should add explanations for RPE, Low Att, Mid Att, etc. in Table 2, but it did not specify what kind of explanations would be helpful or how they could be incorporated into the table. Similarly, the review point suggested that the authors should add descriptions about the merging operation, but it did not provide any examples or guidance on how to do this. The review point also mentioned that the authors should include the number of parameters and flops in Table 2, but it did not explain why this information is important or how it would affect the computation. Overall, the review point was more of a critique than a set of actionable steps, and it did not provide the authors with clear guidance on how to improve their work. Therefore, based on the score rubric, the review point did not give any direct actionable comments, and the authors would still be unsure about what needs to be done. So the score is -1.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement, such as adding explanations for terms in Table 2, providing clearer definitions for stages, and including additional information like Param and Flops. The reviewer also points out incomplete sentences and requests more details about the merging operation. These suggestions are delivered in a respectful and helpful tone, which is conducive to a positive and constructive review process. The reviewer's approach is encouraging and aims to enhance the quality of the paper, rather than criticizing it. Therefore, based on the score rubric, the review point is helpful and encouraging, which is why it would receive a score of 1.",-1,"The review point provided contains several claims that are not verified. For instance, the claim about the lack of explanation for RPE, Low Att, Mid Att, etc. in Table 2 is not supported by any logical deductions or references. Similarly, the claim about the missing stage definition and the need for additional information such as Param and Flops in Table 2 is not substantiated with any evidence or justification. The incomplete sentences in the review point, such as 'causes a significant accuracy' and 'all tokens in Stage ? are symmetric', are not clarified or explained, leaving the reader without a clear understanding of the claims being made. Furthermore, the request for more details about the merging operation in the paper is not addressed, leaving the reader to infer the information from the figure alone. These claims are not verified, and the review point does not provide any logical explanations or references to support the claims. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability.",1,"The review point provided is quite specific and directly addresses several parts of the paper. It points out the lack of explanation for certain terms in Table 2, the absence of a clear stage definition, and the need for additional information about the merging operation. The review point also suggests the inclusion of parameters and flops in Table 2, which would help in understanding the computational impact of different components. Furthermore, it identifies incomplete sentences and requests more detailed descriptions. This level of specificity makes it clear which parts of the paper are being addressed, and the authors can confidently identify and respond to these points. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
5633,NIPS_2018_197,"weakness of the paper: its clarity. From the presentation, it seems evident that the author is an expert in the field of computer algebra/algebraic geometry. It is my assumption that most members of the NIPS community will not have a strong background on this subject, me including. As a consequence, I found it very hard to follow Sect. 3. My impression was that the closer the manuscript comes to the core of algebraic geometry results, the less background was provided. In particular, I would have loved to see at least a proof idea or some more details/background on Thm. 3.1 and Cor. 3.2. Or maybe, the author could include one less example in the main text but show the entire derivation how to get from one concrete instance of A to right kernel B by manual computation? Also, for me the description in Sect. 2.4 was insufficient. As a constructive instruction, maybe drop one of the examples (R(del_t) / R[sigma_x]), but give some more background on the other? This problem of insufficient clarity cannot be explained by different backgrounds alone. In Sect. 3.2, the sentence ""They are implemented in various computer algebra systems, 174 e.g., Singular [8] and Macaulay2 [16] are two well-known open source systems."" appears twice (and also needs grammar checking). If the author could find a minimal non-trivial example (to me, this would be an example not including the previously considered linear differential operator examples) for which the author can show the entire computation in Sect. 3.2 or maybe show pseudo-code for some algorithms involving the Groebner basis, this would probably go a long way in the community. That being said, the paper's strengths are (to the best of this reviewer's knowledge) its originality and potential significance. The insight that Groebner bases can be used as a rich language to encode algebraic constraints and highlighting the connection to this vast background theory opens an entirely new approach in modelling capacities for Gaussian processes. I can easily imagine this work being the foundation for many physical/empirical-hybrid models in many engineering applications. I fully agree and applaud the rationale in lines 43-54! Crucially, the significance of this work will depend on whether this view will be adopted fast enough by the rest of the community which in turn depends on the clarity of the presentation. In conclusion: if I understood the paper correctly, I think the theory presented therein is highly original and significant, but in my opinion, the clarity should be improved significantly before acceptance, if this work should reach its full potential. However, if other reviewers have a different opinion on the level of necessary background material, I would even consider this work for oral presentation. Minor suggestions for improvements: - In line 75, the author writes that the ""mean function is used as regression model"" and this is how the author uses GPs throughout. However, in practice the (posterior) covariance is also considered as ""measure of uncertainty"". It would be insightful, if the author could find a way to visualize this for one or two of the examples the author considers, e.g., by drawing from the posterior process. - I am not familiar with the literature: all the considerations in this paper should also be applicable to kernel (ridge) regression, no? Maybe this could also be presented in the 'language of kernel interpolation/smoothing' as well? - I am uncertain about the author's reasoning on line 103. Does the author want to express that the mean is a sample from the GP? But the mean is not a sample from the GP with probability 1. Generally, there seems to be some inconsistency with the (algebraic) GP object and samples from said object. - The comment on line 158 ""This did not lead to practical problems, yet."" is very ominous. Would we even expect any problem? If not, I would argue you can drop it entirely. - I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.","- I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely expresses uncertainty and assumptions about the content of the paper without suggesting any specific actions or improvements that could be made. The reviewer's suggestions are vague and do not offer any concrete guidance for the authors to follow. The reviewer's comments about the use of differential equations and the presentation of the work do not provide any actionable insights that could help the authors improve their paper. The reviewer's mention of the Weyl algebra and the suggestion to make it the basic piece also lacks actionable details and does not offer any clear direction for the authors. Therefore, the review point does not meet the criteria for providing actionable comments or suggestions.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses uncertainty and seeks clarification, which is a respectful way to engage with the content. The reviewer also offers alternative suggestions, such as using computer-algebra code or presenting the work in the context of kernel ridge regression, which demonstrates a helpful and encouraging tone. Furthermore, the reviewer acknowledges the author's rationale for using differential equations and suggests an alternative approach, which shows a constructive approach to the review. The reviewer's tone is consistently polite and constructive throughout the feedback, which is why it receives a score of 1.",-1,"The review point contains a claim about the relationship between u(t) and x(t) in the context of a Gaussian Process (GP). However, the claim is not verified, as it lacks a logical explanation or reference to external or internal sources. The reviewer's assumption about the order of operations in Fig. 3 is not substantiated, and the suggestion to use computer-algebra code or a video to represent uncertainty is not directly related to the claim. The reviewer's intuition about the generality of the work with respect to Groebner bases is not clearly connected to the claim. The review point does not provide a clear justification for the claims made, and thus does not meet the criteria for verifiability. So the overall score is -1.",-1,"The review point provided is somewhat vague and does not clearly identify which part of the paper is being addressed. The reviewer seems to be unsure about the relationship between u(t) and x(t), and the direction of the figures. While the reviewer does suggest some improvements, such as adding computer-algebra code and presenting the work in the context of kernel ridge regression, these suggestions are not directly related to the specificity of the review point. The reviewer also mentions the use of differential equations and the potential for a video representation, but these points do not directly address the specificity of the review point. The reviewer's uncertainty about the paper's content and the lack of clear direction in the review point indicate a lack of specificity. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors would not be confident about which parts of the paper are related to the review point. So the overall score is -1."
3433,NIPS_2020_1241,"1. The analysis on neural networks are direct results induced from the kernel method results. It may not be tight, and can even suffer from the curse of dimensionality. Hence, the different between two types of models showed in the theorems may not fully characterize the actual performance difference. Neural networks can possibly perform much better than the bound. 2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?","2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out a potential issue with the activation function used in the numerical experiments. The review point does not suggest any specific changes or improvements that could be made to address this issue. It also does not provide any implicit actionable comments that could be inferred from the review point. The authors are left without any clear direction on what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point in question is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer suggests an alternative activation function, which is a helpful and encouraging suggestion. The tone is respectful and the feedback is focused on improving the paper, rather than criticizing it. The reviewer also acknowledges the use of ReLU in the numerical experiments, which shows an understanding of the context. The feedback is also specific, mentioning tanh and sigmoid as potential alternatives, which demonstrates a constructive approach to the review. The reviewer's tone is helpful and encouraging, which is in line with the score rubric's criteria for a score of 1. So the overall score is 1.",0,"The review point in question does not make any claims, but rather poses a question about the choice of activation function in the numerical experiments. The question is about the use of ReLU instead of a smooth activation function, and it does not provide any verification or justification for the claim. The review point does not contain any logical deductions or references to external or internal sources that could support the claim. It is a factual statement that does not require verification, as it does not make any claims. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. So the overall score is 0.",1,"The review point in question is quite specific, addressing the use of ReLU activation function in the paper. It suggests that a smoother activation function might be more illustrative, which directly points to the numerical experiments section of the paper. This specificity allows the authors to identify the exact part of the paper that is being addressed by the review point. The review point is clear and concise, making it easy for the authors to understand what needs to be addressed. It also provides a potential solution by suggesting the use of a smoother activation function. This level of specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. Therefore, the review point is well-crafted and meets the criteria for a score of 1."
796,ICLR_2021_2043,"Weaknesses: - The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.
- Clarify the difference between S-Full and S-PGD from Experiments section. Since S-Full also uses T-step PGD, how it is different than S-PGD? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric, the performance gains are very minimal and not very significant than the baseline OE (Hendrycks et al., 2019b) under two metrics AUROC and AUPR.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on adversarial robustness is significant, there are critical points yet to be addressed. Therefore, I marginally accept this paper.
Post rebuttal: The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper.","- The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the need to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. However, it does not provide any guidance on how to do so or what specific actions should be taken. The review point also lacks any implicit actionable comments that could be inferred from the text. It simply states the need for further evaluation without suggesting any particular method or approach to be used. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer's comments are focused on the technical aspects of the paper, such as the evaluation of the proposed technique and the possibility of safe spot aware adversarial attack. The reviewer does not express any personal opinions or judgments, which is in line with the neutral tone expected in a review. The feedback is also constructive, as it provides a clear direction for the authors to improve their work. However, the reviewer could have been more polite by using softer language and providing more specific suggestions for improvement. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It merely states the importance of addressing the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. The review point does not make any claims about the effectiveness of the proposed defense or the feasibility of safe spot aware adversarial attacks. It also does not provide any logical deductions or references to external or internal sources that could verify any claims. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not provide any logical deductions or references to external or internal sources. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the evaluation of the proposed technique. It clearly identifies the need to assess the safety of the proposed defense against safe-spot aware adversarial attacks. The review point also asks for a rationale if such an attack is deemed infeasible, which shows a clear understanding of the paper's content and the specific concerns raised.

The authors of the paper would be able to identify which parts of the paper are being addressed by the review point. They would be confident that the review point is focused on the evaluation of the proposed technique and the possibility of safe-spot aware adversarial attacks. This is because the review point is directly related to the paper's content and the specific concerns raised.

The review point does not leave any ambiguity about which parts of the paper are being addressed. It is clear and specific, which aligns with the score rubric's criteria for a score of 1. The authors would be confident about which part of the paper is being addressed by the review point, and they would be able to address the concerns raised in the review point.

In conclusion, the review point is specific and directly addresses the evaluation of the proposed technique, making it obvious which part of the paper is being addressed. The authors would be confident about which part of the paper is being addressed by the review point, and they would be able to address the concerns raised in the review point. Therefore, the score is 1."
3455,NIPS_2020_725,"- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks. - One particular concern is that in this setup, as shown Figure 4a, a strong Hebbian learning rule leads to a much faster convergence than gradient descent, presumably because Hebbian learning quickly memorizes the training set. It might very well be that Hebbian learning leads to more progressive differentiation in tasks where it results in equal or slower convergence than gradient descent.",- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states a limitation of the present work without suggesting any specific actions that the authors should take to address this limitation. The review point does not provide any implicit actionable comments that can be inferred, nor does it give any hints or direct comments that could guide the authors on what needs to be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It is clear that the authors would not be able to deduce what needs to be done after reading the review point. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the limitations of the work without any personal opinion or emotional tone. The reviewer's comments are focused on the content of the paper and do not address the authors' efforts or the quality of their work. The feedback is purely objective, which is the hallmark of a neutral review. The reviewer's comments are also constructive, as they provide a clear direction for future work. The reviewer's tone is consistent throughout the review, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point provided makes a claim about the limitations of the present work, specifically regarding the simplicity of the hierarchical task and the linearity of the network used. However, the review point does not provide any verification or justification for this claim. It merely states that the lessons obtained may not generalize to harder tasks and non-linear networks without offering any logical deductions or references to support this assertion. This lack of verification and justification is in line with the score rubric's criteria for a score of -1, as it does not contain any claims that are verified or supported by logical explanations or references. Therefore, based on the score rubric, the review point does not meet the criteria for a higher score.",1,"The review point provided is quite specific, addressing a particular limitation of the paper. It clearly identifies the hierarchical task and the linear network used, and it questions the generalizability of the findings to more complex tasks and non-linear networks. This specificity allows the authors to pinpoint the exact part of the paper that is being critiqued, which is the methodology and its applicability. The authors can confidently identify the section of the paper that discusses the hierarchical task and the linear network, and they can also understand the implications of the review point on their work. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
3000,NIPS_2022_1200,"Weakness: Originality:
1.I want to know if this paper is the first time to study the problem of the robust collaborative inference, where there are both arbitrary agents and adversarial agents. The arbitrary agents are easy to identify. However, I’m afraid the proposed method achieves a similar performance to identify the adversarial agents compared with baselines.
From Eq.(5), the framework aims to find a combined feature l
which is on the manifold and is near h
. The manifold projection could get a similar results for the adversarial sub-features. Could the authors discuss more about it? Writtings:
1.After so many times of reading, I guess I understand this paper. The authors introduce their method in Section 2.3, which is very simple. However, it relies block-sparse structure which is detailed stated in Section4. This could cause confuse when understanding the proposed method.
2.The notations are confusing. For example, h and l
both denote the feature. Why not use a letter (or with its variants)?
the citation format may be ICLR rather NeurIPS.
Theoretical analysis:
1.This paper provides an extensive theoretical analysis. In fact, I suggest the authors discuss more what the analysis means. Compared with baselines, why CoPur could do better.
2.Could the authors give an intuitive explanation about the effect of the sparsity α
on CoPur? Experiments:
1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c
and different Ω a d v ?
2.More analysis is helpful, for example, The comparison on optimization efficiency.
The authors discuss the limitations in Appendix. I have no other suggestions.","1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c and different Ω a d v ?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the differences between CoPur and the manifold projection without suggesting any specific actions or improvements that could be made. The review point also does not include any actionable comments that can be implicitly inferred. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the technical aspects of the paper. The language used is clear and concise, which is appropriate for a review of a scientific paper. The reviewer does not make any personal attacks or disrespectful comments, and the feedback is delivered in a manner that is neither encouraging nor discouraging. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point in question does not contain any claims, nor does it require any form of verification. It is a hypothetical scenario that is not grounded in any specific data or findings. The review point is purely speculative and does not make any assertions that could be verified or refuted. It is a thought experiment that is not meant to be taken as a definitive statement about the performance of CoPur or the manifold projection. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a discussion point rather than a claim that needs to be verified. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the performance of CoPur in comparison to the manifold projection. It also considers the possibility of different Ω c and different Ω a d v, which shows a clear understanding of the paper's content. The authors can be confident that the review point is addressing the specific performance of CoPur and its comparison to the manifold projection, as well as the implications of different Ω c and different Ω a d v. This level of specificity aligns with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed. The authors can be confident about which part of the paper is being addressed by the review point, and they can also be confident that the review point is directly related to the paper's content. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
1170,ICLR_2023_3572,"Weakness: 1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body. 2. Figure 3 is the most important picture in the article ,but it is confusing. Although the style of the figure is nice, it does not help me understand PRG. There is no need to place a black box on (1,1).","1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a lack of clarity in the explanation of the symbol \xi and its disappearance in the pseudo-code. The review point does not suggest any specific changes or improvements that could be made to the paper. It also does not provide any implicit actionable comments that the authors could infer from the review point. The review point only mentions the limitations of the paper without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The reviewer simply states the issue with the symbol \xi and its disappearance in the pseudo-code, without any additional commentary or suggestions for improvement. The tone is neutral and does not convey any particular emotion or stance. The reviewer's language is straightforward and lacks any form of politeness or discourtesy. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question makes a claim about the difficulty of understanding the paper, particularly in Section 3.1. However, it fails to provide any logical deductions or references to support this claim. The reviewer mentions the symbol \xi and its disappearance in pseudo-code, but does not offer any explanation or justification for this observation. The lack of verification in the review point means that the claim is not supported by any logical reasoning or external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. The reviewer's claim is not substantiated, and the absence of any justification or explanation makes the review point unverified. So the overall score is -1.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The mention of the symbol \xi and its disappearance in pseudo-code is vague and does not provide a clear direction for the authors to improve their paper. The lack of specificity in the review point makes it difficult for the authors to understand which parts of the paper are being criticized or need clarification. This lack of specificity is in line with the score rubric's criteria for a score of -1, as it does not help the authors to identify the exact sections of the paper that need attention. Therefore, the authors would likely be unsure about which parts of the paper are being addressed by the review point."
5211,NIPS_2020_1428,"- Novelty: This work can appear as incrementally innovative since Sobolev descent and unbalanced optimal transport were already present in the literature. This work is essentially the merging of these two existing works, resulting however in a new contribution. - Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.",- Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the experiments without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, nor does it give any explicit or implicit actions that should be done. It lacks the necessary detail and guidance that would help the authors to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is not particularly polite. It uses phrases like ""toyish"" and ""does not really outperform existing results,"" which can be perceived as dismissive and critical. The tone of the review point is more neutral than constructive, and it lacks the helpfulness that would be expected from a reviewer. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a key aspect of a polite review. Instead, the reviewer simply states the shortcomings of the paper without providing any guidance or encouragement. This approach does not foster a positive and constructive dialogue between the reviewer and the authors. Therefore, based on the score rubric, the review point is not helpful or encouraging, and it does not meet the criteria for a score of 1. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the experiments are ""toyish"" and do not outperform existing results. This is a factual statement that does not require any justification or reference to support it. The review point does not make any claims that could be verified or disproved, which is why it does not meet the criteria for a score of 1 or 0. It also does not contain any claims that are not verified, which is why it does not meet the criteria for a score of -1. Therefore, the review point is neutral in terms of verifiability and does not contribute to the overall score.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The use of the term ""toyish"" is not specific and does not provide any concrete feedback on the paper's content. Furthermore, the review point does not offer any constructive criticism or suggestions for improvement. It merely states that the experiments do not outperform existing results without providing any specific examples or explanations. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. As a result, the authors are left with no clear direction on how to improve their paper based on the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. So the overall score is -1."
3081,NIPS_2017_330,"Weaknesses
- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.
- Issues of convergence, especially when applying gradient descent over a non-Euclidean space, is not addressed
In all, a rather thorough paper that derives an efficient way to compute gradients for optimization on LDSs modeled using extended subspaces and kernel-based similarity. At one hand, this leads to improvements over some competing methods. Yet, at its core, the paper avoids handling of the harder topics including convergence and any analysis of the proposed optimization scheme. None the less, the derivation of the gradient computations is interesting by itself. Hence, my recommendation. ",- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the section is tersely written and could have benefitted from a slower development. This does not provide any guidance or suggestions for improvement. The review point does not include any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific changes or improvements that could be made to the section in question. It lacks the depth and detail necessary to provide actionable feedback. The reviewer does not offer any insights into how the section could be improved or what aspects of the section could be expanded upon. This makes it difficult for the authors to deduce what needs to be done based on the review point. 

In conclusion, the review point does not provide any actionable comments or suggestions, and it does not meet the criteria for actionability as outlined in the score rubric. Therefore, the score is -1.",-1,"The review point provided is quite terse and lacks the necessary politeness that is expected in a review. The tone of the review point is more critical than constructive, which could potentially discourage the authors from improving their work. The reviewer's comments are direct and do not provide any constructive feedback or suggestions for improvement. This approach does not align with the score rubric's criteria for politeness, which emphasizes helpfulness and encouragement. The reviewer's comments could be perceived as aggressive or disrespectful, which is not conducive to a positive review process. Therefore, based on the score rubric, the review point does not meet the criteria for politeness and is not helpful or encouraging. So the overall score is -1.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the section is tersely written and could have benefitted from a slower development for an easier read. This is a factual statement and does not require any form of verification. The review point does not make any claims about the content of the section or its implications, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The statement ""Section 4 is very tersely written"" is a general comment that does not provide specific guidance or feedback on the content of the section. It lacks the specificity required to help the authors understand what aspects of their work need improvement. The review point does not offer any concrete suggestions or examples that would help the authors to identify and address the issues mentioned.

Moreover, the review point does not provide any context or explanation as to why the section is tersely written or how it could be improved. This lack of detail and specificity makes it difficult for the authors to understand what changes they need to make. The review point does not align with the score rubric's criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed.

In contrast, a review point that would score a 1 would provide specific examples and suggestions that would help the authors to identify and address the issues in their work. It would clearly state which parts of the paper are being addressed and offer concrete guidance on how to improve those sections.

Therefore, based on the score rubric, the review point provided is not specific enough to help the authors understand and address the issues in their work. It lacks the necessary detail and context to be considered a helpful review point. So the overall score is -1."
85,ICLR_2022_3099,"Weaknesses
W1: The setting seems to be limited and not well justified. 1) It only consider ONE truck and ONE drone. Would it be easy to extend to multiple trucks and drones? This seems to be a more interesting and practical setting. 2) What is the difference of this setting versus settings where there are multiple trucks? Are there methods solving this setting, and why are they not working in TSP-D? 3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.
W2: It is not clear why exactly an LSTM-decoder is better than an attention-based decoder. The paper justifies that ""AM loses its strong competency in routing multiple vehicles in coordination"". However, AM decoder still conditions ""on the current location of the vehicle and the current state of nodes"". Thus, I don't think it overlooks the interaction between different vehicles. It depends more on how you design the decoder. Compared to attention, an LSTM essentially adds to the historical decisions to the policy, not the interactions between vehicles. Therefore, it is not clear why exactly LSTM-decoder is better, and the justification is quite vague in the paper.
W3: Except for AM, NM by Nazari et al. (2018) has also been an important counterpart of the proposed HM. However, it is not compared as a baseline. Whereas I understand that not every baseline should be compared, but NM is mentioned a few times throughout. If historical information is important in decoding an action, why is it not important in encoding a state? Because of this, the empirical evaluation is not totally convincing to me.","3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a contradiction in the assumptions made in the paper, without suggesting any specific actions or experiments that could be taken to resolve the contradiction or to further investigate the matter. The review point also does not provide any implicit actionable comments that could be inferred from the review point. It simply states the contradiction and leaves it at that. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither suggests direct and explicit actionable comments nor includes implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The reviewer simply points out a potential contradiction in the assumptions made in the paper. This is a neutral observation that does not engage with the content in a way that would be considered polite or impolite. The reviewer's tone is neither aggressive nor encouraging, but rather a straightforward statement of a perceived issue. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",0,"The review point in question does not make any claims, but rather points out a potential contradiction in the assumptions made in the text. The reviewer is questioning the logic behind allowing unlimited distance for the drone and then stating that the drones need to be recharged. This contradiction is not verified, and the reviewer does not provide any logical deductions or references to support their claim. The reviewer also does not provide any justification or explanation for why they believe the assumptions are contradictory. Therefore, the review point does not meet the criteria for verifiability as it lacks any claims that are verified. The reviewer's concern about the limited setting and its potential lack of interest to a large audience is also not relevant to the aspect of verifiability. So the overall score is 0.",-1,"The review point in question does not provide a clear and specific reference to the paper's content. It vaguely refers to the second paragraph of section 2.1, but fails to identify the exact assumptions that are being questioned. The review point does not specify which parts of the paper are related to the assumptions in question, nor does it provide a clear direction for the authors to address the contradiction. This lack of specificity makes it difficult for the authors to identify the exact parts of the paper that need to be addressed. As a result, the authors are left with uncertainty about which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and thus, the authors are not confident about which part of the paper is being addressed by the review point. So the score is -1."
587,ICLR_2022_1057,"Weakness:
Experiments: 1. Why experiments do not contained the same as RIS? Just for completeness to show that you do better than RIS on their experiments. 2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?",2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the differences between the Skew-fit algorithm and the proposed method without suggesting any specific actions or improvements that could be made. The review point also lacks any implicit actionable comments that could be inferred from the text. It simply states the limitations of the proposed method without offering any solutions or further research directions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point in question does not exhibit the level of politeness expected in a constructive critique. The tone is somewhat confrontational, with phrases like ""Why you don't consider to compare to the Skew-fit algorithm?"" and ""Even if its different there some similarity on the curriculum learning with distribution that keep being modified?"" These statements can be perceived as accusatory and lacking in respect for the authors' work. Additionally, the review point does not provide any constructive feedback or suggestions for improvement, which is a key aspect of a polite and helpful critique. Instead, it focuses on the perceived shortcomings of the paper without offering any guidance or encouragement. This approach is not conducive to fostering a positive and productive dialogue between the reviewer and the authors. Therefore, based on the score rubric, the tone of the review point is not helpful or encouraging, and it does not meet the criteria for a polite and constructive critique. So the overall score is -1.",0,"The review point in question does not contain any claims or assertions that require verification. It merely poses a question about the comparison to the Skew-fit algorithm, which is not a claim but rather a query about the methodology. The review point does not make any claims about the validity of the Skew-fit algorithm or its similarity to the curriculum learning with distribution that keep being modified. It also does not provide any logical deductions or references to external or internal sources that could verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it neither contains claims nor provides any form of justification or explanation. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. It vaguely refers to a comparison with the Skew-fit algorithm and mentions a similarity in curriculum learning with distribution that keeps being modified. However, it does not specify which sections of the paper are related to this comparison or how the similarity manifests in the paper. The lack of specificity in the review point makes it difficult for the authors to pinpoint the exact parts of the paper that are being addressed. This ambiguity could lead to confusion and uncertainty among the authors, as they may not be sure which parts of the paper are being critiqued. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would likely be unsure about the paper part that is being addressed by the review point comment."
1723,ICLR_2023_2202,"Weakness: 1. The results are not stunning. The multi-task training does not lead to consistently better performance on all graph-structure prediction tasks. The performance improvement on downstream tasks is not effective. 2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.","2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of rigor in the experimental design and the need for further discussion or explanation of the experimental results. The review point does not suggest any specific actions or improvements that the authors should make. It also does not provide any implicit actionable comments that the authors can infer after reading the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper and the potential implications of the experimental design. The feedback is factual and does not contain any negative or positive connotations, which is in line with the score of 0. The reviewer does not make any personal judgments about the quality of the paper or the authors' abilities, but rather provides a critique of the experimental design and the potential impact on the paper's results. This approach is consistent with the neutral tone expected in a review setting. Therefore, the review point is appropriately scored as 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that some experimental settings are not rigorously designed and that the experimental results could be further discussed or explained. This is a factual statement that does not require any form of verification. The review point does not make any claims that could be supported by logical deductions or external or internal sources. It also does not make any claims that could be invalidated or questioned. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not contain any claims that could be supported or invalidated. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It points out that the experimental settings are not rigorously designed and suggests that the MTT should test on unseen tasks. This review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is referring to the experimental design and the need for further discussion or explanation of the model behavior under different datasets/settings.

In contrast, the review points that lack specificity are vague and do not provide clear guidance for the authors. For example, the review point that mentions the paper is hard to follow and certain sections need to be made clearer does not specify which sections are unclear or how they could be improved. Similarly, the review point that suggests the writing should be improved and some points in the paper are unclear to me does not provide any concrete suggestions for improvement.

The review points that correspond to a score of 0 are somewhat specific but do not fully convey the necessary information for the authors to confidently identify the parts of the paper being addressed. For instance, the review point that questions the relevance of the proposed label classes in claim detection does not clearly state what aspects of the paper are being questioned.

In conclusion, the review point provided is specific and directly addresses a particular aspect of the paper, making it easy for the authors to identify which parts of the paper are being addressed. The review points that lack specificity are vague and do not provide clear guidance for the authors. The review points that correspond to a score of 0 are somewhat specific but do not fully convey the necessary information for the authors to confidently identify the parts of the paper being addressed. Therefore, the review point provided is more effective in addressing the specificity aspect of the score rubric."
2250,ACL_2017_104_review.json,"Weaknesses: - Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion: ",- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the difficulty in determining the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. This statement does not provide any guidance or direction for the authors to follow. It lacks any actionable comments that can be implicitly inferred, and it does not suggest any limitations that could be addressed with specific actions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it suggest implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the difficulty in determining the impact of the sense disambiguation order without comparison to other unsupervised entity linking methods. This is a factual observation and does not contain any personal opinion or judgment. The reviewer does not make any negative or positive comments about the paper, which is why the tone is neutral. The reviewer's tone is consistent with the score of 0, as it does not fit well with the other two scores. Therefore, the review point is neutral and factual, and it does not contain any elements of politeness or rudeness.",-1,"The review point in question makes a claim about the difficulty of determining the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. However, it fails to provide any verification or justification for this claim. The review point does not offer any logical deductions or references to external or internal sources that could support the claim. It simply states the difficulty without any supporting evidence or explanation. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified. Therefore, based on the score rubric, the review point does not meet the standards for verifiability.",-1,"The review point provided is quite vague and does not offer a clear direction for the authors to address. It lacks specificity and does not provide any concrete examples or suggestions for improvement. The review point does not specify which parts of the paper are being addressed, nor does it offer any guidance on how to improve the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors are left with no clear direction on how to address the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
2803,NIPS_2022_1523,"Weakness:
1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data.
[1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.
2 Unclear model design: The model architecture and learning details are fragmented or missing. The authors could either provide a plot of model illustration, pseudo-code table, or code repository. Considering that Neurochaos Learning is not a well-known method, it is important to demonstrate integrated details to facilitate reproductivity.
3 Experimental design: The experiments regarding Coupled autoregressive (AR) processes and Coupled 1D chaotic maps etc. don't seem to be well-motivated. Could the authors reason why particularly using such setting to investigate cause-effect of time series. Lastly, the comparison to a five-layer neural network seems to be less convincing, given the rapid developments of deep learning architectures.
Yes, the authors fairly discussed the limitations of the method. The potential negative impact may not be applicable to this study.","1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data. [1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of clarity in the discussion of causality and the terminology used. The reviewer suggests that the authors should provide the exact definition of causality and discuss its connection to causal variable identification, but it does not offer any specific guidance on how to do this. Furthermore, the reviewer questions the relevance of the 'classification of cause-effect' terminology without providing any constructive feedback on how to address this issue. The review point does not include any actionable comments that can be implicitly inferred, and it does not suggest any limitations that could be addressed with explicit or implicit actions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is not particularly polite. The reviewer's tone is somewhat critical and lacks the constructive approach that would be expected in a review. The reviewer's comments about the definition of causality and the terminology used are not phrased in a way that would be helpful to the authors. Instead, the reviewer seems to be questioning the validity of the authors' work, which could be perceived as disrespectful. The reviewer also fails to provide any constructive feedback or suggestions for improvement. This approach does not align with the score rubric's criteria for politeness, which emphasizes helpfulness and encouragement. Therefore, the review point does not meet the criteria for a score of 1, as it lacks the necessary politeness and constructive tone.",-1,"The review point provided contains a claim regarding the discussion of causality in the manuscript. The claim is that the authors should have explicitly given the definition of causality and discussed related definitions. However, the review point does not provide any logical deductions or references to support this claim. It merely states that the lack of explicit definitions and related discussions makes it hard for readers to understand the main idea. This lack of verification and justification for the claim is why the review point does not meet the criteria for a score of 1. The review point also fails to provide any factual statements or claims that do not need verifications, which is why it does not meet the criteria for a score of 0. Therefore, based on the score rubric, the review point is not verified and does not contain any claims that are supported by logical deductions or internal or external sources. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses a particular aspect of the manuscript, which is the discussion of causality. The reviewer has pointed out the lack of explicit definition of causality and the confusion caused by the term 'classification of cause-effect'. This specificity in the review point allows the authors to identify exactly which parts of the paper are being addressed. The reviewer's request for the exact definition of causality and the clarification of the term 'classification of cause-effect' is clear and actionable, which would help the authors to improve their manuscript. The reviewer has also provided relevant literature references, which can be used by the authors to better understand the nuances of causality. Therefore, the authors can be confident about which part of the paper is being addressed by the review point."
5224,NIPS_2020_1363,"- Even if MLE was better than DuetRL in objective evaluations, it is not a reason to leave DuetRL out of subjective evaluations. - The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references. - The quality and reusability of Chinese folk music transcriptions are not discussed. I think that the dataset is fine however, the usage of this dataset has to be justified by the authors, at least part of the supplementary. See the next three comments below: 1) Folk music is transmitted orally; scores are transcriptions into a single melody. These transcriptions do not typically include heterophony and other stylistic features like glissandi, ornaments, dynamics contrasts, etc. that characterize the folk song genre or style. The authors should discuss the limitations of the lack of performance elements in the transcriptions. 2) Another important aspect is the collection process and purpose of the transcriptions and if they are suitable for music generation? Given that the data is retrieved from Essen folk collection, is it possible that European scholars who prepared transcriptions with a Western classical music perspective and without adequate understanding of the characteristics of Chinese folk music? Are we sure that the transcription process retained the melodic characteristics? Such transcriptions are sometimes useful for descriptive reasons from a ""Western"" point of view, and they are not intended for prescriptive work, e.g., to be used in performance practice. (https://www.amherst.edu/system/files/media/1770/Seeger%252520-%252520Prescriptive%252520and%252520Descriptive%252520Music-Writing.pdf). There is no discussion if these transcriptions are re-usable for rendering music. To the authors' defense, I explored the dataset and Essen archives for more evidence (https://wiki.ccarh.org/wiki/EsAC) and consulted an ethnomusicologist specializing in Chinese music to validate the dataset. The response was that the transcriptions sound OK albeit that lack the performance elements as described above. Nevertheless, - since it is going to be the first paper on the topic - the authors need to include these discussions as part of the study (paper or the supplementary). 3) There are many different Chinese folk music traditions, which significantly differ in characteristics due to region, ethnicity, and era. This is not acknowledged by the authors, except in Section 4.1: ""It comprises 2250 traditional Chinese folk songs, from Han, Natmin, Shanxi, and Xinhua origins."" However, the origins are confusing: Han is an ethnicity; Shanxi is a province; Xinhua is a small county or the official state-run press agency, and the transcriptions listed under Xinhua are from all over China and modern-day Mongolia; I am not sure what Natmin is. This is a sloppy explanation from the authors' side.","- The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided lacks actionable steps or comments that could guide the authors in improving their work. It merely points out the weaknesses in the explanation of the musical motivation and background, without suggesting any specific actions or experiments that could be done to address these weaknesses. The review point also fails to provide any implicit actionable comments that could be inferred by the authors. Instead, it leaves the authors without a clear direction on how to proceed with their work. This lack of actionable content is why the review point does not meet the criteria for a higher score. The authors are left without any guidance on how to improve their work, which is a significant drawback. Therefore, based on the score rubric, the review point does not provide any direct or explicit actionable comments, and it does not include any implicit actionable comments. This makes the review point ineffective in guiding the authors towards enhancing their work. So the overall score is -1.",-1,"The review point provided is not polite and comes across as aggressive and disrespectful. The tone is critical and dismissive, with phrases like ""The musical motivation and background are explained weakly"" and ""There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20)."" This approach does not encourage the authors to improve their work or provide constructive feedback. Instead, it belittles the authors' efforts and fails to acknowledge any potential merits of their work. The review point also lacks any form of encouragement or helpful suggestions, which are essential for a constructive review. The tone is not neutral or factual, but rather negative and confrontational. Therefore, based on the score rubric, the review point is not polite and does not meet the criteria for a helpful or encouraging tone. So the score is -1.",-1,"The review point in question makes several claims about the musical motivation and background of the compositions in question. However, it fails to provide any verifiable evidence or sources to support these claims. For instance, the review point mentions the use of counterpoint and Chinese folk music but does not cite any musicological sources or provide any information about the composers or their prominence. Additionally, the review point raises questions about the nature of the compositions and their relevance to various contexts, such as Western classical music, Beijing opera, or film music, but does not offer any answers or justifications.

The lack of verification in the review point is a significant issue, as it does not allow the reader to assess the validity of the claims made. The review point also fails to provide any logical deductions or explanations that could help to verify the claims. Instead, it leaves the reader with unanswered questions and a lack of understanding of the subject matter.

In light of the score rubric, which emphasizes the importance of verifying claims through logical explanations or external sources, the review point in question does not meet the criteria for a score of 1. It contains several unverified claims and does not provide any justification or explanation to support these claims. Therefore, based on the evaluation criteria, the review point is not verifiable and does not meet the standards set for performance.",-1,"The review point provided is vague and lacks specificity. It fails to identify which parts of the paper are being addressed by the review point. The authors are left with no clear direction on how to improve their work based on the feedback. The review point does not provide any concrete examples or suggestions for improvement. It also does not cite any specific sources or references that could help the authors understand the feedback better. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are being addressed and to make any informed changes. Therefore, the score is -1."
2656,NIPS_2019_346,"weakness of the paper is a lack of theoretical results on the proposed methodology. Most of the benefits of the new model have been demonstrated by simulations. It would be very helpful if the authors could provide some theoretical insights on the relation between the model parameters and the tail dependence measures, and on the performance (consistency, efficiency etc) of the parameter estimators. Itemized comments: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired. 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2. 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general. 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful. 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.","1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a change in the quantile function without suggesting any specific actions or improvements that could be made. The review point also lacks any implicit actionable comments that could be inferred from the review. It simply states the change in the function and its potential impact without providing any guidance on how to address the issue or improve the function. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The reviewer simply states the facts about the new quantile function and its comparison to the existing function. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. The reviewer also does not provide any examples of politeness or lack of politeness, which is a requirement of the task. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the new quantile function (3) and its comparison to the existing function (2). The claim is that the new function changes the multiplicative factors into an additive term, which makes the function less sensitive to the tail parameters when they are large. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The review point simply states the change in the function without explaining why this change is beneficial or necessary. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that are verified or supported by logical explanations or references. So the overall score is -1.",1,"The review point provided is quite specific, focusing on the change in the quantile function from a multiplicative to an additive term. It clearly identifies the part of the paper that is being addressed, which is the function's sensitivity to tail parameters. The review point also points out the lack of supporting data to justify the change, which is a critical aspect of the paper. This specificity allows the authors to understand exactly what part of their paper is being critiqued, and they can confidently address the concerns raised in the review point. The review point is well-aligned with the score rubric, as it provides a clear and specific critique of the paper, which is essential for the authors to understand and improve their work. Therefore, the score is 1."
3138,NIPS_2017_15,"WEAKNESS AND CONCONERN
   	1) As already mentioned in previous section, the description lacks certain levels of details for complete reproduction, for example, how is the physics engine implemented, it's understandable that the authors left out some details with proper reference, however it is not very clear, in paricular as physics engine is an important component in the system, how the engine is set up and the set up affect the results.
   	2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.
   	3) The authors should also cite recent work on frame prediction. this is very related.","2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of comparison with previous results and the absence of a detailed explanation of the settings used in the block stability prediction. The review point does not suggest any specific actions that the authors could take to address these concerns. It also does not provide any implicit actionable comments that the authors could infer from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer's tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply points out the lack of comparison with previous results and questions the settings used in the block stability prediction. This approach is constructive, as it provides a clear direction for improvement without being overly critical or dismissive. The reviewer's tone is neutral, and the feedback is focused on the specific aspects of the paper that need attention. The reviewer does not make any personal judgments or comments about the quality of the paper or the authors' abilities. Instead, the reviewer sticks to the facts and offers a balanced perspective. Therefore, the review point is neutral and factual, which is why it receives a score of 0 according to the score rubric.",-1,"The review point in question raises concerns about the evaluation protocols for the billiard cases, specifically questioning why the authors did not compare the results to previous results on the datasets other than the proposed baseline. This review point contains a claim that the proposed fully reconstruction and simulation approach did not make a difference on a specific task over existing methods. However, the review point does not provide any logical deductions or references to external or internal sources to verify this claim. It merely states the concern without offering any justification or explanation. Therefore, the review point does not meet the criteria for verifiability as it lacks the necessary verification to support its claims. So the overall score is -1.",1,"The review point in question is quite specific, addressing the evaluation protocols and the comparison of the proposed method to previous results. It directly questions the lack of comparison to other datasets and the settings used for the block stability prediction. This specificity allows the authors to pinpoint exactly which parts of the paper are being addressed, which is crucial for them to understand and respond to the review point. The review point is clear and concise, making it easy for the authors to identify the areas of concern. It also provides a clear direction for the authors to improve their paper, by suggesting that they should compare their results to previous ones and ensure that the settings used are comparable. This level of specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not allow the authors to identify the exact parts of the paper that are being addressed. It would leave them unsure about which aspects of their work are being critiqued. This lack of specificity would result in a score of -1, as it would not help the authors to understand and address the review point effectively. 

Therefore, based on the score rubric, the review point in question is well-crafted and provides a clear and specific direction for the authors to improve their paper. It directly addresses the evaluation protocols and the comparison of the proposed method to previous results, which is essential for the authors to understand and respond to the review point."
3611,NIPS_2020_878,"In short, the paper misses some critical details and some explanations are not clear. After reading the paper and the supplementary materials, I still believe it is hard to reproduce their results. - The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem. - The meaning of the shaded regions of Figure 2 (right), Figure 5 (left), and Figure 6 (left) is never explained in the caption or main text. Morevoer, it neighor represents standard deviation or the range between the best and worst case because the line with the same color can go outside the shaded region. For example, the solid red line in Figure 5 below the red shaded region between 60 and 90 trained models. - In the equation after line 156: ""\sigma ( A H^l W H^l)"" should be ""\sigma( A H^l W )"". Is this a typo? Otherwise, this is not a graph convolution layer. I have low tolerance with a typo in the ""only one"" equation in a paper. - It is not clear how the adjacency matrix A is defined in line 155. A_ij is the edge of node i to node j or node j to node i? Although I can figure it out from Figure S1 in supplementary materials, it is not clear in the main paper. - Following the previously point, based on Figure S1, the graph convolution operation propogates the features ""backward"" in the graph (in the opposite direction of an edge). I wonder if the authors can explain why you made this decision instead of adding edges for each node to the ""global"" node and following the edge direction to do graph convolution? - The original GCN (Kipf and Welling) uses normalized adajancy matrix with renormalization trick. However, this paper uses the original adjacency matrix without explantation or ablation study. I wonder if there is a reason behind not using the conventional graph convoltuional operation.","- The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a limitation of the model without suggesting any improvements or further investigation. The reviewer does not provide any implicit actionable comments that could be inferred from the review point. The lack of actionable comments or suggestions makes it difficult for the authors to deduce what needs to be done to address the issue raised. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer simply states the fact that the outputs of the binary relation predictor model are not symmetric, and questions how the authors handle this problem. The reviewer does not offer any constructive feedback or suggestions for improvement. The tone is neutral and the reviewer does not make any personal judgments or comments about the quality of the paper or the authors' work. Therefore, the review point is in line with the score of 0, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question makes a claim about the outputs of the binary relation predictor model not being symmetric. This claim is not verified, as it lacks a justification or explanation. The reviewer simply states that the outputs are not symmetric and questions how the authors handle this problem, but does not provide any logical deductions or references to support this claim. The review point does not meet the criteria for verifiability, as it does not contain any claims that are verified or supported by logical explanations or references. Therefore, the review point is not aligned with the score rubric's requirements for verifiability. 

In contrast, a review point that contains a claim and verifies it would be scored a 1. It would be supported by logical deductions and internal or external sources, and the claim would be justified. For example, a review point that points out an error in the statistical analysis and provides a reference to the guidelines in 'Statistics for Biologists' would be a good example of a review point that meets the criteria for verifiability. 

So the overall score is -1, as the review point does not contain any claims that are verified, and it lacks justification, explanation, or reference.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the lack of symmetry in the outputs of the binary relation predictor model. The review point clearly identifies the problem and provides a concise explanation of why the outputs are not symmetric. It also suggests a potential solution to the problem, which is to handle the issue of symmetry in the model's outputs. This specificity in the review point makes it obvious which part of the paper is being addressed, and the authors can confidently identify the relevant sections of the paper. The review point is well-structured and directly addresses the issue at hand, without any ambiguity or uncertainty. Therefore, based on the score rubric, the review point is of high quality and demonstrates a clear understanding of the paper's content. So the overall score is 1."
225,ICLR_2022_2575,"Weakness:
1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?
2). 80.0% top-1 accuracy on ImageNet is not hard. Does the design principle can scale well across different (data size, model parameters, FLOPs)?
3). I don't know which of the proposed components contribute the most to the overall performance. What can be concluded from Appendix Figure 7 and Figure 8?","1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely questions the motivation of WGM if refined by SRM, without suggesting any specific actions or experiments that could be done. The review point does not guide the authors on what needs to be done or what aspects of their work could be improved. It lacks the actionability that is required for a score of 1. The review point does not give any hint or direct actionable comments, and after reading it, the authors still do not know what needs to be done. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point in question is not particularly polite or constructive. It begins with a question that could be interpreted as slightly confrontational, which may not be well-received by the authors. The tone of the review point is more neutral than anything else, which is why it does not score higher on the politeness scale. The reviewer does not provide any constructive feedback or suggestions for improvement, which is a missed opportunity to be helpful. Instead, the reviewer simply states the potential issue with the randomized selection without offering any solutions or alternatives. This lack of constructive feedback and the somewhat neutral tone of the review point do not align well with the criteria for a score of 1, which requires the review point to be helpful or encouraging. Therefore, the review point does not meet the criteria for a score of 1, and it is not particularly polite or constructive. So the overall score is 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a question that seeks to understand the motivation behind the use of WGM in the context of SRM. The review point is focused on the conceptual understanding of the methodology, rather than on the verification of any specific claims. It does not make any assertions that need to be substantiated with logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain a claim that needs to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or refuted. Hence, it is not relevant for the aspect of verifiability in review points. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The question posed by the review point is quite broad and does not provide a clear direction for the authors to follow. It lacks specificity and does not guide the authors towards a particular section of the paper or a specific aspect of the methodology. The review point does not offer a clear hypothesis or a specific area of the paper that needs further clarification or validation. It also fails to provide a clear rationale for the proposed methodology, which is essential for understanding the motivation behind the method. The lack of specificity in the review point makes it difficult for the authors to confidently identify which parts of the paper are being addressed. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that is specific and directly addresses a particular section or aspect of the paper would allow the authors to confidently identify which parts of the paper are being addressed. For example, a review point that specifically questions the rationale behind a particular methodology or the validity of a hypothesis would provide clear guidance for the authors. This would enable the authors to focus their efforts on addressing the specific concerns raised in the review point. 

In conclusion, the review point in question is not specific enough to guide the authors towards a particular section or aspect of the paper. It lacks the necessary specificity to allow the authors to confidently identify which parts of the paper are being addressed. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric."
2468,NIPS_2021_783,"Weaknesses: 1. The introduction of the knowledge coefficient matrix may raise the concern of privacy leaks and the communication cost. 2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting. 3. To my best knowledge, there are some other personalized FL works (Hierarchical Personalized Federated Learning for User Modeling, WWW'21; Exploiting Shared Representations for Personalized Federated, arXiv:2102.07078), which need to be listed as baselines.",2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states a conflict between the use of public data and the general setting of the paper, without suggesting any specific actions or experiments that could be taken to resolve this conflict. The review point does not offer any implicit actionable comments that could be inferred from the text, nor does it mention any limitations without providing any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor disrespectful, nor is it particularly helpful or encouraging. The reviewer simply states the conflict between the use of public data and the general FL setting, without adding any value or constructive criticism. The reviewer does not offer any suggestions or improvements, which could have made the review point more helpful. The reviewer also does not express any negative judgments or criticisms, which could have made the review point more aggressive or disrespectful. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question makes a claim about the use of public data in step 2 of the figure, stating that it conflicts with the general FL setting. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The review point simply states the conflict without any supporting evidence or explanation. This is a clear deviation from the score rubric, which requires claims to be verified with logical explanations or references. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified.",1,"The review point provided in the task is quite specific and directly addresses a particular aspect of the paper. It points out a conflict between the general setting of the paper and the use of public data in a specific step. This specificity allows the authors to identify exactly which part of the paper is being addressed, which is the main criterion in the score rubric. The authors can be confident that the review point is referring to the use of public data in step 2 of the paper. This level of specificity is in line with the score rubric's requirement for a score of 1. The authors can be confident about which part of the paper is being addressed by the review point, and they can also be confident that the review point is addressing a specific aspect of the paper. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
5707,NIPS_2018_845,"Weakness - Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks. - One of the main advantages from the simulated random-walks is parallelization (at least for sequence generation) and scalability. Section 3.5 is not enough for scalability argument.  * Detailed comments - Due to [28] and the attention usage in the language models, the attention is generally regarded per instance (or sentence/sequence) while here the attention is globally defined. The proposed method has its own value as the authors describe in the paper, but the terminology can lead to some confusion. - Some references such as [16] need to be polished. - Section 3.5 seems somewhat contradictory to the original purpose of trying the graph embedding because it will cut out the information that SVD cannot preserve. Also, whether Section 3.5 is just an idea or it is actually implemented to be used in experiments is not clear. It would be interesting to see the difference in performance between full-matrix context and SVD-approximate context. - Section 3.6 seems more promising to generalize the proposed idea beyond the simple random-walks. - While the authors sweep C for node2vec, the other hyperparameters are not mentioned where it is hard to believe that the default values of the other hyperparameters work best across the datasets. Mentioning the detail about C without mentioning the others naturally raise those questions, so the authors need to add some description.","- Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a limitation in the proposed method without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that the authors can infer, nor does it give any hints or direct actionable comments. It simply states the difference between the first-order and second-order Markov chains without offering any guidance on how to address this limitation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The review point simply states the difference between the proposed method and node2vec, without any additional commentary or suggestions for improvement. It is a straightforward comparison that does not offer any constructive feedback or criticism. The review point is also not passive, as it does not merely state facts without any further analysis or interpretation. It is a balanced review that neither praises nor criticizes the paper, but rather points out a specific difference in the methodology. Therefore, the tone of the review point is neutral, factual, or passive, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question makes a claim about the difference between the first-order and second-order Markov chains in the context of node2vec. However, it fails to provide any verification or justification for this claim. The review point merely states that the authors only deal with the first-order Markov chain and that this means D in node2vec is not obtainable by the power-series in the proposed method. It does not offer any logical deductions or references to external or internal sources that could support this claim.

Furthermore, the review point does not address the implications of this difference or how it affects the overall comparison between the proposed method and node2vec. It simply states the difference without any further analysis or discussion.

In light of the score rubric, which emphasizes the importance of verifying claims through logical explanations or references, the review point falls short. It does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains a claim that is not verified, which is why it would receive a score of -1.

Therefore, based on the evaluation criteria, the review point does not meet the standards for verifiability and thus would be scored as -1.",1,"The review point provided is specific and directly addresses a key aspect of the paper, which is the use of the first-order Markov chain in the proposed method. It clearly states that the authors only deal with the first-order Markov chain, and this is a significant difference from node2vec, which uses the second-order Markov chain. The review point also points out that all the random-walks are regarded the same as simple random walks, which is a crucial detail that the authors should be aware of. This specificity in the review point allows the authors to identify exactly which part of the paper is being addressed, and they can be confident about this. The review point is focused and directly related to the paper's content, making it easy for the authors to understand and respond to. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
5813,NIPS_2016_95,"Weaknesses 1. The time complexity of the learning algorithm should be explicitly estimated to proof the scalability properties. 2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained. ","2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states that the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same without offering any insight into why this is the case or suggesting any further investigation or experimentation. The review point does not prompt the authors to take any specific actions or to consider any particular aspects of their work. It also does not provide any implicit actionable comments that the authors could infer from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the observation about the time complexity of the TRMF-AR methods without providing any additional context or suggestions for improvement. The reviewer's tone is passive, and the feedback is purely factual, which is in line with the score of 0. The reviewer does not offer any constructive criticism or praise, which is why the score is 0. 

The reviewer's feedback is not overly critical or overly positive, and it does not contain any elements of politeness or rudeness. It is a straightforward observation that the time complexity for both TRMF-AR methods is the same. The reviewer's tone is neutral, and the feedback is purely factual, which is why the score is 0. 

In conclusion, the review point is neutral and factual, and it does not contain any elements of politeness or rudeness. The reviewer's tone is passive, and the feedback is purely factual, which is why the score is 0.",0,"The review point in question does not contain any claims, nor does it require verification. It merely states a fact about the time complexity of TRMF-AR for different sets of parameters. The claim is not verifiable as it does not make any assertions that need to be substantiated with logical deductions or references to external or internal sources. The review point is purely descriptive and does not attempt to verify any claims. Therefore, it does not meet the criteria for a score of 1, which requires a claim to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the time complexity of TRMF-AR for different sets of data. It clearly states that the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same, and it asks for an explanation for this observation. This specificity in the review point makes it obvious which part of the paper is being addressed, and the authors can confidently identify the relevant sections of the paper. The review point is focused on a particular aspect of the paper, which is the time complexity of the proposed method, and it does not leave any ambiguity about the part of the paper that is being addressed. Therefore, based on the score rubric, the review point is highly specific and directly addresses a particular aspect of the paper, which is why it would receive a score of 1."
4583,NIPS_2020_1678,"1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested. 2. I am not an expert in the are, but my impression is that the novelty of the work is somewhat limited. In particular the novelty is mainly to refine the framework of Dai et al. [4] on the particular problem, and to introduce the components of noise predictor and importance sampling for scalability.","1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the method without suggesting any specific actions that could be taken to address these limitations. The review point does not provide any implicit actionable comments that could be inferred by the authors. It lacks any guidance on how to improve the method or what further research could be conducted. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor does it provide implicit actionable comments. Hence, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not provide any encouragement or constructive feedback. The reviewer simply states the facts about the method and its performance, without offering any suggestions for improvement or expressing any opinions on the quality of the research. The reviewer's comments are focused on the method's heuristic nature and the lack of empirical evaluation, which is a factual observation rather than a critique. The reviewer does not make any personal judgments or comments on the quality of the paper or the performance of the method. Therefore, the review point is neutral and does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the method is heuristic and that its performance cannot be guaranteed without empirical testing. This is a factual statement that does not require any form of verification. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any assertions that could be evaluated for verifiability. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer a clear direction for the authors to follow. It lacks specificity and does not address any particular part of the paper. The authors are left with no clear guidance on how to improve their work based on the review point. The feedback does not help the authors to identify which parts of the paper are related to the review point, nor does it provide any actionable insights. The authors are left with a sense of uncertainty about what exactly needs to be addressed in their paper. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. The authors are not confident about which part of the paper is being addressed by the review point, and they have no idea about the paper part that is being addressed by the review point comment. So the overall score is -1."
4000,NIPS_2020_1878,"- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors? - Authors DBSCAN++ show cases where DBSCAN++ is even providing better clustering partition than for DBSCAN. Could the authors perform a similar analysis here with SNG-DBSCAN? - DBSCAN++ is also applied to outlier detection. How is performing SNG-DBSCAN compared to DBSCAN++ in this context? - Which parameters are used with DBSCAN for time and space consumption comparison? - Would it be possible to have empirical results on datasets for which DBSCAN is actually working? It seems that DBSCAN is performing well only for Still. Hence DBSCAN and SNG-DBSCAN are equally performing bad on the remaining sets...","- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a perceived weakness in the empirical evaluation and suggests that the authors should provide experiments for which sampling rate is optimized for both competitors. However, it does not provide any guidance on how to conduct these experiments or what specific experiments should be performed. The review point also does not offer any implicit actionable comments that the authors can infer from the review point. It simply states that the comparison with DBSCAN++ is not fair without providing any constructive feedback or suggestions for improvement. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the expected tone of a constructive review. The language used is confrontational and dismissive, which is not conducive to a helpful and encouraging review. The reviewer's tone is aggressive, as seen in phrases like ""The comparison with DBSCAN++ is not fair"" and ""Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2), this is definitely not fair."" This approach does not foster a positive dialogue and fails to provide constructive feedback that could help the authors improve their work. The reviewer's tone is not neutral or factual, but rather aggressive and disrespectful. Therefore, the review point does not meet the criteria for a helpful, encouraging, or neutral tone. So the overall score is -1.",-1,"The review point in question makes a claim about the fairness of the comparison between the authors' approach and DBSCAN++. It suggests that the same sampling rate should not be used for both methods, as they have different requirements. However, the review point does not provide any logical deductions or references to support this claim. It merely states that the comparison is not fair without offering any justification or explanation. This lack of verification and support for the claim is why the review point does not meet the criteria for a score of 1. The review point also fails to provide any factual statements or claims that do not need verifications, which would have been necessary for a score of 0. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a higher score. So the overall score is -1.",1,"The review point provided is quite specific, focusing on the comparison between the authors' approach and DBSCAN++. It directly addresses the runtime complexity and the sampling rate used in both methods. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. It also provides a specific suggestion for the authors to provide experiments for different sampling rates, which shows a clear understanding of the paper's content and the review point's requirements.

The review point is well-structured and directly addresses the comparison between the authors' approach and DBSCAN++, which is the main focus of the paper. It also provides a specific suggestion for the authors to provide experiments for different sampling rates, which shows a clear understanding of the paper's content and the review point's requirements.

The review point is also well-aligned with the score rubric, as it demonstrates a high level of specificity and directly addresses the paper's content. The authors can be confident about which part of the paper is being addressed by the review point, and they can also understand the implications of the review point on their work.

In conclusion, the review point is well-structured, specific, and directly addresses the paper's content, making it easy for the authors to identify and respond to the review point. It also provides a clear suggestion for the authors to improve their work, which is in line with the score rubric's criteria for a score of 1."
4808,NIPS_2020_1318,"1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space). 2. In a similar vein, I was envisioning a simple extension of SG-MCMC methods where we just do occasional random walks/Brownian motion in the original space (using Gaussian noise and ignoring the energies entirely) using some relatively cheap heuristic to detect if the fancier iterate sequence is stuck in local optima. Something like that would help explore the entire space (triggering some iterations of random walk if heuristic flags a local optima) without changing the underlying energy used in the real Sg-MCMC at each iteration. Wonder how that would compare to a method like the one in the paper. 3. Following from these, it would have been nice if the authors included synthetic experiments with asymmetric modes (one small mode to one side of a 2d space and one taller one on the other side) where the real mean would not be close to the 'center' and thus would not be easily approximable by a flatter version of the energy. Comparisons with something like the approach in 2. above would help understand the main insight of the paper better. 4. Runtime plots in the experiment sections on real data would be nice to have, for example plots of energy/error vs wall clock time / iteration count as provided for the synthetics. 5. Some insight on how to choose \delta_{u} and partition count for real datasets (like the values mentioned on line 173 for synthetic) would be nice to have.","1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It mainly discusses the limitations of using a flattened surrogate energy and the potential impact on point estimation situations. However, it does not suggest any specific actions or improvements that could be made to address these limitations. The review point also does not provide any implicit actionable comments that could be inferred from the text. It simply states the potential issues without offering any solutions or further investigation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of 0.",0,"The review point provided is quite detailed and offers a critique of the methodology used in the paper. It discusses the implications of using a flattened surrogate energy and how this might affect the accuracy of the samples obtained. The reviewer also considers the potential for the algorithm to escape local optima, which is a positive aspect of the method. However, the tone of the review point is somewhat critical and could be perceived as slightly aggressive. The reviewer seems to be questioning the suitability of the method for most sampling situations, which could be interpreted as a lack of confidence in the paper's approach. The reviewer also suggests that the method might not be as accurate for skewed distributions of modes, which could be seen as a negative critique. While the reviewer does provide some constructive feedback, the overall tone is not as helpful or encouraging as it could be. Therefore, based on the score rubric, the review point is not as polite as it could be, and it does not fully meet the criteria for a score of 1. So the overall score is 0.",-1,"The review point provided makes a claim about the suitability of using a flattened surrogate energy in sampling situations. It discusses the potential deviation from the true distribution and the implications for point estimation. However, the review point does not provide any verification or justification for its claims. It lacks logical deductions or references to external or internal sources that could support the claims made. The review point also does not address the potential inaccuracies that could arise from flattening energies with skewed distributions of modes. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It contains claims that are not verified, and it misses a justification, explanation, or reference. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the use of a flattened surrogate energy in the paper. It clearly identifies the potential issues with this approach, such as the deviation from the true energy and the impact on point estimation. The review point also provides a nuanced understanding of the potential accuracy issues when dealing with skewed distributions of modes. This specificity allows the authors to confidently identify which parts of the paper are being addressed by the review point. The review point is well-aligned with the score rubric, as it demonstrates a clear understanding of the paper's content and provides a detailed critique of the method used. Therefore, the score is 1."
1482,ICLR_2023_3023,"Weaknesses: 
 1. The proposed technique does not seem to predict uncertainty. To the best
  of my understanding, it predicts where the original model is making
  errors. An easy way to see this would be to consider a point where the
  model is confident but inaccurate, i.e., a bias. The predicted mask
  should cover this point, even though it is not an uncertain prediction
  of the model. The problem is actually in the definition of
  ``uncertainty''. Equation 1 (or 3) do not necessarily only correspond to
  uncertainty. It corresponds to expected error for a sample. It also
  includes the contribution of bias. In this regard, I think the
  uncertainty positioning may not be very accurate. Consequently, the
  comparison with a method that computes confidence intervals may not be
  appropriate. Here, I should note that predicting expected error or the error is not a
  bad target. However, the difference between this and uncertainty
  estimation should probably be made clear. 
 2. I am not sure about the contribution of the corollary 1. The result is
  not very surprising in my opinion. The mask that aims to minimize the
  ``uncertainty'' is bound to be related to the ``uncertainty''. 
 3. It is unclear how authors estimate the expectation in Equations 4
  or 5. This is over y x variable. However, in Equation 6, they drop the
  expectation and simply take only one sample to compute this
  expectation. Using this I am assuming the model learns to predict the
  error and not the expected error. As a result, the model may not be able
  to identify an uncertain prediction at a pixel for a given sample since
  for that pixel, there may be only 1 output that happens to be close to
  the ground truth. 
 4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0,
  m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} =
  \infty. It is unclear how authors deal with this. Furthermore, the
  intuition of this specific calibration form is unclear. ","4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0, m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} = \infty. It is unclear how authors deal with this. Furthermore, the intuition of this specific calibration form is unclear.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a potential issue with the equation and the intuition behind it, without suggesting any specific actions or improvements that could be made. The review point also does not provide any implicit actionable comments that could be inferred by the authors. It simply states the problem without offering any guidance on how to address it. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The reviewer simply states the facts about the equation and its behavior, without adding any personal opinion or judgment. The reviewer also does not provide any constructive feedback or suggestions for improvement, which could have been helpful in guiding the authors. The reviewer's tone is neutral and the content is factual, which is why the score is 0.",-1,"The review point in question makes a claim about the behavior of the equation when m_{\theta} and m_{\lambda} are set to specific values. However, it fails to provide any verification or justification for this claim. The review point merely states that the behavior is suspicious and unclear, without offering any logical deductions or references to external or internal sources that could substantiate the claim. This lack of verification and justification is in direct contrast to the score rubric's requirement for a review point to contain claims that are verified. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any verified claims. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which part of the paper is being addressed. The review point mentions that the behavior of the equation is suspicious and that the intuition of the calibration form is unclear, but it does not provide any concrete examples or suggestions for improvement. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being criticized or what changes they should make. The review point does not guide the authors on how to address the issues raised, nor does it provide any constructive feedback. Therefore, the authors would likely be unsure about which parts of the paper are being addressed by the review point. So the overall score is -1."
4527,NIPS_2020_84,"- I think the definition of ""near convex"" is quite restrictive towards the necessities of the sensitivity framework. However, it is presented as a very general and natural class of functions. The limitations are not clearly discussed in my opinion. - related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.",- related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the work without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred, which is a key aspect of the score rubric. It lacks any guidance or suggestions that could help the authors improve their work or understand the implications of their findings. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the facts about the SVM result and the structure of the data, without adding any personal opinion or judgment. The reviewer also refrains from making any suggestions or comments that could be perceived as polite or impolite. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the SVM result depends on regularization and the structure of the data, which is a factual statement that does not require any justification or reference. The review point does not make any claims that could be verified or invalidated, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the score is 0.",0,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions the SVM result, regularization, and the structure of the data, but it does not specify which sections of the paper these elements are related to. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The authors might be unsure about which parts of the paper are being discussed, which could lead to confusion or misinterpretation of the review point. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors are not fully confident about the paper part that is being addressed by the review point comment. So the score is 0."
2141,ARR_2022_210_review,"1) Improvements are overstated. The abstract mentions "" two-fold improvements"" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?
2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.
There are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as ""woman"" with prob 2/100, ""king"" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.
3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.
4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?
5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. 
There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.
The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.
Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.
Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.
- Line 195: plane -> hyperplane - Figure 1 caption: middle -> midpoint - Line 1453: ""the tanh removes the magnitude of facets"" -> Could you explain this in more detail? I could not make sense of why tanh ""removes the magnitude"" - what do you mean specifically by ""magnitude"" here?
- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).
- There is a mistake in rows ""MFS - Multi partition"" and ""MFS - Multi Input"" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.
Rephrasing: Line 063: ""output the woman or king."" - > ""output woman or king"" Line 121: ""is sometimes not be able"" -> ""is sometimes not able"" Line 1271: ""the GPT-2"" -> ""GPT-2"" Line 235: ""multi-mode"" -> ""multimodal"" also line 263. 
Line 911: ""As in section 5"" -> ""As in Section 5"" ","5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger. The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful. Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format. Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of clarity and detail in the paper, such as the absence of an explanation for the choice of H and W, the omission of the bias term in the softmax, and the unusual formatting of the captions. However, it does not suggest any specific actions that the authors could take to address these issues. The review point also fails to provide any implicit actionable comments that the authors could infer from the review. Instead, it simply states the problems without offering any solutions or guidance. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither provides explicit actionable comments nor implies any implicit actionable comments. So the overall score is -1.",-1,"The review point provided is not polite and is quite aggressive. The reviewer uses phrases like ""it's incomprehensible"" and ""the reader of the paper would struggle to understand,"" which are not constructive and can be seen as disrespectful. The reviewer also makes a negative comment about the format of the paper, which is not relevant to the content of the paper and can be seen as a personal attack. Furthermore, the reviewer fails to provide any constructive feedback or suggestions for improvement. Instead, the reviewer simply criticizes the paper without offering any helpful insights or guidance. This approach is not conducive to a productive review process and does not align with the score rubric's criteria for politeness. So the overall score is -1.",-1,"The review point contains several claims that are not verified. The claim about the probability distribution of the top-k results and the softmax is not justified or explained. The reader is left without understanding why this is an issue. The claim about the bias term in the softmax is also not addressed, leaving the reader without a clear understanding of its potential impact on the output layer. The choice of H and W is mentioned but not explained, which could lead to confusion for the reader. The unusual formatting of the captions in table 3 and table 4 is noted but not explained, which could be seen as a lack of attention to detail. The missing reference to the main point of theorem 1 is not justified or explained, leaving the reader without a clear understanding of its significance. Overall, the review point lacks the necessary verification of its claims, which is essential for the reader to fully understand and appreciate the paper's content.",1,"The review point provided is quite specific and directly addresses several key points in the paper. It points out the lack of explanation regarding the probability distribution of the top-k results and the softmax function, which is a crucial aspect of the paper's methodology. The review point also highlights the absence of discussion on the bias term in the softmax, which is a significant omission given its potential impact on the output layer. Furthermore, the review point questions the choice of H and W, indicating a need for more detailed justification. The review point also raises concerns about the formatting of the captions in the tables, which is a deviation from the standard format for ACL papers. Lastly, the review point brings up a missing reference, which is a critical oversight given the historical significance of the main point of theorem 1. These specific points of critique demonstrate a clear understanding of the paper's content and the issues that need to be addressed. Therefore, the authors can be confident about which parts of the paper are being addressed by the review point."
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out a lack of an ablation study on the hyper-parameter K, which is not an actionable comment. The reviewer does not suggest any specific actions or experiments that the authors should undertake to address the issue. Instead, it leaves the authors without a clear direction on what needs to be done. This lack of actionable comments is why the review point does not meet the criteria for a score of 1. The review point does not give any hint, or direct actionable comments, and after reading the review point, the authors still do not know or can not deduce what needs to be done. Therefore, the review point is not actionable and does not provide any value to the authors in terms of guiding them on how to improve their work.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The reviewer simply states their observation regarding the absence of an ablation study on the hyper-parameter K, which is a factual statement. The tone is neither rude nor encouraging, but rather neutral and factual. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a key aspect of the rubric. Therefore, the review point is consistent with the score of 0, as it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the consistency of the theorem with empirical observations, specifically regarding the gradient estimation and its relationship with the number of samples N. However, the review point does not provide any verification or justification for this claim. It merely states that an ablation study on the hyper-parameter K was not found, which does not directly address the claim made about the theorem.

The review point also fails to provide any logical deductions or references to external or internal sources that could support the claim. This lack of verification and justification is in direct contrast to the score rubric, which requires claims to be supported by logical deductions and internal or external sources.

In conclusion, the review point contains a claim that is not verified, and it misses a justification, explanation, or reference. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and would receive a score of -1.",1,"The review point provided is quite specific, addressing a particular claim made in the paper regarding the relationship between the size of the dataset and the bias of the gradient estimation. It directly questions the consistency of this claim with empirical observations, which is a clear and focused critique. The review point also points out a lack of an ablation study on a specific hyper-parameter, which is a relevant and specific aspect of the paper. This level of specificity allows the authors to identify exactly which parts of the paper are being addressed by the review point, and they can be confident about the nature of the critique. Therefore, based on the score rubric, the review point is highly specific and directly addresses a part of the paper, making it clear which aspects are being critiqued. So the overall score is 1."
2707,NIPS_2022_776,"Weakness:
1.The technique contribution is limited. This paper mainly uses two existing techniques, DP-SGD and gradient matching.
2.This work only uses one algorithm from data condensation, i.e., gradient matching. It would be better if the authors can try more algorithms so the community can have a better understanding about data condensation for differentially private data generation. For example, distribution matching [1] that minimizes the distance between the averaged feature of real data and the averaged feature of synthetic data, which is also easy to implement with DP.
3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?
4.(Minor) Line 121, minimized -> minimize.
[1]: DATASET CONDENSATION WITH DISTRIBUTION MATCHING, https://arxiv.org/pdf/2110.04181v1.pdf.",3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states the comparison between the visual quality of the algorithm and the visual quality of data directly generated from the generator. There is no suggestion of what the authors should do or what experiments they could conduct to improve the algorithm. The review point does not offer any implicit actionable comments that the authors could infer, nor does it mention any limitations without providing any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the author. The reviewer simply states the facts about the algorithm and the generator, without offering any constructive feedback or criticism. The reviewer also refrains from making any personal judgments about the quality of the work or the author's abilities. The review point is purely objective and does not contain any elements of politeness or rudeness. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the visual quality of the algorithm and its comparison with data generated from a previous work. However, it fails to provide any form of verification or justification for this claim. The review point does not offer any logical deductions or references to external or internal sources that could substantiate the claim. This lack of verification and justification is a clear deviation from the score rubric's requirement for verifiable claims. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains a claim that is not verified, which is why it would receive a score of -1.",1,"The review point provided in Section 6 is quite specific and directly addresses the comparison between the visual quality of the algorithm and the visual quality of data generated from a previous work. It clearly identifies the part of the paper that is being addressed, which is the improvement in visual quality due to the generator from a previous work. The authors can be confident that the review point is focused on this particular aspect of the paper. The review point is also well-structured, making it easy for the authors to understand which part of the paper is being discussed. It does not leave any ambiguity about the paper part being addressed. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper part in question, which aligns with a score of 1."
1885,ARR_2022_24_review,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. 
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ","- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the existence of interpretable topological relations in embeddings without suggesting any specific actions or methods to explore or verify these relations. The review point also does not include any implicit actionable comments that could be inferred by the authors. It simply points out the existence of these relations without providing any guidance on how to approach or investigate them further. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the technical aspects of the paper. The feedback is based on the content of the paper and does not include any personal judgments or comments on the quality of the paper. The reviewer's tone is consistent throughout the feedback, maintaining a neutral stance. The feedback is also concise and to the point, which is a characteristic of a neutral tone. Therefore, based on the score rubric, the review point is neutral and factual, and the tone is consistent with the score of 0.",0,"The review point provided does not contain any claims or make any assertions that require verification. It simply suggests a method to check if embeddings of L2 have interpretable topological relations, such as analogy. This is a factual statement that does not need any justification or reference to external or internal sources. The review point does not make any claims about the performance of the LM or the quality of the embeddings, nor does it compare the results to previous work or discuss the implications of the findings. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains a claim nor verifies it. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the interpretability of embeddings in L2. It specifically mentions the potential for topological relations, such as analogy, which is a clear and focused area of interest. This level of specificity allows the authors to pinpoint exactly which part of the paper is being discussed, and they can be confident that the review point is directly related to their work. The review point is not only specific but also directly related to the pre-training of the LM, which is a key element of the paper. Therefore, the authors can be confident that the review point is addressing a specific part of their paper, which is the use of conditioned token sampling in the LM pre-training. 

In contrast, the review points that lack specificity do not provide clear guidance on which parts of the paper are being addressed. The authors are left with uncertainty and are unable to identify which parts of the paper are related to the review point. This lack of specificity makes it difficult for the authors to understand the reviewer's concerns and to address them in their paper. 

The review point provided is well-aligned with the score rubric, as it clearly identifies the specific part of the paper that is being addressed, and it does so in a way that is both specific and directly related to the pre-training of the LM. Therefore, the score is 1, as the authors can be confident about which part of the paper is being addressed by the review point."
5162,NIPS_2020_1750,"- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence. - Choice of methodology: 3.1: The proposed experimental methodology is poorly justified. The pruning methodology (3.1) does not seem to directly correspond to previous approaches which have found found that pruning increases generalization. The experiments are ran on two unconventional networks on CIFAR-10, using unconventional pruning schedules. These networks reach much lower accuracy than expected for CIFAR-10 (85%-87% test accuracy), possibly due to the fact that these networks are relatively overparameterized for CIFAR-10 (a CIFAR-10 ResNet-20 has 0.27M parameters and typically reaches about 91% test accuracy; the ResNet-18 in the paper has 11.5M parameters, and the VGG-11 has about 110M) and L1/L2 regularization is disabled. It is therefore hard to extrapolate these results beyond the two CIFAR-10 networks and their bespoke hyperparameters chosen in the paper. - 3.3: the choice of duration of holding parameters at zero, and the conclusions drawn from it, don't seem justified from the data: the drop in test accuracy from ""Zeroing 50"" does not seem to be the same as the drop in accuracy from ""Prune_L"", and it is unsurprising to see that any regularization technique improves test accuracy for these heavily overparameterized CIFAR-10 networks, so the claim that ""pruning-based generalization improvements in overparameterized DNNs do not require the model’s parameter count to be reduced."" (lines 232-233) is hard to extract from just this experiment on these networks. Overall: the work would be significantly strengthened by having much more well-justified and clearly presented methodology for the networks that are used, the experiments that are performed, and the conclusions that are drawn from those experiments. # UPDATE AFTER AUTHOR RESPONSE The fact that the results seem to hold almost exactly as strongly on the ResNet-20 as they do on the ResNet-18 seems to imply that the results are not just equivalent to adding regularization into an over-parameterized and under-regularized network, and do in fact provide some effect when applied alongside standard regularization (as opposed to my suspicion that the effect would disappear or even reverse when applied alongside standard regularization). I do still think that there is more work to be done on that front (making more precise exactly the relationship between regularization and this effect), but the rebuttal did satisfactorily address my main concern with the paper. Regarding the addition of Kendall Tau, I am satisfied that they do show a correlation, even with the relatively noisy data. Overall, I'm raising my score to a 6. I still think the paper would be improved by a more thorough discussion of the relationship to other more standard forms of regularization, and I'm hesitant to trust that the methodology will be made substantially more clear without seeing a revised version of the paper, but enough of my concerns were addressed by the experiment on the standard ResNet-20 that I no longer see a strong reason to reject the paper.","- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided lacks direct and explicit actionable comments. It does not suggest any specific steps or experiments that the authors should undertake. Instead, it merely points out the lack of methodological details and the need for more clarity. The review point does not offer any guidance on how to improve the methodology or what specific changes could be made. It also fails to provide any implicit actionable comments that could be inferred from the review point. The authors are left without any clear direction on what needs to be done to address the issues raised in the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided is not aligned with the score rubric's criteria for politeness. The tone of the review point is not constructive or encouraging, but rather critical and somewhat aggressive. The reviewer's language is direct and lacks the necessary politeness, which is evident in phrases such as ""there are not enough methodological details"" and ""the methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence."" These statements are not delivered in a helpful or encouraging manner, but rather in a way that could be perceived as disrespectful or aggressive. The reviewer's approach to the paper's methodology is not conducive to a constructive dialogue, and the tone is not in line with the expected level of politeness in a review. Therefore, the review point does not meet the criteria for a score of 1, as it is not helpful or encouraging.",-1,"The review point provided lacks the necessary verification of claims, which is a critical aspect of the score rubric. The review point makes a claim about the lack of methodological clarity in the paper, but it does not provide any logical deductions or references to support this claim. The reviewer simply states that there are not enough details to reproduce the results, without offering any specific examples or explanations. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The reviewer also fails to provide any factual statements or claims that do not need verifications, which would have been appropriate for a score of 0. Instead, the review point remains vague and unsubstantiated. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a higher score. So the overall score is -1.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the lack of methodological details but does not specify which details are missing or how they impact the paper's results. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors are left with a vague understanding of the issues raised in the review point, which does not provide them with a clear direction for improvement. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that meets the criteria for specificity would clearly identify which part of the paper is being addressed and provide specific examples or questions that the authors can address. This would allow the authors to confidently identify the areas of the paper that need improvement and to take actionable steps to address these issues. 

The examples provided in the score rubric demonstrate the difference between a review point that is specific and one that is not. The examples in the score rubric show that a review point that is specific can lead to a clear understanding of the issues and a confident approach to addressing them. 

In conclusion, the review point provided does not meet the criteria for specificity, as it fails to clearly identify which part of the paper is being addressed and does not provide specific examples or questions that the authors can address. This lack of specificity makes it difficult for the authors to identify the areas of the paper that need improvement and to take actionable steps to address these issues."
1486,ICLR_2023_3777,"Weakness
1: The technical novelty is limited and some details are confusing.
Slimmable networks are a special case of widely studied one-shot NAS (e.g., [R2, R3, R4, R5, R6]), which only considers the width dimension (see discussion in OFA [R2]). There are many techniques to deal with interference among subnetworks. Specifically,
“Slow start” belongs to progressive training in one-shot NAS. For example, OFA proposes a “Progressive Shrinking” strategy, which starts with training the largest sub-network and then progressively fine-tunes the network to support smaller sub-networks by gradually adding them into the sampling space.
“Online distillation” was originally proposed in US-Nets [Yu et al., 2019b]. Apart from the inplace distillation, it also proposes the sandwich rule.
“loss reweights” aims to assign larger weights for sub-networks with large widths. However, it violates the training objective of slimmable networks. The objective is to make each supported sub-network maintain the same level of accuracy as independently training a network with the same architectural configuration, rather than only training an accurate large “supernet”. This is evidenced in Table 2 (e), where adding loss reweighting makes R-50(0.25) perform worse, so what’s the meaning there?
2: Another concern is what are the fundamental differences between self-supervised and supervised training for slimmable networks? This is not clear to me as all the training techniques used are common practices in supervised training.
3: What’s the relationship between unsupervised NAS (e.g., [R4, R5]), including the contrastive self-supervised one (e.g., [R6])?
4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?
5: The paper lacks mathematical modeling for the gradient divergence issue which leads to the optimization difficulty claimed by the authors. I think there are only four possible widths and it is not difficult to analyze the gradient magnitude and directions using SGD with maths formulations. Also, some theoretical analysis on convergence is expected, even assuming a linear neural network is fine [R1].
6: The experiments are far from enough to justify the effectiveness of the proposed method.
6.1: The results are merely based on the ResNet-50 backbone. However, I would like to see more ResNet backbones such as R-101 and R-152. More importantly, experiments on Vision Transformers, such as ViT-B in MoCo v3, must be included in the experiments.
6.2: The paper only evaluates the representation quality using linear probing. However, it must evaluate transfer learning performance which is the standard practice in self-supervised learning (e.g., in MoCo v3). For example, experiments on dataset transfer and downstream tasks such as dense segmentation and detection on COCO and ADE20k are needed.
6.3: How about training the whole network (width 1.0) first then using network pruning (e.g., [R7]) to obtain small networks (width 0.25, 0.5, 0.75)? As this strategy can avoid the interference issue during training.
6.4: It lacks comparisons with methods dealing with sub-network interference, such as switchable BN [Yu et al., 2019], sandwich rules [Yu et al., 2019b] and many others.
7: The discussions and references in related work are far from enough. There are few discussions with single-shot NAS and unsupervised NAS methods. In addition, as I point out in the technical novelty part, the differences and advantages with the related work must be discussed.
8: Writing also needs to be improved.
8.1: What is the definition of the “main parameters” in the introduction?
8.2: In Sec. 3.2, “..., where L
is the loss function”. It should be defined in Eq. (1) where it first appears.
8.3: Many grammar issues. I only point out a few. “Slimmable neworks” in Sec. 2; “server performance degradation” in Sec. 3.2.
9: In Sec. 3.2, authors argue that the two ratios in Figure 3 should be large enough. “In Figure 3f, ..., are larger than 1.0 by a clear margin”. It does provide a clear concept of how large is good enough. In my opinion, it also depends on the network architectures and self-supervised learning frameworks. So Figure 3 may not be statistically significant. References:
[R1]: “On the optimization of Deep Networks: Implicit Acceleration by Overparameterization”, ICML 2018
[R2]: “ONCE FOR ALL: TRAIN ONE NETWORK AND SPECIALIZE IT FOR EFFICIENT DEPLOYMENT”, ICLR 2020
[R3]: “BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models”, ECCV 2020
[R4]: “Are Labels Necessary for Neural Architecture Search?”, ECCV 2020
[R5]: “Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?”, NeurIPS 2020
[R6]: “Contrastive Self-supervised Neural Architecture Search”, Arxiv 2021
[R7]: “Resrep: Lossless cnn pruning via decoupling remembering and forgetting”, CVPR 2022","4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a disagreement with the author's explanation and suggests that all sub-networks with different widths should be equally important. However, it does not provide any concrete suggestions or recommendations on how to address this disagreement or improve the paper. The review point also lacks any implicit actionable comments that the authors can infer from the review point. It simply states the disagreement without offering any guidance on how to proceed. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any aggression or rudeness, nor do they offer any encouragement or helpfulness. Instead, they simply state their disagreement with the author's choice of reweighting case and question the importance of the smallest network in the context of slimmable networks. This is a neutral stance that does not cross into the territory of being rude or disrespectful. The reviewer's tone is factual and does not contain any personal attacks or negative judgments about the paper or the authors. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question makes a claim regarding the choice of loss reweighting case in the paper. The claim is that the authors should have used case (3) instead of case (1) because it performs better for most network widths. However, the review point does not provide any logical deductions or references to support this claim. It simply states the disagreement with the author's explanation without offering any justification or explanation.

The review point also fails to mention any external or internal sources that could have been used to verify the claim. This lack of verification and justification is a significant shortcoming in the review point, as it does not meet the criteria for verifiability.

In contrast, a review point that contains a claim and verifies it would provide logical deductions and references to support the claim. It would also explain why the claim is valid and how it contributes to the overall understanding of the paper.

Therefore, based on the score rubric, the review point in question does not meet the criteria for verifiability. It contains a claim but does not verify it, and it lacks logical deductions, references, and justification. As a result, the review point is not effective in addressing the aspect of verifiability.",1,"The review point clearly identifies the specific part of the paper that is being addressed, which is the reweighting method and its application to different network widths. The reviewer's disagreement with the author's choice of reweighting method (case (1)) is well articulated, and the reviewer's reasoning is based on the paper's own premise of slimmable networks. This shows that the reviewer has a good understanding of the paper's content and is able to provide a critique that is directly related to the paper's claims.

The reviewer's critique is also specific, as it directly addresses the choice of reweighting method and its implications for the performance of the network. This specificity is in line with the score rubric, which emphasizes the importance of identifying the exact part of the paper that is being addressed.

Furthermore, the reviewer's critique is constructive and offers an alternative perspective, which is beneficial for the authors to consider. This demonstrates that the reviewer is not only identifying issues but is also contributing to the development of the paper.

In conclusion, the review point is well-articulated, specific, and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer's critique is directly related to the paper's content and provides a clear and focused critique of the paper's reweighting method."
4671,NIPS_2020_1856,"* See section (Clarity). * 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this). * For better or for worse, there is a lot going on -- and it feel like the pages 5 and 8 were crammed up. I wonder if maybe the theory of OT (pages 3-4)can be reduced to 1 page, so that there can be a more thorough discussion, and explanation of the results and metrics rather than having the reader refer to the Supplementary Material consistently for some minor details.",* See section (Clarity).* 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this).,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of information in the original paper, such as the number of trials and the role of the observers. It does not suggest any specific actions that the authors should take to improve their paper or to clarify the ambiguities present. The review point also fails to provide any implicit actionable comments that the authors could infer from the review. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the paper and providing constructive feedback. The reviewer's comments are based on the information provided in the paper and do not include any personal attacks or disrespectful remarks. The feedback is also factual and does not contain any subjective or emotional language. The reviewer's tone is consistent throughout the review, maintaining a neutral stance. Therefore, the review point is in line with the score of 0, as it is neither rude nor overly polite, but rather neutral and factual.",0,"The review point provided does not contain any claims that need to be verified. It merely states the lack of information regarding the number of trials and the role of the observers in the experiment. This lack of information does not constitute a claim that requires verification. The review point does not make any assertions that could be validated or refuted, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the verifiability of any claims. So the overall score is 0.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which part of the paper is being addressed. The review point mentions the use of two observers and the absence of information about the number of trials and the observers' knowledge of the experiment's goals. However, it fails to provide any concrete examples or suggestions for improvement. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors would likely be unsure about which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
3428,NIPS_2020_747,"I have a few concerns regarding this paper. 1. How practical/realistic is the threat model? In the paper, chapter 2.2, the NoBox attack demands the training set. In the real world however, isn't the training set even more precious than the trained models? 2. In the AEG objective, the generator needs to get the gradient to be trained. Would AEG still applicable to the non-differentiable robust classifiers, such as: [1] Countering Adversarial Images using Input Transformations [2] THERMOMETER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES [3] Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples The common point of these approaches is they all incorporate some sort of in-differentiability. 3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST. 4. One experiment I'd like to request: (i)- get a model trained on some dataset at epoch N, N+1, N+2... N+k (ii)- use the generator to generate pertubed imagess to attack all of them. (iii)- show the effectiveness of the attack. This should be a more realistic scenario and it aligns with the main point. 5. A portion of the experiment has compared the NoBox attack to the other attacks. These are generally under different threat model assumptions. However the main claim of the paper is that the NoBox is capable of attacking different models in the same function space. It would be better if the authors can present the NoBox's effectiveness attacking more diversified trained neural network models.",3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the limitations of the experiments, which are CIFAR and MNIST, without suggesting any specific actions or improvements that could be made. The review point does not provide any implicit actionable comments that could be inferred by the authors. It lacks any guidance or suggestions that would help the authors to improve their work or to better understand the implications of their research. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer simply states the limitations of the experiments in the paper, which is a factual observation. The reviewer does not offer any constructive feedback or suggestions for improvement, which would have been helpful and encouraging. The tone is neutral and does not convey any particular emotion or stance. Therefore, the review point is in line with the score of 0, as it is neither aggressive nor particularly helpful or encouraging.",-1,"The review point in question makes a claim about the scope of the experiments in the paper. It states that the experiments are limited to only CIFAR and MNIST, which is a factual claim. However, the review point does not provide any verification or justification for this claim. It does not mention any logical deductions or external or internal sources that could support the claim. The claim is simply stated without any supporting evidence or explanation.

The review point also fails to address the implications of the limited scope of the experiments. It does not discuss how the limited scope might affect the results or the conclusions drawn from the experiments. This lack of discussion on the implications of the limited scope further weakens the review point.

In contrast, a review point that would score a 1 would contain a claim and would verify it by providing logical deductions and references to external or internal sources. It would also discuss the implications of the claim, which would strengthen the review point.

Based on the score rubric, the review point in question does not meet the criteria for verifiability. It contains a claim but fails to verify it or discuss its implications. Therefore, the review point is not well-supported and does not provide a thorough analysis of the paper. 

So the overall score is -1.",1,"The review point provided is specific and directly addresses the experiments section of the paper. It clearly states that the paper's experiments are limited to only CIFAR and MNIST, which is a direct critique of the paper's scope. This specificity allows the authors to identify exactly which part of the paper is being addressed by the review point. The authors can be confident that the review point is focused on the experiments and their limitations. The review point does not leave any ambiguity about the paper's content, and it directly relates to the paper's methodology and its comparison to other benchmarks. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1, as it provides a clear and specific critique of the paper's experiments."
5171,NIPS_2020_880,"1. I could not see a strong motivation for explicitly enforcing sparsity on architecture parameters. This is because there are already many works trying to decouple the dependency of evaluating sub-networks on the training of supernet (i.e., making the correlation higher). This means that we have ways to explicitly decouple the network evaluation with supernet training without adding a sparsity regularizaiton. 2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2. 3. The one-stage method proposed basically focusing on training network weights W after the training of architecture parameters is converged. However, similar idea can also be achieved in other differentiable NAS framework, where one can continue training the supernet weights after the architecture remains little change. For example, in GDAS, after the entropy of edges is well minimized, the sampled architecture will be close to determnistic, and one can keep training W to obtain the optimal weights. Moreover, other one-stage methods like [4] are not compared nor discussed. ======================== After reading the author's response, most of my concerns have been addressed. I choose to accept this submission. ======================== [1] Guo, Zichao, et al. ""Single path one-shot neural architecture search with uniform sampling."" ICLR 2020. [2] X. Dong and Y. Yang. Searching for a robust neural architecture in four gpu hours. CVPR 2019. [3] Bender, Gabriel, et al. ""Understanding and simplifying one-shot architecture search."" ICML2018. [4] Cai, Han, Chuang Gan, and Song Han. ""Once for all: Train one network and specialize it for efficient deployment."" ICLR 2020.","2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the lack of comparison with other works and the absence of an explanation regarding Table 2. The review point does not suggest any specific actions that the authors should take to improve their work or to address the issues raised. It also does not provide any implicit actionable comments that the authors can infer from the review point. The review point is more focused on the limitations of the work without offering any solutions or improvements. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly encouraging or helpful. The reviewer simply states the lack of detail in the experiment and the absence of comparison with other methods, which is a factual observation. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. The reviewer's tone is neither rude nor overly polite, and it does not encourage or discourage the authors. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims, nor does it require any form of verification. It merely states that more experiment details are needed to understand Table 2, and it points out that certain methods and comparisons are not mentioned. However, it does not make any claims or assertions that need to be verified. The review point is more of a commentary on the lack of detail and comparison in the paper. It does not provide any logical deductions or references to external or internal sources that could support or verify any claims. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any assertions that could be verified or refuted. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the understanding of Table 2. It points out the need for more experiment details and highlights the importance of re-calculating the BN for proper measurement of the Kendall correlation. The review point also mentions other works that could be compared to the paper, such as methods to reduce the gap between supernet and sub-networks or to make the edges activated to be sparse like GDAS. Furthermore, it notes the lack of explanation in the main content regarding Table 2.

The specificity of the review point is evident, as it clearly identifies the part of the paper that is being addressed. The authors can be confident about which part of the paper is being addressed by the review point, as it provides a clear direction for improvement. The review point also encourages the authors to consider additional experiments and comparisons with other works, which would help in enhancing the paper's quality.

In contrast, a review point with a score of -1 would not provide any specific guidance or direction for the authors to follow. It would leave the authors unsure about which parts of the paper are being addressed by the review point. The lack of specificity in such a review point would not help the authors in identifying the areas that need improvement.

Therefore, based on the score rubric, the review point provided is specific and directly addresses a particular aspect of the paper, making it clear for the authors to understand which part of the paper is being addressed. The authors can be confident about the review point's focus, and it provides a clear direction for improvement. So the overall score is 1."
1908,ARR_2022_64_review,"- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:   - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction   - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. 
In line ""544"", I still quite understand why using entity representations can reduce language bias. ","- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the paper without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific changes or improvements that could be made to the paper. It also does not provide any insights into how the paper could be improved or what further research could be conducted. The review point simply states the limitations of the paper without offering any constructive feedback or suggestions for improvement. 

In conclusion, the review point does not provide any actionable comments or suggestions, which is why it does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor constructive, but rather a straightforward critique of the paper. The reviewer does not express any form of politeness or rudeness, instead focusing on the content of the paper and providing a factual assessment. The feedback is delivered in a neutral manner, without any personal attacks or overly positive comments. The reviewer's tone is consistent throughout the feedback, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the contribution of the paper is incremental and refers to previous works without making any specific claims. The review point does not include any logical deductions, nor does it mention any external or internal sources to support its statements. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a general commentary rather than a critical analysis with verifiable claims. As such, it does not fulfill the requirements for a score of 1, which necessitates the verification of claims through logical explanations or references to external or internal sources. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It lacks specificity and does not provide any concrete examples or suggestions for improvement. The authors are left with no clear direction on how to address the issues raised in the review point. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors are not confident about which part of the paper is being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and is not helpful in guiding the authors to improve their paper. So the overall score is -1."
3403,NIPS_2020_585,"I think the major problem of this paper is that the novelty is limited. It seems that nearly all the components in the proposed method have been used, and the combination manner is also not so novel. 1. The two-stage method is widely used to deal with partial label examples. The first stage in this paper adopts the widely used label probagation strategy to obtain labeling confidence for partial label examples. However, SSPL [22] also adopts this strategy (which obtain labeling confidence for both partial label examples and unlabeled examples). So there seems no novelty for the first stage. 2. For the second stage, a modified maximum margin formulation is introduced, which can jointly enable the induction the induction of predictive model and the estimation of labeling confidence over unlabeled data. The formulation of the second stage mainly follows [17]. There are two differences between this paper and [17]: Firstly, this paper uses the obtained labeling confidence of partial examples to give different weights on the losses of different examples. Secondly, the unlabeled data is used in the formulation by using the widely-used manifold regularization, so that the labeling confidence of unlabeled data will be jointly estimated. Generally, I think the improvement over [17] is intuitive and reasonable. But I think that the novelty is not enough, especially on such a prestiguous venue NeurIPS. 3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage. 4. This paper may not effectively deal with large-scale datasets because it uses alternating optimization and needs to construct a similarity graph in advance.","3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a lack of theoretical analysis or empirical evidence in the paper, which is a limitation but does not suggest any specific actions that the authors should take to address this limitation. The review point does not provide any implicit actionable comments that can be inferred, nor does it give any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does it provide any form of encouragement or helpfulness. The tone is purely informative, pointing out the lack of theoretical analysis and empirical evidence in the paper. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a key aspect of maintaining a neutral tone. The feedback is focused on the content of the paper and does not include any personal opinions or criticisms. Therefore, the review point is delivered in a neutral and factual manner, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question makes a claim about the lack of theoretical analysis or empirical evidence in the paper regarding the convergence of the modified maximum margin formulation in the second stage. However, it fails to provide any justification or reference to support this claim. The review point does not offer any logical deductions or external or internal sources to verify the claim. It simply states the absence of such analysis or evidence without any supporting details. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. It is also why the review point does not meet the criteria for a score of 0, as it does not contain any factual statements or claims that do not need verifications. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a higher score. So the overall score is -1.",1,"The review point provided is quite specific, focusing on the lack of theoretical analysis or empirical evidence regarding the convergence of the modified maximum margin formulation in the second stage of the paper. This specificity allows the authors to identify the exact part of the paper that is being addressed, which is the second stage of the optimization process. The review point is clear and concise, making it easy for the authors to understand what aspect of their work is being critiqued. The authors can be confident that the review point is directly related to their paper, as it specifically addresses the absence of certain elements in their work. Therefore, the review point meets the criteria for a score of 1, as it clearly identifies the part of the paper that is being addressed."
3980,NIPS_2020_1314,"1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP. 2. In Section3.1 the authors give a thorough comparison between the discrete Gaussian and discrete Laplace. The conclusions deduced by the discrete distribution are almost the same as the prior conclusions of the continuous distribution. 3. This paper does not compare their methods with the exponential mechanism(Frank McSherry and Kunal Talwa 2007). Exponential mechanism can also output a discrete answer for a query.","1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the discrete Gaussian mechanism is not a big innovation and is a natural derivation of the discrete Laplacian noise to DP. This statement does not provide any guidance or suggestions for the authors to improve their work or to address the concerns raised. The review point does not include any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer simply states their opinion on the innovation of the discrete Gaussian mechanism, comparing it to the discrete Laplace distribution, which is a factual comparison. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. The reviewer's tone is passive, and the content of the review point is focused on the comparison between the two mechanisms, without any additional commentary or critique. Therefore, the review point is neutral and factual, fitting the score of 0 in the rubric.",-1,"The review point provided makes a claim about the innovation of the discrete Gaussian mechanism in the context of differential privacy (DP). However, it fails to provide any verification or justification for this claim. The reviewer simply states that the mechanism is not a big innovation because it is a natural derivation from the discrete Laplacian noise, which has been used in the 2020 US Census. This statement lacks any logical deductions or references to external or internal sources that could support the claim. The reviewer does not explain why the discrete Gaussian mechanism is not a significant innovation, nor does it provide any evidence or reasoning to back up the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It is important for reviewers to provide logical explanations or references to support their claims, which is not done in this case. So the overall score is -1.",1,"The review point provided is quite specific, directly addressing the use of discrete Gaussian noise in the context of differential privacy (DP). It clearly identifies the innovation in the paper, comparing it to the discrete Laplace distribution introduced in (GRS12) and its application in the 2020 US Census. This comparison not only highlights the novelty of the proposed mechanism but also provides a clear link to existing work in the field. The review point is well-articulated and directly addresses the paper's main contribution, making it evident which part of the paper is being discussed. The authors can be confident that the review point is focused on the specific aspect of the paper, which is the application of discrete Gaussian noise to DP. Therefore, based on the score rubric, the review point demonstrates a high level of specificity, aligning with the criteria for a score of 1."
608,ICLR_2021_1900,"weaknesses:
Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq. (2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 2) Regarding Eq. (5), what the intuition of the adaptive matrix (i.e., (log_{\mu+1} (N_i+1))^{\delta^{\tau}}) when i = j should be provided to the authors.
The major issue of this paper is the experimental evaluations. 1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic. 2) Some state-of-the-art methods are not involved in the experimental comparisons, such as [ref1-ref5]. Moreover, the accuracy of the proposed method cannot outperform these methods.
Minor issues:
There are several typos and writing problems in this paper. For example, on Page 3, ""Dubey et al.Dubey et al. (2018)"", and ""Chen et al.Chen et al. (2019)"". On Page 4, ""PC Dubey et al. (2018)"". On Page 8, ""And also solves the long-tailed problem by an adaptive matrix term.""
[ref1] Weakly Supervised Fine-grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning, CVPR 2020.
[ref2] Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019.
[ref3] Fine-Grained Visual Classification via Progressive Multi-Granularity Training of Jigsaw Patches, ECCV 2020.
[ref4] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.
[ref5] BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition, CVPR 2020.","1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that the authors should undertake. It merely states the limitations of the proposed method without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors can deduce what needs to be done. It lacks the provision of any implicit actionable comments that the authors can infer after reading the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point does not suggest any specific changes or improvements that could be made to the proposed method. It does not provide any guidance on how to address the limitations mentioned. The review point also fails to suggest any additional experiments or studies that could be conducted to further understand the proposed method. 

In conclusion, the review point does not offer any actionable insights or recommendations, which is a critical aspect of the score rubric. It does not provide any explicit or implicit actions that should be taken, and therefore, it does not meet the criteria for actionability. 

So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer simply states the facts about the classification accuracy and the effectiveness of the proposed method. The feedback is focused on the content of the paper and does not include any personal opinions or comments on the quality of the paper. The reviewer does not make any judgments about the quality of the paper or the authors' work, which is in line with the neutral tone expected in a review. The feedback is purely factual and does not contain any elements of politeness or rudeness. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question makes a claim about the classification accuracy of the proposed method, stating that it is not significantly better than previous work. However, the review point fails to provide any verification or justification for this claim. It does not offer any logical deductions or references to external or internal sources that could support the claim. The lack of verification in the review point means that the claim remains unsubstantiated, which is a critical shortcoming according to the score rubric. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions the classification accuracy on fine-grained benchmark datasets and iNat18, but it does not specify which sections of the paper are related to these datasets or how the proposed method is problematic. The authors would likely be unsure about which parts of the paper are being addressed by the review point. The lack of specificity in the review point does not allow the authors to confidently identify the parts of the paper that are being critiqued. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide the authors with a clear understanding of the paper part that is being addressed by the review point comment."
2060,ARR_2022_1_review,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. ","- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a potential issue with the training data sizes and the use of multilingual information, without suggesting any specific actions to address these concerns. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the review point is not aggressive or disrespectful, nor is it overly encouraging or helpful. It simply states the need for an ablation study to determine the impact of multilingual data on the model's performance. The review point does not make any personal judgments or comments about the quality of the paper or the authors' work, which is a key aspect of maintaining a neutral tone. The review point focuses on the specific aspect of the training data and the model's performance, which is a factual and objective approach to reviewing. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the performance of the Prix-LM (All) model in comparison to a single model. It suggests that the model's ability to leverage multilingual information is the reason for its superior performance. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The absence of any form of verification or justification makes it difficult to assess the validity of the claim. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability. It contains a claim that is not verified, and it misses a justification, explanation, or reference. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the authors' claim about the performance of the Prix-LM (All) model. It specifically asks for an ablation study to determine whether the performance gain is due to the large dataset size or the multilingual training data. This request for additional information is directly related to the authors' argument and is a clear indication of which part of the paper is being addressed. The authors would be able to identify the relevant sections of their paper and would be confident in their understanding of the review point. Therefore, based on the score rubric, the review point is highly specific and directly addresses the authors' claim, making it clear which part of the paper is being discussed. So the score is 1."
3525,NIPS_2020_324,"The experiments are performed on two simple environments: 1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.","1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the proposed method AdaMB and compares it with AdaQL, without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors should make. It lacks any explicit or implicit actions that should be done, which is why it does not meet the criteria for actionability. The authors are left without any clear direction on how to proceed with their research. Therefore, the review point does not fulfill the requirements of the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The reviewer simply states the facts about the performance of AdaMB and AdaQL in the given problems, and the lack of a thorough comparison between the two methods. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a key aspect of a helpful review. The tone is neutral, and the reviewer does not make any personal judgments or comments about the quality of the paper or the authors' work. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states the performance of AdaMB and AdaQL in two specific problems without making any claims about their general performance or scalability. The review point does not make any assertions that could be verified or refuted, and it does not provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is factual and does not require any form of verification. It is simply a statement of observed performance in two specific problems. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly identify which parts of the paper are being addressed. It mentions two problems, but it does not specify which parts of the paper are related to these problems. The review point also fails to provide a clear comparison between AdaMB and AdaQL, which is a crucial aspect of the paper. The lack of specificity in the review point makes it difficult for the authors to understand which parts of the paper are being addressed. This lack of clarity and specificity is why the review point would receive a score of -1 according to the score rubric. 

The review point does not provide any concrete examples or suggestions for improvement, which would have been helpful in guiding the authors on how to address the issues raised. Instead, it leaves the authors with a general sense of uncertainty about the paper's content and the reviewer's expectations. 

In conclusion, the review point lacks the necessary specificity to effectively address the paper's content and does not provide clear guidance for the authors. This lack of specificity and clarity is why the review point would receive a score of -1 according to the score rubric."
114,ICLR_2022_1971,"Weakness: Method:
1. Verification and Optimization:
The proposed HNPF method is for verification (e.g., check whether a given solution x is Pareto optimal), but not for optimization (e.g., find an (approximate) Pareto solution x). It needs an extra search method, such as random sampling in this work, to first generate a large number of feasible solutions to cover the whole search space. Therefore, the underlying optimization is indeed random sampling (independent from HNPF), which could be extremely inefficient for a non-trivial search space. It is not suitable to put and compare the proposed HNPF method with other optimization methods that can directly find the (approximate) Pareto solution.
Since HNPF depends on random sampling, it is not surprising that it can only work for small scale problems.
2. The Reason to Build the Model:
HNPF needs to first build a neural network to check whether a given solution x satisfies the Fritz-John Condition (FJC), which requires a large number of training samples (e.g., 11K for a two-dimensional problem). The learned model is mainly used to classify whether the extra randomly sampled solutions (e.g., 9K) are weak Pareto optimal or not. The reason for model building, such as the advantage over the simple FJC rule-based classification, is not well motivated and justified in this work.
The proposed Pareto filter in stage 2 is not discussed and compared with other related nondominated sorting algorithms (e.g., [2]).
3. Necessary Condition for Pareto Optimality:
The KKT[3] and FJC[4] are two types of first order necessary conditions for (local) Pareto efficiency (Pareto optimality). In my understanding, the multi-objective optimization based MTL algorithms mentioned in this work (Sener & Koltun, 2018; Lin et al., 2019a; Mahapatra & Rajan, 2020; Ma et al., 2020; Navon et al., 2021) mainly use the gradient-based multi-objective optimization methods (e.g., MGDA) [5-7], which is based on the KKT condition. For these methods, in each update step, the gradient can be written as a linear combination of the gradient for each objective with adaptive weights derived from the KKT condition. Therefore, they are all different from the simple linear scalarization with fixed weights. All the claims and analyses in this work that the previous works use simple linear scalarization is not correct.
The FJ condition is also for local Pareto convergence, similar to the KKT condition. The global convergence property is solely due to random sampling that only works for extremely low-dimensional problems. It is unfair to say the proposed algorithm can overcome the local convergence of other gradient-based methods. In addition, the proposed algorithm heavily depends on the Fritz-John condition, but the original work [4] is not cited.
4. Linear Scalarization and Convex Pareto Front:
It is well-known that the simple linear scalarization cannot find the non-convex part of the Pareto front [8]. This finding leads to the seminal work on NBI scalarization (Das & Dennis.,1998), which is indeed one fundamental work that inspires the proposed method in this work (section 4, first sentence). The claim ""it is incorrect to state that LS itself fails if the Pareto front is non-convex"" (appendix, page 15) is questionable.
Since the proposed HNPF can only verify whether a given solution is weak Pareto optimal, its ability to find the whole Pareto front totally depends on the extra sampling method (e.g., random sampling) to generate all the Pareto solutions (might be infinite). It is misleading to indicate the proposed HNPF method itself can find the whole Pareto front. In addition, since the Pareto set has measure zero and infinite cardinality, the random sampling + HNPF method can at most find a dense approximation to the Pareto set. Experiment:
5. Algorithms for Comparison:
All the multi-objective optimization based MTL algorithms are designed for optimizing a deep neural network with millions of parameters. They implicitly depend on the assumption that the deep neural network has good properties (e.g., no bad local optimum [9][10]) on its loss functions, which is consistent with other gradient-based single-objective optimization methods. They are not designed to find the global Pareto front for low-dimensional problems.
For low-dimensional problems, it is more suitable to compare with the model-free multi-objective optimization methods such as the multi-objective evolutionary algorithm [11,12] and multi-objective CMA-ES [13]. If the model building is allowed, Multi-Objective Bayesian Optimization (MOBO) algorithms can have a very good sampling efficiency for the low-dimensional problems [14,15]. It is also very common to conduct non-dominated filtering at the end of those model-free algorithms or MOBOs (e.g., only keeping the current non-dominated solutions).
6. Training + Sampling:
The proposed method needs to first sample 11k solutions to train the neural network model, then randomly generate extra 9K solutions for filtering. Is there any advantage over simply using FJC to filter 9K (or 11k + 9K) randomly sampling solutions?
7. Figure from Other Works:
Many figures in the main paper and the appendix are directly borrowed from other works. I think this is not appropriate even the credits are given to the original works. Reference:
[1] Ruchte, Michael, and Josif Grabocka. Multi-task problems are not multi-objective. arXiv preprint arXiv:2110.07301, 2021.
[2] Roy, Proteek Chandan, Kalyanmoy Deb, and Md Monirul Islam. An efficient nondominated sorting algorithm for large number of fronts. IEEE transactions on cybernetics 49, no. 3: 859-869, 2018.
[3] Kuhn, H. W., and A. W. Tucker. Nonlinear Programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pp. 481-492. University of California Press, 1951.
[4] Da Cunha, N. O., and E. Polak. Constrained minimization under vectorvalued criteria in finite dimensional spaces. Journal of Mathematical Analysis and Applications, 19(1), 103–124 ,1967.
[5] Fliege, Jorg, and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research 51, no. 3: 479-494, 2000.
[6] Fliege, Jorg, and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on SQP techniques. SIAM Journal on Optimization 26, no. 4: 2091-2119, 2016.
[7] Desideri, Jean-Antoine. Multiple-gradient descent algorithm (MGDA) for multiobjective optimization. Comptes Rendus Mathematique 350, no. 5-6: 313-318, 2012.
[8] Das, Indraneel, and John E. Dennis. A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems. Structural optimization 14, no. 1: 63-69, 1997.
[9] Kawaguchi, Kenji. Deep learning without poor local minima. NeurIPS 2016.
[10] Kawaguchi, Kenji, and Leslie Kaelbling. Elimination of all bad local minima in deep learning. AISTATS 2020.
[11] Deb, Kalyanmoy, Amrit Pratap, Sameer Agarwal, and T. A. M. T. Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, no. 2: 182-197, 2002.
[12] Zhang, Qingfu, and Hui Li. ""MOEA/D: A multiobjective evolutionary algorithm based on decomposition."" IEEE Transactions on evolutionary computation 11, no. 6: 712-731, 2007.
[13] Igel, Christian, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evolutionary computation 15, no. 1: 1-28, 2007.
[14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020.
[15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.","1: 1-28, 2007. [14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020. [15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It lacks any specific guidance or suggestions that the authors could follow to improve their work. The review point merely points out some limitations of the proposed methods without providing any actionable comments or inferences that could be made. This makes it difficult for the authors to deduce what needs to be done to address the issues raised in the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review points provided are neutral and factual, which aligns with the score of 0. The tone of the review points is neither aggressive nor overly polite. The reviewers are simply stating their observations and providing their opinions on the papers. They are not being disrespectful or rude, nor are they being overly encouraging or helpful. The feedback is focused on the content of the papers and does not include any personal comments or judgments about the authors or their work. The reviewers are maintaining a professional tone throughout their feedback. Therefore, based on the score rubric, the review points are neutral and factual, and the tone is neither aggressive nor overly polite.",-1,"The review point provided contains a claim that is not verified. The claim is that the method described in the paper is promising, but the results fall behind previous work. However, the reviewer does not provide any justification, explanation, or reference to support this claim. The reviewer also makes a statement about the choice of p values not contributing much to the overall score, but again, there is no justification or explanation provided to support this claim. The reviewer also makes a claim about the inclusion of the Jacobean in the output transformation, but again, there is no justification or explanation provided to support this claim. Therefore, the review point does not meet the criteria for verifiability as it contains claims that are not verified.",1,"The review point provided is quite specific and directly addresses the content of the papers. It mentions the key concepts and methodologies used in the papers, such as ""Differentiable Expected Hypervolume Improvement"" and ""Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations."" This specificity allows the authors to identify which parts of the paper are being addressed by the review point. The authors can be confident that the review point is discussing the methodologies and their implications in the context of the papers.

The review point also provides a clear direction for the authors to follow, by suggesting that the methodologies could be improved or further validated. This guidance is specific and actionable, which is beneficial for the authors.

In contrast, the review points that lack specificity do not provide clear guidance or direction for the authors. They are vague and do not address specific aspects of the papers. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point.

Therefore, based on the score rubric, the review point provided is specific and directly addresses the content of the papers, which allows the authors to be confident about which parts of the paper are being addressed by the review point. This specificity is in line with the criteria for a score of 1."
2507,NIPS_2021_537,"Weakness: The main weakness of the approach is the lack of novelty. 1. The key contribution of the paper is to propose a framework which gradually fits the high-performing sub-space in the NAS search space using a set of weak predictors rather than fitting the whole space using one strong predictor. However, this high-level idea, though not explicitly highlighted, has been adopted in almost all query-based NAS approaches where the promising architectures are predicted and selected at each iteration and used to update the predictor model for next iteration. As the authors acknowledged in Section 2.3, their approach is exactly a simplified version of BO which has been extensively used for NAS [1,2,3,4]. However, unlike BO, the predictor doesn’t output uncertainty and thus the authors use a heuristic to trade-off exploitation and exploration rather than using more principled acquisition functions.
2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C. 3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper.
Detailed Comments: 1. The authors conduct some ablation studies in Section 3.2. However, a more important ablation would be to modify the proposed predictor model to get some uncertainty (by deep-ensemble or add a BLR final output layer) and then use BO acquisition functions (e.g. EI) to do the sampling. The proposed greedy sampling strategy works because the search space for NAS-Bench-201 and 101 are relatively small and as demonstrated in [6], local search even gives the SOTA performance on these benchmark search spaces. For a more realistic search space like NAS-Bench-301[7], the greedy sampling strategy which lacks a principled exploitation-exploration trade-off might not work well. 2. Following the above comment, I’ll suggest the authors to evaluate their methods on NAS-Bench-301 and compare with more recent BO methods like BANANAS[2] and NAS-BOWL[4] or predictor-based method like BRP-NAS [5] which is almost the same as the proposed approach. I’m aware that the authors have compared to BONAS and shows better performance. However, BONAS uses a different surrogate which might be worse than the options proposed in this paper. More importantly, BONAS use weight-sharing to evaluate architectures queried which may significantly underestimate the true architecture performance. This trades off its performance for time efficiency. 3. For results on open-domain search, the authors perform search based on a pre-trained super-net. Thus, the good final performance of WeakNAS on MobileNet space and NASNet space might be due to the use of a good/well-trained supernet; as shown in Table 6, OFA with evalutinary algorithm can give near top performance already. More importantly, if a super-net has been well-trained and is good, the cost of finding the good subnetwork from it is rather low as each query via weight-sharing is super cheap. Thus, the cost gain in query efficiency by WeakNAS on these open-domain experiments is rather insignificant. The query efficiency improvement is likely due to the use of a predictor to guide the subnetwork selection in contrast to the naïve model-free selection methods like evolutionary algorithm or random search. A more convincing result would be to perform the proposed method on DARTS space (I acknowledge that doing it on ImageNet would be too expensive) without using the supernet (i.e. evaluate the sampled architectures from scratch) and compare its performance with BANANAS[2] or NAS-BOWL[4]. 4. If the advantage of the proposed method is query-efficiency, I’d love to see Table 2, 3 (at least the BO baselines) in plots like Fig. 4 and 5, which help better visualise the faster convergence of the proposed method. 5. Some intuitions are provided in the paper on what I commented in Point 3 in Weakness above. However, more thorough experiments or theoretical justifications are needed to convince potential users to use the proposed heuristic (a simplified version of BO) rather than the original BO for NAS. 6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.
7. The results in Table 2 which show linear-/exponential-decay sampling clearly underperforms uniform sampling confuse me a bit. If the predictor is accurate on the good subregion, as argued by the authors, increasing the sampling probability for top-performing predicted architectures should lead to better performance than uniform sampling, especially when the performance of architectures in the good subregion are rather close. 8. In Table 1, what does the number of predictors mean? To me, they are simply the number of search iterations. Do the authors reuse the weak predictors from previous iterations in later iterations like an ensemble?
I understand that given the time constraint, the authors are unlikely to respond to my comments. Hope those comments can help the authors for future improvement of the paper.
References: [1] Kandasamy, Kirthevasan, et al. ""Neural architecture search with Bayesian optimisation and optimal transport."" NeurIPS. 2018. [2] White, Colin, et al. ""BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search."" AAAI. 2021. [3] Shi, Han, et al. ""Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS."" NeurIPS. 2020. [4] Ru, Binxin, et al. ""Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels."" ICLR. 2020. [5] Dudziak, Lukasz, et al. ""BRP-NAS: Prediction-based NAS using GCNs."" NeurIPS. 2020. [6] White, Colin, et al. ""Local search is state of the art for nas benchmarks."" arXiv. 2020. [7] Siems, Julien, et al. ""NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search."" arXiv. 2020.
The limitation and social impacts are briefly discussed in the conclusion.","6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a contradiction between the results in Table 3 and Table 4 without suggesting any potential solutions or further investigations. The review point does not provide any implicit actionable comments that can be inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The reviewer simply points out a discrepancy between the results in Table 3 and Table 4, without any additional commentary or suggestions for improvement. The tone is purely informative, and the reviewer does not make any personal judgments or opinions about the quality of the work. Therefore, the review point is neutral and does not fit well with the other two scores, which are more focused on the tone of the review. 

The reviewer's approach is consistent with the expected behavior in a review process, where the focus is on the content and quality of the work, rather than the tone of the review. The reviewer's neutrality ensures that the review is objective and unbiased, which is crucial in a review process. 

In conclusion, the review point is neutral and factual, and it does not fit well with the other two scores, which are more focused on the tone of the review. The reviewer's approach is consistent with the expected behavior in a review process, where the focus is on the content and quality of the work, rather than the tone of the review.",-1,"The review point in question makes a clear claim that the results in Table 3 contradict those in Table 4. However, it fails to provide any form of verification or justification for this claim. The reviewer simply states the contradiction without offering any logical deductions or references to external or internal sources that could support the claim. This lack of verification is a significant shortcoming in the review point, as it does not meet the criteria for verifiability as outlined in the score rubric. The reviewer's statement is left unsupported, which does not contribute to the overall quality of the review. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and is not supported by logical deductions or references. So the overall score is -1.",1,"The review point provided is specific and directly addresses a discrepancy in the results presented in two different tables within the paper. It points out a clear contradiction between the results in Table 3 and Table 4, specifically regarding the performance of the WeakNAS algorithm. This specificity allows the authors to pinpoint exactly which part of the paper is being addressed, and they can be confident that the review point is focused on the performance of the WeakNAS algorithm. The review point does not leave any ambiguity about the paper part being addressed, and it is clear that the authors can identify the relevant sections of the paper. Therefore, based on the score rubric, the review point is highly specific and directly addresses a particular aspect of the paper, which aligns with a score of 1."
3487,NIPS_2020_1312,"- The contribution of local voxel-bounded implicit fields is not motivated and evaluated enough. The authors showed that voxel embeddings improve the performance. However, an experiment that shows the performance with different resolutions (partly done with the progressive training evaluation) would improve the paper. - Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution. - For the experiments on large-scale scenes and dynamic scenes, qualitative results for existing methods are missing. Furthermore, quantitative results are missing for these experiments (also for existing methods). - The geometric result in the ScanNet experiment does not seem to be very good. Therefore, it would be interesting to see how accurate the represented geometry is and how it affects the overall performance of the method. - The effect of the early termination is not properly evaluated in the paper. - The initial grid resolution seems to be already very high. It would be interesting to see how this affects the method. - There should also be comparisons to DeepVoxels since this work also leverages local voxels as a feature representation.","- Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out a limitation in the study without suggesting any specific actions that could be taken to address this limitation. The review point does not provide any implicit actionable comments that could be inferred by the authors. It simply states that the contribution of progressive training is not evaluated enough, without offering any guidance on how to improve the evaluation of progressive training. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it offer implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The review point simply states the need for a more rigorous evaluation of the progressive training, without any personal attacks or overly positive comments. It is a straightforward critique of the study's methodology, which is a neutral approach to reviewing. The review point does not contain any elements of politeness or rudeness, which is why it fits the score of 0. It is a factual observation that the study's methodology could be improved, without any personal judgments or emotional undertones. So the overall score is 0.",-1,"The review point in question makes a claim about the contribution of progressive training in the ablation study. It suggests that the improved performance could be due to the higher voxel resolution, rather than the progressive training itself. However, this claim is not verified. The review point does not provide any logical deductions or references to external or internal sources that would support the claim. It merely states the possibility without any justification or explanation. This lack of verification is why the review point does not meet the criteria for a score of 1. The review point does not contain any claims that are verified, nor does it include factual statements that do not need verifications. Therefore, based on the score rubric, the review point is not validated and does not meet the criteria for a higher score. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the contribution of progressive training in the paper. It clearly states that the authors need to show that the progressive training is better than one round of training at both the initial and increased resolutions. This specificity allows the authors to understand exactly which part of the paper is being addressed by the review point. The authors can confidently identify that the review point is focused on the progressive training aspect of the paper. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
1654,ICLR_2023_1765,"weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, t ∈ R while y , f ( θ ) ∈ R n
, where n
is the size of the training set, so that the operation y − t − f ( θ )
is not well-defined. Moreover, even if y , f ( θ ) ∈ R
, the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2
, rather than ( θ ) ≤ t
. Since, in general, the training problem will be defined for y ∈ R n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are ""reasonable"". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- ""Issue 1: Is non-convex deep neural networks always better?"": ""is"" should be ""are"". - ""While some experiments show that certain local optima are equivalent and yield similar learning performance"" -- this should be supported by a reference. - ""However, the derivation of strong duality in the literature requires the planted model assumption"" --- what do you mean by ""planted model assumption""? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - ""In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m
representation power."" -- A statement like this only makes sense under some definition of ""representation power"". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - ""Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient."" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - ""... too many minimization pooling layers will damage the representation power of the neural network"" --- why? Can the authors expand on this observation?
Section 5: - ""... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y
"" --- note that this observation requires y ≥ 0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): ""conveyers"" -> ""converges"".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - ""It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)"" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References
[1] Robbins, Herbert, and Sutton Monro. ""A stochastic approximation method."" The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. ""Learning representations by back-propagating errors."" nature 323.6088 (1986): 533-536.","- Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the model without suggesting any improvements or further research. The review point does not provide any implicit actionable comments that can be inferred from the text. It simply states the model's limitations without offering any guidance on how to address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor does it provide implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts about the model, such as the number of neurons and the type of functions it can model. However, the reviewer could have been more polite by using phrases like ""it would have been nice"" or ""it might be helpful,"" which would have made the feedback more constructive and less neutral. The reviewer also missed the opportunity to provide suggestions for improvement, which could have made the feedback more helpful. So the overall score is 0.",0,"The review point provided does not contain any claims, nor does it require verification. It simply states two aspects of the model: the number of neurons and the type of functions it can model. These are factual statements that do not need to be verified. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the score is 0.",1,"The review point provided is quite specific and directly addresses the model's limitations and characteristics. It clearly states that the model consists of only one neuron and can only model non-negative functions. This specificity makes it easy for the authors to identify which parts of the paper are being addressed by the review point. The authors can be confident that the review point is discussing the model's structure and its functional limitations. The review point is not ambiguous and does not leave room for interpretation, which is crucial for the authors to understand and address the feedback. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it clear for the authors to identify the relevant sections. So the score is 1."
1526,ICLR_2023_3808,"Weaknesses: • The writing of the paper is at times confusing. For example, it is unclear of the significance of the \cap architecture. No detail is included in the paper. • The rationale of dividing the sequence of intermediate results into 11 sets is not provided. • No ablation studies are provided for \mu and \lambda, and the size of each set. In fact, their values are not provided in the paper (unless I missed them). • Using different sets and varying \phi appear to have a similar effect. However, there is no discussion on this issue. • It appears that there are two types of sets (Table 1 and 2), one for style and one for content. But in the paper, they are regarded to be the same, which is quite confusing. • There is insufficient discussion of the novelty of the proposed method, e.g. what the proposed method can do that other recently proposed style transfer methods cannot do? For example, in Fig. 6, the only discussion is one single sentence on p. 5 stating that the proposed method preserve the semantic structure of the content image and simulate the strokes of the style image from the style domain. Such a statement, which is quite generic, can be easily applied to other style transfer methods. • There is no quantitative comparison, e.g. style loss, user study, and efficiency, between the proposed method and other methods. • One issue not discussed is the importance of using the diffusion model. What can it provide to style transfer that no other methods can? • There are some occasional typos and grammatical mistakes.",• The rationale of dividing the sequence of intermediate results into 11 sets is not provided.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the lack of rationale in dividing the sequence of intermediate results into 11 sets without suggesting any improvements or further investigation. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided does not offer any constructive feedback or encouragement. It lacks the polite tone that is expected in a review process. The reviewer does not provide any rationale for the critique, nor does it offer any suggestions for improvement. The tone is more critical than constructive, which could be perceived as disrespectful or aggressive. The reviewer does not engage with the content of the paper in a way that would be helpful to the authors. Instead, it simply states that the paper is not suitable for publication without providing any specific reasons or suggestions for improvement. This approach does not align with the score rubric's criteria for politeness and constructive feedback. Therefore, the review point does not meet the standards set for performance in terms of politeness and constructive feedback. So the score is -1.",0,"The review point in question does not provide any rationale for dividing the sequence of intermediate results into 11 sets. This lack of justification or explanation is a significant shortcoming, as it fails to meet the criteria for verifiability. The review point does not contain any claims that could be verified, nor does it provide any logical deductions or references to external or internal sources that could support any claims. As such, the review point does not meet the requirements of the score rubric, which emphasizes the importance of verifying claims through logical explanations or references. Therefore, the review point is not aligned with the score rubric's criteria for verifiability, and it is not possible to assign a score of 1, 0, or -1 to it.",-1,"The review point provided does not offer a clear understanding of which part of the paper is being addressed. The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors are left with uncertainty about the paper part that is being addressed by the review point comment. This lack of specificity does not align with the score rubric's criteria for a score of 1, where the authors should be confident about which part of the paper is being addressed by the review point. Therefore, the review point does not meet the criteria for a score of 1, and instead, it aligns with the criteria for a score of -1."
5012,NIPS_2020_1049,"- The experimental results are not significant. Two straightforward error rate indicators: the dimension of the space and LR error is chosen as baselines and the authors claim that the proposed bound is superior to these two baselines. However, from the experimental results, Pearson’s r correlation in Fig.4 shows that the proposed improvement is marginal. The results on image dataset from the supplementary material also shows that the Pearson’s r correlation is very close between the LR Error and MSE. - The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1. - The figures are not clear enough, e.g. what does each point stand for in Fig.1 and Fig.2. The authors could add the necessary information to make this paper more self-contained.",- The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out a limitation in the authors' work, which is the use of k=1 in their experiments. However, it does not suggest any specific actions that the authors could take to address this limitation or to further their research. The review point also does not provide any implicit actionable comments that the authors could infer from the review point. It simply states the limitation without offering any guidance or direction for future work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither provides explicit actionable comments nor does it offer implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point simply states a limitation of the study, which is a factual observation. It does not provide any constructive feedback or suggestions for improvement, which would have been helpful and encouraging. The review point also does not make any negative comments about the quality of the paper or the authors' work, which would have been rude or disrespectful. Therefore, the review point is neutral and does not fit well with the other two scores. 

So the overall score is 0.",-1,"The review point in question makes a claim about the empirical conclusion of the study, which is that the results may not be valid if the number of dimensions (k) is larger than 1. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The review point simply states the claim without any supporting evidence or explanation. This is a clear deviation from the score rubric, which requires claims to be verified through logical explanations or references. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",1,"The review point provided is quite specific, focusing on the number of experiments conducted with k=1. It directly addresses the empirical conclusion presented in the paper and questions its validity for larger values of k. This specificity allows the authors to identify the exact part of the paper that is being addressed by the review point. The authors can be confident that the review point is discussing the implications of the number of experiments on the empirical conclusion. The review point does not leave any ambiguity about which part of the paper is being addressed, which is in line with the score rubric's requirement for a score of 1. Therefore, the authors can be confident about which part of the paper is being addressed by the review point."
2844,NIPS_2022_477,"Weaknesses:
1.In experiments, the PRODEN method also uses mixup and consistency training techniques for fair comparisons. What about other competitive baselines? I'd like to see how much the strong CC method could benefit from the representation training technique.
2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.
3.In App. B.2, a relaxed solution of Sinkhorn-Knopp algorithm is proposed. Why the relaxed problem guarantees to converge?Does Solar always run this relaxed version of Sinkhorn-Knopp?
4.How is gamma in the Sinknhorn-Knopp affect the performance?
5.How does the class distribution estimate for PRODEN in Figure 1?
Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment.
Limitations: The experiments need to be further improved.",5.How does the class distribution estimate for PRODEN in Figure 1? Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment. Limitations: The experiments need to be further improved.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the negative impact of lower annotation costs and the need for further experimentation without providing any specific guidance or suggestions for improvement. The review point does not give any hint or direct actionable comments that the authors should make, nor does it include any implicit actionable comments that the authors can infer. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, focusing solely on the content of the paper. The feedback is factual, pointing out the need for further experimentation without any negative or positive connotations. The reviewer does not make any personal judgments about the quality of the paper or the authors' work, which is in line with the neutral tone expected in a review. The feedback is also constructive, suggesting that the experiments need to be improved, which is a helpful and objective piece of advice. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states the potential negative impact of lower annotation costs and acknowledges the need for further experimentation. The review point does not make any claims that could be verified or disproven, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a general commentary rather than a claim that requires verification. As such, it does not fulfill the requirements for a score of 1, which would involve making a claim and verifying it with logical deductions or sources. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the class distribution estimate for PRODEN in Figure 1. It clearly identifies the main negative impact of lower annotation costs on annotator employment, which is a specific aspect of the paper. Additionally, the review point points out the need for further improvement in the experiments, which is a direct critique of the paper's methodology. This specificity in the review point allows the authors to confidently identify which parts of the paper are being addressed. The review point does not leave any ambiguity about the paper's content and its implications. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, making it clear to the authors which parts of the paper are being discussed. So the score is 1."
1317,ICLR_2023_3854,"Weakness:
1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.
2.The novelty of the paper is limited as some parts seems like a direct combination of TD3 and SAC, but the paper does not provide detailed insight of why SAC works in complex envs while TD3 works in simple envs.
3.Theorem 2 assumes that Yi follows a uniform distribution, which might be too strong in practice.
4.The experimental results are not quite convincing. TD3 fails to learn in Humanoid-v2, can you explain the reason?","1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the limitations of the paper without suggesting any specific actions that could be taken to address these limitations. The review point does not offer any insights into how the authors could improve their work or what further research could be conducted. It also fails to provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review point is not aggressive or rude, nor is it overly encouraging or helpful. It simply states the facts about the paper's claims and the lack of comparison with other methods. The review point does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. It also does not provide any examples of politeness or lack of politeness, which are essential for a comprehensive evaluation of the review point's tone. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the ability of the paper's method to reduce the overestimation bias compared to TD3, and also claims that SD3 and DATD3 can reduce the underestimation bias of TD3. However, the review point fails to verify these claims. It does not provide any logical deductions or references to external or internal sources that would support these claims. The review point simply states the claims without any justification or explanation. This lack of verification is a significant shortcoming, as it does not meet the criteria for a score of 1, which requires the claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",0,"The review point provided is quite specific, focusing on the claim of the paper regarding the reduction of overestimation bias and underestimation bias compared to TD3. It also mentions the comparison with SD3 and DATD3, which are relevant to the paper's claims. However, the review point does not address the performance aspect of the paper, which is a significant part of the evaluation criteria. This lack of specificity in addressing the performance aspect of the paper is what makes the review point less effective. The authors would be able to identify the parts of the paper related to the review point, but they would not be fully confident about the specificity of the review point, as it does not cover all aspects of the paper. Therefore, the score is 0."
1875,ARR_2022_2_review,"-	The authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. 
-	Given that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? 
-	It would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. 
-	It was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. 
-	In addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. 
-	In Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. 
-	In Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? 
-	In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? 
-	The fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). 
-	In Section 4.3.3, how do you deal with the RoBERTa token limit? 
-	In Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. 
-	In Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. 
-	It would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ","- In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a lack of clarity in the text, without suggesting any specific actions or improvements that could be made. The review point does not offer any guidance on how to address the ambiguity in the text, nor does it provide any insight into what the ""top scoring"" sentences might be. This lack of actionable content makes it difficult for the authors to deduce what needs to be done in order to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is not particularly polite or constructive. It lacks the helpful tone that would be expected in a review. The reviewer's question about the ""top scoring"" sentences is not framed in a way that encourages the authors to provide a clearer explanation. Instead, it comes across as slightly confrontational, which could potentially discourage the authors from addressing the issue. The reviewer's tone is more neutral than constructive, which is why it does not score higher on the politeness scale. The reviewer's language is also more passive than active, which further contributes to the neutral tone. The reviewer's question does not provide any specific guidance or suggestions that could help the authors improve their work. Therefore, the review point does not meet the criteria for a score of 1, as it does not exhibit a helpful or encouraging tone. 

So the overall score is 0.",0,"The review point in question is unclear about the definition of ""top scoring"" sentences in Section 4.2.2. It does not make any claims, but it does ask a question about the methodology used in the paper. The question is about the number of sentences or the definition of a cut-off threshold for the ""top scoring"" sentences. However, the review point does not provide any logical deductions, explanations, or references to support the claim or to verify the claim. It simply asks a question without any attempt to verify the claim or to provide a logical explanation. Therefore, according to the score rubric, the review point does not contain any claims or only includes factual statements and claims that do not need verifications. So the score is 0.",-1,"The review point in question is vague and does not provide clear guidance on what the ""top scoring"" sentences are. It fails to specify whether it refers to a certain number of sentences or a cut-off threshold. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors are left with uncertainty about which parts of the paper are being addressed by the review point comment. This uncertainty is not conducive to a constructive review process, as it does not provide clear direction for the authors to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point."
5730,NIPS_2018_857,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo?  [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018.  ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.",- The writing should be improved. Some points in the paper is unclear to me.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out some limitations and areas for improvement without suggesting any specific actions to be taken. The review point does not provide any implicit actionable comments that the authors can infer, which is a key aspect of the score rubric. The lack of actionable comments in the review point means that the authors are left without clear guidance on what needs to be done to improve their paper. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is not as polite as it could be. The language used is somewhat direct and lacks the warmth and encouragement that would be expected in a constructive review. For instance, phrases like ""The writing should be improved"" and ""Some points in the paper is unclear to me"" come across as blunt and could be perceived as slightly aggressive. This is not in line with the desired tone of a review that is meant to be helpful and encouraging.

Moreover, the review point does not provide any specific suggestions or guidance on how to improve the paper. It merely states that the writing needs to be improved without offering any constructive feedback or examples of what could be done better. This lack of specificity and guidance is not conducive to a constructive review.

In contrast, a review point that is polite and constructive would provide specific suggestions and guidance on how to improve the paper. It would also use language that is encouraging and supportive, rather than blunt and direct. This would help the authors to understand what needs to be improved and how they can do so.

Therefore, based on the score rubric, the tone of the review point is not as helpful or encouraging as it could be. It lacks the politeness and constructive nature that would be expected in a review. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the writing needs improvement and that some points in the paper are unclear. This does not align with the score rubric, which requires the review point to contain claims that need to be verified. The review point does not make any claims, nor does it provide any logical deductions or references to support or refute any claims. Therefore, it does not meet the criteria for a score of 1, 0, or -1. The review point is more of a general commentary rather than a specific critique with verifiable claims. 

In conclusion, the review point does not contain any claims that need to be verified, nor does it provide any logical deductions or references to support or refute any claims. It is more of a general commentary rather than a specific critique with verifiable claims. Therefore, it does not meet the criteria for a score of 1, 0, or -1.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which parts of the paper are being addressed. The authors are left with uncertainty about which aspects of the paper are being critiqued. The review point does not provide any concrete examples or suggestions for improvement, which would have been helpful in guiding the authors to address the issues raised. The lack of specificity in the review point does not allow the authors to confidently identify the areas of the paper that need attention. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide the authors with any clear direction on how to improve their paper."
229,ICLR_2022_410,"Weaknesses:
. Theorem 4.2: For the downstream classification, the loss is upper and lower bounded in terms of the L_NCE loss. The authors provided comparison with Saunshi et al. (2019) from the technical perspective. Is there any intuitive explanation on how to evaluate the classification performance in terms of contrastive learning (loss)?
. Assumption 4.5 (intra-class connectivity): This assumption is strong. Without the label information, it seems impossible to derive such augmentation set. Please add discussion on the practicality of this assumption, and show an example on some datasets if possible.
. Proposition 4.7: Based on the proof provided in the appendix the conclusion not only relies on the existence of such augmentation set (Assumption 4.5), but also that such augmentation should be applied to intra-class samples, ie, t_i(x_i) = t_j(x_j). This kind of operation is impractical without the label information. Please add comment on that.
. In the experiments, RandomResizedCrop is used to illustrate the relationship between Aug Strength and ACC(ARC). The best performance for different datasets all achieves at Aug Strength = 0.92. Any comments on that? eg., in terms of data augmentation for intra-class samples at Aug Strength = 0.92?
. In practice, there are different kinds of data augmentation, eg, flipping, rotation, and scaling. The authors only showed results on RandomResizedCrop. Can you show results for other data augmentation types? Do you have similar conclusion as that for RandomResizedCrop?
. Different data augmentation types are often used together in practice (eg, randomly pick two augmentations from the augmentation set for the raw image). Then how to apply the proposed analysis in such practical case? In particular, how to measure the Aug Strength?
. The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping). 1) The study on applying the analysis to existing contrastive learning algorithms is, however, preliminary (only with RandomResizedCrop). 2) Based on the proposed analysis how to find the sweet spot of data augmentation for contrastive learning is crucial, but this is not discussed.",". The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the importance of data augmentation design for intra-class samples without suggesting any specific actions or improvements that could be made. The review point also lacks any implicit actionable comments that could be inferred from the text. It does not provide any guidance or suggestions that would help the authors to improve their work or to better understand the implications of their research. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the paper and providing a critique based on the information presented. The feedback is constructive, offering suggestions for improvement without being overly critical or dismissive. The reviewer's approach is consistent with the expected tone for a neutral review, which is to provide a balanced and objective assessment of the paper. Therefore, the review point is in line with the score of 0, as it does not fit well with the other two scores.",0,"The review point provided does not contain any claims or assertions that require verification. It merely states the importance of data augmentation design for intra-class samples, which is a factual statement that does not need any justification or reference. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the data augmentation design for intra-class samples. It directly addresses the importance of this aspect in the paper, which is a clear indication of the review point's specificity. The authors can be confident that the review point is discussing the design of data augmentation for perfect overlapping, which is a specific part of the paper. This specificity aligns with the score rubric's criteria for a score of 1, as it makes it obvious which part of the paper is being addressed. The authors can be confident about the paper part that is being addressed by the review point comment. 

In contrast, the review points that would score a 0 or a -1 are not specific enough. They do not clearly identify which parts of the paper are being addressed. For example, the review point that mentions the paper is hard to follow and certain sections need to be made clearer does not specify which sections are unclear or how they could be improved. Similarly, the review point that mentions the writing should be improved and some points in the paper are unclear to me does not provide any specific feedback on which parts of the paper are unclear or how the writing could be improved. 

Therefore, based on the score rubric's criteria, the review point provided is specific and directly addresses a part of the paper, making it clear which part is being discussed. This specificity is what sets it apart from the review points that would score a 0 or a -1, which are not specific enough to clearly identify which parts of the paper are being addressed. 

So the overall score is 1."
2403,NIPS_2021_1571,"How do you validate the estimated uncertainty maps, in the absence of GTs for the same ?
Wonder what is the expansion of the acronym ""D_CBase"" used ? What does it mean ?
What is MAE in fig. 4 ? Not defined anywhere in main doc. - got it in Supple though.
Fig. 9 of Supple material - how do you validate these complexity scores ? Even the GT or human evaluation is not provided. What are the typical ranges of complexity values you get by the entropy based measure ? How do you thus claim it to be consistent with human visual perception ?
Some typos: line 169: ......local minimum, it solve the following estimating.... - change to : it solves the....
Some more references may be added as:
Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011;
Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.","- change to : it solves the.... Some more references may be added as: Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011; Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It lacks specificity and does not guide the authors on what needs to be done. The review point merely mentions some limitations without providing any actionable comments that can be inferred. It does not suggest any improvements or changes that could enhance the quality of the paper. The review point is vague and does not contribute to the development of the paper. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The feedback is based on the facts presented in the paper, and the reviewer does not make any subjective judgments or comments on the quality of the paper. The reviewer also does not provide any constructive feedback or suggestions for improvement. The feedback is purely factual and does not attempt to encourage or discourage the authors. Therefore, the review point is neutral and does not fit well with the other two scores. 

The reviewer's tone is consistent throughout the feedback, and there is no variation in the tone that could be interpreted as either helpful or disrespectful. The feedback is delivered in a manner that is neither encouraging nor discouraging, and it does not contain any elements of politeness or rudeness. The reviewer's approach is purely objective, and the feedback is focused on the content of the paper rather than the authors' performance or the quality of the paper. 

In conclusion, the review point is neutral and factual, and it does not fit well with the other two scores. The tone is neither aggressive nor overly polite, and the feedback is purely factual and objective. The reviewer's approach is consistent and does not contain any elements of politeness or rudeness. Therefore, the review point is neutral and does not fit well with the other two scores. 

So the score is 0.",0,"The review point provided does not contain any claims or factual statements that require verification. It merely states that the method is promising and that the results fall behind previous work, without providing any evidence or justification to support these claims. The review point also mentions the use of MapNet as an example, but it does not make any claims about the method or its performance. Instead, it simply acknowledges the choice of MapNet and its compatibility with other systems used in the paper.

The review point does not address the relationship between the CDF and percentile rank, nor does it discuss the choice of p values or the performance of the method when multiple constraints are used. These are all factual statements that do not require verification, as they do not make any claims or assertions that need to be supported by evidence or justification.

Therefore, based on the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified using logical deductions or external or internal sources. It is a review point that is not relevant for verifiability, and it does not meet the criteria for a score of 1, 0, or -1.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The review point mentions ""Bayesian Learning via Stochastic Gradient Langevin Dynamics"" and ""Variance Reduction in Stochastic Gradient Langevin Dynamics,"" but it does not specify which sections of the paper these concepts are related to. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. The authors would likely be unsure about which parts of the paper are being discussed, which is not ideal for a review process. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the overall score is -1."
3498,NIPS_2020_944,"1. Weighted retraining is not new. The cross-entropy method (De Boer et al., 2005; Neil et al., 2018) maximizes the expectation E_p(x)[f(x)] of the objective function f(x) when sampling from a policy p(x) by periodically retraining p(x) on the samples with the highest reward, e.g. those with a reward above a quantile cutoff (i.e. using a stepwise weighting function). Instantiations of the cross-entropy method include DbAs (Brooks et al) and FBGAN (Gupta et al). Reward weighted regression (RWR) (Hachiya et al) is another existing optimization technique that employs weighted retraining. Angermueller et al. (http://arxiv.org/abs/2006.03227) recently employed these techniques as baselines for high-dimensional discrete optimization. 2. The described rank-based weighting function is not new. See RankGAN (Lin et al. 2017) or LeakGAN (Guo et al. 2017) for an example. 3. The evaluation is missing important baselines such a DbAs, FBGAN, RWR, and model-based optimization. 4. Chemical design task: It is unclear how the optimization trajectory of ‘original’ was obtained. How were new data points sampled from JT-VAE? Why does the trajectory stop at 250? 5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning. 6. What do error bars represent? How often were experiments repeated with different random seeds?","5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It lacks specificity and does not suggest any particular actions that the authors should take. The review point merely mentions some limitations without giving any explicit or implicit actions that should be done. It does not offer any guidance on how to improve the paper or what further research could be conducted. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement, such as providing a simpler expression for the bounds and strengthening the introduction with a contextual overview of the existing literature. These suggestions are delivered in a respectful and encouraging tone, which is indicative of a helpful review.

In contrast, the review point could have been more polite and constructive. For example, the reviewer could have used softer language when critiquing the introduction and the technical novelty of the paper. Instead of stating that the introduction is ""terrible"" and the paper's technical novelty is ""limited,"" the reviewer could have provided more specific and constructive feedback.

The review point also maintains a neutral tone when asking questions about the methodology and the relevance of certain sections in the paper. This neutrality is consistent with the score rubric's criteria for a score of 0.

Overall, the review point is polite and constructive, which is why it receives a score of 1. However, there is room for improvement in terms of providing more specific and constructive feedback, which would have made the review point even more helpful and encouraging.",0,"The review point in question does not contain any claims that need to be verified. It merely suggests a comparison with GCPN and reinforcement learning, which are not claims but rather suggestions for further research. The review point does not make any assertions that could be verified or refuted, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not make any claims that could be verified or refuted. It is a simple statement of intent to compare different methods, which does not require any form of verification. So the overall score is 0.",1,"The review point provided is quite specific, as it directly asks for a comparison with GCPN and reinforcement learning, which are clearly defined concepts in the field of machine learning. This specificity allows the authors to identify which parts of the paper are related to the review point. The authors can be confident that the review point is addressing the comparison with GCPN and reinforcement learning, as these are distinct methodologies that can be compared and contrasted with the JT-VAE methodology. The authors can also be confident that the review point is not a general theme or idea that is spread across many paper sections, but rather a specific aspect of the paper that needs to be addressed. Therefore, the review point meets the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed."
5083,NIPS_2020_734,"- Main math section/derivation could be improved for clarity: it's a very simple idea but it is presented in an obfuscated way that requires a lot of energy from the reader to go through. - It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper. - The gains obtained by the proposed method modification are not that large.","- It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely states that Section 6 does not belong in the paper and suggests that the extra space could be used to extend other parts of the paper. This does not provide any guidance on how to improve the paper or what specific changes could be made. The review point also does not include any implicit actionable comments that the authors could infer. It simply points out a perceived issue without offering any solutions or suggestions for improvement. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they provide any form of encouragement or helpfulness. The tone is straightforward and does not contain any personal opinions or judgments. The reviewer simply states that Section 6 does not belong in the paper and suggests that the extra space could be used to extend other parts of the paper. This is a neutral observation that does not contribute to the overall quality of the paper. The reviewer does not make any negative or positive comments about the content of the paper, which is why the tone is considered neutral. Therefore, based on the score rubric, the review point is neutral and does not fit well with the other two scores.",-1,"The review point in question makes a claim that Section 6 does not belong in the paper and that the extra space could be used to extend other parts of the paper. However, this claim is not verified. There is no logical deduction or reference to external or internal sources that would support this claim. The reviewer simply states their opinion without providing any justification or explanation. This lack of verification is in direct contrast to the score rubric, which requires claims to be supported by logical deductions and internal or external sources. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and would receive a score of -1.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The statement ""It feels Section 6 doesn't belong in this paper"" is subjective and does not provide any specific feedback on the content or structure of the paper. Furthermore, the suggestion to use the extra space for extending other parts of the paper is not substantiated with any specific examples or suggestions for improvement.

The authors would likely be unsure about which parts of the paper are being addressed by the review point. The lack of specificity in the review point does not allow the authors to confidently identify the areas of the paper that need attention. Instead, the authors might be left guessing which parts of the paper are being referred to, which is not helpful for improving the paper.

In conclusion, the review point does not meet the criteria for specificity as outlined in the score rubric. It fails to provide clear and actionable feedback to the authors, which is essential for improving the paper. Therefore, the score is -1."
3452,NIPS_2020_1505,"Even though the paper reads well in its current form, I found during the first reading that the presentation of the conceptual messages (which are the most important part in my opinion) were overshadowed by the algorithmic comparison between GOLEM and NOTEARS. The paper is dense and it is not necessarily clear what the take-away messages of the paper are. - I would encourage the authors to elaborate more on the differences between the findings their findings that are related to algorithmic implementations and the findings that pertain to the asymptotic of loss functions. - I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.",- I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the importance of discussing certain aspects in more detail and suggests the inclusion of a paragraph on the limitations of the study. However, it does not provide any specific guidance on how to improve the paper or what experiments could be conducted to enhance the results. The review point also lacks any implicit actionable comments that could be inferred by the authors. It simply states what needs to be done without providing any concrete suggestions or questions that could lead to actionable insights. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer offers a detailed critique of the paper, focusing on the importance of discussing the relevance of transforming a constraint objective into a penalized objective. The reviewer also suggests the inclusion of a paragraph on the limitations of the study in the conclusion, which is a helpful and encouraging suggestion. The tone of the review point is respectful and focused on improving the paper, rather than being aggressive or disrespectful. The reviewer's comments are specific and constructive, which demonstrates a helpful and encouraging approach to the review process. Therefore, based on the score rubric, the review point is of high quality and is delivered in a helpful and encouraging tone.",-1,"The review point provided contains a claim regarding the relevance of transforming a constraint objective into a penalized objective in the given setting. The claim is that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. However, the review point does not provide any logical deductions or references to support this claim. It also lacks a discussion on the limitations of the current study, which was suggested in the review point. Therefore, the review point does not meet the criteria for verifiability as it fails to verify its claims with logical explanations or references. This lack of verification and justification is why the review point would receive a score of -1 according to the score rubric.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It points out the importance of discussing the relevance of transforming a constraint objective into a penalized objective in the paper. The reviewer also notes the discrepancy in the results for the MLE and the square-loss, which is a critical point in the paper. Furthermore, the reviewer suggests including a paragraph on the limitations of the study in the conclusion, which is a valuable addition to the paper.

The authors can be confident that the review point is addressing a specific part of the paper, as it clearly identifies the areas that need improvement. The review point is not vague and does not leave the authors guessing about which parts of the paper are being addressed. It directly points out the need for more detailed discussion on the transformation of objectives and the discrepancy in results, as well as the inclusion of a limitations paragraph.

In contrast, a review point that lacks specificity would leave the authors unsure about which parts of the paper are being addressed. It would not provide clear guidance on what needs to be improved or clarified in the paper. The authors would be left guessing about the reviewer's concerns and would not be confident about the areas that need attention.

Therefore, based on the score rubric, the review point is specific and directly addresses a particular aspect of the paper, making it clear for the authors which parts of the paper are being addressed. The authors can be confident about the review point, and it does not leave them guessing about the paper part that is being addressed by the review point comment. So the score is 1."
4673,NIPS_2020_1254,"- The theorem may slightly overstate its result in the following way: it seems that in order for this correspondence between adversarial training and the proposed regularization scheme to hold, \epsilon must be quite small. That is, we are assuming here that all of the points in an \epsilon ball around some data point x are mapped by the model to the same activation pattern \phi_x (i.e. that B_\epsilon^p(x) \subset X(\phi_x)). I would imagine that this may not hold for ""realistic"" values of \epsilon (e.g. 8/255) all the time. Indeed, my concern is that while this theorem is certainly compelling, it may be the case that it only holds for \epsilon so small that it may not hold in practice. Perhaps the authors can clarify here. I see there is an experiment to this effect in Section 7.16, but this seems to be for only one data point. [EDIT: post-rebuttal] Based on the authors response and a closer look at Section 5.4, I'm satisfied that the authors looked into this potential weakness and were able to add explanation as to its implications.] - Figure 1 is too small to really be useful. It's not really clear what the arrows represent. A more detailed and larger figure here would be appreciated. - The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.","- The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely suggests a rearrangement of the steps in the power iteration process without providing any specific guidance or recommendations. The review point does not give any hint or direct actionable comments that the authors should make, nor does it include any implicit actionable comments that the authors can infer. It only mentions a limitation in the notation without suggesting any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers a suggestion to rearrange the steps in the power iteration, which is a helpful and encouraging approach. The tone is respectful and the feedback is focused on improving the clarity of the explanation, rather than criticizing the work. The reviewer's language is polite and the feedback is delivered in a way that is likely to be well-received by the authors. The reviewer's approach to providing feedback is in line with the desired tone of helpfulness and encouragement. Therefore, the review point is a good example of a polite and constructive review.",0,"The review point provided does not contain any claims or assertions that need to be verified. It merely suggests a rearrangement of steps in the power iteration process, which is a factual statement and does not require any justification or reference. The reviewer's suggestion to reorder the steps is a matter of personal preference and does not affect the validity or accuracy of the power iteration method. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that need to be verified. 

The review point does not make any claims about the power iteration method, nor does it provide any logical deductions or references to external or internal sources. It simply proposes a different order of steps without any supporting evidence or justification. This lack of claims and verification makes the review point fall under the score of 0, as it does not contain any claims or assertions that need to be verified. 

In conclusion, the review point does not contain any claims or assertions that need to be verified, and it does not meet the criteria for verifiability. Therefore, the score is 0.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer suggests rearranging the steps in the power iteration, but does not specify which steps should be rearranged or why this would improve the clarity of the paper. The reviewer's comment is vague and does not provide clear guidance for the authors to follow. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that is specific would clearly identify which part of the paper is being addressed and provide clear guidance for the authors to follow. For example, a review point that addresses a specific section of the paper and provides specific feedback on that section would help the authors to understand exactly what needs to be improved. 

The review point provided does not meet the criteria for specificity as outlined in the score rubric. It lacks the necessary detail to make it obvious which part of the paper is being addressed. Therefore, the score is -1."
1943,ARR_2022_247_review,"- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text. - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).
- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.
- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. 
Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).
- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.
- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").
- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).
- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""
- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?
Typos: - 166: Space after footnote 5.
- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""? ","- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out some limitations and potential issues without suggesting any specific actions to address these issues. The review point does not provide any implicit actionable comments that the authors can infer, which is a key aspect of the score rubric. It lacks the necessary detail and guidance that would help the authors to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, which is a characteristic of a neutral tone. The feedback is focused on the content of the paper, providing specific examples and suggestions for improvement. However, the reviewer could have been more constructive by offering more specific guidance or by acknowledging the potential merits of the paper. The feedback is also somewhat passive, as it does not actively engage with the paper's content or suggest ways to enhance the paper's quality. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely discusses the nature of intents in a conversational AI system, without making any assertions that could be verified or refuted. The review point does not provide any logical deductions, nor does it reference any external or internal sources to support its statements. It is a general commentary on the nature of intents and their potential for overloading or over-specification. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or refuted. So the overall score is 0.",0,"The review point provided is somewhat vague and does not clearly identify which parts of the paper are being addressed. The review point mentions that some intents do not conform to standard definitions and that others may be overloaded, but it does not provide specific examples or sections of the paper that are being discussed. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors might be left guessing which parts of the paper are being addressed, which is not ideal for a review process. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors are not fully confident about the paper part that is being addressed by the review point comment. So the overall score is 0."
5689,NIPS_2018_680,"weaknesses are: 1) The statistical aspects are a bit eluded: is the given algorithm robust to statistical imprecision? 2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?","2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the algorithm runtime in N^6 can still be quite slow and asks if there are other heuristics that are faster or if this can be improved in special cases. This does not give the authors any clear direction on what to do next. The review point also lacks any implicit actionable comments that can be inferred from the text. It does not suggest any potential improvements or areas for further research. Instead, it only mentions the limitations of the current approach without providing any actionable insights. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point is neutral and factual, which aligns with the score of 0. It does not contain any aggressive or disrespectful language, nor does it offer any encouragement or helpful suggestions. The tone is purely informative, focusing on the algorithm's runtime and the possibility of faster heuristics or improvements in special cases. The reviewer's approach is straightforward and lacks any personal opinion or emotional undertones. The feedback is strictly based on the technical aspects of the paper, which is the expected behavior in a review context. Therefore, the tone of the review point is neutral, factual, or passive, which is the default score if the review point does not fit well with the other two scores.",0,"The review point in question does not contain any claims, nor does it require verification. It is a general question about the efficiency of an algorithm and whether there are faster alternatives or improvements in special cases. The question is open-ended and does not make any assertions that need to be validated. It is a factual statement that does not require any justification or reference to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not make any claims that could be verified or refuted. It is a question that seeks information rather than making a claim that needs to be supported by logical deductions or sources. So the overall score is 0.",1,"The review point in question is quite specific, addressing the algorithm's runtime and suggesting that there may be faster heuristics or improvements in special cases. This specificity allows the authors to pinpoint the exact part of the paper that is being discussed, which is the algorithm's performance. The review point is clear and concise, making it easy for the authors to identify the relevant sections of their paper. It also provides a direction for potential improvements, which is beneficial for the authors.

In contrast, the review points that lack specificity are vague and do not clearly identify the parts of the paper that are being addressed. For example, the review point that mentions the paper being hard to follow and needing clearer sections does not provide any specific guidance on how to improve the paper. Similarly, the review point that suggests that some points in the paper are unclear does not offer any concrete suggestions for clarification.

The review point that asks if the authors have experimented with having both BN and VCL is a good example of a review point that is specific but not fully confident. It identifies a particular aspect of the paper that could be improved, but it does not provide a definitive answer or direction.

Overall, the review point in question is well-defined and directly addresses the algorithm's runtime, which is a critical aspect of the paper. It provides clear guidance for the authors to follow, which is why it is a good example of a review point with high specificity."
5198,NIPS_2020_1687,"- Since the proposed optimization is non-convex, there is no guarantee for the correctness of the bounds. - Perhaps the most important missing result in this work is confidence intervals for the bounds. - At some parts, for instance the choice of function family for p_\eta, it seems that the only criteria for the choices in the model is to make the optimization task efficient and no other justification is provided. - Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose. - In the experiments, only two cases are considered: linear Gaussian case and a second case in which the treatment is again linear and the outcome is generated by 0.3X^2−1.5XC+e. It seems necessary to consider other instances of non-linear cases as well. - The choice of the outcome equations (X-6C+e and 0.3X^2−1.5XC+e) look random. Was there any specific reason for this choice?","- Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the MLP model without suggesting any improvements or alternatives. The review point does not give any hint or direct actionable comments that the authors should make. It also does not include any implicit actionable comments that the authors can infer after reading the review point. Therefore, the authors are left without any clear direction on what needs to be done to improve the model. This lack of actionability is why the review point receives a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they provide any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating their observations and asking questions. The reviewer does not make any negative or positive comments about the paper, which is a characteristic of a neutral tone. The reviewer's language is also passive, which is another characteristic of a neutral tone. The reviewer's questions and observations are presented in a straightforward manner, without any embellishments or emotional undertones. Therefore, the review point is neutral and factual, which is why it would receive a score of 0 according to the rubric.",0,"The review point in question does not contain any claims that need to be verified. It is a general statement about the choice of response functions and the resulting bounds, without any specific claim or assertion. The reviewer simply expresses a thought about the choice of MLP and the loose bounds, without providing any evidence or justification to support their claim. This lack of specific claims and the absence of any verification process makes the review point fall under the score of 0, as it does not meet the criteria for verifiability. The reviewer does not attempt to validate their claim with logical deductions or by referencing external or internal sources. Therefore, the review point does not contain any claims that need to be verified, and it does not meet the criteria for verifiability.",-1,"The review point provided is somewhat vague and does not clearly specify which part of the paper is being addressed. The reviewer seems to be questioning the choice of response functions, but the specifics of what is loose in the bounds or why MLP might not be a good choice are not clearly articulated. This lack of specificity could lead to confusion among the authors, as they may not be sure which aspects of their paper the reviewer is referring to. The review point does not provide a clear direction for the authors to follow in order to address the concerns raised. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors may not be confident about the paper part that is being addressed by the review point comment. So the overall score is -1."
2019,ARR_2022_292_review,"1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior. ","1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a lack of interpretability in the paper, without suggesting any specific actions that the authors could take to improve the paper. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not give any hint, or direct actionable comments. After reading the review point, the authors still do not know or can not deduce what needs to be done. This lack of actionable content is why the review point would receive a score of -1 according to the score rubric. 

In contrast, a review point that provides direct and explicit actionable comments would be more helpful to the authors. It would guide them on what specific steps they could take to improve their paper. For example, a review point might suggest that the authors should try to show the value of the dataset, increase the number of annotators, or reconsider the choice of activation function. 

The review point does not provide any implicit actionable comments, which would allow the authors to infer what actions they should take. Instead, it leaves the authors without any clear direction on how to proceed. 

Therefore, the review point does not meet the criteria for actionability, and as a result, it would receive a score of -1 according to the score rubric.",-1,"The review point provided lacks the necessary politeness and constructive tone that is expected in a review. The language used is somewhat critical and does not offer any constructive feedback or suggestions for improvement. The reviewer does not provide any positive reinforcement or encouragement, which is essential for a helpful review. Instead, the reviewer seems to focus on the shortcomings of the paper without offering any constructive criticism or guidance. This approach can be perceived as aggressive and disrespectful, which is not conducive to a productive review process. Therefore, based on the score rubric, the tone of the review point is not helpful or encouraging, and it does not meet the criteria for a score of 1. So the overall score is -1.",0,"The review point in question does not make any claims, nor does it provide any logical deductions or references to support or verify any claims. It merely states that the paper could have been more detailed in explaining the semantic encoder's function. This lack of claims and verification means that the review point does not meet the criteria for verifiability as outlined in the score rubric. Therefore, the review point does not contribute to the overall score, as it neither verifies nor refutes any claims. 

In contrast, the examples provided in the score rubric demonstrate a clear understanding of the concept of verifiability. Each example contains a claim that is either verified or not, and the justification for the claim is clearly explained. This approach aligns with the score rubric's requirement for logical deductions and references to support or verify claims. 

The review point in question, however, does not provide any such logical deductions or references. It merely states that the paper could have been more detailed, without offering any specific claims or justifications. This lack of detail and verification makes the review point less useful for assessing the paper's quality. 

In conclusion, the review point does not meet the criteria for verifiability as outlined in the score rubric, and therefore does not contribute to the overall score.",0,"The review point provided lacks specificity and does not clearly identify which part of the paper is being addressed. The review point is vague and does not provide a clear direction for the authors to follow. It fails to address specific aspects of the paper, such as the semantic encoder's understanding of semantics in lower layers and the gathering of prototype equations in higher layers. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being critiqued. The review point does not provide any actionable feedback for the authors to improve their paper. It also does not give any indication of the authors' confidence in understanding the review point. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, 0, or -1."
5808,NIPS_2018_630,"Weaknesses: - While there is not much related work, I am wondering whether more experimental comparisons would be appropriate, e.g. with min-max networks, or Dugas et al., at least on some dataset where such models can express the desired constraints. - The technical delta from monotonic models (existing) to monotonic and convex/concave seems rather small, but sufficient and valuable, in my opinion. - The explanation of lattice models (S4) is fairly opaque for readers unfamiliar with such models. - The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions: - Lattice models expect the input to be scaled in [0, 1]. If this is done at training time using the min/max from the training set, then some test set samples might be clipped, right? Are the constraints affected in such situations? Does convexity hold? - I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize? - Why is this paper categorized under Fairness/Accountability/Transparency, am I missing something? - The SCNN getting ""lucky"" on domain pricing is suspicious given your hyperparameter tuning. Are the chosen hyperparameters ever at the end of the searched range? The distance to the next best model is suspiciously large there. Presentation suggestions: - The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned. - ""We have found it easier to be confident about applying ceterus paribus convexity;"" -- the word ""confident"" threw me off a little here, as I was not sure if this is about model confidence or human interpretability. I suspect the latter, but some slight rephrasing would be great. - Unless I missed something, unconstrained neural nets are still often the best model on half of the tasks. After thinking about it, this is not surprising. It would be nice to guide the readers toward acknowledging this. - Notation: the x[d] notation is used in eqn 1 before being defined on line 133. - line 176: ""corresponds"" should be ""corresponding"" (or alternatively, replace ""GAMs, with the"" -> ""GAMs; the"") - line 216: ""was not separately run"" -> ""it was not separately run"" - line 217: ""a human can summarize the machine learned as"": not sure what this means, possibly ""a human can summarize what the machine (has) learned as""? or ""a human can summarize the machine-learned model as""? Consider rephrasing. - line 274, 279: write out ""standard deviation"" instead of ""std dev"" - line 281: write out ""diminishing returns"" - ""Result Scoring"" strikes me as a bit too vague for a section heading, it could be perceived to be about your experiment result. Is there a more specific name for this task, maybe ""query relevance scoring"" or something? === I have read your feedback. Thank you for addressing my observations; moving appendix D to the main seems like a good idea. I am not changing my score. ","- The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the lack of explanation and comparison to ICNN, and the absence of a figure to illustrate the SCNN architecture. The reviewer's request for more guidance on the model's properties, such as its being convex and monotonic, is not met with any actionable advice or suggestions. The review point does not provide any implicit actionable comments that could be inferred by the authors. It merely states the limitations of the paper without offering any solutions or improvements that could be made. Therefore, based on the score rubric, the review point does not fulfill the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses a desire for more explanation and comparison to ICNN, which is a helpful and encouraging approach. The request for a figure to aid in understanding the model's properties is also a polite way to suggest improvements to the paper. The tone is respectful and the feedback is focused on enhancing the paper's quality, rather than criticizing it. The reviewer's questions are framed in a way that invites further discussion and clarification, which is a positive and polite way to engage with the paper's content. Therefore, the review point is helpful and encouraging, which is why it would receive a score of 1 according to the score rubric.",0,"The review point in question does not make any claims, nor does it require verification. It is a request for more information and explanation, which is not a claim and does not need to be verified. The reviewer is asking for more details about the SCNN architecture, its comparison to ICNN, and a figure to illustrate the model. They also seek clarification on how the model leads to a convex and monotonic model. These are all requests for additional information, not claims that need to be verified. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. 

The reviewer's questions are not claims, and they do not require any justification or reference to external or internal sources. The reviewer is simply asking for more information, which is not a claim and does not need to be verified. This is why the review point does not meet the criteria for verifiability. 

In conclusion, the review point does not contain any claims, nor does it require verification. It is a request for more information and explanation, which is not a claim and does not need to be verified. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric.",1,"The review point provided is quite specific, addressing the lack of explanation and comparison to ICNN in the paper. It also requests a figure to aid in understanding the model's properties. The review point is clear and directly addresses the paper's shortcomings, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is focused on the specific aspects of the paper that need improvement. The review point's specificity aligns with the score rubric's criteria for a score of 1, as it clearly identifies the parts of the paper that need attention. Therefore, the review point is well-crafted and meets the criteria for a high score."
589,ICLR_2022_3053,"Weakness:
It’s difficult to identify the real novelty/contribution of the proposed method. The core techniques such as importance score-based pruning and full model distillation (essentially self-distillation) have been explored in the network pruning literature.
The rules designed for parameter redistribution are draw from the observations in Fig. 4. However, there is no ablation study to validate its effectiveness as compared to other design rules.
The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction.
Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters).
There is a lack of comparison with other pruning methods.
Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).","4. However, there is no ablation study to validate its effectiveness as compared to other design rules. The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction. Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters). There is a lack of comparison with other pruning methods. Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the proposed method without suggesting any improvements or further research. The review point does not give any hint or direct actionable comments that the authors should make. It also lacks any implicit actionable comments that the authors can infer after reading the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone of the review point is not aggressive or rude, nor is it overly encouraging or helpful. It simply states the facts and observations regarding the proposed method and its comparison with other models and pruning techniques. The review point does not make any personal judgments or comments about the quality of the paper or the authors' work, which is a key aspect of maintaining a neutral tone. The feedback is focused on the content of the paper and the methodology used, without any personal opinions or biases. Therefore, the review point is in line with the score rubric's criteria for a score of 0.",-1,"The review point makes several claims, such as the effectiveness of the proposed reshaped attention block, the comparison with the DEIT model family, and the impact of latency-aware regularization. However, the review point fails to provide any verification or justification for these claims. The lack of ablation studies, comparison with other pruning methods, and logical deductions to support the claims results in a review point that does not meet the criteria for verifiability. The absence of any reference to external or internal sources further weakens the review point's credibility. Therefore, based on the score rubric, the review point does not contain any claims that are verified, and it misses a justification, explanation, or reference. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the effectiveness of the proposed reshaped attention block in the context of model compression. It also compares the proposed method with the DEIT model family and other pruning methods, which is a clear indication of the reviewer's understanding of the paper's content. The review point also points out the lack of ablation studies to validate the effectiveness of the proposed method, which is a critical aspect of the paper's evaluation. The specificity of the review point is evident in the way it directly addresses the paper's content and provides a clear critique of the proposed method. The authors can be confident that the review point is addressing a specific part of the paper, which is the proposed reshaped attention block and its effectiveness in model compression. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1."
2404,NIPS_2021_655,"weakness about their results in the final section. However, I have some concerns on the paper. First, the related works might have been cited adequately, but the authors could have done better comparing with them. Second, there is a concurrent paper that covers many of the results in this paper. I vote for a weak reject.
The concurrent paper shall not be a concern. After discussion I would raise my vote to accept. Limitations
The recent paper [1] studies the more general tensor bandit, where the reward function can be more general low-rank p-tensors. In particular, the p = 2
case in [1] seems cover the main contributions (Theorem 1-3) in this paper. Besides, Algs. 1 and 2 in [1] look quite similar to Alg. 1 in the submission, but the ones in [1] are much more general. For example, the ones in [1] can solve general ⟨ θ ⋆ , a t ⟩ p
reward, while the submission only studies ⟨ θ ⋆ , a t ⟩ 2
. The authors may need to compare their results with those in [1].
The lower bound (Theorem 2) seems to be the same as [2, Theorem 6] by putting r = 1
. The authors might need to point it out more explicitly, and it would be strange to list it as a contribution.
Other Comments
Line 72, what is x x ?
At Line 125, the idea of using curvature to obtain sqrt regret for ETC could date back to even earlier papers, such as [3].
Societal Impact
The paper is mainly theoretical, and I don't see any potential negative societal impact. Bibliography
[1] Huang, Baihe, Kaixuan Huang, Sham M. Kakade, Jason D. Lee, Qi Lei, Runzhe Wang, and Jiaqi Yang. ""Optimal Gradient-based Algorithms for Non-concave Bandit Optimization."" arXiv preprint arXiv:2107.04518 (2021).
[2] Lu, Yangyi, Amirhossein Meisami, and Ambuj Tewari. ""Low-rank generalized linear bandit problems."" In International Conference on Artificial Intelligence and Statistics, pp. 460-468. PMLR, 2021.
[3] Rusmevichientong, Paat, and John N. Tsitsiklis. ""Linearly parameterized bandits."" Mathematics of Operations Research 35, no. 2 (2010): 395-411.",1 and 2 in [1] look quite similar to Alg.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out similarities between the review point and Alg. without suggesting any specific actions or improvements. The review point does not provide any implicit actionable comments that can be inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct actions nor provides implicit actions that can be inferred. Hence, the score is -1.",0,"The review point in question is not particularly polite or constructive. It lacks the encouraging tone that would be expected in a review. The language used is somewhat neutral, but it does not provide any positive reinforcement or constructive criticism. The reviewer does not offer any suggestions for improvement or acknowledge any strengths in the paper. Instead, the reviewer simply states that the paper is similar to Alg. without providing any context or explanation as to why this is the case. This lack of detail and constructive feedback does not align with the criteria for a score of 1, which requires the review point to be helpful or encouraging. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, and it is not particularly polite or constructive. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It merely states that the review points 1 and 2 in [1] look quite similar to Alg. This is a factual statement and does not require any form of verification. The review point does not make any claims about the accuracy of the statistical analysis, the effectiveness of the texture synthesis, or the appropriateness of the p-values used in the experiments. It also does not discuss the implications of using only one constraint in the experiments. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains any claims that need to be verified nor does it provide any logical deductions or references to support any claims. Hence, the score is 0.",-1,"The review point provided in the task is quite vague and does not clearly specify which part of the paper is being addressed. The review point merely states that the sections 1 and 2 in the paper look similar to Alg., without providing any context or explanation as to why this is the case or what aspects of the paper are being compared to Alg. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors would likely be unsure about which parts of the paper are being addressed by the review point comment. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the overall score is -1."
5037,NIPS_2020_1663,"Besides the concerns about the assumptions (discussed in Strengths), my other major comment is related to experiments. 1. It seems that the proposed approach was not compared against any existing imputation method. Without doing so, it is very difficult to see the real value of the work. 2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice. 3. I find the results in Section 6.2 a bit weak. Do the results echo findings reported in the literature? Such comparisons could be useful.","2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments. It merely questions the choice of probability and the robustness of the method without suggesting any specific actions or experiments that could be done to address these concerns. The review point does not offer any implicit actionable comments that could be inferred from the text. It simply states the limitations of the method without providing any guidance on how to improve or address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point is delivered in a neutral tone, which aligns with the score rubric's criteria for a score of 0. The reviewer's question about the choice of probability in the authors' method is factual and does not contain any aggressive or disrespectful language. The reviewer also asks for a parameter analysis, which is a constructive suggestion for improving the paper. However, the reviewer's tone could be perceived as slightly passive, as it does not actively encourage the authors to address the concerns raised. The reviewer's language is more focused on the facts of the paper rather than engaging with the authors in a more encouraging or supportive manner. Therefore, while the review point is polite and neutral, it could benefit from a more active and encouraging tone to better align with the score rubric's criteria for a score of 1.",-1,"The review point in question makes a claim about the probability of deleting intervals in the proposed method. However, it does not provide any verification or justification for this claim. The reviewer expresses doubt about the chosen probability of 0.2, but fails to provide any logical deductions or references to external or internal sources that could support or refute this claim. The lack of verification in the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains a claim that is not verified, which is why it would receive a score of -1 according to the score rubric. The reviewer's question about the robustness of the work with respect to the probability is also not addressed, which further contributes to the lack of verification in the review point. So the overall score is -1.",1,"The review point in question is quite specific, addressing a particular aspect of the paper - the random deletion of intervals with a probability of 0.2. This specificity is evident in the way the review point directly questions the choice of probability and the implications of this choice on the method's performance. The review point also suggests a parameter analysis to assess the robustness of the method, which is a clear indication of the reviewer's understanding of the paper's content and the need for further exploration.

The authors of the paper would be able to identify the exact part of the paper that is being addressed by the review point. They would be confident in their understanding of the review point, as it directly relates to a specific methodological choice and its potential impact on the paper's results.

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of which part of the paper is being addressed. The authors would be left guessing which sections of the paper are related to the review point, leading to uncertainty and a lack of confidence in their understanding of the reviewer's concerns.

Therefore, based on the score rubric, the review point in question is highly specific and directly addresses a particular aspect of the paper, making it clear to the authors which part of the paper is being discussed. This specificity aligns with the criteria for a score of 1, as the authors would be confident about the part of the paper being addressed by the review point."
4199,NIPS_2020_630,"- Authors miss to cite and comment some very related work in network interpretability such as: Bau et al, CVPR 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. I think the work in Network Dissection is directly relevant to the topic discussed in the paper, particularly for its hierarchical analysis as well as focus on ImageNet. - It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics. - Why did authors restrict themselves to a smaller number of categories? I think it would be interesting to understand the behavior on a larger category set. - Have authors evaluated qualitatively some of the metrics? I think it would be interesting to see whether, additionally to the quantitative evaluation, the images have some features in common. After rebuttal: The authors have addressed most of my concerns and I will update my score to 7.","- It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely suggests that the reader might wonder about the variability of the effects and the possibility of artifacts specific to the training run or dataset. The review point does not provide any guidance on how to address these concerns or how to improve the paper. It lacks any actionable comments that can be inferred by the authors, and it does not suggest any limitations that could be addressed with specific actions. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer expresses interest in the paper's findings and suggests that additional analysis on multiple runs and other datasets would enhance the paper's credibility. This approach is helpful and encourages the authors to consider the implications of their work on a broader scale. The tone is respectful and the feedback is delivered in a manner that is likely to be well-received by the authors. The reviewer's comments are focused on the paper's content and do not include any negative or aggressive language. Therefore, the review point is in line with the score rubric's requirements for a score of 1. 

In contrast, a review point that would receive a score of -1 would be characterized by a tone that is rude, disrespectful, or aggressive. Such a review point would not be helpful or encouraging to the authors. Instead, it would likely discourage them and potentially harm their professional reputation. The reviewer's comments would be focused on the paper's shortcomings and would be delivered in a manner that is likely to be offensive. 

The review point provided is not only polite and constructive but also aligns with the score rubric's criteria for a score of 1. It is clear that the reviewer has taken care to deliver feedback in a respectful and helpful manner. Therefore, the review point is of high quality and meets the standards set by the score rubric. 

So the overall score is 1.",0,"The review point in question does not make any claims, nor does it require verification. It is a general commentary on the need for more variability in the paper's analysis, which is a factual statement that does not require any form of justification or reference. The review point does not question the validity of the paper's findings or suggest that the effects observed might be artifacts specific to a particular training run or dataset. It simply expresses a desire for more comprehensive analysis. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",0,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. It mentions the need for analysis on multiple runs and other datasets, but it does not specify which sections of the paper are related to this need. The authors might be left guessing which parts of the paper are being addressed, which is not ideal for a review process. The review point does not provide any concrete examples or suggestions for improvement, which would have been helpful in guiding the authors to address the concerns raised. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors are not fully confident about the paper part that is being addressed by the review point comment. So the score is 0."
2300,ACL_2017_371_review.json,"Weaknesses: - The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?
- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section ",- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It lacks any specific recommendations or questions that could guide the authors in improving their work. The review point merely states the performance of the GRU and its comparison with other methods, without suggesting any particular actions or improvements that could be made. It also does not provide any implicit actionable comments that could be inferred from the review point. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor disrespectful, and it does not contain any encouraging or helpful language. The reviewer simply states the facts about the GRU and its performance on a large data set. The reviewer does not offer any constructive feedback or suggestions for improvement. The reviewer also does not express any opinion on the quality of the paper or the performance of the GRU. The reviewer's tone is passive, and the review point is delivered in a straightforward manner without any additional commentary. Therefore, the review point does not fit well with the other two scores, as it does not contain any elements of politeness or rudeness. So the overall score is 0.",0,"The review point in question does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to external or internal sources. The review point simply asks for a simplified version of the GRU, its performance, and its performance on a large data set. It does not make any claims about the GRU's performance or its ability to handle large data sets. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not make any assertions that could be verified or refuted. It is a straightforward question that does not require any form of validation. So the score is 0.",-1,"The review point provided is quite vague and does not clearly specify which part of the paper is being addressed. It asks for a simplified version of the GRU, its performance, and its performance on a large data set, but it does not provide any context or examples that would help the authors identify which parts of the paper are related to the review point. The lack of specificity in the review point makes it difficult for the authors to confidently identify the parts of the paper that are being addressed. This is in contrast to the examples provided in the score rubric, which clearly indicate which parts of the paper are being addressed. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
5033,NIPS_2020_1579,"- The presented analysis is limited to a single decision affected by a confounding variable. It is a sensible assumption in many cases, but I got dissapointed by the fact that the current analysis doesn't seem to **conceptually** extend to the general case. - The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one. - Really, the paper presents a **family of bounds** parameterized by Gamma, but Gamma isn't an easily interpretable quantity. Can this be rephrased in terms of a more standard notion (e.g. an information quantity)?",- The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the difficulty in computing the bounds without suggesting any solutions or improvements. The reviewer does not provide any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific changes or improvements that could be made to the paper. It does not provide any guidance on how to address the issues raised or how to improve the paper's content or methodology. The reviewer's comments are more of an observation rather than a constructive critique with actionable steps. 

In conclusion, the review point does not offer any actionable insights or suggestions, which is why it does not meet the criteria for actionability as outlined in the score rubric.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the difficulty of the bounds and suggests a more conservative yet simpler expression, which is a helpful and encouraging suggestion. The tone is respectful and the feedback is focused on improving the paper, rather than criticizing it. The reviewer's approach is constructive, offering a potential solution to the problem presented. This is in contrast to the examples of a score of -1, which are aggressive and disrespectful. The reviewer's tone is neutral and factual, which is the default score if the review point does not fit well with the other two scores. The reviewer's feedback is not overly critical or dismissive, but rather offers a suggestion for improvement. Therefore, based on the score rubric, the review point is polite and constructive, and thus, it would receive a score of 1.",-1,"The review point in question makes a claim about the difficulty of computing the bounds, which is not verified. The reviewer suggests that a simpler expression would have been beneficial, but does not provide any logical deductions or references to support this claim. The reviewer's statement is based on a subjective opinion rather than a verifiable fact, which is why it does not meet the criteria for verifiability. The reviewer's claim is not supported by any logical explanations or references to external or internal sources, which is why it does not meet the criteria for a score of 1. Therefore, the review point does not contain any claims that are verified, and it misses a justification, explanation, or reference. So the overall score is -1.",-1,"The review point provided is quite vague and does not clearly identify which part of the paper is being addressed. The statement ""The bounds seem really hard to compute"" is a general comment that does not pinpoint any specific section or idea within the paper. It lacks specificity and does not provide any constructive feedback or suggestions for improvement. The authors would likely be unsure about which parts of the paper are being addressed by this review point, as it does not offer any clear direction or guidance. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and would receive a score of -1."
4330,NIPS_2020_1680,"The work is already quite extensive and more cannot be reasonably expected. Still, perhaps the authors can comment on these issues: 1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1. 2) While Manchev and Spratling 2020 claim that their difference target prop for RNNs outperforms BPTT in (simpler) 4 of 5 tasks, why does DRL which is an improvement on DTP not come closer to backprop in these tasks? 3) Would a correction different from GN targets enable to come closer to backprop? 4) In lines 177-178, the authors write 'This damping interpolates between the pseudo-inverse and the transpose of Jf ̄ i,L , so for large λ, GN targets resemble gradient targets.' I would assume that the authors would optimize over lambda as a hyperparameter, but I did not find it in the list of hyperparams in Tables S1-S4. WHy not? In any case, it would be good to explore the dependence of performance on lambda explicitly in a (supplementary) plot and how this impacts performance expecially given that for 'large λ, GN targets resemble gradient targets'.",1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the work without suggesting any specific actions that could be taken to address these limitations. The review point does not provide any implicit actionable comments that could be inferred by the authors. It simply states the issues without offering any guidance on how to improve the work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor does it provide implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper and the potential implications of the findings. The feedback is delivered in a manner that is neither encouraging nor discouraging, but rather neutral and informative. The reviewer does not make any personal judgments about the quality of the paper or the abilities of the authors, which is in line with the rubric's requirement for a neutral tone. The feedback is also factual, as it is based on the information provided in the paper and does not include any subjective interpretations or opinions. Therefore, the review point is consistent with the score of 0 in the rubric.",0,"The review point in question does not contain any claims that need to be verified. It merely states the existence of a gap between DRL+variants and backprop, as well as the non-convergence of GNT on nonlinear networks. These statements are factual and do not require any justification or reference to external or internal sources. The review point does not make any claims that could be verified or disproven, which is why it does not meet the criteria for a score of 1 or 0. It also does not contain any claims that are not verified, which is why it does not meet the criteria for a score of -1. Therefore, the review point is neutral in terms of verifiability.",1,"The review point provided is quite specific and directly addresses the paper's discussion on the convergence of GNT on nonlinear networks and the experimental gap between DRL+variants and backprop. It clearly identifies the sections of the paper that are being discussed, namely lines 245-246 and section 4 / Table 1. This specificity allows the authors to confidently identify which parts of the paper are being addressed by the review point. The review point is also well-structured, providing a clear and concise critique of the paper's content. It does not leave any ambiguity about the paper's sections that are being addressed, which is crucial for the authors to understand and respond to the review point. Therefore, based on the score rubric, the review point is of high quality and meets the criteria for a score of 1."
4757,NIPS_2020_1819,"- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction. - The results are not significantly higher than that of the closest previous work on English. On Chinese it is, but one has to wonder whether that might be due to implementation or hyperparameter differences rather than the particular model. - There are some missing ablations for quantifying where the performance gains come from, and for justifying some design choices (see details below).",- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states that the approach is not novel and that the parameterization of parsing action prediction is more expressive. This does not provide any guidance or suggestions for the authors to improve their work or to address the limitations mentioned. The review point also lacks any implicit actionable comments that could be inferred by the authors. It does not prompt the authors to consider alternative methods or to explore different aspects of their work. Instead, it simply points out the limitations of the approach without offering any constructive feedback or recommendations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the technical aspects of the paper. The feedback is based on the content of the paper and the reviewer's understanding of the subject matter. The reviewer does not make any personal judgments or criticisms, but rather provides a factual assessment of the paper's strengths and weaknesses. The feedback is also constructive, as it suggests potential improvements to the paper. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the approach is not novel and that the parameterization of parsing action prediction is more expressive. This is a factual statement that does not require any form of verification. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",1,"The review point provided is quite specific, focusing on the parameterization of parsing action prediction. It directly addresses the novelty of the approach and the main contribution of the paper. The authors can be confident that the review point is discussing the specific aspect of their work, which is the expressive parameterization of parsing action prediction. This is because the review point clearly identifies the part of the paper that is being addressed, making it easy for the authors to understand and respond to the feedback. The review point does not leave any ambiguity about the paper part being discussed, which is crucial for the authors to be able to confidently address the feedback. Therefore, based on the score rubric, the review point is highly specific and directly addresses the main contribution of the paper, making it a score of 1."
3203,NIPS_2020_1444,"1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point. 2. In Theorem 1, it seems that the algorithm can only achieve meaningful results when the bias term decreases very fast with respect to t. In some other cases (such as when the bais is fixed), the regret is linear. 3. In Theorem 2, to achieve meaningful results, one has to know the value of v in advance, which is generally impossible.","1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the difficulties in implementing the algorithm and the challenges in sampling an action from the decision set K. The reviewer does not suggest any specific improvements or modifications that could be made to the algorithm or the methodology. Instead, the reviewer leaves the authors to deduce what needs to be done, which is not helpful in terms of actionability.

The review point does not provide any implicit actionable comments either. It does not give any hints or suggestions that the authors could infer from the review point. The reviewer simply states the limitations of the algorithm without offering any guidance on how to overcome these limitations.

In conclusion, the review point does not meet the criteria for actionability as it neither provides explicit actionable comments nor does it offer implicit actionable comments. Therefore, the score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The tone is purely informative, with the reviewer pointing out potential difficulties in implementing the algorithm and the lack of discussion on the decision set K. The reviewer's comments are focused on the technical aspects of the paper, without any personal judgments or emotional undertones. The reviewer's language is straightforward and does not contain any language that could be construed as disrespectful or offensive. Therefore, based on the score rubric, the review point is neutral and factual, and does not fit well with the other two scores.",-1,"The review point in question makes a claim about the difficulty of implementing Algorithm 1 in real-world applications and the challenge of sampling an action from the decision set K after obtaining p_t. However, the review point does not provide any logical deductions or references to support these claims. The lack of verification and justification for the claims made in the review point is a significant shortcoming. The review point does not meet the criteria for verifiability as it fails to provide a logical explanation or reference to external or internal sources to support its claims. Therefore, the review point does not fulfill the requirements of the score rubric, which emphasizes the importance of verifying claims through logical explanations or references. 

In contrast, a review point that meets the criteria for verifiability would provide a logical explanation or reference to external or internal sources to support its claims. For example, a review point might discuss the statistical analysis of the paper and compare the p-values reported with the standard thresholds for significance, providing a logical deduction based on established statistical guidelines. 

The review point in question does not meet the criteria for verifiability, as it lacks logical deductions and references to support its claims. Therefore, the review point is not verified and does not meet the standards set by the score rubric. 

So the overall score is -1.",1,"The review point provided is quite specific, addressing the implementation challenges of Algorithm 1 and the difficulty in sampling an action from the decision set K. It directly points out the issues with the problem in Eq.(7) and the form of the decision set K, which are key aspects of the paper. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is focused on the implementation difficulties and the decision set K, which are central to the paper's content. Therefore, the review point meets the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed and the authors can be confident about it."
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the lack of clarity in the experiment's architecture and the computation of the DIM similarity. The reviewer suggests moving information to different sections, but this is not a direct action that the authors can take. The reviewer also mentions the lack of connection between the theoretical results and the main ideas of the paper, but again, this is not an actionable comment. The reviewer does not provide any explicit or implicit actions that should be done, which is why the score is 0. The review point does not fulfill the criteria of providing actionable comments or steps that the authors can infer or take.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer offers a clear and helpful suggestion to improve the organization of the paper, specifically by moving certain information to the appendix and the main text. This advice is delivered in a respectful and encouraging tone, which is essential for maintaining a positive and constructive dialogue. The reviewer also provides a rationale for the suggested changes, explaining how the information in the appendix could be used to motivate the hypothesis of the experiment. This demonstrates a thoughtful approach to the review process, which is indicative of a high level of politeness. Furthermore, the reviewer addresses the issue of the theoretical results' organization and lack of connection to the main ideas of the paper, offering a constructive critique without being aggressive or disrespectful. The reviewer's tone is consistently helpful and encouraging throughout the feedback, which is why it is scored as a 1.",0,"The review point provided does not contain any claims that need to be verified. It merely suggests a reorganization of the paper's content without making any assertions that require justification or explanation. The review point does not mention any external or internal sources that could be used to verify the claims. It also does not provide any logical deductions that could be used to support the claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains claims that need to be verified nor does it provide any logical deductions or references to support the claims. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the issues in Section 6.2 of the paper. It suggests a clear and actionable solution to the problem of the architecture and the computation of the DIM similarity. The review point also provides a rationale for the experiment, which is a crucial aspect of the paper. It connects the theoretical results to the main ideas of the paper, which is a significant improvement in the organization of the paper. The authors can be confident that the review point is addressing a specific part of the paper, and they can identify the related sections. The review point is well-structured and directly addresses the issues raised in the paper, making it easy for the authors to understand and act upon the feedback. Therefore, the review point is highly specific and directly addresses the issues in the paper, which aligns with the score rubric's criteria for a score of 1."
1580,ICLR_2023_3912,"Weaknesses: 1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance. 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations. 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods. 4. I want to know why the mode parameters in Table 1 and Figure 5 are different. 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.","1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the proposed method has a little performance improvement with more parameters and variance compared to DyTox. This statement does not suggest any specific actions or improvements that could be made to the method or the dataset. Furthermore, the review point does not include any actionable comments that can be implicitly inferred. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the content of the paper and providing a critique based on the information presented. The feedback is constructive, offering suggestions for improvement without being overly critical or dismissive. The reviewer also refrains from using any language that could be perceived as disrespectful or rude. Therefore, the review point is in line with the criteria for a score of 0, as it maintains a neutral tone throughout.",-1,"The review point provided makes a claim about the performance improvement of the proposed method compared to DyTox. However, it fails to verify this claim with logical deductions or references to external or internal sources. The claim is made without any supporting evidence or justification, which is a critical aspect of the score rubric. The review point does not provide any logical explanation or reference to external or internal sources that would validate the claim. Therefore, it does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified. 

The review point also lacks any mention of the variance, which is an important aspect of the claim made. The absence of any discussion on the variance further weakens the review point's verifiability. 

In conclusion, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It lacks logical deductions, external or internal sources, and a discussion on the variance, which are all critical aspects of the score rubric. Therefore, the review point is not verifiable and does not meet the standards set for performance.",1,"The review point provided is quite specific, pointing out a performance improvement with more parameters and variance when compared to DyTox. This specificity allows the authors to identify which parts of the paper are related to the review point, namely the performance of the model and the comparison with another method. The authors can be confident that the review point is addressing the performance aspect of the paper.

However, the review point could have been more explicit in identifying the exact sections of the paper that are being addressed. For example, it could have mentioned specific sections or figures where the performance improvement is observed. This would have made the review point even more specific and actionable for the authors.

In conclusion, the review point is specific enough to guide the authors towards the relevant parts of the paper, but it could have been more explicit in identifying the exact sections or figures. Therefore, the score is 1, as the authors can be confident about which part of the paper is being addressed by the review point."
5533,NIPS_2020_1458,"- As shown in Table1, the proposed Element-weighted TriHard loss actually does not perform better than several existing strategies. Although combining EWTH with HTH or HNTH gets improved results, the effectiveness of the designed method of this paper is incremental. - the best value of t is varying for different methods and datasets (Supplementary materials), so it is too sensitive to selected. - Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.","- Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out a problem with the mathematical symbols in the paper without suggesting any specific actions that the authors should take to address the issue. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any direct and explicit actionable comments that the authors should make, nor does it provide any implicit actionable comments that the authors can infer. It merely points out a problem without offering any solutions or further steps that the authors should take. This lack of actionable content is why the review point does not meet the criteria for actionability. 

In conclusion, the review point does not provide any actionable comments or steps, and therefore does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer identifies a potential issue with the mathematical symbols in the paper, specifically the use of x_k in equation (5) without prior mention. This is a constructive critique, pointing out a specific area where clarity could be improved. However, the tone of the review point is not particularly polite or encouraging, nor is it aggressive or disrespectful. It simply states the problem without adding any positive or constructive feedback. The reviewer does not offer any suggestions for improvement or express any appreciation for the paper's strengths. This makes the tone of the review point more neutral than helpful or encouraging. Therefore, based on the score rubric, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",-1,"The review point in question identifies a potential issue with the mathematical symbols used in the paper. It points out that the symbol x_k under equation (5) is not mentioned earlier, which could lead to confusion or misinterpretation. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The review point simply states the problem without offering any solution or further analysis. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. It fails to provide a logical explanation or reference to support the claim, which is essential for verifying the review point. Therefore, based on the score rubric, the review point is not verified and does not meet the criteria for a score of 1.",1,"The review point provided is quite specific, pointing out a particular mathematical symbol, x_k, that is used in equation (5) without any prior mention. This specificity directly addresses the authors' use of symbols and their clarity, which is a critical aspect of the paper. The review point is clear and concise, making it easy for the authors to identify the exact part of the paper that needs attention. The authors can be confident that the review point is addressing a specific issue in their paper, which is the correct use of mathematical symbols. This level of specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to pinpoint the exact part of the paper that is being addressed. Therefore, the review point is well-crafted and directly addresses the paper's content, making it a valuable piece of feedback for the authors."
3185,NIPS_2020_1801,"1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers). 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim. 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score. 4. Were other architectures explored besides WideResNet? 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well. 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?",5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of standard deviation in the results and questions the randomness of hyper-parameter configurations. However, it does not suggest any specific actions or experiments that could be done to address these issues. The review point also fails to provide any implicit actionable comments that could be inferred from the review point. It simply states the limitations without offering any solutions or further investigation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor does it provide implicit actionable comments. So the overall score is -1.",0,"The review point is neutral and factual, which aligns with the score of 0. It does not contain any aggressive or disrespectful language, nor does it offer any encouragement or constructive feedback. The review point simply asks questions about the methodology and the results of the study, without making any judgments or providing any suggestions for improvement. The tone is passive, and the reviewer does not express any opinions or emotions about the study. The review point is focused on the details of the study and the methodology used, which is appropriate for a scientific review. However, it lacks the helpful and encouraging tone that would be expected for a score of 1. The reviewer does not offer any positive feedback or suggestions for improvement, which would have made the tone more constructive. So the overall score is 0.",-1,"The review point in question makes a claim about the lack of standard deviation in the results of the 10 runs. However, it fails to provide any justification or explanation for this claim. The review point does not mention any external or internal sources that could verify the claim. It also does not provide any logical deductions that could support the claim. The lack of verification and justification in the review point makes it difficult to assess the validity of the claim. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability. It contains a claim that is not verified, and it misses a justification, explanation, or reference. So the overall score is -1.",1,"The review point in question is quite specific and directly addresses a particular aspect of the paper. It asks about the standard deviation in the results and whether the hyperparameters were randomly selected or the best ones were used. This specificity allows the authors to pinpoint exactly which part of the paper is being questioned. The review point is clear and concise, making it easy for the authors to understand and respond to. It also encourages the authors to provide more information about their experimental setup, which is crucial for the reproducibility of their work. Therefore, the review point is well-aligned with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed."
88,ICLR_2022_3123,"Weaknesses:
The proposed algorithm is lack of novelty. Compared to the previous video transformer (ViVit, TimeSformer), the difference of this work is: 1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention. 2) this work introduce additional residual connection (with learnable ratio) between attention block from adjacent transformer blocks. 3) the final output is weighted average of the output from each transformer block while the TimeSformer take the output of last block as the final representation.
This paper would need more evidence to support its claim. For example, it's not clear why we should have the current layout of the network. How a set of spatial transformer block followed by a set of temporal transformer block is better than alternating design? considering the cross-stage self-attention could work in both cases.
The improvement of performance is minor. Take the ViVit as the baseline. If we are going to compare between the best performance between ViViT and this paper, the improvement is minor (81.3% from ViViT vs. 81.8% from this paper). If we are going to compare the performance using the same input resolution (16x224x224), the performance of this paper is still on par with ViViT (80.6% from ViViT vs. 80.1% from this paper). In terms of the computational cost, it is wrong to compare GFLOPs between two methods with different inference views. As the performance of any method will not be linearly increased with number of inference views, it is not fair to put 4x3 views for ViViT and 1x3 views for this paper. As a result, the number of 8.6% does not make too much sense. At last, I would suggest to include MViT[1] as one of the reference, which is the SOTA video transformer so far.
The proposed work should be evaluated in at least one or two more video benchmarks. The nature of K400 and K600 is the same. I would suggest also include Something-Something-V2 or Epic-Kitchen datasets.
[1] Multiscale vision transformers, ICCV 2021","1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely states the structure of the model without suggesting any improvements or modifications that could be made. The review point does not provide any implicit actionable comments that can be inferred from the text. It also does not mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor encouraging, but rather a straightforward description of the methodology and its potential implications. The reviewer does not express any personal opinions or judgments, which is a characteristic of a neutral tone. The feedback is focused on the technical aspects of the paper, without any personal attacks or overly positive comments. The reviewer's language is clear and concise, which is appropriate for a technical review. The feedback does not contain any elements of politeness or rudeness, which is why it fits the score of 0. The reviewer's approach is consistent with the expected tone for a technical review, which is to provide a factual and unbiased assessment of the paper. 

So the overall score is 0.",-1,"The review point provided contains a claim about the architecture of the video transformer, specifically the alternating use of spatial and temporal attention in the TimesFormer. However, the review point does not provide any verification or justification for this claim. It lacks logical deductions or references to external or internal sources that could support the claim. The absence of any verification or justification for the claim makes it difficult to assess the validity of the claim. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of -1.",1,"The review point provided is quite specific and directly addresses the structure and function of the video transformer models described in the paper. It clearly distinguishes between the spatial and temporal attention mechanisms used in the different sets of transformers. The review point also highlights the alternating use of spatial and temporal attention in the TimesFormer model. This specificity allows the authors to confidently identify which parts of the paper are being addressed by the review point. The review point is well-aligned with the score rubric's criteria for a score of 1, as it provides a clear and direct connection to the paper's content. The authors can be confident that the review point is addressing a specific aspect of the paper, which is the structure and function of the video transformer models. 

In contrast, a review point with a score of -1 would not provide any specific information about the paper's content. It would leave the authors unsure about which parts of the paper are being addressed by the review point. The review point would not help the authors to identify the specific aspects of the paper that need to be improved or clarified. 

The review point with a score of 0 would allow the authors to guess which parts of the paper are being addressed, but they would not be fully confident. The review point would not provide a clear and direct connection to the paper's content, and it would leave some ambiguity about the specific aspects of the paper that need to be improved or clarified. 

In conclusion, the review point provided is specific and directly addresses the structure and function of the video transformer models described in the paper. It allows the authors to confidently identify which parts of the paper are being addressed by the review point. Therefore, the review point meets the criteria for a score of 1 in the score rubric."
198,ICLR_2022_32,"Weaknesses: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper","- The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out a lack of mention of Tensor Networks in the paper, which is a limitation but does not suggest any specific actions or improvements that could be made. The review point does not provide any implicit actionable comments that could be inferred by the authors, nor does it give any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any direct and explicit actionable comments that the authors should make, nor does it provide any implicit actionable comments that the authors can infer. It only mentions a limitation without giving any explicit or implicit actions that should be done. This lack of actionable content is why the review point would receive a score of -1 according to the score rubric. 

So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The review point simply states the absence of a specific application in the paper, which is a factual observation. It does not offer any constructive criticism or suggestions for improvement, which would have been more aligned with a score of 1. The review point also does not contain any negative or rude language, which would have been more aligned with a score of -1. Therefore, based on the rubric, the review point is neutral and factual, and does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not make any claims, nor does it require verification. It simply states that the paper does not mention its application for computing and manipulating Tensor Networks, which is a factual statement. The review point does not contain any claims that need to be verified, nor does it include any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain a claim that is verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the score is 0.",1,"The review point provided is quite specific, as it directly addresses the lack of mention of Tensor Networks in the paper. This specificity is crucial for the authors to understand exactly what aspect of their work is being critiqued. The review point is clear and concise, making it easy for the authors to identify the part of the paper that is being addressed. It is also directly related to the paper's content, which is a key factor in the scoring rubric.

The review point does not leave any ambiguity about the paper's shortcomings. It directly points out the absence of a crucial application of the paper, which is the manipulation of Tensor Networks. This specificity aligns with the score rubric's requirement for a review point to be clear and directly related to the paper's content.

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of what part of the paper is being addressed. It would leave them guessing, which is not helpful for the authors to improve their work.

Therefore, based on the score rubric, the review point provided is of high quality, as it is specific, directly related to the paper's content, and leaves no ambiguity for the authors. It is clear that the authors would be confident about which part of the paper is being addressed by the review point. 

So the overall score is 1."
4113,NIPS_2020_558,"- Further experiments: I would've liked to see more experimental results on additional graph problems. This could drive your point further by showing more examples of how the loss function can be constructed. - Scalability bottleneck: GNN training can only scale up to some level. Thoughts on whether your method can be scaled up despite this limitation? - Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.","- Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of clarity in the architecture selection process and suggests that an ablation study or detailed results on the validation sets could be helpful. However, it does not provide any specific guidance or recommendations on how to proceed with these studies or what aspects to focus on. The review point also does not offer any implicit actionable comments that the authors can infer from the review point. It simply states that the architecture selection process is unclear and that more information would be beneficial. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does it show any form of encouragement or helpfulness. The tone is purely informative, with the reviewer pointing out the lack of clarity in the architecture selection process. The reviewer also suggests potential solutions, such as conducting an ablation study or providing detailed results on the validation sets. However, the reviewer does not provide any specific examples of politeness or lack thereof, which would have been helpful in assessing the tone of the review point. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not make any claims, nor does it require verification. It is a general commentary on the architecture selection process, which is not a claim but rather a statement of fact. The reviewer is expressing a need for more information to understand the architecture choices, which is a valid point but does not require any form of verification. The reviewer is also suggesting potential methods for understanding the architecture, such as ablation studies or detailed results on validation sets. However, these are not claims but rather suggestions for further investigation. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any assertions that need to be substantiated. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the architecture choices made in the paper. It specifically points out the lack of clarity in the appendix regarding the architecture selection, and suggests that an ablation study or detailed results on the validation sets could help clarify the necessary and unnecessary components of the architecture. This review point is clear and directly related to the paper's content, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is focused on the architecture choices and their justification. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, which aligns with a score of 1."
3237,NIPS_2020_1454,"- The novelty seems limited. The idea of building correlation in low-res and then refine for or facilitate the high-res results might be new in the literature for correspondence search, but it is quite common and has been widely adopted in previous work doing stereo match, where the main job is also to find correspondence but along epipolar line. - The main contribution of this paper, IMO, is to run 4D convolution on low-res correlation volume, which saves computation and possibly achieve comparable performance. If so, the experiment showing the saving of computational resources, e.g. gpu runtime, flip-flop, memory, must be given. - Similar multi-scale approach in stereo matching often runs fast at the cost of losing accuracy, since the correlation volume in low-res is not as informative as the high-res one, and it is not easy to fix if some mistakes are made in low-res. However the experiments show that the result is even better than SOTA. It would be good to add more explanation and analysis. - It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.","- It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the limitations of the proposed method without suggesting any specific actions to address these limitations. The review point also lacks any implicit actionable comments that could be inferred by the authors. It does not give any hint or direction on what needs to be done to improve the proposed method. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement, such as providing a simpler expression for the bounds and strengthening the introduction with a clearer context. These suggestions are delivered in a respectful and encouraging tone, which is helpful for the authors.

In contrast, the review point that would receive a score of -1 is aggressive and disrespectful. The reviewer uses harsh language, such as ""terrible"" and ""poor quality,"" which is not conducive to a constructive review. The reviewer also makes negative judgments about the paper's suitability for publication, which is not helpful for the authors.

The review point that would receive a score of 0 is neutral and factual. The reviewer asks questions about the paper's methodology and structure, but does not provide any suggestions for improvement or encouragement. The tone is passive, and the reviewer does not engage with the paper in a way that would be helpful to the authors.

Overall, the review point provided is polite and constructive, which is in line with the score rubric's criteria for a score of 1. The reviewer offers helpful suggestions and maintains a respectful tone throughout the review.",0,"The review point provided does not contain any claims that need to be verified. It merely suggests showing qualitative results and analyzing limitations, which are not claims but rather suggestions for the review process. The review point does not make any assertions that could be verified or refuted, nor does it provide any logical deductions or references to external or internal sources. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is more of a general guideline rather than a claim that requires verification. As such, it does not fulfill the requirements for a score of 1, which would involve a claim supported by logical deductions and internal or external sources. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the paper's methodology and results. It asks for qualitative results and a discussion of the limitations of the proposed method, which are clearly defined aspects of the paper. The authors can easily identify which parts of the paper are related to the review point, as it specifically mentions the need for a zoomed-in view and the analysis of failure cases. This level of specificity ensures that the authors are confident about which part of the paper is being addressed by the review point. The review point is well-structured and directly addresses the paper's content, making it clear and actionable for the authors. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, which aligns with a score of 1."
5051,NIPS_2020_1889,"- The performance of the method is still sub-optimal on large scale datasets, and hence there is a scope of improvement. - The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.","- The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the proposed method is somewhat similar to the earlier work and mentions the usage of data-dependent methodologies for grouping. However, it does not suggest any specific actions or improvements that could be made to the method or the paper. The review point also does not include any actionable comments that can be implicitly inferred. It only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does it show any form of encouragement or helpfulness. The tone is purely informative, with the reviewer pointing out potential issues and areas for improvement without any personal bias or emotional undertones. The reviewer's comments are focused on the content of the paper, rather than the authors or the conference itself. This is evident in the reviewer's use of phrases like ""it might be the case"" and ""the technical novelty of this paper is limited,"" which are neutral and do not convey any form of politeness or lack thereof. Therefore, based on the score rubric, the review point is neutral and factual, and does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the proposed method is somewhat similar to the earlier work [35] and uses data-dependent methodologies for grouping. This is a factual statement that does not require any justification or verification. The review point does not make any claims that could be verified or invalidated, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided is somewhat vague and does not clearly specify which part of the paper is being addressed. It mentions that the proposed method is similar to an earlier work and uses data-dependent methodologies for grouping, but it does not provide any concrete examples or details that would help the authors identify the specific sections of the paper that are being discussed. The review point also lacks specificity in terms of the training and prediction algorithms, which are crucial aspects of the paper.

The authors might be left with a sense of uncertainty about which parts of the paper are being addressed by the review point. They might not be confident in their understanding of the paper's content and how it relates to the review point. The lack of specificity in the review point could lead to confusion and a lack of clarity for the authors.

In contrast, a review point that is more specific would help the authors to pinpoint the exact sections of the paper that are being discussed. It would provide clear examples and details that would allow the authors to confidently identify the parts of the paper that are being addressed by the review point.

Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors might not be confident about the paper part that is being addressed by the review point comment. So the overall score is -1."
663,ICLR_2021_2455,"Weaknesses: In spite of the strengths mentioned above, there are a few questions that are confusing. 1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed. 2. As for the theoretical analysis: a) For Theorem 1, the right hand equation uses L_2 norm of a function of beta. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Therefore, I wonder what does the regularizer proposed in equation(6) means since beta has already been integrated. b) For Theorem 1, the core assumption is ‘the expected loss function as a function of beta belongs to a Sobolev space’, which is confusing. Could you provide some explanations of this assumption or give some examples of it? c) Theorem 1 provides an upper bound for one specific kind of DRO problem whose uncertainty set is formulated as an affine combination of training distributions. However, in this article, the authors do not state what is the definition of the invariance here and why solve such DRO problem could achieve the invariance. 3. As for the proposed objective function: a) As mentioned above, the L_2 norm is taken over a function of beta, which I think is not the Euclidean norm of the vector. Beta has already been integrated and this regularizer has nothing do with beta. I wonder how to compute this when optimizing? b) I wonder how this objective function can be optimized efficiently? The first concern is mentioned above as the computation of L_2 norm. The second concern is how to optimize the variance which is non-convex and hard to optimize. Namkoong et al. [1] convert the optimization of a variance-regularized problem to a f-divergence DRO for better optimization, while in this paper the authors take the opposite way. I wonder is there any theoretical guarantee of the optimization of the objective function(6). 4. As for the experiments: a) The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in 3).
[1] Duchi, J. , & Namkoong, H. . (2016). Variance-based regularization with convex objectives.","1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the performance of the methods without suggesting any improvements or further experiments that could be done. The review point also fails to provide any implicit actionable comments that could be inferred from the review. It simply points out the limitations of the methods without offering any solutions or recommendations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor does it provide implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts about the performance of the methods in the experiment, without adding any personal opinion or encouragement. The reviewer also points out the absence of the result for the IRM method, which is a factual observation. The reviewer does not make any negative or positive comments about the quality of the paper or the performance of the methods, which is why the tone is neutral. The reviewer's comments are focused on the content of the paper and the results of the experiment, rather than the quality of the paper or the performance of the methods. Therefore, the review point is neutral and factual, which is why it receives a score of 0.",-1,"The review point in question makes a claim about the performance of the third figure in the simulated experiment. However, it fails to provide any logical deductions or references to support this claim. The review point merely states that the perfect causal model performs poorly under unobserved conditions, while the other three methods perform almost the same. It also mentions that the performance of DIRM and DRO is quite similar, but it does not provide any evidence or reasoning to back up these statements. Furthermore, the review point notes that the result of IRM for this experiment is missing, but it does not attempt to verify this claim or provide any context or explanation.

In terms of the score rubric, the review point contains a claim but does not verify it. It lacks logical deductions and internal or external sources to support the claim. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and would receive a score of -1.",1,"The review point provided is quite specific, addressing a particular aspect of the paper, which is the performance of the third figure in the experiment. It clearly identifies the purpose of the third figure and compares it with the other three methods. The review point also points out the similarity in performance between the proposed DIRM and DRO, and the absence of the result for the IRM method. This specificity allows the authors to understand exactly which part of the paper is being addressed, and they can confidently identify the related sections. The review point is well-structured and directly addresses the paper's content, making it easy for the authors to follow and respond to. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper's content, which aligns with a score of 1."
4778,NIPS_2020_770,"1. All components, BGV and TFHE, are borrowed from other papers. Similar to Chimera, the switching mechanism is not proposed by the authors. Even transfer learning has developed for a long time. The authors are suspect of just ensembling these ideas together. The reviewer doubts the novelty of the proposed method. 2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.",2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable comments that the authors should make. It merely points out the lack of explanation regarding the performance of FHESGD equipped with BGV compared to their system equipped with TFHE-BGV. The reviewer's expectation for more analysis of mechanics is not met, and there are no actionable comments that can be implicitly inferred. The review point only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer's tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts and points out the lack of analysis in the paper, which is a neutral observation. The reviewer also points out that the comparison with Chimera is not made in the paper, which is a factual observation. The reviewer does not use any language that could be construed as rude or disrespectful, nor does the reviewer use any language that could be construed as overly encouraging or helpful. The reviewer's tone is consistent throughout the review point, which is why it is scored as 0. 

The reviewer's feedback is focused on the lack of analysis and the missing comparison with Chimera, which are factual points. The reviewer does not make any personal judgments or comments about the quality of the paper or the authors' work, which is why the tone is neutral. The reviewer's feedback is also constructive, as it provides specific areas for improvement. 

In conclusion, the review point is neutral and factual, and it does not contain any language that could be construed as rude, disrespectful, or aggressive. The reviewer's tone is consistent throughout the review point, and the feedback provided is focused on the lack of analysis and the missing comparison with Chimera. Therefore, the review point is scored as 0.",-1,"The review point in question makes a claim about the performance of FHESGD with BGV versus FHESGD with TFHE-BGV. However, it fails to provide a logical explanation or any external or internal sources to verify this claim. The reviewer expects a more detailed analysis of the mechanics behind the performance differences, which is not provided. Additionally, the reviewer notes that the figures are supposed to compare with Chimera, but this comparison is not made. This lack of verification and explanation results in a score of -1, as the claim is not supported by logical deductions or references. 

The review point does not contain any factual statements or claims that do not need verifications, which is why it does not score a 0. It is clear that the review point contains a claim that is not verified, and therefore, it does not meet the criteria for a higher score. 

In conclusion, the review point fails to provide a logical explanation or any external or internal sources to verify the claim, and it does not make a comparison as expected. This lack of verification and explanation results in a score of -1.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer's expectation for a comparison with Chimera is not met, and the authors are left with uncertainty about which parts of the paper are being discussed. The review point does not provide a clear direction for the authors to improve their paper, nor does it offer a specific critique that could be addressed. The lack of specificity in the review point makes it difficult for the authors to understand and respond to the feedback. Therefore, the score is -1."
2812,NIPS_2022_1430,"Weakness:
One major issue with this paper is clarity of text and definitions. Examples:
Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature
Equation 3, P ( < s > | x ) = ∑ y ∈ A P ( y | x )
, I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)?
text issues:page 5, ""is is useful think ...""
Many issues in Table1: 4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean? 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo?
The definition of the randomly split the words in bottom of page 6 is not clear to me.
The notation pDrop, how about p_drop ?
Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant.
Finally, there is a major concern about baseline comparisons:
All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3.
STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?","4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states a fact about the dataset's performance without suggesting any improvements or further experiments. The review point does not prompt the authors to consider any specific changes or to provide additional information that could be used to enhance the paper. It also does not ask any questions that could lead to a deeper understanding of the dataset's limitations or potential improvements. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither suggests direct actions nor includes implicit actionable comments. So the overall score is -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The reviewer's comments are based on the information provided in the paper, and they do not make any subjective judgments about the quality of the paper or the abilities of the authors. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. This is in line with the score rubric, which defines a score of 0 as a neutral, factual, or passive tone. Therefore, the review point is appropriately scored as 0.",0,"The review point in question does not contain any claims or make any assertions. It merely presents a factual observation regarding the number under LM in the given dataset. This observation does not require any form of verification, as it is a straightforward comparison of two numerical values. The review point does not attempt to interpret the significance of the number or its implications for the dataset or the research. It also does not provide any logical deductions or references to external or internal sources that could support or refute the claim. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric. It neither contains a claim nor does it verify one. The review point is simply a statement of fact, which is not relevant to the aspect of verifiability. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The mention of ""TRANSFORMER[36]"" and the numbers under LM (2.5/5.9) do not provide any context or explanation that would help the authors understand what the review point is referring to. The lack of specificity in the review point makes it difficult for the authors to pinpoint which sections of the paper are being discussed. This ambiguity could lead to confusion and uncertainty among the authors, as they would not be able to confidently identify the parts of the paper that are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would likely be unsure about the paper part that is being addressed by the review point comment. So the score is -1."
1572,ICLR_2023_3811,"Weaknesses
Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term ""coherence"" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining ""coherence"" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words ""turn"" and ""utterance"" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: ""entailment"", ""contradiction"" and ""neutral"". I suggest specifying that the 1 label in the numerator corresponds to the ""entailment"" class, and whether you consider both ""contradiction"" and ""neutral"" as a single ""non-entailment"" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the ""instances for evaluation"" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the ""Utterance Level Sampling"" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it ""tends to sample previous utterance to be replaced"" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the ""Coherence Rate"" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results ""w/ Noise"" described on page 6, ""w/ Utterance"" and ""w/ Semi-Utterance"" described on page 4, but you also mention ""w/ Hierarchical"". Up to this point I had understood both ""Utterance Level Sampling"" and ""Semi-utterance Level Sampling"" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the ""w/ Hierarchical"" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating ""Rep""? I suggest clarifying this.
On page 7 in the ""Sampling vs w/o Sampling"" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the ""Human Evaluation"" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the ""Explicit Coherence Optimization"" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your ""hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking"". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you ""demonstrate these methods make chatbots more robust in real-world testing"" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term ""coherence"" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition ""Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts."" is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, ""is coherence"" should be ""that is coherent"".
P. 3, p. 4, l. 1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index ""1"" mean in U ^ 1 i − 1
? Does it mean ""starting from index 1""? If this is the case and you never use anything other than ""1"" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to ""conduct"" a classifier does not make much sense. You can either ""conduct"" classification or ""train"", ""use"", ""create"", etc. a classifier.
P. 4, p. 4, l. 4: here you say ""we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: ""Given a training pair U ^ 1 t − 1
"" should be ""Given a training pair U ^ 1 ′ t − 1
"", i.e. the training pair contains an utterance replaced through the ""Utterance Level Sampling"" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it.
P. 5, p. 3, l. 3: ""two annotators are employed"" -> ""two annotators were employed""
P. 5, p. 5, l. 3: The sentence starting with ""As model-based methods"" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: ""Following previous work (Ritter et al., 2011)"" -> ""Following the work by Ritter et al. (2011),""
P. 5, p. 8, l. 2: ""to online evaluate these two methods"" -> ""to evaluate these two methods online""
P. 6, p. 1, l. 2: ""non-repetitive"" -> ""non-repetitiveness""
P. 6, p. 6, l. 1: ""After sample an utterance"" -> ""After sampling an utterance""
P. 6, p. 8, l. 2: ""generate coherence response"" -> ""generate coherent responses""
P. 6, p. 8, l. 4: ""with the number of turns increases"" -> ""as the number of turns increases""
P. 7, Figure 4, a - b: The y-axis should be labeled ""coherence (%)"" instead of ""coherent (%)"". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence ""since sampled noises are difficult to accurately simulate errors of the inference scene during training"" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): ""Contradition"" -> ""Contradiction""; (b): ""Coherent"" -> ""Coherence"". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: ""hierarchy way"" -> ""hierarchical way""
P. 9, p. 2, l. 1: ""incoherence response"" -> ""incoherent response""
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.","1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow. P. 3, p. 4, l.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of certain elements in the paper, such as the absence of a related work section and the insufficient performance improvement compared to a known method. The review point does not suggest any specific improvements or changes that could be made to the paper. It also fails to provide any implicit actionable comments that the authors could infer from the review point. Instead, it simply states the limitations of the paper without offering any guidance on how to address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer's comments are focused on the content of the paper, without any personal attacks or overly positive remarks. The feedback is delivered in a manner that is neither encouraging nor discouraging, but rather a simple statement of fact. The reviewer's comments are also constructive, providing suggestions for improvement without being overly critical or overly complimentary. This approach is in line with the score rubric's criteria for a neutral tone. Therefore, the review point is well-aligned with the score of 0, as it is neither aggressive nor overly polite, but rather neutral and factual.",0,"The review point provided does not contain any claims or assertions that require verification. It merely suggests a minor change in the phrasing of a sentence to improve the flow of the text. This change does not affect the content or the verifiability of the claims made in the original text. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the verification or refutation of any claims. Hence, the score is 0.",1,"The review point provided is a clear and concise statement that directly addresses a specific aspect of the paper. It is evident that the authors can confidently identify which part of the paper is being addressed by the review point. The review point is specific and directly relates to the evaluation of a conventional practice, which is a central theme of the paper. The authors can be sure that the review point is focused on the evaluation of a specific method or approach, and they can confidently address this aspect in their response. The review point is well-structured and provides a clear direction for the authors to follow in their revision. Therefore, the review point meets the criteria for a score of 1, as it is specific and directly addresses a part of the paper."
5570,NIPS_2020_133,"(-) Some claims are way too strong. *“we study and prove the incapability of standard activation functions to extrapolate” is debatable. The proof is mainly related with ReLU and tanh. * After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.","* After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of evidence supporting the proposed activation function, without suggesting any specific actions or experiments that could be done to address this issue. The reviewer also fails to provide any implicit actionable comments that could be inferred from the review point. Instead, the reviewer simply states the limitations of the paper without offering any solutions or improvements. This lack of actionable content makes the review point ineffective in guiding the authors on how to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus receives a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any aggression or rudeness, and the tone is not overly critical or encouraging. The reviewer simply states the facts and observations from the paper, without adding any personal opinion or judgment. The reviewer also acknowledges the possibility of missing something, which shows a level of humility and openness to other perspectives. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the generalization of the proposed activation function, Snake activation, compared to existing methods like ReLU and its variants. However, the review point fails to provide any verification or justification for this claim. It merely states that the proposed method is not better than existing methods without offering any logical deductions or references to external or internal sources. The review point also questions why the proposed method was not used in the last two experiments, but again, it does not provide any evidence or reasoning to support this question. Therefore, the review point does not meet the criteria for verifiability as it lacks any form of justification or explanation. The claim made in the review point is not supported by logical deductions or references, and the review point does not contain any claims that do not need verifications. So the overall score is -1.",1,"The review point provided is quite specific, focusing on the comparison of the proposed Snake activation function with existing activation functions such as ReLU and Leaky ReLU. It directly addresses the claim made in the paper that the Snake activation function is a more general method. The review point also questions the omission of other activation functions in the last two experiments where Snake activation obtained the best results. This specificity in the review point allows the authors to identify which parts of the paper are being addressed, and they can be confident about the exact part of the paper that is being critiqued. The review point is well-aligned with the score rubric, as it clearly indicates which part of the paper is being addressed, and the authors can confidently identify the related sections. Therefore, the review point is well-structured and provides a clear and specific critique of the paper."
4522,NIPS_2020_988,"The introduced method relies on sparse distributions, which is a quite strong assumption. While the authors address the main implications of this assumption, I think there should have been an even more detailed discussion/empirical evaluation to increase the impact of this work in the community: - in the semi-supervised learning experiments in section 5.1 you use a VAE model which is relatively simple and by now quite outdated. Do you expect these results to generalize to more complex architectures? For example, if I took any of the SOTA semi-supervised deep generative models and just replaced the softmax with the sparsemax would you expect similar improvements? - how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data? - since sparsemax is such a core component of this method, it would be useful to add some details on its forward/backward passes and their computational complexity wrt the softmax.",- how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the potential behavior of the model with challenging tasks and ambiguous data points. The review point does not suggest any specific experiments or changes that could be made to the model or the training procedure. It also does not provide any implicit actionable comments that could be inferred from the review point. The authors are left without any clear direction on what needs to be done to improve the model's performance. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer asks questions about the method's behavior with challenging tasks and the sparsity assumption, which are relevant to the paper's content. However, the reviewer does not provide any constructive feedback or suggestions for improvement, which could have made the tone more helpful and encouraging. The reviewer's tone is passive, and the feedback is purely factual, without any attempt to engage with the authors or to provide guidance on how to improve the paper. Therefore, the review point does not meet the criteria for a higher score, as it lacks the helpful and encouraging tone that would be expected for a score of 1.",0,"The review point in question does not contain any claims, nor does it make any assertions that require verification. It is a hypothetical scenario that is not backed by any empirical evidence or logical deductions. The review point is more of a thought experiment, which does not align with the score rubric's requirement for claims that need to be verified. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that could be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims or factual statements that would require verification. 

The review point is focused on the behavior of a method with challenging tasks and ambiguous data points. It does not make any claims about the method's performance or its ability to handle ambiguity. Instead, it poses a question about how the method would behave in such scenarios. This question is not answered, nor does it provide any evidence or logical reasoning to support any claims. 

In conclusion, the review point does not contain any claims that need to be verified, nor does it make any assertions that could be verified. It is a hypothetical scenario that does not align with the score rubric's requirements for claims that need to be verified. Therefore, the review point does not meet the criteria for a score of 1 or -1, and it falls into the category of a score of 0.",1,"The review point provided is quite specific and directly addresses the behavior of the method with challenging tasks containing ambiguous data points. It asks whether the model would use numerous loss evaluations throughout the training procedure or if the sparsity assumption would lead the model to be certain even for ambiguous data. This question is directly related to the paper's methodology and the assumptions made about the data.

The authors can be confident that the review point is addressing a specific aspect of the paper, which is the method's behavior with ambiguous data. The review point is not vague and does not leave room for interpretation. It is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed.

The review point also provides a clear direction for the authors to consider, which is whether the model would use numerous loss evaluations or rely on the sparsity assumption. This direction is crucial for the authors to understand the implications of their methodology.

In conclusion, the review point is specific, directly related to the paper's methodology, and provides clear direction for the authors to consider. It is not vague and leaves no room for interpretation, which is why it would receive a score of 1 according to the score rubric."
141,ICLR_2022_1421,"Weakness:
For the Per-SSFL framework, the local (personalized) model and global model are used. The memory consumption aspect should be discussed. For resource-constrained edge clients, high memory cost could be an issue.
Although it mentioned in the implementation setting that the client number selected per round is 10, it is not clear how many total clients are used in the FL setting.
In Table 1, what is the FL method under the supervised setting?
In Figure 2, what does (0.5) for SSFL on IID and SSFL on non-IID mean?
As can be seen in Figure 2, the convergence rates for the IID and non-IID cases are quite similar. Can you provide an explanation for that?
After reading Appendix D and Figure 10 in Appendix, the experimental setting on GLD-23K is still not quite clear. For example, how the local training set for each client is generated? What about the label distribution? Also, it seems that the number of clients used on the GLD-23K is different from that on CIFAR-10.
For ease of comparison and implementation, it would be good to evaluate the method on more commonly used datasets such as CIFAR-100 and Tiny-ImageNet and other datasets besides vision datasets (e.g., text) for FL.
It would be interesting to see the SSFL results under different numbers of selected participant clients. Since λ
is an important parameter that balances consensus and personalization, its effect should be studied.
Minor issues:
Section 2.1, “… the local empirical risk over the heterogeneous dataset D k
.” -> D k
Figure 10 appeared in Sec. 5.2 and 5.3, it should be Fig. 2 and Fig. 3.
In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent.
Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)”
A careful proofread of the paper is highly recommended.","3. In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent. Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)” A careful proofread of the paper is highly recommended.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out inconsistencies in the paper, such as the difference in color between the curve and its legend, and the discrepancy in the list of contemporary self-supervised learning frameworks. While these are valid points, they do not provide any guidance on how to rectify the issues or improve the paper. The review point also lacks any implicit actionable comments that the authors could infer from the feedback. It simply states what needs to be done without offering any insight into how to achieve these improvements. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither provides explicit actionable comments nor does it offer implicit actionable comments that could be inferred by the authors. So the overall score is 0.",0,"The review point provided is neutral and factual, which aligns with the score of 0. It does not contain any aggressive or disrespectful language, nor does it offer any encouragement or positive reinforcement. The review point simply points out inconsistencies and inaccuracies in the paper, which is a neutral approach to reviewing. The tone is not overly critical or overly positive, but rather a straightforward assessment of the paper's content. The reviewer does not express any personal opinions or judgments, which is a characteristic of a neutral tone. The feedback is focused on the content of the paper and does not include any personal attacks or disrespectful remarks. Therefore, the review point is in line with the score rubric's criteria for a neutral tone.",-1,"The review point provided contains a claim regarding the consistency of colors in a figure and a claim regarding the terminology used in the paper. However, the review point does not provide any logical deductions or references to support these claims. The claim about the colors in the figure is not justified, and the claim about the terminology is not backed by any external or internal sources. Therefore, the review point does not meet the criteria for verifiability as it lacks justification, explanation, or reference. The review point also fails to provide any factual statements or claims that do not need verifications. As a result, the review point does not fulfill the requirements of the score rubric. So the overall score is -1.",1,"The review point provided is specific and directly addresses the inconsistency in the color scheme of the curve and its legend in Figure 2(a). It also identifies a specific error in the text, which is the mention of ""contemporary self-supervised learning frameworks"" instead of ""contemporary self-supervised learning frameworks (e.g., SimCLR, SwAV, BYOL)"". This specificity allows the authors to clearly understand which parts of the paper are being addressed by the review point. The authors can confidently identify the inconsistency in the color scheme and the specific error in the text, and they can take action to correct these issues. Therefore, the review point meets the criteria for a score of 1, as it makes it obvious which part of the paper is being addressed and the authors can be confident about it."
157,ICLR_2022_939,"Weaknesses
Novelty is somewhat low.
Discussion of gradient explosion seems inaccurate.
Some discussion of related work is missing, particularly about related works in the model-based RL literature.
Sometimes they are stating things as facts without providing evidence.
There was no detailed discussion of how the results with the current simulator relate to other recent simulators such as Brax. Are the wallclock times similar? The results seem mixed, but having a detailed discussion would have been useful.
The simulator is fast, so it may have been better to do more than 4 environments. Recommendation
I recommend accepting the paper as I have not seen differentiable simulators used for tasks of the difficulty considered here (while also taking advantage of differentiating through the simulator). I was considering a score of 6 or 8, but gave 6 for now.
Discussion of points brought up
Novelty: It seems the contribution is primarily one of engineering, and they don't propose any surprising new idea. The idea of truncated backpropagation is old. Moreover, the policy training scheme resembles that of the the Dreamer algorithm (Hafner et al. 2019). Dreamer also uses short horizon rollouts together with a terminal value function, and backpropagates through these short horizons to optimize the policy. The differences are only: Dreamer uses lambda return weighting during the short horizons (why didn't you use this?), and the method of constructing the sequences is different (Dreamer samples start states from a replay buffer and performs model rollouts from these states, while the current work splits an episode into chunks). I am surprised that Dreamer was not discussed when explaining the methodology. Certainly it should be mentioned that there are prior works using a similar policy optimization procedure (with slight variations).
Discussion of gradient explosion: There are other earlier works that do a more detailed job of discussing the gradient and landscape issues, such as PIPPS (Parmas et al. 2018), which should have been cited and discussed (moreover your methodology was very similar to these previous works). In your work, if gamma were 1, the value function were perfect and the policy were deterministic, the gradient you compute with your method should be exactly the same as the gradient that is computed using BPTT. From this point of view, your discussion is insufficient, as you do not explain why the loss landscape and gradients end up being smooth despite this fact. I can think of two possible reasons: 1. The value function is an approximator that ends up being smooth because of limited capacity to model the complicated landscape. 2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance.
For the other points see the raw notes at the bottom of this section.
Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions
Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?
Q2. In Figure 2, did you use a deterministic or stochastic policy? Was this the same policy as was used during training the value function? If the policy was stochastic, then how did you evaluate the landscape? This would require sampling many trajectories with the same policy and averaging. Are the scales on the left and right figures the same?
Q3. Did you do any ablation study of the policy noise? Does the method still work when the policy is deterministic? How much does the performance drop?
Additional notes made during reviewing
""for systems ranging from robots (e.g., Cheetah, Shadow Hand) to complex anima- tion characters (e.g., muscle-actuated humanoids) using only high-level reward definitions."" Please provide references.
""A differentiable simulator provides accurate first-order gradients of the task performance reward with respect to the control inputs."" This is speculation. You provide no evidence. Problems with accuracy can arise when the task performance depends on a sampled initial state (so that the gradients are inherently stochastic). Perhaps change to ""may provide"".
""However, despite the availability of differentiable simulators, it has not yet been convincingly demonstrated that they can effectively accelerate policy learning in complex high-dementional and contact-rich tasks, such as some traditional RL benchmarks."" While this may be the case for differentiable simulators, there are several model-based RL works that showed effective learning (e.g., Dreamer. The difference with a simulator is just that the model does not have to be learned. I think the claim here is downplaying such previous contributions. Also, ""dementional"" should be ""dimensional"".
""There are several reasons for this: 1.(), 2.(), 3.()"" These reasons are stated as facts, while they are speculations. Perhaps, ""possible reasons"" would be better. At least points 1 and 3 should be the same for model-free RL, so are they really the reason? No references were provided.
""Because of these challenges, previous work has been limited..."" How do you know that those were the challenges that limited the applicability of the previous methods? The publications themselves do not seem to note your reasons as the reason why they limited their experiments. For example, the PODS paper says their method overcomes the issues of exploding gradients.
""In addition, we propose a truncated learning window to shorten the backpropagation path to address problems with vanishing/exploding gradients and reduce mem- ory requirements."" This is known as ""truncated backpropagation"". It is plagiarism to claim that you ""proposed"" this.
In Equation 1, please provide the definition of the Jacobian's and gradients. Usually, the gradient is a row vector, whereas you are using a column vector.
""This makes the reward function smoother..."" What do you mean by this? The reward function is the same in all cases.
""In addition, we apply state normalization as is common in RL algorithms."" Please explain what ""state normalization"" is.
""First, the terminal value function absorbs the discontinuity of long dynamics horizons and early termination into a smooth function, as shown in Figure 2 (Right)."" This explanation is incomplete. If you had no discount factor, a perfect value model and a deterministic policy, your computed policy gradient would be exactly the same as that of BPTT.
""Finally, the use of short horizons allows us to update the actor more frequently, which, when combined with parallel differentiable simulation, results in a significant speed up of training time."" Do you have an ablation study showing that it speeds up?
""In contrast, our method scales well due to direct access to the true gradients from differentiable simulation."" You don't have access to true gradients when you are using a stochastic policy. It may be better to explain this by referring to the fact that reparamterization gradients are often more accurate when computing gradients of smooth functions.","2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance. For the other points see the raw notes at the bottom of this section. Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out a discrepancy in the description of the simulator used, without suggesting any specific actions or experiments that could be taken to resolve the issue. The review point also fails to provide any implicit actionable comments that could be inferred from the text. It simply states the problem without offering any solutions or further investigation. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and it does not provide any guidance for the authors to improve their work. So the overall score is -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does it provide any form of encouragement or helpfulness. The reviewer simply asks a question regarding the simulator used in the paper, which is a factual inquiry. The tone is neither rude nor encouraging, but rather neutral. The reviewer does not make any negative or positive comments about the paper, which is why it fits the score of 0. The reviewer's question is straightforward and does not contain any form of politeness or lack thereof. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point in question contains a claim regarding the use of Isaac Gym in the simulator. However, it fails to provide a clear and definitive answer to the question posed. The response is ambiguous and does not offer a logical deduction or reference to external or internal sources that could verify the claim. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The response does not provide a clear and definitive answer to the question posed, which is a critical aspect of the score rubric. Therefore, the review point is not verifiable and does not meet the criteria for a score of 1.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the use of Isaac Gym and the creation of a simulator, but it does not provide a clear distinction between the two. This ambiguity could lead to confusion for the authors, as they may not be sure which part of the paper the review point is referring to. Additionally, the review point does not address the specific points raised by Parmas et al. (2018), which were about the smoothness of the landscape and the potential for ill-behaved gradients. This lack of specificity in the review point does not meet the criteria for a score of 1, as it does not allow the authors to confidently identify the part of the paper being addressed. Therefore, the review point is not effective in guiding the authors to improve their paper."
339,ICLR_2022_2842,"Weaknesses: 1. In introduction section, author claimed ""Representation Topology Divergence (RTD) score which measures a dissimilarity between two point clouds of equal size with one-to-one correspondence between points"". The question is how to explain ""one-to-one correspondence between points"", what does ""one-to-one"" mean here? It seems this is an important part of contribution, but there is no specific explanation about this so it is confusing me.
The writing is not clear. For example, some concepts in section 2 are not well explained, such as Barcode, Vietoris Rips complex and homology group are not be familiar with ML community. It should be better to add some figures and intuitive explanation about these abstract topology terms.
3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score?
It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?","3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score? It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely states the need for theoretical or topological explanations for the sensitive RTD score, without suggesting any specific experiments or actions that could be taken to address this need. The review point also questions the applicability of the proposed RTD to network representation, but it does not offer any suggestions or recommendations on how to clarify or validate this aspect. Therefore, the review point does not fulfill the criteria of providing actionable comments or steps that the authors could follow. It lacks the explicitness and directness required to be considered actionable. So the overall score is -1.",0,"The review point is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the author. The reviewer's questions are focused on the technical aspects of the paper, such as the sensitivity of the RTD score and the applicability of the proposed method to network representation. However, the reviewer does not provide any constructive feedback or suggestions for improvement, which could have made the tone more helpful and encouraging. The reviewer's questions are also somewhat passive, which does not contribute to a more active and engaging dialogue. So the overall score is 0.",-1,"The review point in question makes a claim about the sensitivity of the RTD score to cluster, but it fails to provide any verification or justification for this claim. The claim is made without any supporting logical deductions or references to external or internal sources. This lack of verification and justification is a clear deviation from the score rubric, which requires claims to be supported by logical explanations or references.

Furthermore, the review point also makes a claim about the specific application of the RTD score to network representation, but again, it does not provide any explanation or justification for why this is the case. The claim is made without any supporting logical deductions or references, which is a significant shortcoming in terms of verifiability.

In conclusion, the review point contains claims that are not verified, and it misses the opportunity to provide logical explanations or references to support these claims. This lack of verification and justification is why the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Therefore, the review point is not aligned with the score rubric's requirements for verifiability.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the sensitivity of the RTD score to clusters and the need for theoretical or topological explanations, but it does not specify which sections of the paper are related to these aspects. The authors are left with uncertainty about which parts of the paper are being addressed by the review point. The lack of specificity in the review point does not allow the authors to confidently identify the relevant sections of the paper. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

The review point also fails to provide a clear connection between the RTD score and its application to network representation. It does not explain why the RTD score is specifically sensitive to network representation, leaving the authors without a clear understanding of the paper's focus. This lack of specificity further contributes to the uncertainty surrounding the review point. 

In conclusion, the review point is not specific enough to clearly identify which part of the paper is being addressed, and it does not provide a clear connection between the RTD score and its application to network representation. This lack of specificity and clarity in the review point does not meet the criteria for specificity as outlined in the score rubric. 

So the overall score is -1."
698,ICLR_2021_1682,"weaknesses
+ The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.
+ The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.
+ The paper is well-written, easy to follow, and well-connected to the existing literature.
- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.
- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.
- A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments. Recommendation
I’m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that’s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.
Detailed justification
The biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I’m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes while keeping other hyperparameters fixed, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that ""NCA performs better than all PN configurations, no matter the batch size"" with a grain of salt, for instance.
I also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission’s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don’t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.
The connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to ""show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis"", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings – albeit with a Euclidean metric rather than a cosine similarity metric – since both perform comparisons on example pairs.
This relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between ""1-NN with class centroids"" and k-NN / Soft Assignment noted in Section 4.1 – as well as the drop in performance observed in Figure 4’s row 6 – be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?
Finally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don’t know how competing approaches would perform if we applied the paper’s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA’s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant. Questions
In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?
Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?
Can the authors elaborate on the ""no S/Q"" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn’t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class’ prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.
Additional feedback
This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven’t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that ""in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]""; can the authors point to the implementations they have in mind when making that assertion?
I would be careful with the assertion that ""during evaluation the triplet {w, n, m} [...] must stay unchanged across methods"". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.
I’m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.","- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the paper without suggesting any specific actions that the authors should take to improve their work. The review point does not give any hint or direct actionable comments that the authors can deduce what needs to be done. It lacks the necessary detail and guidance that would help the authors to make informed decisions about their research. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",-1,"The review point provided does not exhibit the level of politeness expected in a constructive review. The tone is more critical and lacks the supportive and encouraging language that would be expected in a helpful review. The reviewer's comments are direct and somewhat harsh, which could be perceived as disrespectful or aggressive. For example, phrases like ""This introduction is terrible and makes no sense"" and ""This poor quality of experiments and writing doesn’t live up to the level of this conference"" are not only unhelpful but also potentially damaging to the authors' confidence and the reputation of the conference.

In contrast, a review point that is polite and constructive would provide feedback in a way that is respectful and encouraging. It would offer suggestions for improvement and highlight the strengths of the paper while also pointing out areas that could be enhanced. The reviewer would use language that is supportive and positive, which would help the authors to feel valued and motivated to improve their work.

The review point does not meet the criteria for politeness as outlined in the score rubric. It lacks the helpful and encouraging tone that is necessary for a constructive review. Therefore, based on the score rubric, the review point is not helpful and does not encourage the authors to improve their work. 

So the overall score is -1.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the observations presented may not generalize to few-shot learners beyond Prototypical Networks, which is a factual statement that does not require any form of verification. The review point does not make any claims that could be supported by logical deductions or external or internal sources. It also does not make any claims that could be invalidated or supported by further evidence or analysis. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the understanding of the properties of episodic training in the context of few-shot learners. So the overall score is 0.",-1,"The review point provided does not clearly specify which part of the paper is being addressed. It lacks the specificity required to guide the authors in understanding the review point's focus. The review point does not mention any particular section or idea from the paper, which could lead to confusion among the authors. The absence of a clear connection to the paper's content makes it difficult for the authors to identify which parts of the paper are related to the review point. This lack of specificity is a significant drawback, as it does not provide the authors with a clear direction for improvement or clarification. Therefore, the review point does not meet the criteria for specificity, as it does not make it obvious which part of the paper is being addressed. 

In contrast, a review point that is specific would clearly identify which part of the paper is being addressed, such as a particular section or idea. This would allow the authors to confidently identify the areas of the paper that need attention. The review point would also provide a clear direction for the authors to follow in order to address the concerns raised in the review point. 

The lack of specificity in the review point is a critical issue that limits the scope of the submission's contributions. It does not help the authors understand the properties of episodic training, which is a key aspect of the paper. The absence of a clear connection to the paper's content makes it difficult for the authors to improve their work based on the review point's feedback. 

In conclusion, the review point does not meet the criteria for specificity, as it does not make it obvious which part of the paper is being addressed. This lack of specificity is a significant drawback that limits the scope of the submission's contributions."
873,ICLR_2023_507,"Weaknesses
I encourage the authors to put forward the general notion of sparsity that is assumed across the paper (as defined in S ( w )
) early on in the introduction so that reader can follow the ideas put forward in Fig. 1.
One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent.
From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal)
By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?","1. One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent. From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal) By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided is actionable to a certain extent, but it lacks the explicitness that would make it a clear directive for the authors. The suggestion to expand the dataset-DNN pairs is a good one, but it does not specify which datasets or DNNs should be included, nor does it provide a clear rationale for why this would be beneficial. The question about the sparsity limit is a good one, but it is posed as a question rather than a directive, which may leave the authors unsure about how to proceed. The review point does not provide any specific actions that the authors can take, nor does it suggest any particular experiments that could be conducted. Instead, it leaves the authors to infer what actions might be appropriate. This lack of explicit actionable comments is why the review point would receive a score of 0 according to the score rubric. 

The review point does, however, include implicit actionable comments, such as the suggestion to discuss the absolute limit of sparsity and the difference in performance values at the extreme end of sparsity. These comments are not as direct as they could be, but they do provide a general direction for the authors to consider. 

In conclusion, while the review point does contain some implicit actionable comments, it does not provide the explicit, directive that would be necessary for the authors to take specific actions. Therefore, the review point would receive a score of 0 according to the score rubric.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer encourages the authors to expand their dataset-DNN pairs and discuss the sparsity limit of various networks, which is a helpful and encouraging approach. The tone is respectful and the feedback is focused on improving the research, rather than criticizing it. The reviewer also asks for more information on the performance values and PQI at the extreme end of sparsity, which is a constructive way to engage with the authors' work. The feedback is delivered in a manner that is likely to be well-received by the authors, as it provides clear guidance on how to improve their research. So the overall score is 1.",0,"The review point provided does not contain any claims, nor does it require verification. It is a general suggestion for the authors to consider a wider range of datasets and to explore the limits of sparsity in their models. The review point does not make any specific claims about the existing results or the potential performance of the models at the extreme end of sparsity. It is a factual statement that does not need verification. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain a claim that is verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the use of certain datasets in the paper, which is a key aspect of the paper's evaluation. It also asks for an exploration of the sparsity limit of various networks, which is a central theme in the paper. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. It also encourages the authors to provide more detailed information about the performance of their model at the extreme end of sparsity, which is a critical aspect of the paper's results. The review point is well-structured and directly engages with the paper's content, which is why it would receive a score of 1 according to the score rubric. 

The review point is not vague or ambiguous, and it does not leave the authors guessing about which parts of the paper are being addressed. Instead, it provides a clear direction for the authors to follow in order to improve their paper. The review point is also well-aligned with the paper's theme and the specific issues that the authors should address. This makes it easy for the authors to understand and respond to the review point. 

In conclusion, the review point is specific, clear, and directly addresses the key aspects of the paper, which is why it would receive a score of 1 according to the score rubric."
2056,ARR_2022_18_review,"1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7. The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1, and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? 
See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda = 1.5 nA -> na (short vowel) ",2. No details on the. multi-task learning mentioned in Section 4.4 are available.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that there are no details on the multi-task learning mentioned in Section 4.4, without suggesting any specific actions or improvements that could be made. The review point does not include any actionable comments that can be implicitly inferred, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided lacks the necessary politeness and constructive tone that is expected in a review. The reviewer's tone is rather blunt and does not offer any constructive feedback or suggestions for improvement. The reviewer's comments are direct and do not provide any encouragement or positive reinforcement. This approach is not conducive to a constructive review process, as it does not foster a positive and supportive environment for the authors. The reviewer's comments are also quite negative, which could potentially discourage the authors from making any improvements. Therefore, based on the score rubric, the tone of the review point is not helpful, encouraging, or constructive, and it does not meet the criteria for a score of 1. So the overall score is 0.",0,"The review point in question does not provide any verifiable claims. It merely states that there are no details on the multi-task learning mentioned in Section 4.4. This lack of detail does not constitute a claim that can be verified or refuted. The review point does not make any assertions that could be supported by logical deductions or references to external or internal sources. It also does not provide any justification or explanation for the claim it does not make. Therefore, according to the score rubric, the review point does not contain any claims, or only includes factual statements and claims that do not need verifications. This is why the review point does not meet the criteria for a score of 1, which requires a claim to be verified. So the overall score is 0.",-1,"The review point in question is vague and does not provide specific details about the multi-task learning mentioned in Section 4.4. It fails to identify which parts of the paper are related to the review point, leaving the authors unsure about the exact aspect of the paper that is being addressed. The lack of specificity in the review point does not allow the authors to confidently identify the relevant sections of the paper. This lack of clarity and specificity in the review point is why it would receive a score of -1 according to the score rubric. 

The authors would not be able to pinpoint the exact sections of the paper that need to be addressed or improved based on the review point. The absence of specific details and the lack of a clear direction for the authors to follow makes it difficult for them to understand the review point's implications for their work. 

In contrast, a review point that provides specific details and clear guidance would allow the authors to confidently identify the relevant sections of the paper and understand the review point's implications for their work. This is why a review point that is specific and clear would receive a higher score according to the score rubric. 

Therefore, based on the score rubric, the review point in question is not effective in addressing the specificity of the paper's content. It does not provide the necessary details to guide the authors in improving their work. 

So the overall score is -1."
1900,ARR_2022_334_review,"- There are some technical aspects of the paper that weren't clear to me:  * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
 * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
   * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. 
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) ",- There are some technical aspects of the paper that weren't clear to me:,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out some technical aspects that were not clear to the reviewer. The reviewer does not suggest any specific actions or improvements that the authors should make. Instead, the reviewer leaves the authors to deduce what needs to be done based on the vague comments provided. This lack of actionable comments does not meet the criteria for a score of 1, as it does not give any hint or direct actionable comments. Therefore, the review point does not provide the authors with clear guidance on how to improve their paper. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they provide any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating their observations and questions about the paper. The reviewer does not make any personal judgments or criticisms, which is why the tone is neutral. The reviewer's comments are focused on the technical aspects of the paper, which is appropriate for a review. However, the reviewer could have been more constructive by offering suggestions or pointing out potential improvements. This would have made the review point more helpful and encouraging, which would have resulted in a higher score. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that there are some technical aspects of the paper that were not clear to the reviewer. This statement does not make any claims that could be verified or not, as it does not provide any specific information or observations that could be substantiated or refuted. The reviewer does not attempt to verify any claims, nor do they provide any logical deductions or references to external or internal sources that could support or challenge their observations. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it does not contain any claims that need to be verified. So the overall score is 0.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which part of the paper is being addressed. The reviewer's statement about technical aspects not being clear is too general and does not provide any concrete examples or suggestions for improvement. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. The authors are left with no idea about the paper part that is being addressed by the review point comment. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and clarity. 

In contrast, a review point that is specific and clear would directly address a particular section or aspect of the paper, providing a clear direction for the authors to follow. It would also give the authors a sense of confidence about which part of the paper is being addressed. The examples provided in the score rubric demonstrate this level of specificity and clarity. 

In conclusion, the review point provided is not specific enough to guide the authors in addressing the issues raised. It fails to meet the criteria for specificity and clarity, as outlined in the score rubric."
2242,ACL_2017_699_review.json,"Weaknesses: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1. 
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally,    The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc. 
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845). ","4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the current system and the missing topical details of the dataset. The review point does not suggest any specific actions that the authors should take to address these issues. It also does not provide any implicit actionable comments that the authors can infer from the review point. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric. 

The review point only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionable content makes it difficult for the authors to deduce what needs to be done to improve the current system. 

In conclusion, the review point does not provide any actionable comments or suggestions, which is why it receives a score of -1 according to the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or disrespectful, nor is it overly encouraging or helpful. The reviewer simply states the facts about the training data and the lack of topical details in the dataset. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. The reviewer also does not express any personal opinions or judgments, which further contributes to the neutrality of the tone. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",-1,"The review point raises a question about the use of all publications in training the baselines, and the lack of topical details in the dataset. However, it does not provide any verification or justification for these claims. The review point merely states the facts without offering any logical deductions or references to external or internal sources that could support the claims. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The review point also fails to address the potential impact of these issues on the chances of repeating results, which is a critical aspect of the review point. Therefore, based on the score rubric, the review point does not contain any claims that are verified, and it does not include any factual statements or claims that do not need verifications. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the training process of the model. It points out the discrepancy in the number of documents used for training and the number of publications held out for training baselines. This specificity allows the authors to identify the exact part of the paper that is being addressed, which is the training process. The review point also highlights the lack of topical details in the dataset, which could affect the reproducibility of the results. This specificity in the review point helps the authors to understand the implications of the missing details and how it could impact their work. Therefore, the authors can be confident about which part of the paper is being addressed by the review point. 

In contrast, the review point could have been more general and spread across many paper sections, which would have made it less specific. This would have led to a lower score as it would have been more difficult for the authors to identify the exact part of the paper that is being addressed. 

The review point's specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point. 

So the overall score is 1."
4173,NIPS_2020_1855,"Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as ""undecided"", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting. - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the ""raw"" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening? - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have ""training the neural representation"" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization? - More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.","- More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely comments on the superfluous nature of the ""Algorithm 1"" box and the obviousness of its content. The review point does not suggest any changes or improvements that could be made to the paper. It also does not provide any implicit actionable comments that the authors could infer from the review point. The review point only mentions the superfluous nature of the box without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the quality of the paper and provides a suggestion that could enhance the paper, indicating a helpful and encouraging tone. The reviewer's comment about the ""Algorithm 1"" box being superfluous is delivered in a way that is not aggressive or disrespectful, but rather a neutral observation. The reviewer's suggestion that the space could be used for extending the paper is a constructive way to provide feedback. The reviewer's tone is consistently polite and constructive throughout the review point, which is why it is scored as a 1.",0,"The review point provided does not contain any claims that need to be verified. It merely comments on the presence of the ""Algorithm 1"" box in the paper, stating that it is superfluous and that the content is obvious. This is a factual statement that does not require any form of verification. The review point does not make any claims about the content of the paper or the validity of the methods used. It also does not reference any external or internal sources to support its statements. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point provided does not clearly identify which part of the paper is being addressed. The comment about the ""Algorithm 1"" box being superfluous is vague and does not offer any specific feedback on the content of the box or how it relates to the paper as a whole. The reviewer's statement that the content of the box is ""entirely obvious"" does not provide any constructive criticism or guidance for improvement. Furthermore, the reviewer's suggestion that the box ""can't hurt also"" if there is space, does not contribute to a clear understanding of the paper's content or the reviewer's concerns.

In contrast, a review point that would score a 1 would provide specific feedback on a particular section of the paper, such as a hypothesis presented in section 5.2.1 or the rationale for dividing the sequence of intermediate results into 11 sets. This would allow the authors to identify exactly which parts of the paper are being addressed and to make informed decisions about how to address the reviewer's concerns.

The lack of specificity in the review point makes it difficult for the authors to identify which parts of the paper are being addressed and to make improvements based on the feedback provided. Therefore, the review point does not meet the criteria for providing clear and specific feedback on the paper. 

So the overall score is -1."
1587,ICLR_2023_2147,"Weakness
While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it.
The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained.
I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.).
While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean).
Question to authors
During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error.
I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample m
has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level ( ϵ m , k
) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable.
[1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188.
[2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594.
Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?","2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188. [2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It lacks any specific guidance or suggestions that the authors could follow to improve their work. The review point merely points out some limitations of the paper without providing any actionable comments that could be inferred. It does not suggest any changes or improvements that could be made to the paper, nor does it provide any insights into how the authors could address the issues raised. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone of the review is not aggressive or rude, nor is it overly encouraging or helpful. The reviewer's comments are focused on the content of the paper, providing a critique of the methodology and the structure of the paper. The reviewer's language is straightforward and does not contain any personal attacks or disrespectful remarks. The feedback is constructive, offering suggestions for improvement, but it lacks the warmth and encouragement that would be present in a review with a score of 1. The reviewer's tone is more passive, simply stating the facts and offering a neutral perspective. Therefore, based on the rubric, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to support it. The reviewer simply states that the paper does not address the full range of non-stationary textures, which is a factual observation rather than a claim that needs to be verified. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that need to be verified. So the overall score is 0.",1,"The review point provided for the paper ""Probabilistic Modeling for Human Mesh Recovery"" is quite specific and directly addresses a particular aspect of the paper. It points out the lack of explanation for the division of the sequence of intermediate results into 11 sets, which is a critical part of the method described in the paper. This specificity allows the authors to identify exactly which part of the paper is being addressed by the review point. The review point also provides a clear direction for the authors to improve their paper by suggesting that they should provide a rationale for this division. This level of specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. Therefore, based on the score rubric, the review point is well-crafted and effectively communicates the specific aspects of the paper that need attention."
3293,NIPS_2020_1480,"1. The novelty of Theorem 1 is not entirely clear. The authors argue that the classical another parametrization B, Theta = (C Omega^{1/2}, Omega^{1/2}), which also enables joint convexity. Therefore, Theorem 1 doesn't appear to be a significant result in itself. This may be a lemma instead. 2. It is not clear why the classical parametrization does not allow penalty functions. One can include penalties on C by using the classical parametrization and levering C=B Theta^{-1}, e.g., || B Theta^{-1}||_{1,2} where 1,2 is the group L1 penalty. 3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage. 4. This work needs a more detailed comparative analysis in order to prove the superiority of the proposed approach. In particular, paper is lacking an adequate computational complexity and run-time analysis with respect to the existing methods, e.g. the standard parametrization and penalty approach. A similar weakness also exists in the numerical results section, where comprehensive empirical comparisons are lacking. The supplementary material has a numerical table, which shows a very incremental improvement and is not conclusive. 5. In proving Theorem 3, the authors employ standard methods such as restricted eigenvalues. These methods are already known to extend to non-convex objective functions and does not specifically hold for the claimed convex formulation.","3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments that the authors should make. It merely points out the inconsistency in the authors' argument regarding the advantage of convexity and the introduction of non-convex regularizers. The review point also lacks any implicit actionable comments that the authors can infer. It only mentions the limitations of the authors' work without suggesting any specific actions that should be taken to address these limitations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",-1,"The review point provided is not aligned with the score rubric's criteria for politeness. The tone of the review point is critical and somewhat aggressive, which is not conducive to a constructive review. The reviewer's language, such as ""destroys the purpose"" and ""it is very unclear,"" is confrontational and does not foster a positive dialogue. Furthermore, the reviewer's assertion that ""without any empirical comparison with the standard parametrization, it is hard to claim an advantage"" is dismissive and lacks the necessary politeness expected in a review. The reviewer's tone is not helpful or encouraging, and it does not provide any constructive feedback to the authors. Instead, it comes across as disrespectful and aggressive. Therefore, based on the score rubric's criteria for politeness, the review point is not suitable for a constructive review.",-1,"The review point in question makes a claim about the inconsistency in the authors' argument regarding the advantage of convexity and the introduction of non-convex regularizers. However, it fails to provide a logical explanation or any external or internal sources to support this claim. The reviewer merely states that the resulting overall problem is non-convex and questions the purpose of the convexity argument without offering any justification or explanation. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The reviewer's opinion is presented as a fact without any supporting evidence or logical reasoning. Therefore, the review point does not meet the criteria for a score of 1, as it contains a claim that is not verified.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer mentions the introduction of non-convex regularizers and the resulting non-convex problem, but fails to specify which sections of the paper are related to this issue. The reviewer also questions the advantage of convexity without providing any concrete examples or evidence from the paper. This lack of specificity makes it difficult for the authors to understand which parts of the paper are being addressed by the review point. The reviewer's comments are vague and do not provide clear guidance on how to improve the paper. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

In contrast, a review point that meets the criteria for specificity would clearly identify which part of the paper is being addressed and provide specific examples or evidence from the paper to support the reviewer's comments. The reviewer would also be able to guide the authors on how to improve the paper based on the identified issues. 

So the overall score is -1."
1870,ARR_2022_14_review,"-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here. - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. 
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. ","- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of certain details in the paper, such as the lambda parameters values and the selection process for the oracle snippets. However, it does not suggest any specific actions that the authors could take to address these issues. The review point also fails to provide any implicit actionable comments that the authors could infer from the review. Instead, it simply states that the paper could be improved without offering any concrete guidance on how to do so. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither provides explicit actionable comments nor does it offer implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or rude, nor is it overly encouraging or helpful. The reviewer simply states the lack of important experimental details and suggests that these could be improved. The reviewer also points out that the authors' claim in the introduction could be strengthened by providing a reference. However, the reviewer does not offer any specific suggestions or constructive criticism that would help the authors improve their work. The feedback is purely factual and does not engage with the authors' claims or the content of the paper in a way that would be considered polite or encouraging. Therefore, the review point is neutral and does not fit well with the other two scores.",-1,"The review point provided in the task contains a claim that the extract-then-generate approach mimics human summarization. However, it fails to verify this claim with logical deductions or external or internal sources. The claim is made without any supporting evidence or justification, which is a critical aspect of the score rubric. The review point does not provide any logical explanation or reference to support the claim, which is a key requirement for a score of 1. Therefore, based on the score rubric, the review point does not meet the criteria for verifying claims and thus would receive a score of -1.",1,"The review point provided is quite specific, addressing the lack of experimental details in the paper. It points out the absence of lambda parameters values, the selection process of oracle snippets/sentences, and other important details. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can identify the exact sections of the paper that need improvement based on the review point.

The review point also provides a clear direction for the authors to follow, which is to include these missing details in their paper. This guidance is crucial for the authors to understand what changes need to be made. The review point's focus on specific aspects of the paper ensures that the authors are not left in doubt about what needs to be improved.

In contrast, a review point that lacks specificity would not provide the authors with a clear understanding of what parts of the paper are being addressed. The authors would be left guessing which parts of the paper are related to the review point, which is not helpful for them to make the necessary improvements.

Therefore, based on the score rubric, the review point is effective in addressing the specificity of the paper. It provides a clear and specific direction for the authors to follow, ensuring that they are confident about which parts of the paper are being addressed by the review point."
2144,ARR_2022_61_review,"1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. 
1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. ","2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out the significance of testing and the need for more data, without suggesting any specific actions or methods to be used. The review point also lacks any implicit actionable comments that could be inferred from the text. It does not provide any guidance on how to improve the method or what specific limitations need to be addressed. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer's comments are focused on the content of the paper, asking questions and providing suggestions for improvement. However, the reviewer could have been more polite by using softer language and providing more constructive feedback. For example, instead of stating ""The introduction could be strengthened by providing a clearer context for your research question,"" the reviewer could have said ""It might be helpful to start with a brief overview of the existing literature to highlight the gap your study aims to fill."" This would have been more encouraging and less directive. So the overall score is 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a factual statement that merely describes the process of comparing automatic metrics and human evaluation. The review point does not make any claims about the results or the methodology, nor does it provide any justification or reference to support any claims. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. The review point is neutral and does not contribute to the validation or refutation of any claims. It is a straightforward description of the process, which is not subject to verification. So the overall score is 0.",1,"The review point in question is focused on the significance testing of the results on automatic metrics and human evaluation. It is clear that the review point is addressing a specific aspect of the paper, which is the validation of the proposed method. The review point is well-defined and directly relates to the method's metrics and the comparison with the baseline or gold standard. This specificity allows the authors to identify which parts of the paper are related to the review point, and they can be confident about the part of the paper that is being addressed. The review point is not ambiguous and does not leave room for uncertainty or confusion. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
1678,ICLR_2023_1654,"Weaknesses:
I believe the paper has a couple of fundamental weaknesses as it stands that should be addressed before it could be accepted at ICLR. 1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons. 2. The use of he Oracle twice is circular - therefore invalidates the claims of performance; there are some ways around this that the authors could try More minor weaknesses: 3. the reference used to justify the evaluation metrics is Hoffman et al 2022 - but this paper is about optimising small molecules - which are v different in ""seq distance to function"" relationships - this is especially important in relation to the point about the Novelty measure above.
4. AlphaFold is not at all appropriate to support the claim of functional sequence optimisation - there are may mutations that will cause a protein to unfold that Alpha fold will predict as having almost exactly the same structure as it will align etc - therefore it proves nothing ( From their own FAQ page ""AlphaFold has not been validated for predicting the effect of mutations. In particular, AlphaFold is not expected to produce an unfolded protein structure given a sequence containing a destabilising point mutation."" And there are papers writing about this eg Pak et al 2021","1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the evaluation metrics and the model's performance. The review point does not suggest any improvements or modifications that could be made to the model or the evaluation process. It also fails to provide any implicit actionable comments that the authors could infer from the review point. The lack of actionable comments in the review point means that the authors are left without any clear direction on what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for providing actionable feedback.",-1,"The review point provided is quite critical and lacks the necessary politeness expected in a review. The tone is aggressive and disrespectful, as seen in phrases like ""the metrics and/or the model is poor"" and ""the performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor."" This type of language is not conducive to a constructive review and does not provide any constructive feedback to the authors. The reviewer also fails to offer any suggestions for improvement, which is a key aspect of a review. Instead, the reviewer simply criticizes the work without providing any actionable insights. This approach is not helpful and does not encourage the authors to improve their work. Therefore, based on the score rubric, the tone of the review point is rude and disrespectful, which is why it would receive a score of -1.",-1,"The review point provided contains a claim that the evaluation metrics do not capture the concepts well, specifically regarding performance and novelty. The claim is that the metrics do not account for the distance from training data, which is a critical aspect of the evaluation process. The reviewer suggests that the method's performance is poor, as it is only 1% better than a random single mutation, and that the metrics and/or the model are poor. However, the reviewer does not provide any logical deductions or external or internal sources to support these claims. The reviewer also fails to address the reasons behind the poor performance results, which is a significant omission. Therefore, the review point does not meet the criteria for verifiability as it lacks justification, explanation, or reference to support its claims. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses the performance of the model in relation to the distance from training data. It clearly identifies the issue with the evaluation metrics, which do not account for the distance from training data. The review point also provides a clear direction for improvement by suggesting that the model's performance should be evaluated as a function of the distance from training data. This is a critical aspect of the paper, as it directly relates to the fundamental challenge in protein design. The review point also correctly points out that the performance results shown in the paper indicate a poor model, which is a significant concern. The authors are likely to be confident about which part of the paper is being addressed by the review point, as it directly addresses the key issues with the evaluation metrics and the model's performance. Therefore, the review point is highly specific and directly addresses the key aspects of the paper, making it clear which parts of the paper are being evaluated. So the overall score is 1."
4544,NIPS_2020_1461,"1. The title seems misleading, the method from my understanding is a way to use GNNs for learning natural laws, or understand GNNs in terms of symbolic models, rather than extracting symbolic models from general deep learning models. If the latter is their main claim, it would be helpful if the authors show the utility of this model on more general tasks, other than natural laws. 2. The paper lacks a strong related work section, missing key details about models that have been used previously for relevant tasks. For example, the authors have not discussed how other papers like Neural Relational Inference or using Generative Models for their tasks. 3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model. 4. It would be helpful if the authors address the natural question how utilizing this model for a general deep learning model is more interpretable or generalizable especially in the cases of noise? 5. The paper needs to add more details in the main draft. Key details are in the appendix, the authors should include majority of Section A.1 in the main paper rather than the appendix. The simulation details should be explained more, are the authors generating time series datasets, the number of samples, length of the time series, how the trajectory is processed etc. While some details are covered in the appendix, these details are essential to understand the task and model.","3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out the lack of robustness or generalization in the simulation data, without suggesting any specific actions that could be taken to address this issue. The review point also does not provide any implicit actionable comments that the authors could infer from the review. Instead, it only mentions the limitations of the paper without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, and thus, it would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point is focused on the content of the paper, specifically the interpretability and generalization of the model, and it critiques the lack of robustness or generalization on the simulation data. The feedback is constructive, pointing out the need for a better indicator of the model's claims. However, it lacks the politeness and encouragement that would be present in a score of 1. The feedback is straightforward and does not include any positive reinforcement or suggestions for improvement. Therefore, based on the rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the paper's main claim of interpretability and generalization. However, it fails to provide a verification of this claim. The review point does not offer any logical deductions or references to external or internal sources that could substantiate the claim. It merely states that the authors do not show robustness or generalization on the simulation data, without providing any evidence or justification to support this statement. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Instead, it aligns with the criteria for a score of -1, as it contains a claim that is not verified. Therefore, the review point does not meet the standards set for performance in terms of verifiability.",1,"The review point provided is quite specific, focusing on the interpretability and generalization of the model. It clearly identifies the main claim of the paper and critiques the lack of robustness or generalization on the simulation data. This specificity allows the authors to pinpoint exactly which part of the paper is being addressed, which is the interpretability and generalization of the model. The authors can confidently identify that the review point is discussing the model's ability to fit the ground truth equation and its performance on simulation data. This level of specificity aligns with the score rubric's criteria for a score of 1, as it enables the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, the review point could have been more general, addressing a broader theme or idea spread across many paper sections. This would have resulted in a score of 0, as the authors would have been less confident about which parts of the paper are being addressed by the review point. 

Therefore, based on the score rubric, the review point is well-crafted and provides a clear and specific critique of the paper's main claim. It effectively guides the authors to understand which part of the paper is being addressed, which is the interpretability and generalization of the model. 

So the overall score is 1."
2574,NIPS_2019_175,"Weaknesses: 1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.  ","5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the presence of errors and the need for additional information without suggesting how to address these issues. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any specific improvements or changes that the authors could make to their paper. It lacks the detail and guidance that would help the authors to improve their work. The review point does not provide any actionable insights that could lead to a better understanding of the paper's content or its potential impact on the field. 

In conclusion, the review point does not offer any actionable advice or suggestions, which is why it does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as particularly helpful or encouraging. The feedback is delivered in a straightforward manner, focusing on the content of the paper rather than the tone or manner of the feedback. The reviewer's comments are factual and do not contain any personal judgments or opinions, which is in line with the score rubric's criteria for a neutral tone. The reviewer's feedback is also constructive, providing suggestions for improvement without being overly critical or dismissive. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point provided does not contain any claims that need to be verified. It merely states that the paper could be reasonable to accept and suggests that the authors update the paper with additional information. This does not align with the score rubric, which requires the review point to make claims and verify them. The review point does not provide any logical deductions, external or internal sources, or justifications to support its claims. Therefore, it does not meet the criteria for a score of 1, which requires the review point to contain a claim and verify it. The review point also does not contain any factual statements or claims that do not need verifications, as it does not make any claims at all. So the overall score is 0.",-1,"The review point provided is vague and lacks specificity. It does not clearly identify which parts of the paper are being addressed. The authors are left with uncertainty about which aspects of the paper are being critiqued. The review point does not provide any concrete examples or suggestions for improvement, which would help the authors understand the issues and make necessary adjustments. The lack of specificity in the review point does not allow the authors to confidently identify the areas of concern in their paper. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not make it obvious which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it does not provide the authors with any clear direction on how to improve their paper."
2182,ACL_2017_614_review.json,"Weaknesses: - I don't understand effectiveness of the multi-view clustering approach. 
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                  - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub? ","- The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of clarity and organization in the paper, without suggesting any specific improvements or changes that could be made. The reviewer's comments are more of a critique rather than a constructive guide for the authors to follow. There is no mention of any limitations that could be addressed, nor are there any implicit actionable comments that the authors could infer from the review point. The review point does not fulfill the criteria of providing actionable feedback, as it does not suggest any direct or explicit actions that the authors should take. Therefore, based on the score rubric, the review point does not meet the requirements for a score of 1, 0, or -1.",0,"The review point provided is not fully clear on a first read, and the connection between sections is not immediately clear. The reviewer has pointed out specific issues, such as the lack of forward/backward pointer references and the out-of-order subsections in the multi-view clustering section. However, the tone of the review point is not particularly polite or encouraging. It lacks the constructive approach that would be expected in a review, as it does not provide any positive feedback or suggestions for improvement. The reviewer's comments are more focused on the problems they have identified, without offering any solutions or constructive criticism. This approach could be perceived as somewhat aggressive or disrespectful, as it does not acknowledge the efforts of the authors or provide any positive reinforcement. Therefore, based on the score rubric, the tone of the review point is not helpful or encouraging, and it does not fit well with the other two scores. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It merely points out issues with the structure and content of the paper, such as the lack of clarity in the connections between sections and the need for editing in the multi-view clustering section. These are factual statements that do not require any form of verification. The review point does not make any claims about the accuracy of the paper's content or the validity of its methods, which are the types of claims that would need to be verified according to the score rubric. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims at all. So the overall score is 0.",-1,"The review point provided is not specific enough to guide the authors in identifying the exact parts of the paper that need attention. The feedback is vague, mentioning the need for forward/backward pointer references and editing the multi-view clustering section, but it does not specify which sections or parts of the paper are being referred to. This lack of specificity makes it difficult for the authors to pinpoint the exact areas that require improvement. The review point also fails to provide concrete examples or suggestions that would help the authors understand the issues more clearly. As a result, the authors are left with a general sense of what needs to be addressed, but without the necessary details to take actionable steps. Therefore, based on the score rubric, the review point does not make it obvious which part of the paper is being addressed, and the authors are not confident about the paper part that is being addressed by the review point comment. So the overall score is -1."
4413,NIPS_2020_1552,"* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs. * Relation to relevant prior work is not mentioned and elaborated. For example, Rajasegaran, et al. ""Random path selection for continual learning."" NeurIPS'19 also propose a similar masking based approach to learn non-overlapping paths for dissimilar tasks. Similarly, PathNet (Evolution Channels Gradient Descent in Super Neural Networks) selectively masks out irrelavent model paramters. These papers should be cited and disucssed (preferably compared against) in this manuscript. * To my understanding, the notion of similar and dissimilar tasks is not accurate. E.g., the prior works on task incremental learning have both sets of similar and dissimilar tasks. (E.g., consider CIFAR100 classes in GEM - NeurIPS'17). In fact the considered set of similar and dissimilar tasks is not too different from the ones considered in earlier works. Specifically, consider a seminal work from Li & Hoeim, ""Learning without forgetting"" (TPAMI), where different datasets such as ImageNet/Places365/VOC/CUB/Scenes/MNIST are considered in continual learning experiments). Nevertheless, the proposed splits and dataset choices should be properly motivated and the authors should also report some experiments on previously considred protocols for fair benchmarking against existing methods. * The annealing strategy is somewhat similar to controller proposed in iTAML (iTAML : An Incremental Task-Agnostic Meta-learning Approach - CVPR'20). * The approach assumes that the task ID is known beforehand. Although this is consistent with some prior works, isn't it a bit restrictive in practical settings? It would be good to explain some application scenarios where tasks ID can be known to motivate the readers. * Equation 3 is wrong, it should be explicitly written. * The caption of Figure 1 should have some description for the MTCL architecture (a) as well.","* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the arbitrary choice of datasets without suggesting any specific actions or improvements that could be made. The review point does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any direct and explicit actionable comments that the authors should make. It does not provide any implicit actionable comments that the authors can infer. It merely points out the arbitrary choice of datasets without suggesting any specific actions or improvements that could be made. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not provide any explicit or implicit actions that should be done. It merely points out the arbitrary choice of datasets without suggesting any specific actions or improvements that could be made. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

In conclusion, the review point does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It does not provide any implicit actionable comments that the authors can infer, nor does it mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is not aggressive or rude, nor is it overly encouraging or helpful. The reviewer simply states the facts about the dataset choice and asks a question about the methodology. The reviewer does not offer any constructive feedback or suggestions for improvement. The tone is passive, and the reviewer does not engage with the content of the paper in a way that would be considered polite or encouraging. The reviewer's tone is neutral, and the feedback is factual, which is why the score is 0.",-1,"The review point in question makes a claim about the arbitrary choice of datasets, FEMNIST and FCelebA, for creating similar and dissimilar pairs. However, it fails to provide any verification or justification for this claim. The reviewer does not offer any logical deductions or references to external or internal sources that could substantiate the claim. This lack of verification and justification is a clear deviation from the score rubric's requirement for claims to be supported by logical deductions and internal or external sources. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains a claim that is not verified, which is why it would receive a score of -1 according to the rubric.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The review point merely states that the dataset choice seems arbitrary without providing any concrete examples or explanations as to why this is the case. This lack of specificity makes it difficult for the authors to pinpoint which sections of their paper are being critiqued. The review point does not offer any constructive feedback or suggestions for improvement, which would have been helpful for the authors to understand and address the concerns raised. As a result, the authors are left with no clear direction on how to improve their paper based on the review point. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric."
4950,NIPS_2020_1842,"1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper. 2. The claimed effect of ProtoMod needs more evaluations. The authors provide some qualitative results in Figure 3 and Fig Q.3 & Q.4 (from supp.) to show the proposed ProtoMod can localize the attributes like 'black back' and 'yellow breast'. What concerns me is whether the model is indeed learning the specific attributes (black back) or only the partial semantic like 'black' or 'back'. The part localization experiments (Fig 2 and Table 3) may partially show the model can learn the part information ('back', 'belly') from attributes but does not show if the model can distinguish between 'black back' or 'yellow back'. Neither the attribute similarity maps from Fig 3 and Fig Q.3 & Q.4. I think some qualitative results of the same images with locations of attributes only differ partially (e.g. 'black back' vs. 'yellow back' vs. 'black breast') can further verify whether the ProtoMod is learning the specific attributes or just the color or parts. --------- Ref: R1: Zhu, Pengkai, Hanxiao Wang, and Venkatesh Saligrama. ""Learning classifiers for target domain with limited or no labels."" ICML 2019. --------- Updates: the authors address my concerns in the rebuttal. I raise my score to accept.","1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the limitations of the paper and suggests that the authors elaborate on the differences between their work and other similar works. However, it does not provide any specific guidance or recommendations on how to improve the paper or what aspects to focus on. The review point also lacks any implicit actionable comments that the authors could infer from the review. It does not suggest any potential improvements or areas for further exploration. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the similarities between the paper and other works, but also encourages the authors to elaborate on the differences and specific contributions of their paper. This approach is helpful and encouraging, as it provides a clear direction for the authors to improve their work. The tone is respectful and the feedback is delivered in a manner that is likely to be well-received by the authors. The reviewer's suggestions for improvement are specific and constructive, which can help the authors to refine their paper and make it more competitive. So the overall score is 1.",0,"The review point provided does not contain any claims that need to be verified. It merely points out that the novelty of the paper is limited and suggests that the authors elaborate on the differences between their work and previous works. This is a factual statement that does not require any form of verification. The review point does not make any claims that could be verified or refuted, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",1,"The review point provided is quite specific and directly addresses the novelty of the paper. It points out that the idea of learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for zero-shot learning (ZSL) has been explored in other works, and it encourages the authors to elaborate on the differences between these works and clarify the specific novelties and contributions proposed in the paper. This specificity in the review point allows the authors to identify which parts of the paper are being addressed, and they can be confident about the paper part that is being addressed by the review point comment. The review point is focused on the novelty of the paper and the comparison with other works, which is a clear and specific aspect of the paper. Therefore, the score is 1."
1345,ICLR_2023_1035,"Weaknesses: 1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc.
The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments.
I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e. 600 episodes for each dataset. This is helpful for indicating the chance of handling such hard tasks in real-life scenarios where hard and easy tasks are mixed.
[1] Powering Finetuning for Few-shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. AAAI 2022 [2] Cross-domain few-shot learning with task-specific adapters. CVPR 2022 [3] Exploring Efficient Few-shot Adaptation for Vision Transformers. TMLR 2022","1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc. The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments. I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of insight in the analysis and suggests the inclusion of other metrics and adaptation methods without providing any concrete guidance on how to implement these suggestions. The review point also fails to infer any implicit actionable comments that the authors could deduce from the review. It simply states the limitations of the paper without offering any solutions or improvements. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is not particularly polite. It starts by questioning the quality of the results and analysis, which could be perceived as a critique of the authors' work. The reviewer also suggests that the paper lacks comprehensiveness and proposes additional methods without providing any constructive feedback on the existing methods. The tone of the review point is more critical than encouraging, which is not in line with the score rubric's requirement for a helpful or encouraging tone. The reviewer's comments on the 'hardness' of the original experiment setting and the suggestion to measure it could be seen as a direct criticism of the authors' approach, which is not a polite way to address the issue. The reviewer's language is more confrontational than constructive, which is not conducive to a positive review. Therefore, the review point does not meet the criteria for a score of 1, as it lacks the helpful and encouraging tone that is required.",-1,"The review point provided contains several claims that are not verified. The reviewer suggests that the analysis is not insightful and that the performance gap between the original MD and Hard-MD++ is not well explained. However, the reviewer does not provide any logical deductions or references to external or internal sources to support these claims. The reviewer also suggests that other metrics and adaptation methods could be used in the experiments, but again, there is no justification or explanation for these claims. The reviewer's claim about measuring the average 'hardness' of the original experiment setting is also unverified. The reviewer's claims are not supported by logical deductions or references, which is why the review point does not meet the criteria for verifiability. Therefore, the score is -1.",1,"The review point provided is quite specific and directly addresses the analysis of the paper. It points out the lack of insightful analysis in the paper and suggests that other metrics and adaptation methods could be used to provide a more comprehensive understanding. The review point also questions the average 'hardness' of the original experiment setting, which is a clear indication of the specific part of the paper that is being addressed. The authors would be able to identify which parts of the paper are related to the review point, and they would be confident about the specificity of the review point. Therefore, based on the score rubric, the review point is well-aligned with the criteria for a score of 1."
2214,ACL_2017_66_review.json,"Weaknesses: The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).
- General Discussion: The current presented approach, in comparison to existing approaches, is promising. ","- General Discussion: The current presented approach, in comparison to existing approaches, is promising.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the current approach is promising without suggesting any specific actions or improvements that could be made. The review point also lacks any implicit actionable comments that could be inferred from the text. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionability makes it difficult for the authors to deduce what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers suggestions for improvement, such as providing a simpler expression for the bounds and strengthening the introduction with a clearer context. These comments are delivered in a respectful and encouraging tone, which is helpful for the authors. The reviewer also acknowledges the potential of the proposed method, which is a positive reinforcement.

In contrast, the review point that would receive a score of -1 is aggressive and disrespectful. The reviewer uses harsh language, such as ""terrible"" and ""poor quality,"" which is not conducive to a constructive review. The reviewer also makes negative judgments about the paper's suitability for publication, which is not helpful for the authors.

The review point that would receive a score of 0 is neutral and factual. The reviewer asks questions about the methodology and expresses opinions about the paper's structure and content. However, the tone is not particularly helpful or encouraging, and the reviewer does not provide any specific suggestions for improvement.

In conclusion, the review point provided is polite and constructive, which is in line with the score rubric's criteria for a score of 1. The reviewer offers helpful suggestions and maintains a respectful tone throughout the review.",0,"The review point provided does not contain any claims that need to be verified. It is a general statement about the current approach being promising in comparison to existing approaches. The review point does not make any specific claims about the approach or its results, nor does it provide any logical deductions or references to external or internal sources that could verify any claims. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be verified or not. So the overall score is 0.",-1,"The review point provided is quite vague and does not offer specific guidance or feedback on the paper. It lacks the specificity required to effectively address the paper's content. The review point does not clearly identify which parts of the paper are being addressed, nor does it provide any concrete suggestions for improvement. This lack of specificity makes it difficult for the authors to understand which aspects of their work are being critiqued. As a result, the authors are left with a sense of uncertainty about the paper's strengths and weaknesses. The review point does not fulfill the criteria of making it obvious which part of the paper is being addressed, nor does it allow the authors to confidently identify the parts of the paper related to the review point. Therefore, the review point does not meet the standards set by the score rubric. 

In contrast, a review point that is specific and directly addresses a particular aspect of the paper would allow the authors to confidently identify the parts of the paper that are being addressed. It would provide clear guidance on how to improve the paper, and it would help the authors to understand the reviewer's perspective. The review point should be focused on a specific aspect of the paper, and it should offer clear and actionable feedback. 

Based on the score rubric, the review point provided does not meet the criteria for specificity, and therefore, it would receive a score of -1."
4320,NIPS_2020_386,"The weaknesses are : 1. It is not clear how the reparametrization differ from the original parametrization with low rank covariance in terms of predictive performance. 2. It is not clear why the more expressive diagonal covariance is less predictive than the scaled identity covariance. 3. Why the latter covariance is more computationally efficient than the former for deep neural networks? 4. Given that the ELRG-VI has worse accuracy as in the Modern CNNs, what is the advantage of using this approximate posterior? 5. Evaluation of the quality of the posterior approximation by predictive performance is not appropriate as in many cases MAP can give more accurate predictions.",3. Why the latter covariance is more computationally efficient than the former for deep neural networks?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states the computational efficiency of the latter covariance without suggesting any specific actions or improvements that could be made. The review point also lacks any implicit actionable comments that could be inferred from the text. It does not offer any guidance on how to address the limitations mentioned, nor does it suggest any further research or development. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, which is in line with the neutral tone expected in a review. The feedback is focused on the content of the paper, providing a critique of the methodology and the structure of the paper. However, the feedback could be improved by adding a more constructive tone, which would make the feedback more helpful to the authors. The reviewer could also provide more specific suggestions for improvement, which would make the feedback more actionable. Despite these areas for improvement, the review point is still neutral and factual, which is the default score if the review point does not fit well with the other two scores. So the overall score is 0.",0,"The review point in question does not contain any claims, nor does it require verification. It is a factual statement that is not subject to verification, as it does not make any claims about the computational efficiency of the latter covariance in deep neural networks compared to the former. The review point simply states that the latter covariance is more computationally efficient, which is a factual statement that does not need to be verified. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not make any claims that could be verified or not. So the score is 0.",1,"The review point provided is quite specific and directly addresses the computational efficiency of the latter covariance in deep neural networks. It clearly identifies the part of the paper that is being addressed, which is the comparison between the two covariance methods. The authors can be confident that the review point is focused on the efficiency of the latter covariance, as it directly compares the two methods and discusses their computational implications. The review point is also well-structured, making it easy for the authors to understand which parts of the paper are being discussed. It does not leave any ambiguity about the paper part being addressed, which is crucial for the authors to be confident about the review point. Therefore, based on the score rubric, the review point is highly specific and directly addresses the paper part in question, making it a score of 1."
5685,NIPS_2018_639,"Weakness: - I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.  - How does the reader know that the reward definitions of rho for this tasks necessitates a non-convex reward function. Surveillance and data collection has been studied in POMDP context by many papers. Fortunately/unfortunately, many of these papers show that the increase in the reward due to a rho based PWLC reward in comparison to a corresponding PWLC state-based reward (R(s,a)) is not that big. (Papers from Mykel Kochenderfer, Matthijs Spaan, Shimon Whiteson are some I can remember from top of my head.) The related work section while missing from the paper, if existed, should cover papers from these groups, some on exactly the same topic (surveillance and data collection).  - This basically means that we have devised a new method for solving non-convex value function POMDPs, but do we really need to do all that work? The current version of the paper does not answer this question to me. Also, follow up question would be exactly what situation do I want to use the methodology proposed by this paper vs the existing methods.  In terms of critisim of significance, the above points can be summarized as why should I care about this method when I do not see the results on problem the method is supposedly designed for.  ","- I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out the lack of experiments in certain settings without suggesting any specific actions that could be taken to address this issue. The reviewer's concerns are valid, but they do not offer any constructive feedback or guidance on how to improve the paper. The review point does not fulfill the criteria of suggesting direct and explicit actionable steps or experiments that can be done, nor does it include actionable comments that can be implicitly inferred. Therefore, based on the score rubric, the review point does not meet the requirements for a score of 1, 0, or -1.",-1,"The review point provided is not polite and comes across as aggressive. The reviewer's tone is critical and dismissive, particularly when questioning the absence of experiments in certain settings. The reviewer's language is confrontational, using phrases like ""not quite useful"" and ""why is there not a single experiment on such a setting,"" which can be perceived as disrespectful. This approach does not encourage a constructive dialogue or provide any helpful feedback to the authors. Instead, it focuses on the perceived shortcomings of the paper without offering any constructive criticism or suggestions for improvement. The reviewer's tone is not helpful or encouraging, and it does not meet the standards of politeness as outlined in the score rubric. Therefore, the review point is not aligned with the criteria for a score of 1, which requires a helpful and encouraging tone. So the overall score is -1.",0,"The review point in question does not make any claims, but rather questions the validity of the experimental results presented in the paper. The reviewer is skeptical about the absence of experiments in settings that are not covered by the examples provided. However, the reviewer does not provide any logical deductions or references to external or internal sources to support their skepticism. The reviewer's concerns are based on the lack of experiments in certain settings, but they do not offer any justification or explanation for why this lack of experiments is problematic. Therefore, the review point does not meet the criteria for verifiability as it does not contain any claims that are verified. The reviewer's concerns are valid, but they are not substantiated with logical explanations or references, which is why the review point does not score higher than 0.",1,"The review point provided is quite specific and directly addresses the experimental results of the paper. It questions the absence of experiments in the paper, particularly in the context of POMDP problems with non-convex value functions. The review point is clear and concise, making it obvious which part of the paper is being addressed. It also provides a clear direction for the authors to improve their paper by including relevant experiments. The review point is well-structured and directly engages with the content of the paper, which is in line with the score rubric's criteria for a score of 1. The authors can be confident about which part of the paper is being addressed by the review point, as it directly points out the lack of experimental evidence in the paper. Therefore, the review point is highly effective in addressing the specificity of the paper's experimental results."
304,ICLR_2022_113,"Weaknesses: - The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive? - The method focuses on learning cluster granularity for the object only, and not for the background. - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)
A few comments on the text: - The phrase ""coarse-grained images"" is inaccurate, the ""coarse-grained"" adjective should refer to the clustering and not the images (in the intro). - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro. - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)",- It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline) A few comments on the text:,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely states that the transformation matrix is used without explaining why or how it could be improved. The review point also does not suggest any implicit actionable comments that the authors can infer. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. This lack of actionability makes it difficult for the authors to understand what needs to be done to improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating facts about the paper. The reviewer does not make any personal judgments or comments on the quality of the paper, which is a key aspect of the rubric. The reviewer's comments are focused on the technical aspects of the paper, without any personal opinions or emotional undertones. This is in line with the score of 0, which is the default score if the review point does not fit well with the other two scores. Therefore, the review point is neutral and factual, and it does not meet the criteria for a score of 1 or -1.",0,"The review point in question does not make any claims, nor does it provide any logical deductions or references to support any potential claims. It merely states that the transformation matrix is used in PerturbGAN's pipeline without offering any insight into its purpose or significance. This lack of verifiable claims and justifications makes the review point fall short of the criteria set forth in the score rubric. The review point does not contribute to the understanding of the transformation matrix's role in PerturbGAN, nor does it provide any evidence or reasoning to support its use. Therefore, based on the score rubric, the review point does not meet the criteria for verifiability and is not relevant to the task at hand.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The statement ""It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)"" does not provide any specific insight into the paper's content or the role of the transformation matrix. It lacks the specificity required to guide the authors in understanding and addressing the review point. The authors would likely be unsure about which parts of the paper are being addressed by the review point, as the comment does not provide any concrete information or direction. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the score is -1."
1661,ICLR_2023_3449,"Weaknesses
1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf","3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.). [1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states the setup of the experiment without suggesting any changes or improvements that could be made. The review point does not ask any questions or provide any insights that could lead to actionable steps. It also does not infer any implicit actionable comments that could be inferred from the review point. The review point simply states the experiment setup without offering any guidance or suggestions for future work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided in the score rubric is neutral and factual, which aligns with the score of 0. The tone of the review point is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The reviewer's comments are focused on the content of the paper, such as the experiment setup and the technical novelty, without any personal or emotional undertones. The reviewer's questions and observations are presented in a straightforward manner, which is appropriate for a review of a scientific paper. The reviewer's comments are also constructive, as they provide suggestions for improvement without being overly critical or dismissive. Therefore, the review point is neutral and factual, which is the default score if the review point does not fit well with the other two scores.",0,"The review point in question does not contain any claims or assertions that require verification. It merely describes the experiment setup in Section 3.3 of the paper, which includes data augmentation methods and learning rate. The review point does not make any claims about the validity or accuracy of the methods described, nor does it question the results or the methodology. It also does not provide any logical deductions or references to external or internal sources that could verify or refute the claims made in the paper. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability, as it neither contains claims nor verifies them. So the overall score is 0.",1,"The review point in question is quite specific, focusing on the experiment setup in Section 3.3 of the paper. It directly addresses the data augmentation methods and the learning rate, which are key elements in the setup of the experiment. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are related to the review point. The authors can be confident that the review point is addressing the specifics of the experiment setup, as it directly refers to the methods and parameters used in the experiment. This level of specificity aligns well with the score rubric, which emphasizes the importance of clarity and directness in the review point. Therefore, based on the score rubric, the review point is well-crafted and effectively communicates the focus of the review."
2896,NIPS_2022_1666,"Weaknesses:
I cannot give a clear acceptance to the current manuscript due to the following concerns:
1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work.
For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work.
2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1.
3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper.
In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.
4. Advantage of NAR(heatmap) + RL + Meta-Learning: From Table 1&2, for TSP1000, the generalization performance of AM (G: 31.15, BS: 29.90) trained on TSP100 is not very far from the testing performance of DIMES without meta-learning (27.11) directly trained on TSP1000. It could be helpful to check whether the more powerful POMO approach[4] can have a smaller performance gap. Reporting the results for POMO and DIMES without meta-learning for all instances in Table 1 could make the advantage of the NAR(heatmap) + RL approach in DIMES much clearer.
Hottung et al.[5] shows that POMO + Efficient Active Search (EAS) can achieve promising generalization performance for larger TSP instances on TSP and CVRP. The comparison with POMO + EAS could be important to better evaluate the advantage of meta-learning in DIMES.
[1] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, Thomas Laurent, and Xavier Bresson. Learning tsp requires rethinking generalization. arXiv preprint arXiv:2006.07054,2020.
[2] Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.
[3] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. NeurIPS 2018.
[4] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min. POMO: Policy optimization with multiple optima for reinforcement learning. NeurIPS 2020.
[5] André Hottung, Yeong-Dae Kwon, and Kevin Tierney. Efficient active search for combinatorial optimization problems. ICLR 2022.
Yes, the limitations have been adequately addressed in Section 5 Concluding Remarks. I do not see any potential negative societal impact of this work.","1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work. For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work. 2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1. 3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper. In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point provides a detailed analysis of the work, highlighting several key points. However, it falls short in terms of actionability. The review point does not suggest any direct or explicit actionable steps or experiments that can be done. It merely discusses the limitations and potential improvements of the work without providing any clear guidance on how to address these issues. The review point also lacks any implicit actionable comments that could be inferred from the analysis. It does not offer any suggestions for further research or development, nor does it provide any insights into how the authors could improve their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct and explicit actionable comments nor includes any implicit actionable comments. So the overall score is 0.",1,"The review point provided is constructive and polite, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the contributions of the paper, such as the compact continuous parameterization and the meta-learning approach, but also points out the inaccuracies and lack of clarity in certain aspects. The reviewer's tone is respectful and encouraging, offering suggestions for improvement and highlighting the potential novelty of the NAR + RL combination. The reviewer also addresses the actual cost of meta-learning and the generalization performance of the paper, providing a balanced view of the paper's strengths and weaknesses. The feedback is delivered in a manner that is helpful and constructive, which is in line with the score rubric's criteria for a score of 1. So the overall score is 1.",-1,"The review point contains several claims that need to be verified. The first claim is about the contribution of compact continuous parameterization. The reviewer correctly identifies that this contribution is not unique to the current work, but rather a common approach used in GNN models. The reviewer also correctly notes that the actual cost of meta-learning is not clearly discussed in the main paper, and that the reported runtime in Table 1 is misleading due to the need for extra meta-gradient update steps. The third claim is about the generalization performance of the method. The reviewer correctly points out that the other methods are trained on TSP100 instances, while DIMES is trained on TSP1000 instances, and that the results reported in Table 1 are out-of-distribution generalization performance.

However, the reviewer does not provide any logical deductions or references to external or internal sources to support these claims. The reviewer also fails to clarify the difference in generalization performance between DIMES and other methods, which is a crucial aspect of the review point.

In conclusion, the review point contains several claims that need to be verified, but the reviewer does not provide any logical deductions or references to external or internal sources to support these claims. The reviewer also fails to clarify the difference in generalization performance between DIMES and other methods, which is a crucial aspect of the review point. Therefore, the review point does not meet the criteria for verifiability as outlined in the score rubric.",1,"The review point provided is quite specific and directly addresses the paper's contributions and methodology. It clearly identifies the inaccuracies in the paper's claim about the compact continuous parameterization and the actual cost of meta-learning. The review point also points out the generalization performance of the paper compared to other methods. This specificity allows the authors to understand exactly which parts of the paper are being addressed by the review point. The authors can confidently identify the areas of the paper that need clarification or correction. The review point's focus on the specific aspects of the paper, such as the parameterization, meta-learning costs, and generalization performance, makes it clear and actionable for the authors. Therefore, the review point meets the criteria of the score rubric by providing a clear and specific feedback that directly addresses the paper's content."
5265,NIPS_2020_1700,"- There are a number modelling details that are not entirely clear in the paper (see below). - When comparing Gumbel-CRF directly to Perturb-and-MAP MRF, performance is very similar.",- There are a number modelling details that are not entirely clear in the paper (see below).,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out some limitations and areas for improvement without suggesting any specific actions or methods that could be implemented to address these issues. The review point does not provide any implicit actionable comments that could be inferred by the authors, which is a key aspect of the score rubric. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or judgments, instead focusing on the details of the paper and providing a critique that is based on the content itself. The feedback is delivered in a manner that is neither rude nor overly encouraging, which is in line with the score rubric's criteria for a score of 0. The reviewer's comments are factual and do not contain any personal attacks or disrespectful language. The feedback is also constructive, as it points out specific areas where the paper could be improved. However, it lacks the helpful and encouraging tone that would be present in a review with a score of 1. The reviewer's comments are more focused on the shortcomings of the paper rather than offering suggestions for improvement. Therefore, the review point is neutral and factual, which is why it receives a score of 0.",0,"The review point provided does not contain any claims that need to be verified. It merely states that there are details in the paper that are not entirely clear, without making any specific claims or providing any logical deductions or references to support or refute these details. The review point does not meet the criteria for verifiability as it does not contain any claims that could be verified or refuted. Therefore, the score is 0.",-1,"The review point provided is vague and does not clearly identify which part of the paper is being addressed. The authors are left with uncertainty about the specifics of the review point. The review point does not provide any concrete examples or suggestions for improvement, which would have helped the authors to understand and address the issues raised. Instead, it uses general language that does not offer any actionable insights. This lack of specificity makes it difficult for the authors to confidently identify the parts of the paper that are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Instead, it falls into the category of a score of -1, as it leaves the authors with no clear idea of what needs to be improved or clarified in the paper."
3860,NIPS_2020_1228,"- The method section looks not self-contained and lacks descriptions of some key components. In particular: * What is Eq.(9) for? Why ""the SL is the negative logarithm of a polynomial in \theta"" -- where is the ""negative logarithm"" in Eq.(9)? * Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained. - The proposed approach essentially treats the structured constraints (a logical rule) as part of the discriminator that supervises the training of the generator. This idea looks not new -- one can simply treat the constraints as an energy function and plug it into energy-based GANs (https://arxiv.org/abs/1609.03126). Modeling structured constraints as a GAN discriminator to train the generative model has also been studied in [15] (which also discussed the relation b/w the structured approach with energy-based GANs). Though the authors derive the formula from a perspective of semantic loss, it's unclear what's the exact difference from the previous work? - The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.","* Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or comments. It merely points out the lack of details in the ""Evaluating the Semantic Loss"" part, specifically regarding knowledge compilation (KC). However, it does not suggest any specific actions or experiments that could be done to address this issue. The review point also fails to provide any implicit actionable comments that could be inferred from the text. It simply states the necessity of presenting details about KC without offering any guidance on how to achieve this. Therefore, based on the score rubric, the review point does not meet the criteria for actionability, as it neither provides explicit actionable comments nor does it offer implicit actionable comments. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The reviewer simply points out the lack of detail in the explanation of knowledge compilation, which is a factual observation. The reviewer does not make any personal judgments or criticisms, which is why the tone is neutral. The reviewer's focus is on the content of the paper, rather than the quality of the paper or the performance of the authors. This is why the review point does not fit well with the other two scores. The reviewer's tone is consistent with the neutrality expected in a review, and the feedback is constructive, focusing on the need for more detail in the explanation of a key component of the proposed approach.",-1,"The review point in question makes a claim about the practicality of a specific equation, which is not supported by any logical deductions or references. The claim is that the equation is not practically tractable, but the reviewer does not provide any evidence or reasoning to back up this assertion. Furthermore, the reviewer points out the lack of details about knowledge compilation (KC), which is a critical component of the proposed approach. However, the reviewer does not attempt to verify this claim or provide any additional information that could help to understand the importance of KC.

The review point also fails to address the verifiability of the claims made in the paper. There is no mention of how the claims could be verified or what sources could be used to support or refute the claims. The reviewer simply states that the approach is not self-contained, but does not provide any concrete examples or explanations to support this claim.

In summary, the review point contains a claim that is not verified, and it lacks any logical explanations or references to support the claim. The reviewer also fails to address the verifiability of the claims made in the paper, which is a key aspect of the score rubric. Therefore, based on the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified or supported by logical deductions and internal or external sources.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the practical implementation of the Weighted Model Count (WMC) and knowledge compilation (KC). It clearly identifies the part of the paper that is being addressed, making it easy for the authors to understand which section of their work is being critiqued. The review point also provides a clear direction for the authors to improve their paper by suggesting that more details about KC should be included. This specificity in the review point aligns well with the score rubric, which emphasizes the importance of identifying the exact part of the paper that is being addressed. Therefore, based on the score rubric, the review point is well-crafted and provides a clear and specific critique of the paper. 

In contrast, a review point that lacks specificity would not be able to guide the authors effectively. It would not be clear which part of the paper is being addressed, and the authors would be left uncertain about how to improve their work. The lack of specificity in a review point would not meet the criteria set out in the score rubric, as it would not help the authors to identify the exact part of their paper that needs attention. 

Therefore, considering the score rubric and the examples provided, it is evident that the review point is well-constructed and provides a clear and specific critique of the paper. It effectively addresses the practical implementation of the WMC and KC, which is a crucial aspect of the proposed approach. 

So the overall score is 1."
2038,ARR_2022_256_review,"1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' ","1. I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this!",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely questions the process and the existence of a baseline without suggesting any specific actions or experiments that could be done. The review point does not offer any insight into how the teacher model could be used for zero-shot cross-lingual NER, nor does it propose any improvements or modifications to the existing model. It also fails to suggest any implicit actions that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point is delivered in a neutral tone, which is in line with the score rubric's criteria for a score of 0. The reviewer's questions are factual and do not contain any aggressive or disrespectful language. However, the reviewer's tone could be perceived as slightly passive, as it does not actively encourage the author to provide more information or clarify their points. The reviewer's questions are more focused on understanding the process rather than providing constructive feedback or suggestions for improvement. This is why the review point does not score higher on the politeness scale. The reviewer's tone is neutral and factual, but it lacks the helpful or encouraging tone that would be present in a score of 1. So the overall score is 0.",-1,"The review point in question contains a claim regarding the use of the teacher model for conducting zero-shot cross-lingual NER. However, the claim is not verified, as it lacks a justification or explanation. The reviewer questions why the teacher model is not used directly for the NER task in the target language, but fails to provide any logical deductions or references to support their claim. The absence of verification in the review point does not meet the criteria for a score of 1, as it does not include any claims that are supported by logical deductions and internal or external sources. Therefore, the review point does not meet the standards set for performance in terms of verifiability. So the score is -1.",0,"The review point provided is quite specific and directly addresses a particular aspect of the paper. It questions the process of teaching the student model by the teacher model and the possibility of using the teacher model for zero-shot cross-lingual NER. This specificity allows the authors to identify the exact part of the paper that is being addressed, which is the methodology and the application of the model. The review point also suggests a potential misunderstanding or oversight in the paper, which is the lack of a baseline for comparison. This indicates that the authors are not fully confident about the paper part that is being addressed by the review point comment. Therefore, the review point is not as clear and direct as it could be, and it does not provide a strong basis for the authors to confidently identify the addressed part of the paper. So the overall score is 0."
5573,NIPS_2018_641,"weakness.  First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a ""good"" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless.  Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the ""learning rate"" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD.  The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes.  I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication.  Other comments: 1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. ""Gradient methods for minimizing composite functions"" by Yu. Nesterov. 2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate.  4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties. 5. The proofs in the supplementary material are too terse. ","2. ln.55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely states a limitation without suggesting any specific actions that could be taken to address this limitation. The review point does not offer any guidance on how to improve the convergence of the regularized empirical risk minimizer to the risk minimizer. It also does not provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point in question is neutral and factual, which aligns with the score of 0. The reviewer does not express any form of aggression or rudeness, nor do they offer any form of encouragement or helpfulness. The tone is purely informative, with the reviewer simply stating their observation about the regularization parameter without any additional commentary. The reviewer's statement is a straightforward question about the regularization parameter, which is a neutral and factual approach to reviewing. The reviewer does not make any judgments or criticisms, nor do they offer any suggestions for improvement. The reviewer's tone is consistent throughout the review point, maintaining a neutral stance. Therefore, based on the score rubric, the review point is neutral and factual, and the tone is neither aggressive nor encouraging.",0,"The review point in question does not make any claims, but rather questions the validity of a claim made in the original paper. The reviewer is skeptical about the convergence of the regularized empirical risk minimizer to the risk minimizer without any condition on the regularization parameter. However, the reviewer does not provide any logical deductions or references to external or internal sources to support their skepticism. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The reviewer's claim is not supported by logical deductions or sources, and therefore, it does not verify the claim. The review point is more of a statement of doubt rather than a claim that needs to be verified. So the overall score is 0.",1,"The review point in question is quite specific, addressing a particular aspect of the paper related to the regularization parameter and its impact on the convergence of the regularized empirical risk minimizer. This specificity allows the authors to pinpoint exactly which part of the paper is being discussed, which is the regularization parameter and its role in the convergence of the risk minimizer. The review point is clear and concise, making it easy for the authors to understand and address the concerns raised.

The review point does not leave any ambiguity about the paper part being addressed, which is crucial for the authors to confidently identify and respond to the review point. The specificity of the review point ensures that the authors are not left guessing which parts of the paper are being addressed, but rather are confident about the exact aspect being discussed.

In contrast, a review point that lacks specificity would leave the authors unsure about which parts of the paper are being addressed. This lack of specificity could lead to confusion and a need for further clarification, which is not the case with the review point in question.

Therefore, based on the score rubric, the review point is highly specific and directly addresses a particular aspect of the paper, making it clear and easy for the authors to understand and respond to. This specificity aligns with the criteria for a score of 1, as it allows the authors to confidently identify and address the concerns raised."
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out a potential weakness in the assumption of strong convexity in theorem 2 and a missing dependence on the batch size. However, it does not suggest any specific actions or experiments that could be taken to address these issues. The review point also fails to provide any implicit actionable comments that could be inferred from the review point. It simply states the limitations without offering any solutions or further exploration. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score rubric's criteria for a score of 0. The reviewer does not express any aggression or rudeness, nor do they offer any encouragement or helpfulness. Instead, they focus on the technical aspects of the paper, pointing out potential issues with the assumptions made and the lack of dependence on the batch size. This is a constructive approach, as it provides a critique that could help the authors improve their work. However, the tone of the review point is not particularly engaging or encouraging, which is why it does not score higher on the politeness scale. The reviewer's comments are more focused on the technical aspects rather than the overall quality or potential impact of the paper. Therefore, the review point is neutral and factual, which is why it scores a 0 according to the score rubric.",-1,"The review point raises a valid concern about the assumption of strong convexity in theorem 2 and the lack of dependence on the batch size. However, it does not provide any logical deductions or references to support these claims. The reviewer's question about the assumption of strong convexity and the missing dependence on the batch size is not verified, as it lacks a justification or explanation. The reviewer's claim about the missing dependence on the batch size is not supported by any logical deductions or references, which is a key aspect of the score rubric. Therefore, the review point does not meet the criteria for verifiability, as it contains claims that are not verified. So the overall score is -1.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the assumption of strong convexity in theorem 2. It also raises a concern about the missing dependence of the bound on the batch size, which is a crucial element in the context of SGD with biased gradients. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. It also provides a clear direction for the authors to consider, which is to explore the setting where G(theta, phi) is not strongly convex and to consider the dependence of the bound on the batch size. This specificity in the review point aligns well with the score rubric, which emphasizes the importance of identifying the exact parts of the paper that are being addressed. Therefore, based on the score rubric, the review point is well-crafted and effectively communicates the concerns and suggestions to the authors."
3822,NIPS_2020_179,"1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack. 2. Suppose the evaluation reflects the model robustness, the region for “small perturbations” is quite small. In CIFAR10, the proposed mechanisms only work on par with adversarial training at epsilon=0.001. I would think even epsilon=8/255 on CIFAR10 are not perceptible by humans. 3. In Table 1, it will be interesting to integrate the proposed fixations with SOTA models. The reported RESNET result is not the SOTA ones. 4. In the gradient obfuscation part, it will be good to clarify what does the authors “verified”. 5. Last paragraph in the conclusion generalizes the work a bit too much. 6. There is no adversarial training baseline in ImageNet10, ImageNet results.","1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or comments. It merely points out a limitation in the adversarial attack used in the experiments without suggesting any improvements or further experiments that could be done. The reviewer expresses uncertainty about the strength of the attack and the reliability of the results, but does not offer any concrete suggestions for how to address these concerns. The lack of actionable comments in the review point means that the authors are left without clear guidance on what steps to take next. Therefore, based on the score rubric, the review point does not meet the criteria for providing actionable comments or suggestions.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any aggression or rudeness, nor do they offer any encouragement or constructive feedback. The tone is purely informative, with the reviewer questioning the strength of the adversarial attack used in the experiments and expressing uncertainty about the results. The reviewer's tone is not overly critical or positive, but rather a straightforward expression of doubt. This is in line with the rubric's description of a score of 0, which indicates a neutral tone. The reviewer's language is polite and respectful, and they do not make any personal attacks or disrespectful comments. Therefore, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not make any claims, but rather expresses uncertainty about the robustness of the model as evaluated by the adversarial attack. The reviewer is questioning the strength of the adversarial attack used in the experiments, which is a valid concern when evaluating the robustness of a model. However, the reviewer does not provide any logical deductions or references to external or internal sources to support their claim. The reviewer's uncertainty is not verified, and there is no attempt to justify their skepticism. Therefore, according to the score rubric, the review point does not meet the criteria for verifying claims, and it is not relevant for verifiability in review points. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The reviewer expresses uncertainty about the strength of the adversarial attack used in the experiments, but does not provide any concrete examples or suggestions for improvement. The reviewer's concern about the robustness of the model is valid, but the lack of specificity in the review point makes it difficult for the authors to pinpoint exactly what needs to be addressed. The reviewer's uncertainty about the results and analysis from the weak attack does not contribute to a clear understanding of the paper's content or the reviewer's concerns. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric. 

The reviewer's lack of specificity in the review point is a significant drawback, as it does not allow the authors to confidently identify which parts of the paper are being addressed. The reviewer's uncertainty about the strength of the adversarial attack and the robustness of the model is not effectively communicated, which could lead to confusion or misinterpretation of the paper's content. 

In conclusion, the review point lacks the specificity required to effectively address a particular part of the paper. The reviewer's uncertainty and lack of concrete examples or suggestions for improvement make it difficult for the authors to understand and address the reviewer's concerns. Therefore, the review point does not meet the criteria for specificity as outlined in the score rubric."
3001,NIPS_2022_489,"Weaknesses:
Concern regarding representativeness of baselines used for evaluation
Practical benefits in terms of communication overhead & training time could be more strongly motivated
Detailed Comments:
Overall, the paper was interesting to read and the problem itself is well motivated. Formulation of the problem as an MPG appears sound and offers a variety of important insights with promising applications. There are, however, some concerns regarding evaluation fairness and practical benefits.
The baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE. In particular, there have been a variety of recent works that explore more efficient strategies (e.g., [1-3]) and consistently outperform QMix with relatively low inter-agent communication. Although the proposed work appears effective as a fully-decentralized approach, it is unclear how well it would perform against more competitive CTDE baselines. Comparison against these more recent works would greatly improve the strength of evaluation.
Benefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.
Robustness against dynamic changes in network setup are discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches.
Some confusion in notation: - Algorithm 2, L8 should be t = 1,…,H (for horizon)? - L100, [M] denotes the set of LBs?
Minor notes: - Some abbreviations are not defined, e.g., “NE” on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read.
[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. [2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. [3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.","- L100, [M] denotes the set of LBs? Minor notes:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out some limitations without suggesting any specific actions that should be taken. For instance, the review point mentions that the related work section is severely lacking, but it does not provide any guidance on how to improve it. Similarly, it notes that the proposed method has only a little performance improvement, but it does not suggest any improvements or further experiments that could be conducted. The review point also fails to provide any implicit actionable comments that could be inferred by the authors. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is neither aggressive nor constructive, but rather a straightforward critique of the paper. The reviewer does not use any disrespectful language or make any personal attacks, which is a key aspect of maintaining politeness. However, the reviewer does not provide any positive feedback or suggestions for improvement, which could have made the tone more constructive. The reviewer also does not use any encouraging language, which could have made the tone more helpful. The feedback is focused on the shortcomings of the paper, which is a factual approach but lacks the constructive element that would have made the tone more helpful. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a series of questions and observations that do not make any assertions about the content of the paper. The reviewer is simply asking for clarification or additional information, which does not require any form of verification. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that are verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. Instead, it falls into the category of a score of 0, as it does not contain any claims at all. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly specify which part of the paper is being addressed. The authors are left with a sense of uncertainty about the exact sections of the paper that are being critiqued. The review point does not provide any concrete examples or suggestions for improvement, which would have helped the authors to understand the specific issues with their work. Instead, the review point uses general language that could apply to any part of the paper, making it difficult for the authors to identify the exact areas of concern. This lack of specificity in the review point does not meet the criteria for a score of 1, as it does not allow the authors to be confident about which part of the paper is being addressed. Therefore, based on the score rubric, the review point is not effective in guiding the authors to improve their paper. So the overall score is -1."
924,ICLR_2023_4092,"Weakness: 1. The writing and setting up of this paper are not very clear to me. The motivation does not convincing to me. 2. The novelty of the proposed method seems trivial. Contrastive learning, embedding alignment and etc have been widely discussed in the previous literature.
3. For XQR performance, it is not surprising that with the additional selected XOR finetuning examples, the performance will boost further. 4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.",4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out the lack of clarity in the paper's ablation and the need for a comparison between the proposed method and existing methods. The review point does not suggest any specific changes or improvements that could be made to the paper. It also fails to provide any implicit actionable comments that the authors could infer from the review point. Instead, it only mentions some limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The reviewer does not express any form of aggression or rudeness, nor does the tone come across as overly polite or encouraging. The feedback is focused on the content of the paper, specifically the ablation and the comparison with existing methods. The reviewer's tone is neutral, and the feedback is delivered in a straightforward manner, without any personal attacks or disrespectful language. The reviewer's comments are constructive, aiming to improve the paper by suggesting a clearer comparison with existing methods. The feedback is also factual, as it points out the lack of a clear comparison and the ambiguity regarding the performance boost. Therefore, the review point is neutral and factual, which is why it receives a score of 0 according to the rubric.",-1,"The review point in question makes several claims, but it fails to provide any form of verification or justification for these claims. The reviewer suggests that the ablation of the paper is not ready and that the performance boost is not clear. However, there is no logical deduction or reference to external or internal sources that could substantiate these claims. The reviewer also mentions the need for a parameter comparison or computing comparison, but again, there is no evidence provided to support this claim. The lack of verification and justification in the review point means that it does not meet the criteria for a score of 1, as it does not contain any claims that are verified. Instead, it contains claims that are not verified, which is why it would receive a score of -1 according to the score rubric.",-1,"The review point provided is not specific enough to clearly identify which part of the paper is being addressed. The reviewer's concern about the ablation of the paper and the lack of clarity regarding the performance boost is not directly tied to any specific section or aspect of the paper. The reviewer's confusion about whether the performance boost is from the proposed alignment components or other adjustments is also not clearly linked to any particular part of the paper. This lack of specificity makes it difficult for the authors to identify which parts of the paper are related to the review point. As a result, the authors are left with uncertainty about which parts of the paper are being addressed by the review point. Therefore, based on the score rubric, the review point does not meet the criteria for specificity, and the authors would not be confident about which part of the paper is being addressed by the review point. So the score is -1."
4420,NIPS_2020_341,"- For theorem 5.1 and 5.2, is there a way to decouple the statement, i.e., separating out the optimization part and the generalization part? It would be clearer if one could give a uniform convergence guarantee first followed by how the optimization output can instantiate such uniform convergence. - In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.","- In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"The review point does not provide any direct or explicit actionable steps or comments. It merely states that the German and Law school dataset should have a shorter training time in Gerrymandering than Independent, without suggesting any specific actions or experiments that could be done to achieve this. Furthermore, the review point does not provide any implicit actionable comments that could be inferred from the text. It only mentions the main advantage of ERM and plug-in, which is their computation time, without suggesting any improvements or modifications to the code. Therefore, the review point does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor disrespectful, and it does not encourage or discourage the authors. The review point is focused on the technical aspects of the experiments and the potential implications of the shorter training time in Gerrymandering compared to the Independent dataset. It also mentions the importance of publishing the code, which is a factual statement rather than a polite suggestion. The review point does not contain any personal opinions or comments on the quality of the paper, which is a characteristic of a neutral tone. Therefore, the review point is in line with the score of 0, as it does not fit well with the other two scores.",-1,"The review point in question makes a claim about the training time for the German and Law school dataset in the context of Gerrymandering and the advantage of ERM over Kearns et al. However, it fails to provide any logical deductions or references to support this claim. The claim is not verified, and there is no justification or explanation provided to substantiate the claim. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. The claim is not supported by logical deductions or internal or external sources, which is a key requirement for a score of 1. Therefore, the review point is not aligned with the score rubric's criteria for verifiability. 

In contrast, a review point that would receive a score of 1 would contain a claim that is supported by logical deductions and internal or external sources. For example, a review point that critiques the statistical analysis in a paper by pointing out that the p-values reported do not match the standard thresholds for significance, and then citing the guidelines in 'Statistics for Biologists' (Smith et al., 2020) to support this claim, would meet the criteria for a score of 1. 

So, the review point in question does not meet the criteria for a score of 1 because it contains a claim that is not verified, and it lacks logical deductions and references to support the claim. 

Therefore, the score is -1.",1,"The review point provided is quite specific and directly addresses the training time for the German and Law school dataset in the context of Gerrymandering and the comparison with the Independent dataset. It also touches upon the performance of ERM and plug-in methods, which are relevant to the paper's content. The review point is clear and concise, making it easy for the authors to identify which parts of the paper are being addressed. The authors can be confident that the review point is focused on the specific aspects of the paper, such as the training time and the performance of the methods. Therefore, the review point meets the criteria for a score of 1, as it is specific and directly addresses the paper's content."
5664,NIPS_2018_185,"Weakness: ##The clarity of this paper is medium. Some important parts are vague or missing. 1) Temperature calibration: 1.a) It was not clear what is the procedure for temperature calibration. The paper only describes an equation, without mentioning how to apply it. Could the authors list the steps they took? 1.b) I had to read Guo 2017 to understand that T is optimized with respect to NLL on the validation set, and yet I am not sure the authors do the same. Is the temperature calibration is applied on the train set? The validation set (like Guo 2017)? The test set? 1.c) Guo clearly states that temperature calibration does not affect the prediction accuracy. This contradicts the results on Table 2 & 3, where DCN-T is worse than DCN. 1.d) About Eq (5) and Eq (7): Does it mean that we make temperature calibration twice? Once for source class, and another for target classes? 1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain? 2) Uncertainty Calibration From one point of view it looks like temperature calibration is independent of uncertainty calibration, with the regularization term H. However in lines 155-160 it appears that they are both are required to do uncertainty calibration. (2.a) This is confusing because the training regularization term (H) requires temperature calibration, yet temperature calibration is applied after training. Could the authors clarify this point? (2.b) Regarding H: Reducing the entropy, makes the predictions more confident. This is against the paper motivation to calibrate the networks since they are already over confident (lines 133-136). 3) Do the authors do uncertainty calibration on the (not-generalized) ZSL experiments (Table 2&3)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 4) Do the authors do temperature calibration on the generalized ZSL experiments (Table 4)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 5) The network structure: 5.a) Do the authors take the CNN image features as is, or do they incorporate an additional embedding layer? 5.b) What is the MLP architecture for embedding the semantic information? (number of layers / dimension / etc..) ##The paper ignores recent baselines from CVPR 2018 and CVPR 2017 (CVPR 2018 accepted papers were announced on March, and were available online). These baseline methods performance superceed the accuracy introduced in this paper. Some can be considered complementary to this work, but the paper canât simply ignore them. For example: Zhang, 2018: Zero-Shot Kernel Learning Xian, 2018: Feature Generating Networks for Zero-Shot Learning Arora, 2018: Generalized zero-shot learning via synthesized examples CVPR 2017: Zhang, 2017: Learning a Deep Embedding Model for Zero-Shot Learning  ## Title/abstract/intro is overselling The authors state that they introduce a new deep calibration network architecture. However, their contributions are a novel regularization term, and a temperature calibration scheme that is applied after training. I wouldnât consider a softmax layer as a novel network architecture. Alternatively, I would suggest emphasizing a different perspective: The approach in the paper can be considered as more general, and can be potentially applied to any ZSL framework that outputs a probability distribution. For example: Atzmon 2018: Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning Ba 2015: Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions  Other comments: It will make the paper stronger if there was an analysis that provides support for the uncertainty calibration claims in the generalized ZSL case, which is the focus of this paper. Introduction could be improved: The intro only motivates why (G)ZSL is important, which is great for new audience, but there is no interesting information for ZSL community. It can be useful to describe the main ideas in the intro. Also, confidence vs uncertainty, were only defined on section 3, while it was used in the abstract / intro. This was confusing. Related work: It is worth to mention Transductive ZSL approaches, which use unlabeled test data during training, and then discriminate this work from the transductive setting. For example: Tsai, 2017: Learning robust visual-semantic embeddings. Fu 2015: Transductive Multi-view Zero-Shot Learning I couldnât understand the meaning on lines 159, 160. Lines 174-179. Point is not clear. Sounds redundant. Fig 1 is not clear. I understand the motivation, but I couldnât understand Fig 1. ","1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely asks questions about the methodology used in the paper, without suggesting any specific actions or experiments that could be taken. The review point does not offer any guidance on how to improve the paper or how to apply the method to other models. It also does not provide any implicit actionable comments that could be inferred from the review point. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. It neither suggests direct actions nor provides implicit actions that could be inferred. Hence, the score is -1.",0,"The review point is presented in a neutral tone, which aligns with the score rubric's criteria for a score of 0. The reviewer's language is factual and does not convey any particular emotion or attitude. The reviewer asks a question about the methodology, which is a constructive approach to providing feedback. However, the reviewer does not offer any encouragement or positive reinforcement, which could have elevated the tone to a more helpful or constructive level. The reviewer's tone is more passive than active, which is why it does not score higher on the politeness scale. The reviewer's language is also more focused on the technical aspects of the paper rather than the overall quality or the potential impact of the research. This focus on the technical details rather than the broader implications of the research is why the reviewer's tone is more neutral than encouraging or constructive. So the overall score is 0.",0,"The review point in question does not contain any claims, but rather asks a series of questions regarding the process of temperature calibration after training a model. The questions are based on the assumption that temperature calibration is performed after hyper-parameter tuning, which is a reasonable assumption given the context of machine learning model training. However, the review point does not provide any verification or justification for this assumption, nor does it attempt to verify the claim that this method can be applied to other already trained models without the need to retrain.

The review point is more of a query than a claim, and it does not make any assertions that need to be verified. It is a factual statement that does not require any form of verification, as it is simply asking for clarification on a process. Therefore, according to the score rubric, the review point does not contain any claims that need to be verified, and it does not miss any justification, explanation, or reference.

In conclusion, the review point does not contain any claims that need to be verified, and it does not miss any justification, explanation, or reference. It is a factual statement that does not require any form of verification, and it is not relevant for verifiability in review point. So the score is 0.",1,"The review point in question is quite specific and directly addresses the methodology of the paper. It asks about the sequence of hyper-parameter tuning and the subsequent temperature calibration, which is a clear and focused question. The review point also inquires about the applicability of this method to other models, which is a relevant and specific aspect of the paper.

The authors of the paper would be able to identify the exact parts of the paper that are being addressed by the review point. They would be confident in their understanding of the review point, as it directly relates to the methodology and the potential for applying the method to other models.

The review point does not leave any ambiguity about the paper part being addressed. It is clear and specific, which aligns with the score rubric's criteria for a score of 1. The authors would be able to confidently answer the review point, demonstrating a clear understanding of the paper's methodology and its potential applications.

In conclusion, the review point is well-defined and directly addresses the methodology and potential applications of the paper, making it obvious which part of the paper is being addressed. The authors would be confident in their understanding of the review point, which is why it would receive a score of 1 according to the score rubric."
3447,NIPS_2020_486,"The method is relatively straightforward and some directions could be explored further. For instance, by using fully connected transformations, typical convolutional weight-sharing is not utilised. It would be good to discuss downsides to fully connected Woodbury transforms, and possible alternative formulations that would utilise convolutional weight-sharing. Further, the models utilised in the experimental section are quite small. As a result, the NLL performance is not very good compared to newer flow-based models. In addition the gains in NLL are quite small, and it would be better if the authors included standard deviations over multiple runs. Minor: - If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck. - For a better overview it would be nice to have a table showing the complexity for different methods for their forward/inverse/logdet in one place.","- If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,1,"The review point suggests including the ""changing bottleneck"" experiment in the main paper, which is a direct and explicit actionable step. This suggestion is clear and actionable, providing a specific experiment that the authors can implement. The review point also implicitly suggests that the authors should consider the implications of the experiment on the required size of the bottleneck. This implicit actionable comment is inferred from the review point's mention of the experiment's relation to the bottleneck size.

However, the review point does not provide any additional explicit or implicit actionable comments beyond the suggestion to include the ""changing bottleneck"" experiment. It does not mention any limitations or suggest any other experiments or steps that the authors could take.

Based on the score rubric, the review point gives one direct and explicit actionable comment, which is the suggestion to include the ""changing bottleneck"" experiment. It also includes an implicit actionable comment regarding the implications of the experiment on the bottleneck size. Therefore, the review point meets the criteria for a score of 1, as it provides direct and explicit actionable comments, and it also includes an implicit actionable comment.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer offers a suggestion to include a specific experiment, which is a helpful and encouraging piece of advice. The tone is respectful and the feedback is focused on improving the paper, rather than criticizing it. The reviewer's language is polite and the feedback is delivered in a way that is likely to be well-received by the authors. The reviewer's approach to providing feedback is in line with the desired tone of helpfulness and encouragement. Therefore, based on the score rubric, the review point is of high quality and is delivered in a tone that is helpful and encouraging.",0,"The review point provided does not contain any claims or make any assertions that require verification. It merely suggests the inclusion of a specific experiment in the main paper, without providing any justification or explanation as to why this experiment is important or how it relates to the required size of the bottleneck. The review point does not make any claims that could be verified or disproved, nor does it provide any logical deductions or references to external or internal sources. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability and thus would receive a score of 0.",1,"The review point provided is quite specific and directly addresses a particular aspect of the paper, which is the ""changing bottleneck"" experiment. This review point is clear and concise, making it easy for the authors to identify which part of the paper is being addressed. The authors can be confident that the review point is referring to the experiment related to the required size of the bottleneck. The review point is focused on a specific part of the paper, which is the experiment, and does not make any general statements about the paper as a whole. This specificity aligns with the score rubric's criteria for a score of 1, as it allows the authors to be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that lacks specificity would not provide clear guidance to the authors about which part of the paper is being addressed. The authors would be left guessing which parts of the paper are related to the review point, which would not be helpful for them to improve their paper. A review point that is too general or vague would not meet the criteria for a score of 1, as it would not allow the authors to be confident about which part of the paper is being addressed. 

Therefore, based on the score rubric's criteria for a score of 1, the review point provided is specific and directly addresses a particular aspect of the paper, making it easy for the authors to identify and address the issue."
3749,NIPS_2020_755,"Sometimes the presentation is dense: a table, for example, would be a more efficient way to compare the derived rates with past results. There are a few discoveries I wish the authors would discuss a bit more, including: - the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible. - showing the bias-variance decomposition explicitly, at least one, would be nice - can you explain why the kernel is redundant when beta=2 (line 204)? - Since the claimed lower bound is novel, can you explain what is new about the construction?","- the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that can be done. It merely points out the limitations of the paper without suggesting any specific actions that the authors should take. The review point does not provide any implicit actionable comments that the authors can infer, which is a key aspect of the score rubric. It also does not mention any limitations without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability. 

The review point does not suggest any improvements or changes that could be made to the paper, nor does it provide any guidance on how to address the limitations mentioned. It lacks the necessary detail and specificity that would allow the authors to understand what needs to be done to improve their work. 

In conclusion, the review point does not provide any actionable comments or suggestions, which is why it does not meet the criteria for actionability as outlined in the score rubric.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer's comments are focused on the content of the paper, without any personal attacks or overly positive remarks. The feedback is delivered in a manner that is neither rude nor overly respectful, which is consistent with the score of 0. The reviewer's comments are constructive, providing suggestions for improvement, but they do not go beyond the facts presented in the paper. The reviewer's tone is neutral, and the feedback is factual, which is why it fits the score of 0. 

The reviewer's comments are not overly critical or overly positive, which is why they do not fit the scores of 1 or -1. The reviewer's tone is not aggressive or disrespectful, and the feedback is not overly encouraging or overly discouraging. The reviewer's comments are focused on the content of the paper and provide a balanced view, which is why they fit the score of 0. 

In conclusion, the review point is neutral and factual, which is why it fits the score of 0 in the rubric. The tone is neither aggressive nor overly polite, and the feedback is focused on the content of the paper without any personal attacks or overly positive remarks.",-1,"The review point in question makes a claim about the generalization of ""adversarial noise"" in the context of machine learning. However, it fails to provide any form of verification or justification for this claim. The claim is made without any supporting evidence or logical deductions, which is a critical aspect of the score rubric. The review point does not mention any external or internal sources that could validate the claim, nor does it provide any explanation as to why the generalization is plausible. This lack of verification and justification is why the review point does not meet the criteria for a score of 1. Instead, it falls into the category of a score of -1, as it contains a claim that is not verified. Therefore, based on the score rubric, the review point does not meet the standards for verifiability.",-1,"The review point provided is quite vague and does not offer a clear direction for the authors to follow. It uses the term ""adversarial noise"" without explaining why this generalization is plausible, which leaves the authors uncertain about what exactly is being addressed. The review point does not specify which parts of the paper are related to the term ""adversarial noise,"" nor does it provide any guidance on how to improve these sections. This lack of specificity makes it difficult for the authors to identify which parts of the paper are being addressed by the review point. As a result, the authors are left with no idea about the paper part that is being addressed by the review point comment. Therefore, based on the score rubric, the review point does not meet the criteria for specificity and clarity."
1451,ICLR_2023_2368,"Weaknesses: 1. There is no theoretical guarantee that the discoveries resulting from the sparse network architecture are unique. 2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here. 3. It lacks model complexity analysis and comparison. Given different levels of biological entities' intra and inter interactions, I am worried about the model's real applicability.","2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely points out a missed description in the paper, which is the quantification of the top-k frequent interaction pairs. The review point does not suggest any specific actions or improvements that could be made to the paper. It also does not provide any implicit actionable comments that the authors could infer from the review point. The review point only mentions a limitation without giving any explicit or implicit actions that should be done. Therefore, based on the score rubric, the review point does not meet the criteria for actionability.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor overly polite, but rather straightforward and objective. The reviewer does not express any personal opinions or emotions, and instead focuses on the content of the paper. The feedback is factual and does not contain any negative or positive connotations, which is in line with the score of 0. The reviewer does not make any personal judgments about the quality of the paper or the authors' work, but rather points out the lack of detail in a specific section of the paper. The feedback is also constructive, as it suggests that the paper could be improved by providing more information on how to quantify the top-k frequent interaction pairs. This is a helpful suggestion that could potentially enhance the paper's quality. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores. So the overall score is 0.",-1,"The review point in question makes a claim about the lack of detail in the paper regarding the quantification of top-k frequent interaction pairs. However, it fails to provide a logical explanation or reference to external or internal sources that would verify this claim. The review point merely states that the paper missed this aspect without offering any justification or explanation. This lack of verification and the absence of logical deductions or references to support the claim result in a score of -1. The review point does not meet the criteria for verifiability as it does not contain any claims that are verified or supported by logical explanations or references.",1,"The review point provided is quite specific, pointing out a clear deficiency in the paper's description of how to quantify the top-k frequent interaction pairs. This specificity aligns with the score rubric's requirement for a review point to make it obvious which part of the paper is being addressed. The authors can confidently identify the section in question, which is section 5.4, and understand that the review point is critiquing the lack of detail in this section. The review point also directly addresses the paper's claim about novel discovery, indicating that the paper's results and discussions are limited and not fully developed. This specificity in the review point allows the authors to be confident about the part of the paper that is being addressed. Therefore, based on the score rubric, the review point is well-crafted and meets the criteria for a score of 1."
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point in question does not provide any direct or explicit actionable steps or comments. It merely points out a lack of clarity in the optimization process of auxiliary losses in the algorithm. The reviewer does not suggest any specific actions or experiments that could be taken to address this issue. Furthermore, the reviewer does not provide any implicit actionable comments that could be inferred from the review point. The lack of actionable content in the review point means that the authors are left without any guidance on what steps to take or what aspects to consider in their work. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0. The tone is not aggressive or disrespectful, nor is it overly polite or encouraging. The reviewer simply states a point of confusion regarding the optimization of auxiliary losses in the algorithm. The reviewer does not offer any constructive feedback or suggestions for improvement, which is a characteristic of a neutral tone. The reviewer's language is straightforward and does not contain any personal opinions or judgments, which further supports the neutrality of the tone. Therefore, based on the score rubric, the review point is neutral and factual, and it does not fit well with the other two scores.",0,"The review point in question does not make any claims, but rather poses a question about the optimization process in Algorithm Box 1. The reviewer is questioning the rationale behind optimizing each auxiliary loss separately rather than optimizing all of them at once. However, the review point does not provide any logical deductions or references to external or internal sources to support the claim. It is a factual statement that does not require verification, as it does not make any claims that could be verified or refuted. Therefore, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not contribute to the overall score. So the overall score is 0.",-1,"The review point in question is not specific enough to clearly identify which part of the paper is being addressed. The review point mentions the optimization of auxiliary losses in Algorithm Box 1 but does not provide any context or explanation as to why this is done separately. This lack of specificity leaves the authors unsure about which parts of the paper are being addressed by the review point. The review point does not guide the authors on how to improve or clarify the paper, nor does it provide any constructive feedback. It merely states a problem without offering any solution or direction. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point. 

In contrast, a review point that is specific would clearly identify the part of the paper that is being addressed and provide a clear direction for improvement. It would guide the authors on how to address the issue raised in the review point. For example, a review point that is specific might suggest that the authors should provide a rationale for the optimization of auxiliary losses, or that they should explain how the optimization of auxiliary losses contributes to the overall performance of the algorithm. 

In conclusion, the review point in question is not specific enough to guide the authors on how to improve the paper. It fails to provide any constructive feedback or direction for improvement. Therefore, the authors would not be confident about which part of the paper is being addressed by the review point."
2710,NIPS_2022_327,"Weaknesses
I think the paper is hard to follow and read, in particular there are a lot of statistical details and some of the machine learning details can be lost in there. There is excessive use of alternative notation, for example η
is used for the conditional density p(Y = 1 | X = x), I think it is better to use probabilistic notation directly and not introduce additional greek letters as makes the notation hard to follow. Same can be said for risk definitions.
Overall about readability, I think the paper should be rewritten to put the terms that are usable for implementation from a machine learning perspective (like binary and multi-class uncertainty components), and leave derivations and proofs for the supplementary. The audience of NeurIPS is largely machine learning people, and the paper notation and arguments could be simplified so it is easier to follow for this audience. The statistical details are important but after reading this paper, if I want to implement this method, to me it is not clear what parts I should use or directly how this method works conceptually.
- The results in the paper only present out of distribution detection results as main result, and classification with rejection on text datasets. There are additional results on the supplementary but I believe these are minor. Overall I think there are missed opportunities for evaluation, for example, calibration error of epistemic uncertainty (Figure 2 could be a start point).
- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.
- Since density estimation is computationally expensive, the authors resort to an approximation of the kernel density estimate with nearest neighbors in feature space. I think this is fine, but there is no evaluation of approximation quality or ablations on how to select an approximation for nearest neighbors and kernel density estimates, the authors just use HNSW for this purpose.
Minor issues
I think a diagram showing how the method works and its training/inference process would be an easy way to understand the overall proposed approach.
- I think the paper is missing references to the text datasets (CoLA, SST-2, and MRPC) and other datasets used in the text classification experiments.
- I am not sure what score is used for OOD detection, my guess is that it is the epistemic uncertainty U e
, and this should be explicit in the evaluation section.
As I mentioned in weaknesses, the paper does not mention that it uses an approximation for the kernel density estimate, and this could be a limitation that could be addressed in the future.
There are no other limitations mentioned in the paper about the proposed method, even as the checklist says the paper does discuss limitations (I do not see where, I might be wrong).
Overall I see no negative societal impact to discuss.","- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors could undertake. It merely points out a missed opportunity and suggests a comparison with a known baseline, without providing any specific guidance on how to improve the evaluation of the method. The review point also lacks any implicit actionable comments that could be inferred by the authors. It does not suggest any changes or improvements that could be made to the method or the evaluation process. Instead, it focuses on the limitations of the method without offering any solutions or recommendations. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and does not provide any actionable comments or suggestions. So the overall score is -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the importance of the topic and suggests a way to improve the evaluation of the method, which is a helpful and encouraging approach. The reviewer also recommends a comparison with a well-known baseline, which is a constructive suggestion for further research. The tone of the review point is respectful and focused on providing constructive feedback, which is in line with the score rubric's requirements for a score of 1. 

In contrast, the review point could have been more aggressive or disrespectful to score lower on the rubric. For example, the reviewer could have used more negative language or made more critical statements about the paper's content or quality. However, the reviewer maintains a polite and constructive tone throughout, which is why the score is 1. 

So the overall score is 1, as the review point is helpful, encouraging, and maintains a polite and constructive tone.",0,"The review point provided does not contain any claims that need to be verified. It merely suggests that the authors should evaluate the method's ability to disentangle aleatoric and epistemic uncertainty, and compare it with Kendall and Gall. 2017. However, it does not make any claims about the method's performance or the validity of the comparison. The review point also does not provide any logical deductions or references to external or internal sources that could verify the claims. Therefore, according to the score rubric, the review point does not meet the criteria for verifiability. It neither contains a claim nor verifies it. So the overall score is 0.",0,"The review point provided is quite specific, focusing on the disentanglement of aleatoric and epistemic uncertainty in the paper. It clearly identifies the part of the paper that is being addressed, which is the method's ability to separate these types of uncertainty. The review point also suggests a comparison with Kendall and Gall (2017), which is a well-known baseline for uncertainty disentanglement. This comparison would help in evaluating the effectiveness of the method in question.

However, the review point does not provide a detailed analysis of how the method disentangles the uncertainties, nor does it suggest specific experiments or methods to evaluate the method's performance. This lack of detail and specificity in the review point could lead to some ambiguity about which parts of the paper are being addressed.

Despite this, the review point does make it clear that the authors should consider evaluating the method's ability to separate uncertainties and compare it with a known baseline. This guidance is valuable and directly addresses the paper's main topic.

In conclusion, the review point is specific and addresses the main topic of the paper, but it lacks the detailed analysis and specific suggestions for evaluation that would make it fully confident. Therefore, the score is 0."
4202,NIPS_2020_630,"- more analysis on other network architectures are required. analyzing merely the ResNet and VGG family is not enough. the authors shall include results on other families such as DenseNet, GoogleLeNet, etc. - how do the authors estimate the local volume density with kNN (L116)? if one simply uses the region that contains k nearest neighbors to define density, it will have some issues. For instance, it will have a lot of discontinuities. Can the authors be more specific on this and provide more explanations? - I'm slightly confused about Fig. 2(a). the number of measurements for \chi^{l, l+1} is different to (more than) those of \chi^{gt} and \chi^{out}. what happened? there seems to be more variantions for \chi^{l, l+1} when the measurement scale is more fine-grained - does it apply to \chi^{gt} and \chi^{out} too? - the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?","- the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point provided does not offer any direct or explicit actionable steps or experiments that the authors should undertake. It merely suggests that the authors should show preliminary results on some of the directions mentioned, without specifying what those directions are or how the authors could go about demonstrating these results. The review point also lacks any implicit actionable comments that the authors could infer from the review. Instead, it focuses on the depth of the paper and the need for more solid evidence, without providing any guidance on how to achieve this. Therefore, based on the score rubric, the review point does not meet the criteria for actionability and thus would receive a score of -1.",1,"The review point provided is polite and constructive, which aligns with the score rubric's criteria for a score of 1. The reviewer acknowledges the potential of the observation and suggests that preliminary results could strengthen the submission. This approach is helpful and encouraging, which is the essence of a score of 1. The reviewer's tone is respectful and the feedback is focused on improving the paper, rather than criticizing it. The reviewer's language is also polite, using phrases like ""I totally agree"" and ""the depth of the paper can be improved a bit."" These phrases show a positive attitude towards the paper and a willingness to help the authors improve their work. The reviewer's feedback is also specific, suggesting that preliminary results on certain directions would make the submission more solid. This specificity is a key aspect of a constructive review. Therefore, based on the score rubric, the review point is indeed helpful and encouraging, and it is delivered in a polite and constructive tone.",0,"The review point provided does not contain any claims that need to be verified. It is a general commentary on the paper's depth and the need for preliminary results to support the claims made. The reviewer's opinion is based on the content of the paper and their own understanding of the subject matter. There are no logical deductions or references to external or internal sources that could be used to verify the claims. The review point is more of a critique rather than a verification of claims. Therefore, it does not meet the criteria for verifiability as outlined in the score rubric. 

The reviewer's feedback is focused on the need for more solid evidence to support the claims made in the paper. However, the feedback itself does not contain any claims that need to be verified. It is a subjective opinion that does not require any form of verification. 

In conclusion, the review point does not contain any claims that need to be verified, and it does not meet the criteria for verifiability as outlined in the score rubric. Therefore, the score is 0.",0,"The review point provided is somewhat vague and does not clearly identify which parts of the paper are being addressed. The reviewer suggests that the authors should show preliminary results on certain directions, but does not specify which directions or what kind of results would be helpful. This lack of specificity makes it difficult for the authors to understand exactly what needs to be improved or clarified in their paper.

The reviewer also mentions that the depth of the paper can be improved, but again, does not provide any concrete examples or suggestions on how to achieve this. This lack of specificity could leave the authors unsure about what aspects of their paper need attention.

In contrast, a review point that is more specific would clearly identify which parts of the paper are being addressed and provide clear guidance on how to improve those parts. For example, the reviewer could point out a specific section of the paper where the argument is weak or where the results are not clearly presented. This would allow the authors to focus their efforts on these specific areas.

Therefore, based on the score rubric, the review point provided is not as effective as it could be in guiding the authors to improve their paper. The lack of specificity in the review point makes it difficult for the authors to confidently identify which parts of the paper are being addressed. So the overall score is 0."
1686,ICLR_2023_1725,"Weakness: • This method utilize a DETR pretrained on COCO which limits the application of this method. For some common used inpainting dataset like CelebA, FFHQ, Places, Paris street View, LSUN, it is hard to acquire the corresponding segmentation map to train the DETR. • The author claims ‘COCO-panoptic is more challenging than center-aligned datasets’, but the author should prove the generalization of the proposed method on other domain datasets like face (FFHQ)、buildings (Paris Street View). • One important application of inpainting is object removing while this method aims for new object generation. What about the performance of the proposed method on the object removing? • I find in most figures (Fig 1, Fig 3-5, Fig 8-9), there exist an obvious color discrepancy between the ground truth and the results of this paper. Can you make an explanation? • When processing large missing area, does the segmentation completion network still has the ability to generate new objects or just inpaint with background contents? • How is the model complexity and inference time compared to other methods?",• How is the model complexity and inference time compared to other methods?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"The review point does not provide any direct or explicit actionable steps or experiments that can be done. It merely points out some limitations of the method without suggesting any specific actions that should be taken. For example, the review point mentions that the related work section is severely lacking and that the proposed method only achieves a little performance improvement with more parameters and variance. However, it does not suggest any concrete steps to improve these aspects. Similarly, the review point notes that the contribution of the paper is incremental, but it does not provide any guidance on how to make the contribution more substantial. Therefore, based on the score rubric, the review point does not give any hint, or direct actionable comments, and the authors still do not know or can not deduce what needs to be done. So the overall score is -1.",0,"The review point provided is neutral and factual, which aligns with the score of 0 in the rubric. The tone is neither aggressive nor encouraging, but rather a straightforward critique of the paper's methodology and structure. The reviewer's comments are focused on the technical aspects of the paper, such as the sampling of reference tokens and the inclusion of Section 6. The language used is polite and does not contain any disrespectful or aggressive remarks. However, the reviewer's comments could be perceived as slightly passive, as they do not offer any constructive suggestions or positive reinforcement. This could be improved by providing more detailed feedback and offering specific suggestions for improvement. So the overall score is 0.",0,"The review point provided does not contain any claims that need to be verified. It is a factual statement that does not require any justification or reference to external or internal sources. The review point simply states that the model complexity and inference time are compared to other methods, which is a straightforward factual statement. It does not make any claims that could be verified or invalidated. Therefore, according to the score rubric, the review point does not meet the criteria for a score of 1, as it does not contain any claims that need to be verified. It also does not meet the criteria for a score of -1, as it does not contain any unverified claims. The review point is neutral and does not make any claims that could be evaluated for verifiability. So the overall score is 0.",-1,"The review point provided is quite vague and does not clearly specify which part of the paper is being addressed. It lacks the specificity required to guide the authors in understanding which aspects of their work are being critiqued. The review point does not provide any concrete examples or suggestions for improvement, which would have been helpful in guiding the authors. Instead, it uses general language that could apply to any part of the paper, making it difficult for the authors to identify which parts are being discussed. This lack of specificity is why the review point would receive a score of -1 according to the score rubric. The authors would not be confident about which part of the paper is being addressed by the review point, and they would have no idea about the paper part that is being addressed by the review point comment."

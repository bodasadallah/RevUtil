review_id,source,focused_review,review_point,human_actionability,human_specificity,human_verifiability,human_politeness,Notes,llm_actionability,llm_politeness,llm_verifiability,llm_specificity
3412,NIPS_2020_1180,"Here are the following things that I think are missing from this work and should be addressed: 1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture. 2. Competitive Baselines: I carefully looked through the outputs of Self-Tuning [2] and the results are equally impressive. The quantitative analysis and the human studies also seemingly suggest that. Impressively, [2] runs on a CPU with 8 core and the proposed formulation requires a Tesla V100 GPU. I would also point to the quality of results synthesized using Texture CNN. One may, however, complain about the amount of time it takes to synthesize a new texture using this approach. 3. I have some reservation about the evaluations. Please see the next section for specific details.","1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture.",-1.0,-1.0,1.0,0.0,nan,-1,-1,1,-1
4916,NIPS_2020_1560,"The paper has a number of issues which are beyond the authors control. Most are caused by the extreme publication speed in the field and the limitations of the conference publishing model. Indeed all of my major concerns are of this type: 1. The use of ImageNet-V2 as primary example: The main issue I find in the paper is the use of ImageNet-V2 as the primary example for “natural distribution shift”. There is indeed a large gap between model performance on ImageNet and ImageNet-V2 but almost all of this can be attributed to subtle effects that arise in the dataset replication process as described by Engstrom et. al. 2020. Using this to criticize the paper is however unfair as Engstrom et. al. 2020 was published on 19. May 2020 only a week before the abstract deadline. ### Post-rebuttal comments: I did not know Shankar et. al., ICML ’20. Thanks for pointing out that reference. I have to look more deeply into this but judging from a quick read their results do indeed change my perception on the performance gap in ImageNet-V2. Nevertheless I think ObjectNet is the more obvious example and should be front and center. As Djolonga et. al. 2020 show it's the least correlated with other benchmarks with a very different design making it (in my current view) the most interesting of the selected benchmarks. 2. The definition of “distribution shifts arising in real data”: While the distribution shift from ImageNet to ImageNet-V2 has mostly been explained by Engstrom et. al. 2020 those to ObjetNet, ImageNet-vid-robust and YTBB-vid-robust can reasonably be expected to be real and existent. They do however only cover a subset of distribution shifts arising in real-world images. As to what is missing take for example ImageNet-R which was recently introduced by Hendrycks et. al. 2020. ImageNet-R shows real-world images not of the original objects but of different artistic renditions like paintings or sculptures. In this case some robustness interventions do have an effect. So the discussion of what constitutes a “natural distribution shift” has to be more nuanced. But as before this information was not available to the authors at time of submission because Hendrycks et. a. 2020 was published 3 weeks after the submission deadline. What turns this situation absurd is the fact that Hendrycks et. al. 2020 heavily builds upon what can readily be assumed to be an earlier version of the present article (not cited here to keep the double-blind mechanism as much intact as possible). ### Post-rebuttal comments: Thanks for including ImageNet-R even though it doesn't make the story easier. The dedication to completing the testbed is really amazing. 3. Big Transfer (BiT) models are missing from the analysis: The recently published Big Transfer model family (Kolesnikov et. al. 2019) was shown to have stunning generalization properties. The most interesting model of that family, BiT trained on the JFT300 dataset, has however not yet been publicly released. After seeing the L2-NoisyStudent model perform so well it would have been interesting to see if BiT-JFT can live up to it’s hype. Instead the authors of BiT have released their own robustness study using partly similar methodology as in the paper presented here (Djolonga et. al. 2020). This should not imply that Djolonga et. al. 2020 is biased or does something wrong but simply illustrate how fast paced the field has become. ### Post-rebuttal comments: Thanks for including BiT-M and reaching out to the authors. As I said above the commitment to completeness is great! 4. Too much information for 8 pages: It is pretty obvious that the amount of content presented in this paper is more than fits 8 pages in the NeurIPS template. I think the authors did a good job presenting their work in that format but when reading the paper it is still noticeable that there was much more content than could fit. It becomes even more obvious when reading the appendix which is full of exciting experiments that provide valuable information but have a good chance of being overlooked there. I want to repeat here that these problems are beyond the authors control. Most of it is caused by the huge amount of related work that was done in parallel and the conference submission system makes it impossible to publish longer papers or significantly update them during the review process. As a result I think it would be unfair to judge the submission based on these flaws. I would still appreciated if the authors could adapt their interpretations and related work prior to submission. Thus the following suggestions contain points regarding the above mentioned issues: 1. Use ObjectNet instead of ImageNet-V2 as the go to example (especially in Figures 1 & 5 as well as in Section 4) 2. Discuss different possibilities to select distribution shifts in real data in the introduction. State and motivate your choice. If I was asked I’d call them distribution shifts to ImageNet like images, as opposed to sketches, renditions, images with specific environmental factors like nighttime scenes or images taken in bad weather etc. which have a specific and easy to point out distribution shift. I think the second to last paragraph of the broader impact statement does a good job in justifying and contextualizing this approach and could be used here. 3. Follows thereof: Try to be a bit more specific as the chosen “natural” distribution shifts are just a subset of what is possible. The paper sometimes reads as if it covered all natural distribution shifts while it doesn’t (especially in section 1). Smaller suggestions: 4. Place “Dataset shifts” before “Consistency shifts” in section 3.1.1 or mention the video datasets in the first section 5. Change the description of “Image corruptions” in 3.1.2 removing the statement that you used corruptions from Geirhos et. al. 2019 which according to the appendix were not used and either don’t mention the number of corruptions (38) or explain why it’s 38 and not 19. I’d probably just remove that number as the nuanced discussion of “in memory” and “on disk” corruptions is only mentioned in the appendix. 6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”",6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”,1.0,1.0,0.0,0.0,nan,1,0,-1,1
1629,ICLR_2023_840,"Weaknesses:
Some important related works are missing: [1] tackles temporal causal discovery with Neural ODEs that would be able to handle inconistent sampling intervals, [2] performs joint structure learning and data imputation, [3] performs temporal causal discovery using the NOTEARS framework for continuous DAG learning. Methods based on the same framework have been applied to static data for joint causal discovery and data imputation [4]. All these weaken the novelty of this paper.
The paper mentions that it is based on Granger causality. However, the current formulation also allows for an interpretation as an additive noise model: ie x i = f ( p a i ) + e i
. Could you please comment on this? This interpretation would also allow for the identification of the temporal causal graph A 0 , τ
rather than just the summary graph A ^ = m a x t A t .
Please add some comments about the difference between Granger causality and ANMs or PCMCI that also identify the temporal causal graph.
It is unclear how τ m a x
is chosen. Is this assumed to be known? What if this isn't known?
All experiments use missing data. It would be great to see a baseline comparing to datasets with full observability. Misc:
What's the intuition of using the moving average as a training signal for the imputation network?
For the graph discovery stage - do you calculate an expectation over multiple graph samples or is this amortised over different batches? Or do you use the same graph sample for optimising this loss?
Please explain ZOH earlier in the text.
Eq 3: what is e
? What's the assumption about it? This might make or break the use of the L2 loss.
Please pay attention to the use of \citep and \citet.
Eq 5: You use inconsistent τ = 0. . . and τ = 1 , . . . .
p5 just above eq 11: I believe n 2
should be n 3 .
p2: "" to conduct causal inference and .."" - should this be ""causal discovery""? Causal inference tackles the question of inferring causal estimates (e.g. ATEs).
[1] Bellot, Alexis, Kim Branson, and Mihaela van der Schaar. ""Neural graphical modelling in continuous-time: consistency guarantees and algorithms."" International Conference on Learning Representations. 2021. [2] Morales-Alvarez, Pablo, et al. ""VICause: Simultaneous Missing Value Imputation and Causal Discovery with Groups."" arXiv preprint arXiv:2110.08223 (2021). [3] Pamfil, Roxana, et al. ""Dynotears: Structure learning from time-series data."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020. [4] Geffner, Tomas, et al. ""Deep End-to-end Causal Inference."" arXiv preprint arXiv:2202.02195 (2022).",... p5 just above eq 11: I believe n 2 should be n 3 .,1.0,1.0,0.0,0.0,nan,-1,-1,-1,1
1073,ICLR_2023_642,"Weaknesses:
Unclear notations. The authors used the same notations to write vectors and scalars. Reading these notations would be challenging to follow for many readers. Please consider updating your notations and refer to the notation section in the Formatting Instructions template for ICLR 23.
The framework impact is unclear. The authors mentioned that the case of intrinsic but known bias and variance is often the case in computational neuroscience and neuromorphic engineering. This is the main motivation for their approach. However, the framework provided is limited to specific cases, namely, white noise and fixed bias. The authors argue that their assumptions are reasonable for most cases computational scientists and neuromorphic engineers face, but they don’t provide evidence for their claims. Clearly, this framework provides an important way for analyzing methods such as perturbed gradient descent methods with Gaussian noise, but it’s unclear how it can help analyze other cases. This suggests that the framework is quite limited.
The authors need to show that their choices and assumption are still useful for computational neuroscience and neuromorphic engineering. This can happen by referring to contributing and important works from these fields having known bias and variance with Gaussian noise.
In the experiments, the used bias is restricted to having the same magnitude for all weights ( b 1 →
). Can we reproduce the results if we use arbitrary biases? It would be better if the authors tried a number of arbitrary biases and averaged the results.
The paper is not well-placed in the literature. The authors didn’t describe the related works fully (e.g., stochastic gradient Langevin dynamics). This makes the work novelty unclear since the authors didn’t mention how analyzing the gradient estimator was done in earlier works and how their contribution is discernible from the earlier contributions. Mentioning earlier contributions increases the quality of your work and makes it distinguishable from other work. Please also refer to my comment in the novelty section.
Missing evidence of some claims and missing details. Here, I mention a few:
It’s not clear how increasing the width and/or depth can lower the trace of the Hessian (Section 2.1). If this comes from a known result/theory, please mention it. Otherwise, please show how it lowers the trace.
The authors mentioned that they use an analytical and empirical framework that is agnostic to the actual learning rule. However, the framework is built on top of a specific learning rule. It’s unclear what is meant by agnostic in this context (Section 1).
The authors mentioned in the abstract that the ideal amount of variance depends on the size and sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. However, the authors didn’t mention the sparsity dependence anywhere in the paper.
The authors mentioned in a note after the proof of Theorem A.5 that it is also valid for Tanh but not Sigmoid. However, the proof assumed that the second derivative is zero. It’s unclear whether a similar derivation can be developed without this assumption. However, the authors only mention the relationship with the gain of ϕ ( . ) .
More information is needed on how the empirical likelihood of descent is computed (Fig. 7).
The use of MSE should be mentioned in Theorem A.3 since it’s not proven for any loss function. In addition, the dataset notation is wrong. It should be D = { ( x 1 , y 1 ) , . . . , ( x M , y M ) }
, where M
is the number of examples since it’s a set containing input-output pairs, not just a single pair.
The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead.
A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1).
The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work.
It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary.
The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30.
It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance.
The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results.
Alternative hypotheses can be made with some experiments. The experiment in Fig. 3.A needs improvement. The authors mention that excessive amounts of variance and/or bias can hinder learning performance. In Fig. 3. A, they only show levels of variance that help decrease loss. An alternative explanation from their figure is that by increasing the variance, the performance improves. This is not the case, of course, so I think the authors need to add more variance curves that hinder performance to avoid alternative interpretations.
Minor issues that didn’t impact the score:
There are nine arXiv references. If they are published, please add this information instead of citing arXiv.
What is a norm N
vector? Can you please add the definition to the paper?
You mentioned that the step size has to be very small. However, in Fig. 1, the step size used is large (0.02). Can you please explain why? Can this be an additional reason why there is no smooth relationship between the values of the variance and performance?
No error bars are added in Fig. 4 or Fig. 7. Can you please add them?
In experiments shown in Fig. 3 and Fig. 5, the number of runs used to create the error bars is not mentioned in Appendix B.2.
A missing 2 D
in Eq. 27.
In Theorem A.3 proof, how the input x
has two indices? The input is a vector, not a matrix. Moreover, shouldn’t ∑ k ( W k ( 2 ) ) 2 = 1 / d
, not d ?",".. , ( x M , y M ) } , where M is the number of examples since it’s a set containing input-output pairs, not just a single pair. The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead. A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1). The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work. It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary. The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30. It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance. The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results. Alternative hypotheses can be made with some experiments. The experiment in Fig.",1.0,1.0,1.0,1.0,"This is tricky, cause some comments lack some aspects, but others doesn't",1,-1,-1,-1
4133,NIPS_2020_73,"1. While I find the combination of single-image 3D reconstruction and GAN interesting, I am concerned about the technical contribution of the paper. It seems that each component is similar to previous works. The single-image 3D reconstruction network is almost identical to [24], and the GAN network also are standard. It feels like the contribution of the paper is just a combination of these two tasks. 2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network. 3. Why using the sinusoidal encoding in the network? How does it compare to directly using the (u, v) coordinates? Overall, I like the results of the paper. However, I am not fully convinced about the choice of the framework, particularly for the questions discussed in point 2. The technical contributions of the proposed method is also not significant to me.","2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network.",1.0,1.0,1.0,0.0,nan,1,0,-1,-1
1433,ICLR_2023_2880,"Weaknesses: 1. The first question is that the evidence of the motivation is not direct. Since the problem to be solved is that “a predictor suffers from the accuracy decline due to long-term and continuous usage”, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim. 2. Another question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match. 3. Is there any related work about the mixed-batch method?",3. Is there any related work about the mixed-batch method?,0.0,1.0,0.0,0.0,nan,0,0,-1,-1
531,ICLR_2022_1119,"weaknesses, starting from the most significant ones.
Assumptions and Threat Model? This is probably the only “true” problem of the paper, which should be absolutely rectified. I was not fully able to understand the assumptions made by Tesseract. Does it work “only” against the “directed deviation attack” proposed by Fang et al.? Or does it also protect against different attacks? In general, Section 2.2, Threat Model, is not very comprehensive. The authors should better expand this section by clearly pointing out all the assumptions and requirements of the proposed method. This is especially true because the Fang et al. attack was proposed in 2020, and some of its assumptions are not yet well-known. Specifically, this statement is suspicious: “We assume a full-knowledge (white-box) attack where the attackers have access to the current benign gradients.”. Does it mean that Tesseract only works under this assumption? I.e., the attacker knows, and exploits, the current benign gradients? This is a rather “unrealistic” assumption: I understand the willingness to work against “worst case” scenarios; yet, if such “worst case” scenarios are not realistic in the first place, then what is the purpose of the proposed mechanism? What benefit is there in protecting against an attack that will never happen in the first place? I invite the authors to restructure this section by using the common taxonomies adopted in adversarial ML papers [I].
Problem or Feature Space attacks? The authors perform their experiments on four well-known datasets: MNIST, CIFAR, Shakespeare, FEMNIST; for each dataset, a different (deep) ML model is targeted. Three of these datasets are of images, whereas Shakespeare contains text data. There are different ways to create “adversarial examples”, depending on the ‘space’ where the perturbation is applied. As far as I am aware, the adversarial examples considered in this paper to perform the poisoned updates are created in the feature space. It would be a lot more interesting if at least one evaluation included adversarial examples generated in the “problem” space [A]—or, at the very least, considered samples generated by “physically realizable” adversarial perturbations [B]. I acknowledge that the method should work even in these circumstances, as the proposed Tesseract defense is agnostic of the process used to apply the perturbation. However, considering the strong relationship with (real) security that permeates the paper, I believe that a more convincing use-case would dramatically improve the quality of the paper. This is also motivated by the current state-of-the-art: after almost a decade of adversarial attacks, more recent efforts are leaning towards evaluation that consider more realistic circumstances, where the attacker is constrained by the limitations of the real world; this is even more true in “distributed system” scenarios, such as Network Intrusion Detection Systems, which bear a strong relationship with federated learning (e.g., [C, D, E, F]). As such, I invite the authors to perform an additional “proof-of-concept” experiment where they consider adversaries with constrained capabilities. This is also motivated by the fact that some perturbations may yield different effects when created in the problem space (as shown in [A]).
Tradeoff? A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients. Even if the baseline performance does not decrease, what is the overhead of the proposed method? For instance, in Table 2 the authors report some results for “Attack=None”, which I assume represent the accuracy when no attack takes place. However, all the rows of these experiments (namely, FedSGD, Tesseract, Faba, FoolsGold, FLTrust) consider hardening FL techniques; for instance, on MNIST the proposed Tesseract has an accuracy of 92.52 when no attack takes place—the best among all other defences. Despite being appreciable, I am interested in knowing the performance when NO defense is applied. Surely, the test accuracy in a “fully trusted” FL setting should be superior than 92.52. Hence, I ask: what is the ‘cost’ of Tesseract?
Lack of a concrete use-case. I believe that the paper could be further improved with a concrete use-case, where the authors explain, step-by-step, how a (single, or multiple) attacker can compromise a federated learning system, and how the proposed method can help in solving such problem. Hence, I request the description of a concrete use-case explaining the abstract scenario reported in Figure 1. Such use-case can be at the basis of the “constrained” attack that I invite the authors to perform in my ""problem space perturbations"" suggestion.
Some additional issues:
• In the Introduction, the authors state: “To counter this threat, a set of approaches has been developed for countering Byzantine clients in FL…”. I believe that “Byzantine Clients” is a wrong term: what is countered by Tesseract are not byzantine clients, but ""unloyal"" clients, that are “against” the byzantine clients (at least by referring to the well-known problem of the byzantine generals, which should agree on a method to reach consensus in the presence of unloyal generals).
• The caption of Figure 1 has a typo “c out of m clients maybe be malicious”.
• In Figure, the gradient “LM_{c-1}” is out of place.
• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?
• Figure 1 appears before Figure 2, but in the text it is referenced after Figure 2.
• Putting Figure 2 so early on is very confusing. The “flip score” is a measure introduced for the first time in this paper. As such, any reader would be thrown off by such graphs before reading the paper, meaning that the findings of Figure 2 are difficult to interpret---during the Introduction---, as the flip score has not been defined yet. As such, such graphs are ultimately meaningless: I have to trust the authors that they correspond to “interesting” observations and “fair” experiments, which is not scientific.
• The presentation and notation in the “Flip-score” (page 5) is very ugly and difficult to follow.
• Section 5 should be merged in Section 6
• W.r.t. Table 2, the authors state “We see that TESSERACT is the winner or 2nd place finisher in 7 of the 12 cells (benign + two attacks * 4 datasets)”. This should be better highlighted. I only see three bold values for Tesseract in Table 2.
• W.r.t. Table 2, the authors state “We have not shown the test loss curve for Krum aggregation because of the large loss values.”. I invite the authors to report such values in Table 2, because the different “formats” of the three subtables (None, Full-Krum, Full-Trim) make this table very hard to interpret.
EXTERNAL REFERENCES
[A]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[B]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Conaml: Constrained adversarial machine learning for cyber-physical systems."" Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021.
[F]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.
[I]: ""Wild patterns: Ten years after the rise of adversarial machine learning."" Pattern Recognition 84 (2018): 317-331.","• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?",0.0,1.0,0.0,0.0,nan,0,0,-1,1
2329,ACL_2017_318_review.json,"Weaknesses: 1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted. 
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper. 
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings? 
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work. 
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs. 
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task. 
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"". 
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2? ","3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",1.0,1.0,-1.0,1.0,nan,-1,1,-1,-1
5163,NIPS_2020_1524,"Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer. 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense? 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion. 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this. 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues: - ""philosophical note"" paragraph seems a bit digressive. - Line 109 failed to use superscript? - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.","3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues:",-1.0,1.0,-1.0,0.0,nan,-1,-1,-1,-1
1904,ARR_2022_306_review,"1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. 
1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set. ","1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI?",0.0,1.0,0.0,0.0,nan,-1,-1,-1,-1
1972,ARR_2022_357_review,"- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. 
- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion ","- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.",-1.0,0.0,-1.0,-1.0,nan,-1,-1,-1,-1
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.",- A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance?,0.0,1.0,0.0,0.0,nan,-1,0,-1,-1
1232,ICLR_2023_3724,"Weaknesses:
The paper has low readability. A lot of the issues are certainly fixable but in its current form, it is confusing enough to distract from evaluating the technical contributions of the paper. Certain examples are:
“DeVAE surpasses 2% for β-TCVAE and 9% for β-VAE.” In what terms exactly?
Many instances in the introduction talking about spreading the conflict of disentanglement and reconstruction over time and space is not easy to follow and understand.
“However, in this work, we get rid of calculating TC by leveraging the narrow information bottleneck (Tishby et al., 1999; Burgess et al., 2018) to find efficient codes for representing the data, which promotes disentanglement.” This is pretty confusing right where it is in the introduction and only becomes somewhat clear after reading the method section.
The authors claim that DynamicVAE suffers from Information Diffusion problems. If that is the case, wouldn’t that result in low disentanglement scores or at least high variance across different seeds for DynamicVAE? But, that’s not the case in Figure 2.
The experiment for high-dimensional latent space is weak. 1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space?
A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set?
What are the layer embeddings? They seem to be an important component of the method but are never explained.
How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper?
Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.","1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space? A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set? What are the layer embeddings? They seem to be an important component of the method but are never explained. How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper? Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.",-1.0,1.0,-1.0,0.0,nan,-1,-1,-1,-1
2353,NIPS_2021_1788,"Weaknesses: - The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution. - Some parts of the paper need clearer writing (more below)
Comments and questions: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching. - More explanation is needed for Eq (5). For example, what is the meaning of the cost c 0 ( s , a )
? (e.g., to quantify out-of-distribution actions) - The use of s ′ and a ′
when defining $\hat{\pi}{\beta} a t l i n e 107 m i g h t c a u s e c o n f u s i o n a s \mathcal{D} c o n t a i n s
(s,a,r,s’)$. - This paper is about deriving a conservative estimate of the quantiles of the return from offline data where the conservativeness is for penalizing out-of-distribution actions. In the paper, they define OOD actions as those are not drawn from \hat{\pi}{\beta}(.|s) (line 109) but in Assumption 3.1. they assume that \hat{\pi}_{\beta}(a|s) > 0, i.e., there is no OOD actions. Thus, what is the merit of the theoretical result presented in the paper?
The authors have adequately addressed the limitations and social impact of their work.","- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution.",-1.0,1.0,1.0,-1.0,nan,-1,-1,-1,-1
3805,NIPS_2020_1707,"There are some concerns: 1. In line 82, authors should provide more explanations why they assumed linear constrains. How does it compare with non-linear combination in terms of performance and optimization speed. 2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability. 3. How does the cheap upsampling method (bicubic in the paper) influence the result? What is the limitations of upscaling factor, say will it fail if the factor is 8? 4. More comparisons and results from RAISR should be presented. 5. Experiments on image denoising and deblocking is very limited, lacking quantitative comparisons on benchmarks and intuitive explanation of this generalization.",2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability.,0.0,1.0,-1.0,0.0,nan,-1,-1,-1,-1
5651,NIPS_2018_195,"Weaknesses: - What is the time comparison of VCL relative to BatchNorm and having no normalization? - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently. - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed. - Line 177 typo: ""batchsized"" - It would aid a reader if the authors summarized the loss and how it is computed at the end of Section 2.2. - How sensitive is the framework to the choice of n? - How does \beta vary over time? Could the authors include a graph for this in the Appendix? Questions: - Will the authors be open-sourcing the code for the experiments? - Have you experimented with a constant \beta? - Have you experimented with having _both_ BN and VCL?  Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal. ",- Have you experimented with having _both_ BN and VCL? Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal.,0.0,0.0,0.0,0.0,nan,-1,1,-1,-1
5726,NIPS_2018_245,"Weakness] 1: Poor writing and annotations are a little hard to follow. 2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.  3: The motivation is to solve when the question doesn't focus on the most obvious visual concept when there are synonyms and homographs. However, from the experiment, it's hard to see whether this specific problem is solved or not. Although the number is better than the previous method, it will be great if the authors could product more experiments to show more about the question/motivation raised in the introduction.  4: Following 3, applying MLP after GCN is very common, and I'm not surprised that the performance will drop without MLP. The authors should show more ablation studies on performance when varying the number of facts retrieval, what happened if we different number of layer of GCN? ","2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.",-1.0,1.0,-1.0,-1.0,nan,-1,-1,NO_LABEL,-1
1656,ICLR_2023_1511,"Weakness_ - The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented). - Similarly, it is lacking a ""So What"" on the bounds provided, which are often just left there as final statements, without an analysis that explains whether 1) they are (likely to be) tight and 2) what this implies for practitioners. - Although well-written, the paper felt quite dense, even compared to other pure-math ML papers. More examples such as Figure 2 would help. - As far as I understood, the assumption on the non-linearities discards the sigmoid and the softmax, which are popular non-linearities. It would be good to acknowledge this directly by name.","- The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented).",1.0,-1.0,-1.0,-1.0,nan,-1,0,0,-1
3604,NIPS_2020_356,"1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community. 2. Evaluation conducted using 102 scenes from the simulated dataset. There is no mention of why or how these scenes were selected, whether they are complex enough, or whether they pose a significant challenge. Will their proposed method scale to any environment? Some statistics about the selected scenes are necessary. How big are the selected scenes? How many rooms? How many of each target object in each scene? List the scenes used in the appendix for reproducibility. --POST-REBUTTAL--- I am happy with the author's response on this. 3. Though great to have reported real world transfer, the experiments run seem to be minimal and the evaluation premature. --POST-REBUTTAL--- I am happy with the author's response on this. 4. There is overall lack of a deeper discussion about the importance and implications of this work for the wider research effort in autonomous agents and how the authors expect the proposed method to scale. In addition, the paper is overly focused on results from the datasets and challenge, with little interpretation of these results. 5. No standard deviations are reported in the quantitative results. ---POST-REBUTTAL--- The authors have not addressed this concern. I strongly believe that results without any notion of deviation or error (without a good reason for their absence) are not reliable. 6. I am not sure if I missed it, but I could not find an explanation of the “Random” baseline. Is this random at the level of the low level action (local policy) or random at the level of the long-term goal? Also, the distance to success values reported in Table 1 are not that much larger than that achieved by the proposed model. This is of course difficult to judge because of the lack of more scene information or results interpretation. --POST-REBUTTAL--- I am happy with the author's response on this. I understand now that this is indeed a challenging task - looking at the very low success rate of baselines.","1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community.",-1.0,-1.0,-1.0,0.0,nan,-1,1,-1,-1
2313,ACL_2017_657_review.json,"Weaknesses: one of the main weaknesses of the paper lies in the fact that the goals are not clear enough. One overall, ambitious goal put forward by the authors is to use approaches from experimental psychology to interpret LSTMs. 
However, no clear methodology to do so is presented in the paper. On the other hand, if the goal is to validate sociological assumptions, then one should do so by studying the relationships between gender markers and the written justifications, independently on any model. The claim that ""expected gender differences (are) a function of theories of gendered self-construal"" is not proven in the study.
- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up.
As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed. ","- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up. As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed.",1.0,1.0,1.0,1.0,nan,-1,-1,-1,-1
1569,ICLR_2023_303,"Weaknesses: 1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it? 2. Regarding the proposed hierarchical patch embedding, can authors give more explanation about the specifical design as 2 consecutive MLP with ratio 3, how authors reach this design. As this is a critical module proposed, can authors provide some ablation studies about it, e.g. why uses a different MLP ratio of 3 as latter stages uses 4 instead? Will allocate computations differently to 56x56, 28x28 influence the final performance? What computation percentage should be used in patch embedding v.s. latter stage (14x14) for better performance?","1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it?",1.0,1.0,1.0,1.0,nan,1,-1,-1,1
5633,NIPS_2018_197,"weakness of the paper: its clarity. From the presentation, it seems evident that the author is an expert in the field of computer algebra/algebraic geometry. It is my assumption that most members of the NIPS community will not have a strong background on this subject, me including. As a consequence, I found it very hard to follow Sect. 3. My impression was that the closer the manuscript comes to the core of algebraic geometry results, the less background was provided. In particular, I would have loved to see at least a proof idea or some more details/background on Thm. 3.1 and Cor. 3.2. Or maybe, the author could include one less example in the main text but show the entire derivation how to get from one concrete instance of A to right kernel B by manual computation? Also, for me the description in Sect. 2.4 was insufficient. As a constructive instruction, maybe drop one of the examples (R(del_t) / R[sigma_x]), but give some more background on the other? This problem of insufficient clarity cannot be explained by different backgrounds alone. In Sect. 3.2, the sentence ""They are implemented in various computer algebra systems, 174 e.g., Singular [8] and Macaulay2 [16] are two well-known open source systems."" appears twice (and also needs grammar checking). If the author could find a minimal non-trivial example (to me, this would be an example not including the previously considered linear differential operator examples) for which the author can show the entire computation in Sect. 3.2 or maybe show pseudo-code for some algorithms involving the Groebner basis, this would probably go a long way in the community. That being said, the paper's strengths are (to the best of this reviewer's knowledge) its originality and potential significance. The insight that Groebner bases can be used as a rich language to encode algebraic constraints and highlighting the connection to this vast background theory opens an entirely new approach in modelling capacities for Gaussian processes. I can easily imagine this work being the foundation for many physical/empirical-hybrid models in many engineering applications. I fully agree and applaud the rationale in lines 43-54! Crucially, the significance of this work will depend on whether this view will be adopted fast enough by the rest of the community which in turn depends on the clarity of the presentation. In conclusion: if I understood the paper correctly, I think the theory presented therein is highly original and significant, but in my opinion, the clarity should be improved significantly before acceptance, if this work should reach its full potential. However, if other reviewers have a different opinion on the level of necessary background material, I would even consider this work for oral presentation. Minor suggestions for improvements: - In line 75, the author writes that the ""mean function is used as regression model"" and this is how the author uses GPs throughout. However, in practice the (posterior) covariance is also considered as ""measure of uncertainty"". It would be insightful, if the author could find a way to visualize this for one or two of the examples the author considers, e.g., by drawing from the posterior process. - I am not familiar with the literature: all the considerations in this paper should also be applicable to kernel (ridge) regression, no? Maybe this could also be presented in the 'language of kernel interpolation/smoothing' as well? - I am uncertain about the author's reasoning on line 103. Does the author want to express that the mean is a sample from the GP? But the mean is not a sample from the GP with probability 1. Generally, there seems to be some inconsistency with the (algebraic) GP object and samples from said object. - The comment on line 158 ""This did not lead to practical problems, yet."" is very ominous. Would we even expect any problem? If not, I would argue you can drop it entirely. - I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.","- I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.",1.0,1.0,0.0,1.0,nan,1,1,-1,-1
3433,NIPS_2020_1241,"1. The analysis on neural networks are direct results induced from the kernel method results. It may not be tight, and can even suffer from the curse of dimensionality. Hence, the different between two types of models showed in the theorems may not fully characterize the actual performance difference. Neural networks can possibly perform much better than the bound. 2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?","2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?",1.0,1.0,-1.0,1.0,nan,-1,1,-1,-1
796,ICLR_2021_2043,"Weaknesses: - The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.
- Clarify the difference between S-Full and S-PGD from Experiments section. Since S-Full also uses T-step PGD, how it is different than S-PGD? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric, the performance gains are very minimal and not very significant than the baseline OE (Hendrycks et al., 2019b) under two metrics AUROC and AUPR.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on adversarial robustness is significant, there are critical points yet to be addressed. Therefore, I marginally accept this paper.
Post rebuttal: The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper.","- The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.",1.0,1.0,1.0,1.0,nan,1,1,-1,-1
3455,NIPS_2020_725,"- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks. - One particular concern is that in this setup, as shown Figure 4a, a strong Hebbian learning rule leads to a much faster convergence than gradient descent, presumably because Hebbian learning quickly memorizes the training set. It might very well be that Hebbian learning leads to more progressive differentiation in tasks where it results in equal or slower convergence than gradient descent.",- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks.,-1.0,-1.0,1.0,0.0,nan,-1,0,-1,-1
3000,NIPS_2022_1200,"Weakness: Originality:
1.I want to know if this paper is the first time to study the problem of the robust collaborative inference, where there are both arbitrary agents and adversarial agents. The arbitrary agents are easy to identify. However, I’m afraid the proposed method achieves a similar performance to identify the adversarial agents compared with baselines.
From Eq.(5), the framework aims to find a combined feature l
which is on the manifold and is near h
. The manifold projection could get a similar results for the adversarial sub-features. Could the authors discuss more about it? Writtings:
1.After so many times of reading, I guess I understand this paper. The authors introduce their method in Section 2.3, which is very simple. However, it relies block-sparse structure which is detailed stated in Section4. This could cause confuse when understanding the proposed method.
2.The notations are confusing. For example, h and l
both denote the feature. Why not use a letter (or with its variants)?
the citation format may be ICLR rather NeurIPS.
Theoretical analysis:
1.This paper provides an extensive theoretical analysis. In fact, I suggest the authors discuss more what the analysis means. Compared with baselines, why CoPur could do better.
2.Could the authors give an intuitive explanation about the effect of the sparsity α
on CoPur? Experiments:
1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c
and different Ω a d v ?
2.More analysis is helpful, for example, The comparison on optimization efficiency.
The authors discuss the limitations in Appendix. I have no other suggestions.","1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c and different Ω a d v ?",0.0,1.0,0.0,0.0,nan,-1,0,-1,-1
1170,ICLR_2023_3572,"Weakness: 1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body. 2. Figure 3 is the most important picture in the article ,but it is confusing. Although the style of the figure is nice, it does not help me understand PRG. There is no need to place a black box on (1,1).","1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body.",-1.0,1.0,1.0,0.0,nan,-1,1,-1,1
5211,NIPS_2020_1428,"- Novelty: This work can appear as incrementally innovative since Sobolev descent and unbalanced optimal transport were already present in the literature. This work is essentially the merging of these two existing works, resulting however in a new contribution. - Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.",- Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.,-1.0,-1.0,-1.0,-1.0,nan,-1,-1,-1,-1
3081,NIPS_2017_330,"Weaknesses
- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.
- Issues of convergence, especially when applying gradient descent over a non-Euclidean space, is not addressed
In all, a rather thorough paper that derives an efficient way to compute gradients for optimization on LDSs modeled using extended subspaces and kernel-based similarity. At one hand, this leads to improvements over some competing methods. Yet, at its core, the paper avoids handling of the harder topics including convergence and any analysis of the proposed optimization scheme. None the less, the derivation of the gradient computations is interesting by itself. Hence, my recommendation. ",- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.,1.0,1.0,-1.0,0.0,nan,-1,1,0,-1
85,ICLR_2022_3099,"Weaknesses
W1: The setting seems to be limited and not well justified. 1) It only consider ONE truck and ONE drone. Would it be easy to extend to multiple trucks and drones? This seems to be a more interesting and practical setting. 2) What is the difference of this setting versus settings where there are multiple trucks? Are there methods solving this setting, and why are they not working in TSP-D? 3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.
W2: It is not clear why exactly an LSTM-decoder is better than an attention-based decoder. The paper justifies that ""AM loses its strong competency in routing multiple vehicles in coordination"". However, AM decoder still conditions ""on the current location of the vehicle and the current state of nodes"". Thus, I don't think it overlooks the interaction between different vehicles. It depends more on how you design the decoder. Compared to attention, an LSTM essentially adds to the historical decisions to the policy, not the interactions between vehicles. Therefore, it is not clear why exactly LSTM-decoder is better, and the justification is quite vague in the paper.
W3: Except for AM, NM by Nazari et al. (2018) has also been an important counterpart of the proposed HM. However, it is not compared as a baseline. Whereas I understand that not every baseline should be compared, but NM is mentioned a few times throughout. If historical information is important in decoding an action, why is it not important in encoding a state? Because of this, the empirical evaluation is not totally convincing to me.","3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.",0.0,1.0,1.0,1.0,nan,-1,-1,-1,1
587,ICLR_2022_1057,"Weakness:
Experiments: 1. Why experiments do not contained the same as RIS? Just for completeness to show that you do better than RIS on their experiments. 2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?",2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?,1.0,1.0,1.0,0.0,nan,-1,-1,-1,-1
1723,ICLR_2023_2202,"Weakness: 1. The results are not stunning. The multi-task training does not lead to consistently better performance on all graph-structure prediction tasks. The performance improvement on downstream tasks is not effective. 2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.","2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.",1.0,1.0,-1.0,0.0,nan,-1,-1,-1,-1
2250,ACL_2017_104_review.json,"Weaknesses: - Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion: ",- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.,-1.0,1.0,-1.0,0.0,nan,-1,0,-1,-1
2803,NIPS_2022_1523,"Weakness:
1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data.
[1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.
2 Unclear model design: The model architecture and learning details are fragmented or missing. The authors could either provide a plot of model illustration, pseudo-code table, or code repository. Considering that Neurochaos Learning is not a well-known method, it is important to demonstrate integrated details to facilitate reproductivity.
3 Experimental design: The experiments regarding Coupled autoregressive (AR) processes and Coupled 1D chaotic maps etc. don't seem to be well-motivated. Could the authors reason why particularly using such setting to investigate cause-effect of time series. Lastly, the comparison to a five-layer neural network seems to be less convincing, given the rapid developments of deep learning architectures.
Yes, the authors fairly discussed the limitations of the method. The potential negative impact may not be applicable to this study.","1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data. [1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.",1.0,1.0,1.0,1.0,nan,1,1,1,1
5224,NIPS_2020_1363,"- Even if MLE was better than DuetRL in objective evaluations, it is not a reason to leave DuetRL out of subjective evaluations. - The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references. - The quality and reusability of Chinese folk music transcriptions are not discussed. I think that the dataset is fine however, the usage of this dataset has to be justified by the authors, at least part of the supplementary. See the next three comments below: 1) Folk music is transmitted orally; scores are transcriptions into a single melody. These transcriptions do not typically include heterophony and other stylistic features like glissandi, ornaments, dynamics contrasts, etc. that characterize the folk song genre or style. The authors should discuss the limitations of the lack of performance elements in the transcriptions. 2) Another important aspect is the collection process and purpose of the transcriptions and if they are suitable for music generation? Given that the data is retrieved from Essen folk collection, is it possible that European scholars who prepared transcriptions with a Western classical music perspective and without adequate understanding of the characteristics of Chinese folk music? Are we sure that the transcription process retained the melodic characteristics? Such transcriptions are sometimes useful for descriptive reasons from a ""Western"" point of view, and they are not intended for prescriptive work, e.g., to be used in performance practice. (https://www.amherst.edu/system/files/media/1770/Seeger%252520-%252520Prescriptive%252520and%252520Descriptive%252520Music-Writing.pdf). There is no discussion if these transcriptions are re-usable for rendering music. To the authors' defense, I explored the dataset and Essen archives for more evidence (https://wiki.ccarh.org/wiki/EsAC) and consulted an ethnomusicologist specializing in Chinese music to validate the dataset. The response was that the transcriptions sound OK albeit that lack the performance elements as described above. Nevertheless, - since it is going to be the first paper on the topic - the authors need to include these discussions as part of the study (paper or the supplementary). 3) There are many different Chinese folk music traditions, which significantly differ in characteristics due to region, ethnicity, and era. This is not acknowledged by the authors, except in Section 4.1: ""It comprises 2250 traditional Chinese folk songs, from Han, Natmin, Shanxi, and Xinhua origins."" However, the origins are confusing: Han is an ethnicity; Shanxi is a province; Xinhua is a small county or the official state-run press agency, and the transcriptions listed under Xinhua are from all over China and modern-day Mongolia; I am not sure what Natmin is. This is a sloppy explanation from the authors' side.","- The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references.",1.0,1.0,1.0,0.0,nan,-1,-1,-1,1
2656,NIPS_2019_346,"weakness of the paper is a lack of theoretical results on the proposed methodology. Most of the benefits of the new model have been demonstrated by simulations. It would be very helpful if the authors could provide some theoretical insights on the relation between the model parameters and the tail dependence measures, and on the performance (consistency, efficiency etc) of the parameter estimators. Itemized comments: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired. 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2. 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general. 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful. 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.","1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired.",1.0,1.0,0.0,0.0,nan,-1,-1,-1,-1
3138,NIPS_2017_15,"WEAKNESS AND CONCONERN
   	1) As already mentioned in previous section, the description lacks certain levels of details for complete reproduction, for example, how is the physics engine implemented, it's understandable that the authors left out some details with proper reference, however it is not very clear, in paricular as physics engine is an important component in the system, how the engine is set up and the set up affect the results.
   	2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.
   	3) The authors should also cite recent work on frame prediction. this is very related.","2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.",1.0,1.0,1.0,0.0,nan,-1,1,-1,-1
3611,NIPS_2020_878,"In short, the paper misses some critical details and some explanations are not clear. After reading the paper and the supplementary materials, I still believe it is hard to reproduce their results. - The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem. - The meaning of the shaded regions of Figure 2 (right), Figure 5 (left), and Figure 6 (left) is never explained in the caption or main text. Morevoer, it neighor represents standard deviation or the range between the best and worst case because the line with the same color can go outside the shaded region. For example, the solid red line in Figure 5 below the red shaded region between 60 and 90 trained models. - In the equation after line 156: ""\sigma ( A H^l W H^l)"" should be ""\sigma( A H^l W )"". Is this a typo? Otherwise, this is not a graph convolution layer. I have low tolerance with a typo in the ""only one"" equation in a paper. - It is not clear how the adjacency matrix A is defined in line 155. A_ij is the edge of node i to node j or node j to node i? Although I can figure it out from Figure S1 in supplementary materials, it is not clear in the main paper. - Following the previously point, based on Figure S1, the graph convolution operation propogates the features ""backward"" in the graph (in the opposite direction of an edge). I wonder if the authors can explain why you made this decision instead of adding edges for each node to the ""global"" node and following the edge direction to do graph convolution? - The original GCN (Kipf and Welling) uses normalized adajancy matrix with renormalization trick. However, this paper uses the original adjacency matrix without explantation or ablation study. I wonder if there is a reason behind not using the conventional graph convoltuional operation.","- The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem.",-1.0,1.0,1.0,0.0,nan,-1,0,-1,NO_LABEL
225,ICLR_2022_2575,"Weakness:
1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?
2). 80.0% top-1 accuracy on ImageNet is not hard. Does the design principle can scale well across different (data size, model parameters, FLOPs)?
3). I don't know which of the proposed components contribute the most to the overall performance. What can be concluded from Appendix Figure 7 and Figure 8?","1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?",0.0,1.0,0.0,0.0,nan,0,-1,-1,-1
2468,NIPS_2021_783,"Weaknesses: 1. The introduction of the knowledge coefficient matrix may raise the concern of privacy leaks and the communication cost. 2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting. 3. To my best knowledge, there are some other personalized FL works (Hierarchical Personalized Federated Learning for User Modeling, WWW'21; Exploiting Shared Representations for Personalized Federated, arXiv:2102.07078), which need to be listed as baselines.",2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting.,-1.0,1.0,1.0,0.0,nan,-1,0,-1,-1
5707,NIPS_2018_845,"Weakness - Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks. - One of the main advantages from the simulated random-walks is parallelization (at least for sequence generation) and scalability. Section 3.5 is not enough for scalability argument.  * Detailed comments - Due to [28] and the attention usage in the language models, the attention is generally regarded per instance (or sentence/sequence) while here the attention is globally defined. The proposed method has its own value as the authors describe in the paper, but the terminology can lead to some confusion. - Some references such as [16] need to be polished. - Section 3.5 seems somewhat contradictory to the original purpose of trying the graph embedding because it will cut out the information that SVD cannot preserve. Also, whether Section 3.5 is just an idea or it is actually implemented to be used in experiments is not clear. It would be interesting to see the difference in performance between full-matrix context and SVD-approximate context. - Section 3.6 seems more promising to generalize the proposed idea beyond the simple random-walks. - While the authors sweep C for node2vec, the other hyperparameters are not mentioned where it is hard to believe that the default values of the other hyperparameters work best across the datasets. Mentioning the detail about C without mentioning the others naturally raise those questions, so the authors need to add some description.","- Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks.",-1.0,1.0,1.0,0.0,nan,-1,0,-1,1
5813,NIPS_2016_95,"Weaknesses 1. The time complexity of the learning algorithm should be explicitly estimated to proof the scalability properties. 2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained. ","2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained.",1.0,1.0,1.0,1.0,nan,0,0,-1,1
4583,NIPS_2020_1678,"1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested. 2. I am not an expert in the are, but my impression is that the novelty of the work is somewhat limited. In particular the novelty is mainly to refine the framework of Dai et al. [4] on the particular problem, and to introduce the components of noise predictor and importance sampling for scalability.","1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested.",-1.0,-1.0,1.0,0.0,nan,-1,-1,-1,-1
4000,NIPS_2020_1878,"- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors? - Authors DBSCAN++ show cases where DBSCAN++ is even providing better clustering partition than for DBSCAN. Could the authors perform a similar analysis here with SNG-DBSCAN? - DBSCAN++ is also applied to outlier detection. How is performing SNG-DBSCAN compared to DBSCAN++ in this context? - Which parameters are used with DBSCAN for time and space consumption comparison? - Would it be possible to have empirical results on datasets for which DBSCAN is actually working? It seems that DBSCAN is performing well only for Still. Hence DBSCAN and SNG-DBSCAN are equally performing bad on the remaining sets...","- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors?",1.0,1.0,1.0,1.0,nan,1,-1,-1,1
4808,NIPS_2020_1318,"1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space). 2. In a similar vein, I was envisioning a simple extension of SG-MCMC methods where we just do occasional random walks/Brownian motion in the original space (using Gaussian noise and ignoring the energies entirely) using some relatively cheap heuristic to detect if the fancier iterate sequence is stuck in local optima. Something like that would help explore the entire space (triggering some iterations of random walk if heuristic flags a local optima) without changing the underlying energy used in the real Sg-MCMC at each iteration. Wonder how that would compare to a method like the one in the paper. 3. Following from these, it would have been nice if the authors included synthetic experiments with asymmetric modes (one small mode to one side of a 2d space and one taller one on the other side) where the real mean would not be close to the 'center' and thus would not be easily approximable by a flatter version of the energy. Comparisons with something like the approach in 2. above would help understand the main insight of the paper better. 4. Runtime plots in the experiment sections on real data would be nice to have, for example plots of energy/error vs wall clock time / iteration count as provided for the synthetics. 5. Some insight on how to choose \delta_{u} and partition count for real datasets (like the values mentioned on line 173 for synthetic) would be nice to have.","1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space).",-1.0,1.0,1.0,0.0,nan,-1,0,-1,1
1482,ICLR_2023_3023,"Weaknesses: 
 1. The proposed technique does not seem to predict uncertainty. To the best
  of my understanding, it predicts where the original model is making
  errors. An easy way to see this would be to consider a point where the
  model is confident but inaccurate, i.e., a bias. The predicted mask
  should cover this point, even though it is not an uncertain prediction
  of the model. The problem is actually in the definition of
  ``uncertainty''. Equation 1 (or 3) do not necessarily only correspond to
  uncertainty. It corresponds to expected error for a sample. It also
  includes the contribution of bias. In this regard, I think the
  uncertainty positioning may not be very accurate. Consequently, the
  comparison with a method that computes confidence intervals may not be
  appropriate. Here, I should note that predicting expected error or the error is not a
  bad target. However, the difference between this and uncertainty
  estimation should probably be made clear. 
 2. I am not sure about the contribution of the corollary 1. The result is
  not very surprising in my opinion. The mask that aims to minimize the
  ``uncertainty'' is bound to be related to the ``uncertainty''. 
 3. It is unclear how authors estimate the expectation in Equations 4
  or 5. This is over y x variable. However, in Equation 6, they drop the
  expectation and simply take only one sample to compute this
  expectation. Using this I am assuming the model learns to predict the
  error and not the expected error. As a result, the model may not be able
  to identify an uncertain prediction at a pixel for a given sample since
  for that pixel, there may be only 1 output that happens to be close to
  the ground truth. 
 4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0,
  m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} =
  \infty. It is unclear how authors deal with this. Furthermore, the
  intuition of this specific calibration form is unclear. ","4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0, m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} = \infty. It is unclear how authors deal with this. Furthermore, the intuition of this specific calibration form is unclear.",-1.0,1.0,1.0,0.0,nan,-1,-1,-1,-1
4527,NIPS_2020_84,"- I think the definition of ""near convex"" is quite restrictive towards the necessities of the sensitivity framework. However, it is presented as a very general and natural class of functions. The limitations are not clearly discussed in my opinion. - related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.",- related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.,-1.0,1.0,0.0,0.0,nan,-1,0,-1,-1
2141,ARR_2022_210_review,"1) Improvements are overstated. The abstract mentions "" two-fold improvements"" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?
2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.
There are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as ""woman"" with prob 2/100, ""king"" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.
3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.
4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?
5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. 
There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.
The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.
Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.
Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.
- Line 195: plane -> hyperplane - Figure 1 caption: middle -> midpoint - Line 1453: ""the tanh removes the magnitude of facets"" -> Could you explain this in more detail? I could not make sense of why tanh ""removes the magnitude"" - what do you mean specifically by ""magnitude"" here?
- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).
- There is a mistake in rows ""MFS - Multi partition"" and ""MFS - Multi Input"" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.
Rephrasing: Line 063: ""output the woman or king."" - > ""output woman or king"" Line 121: ""is sometimes not be able"" -> ""is sometimes not able"" Line 1271: ""the GPT-2"" -> ""GPT-2"" Line 235: ""multi-mode"" -> ""multimodal"" also line 263. 
Line 911: ""As in section 5"" -> ""As in Section 5"" ","5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger. The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful. Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format. Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.",1.0,1.0,1.0,1.0,nan,-1,-1,-1,1
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K.",-1.0,1.0,0.0,0.0,nan,-1,0,-1,-1
2707,NIPS_2022_776,"Weakness:
1.The technique contribution is limited. This paper mainly uses two existing techniques, DP-SGD and gradient matching.
2.This work only uses one algorithm from data condensation, i.e., gradient matching. It would be better if the authors can try more algorithms so the community can have a better understanding about data condensation for differentially private data generation. For example, distribution matching [1] that minimizes the distance between the averaged feature of real data and the averaged feature of synthetic data, which is also easy to implement with DP.
3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?
4.(Minor) Line 121, minimized -> minimize.
[1]: DATASET CONDENSATION WITH DISTRIBUTION MATCHING, https://arxiv.org/pdf/2110.04181v1.pdf.",3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?,1.0,1.0,0.0,0.0,nan,-1,0,-1,1
1885,ARR_2022_24_review,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. 
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ","- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy.",1.0,1.0,0.0,0.0,nan,-1,0,-1,-1
5162,NIPS_2020_1750,"- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence. - Choice of methodology: 3.1: The proposed experimental methodology is poorly justified. The pruning methodology (3.1) does not seem to directly correspond to previous approaches which have found found that pruning increases generalization. The experiments are ran on two unconventional networks on CIFAR-10, using unconventional pruning schedules. These networks reach much lower accuracy than expected for CIFAR-10 (85%-87% test accuracy), possibly due to the fact that these networks are relatively overparameterized for CIFAR-10 (a CIFAR-10 ResNet-20 has 0.27M parameters and typically reaches about 91% test accuracy; the ResNet-18 in the paper has 11.5M parameters, and the VGG-11 has about 110M) and L1/L2 regularization is disabled. It is therefore hard to extrapolate these results beyond the two CIFAR-10 networks and their bespoke hyperparameters chosen in the paper. - 3.3: the choice of duration of holding parameters at zero, and the conclusions drawn from it, don't seem justified from the data: the drop in test accuracy from ""Zeroing 50"" does not seem to be the same as the drop in accuracy from ""Prune_L"", and it is unsurprising to see that any regularization technique improves test accuracy for these heavily overparameterized CIFAR-10 networks, so the claim that ""pruning-based generalization improvements in overparameterized DNNs do not require the model’s parameter count to be reduced."" (lines 232-233) is hard to extract from just this experiment on these networks. Overall: the work would be significantly strengthened by having much more well-justified and clearly presented methodology for the networks that are used, the experiments that are performed, and the conclusions that are drawn from those experiments. # UPDATE AFTER AUTHOR RESPONSE The fact that the results seem to hold almost exactly as strongly on the ResNet-20 as they do on the ResNet-18 seems to imply that the results are not just equivalent to adding regularization into an over-parameterized and under-regularized network, and do in fact provide some effect when applied alongside standard regularization (as opposed to my suspicion that the effect would disappear or even reverse when applied alongside standard regularization). I do still think that there is more work to be done on that front (making more precise exactly the relationship between regularization and this effect), but the rebuttal did satisfactorily address my main concern with the paper. Regarding the addition of Kendall Tau, I am satisfied that they do show a correlation, even with the relatively noisy data. Overall, I'm raising my score to a 6. I still think the paper would be improved by a more thorough discussion of the relationship to other more standard forms of regularization, and I'm hesitant to trust that the methodology will be made substantially more clear without seeing a revised version of the paper, but enough of my concerns were addressed by the experiment on the standard ResNet-20 that I no longer see a strong reason to reject the paper.","- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence.",1.0,1.0,1.0,0.0,nan,-1,-1,-1,1
1486,ICLR_2023_3777,"Weakness
1: The technical novelty is limited and some details are confusing.
Slimmable networks are a special case of widely studied one-shot NAS (e.g., [R2, R3, R4, R5, R6]), which only considers the width dimension (see discussion in OFA [R2]). There are many techniques to deal with interference among subnetworks. Specifically,
“Slow start” belongs to progressive training in one-shot NAS. For example, OFA proposes a “Progressive Shrinking” strategy, which starts with training the largest sub-network and then progressively fine-tunes the network to support smaller sub-networks by gradually adding them into the sampling space.
“Online distillation” was originally proposed in US-Nets [Yu et al., 2019b]. Apart from the inplace distillation, it also proposes the sandwich rule.
“loss reweights” aims to assign larger weights for sub-networks with large widths. However, it violates the training objective of slimmable networks. The objective is to make each supported sub-network maintain the same level of accuracy as independently training a network with the same architectural configuration, rather than only training an accurate large “supernet”. This is evidenced in Table 2 (e), where adding loss reweighting makes R-50(0.25) perform worse, so what’s the meaning there?
2: Another concern is what are the fundamental differences between self-supervised and supervised training for slimmable networks? This is not clear to me as all the training techniques used are common practices in supervised training.
3: What’s the relationship between unsupervised NAS (e.g., [R4, R5]), including the contrastive self-supervised one (e.g., [R6])?
4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?
5: The paper lacks mathematical modeling for the gradient divergence issue which leads to the optimization difficulty claimed by the authors. I think there are only four possible widths and it is not difficult to analyze the gradient magnitude and directions using SGD with maths formulations. Also, some theoretical analysis on convergence is expected, even assuming a linear neural network is fine [R1].
6: The experiments are far from enough to justify the effectiveness of the proposed method.
6.1: The results are merely based on the ResNet-50 backbone. However, I would like to see more ResNet backbones such as R-101 and R-152. More importantly, experiments on Vision Transformers, such as ViT-B in MoCo v3, must be included in the experiments.
6.2: The paper only evaluates the representation quality using linear probing. However, it must evaluate transfer learning performance which is the standard practice in self-supervised learning (e.g., in MoCo v3). For example, experiments on dataset transfer and downstream tasks such as dense segmentation and detection on COCO and ADE20k are needed.
6.3: How about training the whole network (width 1.0) first then using network pruning (e.g., [R7]) to obtain small networks (width 0.25, 0.5, 0.75)? As this strategy can avoid the interference issue during training.
6.4: It lacks comparisons with methods dealing with sub-network interference, such as switchable BN [Yu et al., 2019], sandwich rules [Yu et al., 2019b] and many others.
7: The discussions and references in related work are far from enough. There are few discussions with single-shot NAS and unsupervised NAS methods. In addition, as I point out in the technical novelty part, the differences and advantages with the related work must be discussed.
8: Writing also needs to be improved.
8.1: What is the definition of the “main parameters” in the introduction?
8.2: In Sec. 3.2, “..., where L
is the loss function”. It should be defined in Eq. (1) where it first appears.
8.3: Many grammar issues. I only point out a few. “Slimmable neworks” in Sec. 2; “server performance degradation” in Sec. 3.2.
9: In Sec. 3.2, authors argue that the two ratios in Figure 3 should be large enough. “In Figure 3f, ..., are larger than 1.0 by a clear margin”. It does provide a clear concept of how large is good enough. In my opinion, it also depends on the network architectures and self-supervised learning frameworks. So Figure 3 may not be statistically significant. References:
[R1]: “On the optimization of Deep Networks: Implicit Acceleration by Overparameterization”, ICML 2018
[R2]: “ONCE FOR ALL: TRAIN ONE NETWORK AND SPECIALIZE IT FOR EFFICIENT DEPLOYMENT”, ICLR 2020
[R3]: “BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models”, ECCV 2020
[R4]: “Are Labels Necessary for Neural Architecture Search?”, ECCV 2020
[R5]: “Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?”, NeurIPS 2020
[R6]: “Contrastive Self-supervised Neural Architecture Search”, Arxiv 2021
[R7]: “Resrep: Lossless cnn pruning via decoupling remembering and forgetting”, CVPR 2022","4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?",-1.0,1.0,-1.0,0.0,nan,-1,-1,-1,1
4671,NIPS_2020_1856,"* See section (Clarity). * 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this). * For better or for worse, there is a lot going on -- and it feel like the pages 5 and 8 were crammed up. I wonder if maybe the theory of OT (pages 3-4)can be reduced to 1 page, so that there can be a more thorough discussion, and explanation of the results and metrics rather than having the reader refer to the Supplementary Material consistently for some minor details.",* See section (Clarity).* 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this).,-1.0,1.0,0.0,0.0,nan,-1,-1,-1,-1
3428,NIPS_2020_747,"I have a few concerns regarding this paper. 1. How practical/realistic is the threat model? In the paper, chapter 2.2, the NoBox attack demands the training set. In the real world however, isn't the training set even more precious than the trained models? 2. In the AEG objective, the generator needs to get the gradient to be trained. Would AEG still applicable to the non-differentiable robust classifiers, such as: [1] Countering Adversarial Images using Input Transformations [2] THERMOMETER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES [3] Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples The common point of these approaches is they all incorporate some sort of in-differentiability. 3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST. 4. One experiment I'd like to request: (i)- get a model trained on some dataset at epoch N, N+1, N+2... N+k (ii)- use the generator to generate pertubed imagess to attack all of them. (iii)- show the effectiveness of the attack. This should be a more realistic scenario and it aligns with the main point. 5. A portion of the experiment has compared the NoBox attack to the other attacks. These are generally under different threat model assumptions. However the main claim of the paper is that the NoBox is capable of attacking different models in the same function space. It would be better if the authors can present the NoBox's effectiveness attacking more diversified trained neural network models.",3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST.,-1.0,-1.0,-1.0,0.0,nan,-1,0,-1,-1
5171,NIPS_2020_880,"1. I could not see a strong motivation for explicitly enforcing sparsity on architecture parameters. This is because there are already many works trying to decouple the dependency of evaluating sub-networks on the training of supernet (i.e., making the correlation higher). This means that we have ways to explicitly decouple the network evaluation with supernet training without adding a sparsity regularizaiton. 2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2. 3. The one-stage method proposed basically focusing on training network weights W after the training of architecture parameters is converged. However, similar idea can also be achieved in other differentiable NAS framework, where one can continue training the supernet weights after the architecture remains little change. For example, in GDAS, after the entropy of edges is well minimized, the sampled architecture will be close to determnistic, and one can keep training W to obtain the optimal weights. Moreover, other one-stage methods like [4] are not compared nor discussed. ======================== After reading the author's response, most of my concerns have been addressed. I choose to accept this submission. ======================== [1] Guo, Zichao, et al. ""Single path one-shot neural architecture search with uniform sampling."" ICLR 2020. [2] X. Dong and Y. Yang. Searching for a robust neural architecture in four gpu hours. CVPR 2019. [3] Bender, Gabriel, et al. ""Understanding and simplifying one-shot architecture search."" ICML2018. [4] Cai, Han, Chuang Gan, and Song Han. ""Once for all: Train one network and specialize it for efficient deployment."" ICLR 2020.","2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2.",1.0,1.0,1.0,1.0,nan,-1,0,-1,1
1908,ARR_2022_64_review,"- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:   - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction   - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. 
In line ""544"", I still quite understand why using entity representations can reduce language bias. ","- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:",-1.0,-1.0,-1.0,-1.0,nan,-1,-1,-1,-1
3403,NIPS_2020_585,"I think the major problem of this paper is that the novelty is limited. It seems that nearly all the components in the proposed method have been used, and the combination manner is also not so novel. 1. The two-stage method is widely used to deal with partial label examples. The first stage in this paper adopts the widely used label probagation strategy to obtain labeling confidence for partial label examples. However, SSPL [22] also adopts this strategy (which obtain labeling confidence for both partial label examples and unlabeled examples). So there seems no novelty for the first stage. 2. For the second stage, a modified maximum margin formulation is introduced, which can jointly enable the induction the induction of predictive model and the estimation of labeling confidence over unlabeled data. The formulation of the second stage mainly follows [17]. There are two differences between this paper and [17]: Firstly, this paper uses the obtained labeling confidence of partial examples to give different weights on the losses of different examples. Secondly, the unlabeled data is used in the formulation by using the widely-used manifold regularization, so that the labeling confidence of unlabeled data will be jointly estimated. Generally, I think the improvement over [17] is intuitive and reasonable. But I think that the novelty is not enough, especially on such a prestiguous venue NeurIPS. 3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage. 4. This paper may not effectively deal with large-scale datasets because it uses alternating optimization and needs to construct a similarity graph in advance.","3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage.",1.0,1.0,1.0,0.0,nan,-1,0,-1,-1
3980,NIPS_2020_1314,"1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP. 2. In Section3.1 the authors give a thorough comparison between the discrete Gaussian and discrete Laplace. The conclusions deduced by the discrete distribution are almost the same as the prior conclusions of the continuous distribution. 3. This paper does not compare their methods with the exponential mechanism(Frank McSherry and Kunal Talwa 2007). Exponential mechanism can also output a discrete answer for a query.","1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP.",-1.0,1.0,1.0,0.0,nan,-1,-1,-1,-1
608,ICLR_2021_1900,"weaknesses:
Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq. (2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 2) Regarding Eq. (5), what the intuition of the adaptive matrix (i.e., (log_{\mu+1} (N_i+1))^{\delta^{\tau}}) when i = j should be provided to the authors.
The major issue of this paper is the experimental evaluations. 1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic. 2) Some state-of-the-art methods are not involved in the experimental comparisons, such as [ref1-ref5]. Moreover, the accuracy of the proposed method cannot outperform these methods.
Minor issues:
There are several typos and writing problems in this paper. For example, on Page 3, ""Dubey et al.Dubey et al. (2018)"", and ""Chen et al.Chen et al. (2019)"". On Page 4, ""PC Dubey et al. (2018)"". On Page 8, ""And also solves the long-tailed problem by an adaptive matrix term.""
[ref1] Weakly Supervised Fine-grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning, CVPR 2020.
[ref2] Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019.
[ref3] Fine-Grained Visual Classification via Progressive Multi-Granularity Training of Jigsaw Patches, ECCV 2020.
[ref4] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.
[ref5] BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition, CVPR 2020.","1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic.",-1.0,1.0,-1.0,-1.0,nan,-1,-1,-1,-1
2060,ARR_2022_1_review,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. ","- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.",1.0,1.0,1.0,1.0,nan,1,1,-1,1
3525,NIPS_2020_324,"The experiments are performed on two simple environments: 1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.","1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.",1.0,1.0,1.0,0.0,nan,-1,-1,-1,-1
114,ICLR_2022_1971,"Weakness: Method:
1. Verification and Optimization:
The proposed HNPF method is for verification (e.g., check whether a given solution x is Pareto optimal), but not for optimization (e.g., find an (approximate) Pareto solution x). It needs an extra search method, such as random sampling in this work, to first generate a large number of feasible solutions to cover the whole search space. Therefore, the underlying optimization is indeed random sampling (independent from HNPF), which could be extremely inefficient for a non-trivial search space. It is not suitable to put and compare the proposed HNPF method with other optimization methods that can directly find the (approximate) Pareto solution.
Since HNPF depends on random sampling, it is not surprising that it can only work for small scale problems.
2. The Reason to Build the Model:
HNPF needs to first build a neural network to check whether a given solution x satisfies the Fritz-John Condition (FJC), which requires a large number of training samples (e.g., 11K for a two-dimensional problem). The learned model is mainly used to classify whether the extra randomly sampled solutions (e.g., 9K) are weak Pareto optimal or not. The reason for model building, such as the advantage over the simple FJC rule-based classification, is not well motivated and justified in this work.
The proposed Pareto filter in stage 2 is not discussed and compared with other related nondominated sorting algorithms (e.g., [2]).
3. Necessary Condition for Pareto Optimality:
The KKT[3] and FJC[4] are two types of first order necessary conditions for (local) Pareto efficiency (Pareto optimality). In my understanding, the multi-objective optimization based MTL algorithms mentioned in this work (Sener & Koltun, 2018; Lin et al., 2019a; Mahapatra & Rajan, 2020; Ma et al., 2020; Navon et al., 2021) mainly use the gradient-based multi-objective optimization methods (e.g., MGDA) [5-7], which is based on the KKT condition. For these methods, in each update step, the gradient can be written as a linear combination of the gradient for each objective with adaptive weights derived from the KKT condition. Therefore, they are all different from the simple linear scalarization with fixed weights. All the claims and analyses in this work that the previous works use simple linear scalarization is not correct.
The FJ condition is also for local Pareto convergence, similar to the KKT condition. The global convergence property is solely due to random sampling that only works for extremely low-dimensional problems. It is unfair to say the proposed algorithm can overcome the local convergence of other gradient-based methods. In addition, the proposed algorithm heavily depends on the Fritz-John condition, but the original work [4] is not cited.
4. Linear Scalarization and Convex Pareto Front:
It is well-known that the simple linear scalarization cannot find the non-convex part of the Pareto front [8]. This finding leads to the seminal work on NBI scalarization (Das & Dennis.,1998), which is indeed one fundamental work that inspires the proposed method in this work (section 4, first sentence). The claim ""it is incorrect to state that LS itself fails if the Pareto front is non-convex"" (appendix, page 15) is questionable.
Since the proposed HNPF can only verify whether a given solution is weak Pareto optimal, its ability to find the whole Pareto front totally depends on the extra sampling method (e.g., random sampling) to generate all the Pareto solutions (might be infinite). It is misleading to indicate the proposed HNPF method itself can find the whole Pareto front. In addition, since the Pareto set has measure zero and infinite cardinality, the random sampling + HNPF method can at most find a dense approximation to the Pareto set. Experiment:
5. Algorithms for Comparison:
All the multi-objective optimization based MTL algorithms are designed for optimizing a deep neural network with millions of parameters. They implicitly depend on the assumption that the deep neural network has good properties (e.g., no bad local optimum [9][10]) on its loss functions, which is consistent with other gradient-based single-objective optimization methods. They are not designed to find the global Pareto front for low-dimensional problems.
For low-dimensional problems, it is more suitable to compare with the model-free multi-objective optimization methods such as the multi-objective evolutionary algorithm [11,12] and multi-objective CMA-ES [13]. If the model building is allowed, Multi-Objective Bayesian Optimization (MOBO) algorithms can have a very good sampling efficiency for the low-dimensional problems [14,15]. It is also very common to conduct non-dominated filtering at the end of those model-free algorithms or MOBOs (e.g., only keeping the current non-dominated solutions).
6. Training + Sampling:
The proposed method needs to first sample 11k solutions to train the neural network model, then randomly generate extra 9K solutions for filtering. Is there any advantage over simply using FJC to filter 9K (or 11k + 9K) randomly sampling solutions?
7. Figure from Other Works:
Many figures in the main paper and the appendix are directly borrowed from other works. I think this is not appropriate even the credits are given to the original works. Reference:
[1] Ruchte, Michael, and Josif Grabocka. Multi-task problems are not multi-objective. arXiv preprint arXiv:2110.07301, 2021.
[2] Roy, Proteek Chandan, Kalyanmoy Deb, and Md Monirul Islam. An efficient nondominated sorting algorithm for large number of fronts. IEEE transactions on cybernetics 49, no. 3: 859-869, 2018.
[3] Kuhn, H. W., and A. W. Tucker. Nonlinear Programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pp. 481-492. University of California Press, 1951.
[4] Da Cunha, N. O., and E. Polak. Constrained minimization under vectorvalued criteria in finite dimensional spaces. Journal of Mathematical Analysis and Applications, 19(1), 103–124 ,1967.
[5] Fliege, Jorg, and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research 51, no. 3: 479-494, 2000.
[6] Fliege, Jorg, and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on SQP techniques. SIAM Journal on Optimization 26, no. 4: 2091-2119, 2016.
[7] Desideri, Jean-Antoine. Multiple-gradient descent algorithm (MGDA) for multiobjective optimization. Comptes Rendus Mathematique 350, no. 5-6: 313-318, 2012.
[8] Das, Indraneel, and John E. Dennis. A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems. Structural optimization 14, no. 1: 63-69, 1997.
[9] Kawaguchi, Kenji. Deep learning without poor local minima. NeurIPS 2016.
[10] Kawaguchi, Kenji, and Leslie Kaelbling. Elimination of all bad local minima in deep learning. AISTATS 2020.
[11] Deb, Kalyanmoy, Amrit Pratap, Sameer Agarwal, and T. A. M. T. Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, no. 2: 182-197, 2002.
[12] Zhang, Qingfu, and Hui Li. ""MOEA/D: A multiobjective evolutionary algorithm based on decomposition."" IEEE Transactions on evolutionary computation 11, no. 6: 712-731, 2007.
[13] Igel, Christian, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evolutionary computation 15, no. 1: 1-28, 2007.
[14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020.
[15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.","1: 1-28, 2007. [14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020. [15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.",0.0,0.0,0.0,0.0,nan,0,0,-1,0
2507,NIPS_2021_537,"Weakness: The main weakness of the approach is the lack of novelty. 1. The key contribution of the paper is to propose a framework which gradually fits the high-performing sub-space in the NAS search space using a set of weak predictors rather than fitting the whole space using one strong predictor. However, this high-level idea, though not explicitly highlighted, has been adopted in almost all query-based NAS approaches where the promising architectures are predicted and selected at each iteration and used to update the predictor model for next iteration. As the authors acknowledged in Section 2.3, their approach is exactly a simplified version of BO which has been extensively used for NAS [1,2,3,4]. However, unlike BO, the predictor doesn’t output uncertainty and thus the authors use a heuristic to trade-off exploitation and exploration rather than using more principled acquisition functions.
2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C. 3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper.
Detailed Comments: 1. The authors conduct some ablation studies in Section 3.2. However, a more important ablation would be to modify the proposed predictor model to get some uncertainty (by deep-ensemble or add a BLR final output layer) and then use BO acquisition functions (e.g. EI) to do the sampling. The proposed greedy sampling strategy works because the search space for NAS-Bench-201 and 101 are relatively small and as demonstrated in [6], local search even gives the SOTA performance on these benchmark search spaces. For a more realistic search space like NAS-Bench-301[7], the greedy sampling strategy which lacks a principled exploitation-exploration trade-off might not work well. 2. Following the above comment, I’ll suggest the authors to evaluate their methods on NAS-Bench-301 and compare with more recent BO methods like BANANAS[2] and NAS-BOWL[4] or predictor-based method like BRP-NAS [5] which is almost the same as the proposed approach. I’m aware that the authors have compared to BONAS and shows better performance. However, BONAS uses a different surrogate which might be worse than the options proposed in this paper. More importantly, BONAS use weight-sharing to evaluate architectures queried which may significantly underestimate the true architecture performance. This trades off its performance for time efficiency. 3. For results on open-domain search, the authors perform search based on a pre-trained super-net. Thus, the good final performance of WeakNAS on MobileNet space and NASNet space might be due to the use of a good/well-trained supernet; as shown in Table 6, OFA with evalutinary algorithm can give near top performance already. More importantly, if a super-net has been well-trained and is good, the cost of finding the good subnetwork from it is rather low as each query via weight-sharing is super cheap. Thus, the cost gain in query efficiency by WeakNAS on these open-domain experiments is rather insignificant. The query efficiency improvement is likely due to the use of a predictor to guide the subnetwork selection in contrast to the naïve model-free selection methods like evolutionary algorithm or random search. A more convincing result would be to perform the proposed method on DARTS space (I acknowledge that doing it on ImageNet would be too expensive) without using the supernet (i.e. evaluate the sampled architectures from scratch) and compare its performance with BANANAS[2] or NAS-BOWL[4]. 4. If the advantage of the proposed method is query-efficiency, I’d love to see Table 2, 3 (at least the BO baselines) in plots like Fig. 4 and 5, which help better visualise the faster convergence of the proposed method. 5. Some intuitions are provided in the paper on what I commented in Point 3 in Weakness above. However, more thorough experiments or theoretical justifications are needed to convince potential users to use the proposed heuristic (a simplified version of BO) rather than the original BO for NAS. 6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.
7. The results in Table 2 which show linear-/exponential-decay sampling clearly underperforms uniform sampling confuse me a bit. If the predictor is accurate on the good subregion, as argued by the authors, increasing the sampling probability for top-performing predicted architectures should lead to better performance than uniform sampling, especially when the performance of architectures in the good subregion are rather close. 8. In Table 1, what does the number of predictors mean? To me, they are simply the number of search iterations. Do the authors reuse the weak predictors from previous iterations in later iterations like an ensemble?
I understand that given the time constraint, the authors are unlikely to respond to my comments. Hope those comments can help the authors for future improvement of the paper.
References: [1] Kandasamy, Kirthevasan, et al. ""Neural architecture search with Bayesian optimisation and optimal transport."" NeurIPS. 2018. [2] White, Colin, et al. ""BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search."" AAAI. 2021. [3] Shi, Han, et al. ""Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS."" NeurIPS. 2020. [4] Ru, Binxin, et al. ""Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels."" ICLR. 2020. [5] Dudziak, Lukasz, et al. ""BRP-NAS: Prediction-based NAS using GCNs."" NeurIPS. 2020. [6] White, Colin, et al. ""Local search is state of the art for nas benchmarks."" arXiv. 2020. [7] Siems, Julien, et al. ""NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search."" arXiv. 2020.
The limitation and social impacts are briefly discussed in the conclusion.","6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.",0.0,1.0,1.0,0.0,nan,-1,1,-1,1
3487,NIPS_2020_1312,"- The contribution of local voxel-bounded implicit fields is not motivated and evaluated enough. The authors showed that voxel embeddings improve the performance. However, an experiment that shows the performance with different resolutions (partly done with the progressive training evaluation) would improve the paper. - Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution. - For the experiments on large-scale scenes and dynamic scenes, qualitative results for existing methods are missing. Furthermore, quantitative results are missing for these experiments (also for existing methods). - The geometric result in the ScanNet experiment does not seem to be very good. Therefore, it would be interesting to see how accurate the represented geometry is and how it affects the overall performance of the method. - The effect of the early termination is not properly evaluated in the paper. - The initial grid resolution seems to be already very high. It would be interesting to see how this affects the method. - There should also be comparisons to DeepVoxels since this work also leverages local voxels as a feature representation.","- Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution.",1.0,1.0,1.0,0.0,nan,1,1,-1,-1
1654,ICLR_2023_1765,"weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, t ∈ R while y , f ( θ ) ∈ R n
, where n
is the size of the training set, so that the operation y − t − f ( θ )
is not well-defined. Moreover, even if y , f ( θ ) ∈ R
, the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2
, rather than ( θ ) ≤ t
. Since, in general, the training problem will be defined for y ∈ R n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are ""reasonable"". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- ""Issue 1: Is non-convex deep neural networks always better?"": ""is"" should be ""are"". - ""While some experiments show that certain local optima are equivalent and yield similar learning performance"" -- this should be supported by a reference. - ""However, the derivation of strong duality in the literature requires the planted model assumption"" --- what do you mean by ""planted model assumption""? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - ""In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m
representation power."" -- A statement like this only makes sense under some definition of ""representation power"". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - ""Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient."" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - ""... too many minimization pooling layers will damage the representation power of the neural network"" --- why? Can the authors expand on this observation?
Section 5: - ""... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y
"" --- note that this observation requires y ≥ 0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): ""conveyers"" -> ""converges"".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - ""It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)"" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References
[1] Robbins, Herbert, and Sutton Monro. ""A stochastic approximation method."" The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. ""Learning representations by back-propagating errors."" nature 323.6088 (1986): 533-536.","- Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere.",1.0,1.0,-1.0,1.0,nan,-1,0,-1,1
1526,ICLR_2023_3808,"Weaknesses: • The writing of the paper is at times confusing. For example, it is unclear of the significance of the \cap architecture. No detail is included in the paper. • The rationale of dividing the sequence of intermediate results into 11 sets is not provided. • No ablation studies are provided for \mu and \lambda, and the size of each set. In fact, their values are not provided in the paper (unless I missed them). • Using different sets and varying \phi appear to have a similar effect. However, there is no discussion on this issue. • It appears that there are two types of sets (Table 1 and 2), one for style and one for content. But in the paper, they are regarded to be the same, which is quite confusing. • There is insufficient discussion of the novelty of the proposed method, e.g. what the proposed method can do that other recently proposed style transfer methods cannot do? For example, in Fig. 6, the only discussion is one single sentence on p. 5 stating that the proposed method preserve the semantic structure of the content image and simulate the strokes of the style image from the style domain. Such a statement, which is quite generic, can be easily applied to other style transfer methods. • There is no quantitative comparison, e.g. style loss, user study, and efficiency, between the proposed method and other methods. • One issue not discussed is the importance of using the diffusion model. What can it provide to style transfer that no other methods can? • There are some occasional typos and grammatical mistakes.",• The rationale of dividing the sequence of intermediate results into 11 sets is not provided.,1.0,1.0,-1.0,0.0,nan,-1,0,-1,-1
5012,NIPS_2020_1049,"- The experimental results are not significant. Two straightforward error rate indicators: the dimension of the space and LR error is chosen as baselines and the authors claim that the proposed bound is superior to these two baselines. However, from the experimental results, Pearson’s r correlation in Fig.4 shows that the proposed improvement is marginal. The results on image dataset from the supplementary material also shows that the Pearson’s r correlation is very close between the LR Error and MSE. - The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1. - The figures are not clear enough, e.g. what does each point stand for in Fig.1 and Fig.2. The authors could add the necessary information to make this paper more self-contained.",- The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1.,1.0,1.0,1.0,0.0,nan,-1,-1,-1,1
2844,NIPS_2022_477,"Weaknesses:
1.In experiments, the PRODEN method also uses mixup and consistency training techniques for fair comparisons. What about other competitive baselines? I'd like to see how much the strong CC method could benefit from the representation training technique.
2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.
3.In App. B.2, a relaxed solution of Sinkhorn-Knopp algorithm is proposed. Why the relaxed problem guarantees to converge?Does Solar always run this relaxed version of Sinkhorn-Knopp?
4.How is gamma in the Sinknhorn-Knopp affect the performance?
5.How does the class distribution estimate for PRODEN in Figure 1?
Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment.
Limitations: The experiments need to be further improved.",5.How does the class distribution estimate for PRODEN in Figure 1? Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment. Limitations: The experiments need to be further improved.,-1.0,-1.0,-1.0,0.0,nan,-1,-1,-1,-1
1317,ICLR_2023_3854,"Weakness:
1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.
2.The novelty of the paper is limited as some parts seems like a direct combination of TD3 and SAC, but the paper does not provide detailed insight of why SAC works in complex envs while TD3 works in simple envs.
3.Theorem 2 assumes that Yi follows a uniform distribution, which might be too strong in practice.
4.The experimental results are not quite convincing. TD3 fails to learn in Humanoid-v2, can you explain the reason?","1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.",1.0,1.0,1.0,0.0,nan,-1,-1,-1,-1
1875,ARR_2022_2_review,"-	The authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. 
-	Given that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? 
-	It would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. 
-	It was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. 
-	In addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. 
-	In Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. 
-	In Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? 
-	In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? 
-	The fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). 
-	In Section 4.3.3, how do you deal with the RoBERTa token limit? 
-	In Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. 
-	In Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. 
-	It would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ","- In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold?",-1.0,1.0,0.0,0.0,nan,-1,0,-1,1
5730,NIPS_2018_857,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo?  [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018.  ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.",- The writing should be improved. Some points in the paper is unclear to me.,-1.0,-1.0,-1.0,-1.0,nan,-1,0,0,-1
229,ICLR_2022_410,"Weaknesses:
. Theorem 4.2: For the downstream classification, the loss is upper and lower bounded in terms of the L_NCE loss. The authors provided comparison with Saunshi et al. (2019) from the technical perspective. Is there any intuitive explanation on how to evaluate the classification performance in terms of contrastive learning (loss)?
. Assumption 4.5 (intra-class connectivity): This assumption is strong. Without the label information, it seems impossible to derive such augmentation set. Please add discussion on the practicality of this assumption, and show an example on some datasets if possible.
. Proposition 4.7: Based on the proof provided in the appendix the conclusion not only relies on the existence of such augmentation set (Assumption 4.5), but also that such augmentation should be applied to intra-class samples, ie, t_i(x_i) = t_j(x_j). This kind of operation is impractical without the label information. Please add comment on that.
. In the experiments, RandomResizedCrop is used to illustrate the relationship between Aug Strength and ACC(ARC). The best performance for different datasets all achieves at Aug Strength = 0.92. Any comments on that? eg., in terms of data augmentation for intra-class samples at Aug Strength = 0.92?
. In practice, there are different kinds of data augmentation, eg, flipping, rotation, and scaling. The authors only showed results on RandomResizedCrop. Can you show results for other data augmentation types? Do you have similar conclusion as that for RandomResizedCrop?
. Different data augmentation types are often used together in practice (eg, randomly pick two augmentations from the augmentation set for the raw image). Then how to apply the proposed analysis in such practical case? In particular, how to measure the Aug Strength?
. The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping). 1) The study on applying the analysis to existing contrastive learning algorithms is, however, preliminary (only with RandomResizedCrop). 2) Based on the proposed analysis how to find the sweet spot of data augmentation for contrastive learning is crucial, but this is not discussed.",". The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping).",0.0,0.0,0.0,0.0,nan,-1,0,-1,-1
2403,NIPS_2021_1571,"How do you validate the estimated uncertainty maps, in the absence of GTs for the same ?
Wonder what is the expansion of the acronym ""D_CBase"" used ? What does it mean ?
What is MAE in fig. 4 ? Not defined anywhere in main doc. - got it in Supple though.
Fig. 9 of Supple material - how do you validate these complexity scores ? Even the GT or human evaluation is not provided. What are the typical ranges of complexity values you get by the entropy based measure ? How do you thus claim it to be consistent with human visual perception ?
Some typos: line 169: ......local minimum, it solve the following estimating.... - change to : it solves the....
Some more references may be added as:
Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011;
Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.","- change to : it solves the.... Some more references may be added as: Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011; Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.",1.0,-1.0,0.0,0.0,nan,1,-1,1,-1
3498,NIPS_2020_944,"1. Weighted retraining is not new. The cross-entropy method (De Boer et al., 2005; Neil et al., 2018) maximizes the expectation E_p(x)[f(x)] of the objective function f(x) when sampling from a policy p(x) by periodically retraining p(x) on the samples with the highest reward, e.g. those with a reward above a quantile cutoff (i.e. using a stepwise weighting function). Instantiations of the cross-entropy method include DbAs (Brooks et al) and FBGAN (Gupta et al). Reward weighted regression (RWR) (Hachiya et al) is another existing optimization technique that employs weighted retraining. Angermueller et al. (http://arxiv.org/abs/2006.03227) recently employed these techniques as baselines for high-dimensional discrete optimization. 2. The described rank-based weighting function is not new. See RankGAN (Lin et al. 2017) or LeakGAN (Guo et al. 2017) for an example. 3. The evaluation is missing important baselines such a DbAs, FBGAN, RWR, and model-based optimization. 4. Chemical design task: It is unclear how the optimization trajectory of ‘original’ was obtained. How were new data points sampled from JT-VAE? Why does the trajectory stop at 250? 5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning. 6. What do error bars represent? How often were experiments repeated with different random seeds?","5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning.",1.0,1.0,0.0,0.0,nan,1,1,-1,-1
5083,NIPS_2020_734,"- Main math section/derivation could be improved for clarity: it's a very simple idea but it is presented in an obfuscated way that requires a lot of energy from the reader to go through. - It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper. - The gains obtained by the proposed method modification are not that large.","- It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper.",-1.0,1.0,-1.0,-1.0,nan,-1,-1,0,-1
3452,NIPS_2020_1505,"Even though the paper reads well in its current form, I found during the first reading that the presentation of the conceptual messages (which are the most important part in my opinion) were overshadowed by the algorithmic comparison between GOLEM and NOTEARS. The paper is dense and it is not necessarily clear what the take-away messages of the paper are. - I would encourage the authors to elaborate more on the differences between the findings their findings that are related to algorithmic implementations and the findings that pertain to the asymptotic of loss functions. - I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.",- I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.,1.0,1.0,1.0,1.0,nan,-1,1,-1,-1
4673,NIPS_2020_1254,"- The theorem may slightly overstate its result in the following way: it seems that in order for this correspondence between adversarial training and the proposed regularization scheme to hold, \epsilon must be quite small. That is, we are assuming here that all of the points in an \epsilon ball around some data point x are mapped by the model to the same activation pattern \phi_x (i.e. that B_\epsilon^p(x) \subset X(\phi_x)). I would imagine that this may not hold for ""realistic"" values of \epsilon (e.g. 8/255) all the time. Indeed, my concern is that while this theorem is certainly compelling, it may be the case that it only holds for \epsilon so small that it may not hold in practice. Perhaps the authors can clarify here. I see there is an experiment to this effect in Section 7.16, but this seems to be for only one data point. [EDIT: post-rebuttal] Based on the authors response and a closer look at Section 5.4, I'm satisfied that the authors looked into this potential weakness and were able to add explanation as to its implications.] - Figure 1 is too small to really be useful. It's not really clear what the arrows represent. A more detailed and larger figure here would be appreciated. - The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.","- The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.",1.0,1.0,1.0,1.0,nan,1,1,-1,1
1943,ARR_2022_247_review,"- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text. - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).
- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.
- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. 
Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).
- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.
- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").
- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).
- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""
- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?
Typos: - 166: Space after footnote 5.
- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""? ","- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.",1.0,1.0,1.0,1.0,nan,-1,1,-1,1
5689,NIPS_2018_680,"weaknesses are: 1) The statistical aspects are a bit eluded: is the given algorithm robust to statistical imprecision? 2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?","2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?",0.0,1.0,0.0,0.0,nan,-1,0,-1,-1
5198,NIPS_2020_1687,"- Since the proposed optimization is non-convex, there is no guarantee for the correctness of the bounds. - Perhaps the most important missing result in this work is confidence intervals for the bounds. - At some parts, for instance the choice of function family for p_\eta, it seems that the only criteria for the choices in the model is to make the optimization task efficient and no other justification is provided. - Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose. - In the experiments, only two cases are considered: linear Gaussian case and a second case in which the treatment is again linear and the outcome is generated by 0.3X^2−1.5XC+e. It seems necessary to consider other instances of non-linear cases as well. - The choice of the outcome equations (X-6C+e and 0.3X^2−1.5XC+e) look random. Was there any specific reason for this choice?","- Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose.",0.0,-1.0,0.0,0.0,nan,-1,0,0,-1
2019,ARR_2022_292_review,"1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior. ","1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others.",1.0,1.0,-1.0,0.0,nan,-1,1,-1,-1
5808,NIPS_2018_630,"Weaknesses: - While there is not much related work, I am wondering whether more experimental comparisons would be appropriate, e.g. with min-max networks, or Dugas et al., at least on some dataset where such models can express the desired constraints. - The technical delta from monotonic models (existing) to monotonic and convex/concave seems rather small, but sufficient and valuable, in my opinion. - The explanation of lattice models (S4) is fairly opaque for readers unfamiliar with such models. - The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions: - Lattice models expect the input to be scaled in [0, 1]. If this is done at training time using the min/max from the training set, then some test set samples might be clipped, right? Are the constraints affected in such situations? Does convexity hold? - I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize? - Why is this paper categorized under Fairness/Accountability/Transparency, am I missing something? - The SCNN getting ""lucky"" on domain pricing is suspicious given your hyperparameter tuning. Are the chosen hyperparameters ever at the end of the searched range? The distance to the next best model is suspiciously large there. Presentation suggestions: - The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned. - ""We have found it easier to be confident about applying ceterus paribus convexity;"" -- the word ""confident"" threw me off a little here, as I was not sure if this is about model confidence or human interpretability. I suspect the latter, but some slight rephrasing would be great. - Unless I missed something, unconstrained neural nets are still often the best model on half of the tasks. After thinking about it, this is not surprising. It would be nice to guide the readers toward acknowledging this. - Notation: the x[d] notation is used in eqn 1 before being defined on line 133. - line 176: ""corresponds"" should be ""corresponding"" (or alternatively, replace ""GAMs, with the"" -> ""GAMs; the"") - line 216: ""was not separately run"" -> ""it was not separately run"" - line 217: ""a human can summarize the machine learned as"": not sure what this means, possibly ""a human can summarize what the machine (has) learned as""? or ""a human can summarize the machine-learned model as""? Consider rephrasing. - line 274, 279: write out ""standard deviation"" instead of ""std dev"" - line 281: write out ""diminishing returns"" - ""Result Scoring"" strikes me as a bit too vague for a section heading, it could be perceived to be about your experiment result. Is there a more specific name for this task, maybe ""query relevance scoring"" or something? === I have read your feedback. Thank you for addressing my observations; moving appendix D to the main seems like a good idea. I am not changing my score. ","- The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions:",1.0,1.0,1.0,1.0,nan,-1,1,-1,-1
589,ICLR_2022_3053,"Weakness:
It’s difficult to identify the real novelty/contribution of the proposed method. The core techniques such as importance score-based pruning and full model distillation (essentially self-distillation) have been explored in the network pruning literature.
The rules designed for parameter redistribution are draw from the observations in Fig. 4. However, there is no ablation study to validate its effectiveness as compared to other design rules.
The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction.
Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters).
There is a lack of comparison with other pruning methods.
Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).","4. However, there is no ablation study to validate its effectiveness as compared to other design rules. The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction. Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters). There is a lack of comparison with other pruning methods. Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).",1.0,1.0,1.0,0.0,nan,-1,0,-1,1
2404,NIPS_2021_655,"weakness about their results in the final section. However, I have some concerns on the paper. First, the related works might have been cited adequately, but the authors could have done better comparing with them. Second, there is a concurrent paper that covers many of the results in this paper. I vote for a weak reject.
The concurrent paper shall not be a concern. After discussion I would raise my vote to accept. Limitations
The recent paper [1] studies the more general tensor bandit, where the reward function can be more general low-rank p-tensors. In particular, the p = 2
case in [1] seems cover the main contributions (Theorem 1-3) in this paper. Besides, Algs. 1 and 2 in [1] look quite similar to Alg. 1 in the submission, but the ones in [1] are much more general. For example, the ones in [1] can solve general ⟨ θ ⋆ , a t ⟩ p
reward, while the submission only studies ⟨ θ ⋆ , a t ⟩ 2
. The authors may need to compare their results with those in [1].
The lower bound (Theorem 2) seems to be the same as [2, Theorem 6] by putting r = 1
. The authors might need to point it out more explicitly, and it would be strange to list it as a contribution.
Other Comments
Line 72, what is x x ?
At Line 125, the idea of using curvature to obtain sqrt regret for ETC could date back to even earlier papers, such as [3].
Societal Impact
The paper is mainly theoretical, and I don't see any potential negative societal impact. Bibliography
[1] Huang, Baihe, Kaixuan Huang, Sham M. Kakade, Jason D. Lee, Qi Lei, Runzhe Wang, and Jiaqi Yang. ""Optimal Gradient-based Algorithms for Non-concave Bandit Optimization."" arXiv preprint arXiv:2107.04518 (2021).
[2] Lu, Yangyi, Amirhossein Meisami, and Ambuj Tewari. ""Low-rank generalized linear bandit problems."" In International Conference on Artificial Intelligence and Statistics, pp. 460-468. PMLR, 2021.
[3] Rusmevichientong, Paat, and John N. Tsitsiklis. ""Linearly parameterized bandits."" Mathematics of Operations Research 35, no. 2 (2010): 395-411.",1 and 2 in [1] look quite similar to Alg.,0.0,0.0,0.0,0.0,nan,-1,0,0,-1
5037,NIPS_2020_1663,"Besides the concerns about the assumptions (discussed in Strengths), my other major comment is related to experiments. 1. It seems that the proposed approach was not compared against any existing imputation method. Without doing so, it is very difficult to see the real value of the work. 2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice. 3. I find the results in Section 6.2 a bit weak. Do the results echo findings reported in the literature? Such comparisons could be useful.","2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice.",1.0,1.0,0.0,1.0,nan,-1,1,-1,1
4199,NIPS_2020_630,"- Authors miss to cite and comment some very related work in network interpretability such as: Bau et al, CVPR 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. I think the work in Network Dissection is directly relevant to the topic discussed in the paper, particularly for its hierarchical analysis as well as focus on ImageNet. - It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics. - Why did authors restrict themselves to a smaller number of categories? I think it would be interesting to understand the behavior on a larger category set. - Have authors evaluated qualitatively some of the metrics? I think it would be interesting to see whether, additionally to the quantitative evaluation, the images have some features in common. After rebuttal: The authors have addressed most of my concerns and I will update my score to 7.","- It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics.",1.0,-1.0,-1.0,1.0,nan,-1,1,-1,-1
2300,ACL_2017_371_review.json,"Weaknesses: - The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?
- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section ",- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?,0.0,1.0,0.0,0.0,nan,-1,-1,0,-1
5033,NIPS_2020_1579,"- The presented analysis is limited to a single decision affected by a confounding variable. It is a sensible assumption in many cases, but I got dissapointed by the fact that the current analysis doesn't seem to **conceptually** extend to the general case. - The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one. - Really, the paper presents a **family of bounds** parameterized by Gamma, but Gamma isn't an easily interpretable quantity. Can this be rephrased in terms of a more standard notion (e.g. an information quantity)?",- The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one.,1.0,-1.0,-1.0,1.0,nan,1,1,0,-1
4330,NIPS_2020_1680,"The work is already quite extensive and more cannot be reasonably expected. Still, perhaps the authors can comment on these issues: 1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1. 2) While Manchev and Spratling 2020 claim that their difference target prop for RNNs outperforms BPTT in (simpler) 4 of 5 tasks, why does DRL which is an improvement on DTP not come closer to backprop in these tasks? 3) Would a correction different from GN targets enable to come closer to backprop? 4) In lines 177-178, the authors write 'This damping interpolates between the pseudo-inverse and the transpose of Jf ̄ i,L , so for large λ, GN targets resemble gradient targets.' I would assume that the authors would optimize over lambda as a hyperparameter, but I did not find it in the list of hyperparams in Tables S1-S4. WHy not? In any case, it would be good to explore the dependence of performance on lambda explicitly in a (supplementary) plot and how this impacts performance expecially given that for 'large λ, GN targets resemble gradient targets'.",1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1.,-1.0,1.0,0.0,0.0,nan,-1,-1,-1,1
4757,NIPS_2020_1819,"- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction. - The results are not significantly higher than that of the closest previous work on English. On Chinese it is, but one has to wonder whether that might be due to implementation or hyperparameter differences rather than the particular model. - There are some missing ablations for quantifying where the performance gains come from, and for justifying some design choices (see details below).",- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction.,-1.0,-1.0,-1.0,-1.0,nan,-1,-1,-1,-1
3203,NIPS_2020_1444,"1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point. 2. In Theorem 1, it seems that the algorithm can only achieve meaningful results when the bias term decreases very fast with respect to t. In some other cases (such as when the bais is fixed), the regret is linear. 3. In Theorem 2, to achieve meaningful results, one has to know the value of v in advance, which is generally impossible.","1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point.",1.0,1.0,-1.0,1.0,nan,-1,0,-1,-1
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper.",1.0,1.0,1.0,1.0,nan,1,1,1,1
1580,ICLR_2023_3912,"Weaknesses: 1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance. 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations. 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods. 4. I want to know why the mode parameters in Table 1 and Figure 5 are different. 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.","1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance.",-1.0,1.0,1.0,0.0,nan,-1,-1,-1,-1
5533,NIPS_2020_1458,"- As shown in Table1, the proposed Element-weighted TriHard loss actually does not perform better than several existing strategies. Although combining EWTH with HTH or HNTH gets improved results, the effectiveness of the designed method of this paper is incremental. - the best value of t is varying for different methods and datasets (Supplementary materials), so it is too sensitive to selected. - Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.","- Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.",1.0,1.0,1.0,0.0,nan,1,1,-1,1
3185,NIPS_2020_1801,"1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers). 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim. 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score. 4. Were other architectures explored besides WideResNet? 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well. 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?",5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?,0.0,1.0,0.0,0.0,nan,-1,0,-1,1
88,ICLR_2022_3123,"Weaknesses:
The proposed algorithm is lack of novelty. Compared to the previous video transformer (ViVit, TimeSformer), the difference of this work is: 1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention. 2) this work introduce additional residual connection (with learnable ratio) between attention block from adjacent transformer blocks. 3) the final output is weighted average of the output from each transformer block while the TimeSformer take the output of last block as the final representation.
This paper would need more evidence to support its claim. For example, it's not clear why we should have the current layout of the network. How a set of spatial transformer block followed by a set of temporal transformer block is better than alternating design? considering the cross-stage self-attention could work in both cases.
The improvement of performance is minor. Take the ViVit as the baseline. If we are going to compare between the best performance between ViViT and this paper, the improvement is minor (81.3% from ViViT vs. 81.8% from this paper). If we are going to compare the performance using the same input resolution (16x224x224), the performance of this paper is still on par with ViViT (80.6% from ViViT vs. 80.1% from this paper). In terms of the computational cost, it is wrong to compare GFLOPs between two methods with different inference views. As the performance of any method will not be linearly increased with number of inference views, it is not fair to put 4x3 views for ViViT and 1x3 views for this paper. As a result, the number of 8.6% does not make too much sense. At last, I would suggest to include MViT[1] as one of the reference, which is the SOTA video transformer so far.
The proposed work should be evaluated in at least one or two more video benchmarks. The nature of K400 and K600 is the same. I would suggest also include Something-Something-V2 or Epic-Kitchen datasets.
[1] Multiscale vision transformers, ICCV 2021","1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention.",0.0,0.0,0.0,0.0,nan,-1,0,-1,-1
198,ICLR_2022_32,"Weaknesses: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper","- The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool.",-1.0,-1.0,-1.0,0.0,nan,-1,-1,-1,-1
4113,NIPS_2020_558,"- Further experiments: I would've liked to see more experimental results on additional graph problems. This could drive your point further by showing more examples of how the loss function can be constructed. - Scalability bottleneck: GNN training can only scale up to some level. Thoughts on whether your method can be scaled up despite this limitation? - Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.","- Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.",1.0,1.0,-1.0,0.0,nan,1,1,-1,1
3237,NIPS_2020_1454,"- The novelty seems limited. The idea of building correlation in low-res and then refine for or facilitate the high-res results might be new in the literature for correspondence search, but it is quite common and has been widely adopted in previous work doing stereo match, where the main job is also to find correspondence but along epipolar line. - The main contribution of this paper, IMO, is to run 4D convolution on low-res correlation volume, which saves computation and possibly achieve comparable performance. If so, the experiment showing the saving of computational resources, e.g. gpu runtime, flip-flop, memory, must be given. - Similar multi-scale approach in stereo matching often runs fast at the cost of losing accuracy, since the correlation volume in low-res is not as informative as the high-res one, and it is not easy to fix if some mistakes are made in low-res. However the experiments show that the result is even better than SOTA. It would be good to add more explanation and analysis. - It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.","- It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.",1.0,-1.0,-1.0,1.0,nan,-1,1,-1,-1
5051,NIPS_2020_1889,"- The performance of the method is still sub-optimal on large scale datasets, and hence there is a scope of improvement. - The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.","- The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.",nan,nan,nan,nan,nan,-1,0,-1,-1
663,ICLR_2021_2455,"Weaknesses: In spite of the strengths mentioned above, there are a few questions that are confusing. 1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed. 2. As for the theoretical analysis: a) For Theorem 1, the right hand equation uses L_2 norm of a function of beta. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Therefore, I wonder what does the regularizer proposed in equation(6) means since beta has already been integrated. b) For Theorem 1, the core assumption is ‘the expected loss function as a function of beta belongs to a Sobolev space’, which is confusing. Could you provide some explanations of this assumption or give some examples of it? c) Theorem 1 provides an upper bound for one specific kind of DRO problem whose uncertainty set is formulated as an affine combination of training distributions. However, in this article, the authors do not state what is the definition of the invariance here and why solve such DRO problem could achieve the invariance. 3. As for the proposed objective function: a) As mentioned above, the L_2 norm is taken over a function of beta, which I think is not the Euclidean norm of the vector. Beta has already been integrated and this regularizer has nothing do with beta. I wonder how to compute this when optimizing? b) I wonder how this objective function can be optimized efficiently? The first concern is mentioned above as the computation of L_2 norm. The second concern is how to optimize the variance which is non-convex and hard to optimize. Namkoong et al. [1] convert the optimization of a variance-regularized problem to a f-divergence DRO for better optimization, while in this paper the authors take the opposite way. I wonder is there any theoretical guarantee of the optimization of the objective function(6). 4. As for the experiments: a) The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in 3).
[1] Duchi, J. , & Namkoong, H. . (2016). Variance-based regularization with convex objectives.","1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed.",nan,nan,nan,nan,nan,-1,-1,-1,1
4778,NIPS_2020_770,"1. All components, BGV and TFHE, are borrowed from other papers. Similar to Chimera, the switching mechanism is not proposed by the authors. Even transfer learning has developed for a long time. The authors are suspect of just ensembling these ideas together. The reviewer doubts the novelty of the proposed method. 2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.",2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.,nan,nan,nan,nan,nan,-1,0,-1,-1
2812,NIPS_2022_1430,"Weakness:
One major issue with this paper is clarity of text and definitions. Examples:
Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature
Equation 3, P ( < s > | x ) = ∑ y ∈ A P ( y | x )
, I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)?
text issues:page 5, ""is is useful think ...""
Many issues in Table1: 4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean? 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo?
The definition of the randomly split the words in bottom of page 6 is not clear to me.
The notation pDrop, how about p_drop ?
Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant.
Finally, there is a major concern about baseline comparisons:
All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3.
STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?","4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean?",nan,nan,nan,nan,nan,0,1,-1,-1
1572,ICLR_2023_3811,"Weaknesses
Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term ""coherence"" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining ""coherence"" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words ""turn"" and ""utterance"" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: ""entailment"", ""contradiction"" and ""neutral"". I suggest specifying that the 1 label in the numerator corresponds to the ""entailment"" class, and whether you consider both ""contradiction"" and ""neutral"" as a single ""non-entailment"" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the ""instances for evaluation"" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the ""Utterance Level Sampling"" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it ""tends to sample previous utterance to be replaced"" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the ""Coherence Rate"" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results ""w/ Noise"" described on page 6, ""w/ Utterance"" and ""w/ Semi-Utterance"" described on page 4, but you also mention ""w/ Hierarchical"". Up to this point I had understood both ""Utterance Level Sampling"" and ""Semi-utterance Level Sampling"" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the ""w/ Hierarchical"" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating ""Rep""? I suggest clarifying this.
On page 7 in the ""Sampling vs w/o Sampling"" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the ""Human Evaluation"" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the ""Explicit Coherence Optimization"" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your ""hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking"". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you ""demonstrate these methods make chatbots more robust in real-world testing"" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term ""coherence"" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition ""Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts."" is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, ""is coherence"" should be ""that is coherent"".
P. 3, p. 4, l. 1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index ""1"" mean in U ^ 1 i − 1
? Does it mean ""starting from index 1""? If this is the case and you never use anything other than ""1"" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to ""conduct"" a classifier does not make much sense. You can either ""conduct"" classification or ""train"", ""use"", ""create"", etc. a classifier.
P. 4, p. 4, l. 4: here you say ""we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: ""Given a training pair U ^ 1 t − 1
"" should be ""Given a training pair U ^ 1 ′ t − 1
"", i.e. the training pair contains an utterance replaced through the ""Utterance Level Sampling"" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it.
P. 5, p. 3, l. 3: ""two annotators are employed"" -> ""two annotators were employed""
P. 5, p. 5, l. 3: The sentence starting with ""As model-based methods"" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: ""Following previous work (Ritter et al., 2011)"" -> ""Following the work by Ritter et al. (2011),""
P. 5, p. 8, l. 2: ""to online evaluate these two methods"" -> ""to evaluate these two methods online""
P. 6, p. 1, l. 2: ""non-repetitive"" -> ""non-repetitiveness""
P. 6, p. 6, l. 1: ""After sample an utterance"" -> ""After sampling an utterance""
P. 6, p. 8, l. 2: ""generate coherence response"" -> ""generate coherent responses""
P. 6, p. 8, l. 4: ""with the number of turns increases"" -> ""as the number of turns increases""
P. 7, Figure 4, a - b: The y-axis should be labeled ""coherence (%)"" instead of ""coherent (%)"". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence ""since sampled noises are difficult to accurately simulate errors of the inference scene during training"" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): ""Contradition"" -> ""Contradiction""; (b): ""Coherent"" -> ""Coherence"". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: ""hierarchy way"" -> ""hierarchical way""
P. 9, p. 2, l. 1: ""incoherence response"" -> ""incoherent response""
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.","1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow. P. 3, p. 4, l.",nan,nan,nan,nan,nan,1,1,0,1
5570,NIPS_2020_133,"(-) Some claims are way too strong. *“we study and prove the incapability of standard activation functions to extrapolate” is debatable. The proof is mainly related with ReLU and tanh. * After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.","* After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.",nan,nan,nan,nan,nan,-1,-1,-1,1
4522,NIPS_2020_988,"The introduced method relies on sparse distributions, which is a quite strong assumption. While the authors address the main implications of this assumption, I think there should have been an even more detailed discussion/empirical evaluation to increase the impact of this work in the community: - in the semi-supervised learning experiments in section 5.1 you use a VAE model which is relatively simple and by now quite outdated. Do you expect these results to generalize to more complex architectures? For example, if I took any of the SOTA semi-supervised deep generative models and just replaced the softmax with the sparsemax would you expect similar improvements? - how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data? - since sparsemax is such a core component of this method, it would be useful to add some details on its forward/backward passes and their computational complexity wrt the softmax.",- how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data?,nan,nan,nan,nan,nan,0,0,-1,-1
141,ICLR_2022_1421,"Weakness:
For the Per-SSFL framework, the local (personalized) model and global model are used. The memory consumption aspect should be discussed. For resource-constrained edge clients, high memory cost could be an issue.
Although it mentioned in the implementation setting that the client number selected per round is 10, it is not clear how many total clients are used in the FL setting.
In Table 1, what is the FL method under the supervised setting?
In Figure 2, what does (0.5) for SSFL on IID and SSFL on non-IID mean?
As can be seen in Figure 2, the convergence rates for the IID and non-IID cases are quite similar. Can you provide an explanation for that?
After reading Appendix D and Figure 10 in Appendix, the experimental setting on GLD-23K is still not quite clear. For example, how the local training set for each client is generated? What about the label distribution? Also, it seems that the number of clients used on the GLD-23K is different from that on CIFAR-10.
For ease of comparison and implementation, it would be good to evaluate the method on more commonly used datasets such as CIFAR-100 and Tiny-ImageNet and other datasets besides vision datasets (e.g., text) for FL.
It would be interesting to see the SSFL results under different numbers of selected participant clients. Since λ
is an important parameter that balances consensus and personalization, its effect should be studied.
Minor issues:
Section 2.1, “… the local empirical risk over the heterogeneous dataset D k
.” -> D k
Figure 10 appeared in Sec. 5.2 and 5.3, it should be Fig. 2 and Fig. 3.
In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent.
Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)”
A careful proofread of the paper is highly recommended.","3. In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent. Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)” A careful proofread of the paper is highly recommended.",nan,nan,nan,nan,nan,-1,-1,-1,1
157,ICLR_2022_939,"Weaknesses
Novelty is somewhat low.
Discussion of gradient explosion seems inaccurate.
Some discussion of related work is missing, particularly about related works in the model-based RL literature.
Sometimes they are stating things as facts without providing evidence.
There was no detailed discussion of how the results with the current simulator relate to other recent simulators such as Brax. Are the wallclock times similar? The results seem mixed, but having a detailed discussion would have been useful.
The simulator is fast, so it may have been better to do more than 4 environments. Recommendation
I recommend accepting the paper as I have not seen differentiable simulators used for tasks of the difficulty considered here (while also taking advantage of differentiating through the simulator). I was considering a score of 6 or 8, but gave 6 for now.
Discussion of points brought up
Novelty: It seems the contribution is primarily one of engineering, and they don't propose any surprising new idea. The idea of truncated backpropagation is old. Moreover, the policy training scheme resembles that of the the Dreamer algorithm (Hafner et al. 2019). Dreamer also uses short horizon rollouts together with a terminal value function, and backpropagates through these short horizons to optimize the policy. The differences are only: Dreamer uses lambda return weighting during the short horizons (why didn't you use this?), and the method of constructing the sequences is different (Dreamer samples start states from a replay buffer and performs model rollouts from these states, while the current work splits an episode into chunks). I am surprised that Dreamer was not discussed when explaining the methodology. Certainly it should be mentioned that there are prior works using a similar policy optimization procedure (with slight variations).
Discussion of gradient explosion: There are other earlier works that do a more detailed job of discussing the gradient and landscape issues, such as PIPPS (Parmas et al. 2018), which should have been cited and discussed (moreover your methodology was very similar to these previous works). In your work, if gamma were 1, the value function were perfect and the policy were deterministic, the gradient you compute with your method should be exactly the same as the gradient that is computed using BPTT. From this point of view, your discussion is insufficient, as you do not explain why the loss landscape and gradients end up being smooth despite this fact. I can think of two possible reasons: 1. The value function is an approximator that ends up being smooth because of limited capacity to model the complicated landscape. 2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance.
For the other points see the raw notes at the bottom of this section.
Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions
Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?
Q2. In Figure 2, did you use a deterministic or stochastic policy? Was this the same policy as was used during training the value function? If the policy was stochastic, then how did you evaluate the landscape? This would require sampling many trajectories with the same policy and averaging. Are the scales on the left and right figures the same?
Q3. Did you do any ablation study of the policy noise? Does the method still work when the policy is deterministic? How much does the performance drop?
Additional notes made during reviewing
""for systems ranging from robots (e.g., Cheetah, Shadow Hand) to complex anima- tion characters (e.g., muscle-actuated humanoids) using only high-level reward definitions."" Please provide references.
""A differentiable simulator provides accurate first-order gradients of the task performance reward with respect to the control inputs."" This is speculation. You provide no evidence. Problems with accuracy can arise when the task performance depends on a sampled initial state (so that the gradients are inherently stochastic). Perhaps change to ""may provide"".
""However, despite the availability of differentiable simulators, it has not yet been convincingly demonstrated that they can effectively accelerate policy learning in complex high-dementional and contact-rich tasks, such as some traditional RL benchmarks."" While this may be the case for differentiable simulators, there are several model-based RL works that showed effective learning (e.g., Dreamer. The difference with a simulator is just that the model does not have to be learned. I think the claim here is downplaying such previous contributions. Also, ""dementional"" should be ""dimensional"".
""There are several reasons for this: 1.(), 2.(), 3.()"" These reasons are stated as facts, while they are speculations. Perhaps, ""possible reasons"" would be better. At least points 1 and 3 should be the same for model-free RL, so are they really the reason? No references were provided.
""Because of these challenges, previous work has been limited..."" How do you know that those were the challenges that limited the applicability of the previous methods? The publications themselves do not seem to note your reasons as the reason why they limited their experiments. For example, the PODS paper says their method overcomes the issues of exploding gradients.
""In addition, we propose a truncated learning window to shorten the backpropagation path to address problems with vanishing/exploding gradients and reduce mem- ory requirements."" This is known as ""truncated backpropagation"". It is plagiarism to claim that you ""proposed"" this.
In Equation 1, please provide the definition of the Jacobian's and gradients. Usually, the gradient is a row vector, whereas you are using a column vector.
""This makes the reward function smoother..."" What do you mean by this? The reward function is the same in all cases.
""In addition, we apply state normalization as is common in RL algorithms."" Please explain what ""state normalization"" is.
""First, the terminal value function absorbs the discontinuity of long dynamics horizons and early termination into a smooth function, as shown in Figure 2 (Right)."" This explanation is incomplete. If you had no discount factor, a perfect value model and a deterministic policy, your computed policy gradient would be exactly the same as that of BPTT.
""Finally, the use of short horizons allows us to update the actor more frequently, which, when combined with parallel differentiable simulation, results in a significant speed up of training time."" Do you have an ablation study showing that it speeds up?
""In contrast, our method scales well due to direct access to the true gradients from differentiable simulation."" You don't have access to true gradients when you are using a stochastic policy. It may be better to explain this by referring to the fact that reparamterization gradients are often more accurate when computing gradients of smooth functions.","2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance. For the other points see the raw notes at the bottom of this section. Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?",nan,nan,nan,nan,nan,-1,0,1,1
339,ICLR_2022_2842,"Weaknesses: 1. In introduction section, author claimed ""Representation Topology Divergence (RTD) score which measures a dissimilarity between two point clouds of equal size with one-to-one correspondence between points"". The question is how to explain ""one-to-one correspondence between points"", what does ""one-to-one"" mean here? It seems this is an important part of contribution, but there is no specific explanation about this so it is confusing me.
The writing is not clear. For example, some concepts in section 2 are not well explained, such as Barcode, Vietoris Rips complex and homology group are not be familiar with ML community. It should be better to add some figures and intuitive explanation about these abstract topology terms.
3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score?
It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?","3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score? It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?",nan,nan,nan,nan,nan,-1,-1,-1,-1
698,ICLR_2021_1682,"weaknesses
+ The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.
+ The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.
+ The paper is well-written, easy to follow, and well-connected to the existing literature.
- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.
- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.
- A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments. Recommendation
I’m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that’s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.
Detailed justification
The biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I’m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes while keeping other hyperparameters fixed, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that ""NCA performs better than all PN configurations, no matter the batch size"" with a grain of salt, for instance.
I also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission’s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don’t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.
The connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to ""show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis"", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings – albeit with a Euclidean metric rather than a cosine similarity metric – since both perform comparisons on example pairs.
This relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between ""1-NN with class centroids"" and k-NN / Soft Assignment noted in Section 4.1 – as well as the drop in performance observed in Figure 4’s row 6 – be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?
Finally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don’t know how competing approaches would perform if we applied the paper’s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA’s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant. Questions
In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?
Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?
Can the authors elaborate on the ""no S/Q"" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn’t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class’ prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.
Additional feedback
This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven’t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that ""in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]""; can the authors point to the implementations they have in mind when making that assertion?
I would be careful with the assertion that ""during evaluation the triplet {w, n, m} [...] must stay unchanged across methods"". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.
I’m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.","- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.",nan,nan,nan,nan,nan,-1,0,-1,-1
873,ICLR_2023_507,"Weaknesses
I encourage the authors to put forward the general notion of sparsity that is assumed across the paper (as defined in S ( w )
) early on in the introduction so that reader can follow the ideas put forward in Fig. 1.
One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent.
From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal)
By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?","1. One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent. From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal) By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?",nan,nan,nan,nan,nan,1,1,-1,-1
2056,ARR_2022_18_review,"1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7. The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1, and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? 
See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda = 1.5 nA -> na (short vowel) ",2. No details on the. multi-task learning mentioned in Section 4.4 are available.,nan,nan,nan,nan,nan,-1,0,-1,1
1900,ARR_2022_334_review,"- There are some technical aspects of the paper that weren't clear to me:  * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
 * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
   * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. 
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) ",- There are some technical aspects of the paper that weren't clear to me:,nan,nan,nan,nan,nan,-1,-1,0,-1
2242,ACL_2017_699_review.json,"Weaknesses: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1. 
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally,    The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc. 
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845). ","4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.",nan,nan,nan,nan,nan,-1,-1,-1,-1
4173,NIPS_2020_1855,"Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as ""undecided"", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting. - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the ""raw"" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening? - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have ""training the neural representation"" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization? - More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.","- More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.",nan,nan,nan,nan,nan,-1,1,0,-1
1587,ICLR_2023_2147,"Weakness
While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it.
The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained.
I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.).
While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean).
Question to authors
During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error.
I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample m
has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level ( ϵ m , k
) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable.
[1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188.
[2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594.
Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?","2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188. [2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery.",nan,nan,nan,nan,nan,-1,1,-1,-1
3293,NIPS_2020_1480,"1. The novelty of Theorem 1 is not entirely clear. The authors argue that the classical another parametrization B, Theta = (C Omega^{1/2}, Omega^{1/2}), which also enables joint convexity. Therefore, Theorem 1 doesn't appear to be a significant result in itself. This may be a lemma instead. 2. It is not clear why the classical parametrization does not allow penalty functions. One can include penalties on C by using the classical parametrization and levering C=B Theta^{-1}, e.g., || B Theta^{-1}||_{1,2} where 1,2 is the group L1 penalty. 3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage. 4. This work needs a more detailed comparative analysis in order to prove the superiority of the proposed approach. In particular, paper is lacking an adequate computational complexity and run-time analysis with respect to the existing methods, e.g. the standard parametrization and penalty approach. A similar weakness also exists in the numerical results section, where comprehensive empirical comparisons are lacking. The supplementary material has a numerical table, which shows a very incremental improvement and is not conclusive. 5. In proving Theorem 3, the authors employ standard methods such as restricted eigenvalues. These methods are already known to extend to non-convex objective functions and does not specifically hold for the claimed convex formulation.","3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage.",nan,nan,nan,nan,nan,-1,-1,-1,-1
1870,ARR_2022_14_review,"-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here. - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. 
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. ","- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.",nan,nan,nan,nan,nan,-1,-1,-1,-1
2144,ARR_2022_61_review,"1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. 
1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. ","2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims.",nan,nan,nan,nan,nan,-1,1,-1,-1
1678,ICLR_2023_1654,"Weaknesses:
I believe the paper has a couple of fundamental weaknesses as it stands that should be addressed before it could be accepted at ICLR. 1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons. 2. The use of he Oracle twice is circular - therefore invalidates the claims of performance; there are some ways around this that the authors could try More minor weaknesses: 3. the reference used to justify the evaluation metrics is Hoffman et al 2022 - but this paper is about optimising small molecules - which are v different in ""seq distance to function"" relationships - this is especially important in relation to the point about the Novelty measure above.
4. AlphaFold is not at all appropriate to support the claim of functional sequence optimisation - there are may mutations that will cause a protein to unfold that Alpha fold will predict as having almost exactly the same structure as it will align etc - therefore it proves nothing ( From their own FAQ page ""AlphaFold has not been validated for predicting the effect of mutations. In particular, AlphaFold is not expected to produce an unfolded protein structure given a sequence containing a destabilising point mutation."" And there are papers writing about this eg Pak et al 2021","1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons.",nan,nan,nan,nan,nan,1,-1,-1,1
4544,NIPS_2020_1461,"1. The title seems misleading, the method from my understanding is a way to use GNNs for learning natural laws, or understand GNNs in terms of symbolic models, rather than extracting symbolic models from general deep learning models. If the latter is their main claim, it would be helpful if the authors show the utility of this model on more general tasks, other than natural laws. 2. The paper lacks a strong related work section, missing key details about models that have been used previously for relevant tasks. For example, the authors have not discussed how other papers like Neural Relational Inference or using Generative Models for their tasks. 3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model. 4. It would be helpful if the authors address the natural question how utilizing this model for a general deep learning model is more interpretable or generalizable especially in the cases of noise? 5. The paper needs to add more details in the main draft. Key details are in the appendix, the authors should include majority of Section A.1 in the main paper rather than the appendix. The simulation details should be explained more, are the authors generating time series datasets, the number of samples, length of the time series, how the trajectory is processed etc. While some details are covered in the appendix, these details are essential to understand the task and model.","3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model.",nan,nan,nan,nan,nan,-1,1,-1,-1
2574,NIPS_2019_175,"Weaknesses: 1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.  ","5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.",nan,nan,nan,nan,nan,-1,-1,0,0
2182,ACL_2017_614_review.json,"Weaknesses: - I don't understand effectiveness of the multi-view clustering approach. 
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                  - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub? ","- The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).",nan,nan,nan,nan,nan,1,1,-1,1
4413,NIPS_2020_1552,"* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs. * Relation to relevant prior work is not mentioned and elaborated. For example, Rajasegaran, et al. ""Random path selection for continual learning."" NeurIPS'19 also propose a similar masking based approach to learn non-overlapping paths for dissimilar tasks. Similarly, PathNet (Evolution Channels Gradient Descent in Super Neural Networks) selectively masks out irrelavent model paramters. These papers should be cited and disucssed (preferably compared against) in this manuscript. * To my understanding, the notion of similar and dissimilar tasks is not accurate. E.g., the prior works on task incremental learning have both sets of similar and dissimilar tasks. (E.g., consider CIFAR100 classes in GEM - NeurIPS'17). In fact the considered set of similar and dissimilar tasks is not too different from the ones considered in earlier works. Specifically, consider a seminal work from Li & Hoeim, ""Learning without forgetting"" (TPAMI), where different datasets such as ImageNet/Places365/VOC/CUB/Scenes/MNIST are considered in continual learning experiments). Nevertheless, the proposed splits and dataset choices should be properly motivated and the authors should also report some experiments on previously considred protocols for fair benchmarking against existing methods. * The annealing strategy is somewhat similar to controller proposed in iTAML (iTAML : An Incremental Task-Agnostic Meta-learning Approach - CVPR'20). * The approach assumes that the task ID is known beforehand. Although this is consistent with some prior works, isn't it a bit restrictive in practical settings? It would be good to explain some application scenarios where tasks ID can be known to motivate the readers. * Equation 3 is wrong, it should be explicitly written. * The caption of Figure 1 should have some description for the MTCL architecture (a) as well.","* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs.",nan,nan,nan,nan,nan,-1,1,-1,1
4950,NIPS_2020_1842,"1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper. 2. The claimed effect of ProtoMod needs more evaluations. The authors provide some qualitative results in Figure 3 and Fig Q.3 & Q.4 (from supp.) to show the proposed ProtoMod can localize the attributes like 'black back' and 'yellow breast'. What concerns me is whether the model is indeed learning the specific attributes (black back) or only the partial semantic like 'black' or 'back'. The part localization experiments (Fig 2 and Table 3) may partially show the model can learn the part information ('back', 'belly') from attributes but does not show if the model can distinguish between 'black back' or 'yellow back'. Neither the attribute similarity maps from Fig 3 and Fig Q.3 & Q.4. I think some qualitative results of the same images with locations of attributes only differ partially (e.g. 'black back' vs. 'yellow back' vs. 'black breast') can further verify whether the ProtoMod is learning the specific attributes or just the color or parts. --------- Ref: R1: Zhu, Pengkai, Hanxiao Wang, and Venkatesh Saligrama. ""Learning classifiers for target domain with limited or no labels."" ICML 2019. --------- Updates: the authors address my concerns in the rebuttal. I raise my score to accept.","1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper.",nan,nan,nan,nan,nan,1,1,1,-1
1345,ICLR_2023_1035,"Weaknesses: 1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc.
The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments.
I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e. 600 episodes for each dataset. This is helpful for indicating the chance of handling such hard tasks in real-life scenarios where hard and easy tasks are mixed.
[1] Powering Finetuning for Few-shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. AAAI 2022 [2] Cross-domain few-shot learning with task-specific adapters. CVPR 2022 [3] Exploring Efficient Few-shot Adaptation for Vision Transformers. TMLR 2022","1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc. The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments. I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e.",nan,nan,nan,nan,nan,1,-1,-1,-1
2214,ACL_2017_66_review.json,"Weaknesses: The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).
- General Discussion: The current presented approach, in comparison to existing approaches, is promising. ","- General Discussion: The current presented approach, in comparison to existing approaches, is promising.",nan,nan,nan,nan,nan,-1,1,-1,-1
4320,NIPS_2020_386,"The weaknesses are : 1. It is not clear how the reparametrization differ from the original parametrization with low rank covariance in terms of predictive performance. 2. It is not clear why the more expressive diagonal covariance is less predictive than the scaled identity covariance. 3. Why the latter covariance is more computationally efficient than the former for deep neural networks? 4. Given that the ELRG-VI has worse accuracy as in the Modern CNNs, what is the advantage of using this approximate posterior? 5. Evaluation of the quality of the posterior approximation by predictive performance is not appropriate as in many cases MAP can give more accurate predictions.",3. Why the latter covariance is more computationally efficient than the former for deep neural networks?,nan,nan,nan,nan,nan,-1,0,-1,-1
5685,NIPS_2018_639,"Weakness: - I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.  - How does the reader know that the reward definitions of rho for this tasks necessitates a non-convex reward function. Surveillance and data collection has been studied in POMDP context by many papers. Fortunately/unfortunately, many of these papers show that the increase in the reward due to a rho based PWLC reward in comparison to a corresponding PWLC state-based reward (R(s,a)) is not that big. (Papers from Mykel Kochenderfer, Matthijs Spaan, Shimon Whiteson are some I can remember from top of my head.) The related work section while missing from the paper, if existed, should cover papers from these groups, some on exactly the same topic (surveillance and data collection).  - This basically means that we have devised a new method for solving non-convex value function POMDPs, but do we really need to do all that work? The current version of the paper does not answer this question to me. Also, follow up question would be exactly what situation do I want to use the methodology proposed by this paper vs the existing methods.  In terms of critisim of significance, the above points can be summarized as why should I care about this method when I do not see the results on problem the method is supposedly designed for.  ","- I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.",nan,nan,nan,nan,nan,-1,-1,-1,-1
304,ICLR_2022_113,"Weaknesses: - The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive? - The method focuses on learning cluster granularity for the object only, and not for the background. - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)
A few comments on the text: - The phrase ""coarse-grained images"" is inaccurate, the ""coarse-grained"" adjective should refer to the clustering and not the images (in the intro). - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro. - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)",- It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline) A few comments on the text:,nan,nan,nan,nan,nan,-1,0,-1,-1
1661,ICLR_2023_3449,"Weaknesses
1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf","3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.). [1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",nan,nan,nan,nan,nan,-1,0,-1,-1
2896,NIPS_2022_1666,"Weaknesses:
I cannot give a clear acceptance to the current manuscript due to the following concerns:
1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work.
For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work.
2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1.
3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper.
In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.
4. Advantage of NAR(heatmap) + RL + Meta-Learning: From Table 1&2, for TSP1000, the generalization performance of AM (G: 31.15, BS: 29.90) trained on TSP100 is not very far from the testing performance of DIMES without meta-learning (27.11) directly trained on TSP1000. It could be helpful to check whether the more powerful POMO approach[4] can have a smaller performance gap. Reporting the results for POMO and DIMES without meta-learning for all instances in Table 1 could make the advantage of the NAR(heatmap) + RL approach in DIMES much clearer.
Hottung et al.[5] shows that POMO + Efficient Active Search (EAS) can achieve promising generalization performance for larger TSP instances on TSP and CVRP. The comparison with POMO + EAS could be important to better evaluate the advantage of meta-learning in DIMES.
[1] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, Thomas Laurent, and Xavier Bresson. Learning tsp requires rethinking generalization. arXiv preprint arXiv:2006.07054,2020.
[2] Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.
[3] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. NeurIPS 2018.
[4] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min. POMO: Policy optimization with multiple optima for reinforcement learning. NeurIPS 2020.
[5] André Hottung, Yeong-Dae Kwon, and Kevin Tierney. Efficient active search for combinatorial optimization problems. ICLR 2022.
Yes, the limitations have been adequately addressed in Section 5 Concluding Remarks. I do not see any potential negative societal impact of this work.","1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work. For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work. 2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1. 3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper. In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.",nan,nan,nan,nan,nan,-1,-1,1,1
5265,NIPS_2020_1700,"- There are a number modelling details that are not entirely clear in the paper (see below). - When comparing Gumbel-CRF directly to Perturb-and-MAP MRF, performance is very similar.",- There are a number modelling details that are not entirely clear in the paper (see below).,nan,nan,nan,nan,nan,-1,0,-1,-1
3860,NIPS_2020_1228,"- The method section looks not self-contained and lacks descriptions of some key components. In particular: * What is Eq.(9) for? Why ""the SL is the negative logarithm of a polynomial in \theta"" -- where is the ""negative logarithm"" in Eq.(9)? * Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained. - The proposed approach essentially treats the structured constraints (a logical rule) as part of the discriminator that supervises the training of the generator. This idea looks not new -- one can simply treat the constraints as an energy function and plug it into energy-based GANs (https://arxiv.org/abs/1609.03126). Modeling structured constraints as a GAN discriminator to train the generative model has also been studied in [15] (which also discussed the relation b/w the structured approach with energy-based GANs). Though the authors derive the formula from a perspective of semantic loss, it's unclear what's the exact difference from the previous work? - The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.","* Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained.",nan,nan,nan,nan,nan,-1,1,-1,-1
2038,ARR_2022_256_review,"1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' ","1. I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this!",nan,nan,nan,nan,nan,-1,0,-1,-1
5573,NIPS_2018_641,"weakness.  First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a ""good"" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless.  Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the ""learning rate"" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD.  The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes.  I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication.  Other comments: 1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. ""Gradient methods for minimizing composite functions"" by Yu. Nesterov. 2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate.  4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties. 5. The proofs in the supplementary material are too terse. ","2. ln.55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter.",nan,nan,nan,nan,nan,-1,-1,-1,1
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way.",nan,nan,nan,nan,nan,-1,1,-1,1
3822,NIPS_2020_179,"1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack. 2. Suppose the evaluation reflects the model robustness, the region for “small perturbations” is quite small. In CIFAR10, the proposed mechanisms only work on par with adversarial training at epsilon=0.001. I would think even epsilon=8/255 on CIFAR10 are not perceptible by humans. 3. In Table 1, it will be interesting to integrate the proposed fixations with SOTA models. The reported RESNET result is not the SOTA ones. 4. In the gradient obfuscation part, it will be good to clarify what does the authors “verified”. 5. Last paragraph in the conclusion generalizes the work a bit too much. 6. There is no adversarial training baseline in ImageNet10, ImageNet results.","1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack.",nan,nan,nan,nan,nan,-1,-1,-1,-1
3001,NIPS_2022_489,"Weaknesses:
Concern regarding representativeness of baselines used for evaluation
Practical benefits in terms of communication overhead & training time could be more strongly motivated
Detailed Comments:
Overall, the paper was interesting to read and the problem itself is well motivated. Formulation of the problem as an MPG appears sound and offers a variety of important insights with promising applications. There are, however, some concerns regarding evaluation fairness and practical benefits.
The baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE. In particular, there have been a variety of recent works that explore more efficient strategies (e.g., [1-3]) and consistently outperform QMix with relatively low inter-agent communication. Although the proposed work appears effective as a fully-decentralized approach, it is unclear how well it would perform against more competitive CTDE baselines. Comparison against these more recent works would greatly improve the strength of evaluation.
Benefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.
Robustness against dynamic changes in network setup are discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches.
Some confusion in notation: - Algorithm 2, L8 should be t = 1,…,H (for horizon)? - L100, [M] denotes the set of LBs?
Minor notes: - Some abbreviations are not defined, e.g., “NE” on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read.
[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. [2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. [3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.","- L100, [M] denotes the set of LBs? Minor notes:",nan,nan,nan,nan,nan,0,0,0,1
924,ICLR_2023_4092,"Weakness: 1. The writing and setting up of this paper are not very clear to me. The motivation does not convincing to me. 2. The novelty of the proposed method seems trivial. Contrastive learning, embedding alignment and etc have been widely discussed in the previous literature.
3. For XQR performance, it is not surprising that with the additional selected XOR finetuning examples, the performance will boost further. 4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.",4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.,nan,nan,nan,nan,nan,-1,0,-1,-1
4420,NIPS_2020_341,"- For theorem 5.1 and 5.2, is there a way to decouple the statement, i.e., separating out the optimization part and the generalization part? It would be clearer if one could give a uniform convergence guarantee first followed by how the optimization output can instantiate such uniform convergence. - In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.","- In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.",nan,nan,nan,nan,nan,-1,0,-1,-1
5664,NIPS_2018_185,"Weakness: ##The clarity of this paper is medium. Some important parts are vague or missing. 1) Temperature calibration: 1.a) It was not clear what is the procedure for temperature calibration. The paper only describes an equation, without mentioning how to apply it. Could the authors list the steps they took? 1.b) I had to read Guo 2017 to understand that T is optimized with respect to NLL on the validation set, and yet I am not sure the authors do the same. Is the temperature calibration is applied on the train set? The validation set (like Guo 2017)? The test set? 1.c) Guo clearly states that temperature calibration does not affect the prediction accuracy. This contradicts the results on Table 2 & 3, where DCN-T is worse than DCN. 1.d) About Eq (5) and Eq (7): Does it mean that we make temperature calibration twice? Once for source class, and another for target classes? 1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain? 2) Uncertainty Calibration From one point of view it looks like temperature calibration is independent of uncertainty calibration, with the regularization term H. However in lines 155-160 it appears that they are both are required to do uncertainty calibration. (2.a) This is confusing because the training regularization term (H) requires temperature calibration, yet temperature calibration is applied after training. Could the authors clarify this point? (2.b) Regarding H: Reducing the entropy, makes the predictions more confident. This is against the paper motivation to calibrate the networks since they are already over confident (lines 133-136). 3) Do the authors do uncertainty calibration on the (not-generalized) ZSL experiments (Table 2&3)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 4) Do the authors do temperature calibration on the generalized ZSL experiments (Table 4)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 5) The network structure: 5.a) Do the authors take the CNN image features as is, or do they incorporate an additional embedding layer? 5.b) What is the MLP architecture for embedding the semantic information? (number of layers / dimension / etc..) ##The paper ignores recent baselines from CVPR 2018 and CVPR 2017 (CVPR 2018 accepted papers were announced on March, and were available online). These baseline methods performance superceed the accuracy introduced in this paper. Some can be considered complementary to this work, but the paper canât simply ignore them. For example: Zhang, 2018: Zero-Shot Kernel Learning Xian, 2018: Feature Generating Networks for Zero-Shot Learning Arora, 2018: Generalized zero-shot learning via synthesized examples CVPR 2017: Zhang, 2017: Learning a Deep Embedding Model for Zero-Shot Learning  ## Title/abstract/intro is overselling The authors state that they introduce a new deep calibration network architecture. However, their contributions are a novel regularization term, and a temperature calibration scheme that is applied after training. I wouldnât consider a softmax layer as a novel network architecture. Alternatively, I would suggest emphasizing a different perspective: The approach in the paper can be considered as more general, and can be potentially applied to any ZSL framework that outputs a probability distribution. For example: Atzmon 2018: Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning Ba 2015: Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions  Other comments: It will make the paper stronger if there was an analysis that provides support for the uncertainty calibration claims in the generalized ZSL case, which is the focus of this paper. Introduction could be improved: The intro only motivates why (G)ZSL is important, which is great for new audience, but there is no interesting information for ZSL community. It can be useful to describe the main ideas in the intro. Also, confidence vs uncertainty, were only defined on section 3, while it was used in the abstract / intro. This was confusing. Related work: It is worth to mention Transductive ZSL approaches, which use unlabeled test data during training, and then discriminate this work from the transductive setting. For example: Tsai, 2017: Learning robust visual-semantic embeddings. Fu 2015: Transductive Multi-view Zero-Shot Learning I couldnât understand the meaning on lines 159, 160. Lines 174-179. Point is not clear. Sounds redundant. Fig 1 is not clear. I understand the motivation, but I couldnât understand Fig 1. ","1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain?",nan,nan,nan,nan,nan,-1,0,-1,-1
3447,NIPS_2020_486,"The method is relatively straightforward and some directions could be explored further. For instance, by using fully connected transformations, typical convolutional weight-sharing is not utilised. It would be good to discuss downsides to fully connected Woodbury transforms, and possible alternative formulations that would utilise convolutional weight-sharing. Further, the models utilised in the experimental section are quite small. As a result, the NLL performance is not very good compared to newer flow-based models. In addition the gains in NLL are quite small, and it would be better if the authors included standard deviations over multiple runs. Minor: - If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck. - For a better overview it would be nice to have a table showing the complexity for different methods for their forward/inverse/logdet in one place.","- If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck.",nan,nan,nan,nan,nan,1,1,0,-1
3749,NIPS_2020_755,"Sometimes the presentation is dense: a table, for example, would be a more efficient way to compare the derived rates with past results. There are a few discoveries I wish the authors would discuss a bit more, including: - the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible. - showing the bias-variance decomposition explicitly, at least one, would be nice - can you explain why the kernel is redundant when beta=2 (line 204)? - Since the claimed lower bound is novel, can you explain what is new about the construction?","- the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible.",nan,nan,nan,nan,nan,-1,0,-1,-1
1451,ICLR_2023_2368,"Weaknesses: 1. There is no theoretical guarantee that the discoveries resulting from the sparse network architecture are unique. 2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here. 3. It lacks model complexity analysis and comparison. Given different levels of biological entities' intra and inter interactions, I am worried about the model's real applicability.","2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here.",nan,nan,nan,nan,nan,-1,-1,-1,-1
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once.",nan,nan,nan,nan,nan,-1,0,-1,1
2710,NIPS_2022_327,"Weaknesses
I think the paper is hard to follow and read, in particular there are a lot of statistical details and some of the machine learning details can be lost in there. There is excessive use of alternative notation, for example η
is used for the conditional density p(Y = 1 | X = x), I think it is better to use probabilistic notation directly and not introduce additional greek letters as makes the notation hard to follow. Same can be said for risk definitions.
Overall about readability, I think the paper should be rewritten to put the terms that are usable for implementation from a machine learning perspective (like binary and multi-class uncertainty components), and leave derivations and proofs for the supplementary. The audience of NeurIPS is largely machine learning people, and the paper notation and arguments could be simplified so it is easier to follow for this audience. The statistical details are important but after reading this paper, if I want to implement this method, to me it is not clear what parts I should use or directly how this method works conceptually.
- The results in the paper only present out of distribution detection results as main result, and classification with rejection on text datasets. There are additional results on the supplementary but I believe these are minor. Overall I think there are missed opportunities for evaluation, for example, calibration error of epistemic uncertainty (Figure 2 could be a start point).
- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.
- Since density estimation is computationally expensive, the authors resort to an approximation of the kernel density estimate with nearest neighbors in feature space. I think this is fine, but there is no evaluation of approximation quality or ablations on how to select an approximation for nearest neighbors and kernel density estimates, the authors just use HNSW for this purpose.
Minor issues
I think a diagram showing how the method works and its training/inference process would be an easy way to understand the overall proposed approach.
- I think the paper is missing references to the text datasets (CoLA, SST-2, and MRPC) and other datasets used in the text classification experiments.
- I am not sure what score is used for OOD detection, my guess is that it is the epistemic uncertainty U e
, and this should be explicit in the evaluation section.
As I mentioned in weaknesses, the paper does not mention that it uses an approximation for the kernel density estimate, and this could be a limitation that could be addressed in the future.
There are no other limitations mentioned in the paper about the proposed method, even as the checklist says the paper does discuss limitations (I do not see where, I might be wrong).
Overall I see no negative societal impact to discuss.","- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.",nan,nan,nan,nan,nan,1,1,NO_LABEL,-1
4202,NIPS_2020_630,"- more analysis on other network architectures are required. analyzing merely the ResNet and VGG family is not enough. the authors shall include results on other families such as DenseNet, GoogleLeNet, etc. - how do the authors estimate the local volume density with kNN (L116)? if one simply uses the region that contains k nearest neighbors to define density, it will have some issues. For instance, it will have a lot of discontinuities. Can the authors be more specific on this and provide more explanations? - I'm slightly confused about Fig. 2(a). the number of measurements for \chi^{l, l+1} is different to (more than) those of \chi^{gt} and \chi^{out}. what happened? there seems to be more variantions for \chi^{l, l+1} when the measurement scale is more fine-grained - does it apply to \chi^{gt} and \chi^{out} too? - the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?","- the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?",nan,nan,nan,nan,nan,1,-1,-1,-1
1686,ICLR_2023_1725,"Weakness: • This method utilize a DETR pretrained on COCO which limits the application of this method. For some common used inpainting dataset like CelebA, FFHQ, Places, Paris street View, LSUN, it is hard to acquire the corresponding segmentation map to train the DETR. • The author claims ‘COCO-panoptic is more challenging than center-aligned datasets’, but the author should prove the generalization of the proposed method on other domain datasets like face (FFHQ)、buildings (Paris Street View). • One important application of inpainting is object removing while this method aims for new object generation. What about the performance of the proposed method on the object removing? • I find in most figures (Fig 1, Fig 3-5, Fig 8-9), there exist an obvious color discrepancy between the ground truth and the results of this paper. Can you make an explanation? • When processing large missing area, does the segmentation completion network still has the ability to generate new objects or just inpaint with background contents? • How is the model complexity and inference time compared to other methods?",• How is the model complexity and inference time compared to other methods?,nan,nan,nan,nan,nan,-1,0,-1,-1

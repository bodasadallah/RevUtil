{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of human reviews: 100\n",
      "Number of LLM reviews: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import json\n",
    "\n",
    "raw_human_reviews = []\n",
    "raw_llm_reviews = []\n",
    "\n",
    "### read the reviews from the jsonl files\n",
    "with open(\"human_reviews.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        raw_human_reviews.append(json.loads(line))\n",
    "with open(\"llm_reviews.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        raw_llm_reviews.append(json.loads(line))\n",
    "# print the number of reviews\n",
    "print(f\"Number of human reviews: {len(raw_human_reviews)}\")\n",
    "print(f\"Number of LLM reviews: {len(raw_llm_reviews)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### convert the reviews to a pandas dataframe, and for the human reviews only take entries that are in the llm reviews\n",
    "# human_reviews = pd.DataFrame(raw_human_reviews)\n",
    "# human_reviews['title'] = human_reviews['title'].str.strip()\n",
    "# human_reviews['title'] = human_reviews['title'].str.lower()\n",
    "# llm_reviews = pd.DataFrame(raw_llm_reviews)\n",
    "# llm_reviews['title'] = llm_reviews['title'].str.strip()\n",
    "# llm_reviews['title'] = llm_reviews['title'].str.lower()\n",
    "\n",
    "\n",
    "# matched_reviews = []\n",
    "\n",
    "# for index, row in llm_reviews.iterrows():\n",
    "#     title = row['title'].lower().strip()\n",
    "#     matched = human_reviews[human_reviews['title'].str.contains(title)]\n",
    "#     # print(type(matched))\n",
    "#     if not matched.empty:\n",
    "#         matched_reviews.append(matched)\n",
    "#     else:\n",
    "#         row_to_add = human_reviews.iloc[99]\n",
    "#         row_to_add = row_to_add.to_frame().T  # Convert the row to a DataFrame\n",
    "#         matched_reviews.append(row_to_add)\n",
    "#         print(f\"No match found for title: {title}\")\n",
    "#         print(\"Manually added row 99 as the missing one.\")\n",
    "\n",
    "# matched_reviews = pd.concat(matched_reviews, ignore_index=True)\n",
    "# # print the number of reviews\n",
    "# print(f\"Number of matched human reviews: {len(matched_reviews)}\")\n",
    "# human_reviews = matched_reviews\n",
    "# # print the number of reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# ######################################################## Process the human reviews ########################################################\n",
    "# processed_rows = []\n",
    "# for i in range (len(human_reviews)):\n",
    "\n",
    "\n",
    "#     row = human_reviews.iloc[i]\n",
    "#     random_choice = random.randint(1, 3)\n",
    "#     human_review = row[f'review#{random_choice}']['review']\n",
    "#     gpt_review = llm_reviews.iloc[i]['gpt4']['review']\n",
    "\n",
    "#     human_weakness_text = \"\"\n",
    "#     human_review_text = \"\"\n",
    "#     for segment in human_review:\n",
    "#         topic1 = str(segment['topic_class_1']).lower() if segment['topic_class_1']  else \"\"\n",
    "#         topic2 = str(segment['topic_class_2']).lower() if segment['topic_class_2']  else \"\"\n",
    "#         if (topic1 or topic2)  and  ('weakness' in topic1 or 'weakness' in topic2):\n",
    "#             human_weakness_text += segment['segment_text'] + \"\\n\"\n",
    "#             number_of_weakness_segments += 1\n",
    "#         human_review_text += segment['segment_text'] + \"\\n\"\n",
    "\n",
    "#     gpt_weakness_text = \"\"\n",
    "#     gpt_review_text = \"\"\n",
    "#     for segment in gpt_review:\n",
    "#         topic1 = str(segment['topic_class_1']).lower() if segment['topic_class_1']  else \"\"\n",
    "#         topic2 = str(segment['topic_class_2']).lower() if segment['topic_class_2']  else \"\"\n",
    "#         if (topic1 or topic2)  and  ('weakness' in topic1 or 'weakness' in topic2):\n",
    "#             gpt_weakness_text += segment['segment_text'] + \"\\n\"\n",
    "#         gpt_review_text += segment['segment_text'] + \"\\n\"\n",
    "    \n",
    "\n",
    "#     new_row = {\n",
    "#         'title': row['title'],\n",
    "#         'decision': row['decision'],\n",
    "#         'human_review': human_review_text,\n",
    "#         'gpt_review': gpt_review_text,\n",
    "#         'human_weakness_text': human_weakness_text,\n",
    "#         'gpt_weakness_text': gpt_weakness_text,\n",
    "#     }\n",
    "#     processed_rows.append(new_row)\n",
    "\n",
    "# # Create a new DataFrame from the processed rows\n",
    "# processed_rows = pd.DataFrame(processed_rows)\n",
    "# processed_rows.to_csv('human_gpt_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import sys\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "# sys.path.append(parent_dir)\n",
    "# from utils import *\n",
    "# from data_processing import rule_based_review_split\n",
    "# human_reviews_split = rule_based_review_split.split_and_filter(proccessed_human_reviews,\n",
    "#                                         review_key='focused_review',\n",
    "#                                         consider_only_bullet_points=False, \n",
    "#                                         do_filter_typos=True,\n",
    "#                                         filter_short_reviews=True,\n",
    "#                                         # exclude_long=True,\n",
    "#                                         # exclude_short=True,\n",
    "#                                         split_with_llm=False,\n",
    "#                                         llm_client=None)\n",
    "\n",
    "\n",
    "# llm_review_split = rule_based_review_split.split_and_filter(proccessed_llm_reviews,\n",
    "#                                         review_key='focused_review',\n",
    "#                                         consider_only_bullet_points=True, \n",
    "#                                         do_filter_typos=True,\n",
    "#                                         filter_short_reviews=True,\n",
    "#                                         # exclude_long=True,\n",
    "#                                         # exclude_short=True,\n",
    "#                                         split_with_llm=False,\n",
    "#                                         llm_client=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# human_llm_review_points = pd.read_csv('human_gpt_review_points.csv')\n",
    "# human_review_points = []\n",
    "# llm_review_points = []\n",
    "\n",
    "# for i in range (len(human_llm_review_points)):\n",
    "\n",
    "#     row = human_llm_review_points.iloc[i]\n",
    "#     human_points = row['human_review_points']\n",
    "\n",
    "#     for point in human_points.split(\"\\n\"):\n",
    "#         if point.strip() != \"\":\n",
    "#             entry = {\n",
    "#                 'title': row['title'],\n",
    "#                 'decisicon': row['decision'],\n",
    "#                 'full_human_review': row['human_review'],\n",
    "#                 'review_point': point.strip()}\n",
    "#             human_review_points.append(entry)\n",
    "#     llm_points = row['gpt_review_points']\n",
    "#     for point in llm_points.split(\"\\n\"):\n",
    "#         if point.strip() != \"\":\n",
    "#             entry = {\n",
    "#                 'title': row['title'],\n",
    "#                 'decisicon': row['decision'],\n",
    "#                 'full_llm_review': row['gpt_review'],\n",
    "#                 'review_point': point.strip()}\n",
    "#             llm_review_points.append(entry)\n",
    "# # Create a new DataFrame from the processed rows\n",
    "# human_review_points = pd.DataFrame(human_review_points)\n",
    "# llm_review_points = pd.DataFrame(llm_review_points)\n",
    "\n",
    "# ### convert them into HF datasets with different configs\n",
    "# human_review_points = datasets.Dataset.from_pandas(human_review_points)\n",
    "# llm_review_points = datasets.Dataset.from_pandas(llm_review_points)\n",
    "\n",
    "\n",
    "# human_review_points.push_to_hub(\n",
    "#             f\"boda/human_vs_llm_reviews\",\n",
    "#             split='test',\n",
    "#             config_name='human', private=False)\n",
    "# llm_review_points.push_to_hub(\n",
    "#             f\"boda/human_vs_llm_reviews\",\n",
    "#             split='test',\n",
    "#             config_name='llm', private=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "human_points = datasets.load_dataset(\"boda/human_vs_llm_reviews\", split=\"test\", name=\"human\").to_pandas()\n",
    "llm_points = datasets.load_dataset(\"boda/human_vs_llm_reviews\", split=\"test\", name=\"llm\").to_pandas()\n",
    "\n",
    "### get the average length of the review points\n",
    "avg_human_review_length  = human_points['review_point'].apply(lambda x: len(x.split())).mean()\n",
    "avg_llm_review_length  = llm_points['review_point'].apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "print(f\"Average human review point length: {avg_human_review_length}\")\n",
    "print(f\"Average LLM review point length: {avg_llm_review_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine human and LLM reviews into a single DataFrame\n",
    "human_points['source'] = 'human'\n",
    "llm_points['source'] = 'llm'\n",
    "\n",
    "# Rename the columns\n",
    "human_points = human_points.rename(columns={'full_human_review': 'full_review'})\n",
    "llm_points = llm_points.rename(columns={'full_llm_review': 'full_review'})\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_reviews = pd.concat([human_points, llm_points], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "combined_reviews = combined_reviews.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save to a new CSV file\n",
    "combined_reviews.to_csv('human_vs_llm_human_evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import datasets\n",
    "# import random\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = datasets.load_dataset(\"boda/review_evaluation_human_annotation\",name=\"combined_main_aspects\", split=\"full\").to_pandas()\n",
    "\n",
    "# # Sample 100 random entries\n",
    "# random_samples = dataset.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Add a new column named 'valid_point' with default value as None\n",
    "# random_samples['valid_point'] = None\n",
    "\n",
    "# # Save the sampled dataset to a CSV file\n",
    "# random_samples.to_csv(\"valid_segmentation_human_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Extract labels and compute their average #############################\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(parent_dir)\n",
    "from inference.inference_utils import  extract_predictions\n",
    "\n",
    "\n",
    "human_predicitons = []\n",
    "llm_predicitons = []\n",
    "\n",
    "### load predictions from jsonl file\n",
    "with open(\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/inference/evalute_outputs/adapters/Llama-3.1-8B-Instruct/score_rationale/instruction/all/step-0/human_vs_llm_reviews/raw_outputs_human_test.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        human_predicitons.append(json.loads(line))\n",
    "with open(\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/inference/evalute_outputs/adapters/Llama-3.1-8B-Instruct/score_rationale/instruction/all/step-0/human_vs_llm_reviews/raw_outputs_llm_test.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        llm_predicitons.append(json.loads(line))\n",
    "\n",
    "human_predicitons = extract_predictions(human_predicitons)\n",
    "llm_predicitons = extract_predictions(llm_predicitons)\n",
    "\n",
    "human_predicitons = pd.DataFrame(human_predicitons)\n",
    "llm_predicitons = pd.DataFrame(llm_predicitons)\n",
    "\n",
    "\n",
    "for key in human_predicitons.keys():\n",
    "    human_points[key] = human_predicitons[key]\n",
    "for key in llm_predicitons.keys():\n",
    "    llm_points[key] = llm_predicitons[key]\n",
    "    \n",
    "\n",
    "with pd.ExcelWriter('human_llm_points_predictions.xlsx') as writer:\n",
    "    human_points.to_excel(writer, sheet_name='Human Points', index=False)\n",
    "    llm_points.to_excel(writer, sheet_name='LLM Points', index=False)\n",
    "\n",
    "\n",
    "############################ get the averge of each aspect score ###########33\n",
    "aspects = [ 'actionability', 'grounding_specificity','verifiability', 'helpfulness']\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "aspect_scores = {}\n",
    "for aspect in aspects:\n",
    "\n",
    "    score_column = f'{aspect}_label'\n",
    "    if aspect == 'verifiability':\n",
    "        temp_human_predicitons = human_predicitons[human_predicitons[score_column] != 'X']\n",
    "        temp_llm_predicitons = llm_predicitons[llm_predicitons[score_column] != 'X']\n",
    "    else:\n",
    "        temp_human_predicitons = human_predicitons\n",
    "        temp_llm_predicitons = llm_predicitons\n",
    "\n",
    "    temp_human_predicitons[score_column] = temp_human_predicitons[score_column].astype(int)\n",
    "    temp_llm_predicitons[score_column] = temp_llm_predicitons[score_column].astype(int)\n",
    "    ### assert that the scores are between 1 and 5\n",
    "    assert temp_human_predicitons[score_column].min() >= 1\n",
    "    assert temp_human_predicitons[score_column].max() <= 5\n",
    "    assert temp_llm_predicitons[score_column].min() >= 1\n",
    "    assert temp_llm_predicitons[score_column].max() <= 5\n",
    "    ### assert that the scores are between 1 and 5\n",
    "\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "\n",
    "    human_mean = temp_human_predicitons[score_column].mean()\n",
    "    human_std = temp_human_predicitons[score_column].std()\n",
    "    llm_mean = temp_llm_predicitons[score_column].mean()\n",
    "    llm_std = temp_llm_predicitons[score_column].std()\n",
    "    t_stat, p_value = ttest_ind(temp_human_predicitons[score_column], temp_llm_predicitons[score_column], equal_var=False)\n",
    "\n",
    "    aspect_scores[aspect] = {\n",
    "        'human_mean': human_mean,\n",
    "        'human_std': human_std,\n",
    "        'llm_mean': llm_mean,\n",
    "        'llm_std': llm_std,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "aspect_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Evaluation of LLM Vs. Human reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'decisicon', 'full_review', 'review_point', 'source',\n",
      "       'actionability', 'grounding_specificity', 'verifiability',\n",
      "       'helpfulness'],\n",
      "      dtype='object')\n",
      "Number of human rows: 94\n",
      "Number of llm rows: 45\n",
      "Number of human rows: 94\n",
      "Number of llm rows: 45\n",
      "Number of human rows: 65\n",
      "Number of llm rows: 32\n",
      "Number of human rows: 94\n",
      "Number of llm rows: 45\n",
      "Aspect: actionability\n",
      "Human Mean: 3.148936170212766, Human Std: 1.7224479130256016\n",
      "LLM Mean: 2.911111111111111, LLM Std: 1.6627226060430116\n",
      "T-statistic: 0.7798650635761578, P-value: 0.4375248481663987\n",
      "\n",
      "Aspect: grounding_specificity\n",
      "Human Mean: 3.276595744680851, Human Std: 1.3710074415235243\n",
      "LLM Mean: 2.911111111111111, LLM Std: 1.2581050364847455\n",
      "T-statistic: 1.5560237240687422, P-value: 0.1230668932469232\n",
      "\n",
      "Aspect: verifiability\n",
      "Human Mean: 3.3076923076923075, Human Std: 1.5704604910172537\n",
      "LLM Mean: 2.9375, LLM Std: 1.3663584756098845\n",
      "T-statistic: 1.1930177305831702, P-value: 0.23688603661160734\n",
      "\n",
      "Aspect: helpfulness\n",
      "Human Mean: 3.1595744680851063, Human Std: 1.4391095775219154\n",
      "LLM Mean: 2.977777777777778, LLM Std: 1.2520689947749553\n",
      "T-statistic: 0.7623352915539178, P-value: 0.4476798311469813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################## Extract labels and compute their average #############################\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"human_evaluation_llm_vs_human.csv\")\n",
    "\n",
    "# Rename columns in df, human_predictions, and llm_predictions\n",
    "rename_dict = {\n",
    "    'actionability ': 'actionability',\n",
    "    'gounding': 'grounding_specificity',\n",
    "    'verrifiability': 'verifiability'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# Update aspects list\n",
    "aspects = ['actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "\n",
    "\n",
    "print(df.columns)  # Check unique sources in the DataFrame\n",
    "# Filter rows by source\n",
    "human_predictions = df[df['source'] == 'human'].copy()\n",
    "llm_predictions = df[df['source'] == 'llm'].copy()\n",
    "\n",
    "# Dictionary to store scores\n",
    "aspect_scores = {}\n",
    "\n",
    "# Loop over each aspect and compute stats\n",
    "for aspect in aspects:\n",
    "    if aspect == 'verifiability':\n",
    "        temp_human = human_predictions[human_predictions[aspect] != 'X'].copy()\n",
    "        temp_llm = llm_predictions[llm_predictions[aspect] != 'X'].copy()\n",
    "    else:\n",
    "        temp_human = human_predictions.copy()\n",
    "        temp_llm = llm_predictions.copy()\n",
    "\n",
    "    # Drop rows with NaN in any of the four aspects (using the new column names)\n",
    "    temp_human = temp_human.dropna(subset=aspects)\n",
    "    temp_llm = temp_llm.dropna(subset=aspects)\n",
    "\n",
    "    print(f\"Number of human rows: {len(temp_human)}\")\n",
    "    print(f\"Number of llm rows: {len(temp_llm)}\")\n",
    "    # Convert to integers\n",
    "    temp_human[aspect] = temp_human[aspect].astype(int)\n",
    "    temp_llm[aspect] = temp_llm[aspect].astype(int)\n",
    "\n",
    "    # Sanity checks\n",
    "    assert 1 <= temp_human[aspect].min() <= 5\n",
    "    assert 1 <= temp_human[aspect].max() <= 5\n",
    "    assert 1 <= temp_llm[aspect].min() <= 5\n",
    "    assert 1 <= temp_llm[aspect].max() <= 5\n",
    "\n",
    "    # Compute means and stds\n",
    "    human_mean = temp_human[aspect].mean()\n",
    "    human_std = temp_human[aspect].std()\n",
    "    llm_mean = temp_llm[aspect].mean()\n",
    "    llm_std = temp_llm[aspect].std()\n",
    "    t_stat, p_value = ttest_ind(temp_human[aspect], temp_llm[aspect], equal_var=False)\n",
    "\n",
    "    # Store results\n",
    "    aspect_scores[aspect] = {\n",
    "        'human_mean': human_mean,\n",
    "        'human_std': human_std,\n",
    "        'llm_mean': llm_mean,\n",
    "        'llm_std': llm_std,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "for aspect in aspect_scores:\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    print(f\"Human Mean: {aspect_scores[aspect]['human_mean']}, Human Std: {aspect_scores[aspect]['human_std']}\")\n",
    "    print(f\"LLM Mean: {aspect_scores[aspect]['llm_mean']}, LLM Std: {aspect_scores[aspect]['llm_std']}\")\n",
    "    print(f\"T-statistic: {aspect_scores[aspect]['t_stat']}, P-value: {aspect_scores[aspect]['p_value']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hard rows: 187\n",
      "Statistics for all pairwise absolute differences:\n",
      "mean: 1.896613190730838\n",
      "std: 0.9043580646182018\n",
      "min: 1\n",
      "max: 4\n",
      "median: 2.0\n",
      "mode: [1]\n",
      "value_counts: {1: 226, 2: 202, 3: 98, 4: 35}\n",
      "percentages: {1: 40.29, 2: 36.01, 3: 17.47, 4: 6.24}\n",
      "\n",
      "Detailed breakdown of absolute differences:\n",
      "Difference | Count | Percentage\n",
      "------------------------------\n",
      "        1 |   226 |  40.29%\n",
      "        2 |   202 |  36.01%\n",
      "        3 |    98 |  17.47%\n",
      "        4 |    35 |   6.24%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(\"boda/review_evaluation_human_annotation\",name=\"combined_main_aspects\", split=\"full\").to_pandas()\n",
    "\n",
    "\n",
    "# Filter rows where grounding_specificity_label_type is 'hard'\n",
    "hard_rows = dataset[dataset['grounding_specificity_label_type'] == 'hard'].copy()\n",
    "\n",
    "print(f\"Number of hard rows: {len(hard_rows)}\")\n",
    "\n",
    "# Extract the three labels from the 'grounding_specificity' column (which is a dict with a 'labels' key)\n",
    "hard_rows['label_1'] = hard_rows['grounding_specificity'].apply(lambda x: int(x['labels'][0]))\n",
    "hard_rows['label_2'] = hard_rows['grounding_specificity'].apply(lambda x: int(x['labels'][1]))\n",
    "hard_rows['label_3'] = hard_rows['grounding_specificity'].apply(lambda x: int(x['labels'][2]))\n",
    "\n",
    "# Compute all pairwise differences between the three labels (absolute values)\n",
    "hard_rows['diff_1_2'] = abs(hard_rows['label_1'] - hard_rows['label_2'])\n",
    "hard_rows['diff_1_3'] = abs(hard_rows['label_1'] - hard_rows['label_3'])\n",
    "hard_rows['diff_2_3'] = abs(hard_rows['label_2'] - hard_rows['label_3'])\n",
    "\n",
    "# Combine all differences into a single Series for histogram\n",
    "all_diffs = pd.concat([\n",
    "    hard_rows['diff_1_2'],\n",
    "    hard_rows['diff_1_3'],\n",
    "    hard_rows['diff_2_3']\n",
    "])\n",
    "\n",
    "# # Plot a histogram of all pairwise differences\n",
    "# all_diffs.hist(bins=range(-4, 5), edgecolor='black')\n",
    "\n",
    "# # Display the review_point and the three labels for the hard rows\n",
    "# display_cols = ['review_point', 'label_1', 'label_2', 'label_3']\n",
    "# print(hard_rows[display_cols].to_string(index=False))\n",
    "\n",
    "# Compute statistics for all pairwise differences\n",
    "value_counts = all_diffs.value_counts().sort_index()\n",
    "total_count = len(all_diffs)\n",
    "percentages = (value_counts / total_count * 100).round(2)\n",
    "\n",
    "stats = {\n",
    "    'mean': all_diffs.mean(),\n",
    "    'std': all_diffs.std(),\n",
    "    'min': all_diffs.min(),\n",
    "    'max': all_diffs.max(),\n",
    "    'median': all_diffs.median(),\n",
    "    'mode': all_diffs.mode().tolist(),\n",
    "    'value_counts': value_counts.to_dict(),\n",
    "    'percentages': percentages.to_dict()\n",
    "}\n",
    "\n",
    "print(\"Statistics for all pairwise absolute differences:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nDetailed breakdown of absolute differences:\")\n",
    "print(\"Difference | Count | Percentage\")\n",
    "print(\"-\" * 30)\n",
    "for diff in sorted(value_counts.index):\n",
    "    count = value_counts[diff]\n",
    "    pct = percentages[diff]\n",
    "    print(f\"{diff:9d} | {count:5d} | {pct:6.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aspect: actionability\n",
      "Processing aspect: grounding_specificity\n",
      "Processing aspect: verifiability\n",
      "Processing aspect: helpfulness\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import random\n",
    "\n",
    "dataset = {}\n",
    "# Load the dataset\n",
    "# dataset = datasets.load_dataset(\"boda/review_evaluation_automatic_labels\",name=\"all\", split=\"train\").to_pandas()\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "\n",
    "for aspect in aspects:\n",
    "    print(f\"Processing aspect: {aspect}\")\n",
    "    # Load the Excel files for the specific aspect\n",
    "    df1 = pd.read_excel(f'../../chatgpt/outputs/main_data_batch_gold_results.xlsx', sheet_name=aspect)\n",
    "    df2 = pd.read_excel(f'../../chatgpt/outputs/main_data_batch_silver_results.xlsx', sheet_name=aspect)\n",
    "    dataset[aspect] = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "Review Point:\n",
      "6) Adding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution.\n",
      "Paper ID: ICLR_2022_3330\n",
      "Venue: ICLR_2022\n",
      "Disagreement: False\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 1.0 | GPT Label: 1.0\n",
      "Rationale: The review point comments on the addition of a method to improve transferability, stating that while it is beneficial, it does not constitute a significant contribution. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how the authors might enhance the significance of their contribution or any suggestions for improvement. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 1.0 | GPT Label: 1.0\n",
      "Rationale: The comment critiques the significance of adding a method on top of existing methods to improve transferability, suggesting it may not be a substantial contribution. However, it does not specify which part of the paper this critique is addressing, nor does it provide details on why this addition is not considered significant. Without specific references or detailed feedback, the authors cannot effectively identify the areas needing improvement. As a result, the comment lacks both grounding and specificity, making it not grounded at all. Therefore, this comment aligns with a score of 1.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 1 | GPT Label: 1\n",
      "Rationale: The review point claims that adding a method on top of other methods to improve transferability is not a significant contribution. However, it does not provide any supporting evidence, reasoning, or examples to justify why this addition is not considered significant. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 1.0 | GPT Label: 1.0\n",
      "Rationale: The review comment critiques the significance of the contribution by stating that adding a method on top of existing methods to improve transferability is not a significant contribution. However, it lacks specificity and does not provide actionable feedback or suggestions on how the authors might enhance the significance of their contribution. Without concrete guidance or examples, the authors are left without a clear path to address the issue raised. As a result, the comment is not particularly useful for helping the authors improve their draft, aligning with a score of 1: The comment is not helpful at all.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The proposed method uses AdamW with cosine lr for training, while comparing methods only use adam with fixed lr. Directly comparing with their numbers in paper is unfair. It would be better to reproduce their results using the same setting, since most of the recent methods have their code released.\n",
      "Paper ID: NIPS_2021_2050\n",
      "Venue: NIPS_2021\n",
      "Disagreement: True\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The review comment highlights a potential issue with the fairness of the comparison between the proposed method and other methods due to differences in the training setup, specifically the use of AdamW with cosine learning rate versus Adam with fixed learning rate. The reviewer explicitly suggests reproducing the results of the comparing methods using the same settings as the proposed method, noting that most recent methods have their code released. This feedback provides a clear and concrete action for the authors to take, including the specific suggestion to reproduce results under consistent conditions. The explicit nature of the suggestion and the detailed guidance on how to implement it make this comment highly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the proposed method's use of AdamW with cosine learning rate (lr) for training, compared to other methods using Adam with a fixed learning rate. It suggests that directly comparing these results is unfair and recommends reproducing the results using the same settings. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the methodology or results section. The suggestion is specific, as it outlines a particular approach to improve the fairness of the comparison. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point claims that the comparison between the proposed method and others is unfair due to differences in training configurations, specifically the use of AdamW with cosine learning rate versus Adam with a fixed learning rate. The comment suggests reproducing results using the same settings for a fair comparison. This claim is somewhat verifiable as it highlights a potential inconsistency in the experimental setup, but it lacks specific examples or references to substantiate the impact of these differences. Providing more detailed evidence or examples of how these differences affect the results would strengthen the claim, making it easier for authors to address the critique. Therefore, the comment is rated as \"3: Somewhat Verifiable.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment identifies a potential issue with the fairness of the experimental comparison, noting that the proposed method uses AdamW with a cosine learning rate schedule, while the comparison methods use Adam with a fixed learning rate. This discrepancy could lead to unfair comparisons. The reviewer suggests a specific improvement by recommending that the authors reproduce the results of the comparison methods using the same settings, as most recent methods have their code released. This feedback is clear and actionable, providing the authors with a concrete step to enhance the validity of their comparisons. However, it could be more helpful if it included additional guidance on how to implement this suggestion or the potential impact of these differences on the results. Overall, the comment is mostly helpful, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- l148: known, instead of know - l156: please define \\gamma_0^{***} - Figure 1: Please specify the meaning of the colors in the caption as well as the text.\n",
      "Paper ID: NIPS_2017_110\n",
      "Venue: NIPS_2017\n",
      "Disagreement: False\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The review point provides explicit and concrete actions for the authors to take. It suggests correcting a typo in line 148, defining a term in line 156, and specifying the meaning of colors in Figure 1. Each of these points is specific and actionable, giving the authors clear guidance on how to improve their draft. The feedback provides precise instructions on how to implement the changes, making it highly actionable. Therefore, this comment is rated as a 5 on the actionability scale.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions specific locations in the paper, such as \"l148,\" \"l156,\" and \"Figure 1,\" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it details what needs to be corrected or clarified, such as correcting a typo, defining a term, and specifying the meaning of colors in a figure. Therefore, this comment is fully grounded and specific, aligning with category 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: X\n",
      "Rationale: The review point consists of factual statements and requests for clarification or corrections. It does not contain any subjective opinions, claims, or suggestions that require justification or evidence. The comments are purely descriptive and aim to improve clarity and accuracy in the paper. Therefore, it fits the classification of \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The review comment provides specific and actionable feedback on several minor issues in the paper. It points out a typographical error (\"known\" instead of \"know\"), requests a definition for a term (\\(\\gamma_0^{***}\\)), and suggests clarifying the meaning of colors in Figure 1. These precise corrections and requests for clarification are valuable for improving the clarity and accuracy of the manuscript. The comment is highly helpful as it offers clear guidance that the authors can easily implement to enhance their draft. Therefore, it aligns with a score of 5.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions).\n",
      "Paper ID: NIPS_2018_38\n",
      "Venue: NIPS_2018\n",
      "Disagreement: True\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that the paper should analyze the quality of local minima, specifically the approximation ratio of these local minima under certain assumptions. While the comment provides a clear action for the authors to take, it lacks specific guidance on how to conduct this analysis or what assumptions to consider. The action is explicit, but the details on execution are somewhat vague, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions \"Algorithm 1,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies what needs to be addressed: the analysis of the quality of local minima, such as the approximation ratio under certain assumptions. This provides clear guidance for the authors on how to improve their paper. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point suggests that the paper's analysis is limited to cases where Algorithm 1 converges to permutations as local minima. It further recommends analyzing the quality of these local minima, such as their approximation ratio under certain assumptions. While the comment identifies a potential area for improvement, it lacks specific examples or references to support the claim that this additional analysis is necessary or beneficial. The suggestion is somewhat verifiable, as it provides a logical direction for further exploration but does not offer detailed justification or evidence. Therefore, the comment is classified as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a specific area for improvement by suggesting that the paper should analyze the quality of local minima, specifically the approximation ratio under certain assumptions. This feedback is actionable as it provides a clear direction for the authors to enhance their analysis and strengthen their paper. However, the comment could be more helpful if it offered additional guidance or examples on how to conduct this analysis or what specific assumptions to consider. While it points out a valuable area for improvement, the feedback lacks depth and comprehensive guidance, making it mostly helpful but not fully comprehensive. Therefore, it aligns with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.\n",
      "Paper ID: 7GxY4WVBzc\n",
      "Venue: EMNLP_2023\n",
      "Disagreement: False\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 3.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific area where additional information is needed: the filtering process used to create the Arabic climate change QA dataset. It explicitly states that more details on the translation and filtering methodology are required to assess the dataset quality. While the comment clearly indicates what information is missing, it does not provide specific guidance on how to present this information or what aspects of the methodology should be highlighted. Therefore, the action is explicit but somewhat vague in terms of execution, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a specific aspect of the paper related to the filtering process used to create the Arabic climate change QA dataset. It highlights the need for more information on the translation and filtering methodology to assess the dataset's quality. However, the comment does not explicitly mention which section of the paper discusses this process, making it weakly grounded. The authors might infer that it relates to the methodology or dataset description sections, but this inference is not direct. The comment is specific in detailing what additional information is needed, namely, more details on the translation and filtering methodology. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 3 | GPT Label: 3\n",
      "Rationale: The review point claims that details about the filtering process for creating the Arabic climate change QA dataset are insufficient, suggesting that more information on translation and filtering methodology is necessary to evaluate dataset quality. However, the comment does not provide specific examples or references to support why these details are crucial or how their absence impacts the assessment of dataset quality. This lack of detailed justification makes the claim somewhat verifiable, as the authors would need to infer the importance of the missing information themselves.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific area where the paper lacks detail: the filtering process used to create the Arabic climate change QA dataset. By pointing out the need for more information on the translation and filtering methodology, the comment highlights a crucial aspect that could affect the assessment of the dataset's quality. However, the feedback could be more helpful if it provided specific suggestions on what details should be included or how the authors might present this information. While it identifies an important gap, the lack of detailed guidance makes it somewhat helpful. Therefore, the comment is evaluated as somewhat helpful, consistent with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2: The training of RegMixup seems to see 2x samples per iteration. Thus the running speed is slow (as authors claimed 1.5 x slower). When compared to other methods, RegMixup seeing 2x samples may lead to unfair comparison.\n",
      "Paper ID: NIPS_2022_572\n",
      "Venue: NIPS_2022\n",
      "Disagreement: True\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 1.0 | GPT Label: 1.0\n",
      "Rationale: The review point highlights a potential issue with the RegMixup training process, noting that it sees 2x samples per iteration, which results in slower running speed and may lead to unfair comparisons with other methods. However, the comment does not provide explicit guidance on how the authors should address this issue or suggest any modifications to ensure fair comparisons. The feedback lacks specific actionable steps, leaving the authors uncertain about how to improve their work in response to this concern. Since the comment is vague and does not offer a clear path for action, it is unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the training process of RegMixup, specifically noting that it sees 2x samples per iteration, which results in slower running speed and potentially unfair comparisons with other methods. This provides a clear indication of the issue with the training process. However, the comment does not specify which part of the paper discusses the training of RegMixup, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where the training process is discussed, the comment lacks full grounding. It is specific about the issue of sample size and speed but does not provide detailed guidance on how to address it. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the training of RegMixup involves processing twice as many samples per iteration, which results in slower running speed and potentially unfair comparisons with other methods. This claim is somewhat verifiable as it provides a logical reasoning based on the observation of the training process and the authors' own claim of a 1.5x slower speed. However, it lacks specific examples or references to support the assertion of unfair comparison, making it necessary for the authors to further investigate and validate the claim. Therefore, the comment is rated as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a potential issue with the training process of RegMixup, specifically noting that it processes twice as many samples per iteration, which results in slower running speed. The comment highlights a concern about the fairness of comparisons with other methods due to this difference in sample processing. While it points out a relevant issue, the comment lacks depth and does not provide specific suggestions or guidance on how the authors might address this concern or adjust their comparisons. It raises an important point but does not offer actionable steps for improvement, making it somewhat helpful. Therefore, the comment is evaluated as somewhat helpful, consistent with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. Please define the dashed lines in fig. 2A-B and 4B.\n",
      "Paper ID: NIPS_2016_153\n",
      "Venue: NIPS_2016\n",
      "Disagreement: False\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The review comment explicitly instructs the authors to define the dashed lines in figures 2A-B and 4B. This is a direct and specific request, leaving no ambiguity about what the authors need to do to improve their draft. The comment is clear and provides a straightforward action for the authors to take, making it highly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions specific figures, \"fig. 2A-B and 4B,\" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it clearly specifies what needs to be addressed, namely the definition of the dashed lines in these figures. This level of detail provides clear guidance on what needs to be revised, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: X\n",
      "Rationale: The comment is a request for clarification, asking the authors to define the dashed lines in specific figures. It does not contain an opinion, claim, or suggestion that requires justification or evidence. Therefore, it is a normal statement and should be labeled as \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The review comment provides a specific and actionable request for clarification regarding the dashed lines in figures 2A-B and 4B. By asking for a definition of these elements, the reviewer highlights a potential area of confusion for readers, which, if addressed, could improve the clarity and comprehensibility of the figures. This feedback is precise and directly instructs the authors on how to enhance their draft, making it highly helpful. Therefore, it aligns with a score of 5.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?\n",
      "Paper ID: ICLR_2021_2674\n",
      "Venue: ICLR_2021\n",
      "Disagreement: True\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment raises a question about the generalization of the model to focusing distances not present in the training data. It explicitly points out that Figure 8 only shows images with focusing distances of 1m and 5m, which are included in the training data, and asks about other distances. While the comment clearly identifies an area for improvement, it does not provide specific guidance on how to address this issue, such as suggesting additional experiments or analyses. The action is implicit and somewhat vague, as the authors need to infer that they should test the model on other focusing distances. Therefore, the comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 5.0\n",
      "Rationale: The comment explicitly mentions \"fig 8,\" providing full grounding as the authors can accurately identify the part of the paper being addressed. It specifies the issue by pointing out that the figure only shows images with focusing distances of 1m and 5m, which are present in the training data, and questions the generalization to other focusing distances not included in the training. This provides clear guidance on what the authors need to address regarding the generalization of their model. Therefore, the comment is fully grounded and specific, corresponding to label 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: X\n",
      "Rationale: The comment primarily consists of observations and questions regarding the content of Figure 8. It does not present any claims, opinions, or suggestions that require verification. The focus is on seeking clarification and additional information, which aligns with the criteria for normal statements. Therefore, it is labeled as \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises a pertinent question about the generalization capability of the model concerning focusing distances not present in the training data. It points out that Figure 8 only shows images with focusing distances of 1m and 5m, both of which are included in the training data. By asking about other focusing distances, the comment suggests an area for further exploration and testing, which could enhance the robustness and applicability of the model. However, the comment could be more helpful if it provided specific suggestions on how to test or demonstrate this generalization capability. Overall, the feedback is somewhat helpful as it identifies a potential gap in the analysis but lacks detailed guidance on addressing it.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones.\n",
      "Paper ID: NIPS_2020_295\n",
      "Venue: NIPS_2020\n",
      "Disagreement: True\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The reviewer suggests that the experimental comparisons are insufficient and implies that the authors should test their proposed method, InvP, with wider backbones like ResNet50 (2×) and ResNet50 (4×), similar to methods like MoCo and SimCLR. While the comment implies a specific action, it does not explicitly instruct the authors to conduct these tests. However, the suggestion is concrete, as it specifies the exact backbones to be used for comparison. Therefore, the comment is mostly actionable, as the authors can infer the necessary steps to improve their draft.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests expanding the experimental comparisons by testing the proposed InvP method with wider backbones like ResNet50 (2×) and ResNet50 (4×), similar to methods like MoCo and SimCLR. This provides a specific suggestion for improvement, indicating what additional experiments could be conducted. However, the comment does not explicitly mention which part of the paper it addresses, such as specific sections or tables where these comparisons could be included. The authors can infer that it relates to the experimental results section, but this inference is not as direct as it could be. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the experimental comparisons are insufficient, suggesting that methods like MoCo and SimCLR also test results with wider backbones such as ResNet50 (2×) and ResNet50 (4×). The comment implies that it would be beneficial to see the results of the proposed InvP with these wider backbones. While the comment provides some context by mentioning specific methods and backbones, it lacks detailed reasoning or references to substantiate why these comparisons are necessary or how they would enhance the study. The claim is somewhat verifiable, as it provides a direction for improvement but requires further elaboration to be fully understood and addressed by the authors. Therefore, the comment is classified as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a gap in the experimental comparisons by noting that other methods, such as MoCo and SimCLR, have tested results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It suggests that it would be interesting to see the results of the proposed InvP with these wider backbones. This feedback is somewhat helpful as it points out an area for improvement and provides a specific suggestion for enhancing the experimental setup. However, it could be more helpful if it explained why testing with wider backbones is important or how it could impact the results, providing the authors with a clearer understanding of the potential benefits. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. It is not clear to me whether such a model could generate novel knowledge or testable hypothesis about neuron data.\n",
      "Paper ID: ICLR_2023_149\n",
      "Venue: ICLR_2023\n",
      "Disagreement: False\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 1.0 | GPT Label: 1.0\n",
      "Rationale: The comment raises a question about the model's ability to generate novel knowledge or testable hypotheses regarding neuron data. However, it does not provide any guidance or suggestions on how the authors should address this uncertainty or improve their draft. The comment lacks explicit instructions or concrete details on what actions the authors should take to clarify this aspect. As a result, the authors are left without clear direction on how to proceed, making the comment unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 1.0 | GPT Label: 1.0\n",
      "Rationale: The comment raises a question about the model's capability to generate novel knowledge or testable hypotheses regarding neuron data. However, it does not specify which part of the paper this concern is related to, making it difficult for the authors to identify where improvements or clarifications are needed. Additionally, the comment lacks specificity in detailing what aspects of the model's functionality or results are unclear. As a result, the comment is not grounded and highly unspecific, aligning with category 1.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 1 | GPT Label: 1\n",
      "Rationale: The review point questions whether the model can generate novel knowledge or testable hypotheses about neuron data. However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable. score: 1\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 2.0 | GPT Label: 2.0\n",
      "Rationale: The review comment raises a question about the capability of the model to generate novel knowledge or testable hypotheses regarding neuron data. While it identifies a potential area of concern, it does not provide specific feedback or suggestions on how the authors might address this issue. The comment lacks depth and actionable guidance, leaving the authors with only a vague understanding of the potential weakness. Therefore, it is considered barely helpful, as it points out an area for consideration but does not offer a clear path for improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1810284/1268501274.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['has_disagreement'] = clean_df[mismatch_cols].any(axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "aspect_dfs = dataset\n",
    "aspects = list(aspect_dfs.keys())\n",
    "\n",
    "# Use one DataFrame to get shared metadata\n",
    "base_aspect = aspects[0]\n",
    "base_df = aspect_dfs[base_aspect][['review_point', 'paper_id', 'venue', 'focused_review', 'batch', 'prompt', 'id']].copy()\n",
    "\n",
    "# Get aspect-specific columns\n",
    "aspect_cols = []\n",
    "for aspect, df in aspect_dfs.items():\n",
    "    cols_to_keep = [\n",
    "        'review_point',\n",
    "        f'{aspect}_label',\n",
    "        f'chatgpt_{aspect}_score',\n",
    "        f'chatgpt_{aspect}_rationale'\n",
    "    ]\n",
    "    aspect_cols.append(df[cols_to_keep])\n",
    "\n",
    "# Merge aspect columns on review_point\n",
    "merged_aspects = reduce(lambda left, right: pd.merge(left, right, on='review_point', how='outer'), aspect_cols)\n",
    "\n",
    "# Merge with base metadata\n",
    "merged_df = pd.merge(base_df, merged_aspects, on='review_point', how='outer')\n",
    "\n",
    "# For each aspect, create a boolean mismatch column\n",
    "for aspect in aspects:\n",
    "    merged_df[f'{aspect}_mismatch'] = (\n",
    "        (merged_df[f'{aspect}_label'] != merged_df[f'chatgpt_{aspect}_score']) &\n",
    "        (merged_df[f'{aspect}_label'].notna()) &\n",
    "        (merged_df[f'chatgpt_{aspect}_score'].notna())\n",
    "    )\n",
    "\n",
    "# Drop any rows that contain NaN in label, score, or rationale columns\n",
    "required_cols = []\n",
    "for aspect in aspects:\n",
    "    required_cols.extend([\n",
    "        f'{aspect}_label',\n",
    "        f'chatgpt_{aspect}_score',\n",
    "        f'chatgpt_{aspect}_rationale'\n",
    "    ])\n",
    "clean_df = merged_df.dropna(subset=required_cols)\n",
    "\n",
    "# Create a row-level flag if any mismatch exists\n",
    "mismatch_cols = [f'{aspect}_mismatch' for aspect in aspects]\n",
    "clean_df['has_disagreement'] = clean_df[mismatch_cols].any(axis=1)\n",
    "\n",
    "# Sample 5 with disagreement and 5 with full agreement\n",
    "disagree_samples = clean_df[clean_df['has_disagreement']].sample(n=5, random_state=42)\n",
    "agree_samples = clean_df[~clean_df['has_disagreement']].sample(n=5, random_state=42)\n",
    "\n",
    "final_samples = pd.concat([disagree_samples, agree_samples]).sample(frac=1, random_state=42)  # shuffle\n",
    "\n",
    "# Print\n",
    "for _, row in final_samples.iterrows():\n",
    "    print(f\"\\n========================\")\n",
    "    print(f\"Review Point:\\n{row['review_point']}\")\n",
    "    print(f\"Paper ID: {row['paper_id']}\")\n",
    "    print(f\"Venue: {row['venue']}\")\n",
    "    print(f\"Disagreement: {row['has_disagreement']}\")\n",
    "    for aspect in aspects:\n",
    "        print(f\"\\n--- {aspect.upper()} ---\")\n",
    "        print(f\"Human Label: {row[f'{aspect}_label']} | GPT Label: {row[f'chatgpt_{aspect}_score']}\")\n",
    "        print(f\"Rationale: {row[f'chatgpt_{aspect}_rationale']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "Review Point:\n",
      "* Coming to the multi-fidelity framework and sequential design for learning quantities (e.g. probabilities of threshold exceedance) of interest, see notably Assessing Fire Safety using Complex Numerical Models with a Bayesian Multi-fidelity Approach (Stroh et al. 2017) https://www.sciencedirect.com/science/article/pii/S0379711217301297?via%3Dihub Some further points * Somehow confusing to read \"relatively inexpensive\" in the abstract and then \"expensive to evaluate\" in the first line of the introduction!\n",
      "Paper ID: NIPS_2018_482\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment provides a reference to a relevant work by Stroh et al. (2017) on a multi-fidelity framework and sequential design, which could be useful for the authors to consider in their work. However, it does not explicitly instruct the authors on how to incorporate this reference or how it relates to their current framework. Additionally, the comment points out a potential inconsistency in the text, noting the use of \"relatively inexpensive\" in the abstract and \"expensive to evaluate\" in the introduction. While this observation highlights a discrepancy, it does not provide specific guidance on how to resolve it. The authors are left to infer that they should address this inconsistency, but without concrete steps, the action remains vague. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses two distinct points. The first part suggests a reference to a specific work (Stroh et al. 2017) related to the multi-fidelity framework and sequential design, which could be useful for the authors. However, it does not specify which part of the paper this reference should be integrated into, making it weakly grounded. The second part of the comment highlights a contradiction between the abstract and the introduction regarding the cost of evaluation, which is fully grounded as it points to specific sections of the paper. However, it lacks specificity in terms of how to resolve this contradiction. Overall, the comment is partially grounded and under-specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 4\n",
      "Rationale: The review point provides a reference to Stroh et al. (2017) when discussing the multi-fidelity framework and sequential design for learning quantities of interest. This reference supports the claim by pointing to an external source that may offer relevant insights or methodologies. However, the comment also mentions a confusing aspect of the paper regarding the cost of evaluation, but it does not provide further explanation or evidence to clarify this confusion. While the reference to an external work makes part of the claim verifiable, the lack of detailed reasoning or examples for the confusion aspect leaves some gaps. Therefore, the comment is rated as 4: Mostly Verifiable, as it provides a solid reference but could benefit from more detailed justification for the confusion claim.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment provides a reference to a relevant work, \"Assessing Fire Safety using Complex Numerical Models with a Bayesian Multi-fidelity Approach,\" which could be beneficial for the authors to consider in the context of their multi-fidelity framework and sequential design. This suggests a potential avenue for expanding the literature review or methodology discussion. Additionally, the comment points out an inconsistency in the manuscript regarding the cost of evaluation, noting the conflicting descriptions of \"relatively inexpensive\" in the abstract and \"expensive to evaluate\" in the introduction. This feedback is actionable as it highlights a specific area of confusion that the authors can address to improve clarity and consistency in their draft. While the comment could be more helpful by providing additional context or suggestions, it still offers valuable insights that can guide the authors in refining their work. Therefore, it is rated as mostly helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* I have a lot of problems with these abstract visual reasoning tasks. They seem a bit unintuitive and overly difficult (I have a lot of trouble solving them). Having multiple rows and having multiple and different factors changing between each frame is very confusing and it seems like it would be hard to interpret how much these models actually learn the pattern or just exploit some artifacts. Do we have any proof that more simpler visual reasoning tasks wouldnât do and this formulation in the paper is the way to go?\n",
      "Paper ID: NIPS_2019_1276\n",
      "Venue: NIPS_2019\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 1.0\n",
      "Rationale: The review point expresses concerns about the complexity and intuitiveness of the abstract visual reasoning tasks presented in the paper. It questions whether simpler tasks could achieve the same goals and whether the current formulation is justified. However, the comment does not provide explicit guidance or concrete actions for the authors to address these concerns. It raises questions and doubts but lacks specific suggestions on how to simplify the tasks or demonstrate their necessity. As a result, the authors are left without clear direction on how to improve their draft, making the comment unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The comment expresses concerns about the abstract visual reasoning tasks, describing them as unintuitive and overly difficult. It questions the complexity of having multiple rows and different factors changing between frames, suggesting that this might make it hard to interpret the models' learning. However, the comment does not specify which part of the paper these tasks are discussed, nor does it provide detailed suggestions for improvement or alternatives. The authors might infer that it relates to sections discussing the methodology or experiments, but this inference is not direct. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, and it is not specific as it lacks detailed guidance on how to address the concerns. This aligns with a score of 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point expresses skepticism about the abstract visual reasoning tasks, describing them as \"unintuitive and overly difficult\" and questioning whether simpler tasks might suffice. The reviewer provides some reasoning by mentioning the complexity of having multiple rows and changing factors, which could confuse the interpretation of model learning. However, the comment lacks specific examples, references, or detailed evidence to support the claim that these tasks are indeed problematic or that simpler tasks would be more effective. The absence of such supporting information makes the claim somewhat verifiable, as the authors would need to further explore and substantiate the reviewer's concerns themselves.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises concerns about the abstract visual reasoning tasks used in the paper, describing them as unintuitive and overly difficult. It questions the complexity of having multiple rows and factors changing between frames, suggesting that this setup might confuse the interpretation of model learning. The comment also asks for proof that simpler tasks wouldn't suffice, challenging the necessity of the current formulation. While the comment identifies potential weaknesses and raises valid questions, it lacks specific suggestions or guidance on how the authors might address these concerns or improve their methodology. The feedback is somewhat helpful as it prompts the authors to reconsider their approach and possibly provide justification or evidence for their chosen tasks, but it could be more actionable and detailed to be fully helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* Including a comparison to one of the methods mentioned in the computer vision setting would have been more useful than comparing to, e.g. loss-based sampling. I understand that these are not always applicable and typically require a supervised set-up, but some of them can probably be adapted to language tasks relatively easily.\n",
      "Paper ID: bWXIut4pNM\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests that including a comparison to methods from the computer vision setting would be more useful than comparing to loss-based sampling. It acknowledges that these methods may not always be applicable but suggests that some could be adapted to language tasks. While the comment implies an action—comparing to computer vision methods—it does not specify which methods to consider or how to adapt them to the language tasks. The action is implicit and somewhat vague, as the authors may not know exactly which methods to explore or how to implement the comparison. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests including a comparison to methods mentioned in the computer vision setting, rather than focusing on loss-based sampling. It acknowledges the potential challenges in adapting these methods to language tasks but implies that some adaptation might be feasible. However, the comment does not specify which part of the paper should include this comparison, making it weakly grounded. The suggestion is specific in terms of what the authors could consider (i.e., adapting computer vision methods to language tasks), but it lacks detailed guidance on how to implement this suggestion. Therefore, the comment is weakly grounded and specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point suggests that comparing the method to those used in the computer vision setting would be more beneficial than the current comparison to loss-based sampling. While the reviewer acknowledges the potential challenges in adapting these methods to language tasks, they do not provide specific examples or references to the computer vision methods they are referring to. This lack of detailed justification or examples makes the claim somewhat verifiable, as it requires significant effort from the authors to understand and address the suggestion. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that including a comparison to methods used in the computer vision setting would be more beneficial than the current comparisons, such as loss-based sampling. It acknowledges the potential challenges in applying these methods, noting that they often require a supervised setup, but also suggests that some could be adapted to language tasks. This feedback is somewhat helpful as it identifies a potential area for improvement and provides a direction for expanding the comparative analysis. However, it lacks specific examples of computer vision methods that could be adapted or detailed guidance on how to implement such comparisons, limiting its utility for the authors. Therefore, it is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* Originality The most closely related work of Zemel et al. (2013) is referenced, the present paper explains how it is different, and gives comparisons in simulations. It could be improved by making these comparisons more systematic with respect to the tuning of each method--i.e. compare the best performance of each.\n",
      "Paper ID: NIPS_2017_382\n",
      "Venue: NIPS_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point acknowledges that the paper references closely related work and explains its differences, but suggests that the comparisons could be improved by making them more systematic. Specifically, it recommends comparing the best performance of each method with respect to tuning. While the suggestion is clear and provides a direction for improvement, it does not specify how to achieve a more systematic comparison or what specific tuning parameters should be considered. This makes the action explicit but somewhat vague in execution, resulting in a somewhat actionable comment.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the originality of the paper by referencing the work of Zemel et al. (2013) and acknowledges that the current paper explains its differences and provides comparisons in simulations. However, it suggests an improvement by recommending more systematic comparisons with respect to the tuning of each method, specifically comparing the best performance of each. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the section discussing related work or comparisons. The suggestion is specific, as it outlines a particular approach to improve the paper. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point acknowledges that the paper references closely related work by Zemel et al. (2013) and explains the differences, including comparisons in simulations. However, it suggests that these comparisons could be improved by being more systematic, specifically by comparing the best performance of each method. While the comment provides a logical suggestion for improvement, it lacks specific examples or detailed reasoning about how the current comparisons are insufficient. This makes the claim somewhat verifiable, as it requires the authors to infer the specifics of the suggested improvements. Therefore, the comment is classified as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment is mostly helpful as it acknowledges the originality of the paper by referencing closely related work and explaining the differences. It also suggests an area for improvement by recommending more systematic comparisons with respect to the tuning of each method, specifically comparing the best performance of each. This feedback provides the authors with a clear and actionable suggestion to enhance the rigor and depth of their comparative analysis. However, the comment could be more helpful if it included specific guidance on how to conduct these systematic comparisons or what aspects to focus on during tuning. Overall, the comment offers valuable insights for improving the draft but could be expanded for greater impact. Therefore, it is rated as mostly helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods.\n",
      "Paper ID: NIPS_2020_696\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that the authors should mention recent work on untrained neural networks (NNs) in the context of out-of-distribution (OOD) experiments and ideally compare their method with those approaches. While the comment provides a clear direction to include a discussion on untrained NNs and suggests a comparison, it lacks specific guidance on how to conduct this comparison or which particular aspects of the untrained NNs should be highlighted. The action is explicit in terms of mentioning and comparing with recent work, but it is somewhat vague regarding the execution details. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the Out-of-Distribution (OOD) experiments and suggests that the authors mention and compare their method with untrained neural networks, such as the deep image prior by Ulyanov et al. It provides a specific example of related work that should be considered for context and comparison. However, the comment does not explicitly mention which part of the paper discusses the OOD experiments, making it weakly grounded. The authors might infer that it relates to the experimental or discussion sections, but they cannot confidently pinpoint the exact location. Despite this, the comment is specific in detailing what should be addressed, including mentioning related work and comparing methods. Therefore, this comment is weakly grounded and specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point acknowledges the interest in the OOD (Out-Of-Distribution) experiments and suggests that the paper should mention and compare its method with untrained neural networks (NNs) used in recent years for solving inverse problems, such as the deep image prior by Ulyanov et al., CVPR 2018. The claim is somewhat verifiable as it provides a logical suggestion to place the current method in context with existing work. However, it lacks specific examples or detailed comparisons with the mentioned methods, which would strengthen the claim. Therefore, the comment aligns with a label of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment acknowledges the interesting aspect of the OOD experiments, noting the strong generalization of the trained network. It provides constructive feedback by suggesting that the authors mention and contextualize their method with respect to recent works on untrained neural networks, such as the deep image prior by Ulyanov et al. This suggestion is actionable and encourages the authors to enhance the discussion of their method's novelty and relevance. Additionally, the comment proposes a comparison with this class of methods, which could strengthen the paper's contribution. While the feedback is mostly helpful, it could be more comprehensive by offering specific examples or guidance on how to implement the comparison. Overall, the comment is rated as mostly helpful, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM?\n",
      "Paper ID: NIPS_2016_321\n",
      "Venue: NIPS_2016\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review point raises a question about the impact of non-parametric emission distributions on inference tasks in Hidden Markov Models (HMMs). It specifically asks which common inference tasks (filtering, smoothing, marginal observation likelihood) can be computed exactly or approximately with an NP-SPEC-HMM. While the comment implies that the authors should clarify this aspect, it does not explicitly instruct them to do so. The action is implicit but concrete, as it clearly identifies the information that needs to be addressed. However, since the comment does not directly state the action, it is mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 5.0\n",
      "Rationale: The comment is fully grounded as it clearly identifies the focus of the paper on learning Hidden Markov Models (HMMs) with non-parametric emission distributions. It specifies the issue by questioning how these emission distributions affect inference and which common inference tasks can be computed exactly or approximately with an NP-SPEC-HMM. This provides clear guidance on what needs to be addressed in the paper. Therefore, the comment is fully grounded and specific, aligning with category 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: X\n",
      "Rationale: The review point primarily consists of an observation about the paper's focus and a request for clarification regarding the impact of non-parametric emission distributions on inference tasks. It does not contain any claims, opinions, or suggestions that require verification. The statement is factual and descriptive, fitting the classification of \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a critical gap in the paper regarding the impact of non-parametric emission distributions on inference tasks in Hidden Markov Models (HMMs). It specifically questions how these distributions affect common inference tasks such as filtering, smoothing, and marginal observation likelihood. This feedback is valuable as it highlights an area that requires clarification and further exploration in the paper. However, the comment does not provide specific suggestions or guidance on how the authors might address this issue or improve their explanation. While it points out an important area for improvement, the feedback could be more actionable and detailed to be fully helpful. Therefore, it is rated as somewhat helpful, as it provides insight but not comprehensive guidance for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* The proposed NC measure takes the whole training and test datasets as input. I can hardly imagine how this method can be learned and applied to large scale datasets (e.g. ImageNet). Is there any solution to address the scalability issue? Otherwise, the practical contribution of this paper will be significantly reduced.\n",
      "Paper ID: NIPS_2020_813\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review point raises a concern about the scalability of the proposed NC measure, particularly when applied to large-scale datasets like ImageNet. It questions the practicality of the method and asks if there is a solution to address the scalability issue. While the comment identifies a potential problem, it does not provide specific guidance or suggestions on how to resolve it. The action is implicit and vague, as the authors are left to infer that they need to address scalability but are not given concrete steps or solutions to consider. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment specifically addresses the proposed NC measure and its applicability to large-scale datasets, such as ImageNet. This provides full grounding, as it is clear which part of the paper is being discussed. The comment also specifies the issue of scalability and questions the practical contribution of the paper if this issue is not addressed. However, it does not mention a specific section or provide detailed suggestions for improvement, making it under-specific. Therefore, this comment is fully grounded but under-specific, aligning with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 2\n",
      "Rationale: The review point raises a concern about the scalability of the proposed NC measure, questioning its applicability to large-scale datasets like ImageNet. The comment suggests that without a solution to this scalability issue, the practical contribution of the paper may be limited. However, the reviewer does not provide specific reasoning, examples, or references to support this claim or to illustrate how scalability issues have been addressed in similar contexts. This lack of detailed justification makes the claim borderline verifiable, as it highlights a potential issue but does not substantiate it with sufficient evidence or examples. Therefore, the comment is rated as \"2: Borderline Verifiable.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a potential scalability issue with the proposed NC measure, which requires the entire training and test datasets as input. This is a critical concern, especially for large-scale datasets like ImageNet, as it could limit the practical applicability of the method. The comment prompts the authors to consider solutions to address this scalability issue, which is a valuable direction for improving the paper's contribution. However, the comment does not provide specific suggestions or examples of how to overcome this challenge, which would enhance its helpfulness. Overall, the feedback is somewhat helpful as it highlights a significant limitation and encourages the authors to think about solutions, but it could be more beneficial with additional guidance or suggestions. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "* Unclear Literature Review: The literature review in the paper needs improvement. It is not adequately clear what the main contribution of the proposed method is, and how it distinguishes itself from existing work, particularly in relation to the utilization of GFlowNet for sequence generation. The paper should provide a more explicit and comparative analysis of related work.\n",
      "Paper ID: viNQSOadLg\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point identifies a need for improvement in the literature review, specifically regarding the clarity of the main contribution and its distinction from existing work. It suggests that the paper should provide a more explicit and comparative analysis of related work, particularly in relation to the utilization of GFlowNet for sequence generation. While the comment clearly states that the literature review needs enhancement, it lacks specific guidance on how to achieve this or which aspects of the related work should be emphasized. The action is explicit but vague, as it does not provide concrete steps for the authors to follow. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The comment addresses the literature review section, indicating that it needs improvement in terms of clarity regarding the main contribution of the proposed method and its distinction from existing work. It specifically mentions the utilization of GFlowNet for sequence generation, which provides some context. However, it does not explicitly mention which part of the literature review is unclear or lacking, making it difficult for the authors to pinpoint the exact areas needing revision. While the authors can infer that it relates to the discussion of related work, the comment lacks specificity in detailing what aspects of the literature review should be improved. Therefore, the comment is weakly grounded and not specific, aligning with a score of 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 1\n",
      "Rationale: The review point claims that the literature review is unclear, particularly in distinguishing the proposed method's contributions from existing work, especially regarding the use of GFlowNet for sequence generation. However, the comment does not provide specific examples or references to support this claim. It lacks detailed reasoning or evidence to substantiate why the literature review is inadequate or how it could be improved. This makes the claim difficult for the authors to verify and address effectively, as they are left without clear guidance on what aspects need enhancement. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a significant issue with the literature review, noting that it lacks clarity in distinguishing the main contribution of the proposed method from existing work. It specifically mentions the need for a more explicit and comparative analysis, particularly concerning the utilization of GFlowNet for sequence generation. While the comment highlights a critical area for improvement, it does not provide specific guidance or examples on how to enhance the literature review. The authors are informed of the problem but are left without detailed direction on how to address it. Therefore, the feedback is somewhat helpful, as it points out a crucial weakness but lacks depth and actionable suggestions for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- **Lack of Analysis**: There is insufficient analysis of the effectiveness of each data augmentation method. Additionally, comparing their approach to other paraphrasing methods, such as EDA or LLM-based paraphrasing, would clarify the unique advantages of their method. References [1] Allen-Zhu et al., Physics of language models: Part 3.1, knowledge storage and extraction [2] Ovadia et al., Fine-tuning or retrieval? comparing knowledge injection in llms\n",
      "Paper ID: sNycNM577m\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies two main areas for improvement: the need for more analysis of the effectiveness of each data augmentation method and the suggestion to compare the approach with other paraphrasing methods like EDA or LLM-based paraphrasing. The comment provides specific references for the authors to consider, which adds concreteness to the suggested actions. While the comment does not provide detailed guidance on how to conduct the analysis or comparison, it does offer a clear direction for the authors to follow. The explicit mention of other methods and references makes the action more concrete, although the authors may still need to determine the exact steps to take. Therefore, this comment is mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment highlights a lack of analysis regarding the effectiveness of each data augmentation method and suggests comparing the approach to other paraphrasing methods like EDA or LLM-based paraphrasing. It also references external works that could be relevant for comparison. However, the comment does not specify which part of the paper lacks analysis or where the comparisons should be made, leaving the authors to infer the relevant sections. While the suggestion is specific in terms of what additional analysis and comparisons could be included, the lack of explicit grounding makes it challenging for the authors to pinpoint the exact areas needing improvement. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point claims there is a lack of analysis regarding the effectiveness of each data augmentation method and suggests comparing the approach to other paraphrasing methods like EDA or LLM-based paraphrasing. It provides references to external works (Allen-Zhu et al. and Ovadia et al.) that could be used for comparison, which supports the claim. However, the comment could be more robust by detailing how these references specifically relate to the methods discussed or by providing examples of how such comparisons could be made. The claim is mostly verifiable due to the inclusion of references, but it could be strengthened with more detailed reasoning or examples. Therefore, the comment is rated as 4: Mostly Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a lack of analysis regarding the effectiveness of each data augmentation method used in the paper. It suggests that comparing the authors' approach to other paraphrasing methods, such as EDA or LLM-based paraphrasing, would help clarify the unique advantages of their method. The comment provides specific references for the authors to consider, which can guide them in expanding their analysis and situating their work within the broader context of existing methods. This feedback is clear and actionable, offering a concrete direction for improving the depth and comprehensiveness of the paper. However, it could be more helpful if it included specific aspects of the analysis that need attention or suggested methodologies for conducting the comparison. Overall, the comment is mostly helpful, as it effectively directs the authors toward meaningful enhancements of their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- A number of hyperparameters (e.g. regularization) are not given - For all the latent path figures (eg Fig 3) why is the y value at x= 0 always 0? Is it normalized to this? Be clear in your description (or maybe I missed it) - I would be interested in seeing some further analysis on this model, perhaps using the interpolations themselves\n",
      "Paper ID: ICLR_2021_634\n",
      "Venue: ICLR_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment addresses multiple issues. Firstly, it points out that a number of hyperparameters, such as regularization, are not provided, implicitly suggesting that these should be included. Secondly, it questions the y-value at x=0 in the latent path figures, asking for clarification on whether it is normalized, which implies that the authors should clarify this in their description. Lastly, it expresses interest in further analysis using interpolations, suggesting that additional analysis could be beneficial. While the actions are somewhat implicit, they are clear and provide a direction for the authors to improve their draft. However, the comment lacks specific guidance on how to conduct the further analysis, making it mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment addresses multiple issues within the paper. It mentions the absence of certain hyperparameters, such as regularization, and questions the y-value at x=0 in latent path figures like Figure 3. While it provides some grounding by referencing \"latent path figures\" and \"Fig 3,\" it does not specify where the hyperparameters are discussed, making it only partially grounded. The comment also suggests further analysis using interpolations but does not specify which part of the paper this suggestion pertains to. The comment is specific in its questions about the figures and the need for clarity in descriptions, but it lacks full grounding due to the absence of explicit references to sections or parts of the paper. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point raises several issues: the absence of certain hyperparameters, a question about the y-value in latent path figures, and a suggestion for further analysis using interpolations. However, it lacks detailed reasoning or evidence to support these claims. The comment does not provide specific examples or references to substantiate why these aspects are problematic or how addressing them would improve the paper. As a result, the claims are not fully verifiable, making it difficult for the authors to understand the basis of the suggestions. Therefore, the feedback is categorized as 1: Unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies several areas for improvement in the paper. It points out the omission of important hyperparameters, such as regularization, which is crucial for replicating and understanding the experiments. It also raises a question about the latent path figures, specifically why the y-value at x=0 is always 0, and suggests that the authors clarify this in their description. Additionally, the reviewer expresses interest in further analysis of the model, potentially using the interpolations themselves. While the comment highlights these issues and provides some direction for improvement, it lacks detailed guidance or specific suggestions on how to address them. Therefore, the feedback is somewhat helpful, as it identifies key areas for enhancement but does not fully support the authors in making those improvements.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Although the method discussed by the paper can be applied in general MDP, the paper is limited in navigation problems. Combining RL and planning has already been discussed in PRM-RL~[1]. It would be interesting whether we can apply such algorithms in more general tasks.\n",
      "Paper ID: NIPS_2019_1366\n",
      "Venue: NIPS_2019\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The review comment notes that while the method can be applied to general Markov Decision Processes (MDPs), the paper is limited to navigation problems. It also mentions that combining Reinforcement Learning (RL) and planning has been discussed in PRM-RL, and suggests that it would be interesting to see if such algorithms can be applied to more general tasks. However, the comment does not provide explicit guidance or suggestions on how the authors could expand their work to include more general tasks or what specific steps they should take to address this limitation. The action is implicit and vague, leaving the authors without clear direction on how to improve their draft. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests that while the method can be applied to general Markov Decision Processes (MDP), the paper is limited to navigation problems. It references existing work (PRM-RL) and raises the question of applying the discussed algorithms to more general tasks. However, the comment does not specify which part of the paper discusses the method's application or limitations, making it weakly grounded. The suggestion to explore more general tasks is specific, but without clear guidance on where or how to address this in the paper, the comment lacks full specificity. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review comment suggests that while the method can be applied to general Markov Decision Processes (MDPs), the paper is limited to navigation problems. It references PRM-RL as a prior work that combines reinforcement learning (RL) and planning, implying that the novelty or scope of the current work might be limited. However, the comment does not provide specific examples or detailed reasoning about how the current work is limited or how it could be expanded to more general tasks. The reference to PRM-RL provides some support, but the lack of detailed explanation or examples makes the claim somewhat verifiable. Authors would need to infer the specifics of how their work could be expanded or how it compares to PRM-RL.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment acknowledges that the method discussed in the paper can be applied to general Markov Decision Processes (MDPs) but notes that the paper is limited to navigation problems. It references existing work (PRM-RL) that combines Reinforcement Learning (RL) and planning, suggesting that it would be interesting to see if the algorithms could be applied to more general tasks. While the comment identifies a limitation in the scope of the paper and hints at a potential area for expansion, it lacks specific guidance or actionable suggestions on how the authors might broaden the application of their method. The feedback is somewhat helpful as it points out an area for potential improvement, but it could be more beneficial with additional details or examples on how to achieve this broader application. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- As this work has the perspective of task-oriented recommendation, it seems that works such as [] Li, Xiujun, et al. \"End-to-end task-completion neural dialogue systems.\" arXiv preprint arXiv:1703.01008 (2017). are important to include, and compare to, at least conceptually. Also, discussion in general on how their work differs from other chatbox research works e.g. [] He, Ji, et al. \"Deep reinforcement learning with a natural language action space.\" arXiv preprint arXiv:1511.04636(2015). would be very useful.\n",
      "Paper ID: NIPS_2018_894\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment suggests that the authors should include and conceptually compare their work to specific related works, such as Li et al. (2017) and He et al. (2015). It also recommends a general discussion on how their work differs from other chatbot research. The feedback provides explicit actions for the authors to take, namely including comparisons and discussions with specified works. However, it does not offer detailed guidance on how to conduct these comparisons or discussions. The action is explicit but somewhat vague in terms of execution details, making the comment mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests including and comparing the work with specific existing literature, such as Li et al. (2017) and He et al. (2015), which are relevant to task-oriented recommendation and chatbox research. However, it does not specify which part of the paper should include these comparisons or discussions, making it weakly grounded. The comment is specific in terms of what is missing—conceptual comparisons and discussions with the mentioned works—but lacks guidance on where these should be integrated into the paper. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review comment suggests that the paper should include and compare its work with other relevant studies, specifically mentioning two works: Li et al. (2017) and He et al. (2015). The comment implies that these works are important for conceptual comparison due to their relevance to task-oriented recommendation and chatbox research. While the comment provides specific references, it lacks detailed reasoning or explanation on how these works are directly related or why they are crucial for comparison. This makes the claim somewhat verifiable, as it provides a direction for improvement but requires more detailed justification to fully support the claim.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a gap in the manuscript regarding the inclusion and comparison of relevant works in the field of task-oriented recommendation. It suggests that the authors should include and conceptually compare their work with specific studies, such as Li et al.'s \"End-to-end task-completion neural dialogue systems\" and He et al.'s \"Deep reinforcement learning with a natural language action space.\" Additionally, the comment recommends a general discussion on how the current work differs from other chatbot research. This feedback is clear and actionable, providing specific references and a direction for the authors to enhance the comprehensiveness and contextualization of their work within the existing literature. However, it could be more helpful if it provided more detailed guidance on how to structure these comparisons or discussions. Overall, the comment is mostly helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Does it mean that inference gets slowed down drastically, and there’s no way to only do inference (i.e., predict the label)? I don’t think this is fatal though. What’s the coefficient of the p(L, E | X) term in line 307? Why is it 1? Hyperparamter details are missing, so it’s not clear whether baselines are well-tuned, and whether ablation studies provide confident results. The writing is not careful, and often impedes understanding.\n",
      "Paper ID: ARR_2022_201_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises several issues and questions, including concerns about inference speed, the coefficient of a specific term, missing hyperparameter details, and the quality of writing. However, it does not provide explicit guidance on how the authors should address these issues. The authors are left to infer that they need to clarify the inference process, provide hyperparameter details, and improve the writing, but the comment lacks concrete steps or suggestions for how to implement these changes. As a result, the comment is somewhat actionable, as it identifies areas for improvement but does not offer detailed guidance on execution.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses multiple issues within the paper, including the potential slowdown of inference, a specific question about the coefficient of a term in line 307, missing hyperparameter details, and concerns about the clarity of the writing. The mention of line 307 provides full grounding for that specific part, allowing authors to identify the section being addressed. However, the other points, such as the inference slowdown and hyperparameter details, are not grounded as they do not specify where in the paper these issues are discussed. The comment is specific in detailing what needs to be addressed, such as providing hyperparameter details and improving the writing for clarity. Overall, the comment is partially grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 3 | GPT Label: 1\n",
      "Rationale: The review point raises several claims and questions regarding the paper's content, such as the potential slowdown of inference, the coefficient of a specific term, and missing hyperparameter details. However, these claims are presented as questions or observations without providing specific evidence or references to support them. The comment also critiques the writing quality but does not offer examples or detailed reasoning. Due to the lack of explicit justification or evidence for these claims, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment addresses several important issues that could help improve the draft. It raises a question about the potential slowdown in inference, which could be crucial for understanding the practical implications of the method. The comment also questions the coefficient of a specific term in the paper, indicating a need for clarification in the mathematical formulation. Additionally, it points out the absence of hyperparameter details, which is essential for assessing the validity of the baselines and the confidence in ablation studies. Lastly, it critiques the writing quality, noting that it often impedes understanding. While the comment identifies multiple areas for improvement, it could be more helpful by providing specific suggestions or examples on how to address these issues. Overall, the comment is mostly helpful as it highlights key areas needing attention and provides some direction, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Figure 1 could be optimized to use less whitespace.\n",
      "Paper ID: NIPS_2018_66\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests optimizing Figure 1 to use less whitespace. While it provides a clear and explicit action—reducing whitespace in the figure—it does not offer specific guidance on how to achieve this optimization. The authors know what needs to be done, but the comment lacks concrete details on how to implement the change, such as suggestions for rearranging elements or resizing components. Therefore, the comment is somewhat actionable, as it identifies a clear area for improvement but lacks detailed guidance on execution.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment explicitly mentions \"Figure 1,\" allowing the authors to accurately identify the part of the paper being addressed, which provides full grounding. However, it lacks specificity as it does not provide details on how the whitespace should be optimized or what specific changes could be made to improve the figure. Therefore, this comment is fully grounded and under-specific, aligning with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 1\n",
      "Rationale: The review point suggests that \"Figure 1 could be optimized to use less whitespace.\" This is a subjective observation without any supporting evidence or detailed explanation. The comment does not provide specific reasons or examples of how the whitespace affects the figure's effectiveness or how optimizing it would improve the paper. As a result, the claim lacks verifiability, making it difficult for the authors to understand the basis of the suggestion or how to address it. Therefore, the feedback is categorized as \"1: Unverifiable.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment provides a specific suggestion for improving the visual presentation of Figure 1 by optimizing it to use less whitespace. This feedback is clear and actionable, as it directly addresses a potential issue with the figure's layout and offers a straightforward way to enhance the draft's visual appeal. However, the comment could be more helpful if it included additional guidance on how to achieve this optimization or explained why reducing whitespace would benefit the overall presentation. Despite this, the feedback is mostly helpful as it gives the authors a clear direction for improving their draft's visual elements.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Figure 6: the font size is a little bit small.\n",
      "Paper ID: ARR_2022_113_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies an issue with the font size in Figure 6, suggesting that it is too small. While this observation is explicit, it lacks concrete guidance on how to address the issue, such as recommending a specific font size or providing examples of acceptable font sizes. The authors know they need to adjust the font size, but the lack of specific instructions makes the action somewhat vague. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions \"Figure 6,\" allowing the authors to accurately identify the part of the paper being addressed. However, it is under-specific because it only points out that the font size is small without providing further details or suggestions on how to improve it, such as recommending a specific font size or discussing the impact of the small font on readability. Therefore, this comment is fully grounded but under-specific, aligning with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: X\n",
      "Rationale: The comment makes an observation about the font size in Figure 6, but it does not express an opinion, claim, or suggestion that requires verification. It is a factual statement about the presentation of the figure, fitting the classification of \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 1.0\n",
      "Rationale: The review comment points out a minor issue with Figure 6, specifically noting that the font size is a little bit small. While this is a valid observation, it is a very minor detail and does not significantly impact the overall quality or understanding of the paper. The comment lacks depth and does not provide substantial guidance for improving the draft beyond this small adjustment. Therefore, it is not particularly helpful in guiding the authors to make meaningful improvements to their work, aligning with a score of 1 for being not helpful at all.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- For RBI, they only train on rewarded actions. Then this means rewardless actions that get useful supervision (such as \"No, the answer is Timothy Dalton.\" in Task 3) is ignored as well. I think this could be one significant factor that makes FP + RBI better than RBI alone. If not, I think the authors should provide stronger baseline than RBI (that is supervised by such feedback) to prove the usefulness of FP. Questions / Minor concerns:\n",
      "Paper ID: NIPS_2016_93\n",
      "Venue: NIPS_2016\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review point raises a concern about the training approach for RBI, specifically noting that only rewarded actions are trained on, potentially ignoring useful supervision from rewardless actions. It suggests that this could be a significant factor in the performance difference between FP + RBI and RBI alone. While the comment implies that the authors should consider providing a stronger baseline than RBI to demonstrate the usefulness of FP, it does not explicitly instruct them to do so or provide concrete guidance on how to address the issue. The action is implicit and somewhat vague, making it barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment discusses a specific aspect of the RBI training approach, highlighting a potential issue with ignoring rewardless actions that could provide useful supervision. It suggests that this might be a significant factor in the performance difference between FP + RBI and RBI alone. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the methodology or results section. The comment is specific in pointing out a potential flaw in the training approach and suggesting the need for a stronger baseline. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point makes a claim about the training approach for RBI, suggesting that only rewarded actions are trained on, potentially ignoring useful supervision from rewardless actions. This is presented as a possible reason why FP + RBI performs better than RBI alone. The comment provides a logical reasoning for the claim, but it lacks specific examples or references to support the assertion. While the reasoning is sound, the absence of detailed evidence or external references makes the claim somewhat verifiable. The authors would need to conduct further analysis or provide additional data to fully address the critique, justifying a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a potential limitation in the training approach for RBI, specifically noting that only rewarded actions are trained on, which may lead to the exclusion of useful supervision from rewardless actions. It suggests that this could be a significant factor in the performance difference between FP + RBI and RBI alone. The comment also implies that a stronger baseline should be provided to validate the usefulness of FP. While the comment raises important points and prompts the authors to reconsider their approach, it lacks specific suggestions or detailed guidance on how to address these issues. The feedback is somewhat helpful as it provides insight into potential weaknesses and encourages further exploration, but it could be more actionable with additional guidance or examples. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- How would we choose which ELM to pick (male/female)? Does this require us to know the speaker’s gender beforehand, i.e., at inference time? This seems like a drawback as the accuracy should be calculated after using a gender detection model in the pipeline (at least in the cases where vocal traits match speaker identity).\n",
      "Paper ID: Ie040B4nFm\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises a question about the selection of ELM (male/female) and whether it requires prior knowledge of the speaker's gender at inference time. It points out a potential drawback and suggests that accuracy should be calculated after using a gender detection model in the pipeline. While the comment highlights an issue, it does not provide explicit guidance or concrete steps on how to address it. The authors can infer that they might need to incorporate a gender detection model, but the comment lacks specific instructions on how to implement this or how to address the identified drawback. Therefore, the action is implicit and somewhat vague, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises a specific concern about the choice of ELM (male/female) and its implications for the inference process, particularly regarding the need to know the speaker's gender beforehand. However, it does not explicitly mention which part of the paper discusses this issue, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where this topic is discussed, the comment lacks full grounding. It is specific about the issue of gender-based ELM selection and its potential drawbacks but does not provide detailed guidance on how to address it. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point raises a concern about the selection of an ELM (male/female) and questions whether the speaker's gender needs to be known beforehand, suggesting this as a potential drawback. The comment logically reasons that accuracy should be calculated after incorporating a gender detection model in the pipeline. However, it lacks specific examples or references to support the claim that this is a drawback or how it impacts the overall performance. While the reasoning is sound, the absence of detailed evidence or examples makes the claim somewhat verifiable, as authors might need additional information to fully understand and address the issue.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises an important question about the methodology, specifically regarding the choice of which ELM (male/female) to use and whether this requires prior knowledge of the speaker's gender at inference time. It highlights a potential drawback in the approach and suggests that accuracy should be calculated after incorporating a gender detection model in the pipeline. This feedback is valuable as it prompts the authors to consider an aspect of their methodology that could impact the practicality and accuracy of their results. However, the comment could be more helpful if it provided specific suggestions or examples of how to address this issue. Overall, it is somewhat helpful as it identifies a potential weakness and encourages the authors to think critically about their approach, but it lacks detailed guidance for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- I don't understand effectiveness of the multi-view clustering approach. Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.\n",
      "Paper ID: ACL_2017_614_review\n",
      "Venue: ACL_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment questions the effectiveness of the multi-view clustering approach, noting that the paraphrase similarity view performs significantly better than other views and their combination. It suggests that more analysis is needed to understand the usefulness of the other views and how different clustering techniques differ. However, the comment does not provide explicit instructions or specific guidance on how the authors should conduct this additional analysis or what aspects to focus on. The action is implicit and vague, leaving the authors uncertain about how to address the feedback effectively. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the effectiveness of the multi-view clustering approach and highlights a specific issue: the paraphrase similarity view performs significantly better than other views and their combination. It questions the usefulness of the other views and points out the lack of detailed analysis on how different clustering techniques differ. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the results and discussion sections. The comment is specific in its critique, as it identifies a particular problem and suggests the need for a more detailed analysis. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point questions the effectiveness of the multi-view clustering approach, noting that the paraphrase similarity view outperforms other views and their combination. It highlights the lack of detailed analysis on how different views contribute to clustering, aside from a single empirical example. While the comment identifies a gap in the analysis, it does not provide specific suggestions or references to support the claim, making it somewhat verifiable. The authors would need to conduct further analysis to address the critique, but the comment provides a clear direction for improvement.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment raises a valid concern about the effectiveness of the multi-view clustering approach, noting that the paraphrase similarity view outperforms other views and their combination. It questions the usefulness of the other views and highlights the lack of detailed analysis on how different clustering techniques differ. By pointing out these gaps, the comment provides the authors with a clear direction for improvement, specifically suggesting the need for a more detailed analysis of the differences and similarities between the views. However, while it identifies the issue, it does not offer specific suggestions or methodologies for conducting this analysis, which could enhance its helpfulness. Overall, the comment is mostly helpful as it directs the authors to a critical area needing further exploration, but it could be more comprehensive with additional guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- I would say that section 3.2 can be eliminated - I think that at this point readers can be presumed to know about the Gumbel-Softmax/Concrete distribution.\n",
      "Paper ID: NIPS_2018_66\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment explicitly suggests eliminating section 3.2, based on the assumption that readers are already familiar with the Gumbel-Softmax/Concrete distribution. This provides a clear and direct action for the authors to consider, which is to remove the section. However, the comment does not provide further guidance on whether any content from the section should be integrated elsewhere or if additional context is needed to ensure the paper remains coherent. While the action is explicit, the lack of additional details on how to implement the change makes it somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment explicitly mentions \"section 3.2,\" allowing the authors to accurately identify the part of the paper being addressed. However, it lacks specificity as it does not provide a detailed explanation of why the section can be eliminated or what specific content is redundant. The comment assumes that readers are familiar with the Gumbel-Softmax/Concrete distribution but does not offer further guidance on how to address this assumption. Therefore, this comment is fully grounded but under-specific, aligning with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 3 | GPT Label: 1\n",
      "Rationale: The review point suggests eliminating Section 3.2, assuming that readers are already familiar with the Gumbel-Softmax/Concrete distribution. However, the comment does not provide any supporting evidence or reasoning to justify this assumption. There are no references to external sources or logical arguments that would help verify the claim that readers are presumed to know this information. As a result, the claim is unverifiable, as it lacks the necessary support to substantiate the suggestion.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests eliminating Section 3.2, assuming that readers are already familiar with the Gumbel-Softmax/Concrete distribution. While this feedback provides a clear and actionable suggestion, it lacks depth and does not consider the potential value of the section for readers who may not be familiar with the topic. Additionally, it does not offer alternative ways to present the information or suggest how to integrate any essential content from the section elsewhere in the paper. Therefore, while the comment is somewhat helpful in providing a specific suggestion, it could be more comprehensive in addressing the potential impact of the removal.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.\n",
      "Paper ID: NIPS_2020_341\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises a question about the reasonableness of the training time for the German and Law school datasets in the context of Gerrymandering versus Independent. It also suggests that since the main advantage of the approach is its computation time, it would be beneficial to publish the code. The comment implies that the authors should consider the training time differences and make their code available, but it does not explicitly instruct them to do so. The action is implicit and somewhat vague, as it lacks specific guidance on how to address the training time concern or the process for publishing the code. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses specific aspects of the experiments, particularly the training time for the German and Law school datasets in the context of Gerrymandering versus Independent. It also references Experiment 2, mentioning the performance of ERM and plug-in compared to Kearns et al., and suggests publishing the code. However, it does not explicitly mention which part of the paper these experiments are discussed in, making it weakly grounded. The comment is specific in its inquiry about the reasonableness of the training times and the suggestion to publish the code, providing clear guidance on what the authors should consider. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point raises a question about the reasonableness of the training time differences between datasets in the context of Gerrymandering versus Independent. It suggests that the main advantage of the approach is computational efficiency and recommends publishing the code. However, the comment does not provide specific evidence or references to support the claim about training time differences or the computational advantage. The reasoning is based on a general observation without detailed justification, making it somewhat verifiable. Authors might need to explore the rationale further to fully understand and address the concern.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises a question about the training time for specific datasets in the experiments, suggesting that it might be unreasonable for the German and Law school datasets to have shorter training times in Gerrymandering compared to Independent. It also notes that in Experiment 2, ERM and plug-in have similar performance to Kearns et al., with the main advantage being computation time. The comment concludes with a suggestion to publish the code, which is a clear and actionable recommendation. However, the initial question about the training time lacks depth and does not provide specific guidance on how the authors might address or investigate this issue further. While the suggestion to publish the code is helpful, the overall feedback could be more comprehensive by offering more detailed insights or suggestions related to the training time concern. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- In the paper it is mentioned that the obtained core tensors can be rounded to smaller ranks with a given accuracy by clustering the values of the domain sets or imposing some error decision epsilon if the values are not discrete. It is not clear what is, in theory, the effect on the approximation in the full tensor error. Is there any error bound in terms of epsilon?\n",
      "Paper ID: ICLR_2023_1195\n",
      "Venue: ICLR_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment raises a question about the theoretical effect on the approximation in the full tensor error when core tensors are rounded to smaller ranks. It specifically asks if there is any error bound in terms of epsilon. While the comment implies that the authors should address this theoretical aspect, it does not explicitly instruct them to do so. The action is concrete, as it points to a specific area that needs clarification, but it is communicated indirectly. Therefore, this comment is mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses a specific part of the paper related to the rounding of core tensors to smaller ranks and the potential impact on approximation error. While it does not explicitly mention a section or figure, the authors can infer the relevant part of the paper being discussed. The comment is specific in its request for clarification on the theoretical effect on the full tensor error and whether there is an error bound in terms of epsilon. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: X\n",
      "Rationale: The review point consists of a factual statement followed by a request for clarification. It does not contain an opinion, judgment, or suggestion that requires verification. The reviewer is asking for additional information regarding the theoretical effect on approximation error, which is a request for clarification rather than a claim. Therefore, it should be labeled as \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific area of concern regarding the theoretical implications of rounding core tensors to smaller ranks with a given accuracy. It questions the effect on the approximation error in the full tensor and asks if there is an error bound in terms of epsilon. This feedback is clear and prompts the authors to provide a theoretical analysis or clarification on the impact of their rounding approach. However, while the comment raises an important question, it does not offer specific suggestions or guidance on how the authors might address this issue, such as proposing methods to derive or present the error bound. Therefore, the comment is somewhat helpful as it directs the authors to an area needing clarification but lacks detailed, actionable advice for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- It is interesting to see how DVP perform on video with different length?\n",
      "Paper ID: NIPS_2020_91\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment, \"It is interesting to see how DVP perform on video with different length?\" implies a curiosity about the performance of DVP on videos of varying lengths. However, it does not explicitly instruct the authors to conduct any specific analysis or experiments related to this curiosity. The comment is vague and lacks concrete guidance on what actions the authors should take to address this point. As a result, the authors are left to infer that they might need to explore this aspect, but without explicit instructions, the comment remains barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment raises an interesting point about the performance of DVP on videos of varying lengths. However, it does not specify which part of the paper this observation relates to, making it weakly grounded. Additionally, the comment lacks specificity as it does not provide details on what aspects of the performance should be analyzed or how this analysis could be conducted. Without clear guidance or specific references, the authors may struggle to address this point effectively. Therefore, the comment is weakly grounded and not specific, aligning with label 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 1 | GPT Label: X\n",
      "Rationale: The comment is phrased as an observation and a question, expressing interest in the performance of DVP on videos of varying lengths. It does not contain any subjective claims, judgments, or suggestions that require verification. Therefore, it is a factual and descriptive statement, fitting the classification of \"No.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment raises an interesting point about evaluating the performance of DVP on videos of varying lengths. However, it lacks specificity and does not provide actionable guidance or suggestions for how the authors might incorporate this evaluation into their work. Without further details or context, the authors may find it challenging to address this feedback effectively. The comment identifies a potential area for exploration but does not offer enough depth or clarity to be fully helpful. Therefore, it aligns with a score of 2, as it is barely helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- It is unclear why the model does not fully succeed in identifying the true sources in the triangle dataset. Is one of the assumptions not satisfied? Are there learning difficulties? Further comments:\n",
      "Paper ID: ICLR_2022_488\n",
      "Venue: ICLR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review point raises a concern about the model's performance on the triangle dataset, questioning why it does not fully succeed in identifying the true sources. It suggests potential reasons, such as unsatisfied assumptions or learning difficulties, but does not provide explicit guidance or suggestions on how the authors might investigate or address these issues. The comment implies that the authors should explore these potential causes, but it lacks concrete steps or specific actions to take. As a result, the authors may find it challenging to determine the exact course of action needed to improve their draft. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises a concern about the model's performance on the triangle dataset, questioning why it does not fully succeed in identifying the true sources. However, it does not specify which part of the paper discusses this dataset or the model's performance on it, making it weakly grounded. The comment is specific in that it suggests potential reasons for the issue, such as unsatisfied assumptions or learning difficulties, but it does not provide detailed guidance on how to address these concerns. Therefore, this comment is weakly grounded and specific, aligning with label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 1\n",
      "Rationale: The review comment raises a question about the model's failure to identify true sources in the triangle dataset but does not provide any supporting evidence, reasoning, or examples to explain why this might be the case. The comment suggests potential issues, such as unsatisfied assumptions or learning difficulties, but does not elaborate on these points. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific issue with the model's performance on the triangle dataset, questioning why it fails to fully identify the true sources. It raises important questions about whether the assumptions are not satisfied or if there are learning difficulties, which could be crucial for understanding and improving the model. However, the comment lacks depth and does not provide specific guidance or suggestions on how the authors might investigate or address these issues. While it points out a potential weakness, it does not offer actionable advice, making it somewhat helpful but incomplete. Therefore, the comment aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- It's regrettable that the probability mass function is practically unexploited. In MixBoost it is set to a quasi-uniform distribution, which depends on only one single parameter. Intuitively, each learner class should be considered individually, even in the case of BDT of different depths. I think that considering various probability mass function would've added further depth to the experimental setting (unless I'm missing an obvious reason why the quasi-uniform distribution is well suited...).\n",
      "Paper ID: NIPS_2020_936\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment expresses regret that the probability mass function is underutilized in the MixBoost method and suggests that considering various probability mass functions could add depth to the experimental setting. However, it does not provide explicit guidance on how the authors should address this issue or offer specific actions to improve the draft. The comment implies that the authors should explore different probability mass functions, but it lacks concrete details on how to implement this suggestion. As a result, the action is inferred and vague, making the comment barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment critiques the use of the probability mass function in MixBoost, noting that it is set to a quasi-uniform distribution and suggesting that each learner class should be considered individually. It implies that exploring various probability mass functions could enhance the experimental setting. However, the comment does not specify which section of the paper this critique pertains to, leaving the authors to infer that it relates to the methodology or experimental design sections. While the comment is specific in its suggestion to consider different probability mass functions, it is weakly grounded as it does not explicitly reference a particular part of the paper. Therefore, this comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review comment suggests that the probability mass function is underutilized in the MixBoost method, as it is set to a quasi-uniform distribution dependent on a single parameter. The reviewer argues that each learner class should be considered individually and that exploring various probability mass functions could enhance the experimental setting. However, the comment lacks specific examples or references to support why different probability mass functions would be beneficial or how they could be implemented. This makes the claim somewhat verifiable, as it provides a logical suggestion but lacks detailed justification or evidence.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a potential area for improvement by pointing out that the probability mass function is underutilized in the current approach, as it is set to a quasi-uniform distribution dependent on a single parameter. The reviewer suggests that considering each learner class individually could add depth to the experimental setting. This feedback is clear and actionable, as it provides a specific direction for enhancing the experimental design. However, the comment could be more helpful if it included suggestions on how to implement this consideration or provided examples of alternative probability mass functions. Overall, the comment is mostly helpful, as it highlights a significant area for improvement and offers a constructive suggestion, but it could be more comprehensive with additional guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- It's unclear what this paper's motivation is as I do not see a clear application from the proposed method. The paper showed results mapping one RGB image to another RGB image (with a different style). When do we need this domain adaptation, and how would this be useful? For example, it would have been better to demonstrate the methodology's use on some actual tasks involving domain adaptation, such as adapting a model trained on a synthetic dataset to a real dataset.\n",
      "Paper ID: ICLR_2021_2929\n",
      "Venue: ICLR_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review point raises concerns about the motivation and practical application of the proposed method, specifically questioning the need for the domain adaptation demonstrated in the paper. It suggests that the authors should provide examples of actual tasks where their methodology could be useful, such as adapting a model trained on a synthetic dataset to a real dataset. While the comment implies that the authors should clarify the motivation and provide practical examples, it does not explicitly instruct them to do so. The action is inferred but provides a concrete direction for improvement, making this comment mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment questions the motivation and application of the proposed method, specifically mentioning the results of mapping one RGB image to another with a different style. It suggests demonstrating the methodology's use on actual tasks involving domain adaptation, such as adapting a model trained on a synthetic dataset to a real dataset. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the motivation or application sections. The suggestion is specific, as it provides a clear example of how the methodology could be better demonstrated. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point questions the motivation and application of the proposed method, specifically regarding the domain adaptation from one RGB image to another with a different style. The reviewer suggests that demonstrating the methodology on actual tasks involving domain adaptation, such as adapting a model from a synthetic to a real dataset, would clarify its usefulness. While the comment provides a logical reasoning for the critique, it lacks specific examples or references to substantiate the claim fully. The suggestion for improvement is clear, but the absence of detailed evidence or comparisons makes the claim somewhat verifiable. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment identifies a critical issue with the paper's motivation and application, questioning the practical utility of the proposed method. It highlights the need for a clear application or use case for the domain adaptation technique presented in the paper. The reviewer suggests that demonstrating the methodology on actual tasks involving domain adaptation, such as adapting a model from a synthetic dataset to a real dataset, would strengthen the paper. This feedback is clear and actionable, providing the authors with a specific direction to enhance the relevance and applicability of their work. However, it could be more helpful if it offered additional examples or detailed guidance on how to implement these suggestions. Overall, the comment is mostly helpful as it identifies a significant area for improvement and provides a constructive suggestion, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Kernels are implemented with OpenAI's Triton, not CUDA; a full-page explanation is unnecessary due to well-known engineering improvements.\n",
      "Paper ID: SjgfWbamtN\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The comment suggests that the full-page explanation of kernels implemented with OpenAI's Triton is unnecessary because the engineering improvements are well-known. However, it does not provide explicit guidance on what the authors should do to address this issue, such as suggesting how to condense the explanation or what specific information could be omitted. The action is implicit, as the authors need to infer that they should reduce the explanation, and it is vague because it lacks concrete steps for improvement. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 1.0\n",
      "Rationale: The comment points out that the kernels are implemented with OpenAI's Triton instead of CUDA and suggests that a full-page explanation is unnecessary due to the well-known nature of these engineering improvements. However, it does not specify which part of the paper contains the full-page explanation, leaving the authors unable to confidently determine the exact section being addressed. Additionally, while the comment implies that the explanation is excessive, it does not provide specific guidance on what could be condensed or omitted. As a result, the comment is not grounded because the authors cannot identify which part of the paper is being addressed, and it is not specific because it lacks detailed feedback on what needs to be improved. Therefore, this comment is labeled as \"1: Not Grounded.\"\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 1\n",
      "Rationale: The review point claims that a full-page explanation of kernel implementation with OpenAI's Triton is unnecessary because the engineering improvements are well-known. However, the comment does not provide any specific references or detailed reasoning to support the assertion that these improvements are widely recognized or that the explanation is redundant. Without additional context or evidence, the claim lacks verifiability, making it difficult for the authors to understand the basis of the suggestion. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The comment points out that the paper includes a full-page explanation of kernels implemented with OpenAI's Triton, which the reviewer deems unnecessary due to the well-known nature of the engineering improvements. While this feedback highlights an area where the paper could be more concise, it lacks specific guidance on how to revise the content or what aspects could be condensed. The comment does not provide actionable steps for the authors to improve their draft, such as suggesting which parts of the explanation could be removed or how to integrate the information more succinctly. Therefore, the feedback is barely helpful, as it identifies a potential issue but does not offer sufficient direction for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- No in-depth analysis. The authors found inverse scaling happens over compute, but why? It would make the paper much more solid if the authors can provide some analysis explaining such training dynamics.\n",
      "Paper ID: 5BWvVIa5Uz\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment points out the lack of in-depth analysis regarding the observed inverse scaling over compute. It suggests that providing an analysis to explain these training dynamics would strengthen the paper. While the comment implies that an analysis should be conducted, it does not specify what kind of analysis is needed or how the authors should approach it. The action is implicit and vague, leaving the authors without clear guidance on how to proceed. Therefore, the comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment highlights the need for an in-depth analysis to explain the observed inverse scaling over compute, which is a specific issue. However, it does not mention a specific section or part of the paper where this analysis should be included, leaving the authors to infer that it relates to the results or discussion sections. This makes the comment weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. Nonetheless, the comment is specific in detailing what needs to be addressed, which is the explanation of training dynamics. Therefore, this comment is weakly grounded and specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 1\n",
      "Rationale: The review point claims that there is \"no in-depth analysis\" regarding the inverse scaling observed over compute. While it suggests that an analysis explaining the training dynamics would strengthen the paper, it does not provide any specific reasoning, examples, or references to support the claim or guide the authors on how to address it. The lack of detailed justification or evidence makes the comment unverifiable, as it does not offer a clear path for improvement.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a significant gap in the paper, specifically the lack of in-depth analysis regarding the observed inverse scaling over compute. It suggests that providing an analysis to explain these training dynamics would strengthen the paper. While the comment highlights an important area for improvement, it does not offer specific guidance or suggestions on how the authors might conduct this analysis or what aspects to focus on. This lack of actionable feedback limits the comment's usefulness, as the authors are left with a general idea of what needs to be addressed but without detailed direction on how to proceed. Therefore, the comment is somewhat helpful, providing insight into a critical issue but lacking depth and specificity in its guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Overall, the writing quality of the paper should be improved; e.g., the authors spend the same space on explaining basic memory networks and then the forward model. The related work has missing pieces on more reinforcement learning tasks in the literature.\n",
      "Paper ID: NIPS_2016_93\n",
      "Venue: NIPS_2016\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that the writing quality of the paper should be improved, specifically pointing out that the authors spend an equal amount of space on explaining basic memory networks and the forward model. It also notes that the related work section is missing information on more reinforcement learning tasks in the literature. While the comment identifies areas for improvement, it lacks explicit guidance on how to enhance the writing quality or which specific reinforcement learning tasks should be included. The feedback is somewhat vague, as it does not provide concrete steps for the authors to follow. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The comment provides some guidance on improving the writing quality by pointing out the imbalance in the space allocated to explaining basic memory networks versus the forward model. However, it does not specify which sections or paragraphs need revision, making it weakly grounded. The mention of missing pieces in the related work section regarding reinforcement learning tasks is also not specific enough to guide the authors on what exactly is missing or where to find it. Therefore, the comment is weakly grounded and not specific, aligning with a score of 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point suggests that the writing quality of the paper needs improvement, specifically mentioning the disproportionate space allocated to explaining basic memory networks compared to the forward model. Additionally, it claims that the related work section lacks coverage of reinforcement learning tasks. However, the comment does not provide specific examples or detailed reasoning to support these claims, making it difficult for the authors to understand the basis of the suggestions. The lack of supporting evidence or justification renders the claims unverifiable. Therefore, the feedback is categorized as 1: Unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies two areas for improvement: the writing quality of the paper and the coverage of related work. It suggests that the authors should allocate space more effectively, particularly in explaining basic memory networks and the forward model. Additionally, it points out that the related work section is missing information on reinforcement learning tasks. While the comment highlights important areas for enhancement, it lacks specific guidance or examples on how to improve the writing quality or what specific reinforcement learning tasks should be included. This limits the comment's helpfulness, as the authors are left with a general idea of what needs improvement but without detailed direction. Therefore, the comment is somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Rejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results.\n",
      "Paper ID: rGvDRT4Z60\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that the authors should include rejection rates in their experiments or consider viewing misclassifications as rejections. This is an explicit action, as it directly instructs the authors on what to include in their results. However, it does not provide detailed guidance on how to implement this suggestion, such as how to calculate or present the rejection rates. While the action is clear, the lack of concrete details on execution makes it somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests including rejection rates in the results or viewing misclassifications as rejections. While it provides a specific suggestion for improvement, it does not explicitly mention which part of the paper or which experiments this applies to. The authors can infer that it relates to the results or experimental sections, but this inference is not direct. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point suggests including rejection rates in the results, implying that misclassifications could be viewed as rejections. However, the comment does not provide any specific reasoning, examples, or references to support this suggestion. The lack of justification makes it challenging for the authors to understand the importance or relevance of including rejection rates. As a result, the claim is not verifiable due to the absence of supporting evidence or rationale, making it difficult for the authors to address the suggestion effectively. Therefore, the feedback is categorized as 1: Unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment identifies a specific gap in the experimental results, namely the absence of rejection rates. It suggests a potential way to address this issue by viewing misclassifications as rejections, providing a clear and actionable recommendation for the authors. This feedback is valuable as it highlights an area for improvement and offers a practical solution to enhance the analysis of the results. However, it could be more helpful if it explained the importance of including rejection rates or how this addition could impact the interpretation of the results. Nonetheless, the comment provides a concrete step for the authors to improve their draft, making it mostly helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?\n",
      "Paper ID: ARR_2022_112_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review point explicitly instructs the authors to provide more details about the traits of the experts involved in the annotation process and to justify why expert annotation is necessary beyond its commercial value. It poses specific questions regarding the nature of the experts (linguistic or domain experts), the differences in annotation compared to non-experts, and any linguistic challenges introduced. These questions guide the authors on what additional information to include, making the action explicit. However, while the questions are specific, the comment does not provide detailed guidance on how to present this information. Therefore, the comment is mostly actionable, as it gives clear directions but lacks concrete implementation details.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is somewhat grounded as it refers to a point related to the traits of the experts involved in annotation, but it does not specify which section of the paper this relates to. The authors might infer that it pertains to the methodology or a section discussing the annotation process, but the exact location is not clear. However, the comment is specific in detailing what needs to be addressed: the traits of the experts, the justification for using experts, and whether there were any linguistic challenges. This specificity provides clear guidance on what the authors need to elaborate on. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point suggests that the authors should provide more information about the traits of the experts involved in annotation and justify the necessity of using experts over non-experts. While the comment raises valid questions, it lacks specific reasoning or evidence to support why these details are crucial for the paper's argument or how they impact the study's validity. The comment does not provide examples or references to substantiate the claim, making it difficult for the authors to understand the importance of addressing these questions. Therefore, the claim is considered somewhat verifiable, as it requires additional explanation or evidence to be fully substantiated.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment is mostly helpful as it identifies a specific area where the manuscript could be improved, namely by providing more information about the traits of the experts involved in the annotation process and justifying the necessity of using experts over non-experts. The comment raises pertinent questions about the nature of the experts, whether they are linguistic or domain experts, and whether their involvement introduced any linguistic challenges. These questions prompt the authors to clarify and justify their choice of annotators, which is crucial for understanding the validity and reliability of the annotations. However, the comment could be more helpful if it offered suggestions on how to present this information or provided examples of what might constitute a strong justification. Overall, the comment provides valuable insights and guidance for the authors to enhance the clarity and justification of their methodology, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The CNN experiments are not fully convincing (see below).\n",
      "Paper ID: NIPS_2020_810\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The review point states that \"The CNN experiments are not fully convincing,\" but it does not provide any specific details or guidance on why the experiments are unconvincing or how they could be improved. Without further elaboration or suggestions, the authors are left without a clear understanding of what aspects of the experiments need attention or modification. This lack of actionable advice makes the comment unactionable, as it does not offer any direction for improvement.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The comment indicates that the CNN experiments are not fully convincing, but it does not specify which aspects of the experiments are lacking or need improvement. Additionally, the phrase \"see below\" suggests that there might be more detailed information elsewhere, but without that context, this comment alone does not provide grounding. The authors cannot confidently determine which specific parts of the experiments are being addressed based on this comment alone. Furthermore, the comment lacks specificity regarding what exactly is unconvincing or how it could be improved. Therefore, this comment is not grounded and not specific, aligning with a score of 1.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 2 | GPT Label: 1\n",
      "Rationale: The review point makes a claim that \"the CNN experiments are not fully convincing,\" but it does not provide any supporting evidence, reasoning, or specific examples to substantiate this claim. Without further elaboration or justification, the authors are left without guidance on how to address the issue, making the comment unverifiable. Therefore, it falls into Category 1: Unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The comment indicates that the CNN experiments are not fully convincing but fails to provide any details or specific reasons why this is the case. It also does not offer any suggestions or guidance on how the authors might improve the experiments to make them more convincing. Without further elaboration or actionable feedback, the authors are left without a clear understanding of the issue or how to address it. This lack of specificity and actionable content makes the comment not helpful at all, aligning with a score of 1.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).\n",
      "Paper ID: CEPkRTOlut\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The review point raises a concern about the amount of data used to train the text disambiguation model compared to the end-to-end system, questioning the conclusion that the direct model is clearly better. However, it does not provide any explicit or implicit suggestions for how the authors should address this issue or improve their draft. The comment lacks specific guidance or actionable steps, leaving the authors without a clear direction on how to respond to the feedback. As a result, the comment is unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment highlights a specific issue regarding the amount of data used to train the text disambiguation model compared to the end-to-end system. It questions the conclusion drawn about the superiority of the direct model based on the small difference in performance. However, the comment does not specify which part of the paper this issue is related to, such as a particular section, table, or figure. Without explicit references, the authors may struggle to pinpoint where this discrepancy is discussed or how to address it. Therefore, the comment is weakly grounded but specific in its critique, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point questions the conclusion about the superiority of the direct model over the text disambiguation model by highlighting the discrepancy in the amount of training data used for each. This claim is somewhat verifiable as it provides a logical reasoning based on the data difference, which could indeed affect the performance comparison. However, the comment lacks specific data points or references to substantiate the claim fully, leaving room for interpretation and requiring the authors to infer the implications. Therefore, the comment is classified as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 2.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises a critical point about the discrepancy in the amount of data used to train the text disambiguation model compared to the end-to-end system. It questions the validity of the conclusion that the direct model is clearly superior, given the small difference in performance between the two proposed systems. This feedback prompts the authors to reconsider their conclusions and possibly explore the impact of data size on model performance. However, the comment does not provide specific suggestions or guidance on how to address this issue or improve the analysis. While it identifies an important area for consideration, the lack of actionable feedback limits its usefulness. Therefore, the comment is somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.\n",
      "Paper ID: NIPS_2017_645\n",
      "Venue: NIPS_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment points out that the authors have not sufficiently discussed the computational aspects of their methods, particularly in the context of high-dimensional data. It highlights the challenge of solving several linear programs (LPs) in high dimensions and notes that the parameter involved is not easily calculable. The comment implies that the authors should address these computational concerns and provide more detailed discussion or analysis. However, it does not offer specific guidance on how to address these issues or what aspects to focus on. The action is implicit and somewhat vague, as the authors can infer that they need to expand on the computational feasibility of their methods but may not know exactly how to do so. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment highlights a concern about the lack of detailed discussion on computational aspects, specifically regarding the practicality of the proposed methods for high-dimensional data. It mentions that the algorithm requires solving several linear programs (LPs) in high dimensions, which involves a parameter that is not easily calculable. The comment also notes that the experiments are conducted on small-scale datasets, implying a limitation in the study's scope. While the comment does not explicitly mention specific sections, the authors can infer that it pertains to the methodology and experimental sections. The comment is specific in detailing what needs to be addressed regarding the computational feasibility of the methods. However, since it does not explicitly mention the sections, it is weakly grounded but specific. Therefore, this comment aligns with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point raises a claim about the lack of detailed discussion on computational aspects, specifically questioning the practicality of the proposed methods for high-dimensional data. The reviewer supports this claim by pointing out that the algorithm involves solving several LPs in high dimensions with parameters that are not easily calculable. Additionally, the reviewer notes that the experiments are conducted on small-scale datasets, which suggests limitations in scalability. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, the comment could be strengthened by providing specific examples or references to similar studies that address computational challenges in high dimensions. Therefore, the comment is categorized as 4: Mostly Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment highlights a significant concern regarding the computational feasibility of the proposed methods, particularly in high-dimensional settings. It points out that the authors' algorithm involves solving several linear programs (LPs) in high dimensions, which may not be practical due to the complexity of the parameter calculations. The comment also notes that the experiments are conducted on small-scale datasets, which may not adequately demonstrate the method's scalability. While the comment identifies a critical area for improvement, it lacks specific suggestions or actionable steps for the authors to address the computational challenges. Providing more detailed guidance or examples on how to enhance the scalability of the methods would make the feedback more helpful. As it stands, the comment is somewhat helpful, as it alerts the authors to a potential limitation but does not offer comprehensive advice on how to overcome it.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The experiments are not sufficient. More empirical experiments or toy experiments (for the simplified self-attention model considered in the theoretical analysis) need to be done to show the validity of the model relaxations and the consistence of the theoretical analysis with empirical results, besides citing the result in Kaplan et al. 2020.\n",
      "Paper ID: NIPS_2020_1897\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment explicitly states that the experiments are insufficient and suggests that more empirical or toy experiments are needed. It provides a clear action for the authors to take by recommending additional experiments to demonstrate the validity of the model relaxations and the consistency of the theoretical analysis with empirical results. However, while the suggestion is explicit, it lacks specific guidance on what kind of experiments should be conducted or how to design them. The authors are given a general direction but without detailed instructions on how to implement the suggested improvements. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the need for additional empirical or toy experiments to validate the model relaxations and ensure consistency between theoretical analysis and empirical results. However, it does not specify which part of the paper currently lacks sufficient experimentation or where these additional experiments should be included. The authors can infer that it relates to the experimental or results sections, but this inference is not direct. The comment is specific in suggesting what needs to be done but lacks grounding as it does not pinpoint the exact part of the paper being addressed. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the experiments conducted are insufficient and suggests that additional empirical or toy experiments are necessary to demonstrate the validity of the model relaxations and the consistency of the theoretical analysis with empirical results. While the comment provides a general rationale for the need for more experiments, it lacks specific examples or detailed reasoning to fully substantiate the claim. The mention of Kaplan et al. 2020 is not elaborated upon, leaving the authors to infer the relevance of this citation. As a result, the claim is somewhat verifiable, as it provides a direction for improvement but requires more detailed justification to be fully actionable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a significant gap in the experimental section of the paper, suggesting that more empirical or toy experiments are needed to validate the model relaxations and ensure consistency between theoretical analysis and empirical results. This feedback is actionable as it provides a clear direction for the authors to enhance their work by expanding their experimental validation. However, the comment could be more helpful if it included specific suggestions on the types of experiments to conduct or how to design them to effectively address the identified gaps. Despite this, the comment is mostly helpful as it guides the authors on a critical aspect of their research that requires improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:\n",
      "Paper ID: ACL_2017_31_review\n",
      "Venue: ACL_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point identifies an issue with the organization of Section 3.2, noting that the explanations for features are intertwined and confusing. It suggests that the section would benefit from a more coherent organization with separate paragraphs dedicated to lexical features and sentence-level features. However, the comment does not provide explicit instructions on how to achieve this reorganization or what specific changes should be made to improve clarity. While the action is implied and somewhat clear, it lacks concrete details on implementation. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment explicitly mentions \"Section 3.2,\" providing full grounding by directing the authors to the specific part of the paper being addressed. It identifies the issue of intertwined explanations for features, suggesting that the section would benefit from a more coherent organization. However, the comment does not specify what aspects of the explanations are confusing or how exactly the organization should be improved, making it under-specific. Therefore, this comment is fully grounded and under-specific, corresponding to a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 2\n",
      "Rationale: The review point claims that the explanations for features in Section 3.2 are \"somewhat intertwined and thus confusing,\" suggesting that the section would benefit from a more organized structure with separate paragraphs for lexical and sentence-level features. However, the comment does not provide specific examples or detailed reasoning to support why the current organization is confusing or how the proposed restructuring would improve clarity. This lack of explicit justification makes it difficult for the authors to fully understand and address the critique. Therefore, the claim is considered borderline verifiable, as it offers some insight but lacks sufficient detail to be fully actionable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a specific issue with the organization of Section 3.2, noting that the explanations for features are intertwined and confusing. It suggests that the section would benefit from a more coherent organization, with separate paragraphs dedicated to lexical features and sentence-level features. While the comment provides a clear direction for improvement, it stops short of offering detailed guidance or examples on how to implement this restructuring. The feedback is mostly helpful as it pinpoints a critical area needing attention and provides a general suggestion for improvement, but it could be more comprehensive with additional guidance on how to achieve the proposed changes.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The novelty of using PCA to reduce interaction count seems incremental and the significance of the paper results is unclear to me. Using PCA to reduce the interaction count seems intuitive, as PCA aims to retain the maximum information in the data with the reduced dimensionality chosen, assuming certain assumptions are met. How well are the assumptions met? [1] Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert Müller, and Pan Kessel. \"Towards robust explanations for deep neural networks.\" Pattern Recognition 121 (2022): 108194.\n",
      "Paper ID: 34QscjTwOc\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment questions the novelty and significance of using PCA to reduce interaction count, suggesting that it seems intuitive. It also raises a question about how well the assumptions of PCA are met. However, the comment does not provide specific guidance on how the authors could address these concerns or improve their work. It lacks actionable details, such as recommending ways to demonstrate the novelty or significance of their approach or suggesting methods to verify the assumptions of PCA. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the novelty and significance of using PCA to reduce interaction count, questioning the incremental nature of this approach and the clarity of the paper's results. It also raises a specific question about how well the assumptions of PCA are met, referencing an external work for context. However, the comment does not explicitly mention a specific section or part of the paper, making it weakly grounded. Despite this, it is specific in its critique of the PCA approach and the assumptions involved. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point questions the novelty and significance of using PCA to reduce interaction count, suggesting that it seems intuitive given PCA's purpose. The reviewer provides a logical reasoning for the claim by explaining PCA's goal of retaining maximum information with reduced dimensionality. However, the comment lacks specific examples or detailed analysis of how well the assumptions for PCA are met in this context. The reference to Dombrowski et al. (2022) is mentioned, but it is not directly tied to the claim about PCA's novelty or assumptions. This makes the claim somewhat verifiable, as it provides some reasoning but lacks comprehensive evidence or examples to fully substantiate the argument.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment questions the novelty of using PCA to reduce interaction count, suggesting that it seems incremental and intuitive. It raises a valid point about the assumptions underlying PCA and how well they are met in the context of the paper. However, the comment lacks specific guidance or suggestions on how the authors might address these concerns or demonstrate the significance of their results. While it identifies a potential weakness, it does not provide actionable feedback or detailed insights that could help the authors improve their draft. Therefore, the comment is somewhat helpful, as it highlights an area for consideration but lacks depth and specificity.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.\n",
      "Paper ID: ARR_2022_253_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment expresses curiosity about seeing related experiments where the information axis tool can be applied, as mentioned in the conclusion. While it implies that the authors should include such experiments, it does not explicitly instruct them to do so or provide guidance on what specific experiments should be conducted. The action is implicit and somewhat vague, as the authors can infer that they should add related experiments but may not know exactly what is expected. Therefore, the comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment suggests a curiosity about related experiments where the information axis tool could be applied, as mentioned in the conclusion. However, it does not specify which part of the paper should include these experiments or provide details on what kind of experiments would be relevant. This makes it weakly grounded, as the authors cannot confidently determine where to address this feedback. The comment is also not specific, as it lacks details on what needs to be demonstrated or how the information axis tool should be applied in experiments. Therefore, this comment is weakly grounded and not specific, aligning with label 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: X\n",
      "Rationale: The comment expresses curiosity about seeing related experiments where the information axis tool could be beneficial. While it mentions that the paper uses analysis to justify the tool, it does not make a subjective claim or suggestion that requires evidence or justification. It is more of a request for additional information or clarification, fitting the \"No\" label.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment acknowledges the paper's use of analysis to justify the information axis as a useful tool but expresses curiosity about seeing related experiments that demonstrate its practical application. While the comment highlights an area of interest, it lacks specific guidance or suggestions on how the authors might incorporate such experiments or what types of experiments would be most beneficial. This limits the comment's utility, as it does not provide actionable feedback or a clear path for the authors to enhance their draft. Therefore, the comment is somewhat helpful, as it points out an area for potential improvement but lacks depth and specificity.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The presentation of the simulation study is not really doing a favor to the authors. Specifically, the authors do not really comment on why the GPC (benchmark) is performing better than BPC (their method). It would be worth re-iterating that this is b/c of the bandit feedback and not using information about the form of the cost function.\n",
      "Paper ID: NIPS_2020_902\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment points out a shortcoming in the presentation of the simulation study, specifically noting that the authors do not comment on why the GPC (benchmark) is performing better than BPC (their method). The reviewer suggests that it would be beneficial to reiterate that this performance difference is due to the bandit feedback and not using information about the form of the cost function. While the comment implies that the authors should provide an explanation for the observed performance difference, it does not explicitly instruct them to do so. The action is inferred but concrete, as the authors can deduce that they need to add a discussion on this point. Therefore, this comment is mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the presentation of the simulation study and specifically mentions the need for the authors to comment on why the GPC (benchmark) is performing better than BPC (their method). It provides a specific suggestion for improvement by indicating that the authors should reiterate that the performance difference is due to the bandit feedback and not using information about the form of the cost function. However, the comment does not explicitly mention which part of the paper this issue is discussed in, making it weakly grounded. The authors can infer that it relates to the simulation study section, but this inference is not as direct as it could be. Therefore, the comment is weakly grounded but specific in suggesting a potential improvement, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point claims that the presentation of the simulation study is lacking because the authors do not comment on why the GPC benchmark is performing better than their method, BPC. The reviewer suggests that the reason for this could be due to bandit feedback and not using information about the form of the cost function. However, the comment does not provide any specific evidence, references, or detailed reasoning to support this claim. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment identifies a specific issue with the presentation of the simulation study, noting that the authors do not explain why the GPC benchmark performs better than their method, BPC. It suggests that the authors should reiterate that this performance difference is due to the bandit feedback and the lack of information about the cost function's form. This feedback is clear and actionable, providing the authors with a specific area to address in their draft. By incorporating this explanation, the authors can improve the clarity and depth of their analysis. However, the comment could be more helpful if it offered additional guidance on how to effectively integrate this explanation into the study. Overall, the feedback is mostly helpful, as it directs the authors toward a meaningful enhancement of their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- The simple/traditional experiment for unseen characters is a nice idea, but is presented as an afterthought. I would have liked to see more eval in this direction, i.e. on classifying unseen words - Maybe add translations to Figure 6, for people who do not speak Chinese?\n",
      "Paper ID: ACL_2017_543_review\n",
      "Venue: ACL_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review point consists of two parts. The first part suggests that the experiment for unseen characters is a good idea but is presented as an afterthought, and the reviewer would like to see more evaluation in this direction, specifically on classifying unseen words. This part implies an action but does not explicitly instruct the authors on how to expand the evaluation, making it somewhat actionable. The second part explicitly suggests adding translations to Figure 6 for non-Chinese speakers, providing a clear and concrete action for the authors to take. Overall, the comment contains both explicit and implicit actions, with varying levels of concreteness, making it mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment addresses two distinct points. First, it mentions the experiment for unseen characters, suggesting that it is presented as an afterthought and could benefit from more evaluation. While it does not explicitly mention a specific section, the authors can infer that it relates to the experimental results or discussion sections. Second, it suggests adding translations to Figure 6 for non-Chinese speakers, which is fully grounded as it explicitly mentions Figure 6. The comment is specific in its suggestions for improvement, particularly regarding the need for additional evaluation and translations. Therefore, the comment is weakly grounded but specific for the first part and fully grounded and specific for the second part, aligning with a score of 4 overall due to the mixed grounding.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point suggests that the experiment for unseen characters is underemphasized and could benefit from further evaluation, particularly in classifying unseen words. Additionally, it recommends adding translations to Figure 6 for non-Chinese speakers. While the comment provides a logical suggestion for improvement, it lacks specific examples or references to support the claim that more evaluation is needed. The suggestion to add translations is practical but not backed by evidence of its necessity. Therefore, the claim is somewhat verifiable, as it provides a reasonable direction for improvement but lacks detailed justification or evidence, aligning with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment provides a mix of feedback, identifying both a positive aspect and an area for improvement. It acknowledges the value of the experiment for unseen characters but notes that it is presented as an afterthought. The suggestion to expand evaluation in this direction, particularly on classifying unseen words, is actionable and could significantly enhance the paper's contribution. Additionally, the recommendation to add translations to Figure 6 for non-Chinese speakers is a specific and practical suggestion that would improve accessibility and understanding for a broader audience. While the comment could be more detailed in guiding how to expand the evaluation, it still offers clear and constructive feedback that can help the authors improve their draft. Therefore, the comment is mostly helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- There seems to be forward referencing in the paper. Material is introduced without proper explanation, and is explained in later sections e.g. Figure1 - The exact contribution(s) need to be written more clearly in the Introduction. Moreover, the material supporting the main contributions seems to be in the appendix and not the main sections e.g. deep-rag algorithm or discussion on the high concurrency.\n",
      "Paper ID: kNvwWXp6xD\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review point identifies several issues with the paper's structure and clarity. It notes that there is forward referencing, where material is introduced without proper explanation and is only explained in later sections. It also suggests that the exact contributions need to be more clearly stated in the Introduction and that supporting material for the main contributions is misplaced in the appendix rather than the main sections. While the comment highlights specific areas for improvement, it does not provide explicit guidance on how to address these issues. The authors are left to infer that they need to reorganize the content for better clarity and structure. The actions are implicit and somewhat vague, as the comment does not specify how to restructure the paper or which sections need to be moved. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment addresses multiple issues within the paper, such as forward referencing, unclear contributions in the Introduction, and important material being placed in the appendix rather than the main sections. It specifically mentions Figure 1 and the deep-rag algorithm, which helps the authors identify the relevant parts of the paper. However, while the comment is fully grounded due to these specific references, it lacks specificity in detailing how the issues should be addressed or improved. Therefore, this comment is fully grounded but under-specific, aligning with category 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point claims that there is forward referencing in the paper, where material is introduced without proper explanation and is only explained in later sections. It provides an example with Figure 1 and suggests that the contributions need to be more clearly stated in the Introduction. Additionally, it mentions that supporting material is in the appendix rather than the main sections. While the comment identifies potential structural issues, it lacks specific examples or detailed reasoning to fully substantiate the claim. This makes it somewhat verifiable, as the authors may need more guidance to address the issues effectively.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies two specific issues with the paper: forward referencing and unclear presentation of contributions. It points out that material is introduced without proper explanation and is only explained in later sections, which can confuse readers. Additionally, it notes that the exact contributions are not clearly stated in the Introduction, and important supporting material is relegated to the appendix rather than being included in the main sections. This feedback is clear and actionable, providing the authors with specific areas to address to improve the clarity and structure of their paper. However, the comment could be more helpful if it offered suggestions on how to reorganize the content or examples of how to clearly state contributions. Overall, the feedback is mostly helpful, as it effectively directs the authors toward meaningful enhancements of their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.\n",
      "Paper ID: eI6ajU2esa\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment suggests that the paper should include a comparison with test-time adaptation (TTA) methods, specifically mentioning methods [A-B]. It highlights the need to demonstrate whether data processing is superior to model parameter adjustment. The comment explicitly states that a comparison should be made based on experimental results, providing a clear action for the authors to take. However, it does not provide specific guidance on how to conduct this comparison or what aspects to focus on, leaving some details to be inferred by the authors. While the action is explicit, the lack of detailed instructions makes it mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is specific in its suggestion to include a comparison with test-time adaptation (TTA) methods, providing a clear direction for improvement. It mentions that TTA methods focus on adapting to out-of-distribution data and suggests that the authors should prove the superiority of data processing over model parameter adjustment through experimental results. However, the comment does not explicitly mention which part of the paper should include this comparison, making it weakly grounded. The authors can infer that it relates to the evaluation or results section, but this inference is not as direct as it could be. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the paper lacks a comparison with test-time adaptation (TTA) methods, which also address robustness in video action recognition. The reviewer suggests that a comparison is necessary to demonstrate whether the paper's focus on adjusting input data is superior to the TTA methods' focus on updating model parameters. While the comment highlights a potential gap in the paper, it does not provide specific examples or references to TTA methods, making it somewhat verifiable. The authors would benefit from more detailed guidance on which TTA methods to consider for comparison, but the comment does provide a logical basis for the claim.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a significant gap in the paper by pointing out the lack of comparison with test-time adaptation (TTA) methods, which are relevant to the issue of robustness in video action recognition. It highlights the difference in focus between the paper's approach (adjusting input data) and TTA methods (updating model parameters) and suggests that a comparison based on experimental results would be beneficial. This feedback is mostly helpful as it provides a clear direction for improvement by recommending a specific area for comparison that could strengthen the paper's argument. However, it could be more comprehensive by offering suggestions on how to conduct the comparison or what specific aspects to focus on. Overall, the comment provides valuable insights that can guide the authors in refining their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- This sentence is confusing [93-95] \"After we have trained the model for task t, we memorize each newly added filter by the shape of every layer to prevent the caused semantic drift.\" I believe I understood it after re-reading it and the subsequent sentences but it is not immediately obvious what is meant.\n",
      "Paper ID: NIPS_2018_83\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific sentence that is confusing and suggests that it is not immediately clear what is meant. However, it does not provide explicit guidance on how to clarify the sentence or offer suggestions for rephrasing it. While the comment highlights an area that needs improvement, it lacks concrete instructions or examples to help the authors address the issue. As a result, the authors are left to infer that they need to rewrite the sentence for clarity, but they do not have specific guidance on how to do so. Therefore, the comment is somewhat actionable, as it points out a problem but does not provide detailed guidance on how to resolve it.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions the sentence located in lines 93-95, allowing the authors to accurately identify the specific part of the paper being addressed. It also specifies the issue by stating that the sentence is confusing and not immediately clear, even though the reviewer believes they understood it after re-reading. However, the comment does not provide specific suggestions for how to clarify the sentence, making it under-specific. Therefore, this comment is fully grounded and under-specific, aligning with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point claims that the sentence is confusing, specifically pointing to lines 93-95. The reviewer mentions that they understood the sentence after re-reading it and the subsequent sentences, but it was not immediately clear. However, the comment does not provide any specific reasoning or examples to support why the sentence is confusing or how it could be improved. This lack of detailed explanation makes the claim unverifiable, as it does not offer the authors concrete guidance on how to address the issue.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a specific sentence in the manuscript that is confusing and difficult to understand upon first reading. While it points out an area that needs clarification, it does not provide suggestions on how to rephrase or improve the sentence for better clarity. The feedback is somewhat helpful as it alerts the authors to a potential issue with the readability of their text, but it lacks actionable guidance on how to resolve the confusion. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Typically, expected performance under observation noise is used for evaluation because the decision-maker is interested in the true objective function and the noise is assumed to be noise (misleading, not representative). In the formulation in this paper, the decision maker does care about the noise; rather the objective function of interest is the stochastic noisy function. It would be good to make this distinction clearer upfront.\n",
      "Paper ID: NIPS_2021_1251\n",
      "Venue: NIPS_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point suggests that the authors should clarify the distinction between the typical use of expected performance under observation noise and the specific formulation in their paper, where the decision maker cares about the noise. While the comment implies that this distinction should be made clearer upfront, it does not provide explicit instructions on how to achieve this clarity or where exactly in the paper this clarification should be added. The action is implicit and somewhat vague, as the authors need to infer the specific changes required to address the feedback. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests clarifying the distinction between the typical use of expected performance under observation noise and the specific formulation used in the paper, where the decision-maker cares about the noise. However, it does not specify which part of the paper should include this clarification, making it weakly grounded. The comment is specific in detailing what needs to be clarified—the distinction between the typical evaluation approach and the paper's formulation. Therefore, the comment is weakly grounded but specific, corresponding to label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point makes a claim about the evaluation approach used in the paper, suggesting that the distinction between evaluating under observation noise and focusing on the stochastic noisy function should be made clearer. The comment provides logical reasoning by contrasting typical evaluation practices with the paper's approach, highlighting the importance of clarifying this distinction. However, it lacks specific examples or references to support the claim further. This makes the comment somewhat verifiable, as it provides a reasonable basis for the claim but requires additional detail to be fully substantiated.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a key conceptual distinction that the authors need to clarify in their paper. It points out that, unlike typical evaluations where noise is considered misleading, the paper's formulation treats the noise as part of the objective function. This feedback is actionable as it suggests that the authors should make this distinction clearer upfront, which could significantly enhance the reader's understanding of the paper's approach and contributions. However, the comment could be more helpful if it provided specific suggestions on how to effectively incorporate this clarification into the paper. Overall, the comment is mostly helpful as it directs the authors toward a meaningful improvement in their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Weak supervision could be better evaluated - eg, how realistic are the evaluated tweets? The prompt requires \"all of the structured elements for perspectives to be present in the generated tweets\", which doesn't see the most realistic. The generation of authors is also not realistic (\"[author] embeddings are initialized by averaging the corresponding artificial tweets\").\n",
      "Paper ID: MMrqu8SD6y\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that the evaluation of weak supervision could be improved by assessing the realism of the evaluated tweets. It points out specific issues, such as the requirement for all structured elements to be present in the generated tweets and the unrealistic generation of authors. However, the comment does not provide explicit instructions or concrete steps on how to enhance the evaluation or address these issues. The authors can infer that they need to improve the realism of the evaluation, but the lack of detailed guidance makes the action somewhat vague. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the evaluation of weak supervision, specifically questioning the realism of the evaluated tweets and the generation of authors. However, it does not explicitly mention which part of the paper these issues are discussed in, making it weakly grounded. The comment is specific in pointing out the unrealistic aspects of the evaluated tweets and the generation of authors, providing clear guidance on what needs to be addressed. Therefore, the comment is weakly grounded but specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 2\n",
      "Rationale: The review point raises concerns about the evaluation of weak supervision, specifically questioning the realism of the evaluated tweets and the generation of authors. However, the comment lacks specific examples, detailed reasoning, or references to substantiate these claims. Without concrete evidence or a more thorough explanation, the authors may find it challenging to understand and address the issues raised. Therefore, the comment is considered borderline verifiable, as it provides some insight but lacks sufficient justification to fully support the claims.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a potential issue with the evaluation of weak supervision, specifically questioning the realism of the evaluated tweets and the prompt requirements. It also points out a concern with the generation of authors, noting that the method for initializing embeddings may not be realistic. While the comment raises valid points, it lacks specific suggestions or guidance on how the authors might address these issues or improve the realism of their evaluations. The feedback provides some insight into areas for improvement but does not offer actionable steps or detailed recommendations, making it somewhat helpful. Therefore, it aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- While not familiar with the compared models DMM and DVBF in details, the reviewer understood from the paper their differences with KVAE. However, the reviewer would appreciate a little bit more detailed presentation of the compared models. Specifically, the KVAE is simpler as the state space transition are linear, but it requires the computation of the time-dependant LGSSM parameters \\gamma. Can the authors comment on the computation requirements of the 3 methods compared in Table 1 ?\n",
      "Paper ID: NIPS_2017_345\n",
      "Venue: NIPS_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment requests a more detailed presentation of the compared models, specifically asking for comments on the computation requirements of the three methods compared in Table 1. The reviewer explicitly mentions the need for additional details about the models DMM, DVBF, and KVAE, and highlights the linear state space transitions and time-dependent LGSSM parameters in KVAE. The request for comments on computation requirements is clear and direct, providing the authors with a specific action to take. However, the comment could be more actionable if it included suggestions on how to present these details or what specific aspects to focus on. Despite this, the action is explicit and provides a clear direction for improvement, making the comment mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is partially grounded as it references Table 1, allowing the authors to identify the section of the paper being discussed. However, it does not specify which part of the paper should include more detailed presentations of the compared models DMM and DVBF. The comment is specific in its request for more detailed presentations of the models and inquiries about the computation requirements of the methods compared in Table 1. Therefore, the comment is weakly grounded but specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: X\n",
      "Rationale: The review point primarily consists of a request for more detailed information and a question about the computation requirements of the methods compared in Table 1. It does not contain subjective opinions, judgments, or suggestions that require evidence or justification. Therefore, it is classified as a normal statement, fitting the \"No\" label.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment acknowledges the reviewer's limited familiarity with the compared models DMM and DVBF but indicates an understanding of their differences with KVAE. It requests a more detailed presentation of the compared models, specifically asking for comments on the computation requirements of the three methods compared in Table 1. This feedback is somewhat helpful as it highlights a need for more detailed explanations, which could improve the clarity and comprehensiveness of the paper. However, it could be more helpful if it provided specific suggestions on how to present the models or what aspects to focus on. Overall, the comment prompts the authors to enhance their draft by providing additional details, but it lacks actionable steps or guidance, making it somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Why does the method help on Hopper, which has deterministic dynamics, so given (s, a), there is a unique s', and in this case, it simply reduces to action-conditional masking? Can it be evaluated on some other domains with non-deterministic dynamics to evaluate its empirical efficacy? Otherwise empirically it seems like the method doesn't seem to have much benefit. Why is BEAR missing from baselines?\n",
      "Paper ID: NIPS_2020_106\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises questions about the applicability and benefits of the method on the Hopper domain, which has deterministic dynamics. It suggests evaluating the method on other domains with non-deterministic dynamics to assess its empirical efficacy. Additionally, it questions the absence of BEAR from the baselines. While the comment implies actions—such as testing the method on different domains and including BEAR in the baselines—it does not explicitly instruct the authors to do so. The actions are inferred and somewhat vague, as the reviewer does not provide specific guidance on how to implement these suggestions. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises questions about the method's effectiveness on the Hopper domain, which has deterministic dynamics, and suggests evaluating it on other domains with non-deterministic dynamics. It also questions the absence of BEAR from the baselines. While the comment provides specific concerns and suggestions, it does not explicitly mention which part of the paper these issues are related to, making it weakly grounded. However, the comment is specific in its critique and suggestions for further evaluation. Therefore, this comment is weakly grounded and specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point questions the effectiveness of the method on a specific task (Hopper) with deterministic dynamics and suggests evaluating it on domains with non-deterministic dynamics to better assess its empirical efficacy. It also questions the absence of BEAR from the baselines. While the comment raises valid points, it lacks detailed reasoning or references to support why these suggestions would provide a more accurate evaluation. The absence of specific examples or evidence makes the claim somewhat verifiable, as it requires the authors to infer the underlying reasoning and address the concerns. Therefore, the comment is categorized as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment is mostly helpful as it raises pertinent questions about the applicability and evaluation of the method. It questions the relevance of testing the method on Hopper, which has deterministic dynamics, and suggests evaluating it on domains with non-deterministic dynamics to better assess its empirical efficacy. This feedback provides a clear direction for the authors to consider alternative evaluation scenarios that could strengthen their work. Additionally, the comment points out the absence of BEAR from the baselines, prompting the authors to address this gap. While the comment is insightful and actionable, it could be more comprehensive by offering specific suggestions on how to conduct the evaluations or integrate BEAR into the baselines. Overall, the comment provides valuable guidance for improving the draft, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- Your setting is very specific: you need to know the model or/and have access to a generative model (for expanding or generating trajectories), the problem should be episodic and the reward should be given just at the end of a task (i.e., reaching the target goal). Can you extend this approach to more general settings?\n",
      "Paper ID: NIPS_2018_296\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment highlights the specificity of the current setting and poses a question about extending the approach to more general settings. While the question implies that the authors should consider broader applicability, it does not explicitly instruct them to do so or provide guidance on how to achieve this. The action is implicit and somewhat vague, as the authors may not be entirely sure of the exact steps to take to address the feedback. Given this, the comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the specificity of the setting described in the paper, highlighting the need for knowledge of the model, access to a generative model, episodic problems, and end-of-task rewards. It questions whether the approach can be extended to more general settings. However, the comment does not explicitly mention which part of the paper discusses these specific conditions, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where these topics are discussed, the comment lacks full grounding. It is specific about the limitations of the current setting but does not provide detailed guidance on how to address them. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point highlights the specificity of the setting required for the approach, mentioning the need to know the model, access to a generative model, episodic problems, and end-of-task rewards. However, it does not provide any supporting evidence, reasoning, or references to justify why these requirements are limiting or how they could be addressed. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a limitation in the specificity of the setting described in the paper, noting that it requires knowledge of the model, access to a generative model, an episodic problem structure, and end-of-task rewards. It poses a question about the possibility of extending the approach to more general settings, which is a relevant consideration for broadening the applicability of the research. However, the comment lacks specific suggestions or guidance on how the authors might achieve this extension or what aspects should be considered for generalization. While it raises an important point, the feedback is not fully actionable, making it somewhat helpful. Therefore, the comment aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.\n",
      "Paper ID: ICLR_2021_2674\n",
      "Venue: ICLR_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The review comment points out that the term \"hyper-spectral\" is confusing and provides a definition of hyperspectral imaging as obtaining the spectrum for each pixel in an image. However, it does not offer specific guidance on how the authors should address this confusion. The comment lacks explicit instructions or suggestions for clarifying the terminology or providing additional context. Without concrete recommendations, the authors may be uncertain about how to resolve the issue. Therefore, the comment is unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 1.0\n",
      "Rationale: The comment addresses the use of the term \"hyper-spectral\" and suggests that it is confusing. However, it does not specify where in the paper this term is used, leaving the authors unable to pinpoint the exact location of the issue. Additionally, while the comment provides a definition of hyperspectral imaging, it does not clearly specify what needs to be changed or improved regarding the use of the term in the paper. As a result, the comment is not grounded because it does not identify a specific part of the paper, and it is not specific because it lacks detailed guidance on what needs to be addressed. Therefore, this comment aligns with a score of 1.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point claims that using the term 'hyper-spectral' is confusing and provides a definition of hyperspectral imaging as obtaining the spectrum for each pixel in an image. This explanation offers a clear and precise reasoning for why the term might be misused or misunderstood in the context of the paper. However, the comment could be strengthened by providing specific examples or references to standard definitions or uses of hyperspectral imaging in the literature. As it stands, the claim is mostly verifiable, as it provides a logical basis for the confusion but lacks additional supporting evidence or references. Therefore, the comment aligns with a score of 4: Mostly Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment identifies a potential issue with terminology, specifically the use of the term \"hyper-spectral,\" which could be confusing due to its established definition in the context of imaging techniques. However, the comment does not provide any suggestions for alternative terminology or guidance on how to clarify this point in the paper. While it highlights a potential source of confusion, it lacks actionable feedback that would help the authors address the issue effectively. Therefore, the comment is barely helpful, aligning with a score of 2.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- is this issue solved in the proposed knowledge-CLIP model? Some existing work that combines text and KG (e.g. https://arxiv.org/abs/2104.06378) has done closely-related analyses such as adding negation or changing entities in text to see if the KG-augmented method can robustly handle them. It would be very interesting if the authors perform such analysis on the proposed knowledge-CLIP model that combines image, text and KGs.\n",
      "Paper ID: NIPS_2022_765\n",
      "Venue: NIPS_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises a question about whether the proposed knowledge-CLIP model addresses a specific issue and references existing work that has conducted related analyses. It suggests that performing a similar analysis on the knowledge-CLIP model would be interesting. While the comment implies that the authors should conduct an analysis similar to the one mentioned, it does not explicitly instruct them to do so. The action is inferred and somewhat vague, as it does not provide detailed guidance on how to perform the analysis or what specific aspects to focus on. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises a question about whether a specific issue is addressed in the proposed knowledge-CLIP model. It references existing work that combines text and knowledge graphs (KG) and suggests performing similar analyses, such as adding negation or changing entities in text, to test the robustness of the KG-augmented method. However, the comment does not explicitly mention which part of the paper this issue is discussed in, making it weakly grounded. It does provide specific suggestions for analysis, which adds some level of specificity. Therefore, the comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point raises a question about whether the proposed knowledge-CLIP model addresses a specific issue, suggesting that existing work has conducted related analyses. It references a specific study (https://arxiv.org/abs/2104.06378) that has performed analyses like adding negation or changing entities to test robustness in KG-augmented methods. This reference provides a basis for the suggestion, making the claim mostly verifiable. However, the comment could be strengthened by explicitly stating what the issue is and how the referenced work relates to it. Therefore, the comment is rated as 4: Mostly Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment raises an interesting point about the potential for further analysis of the proposed knowledge-CLIP model. It references existing work that has conducted similar analyses, such as adding negation or changing entities in text, to test the robustness of KG-augmented methods. By suggesting that the authors perform a similar analysis on their model, the comment provides a clear and actionable recommendation that could enhance the paper's depth and robustness. However, it could be more helpful if it included specific guidance on how to conduct these analyses or what specific insights the authors should aim to uncover. Overall, the feedback is mostly helpful as it offers a constructive suggestion for improvement, but it could be more comprehensive with additional details.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- while it is interesting to see the grounding of the proposed method in neuroscience, some of the general ideas are already present in other methods for exploration, in particular, reasoning topologically is captured by methods that use the generalized Voronoi graph or semantic maps to guide the exploration, and the long-term storage through pose graphs in SLAM, where loop closure is applied (discussed in graph-based slam appendix section), or curiosity-driven exploration. The paper should discuss the proposed method with respect to such methods.\n",
      "Paper ID: kYXZ4FT2b3\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point suggests that the paper should discuss the proposed method in relation to existing methods for exploration, such as those using generalized Voronoi graphs, semantic maps, pose graphs in SLAM, and curiosity-driven exploration. While the comment provides a clear direction for the authors to take—comparing their method with these existing approaches—it does not offer specific guidance on how to conduct this comparison or what aspects should be highlighted. The action is explicit but lacks concrete details on implementation, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment discusses the grounding of the proposed method in neuroscience and compares it to existing methods for exploration, such as those using generalized Voronoi graphs, semantic maps, and SLAM techniques. It suggests that the paper should discuss the proposed method in relation to these existing methods. However, the comment does not specify which part of the paper should include this discussion or where the comparison should be made, making it weakly grounded. The comment is specific in terms of what it suggests—discussing the proposed method in relation to other methods—but lacks grounding in terms of pinpointing the exact section or part of the paper. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that some general ideas in the proposed method are already present in other exploration methods, such as those using generalized Voronoi graphs, semantic maps, and SLAM techniques. The comment provides specific examples of related methods, such as graph-based SLAM and curiosity-driven exploration, which support the claim. However, it lacks direct references to external works or detailed comparisons that would fully substantiate the claim. While the comment offers a reasonable basis for the claim, the absence of specific references or detailed evidence makes it somewhat verifiable. The authors would benefit from additional information to fully understand and address the critique, leading to a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment acknowledges the interesting aspect of grounding the proposed method in neuroscience but points out that some general ideas are already present in other exploration methods. It specifically mentions methods like the generalized Voronoi graph, semantic maps, and SLAM with loop closure, suggesting that the paper should discuss its method in relation to these existing approaches. While the comment identifies a gap in the discussion and provides specific examples of related methods, it does not offer detailed guidance on how to incorporate this discussion or improve the paper's contribution. The feedback is somewhat helpful as it highlights an area for improvement and provides a direction for expanding the discussion, but it could be more beneficial with additional suggestions on how to integrate these comparisons effectively. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "- you have listed some of the limitations of evolutionary methods, but I think there are much deeper things to say regarding leveraging state, reactiveness, and learning during an episode. Being honest and direct would work well for this work - the title is way to generic and vague - be precise when being critical. What does \"brittle convergence properties mean\" - I would say DeepRL methods are widely adopted. Consider the landscape 10 years ago.\n",
      "Paper ID: NIPS_2018_109\n",
      "Venue: NIPS_2018\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment touches on several points, including the need for a deeper discussion on the limitations of evolutionary methods, a critique of the paper's title, and a request for clarification on the term \"brittle convergence properties.\" While the comment suggests that the authors should be more precise and direct in their critique, it does not provide specific guidance on how to achieve this. The suggestion to consider the landscape 10 years ago is also vague and lacks concrete steps for the authors to take. Overall, the comment implies actions but does not provide explicit or detailed instructions, making it somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The comment addresses multiple aspects of the paper, including the limitations of evolutionary methods, the title's generic nature, and the term \"brittle convergence properties.\" However, it does not specify which sections or parts of the paper these issues pertain to, making it difficult for the authors to pinpoint the exact areas needing revision. The comment is somewhat specific in suggesting that the authors should be more precise and direct, but it lacks detailed guidance on how to achieve this. Overall, the comment is weakly grounded because it does not clearly identify specific parts of the paper, and it is not specific enough to provide actionable feedback. Therefore, this comment aligns with label 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point makes several claims, including the need for a deeper discussion on leveraging state, reactiveness, and learning in evolutionary methods, and critiques the title's vagueness and the use of the term \"brittle convergence properties.\" However, the comment lacks specific examples or references to support these claims. The suggestion to be more precise and the assertion about the adoption of DeepRL methods are not substantiated with evidence or detailed reasoning. As a result, the comment is somewhat verifiable, as it provides some direction but lacks the necessary detail for the authors to fully understand and address the critique. Therefore, it aligns with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment addresses several points that could help improve the draft. It suggests that the authors delve deeper into the limitations of evolutionary methods, specifically regarding leveraging state, reactiveness, and learning during an episode. This feedback encourages the authors to provide a more thorough analysis, which could enhance the paper's depth. The comment also critiques the title as being too generic and vague, advising the authors to be more precise, which is actionable feedback for improving clarity and focus. Additionally, it questions the meaning of \"brittle convergence properties,\" prompting the authors to clarify their terminology. Lastly, it provides a perspective on the adoption of DeepRL methods, encouraging the authors to consider historical context. While the comment covers several important areas, it could be more helpful by offering specific examples or suggestions on how to address these issues. Overall, the comment is mostly helpful as it identifies key areas for improvement and provides some direction, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "--- W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.\n",
      "Paper ID: ICLR_2023_2322\n",
      "Venue: ICLR_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment highlights an issue with the formatting of the paper, specifically mentioning that whitespace has been reduced excessively, leading to equations being crammed together and captions being too close to figures. The reviewer states that this formatting issue is grounds for rejection as it violates the 9-page paper limit. However, the comment does not provide explicit guidance on how to address the issue, such as suggesting specific formatting changes or adjustments to improve the layout. While the authors can infer that they need to adjust the formatting to comply with the page limit, the lack of concrete instructions makes the action implicit and somewhat vague. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 4.0\n",
      "Rationale: The comment is fully grounded as it specifically addresses the issue of reduced whitespace throughout the paper, mentioning that equations are crammed together and captions are too close to the figures. This allows the authors to accurately identify the parts of the paper being addressed. However, while it highlights the problem of whitespace reduction, it does not provide specific guidance on how to address this issue, such as suggesting where to add more space or how to adjust the layout. Therefore, the comment is fully grounded but under-specific, aligning with category 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The comment claims that the reduction of whitespace in the paper results in crammed equations and captions, which violates the 9-page paper limit. However, the comment does not provide specific examples or detailed reasoning to support this claim. It lacks references to specific sections or figures where this issue is evident, making it difficult for the authors to verify and address the concern. As a result, the comment is categorized as 1: Unverifiable, as it lacks sufficient evidence or justification to support the claim.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies an issue with the formatting of the paper, specifically noting that whitespace has been excessively reduced, leading to cramped equations and captions. This is a clear and actionable piece of feedback, as it points out a violation of the paper's formatting guidelines, which could be grounds for rejection. By addressing this issue, the authors can ensure compliance with submission standards and improve the readability of their paper. However, the comment could be more helpful if it provided specific suggestions on how to adjust the formatting to meet the guidelines. Overall, the feedback is mostly helpful as it highlights a critical issue that needs to be addressed, but it could be more comprehensive with additional guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.\n",
      "Paper ID: ARR_2022_14_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises questions about the novelty and context of the proposed idea in the realm of long document summarization. It implicitly suggests that the authors should include a Related Work section and compare their system to previous extract-then-generate methodologies. However, the comment does not provide explicit instructions or concrete steps on how to address these issues, such as which specific methodologies to compare against or how to structure the Related Work section. While the need for additional context and comparison is clear, the lack of detailed guidance makes the action somewhat vague. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises questions about the novelty and comparative advantage of the proposed methodology for long document summarization. It specifically mentions the absence of a \"Related Work\" section and the lack of experiments comparing the proposed model with other extract-then-generate methodologies. However, it does not explicitly mention which part of the paper discusses the methodology, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might infer that it relates to the methodology or results sections, the comment lacks full grounding. It is specific about the issues of missing related work and comparative experiments but does not provide detailed guidance on how to address them. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review comment questions the novelty and contribution of the paper's approach to long document summarization by asking what others have done in this area with similar methodologies and what the system offers over previous extract-then-generate methodologies. The reviewer highlights the absence of a Related Work section and the lack of experiments comparing the proposed model with other extract-then-generate approaches. This critique is somewhat verifiable as it raises valid concerns about the paper's contextualization within existing literature and methodologies. However, it lacks specific references or examples of similar methodologies, which would strengthen the claim. Therefore, the comment is rated as \"3: Somewhat Verifiable.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises important questions about the novelty and contribution of the paper in the context of long document summarization. It points out the absence of a Related Work section, which is crucial for situating the paper within the existing literature and understanding what others have done with similar methodologies. The comment also questions what the proposed system offers over previous extract-then-generate methodologies, highlighting a potential gap in the paper's discussion. While the comment identifies significant areas for improvement, it lacks specific suggestions or guidance on how the authors might address these issues, such as recommending specific works to cite or methodologies to compare against. This limits the comment's utility, making it somewhat helpful as it provides a direction for improvement but lacks depth and actionable advice.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification\n",
      "Paper ID: NIPS_2017_65\n",
      "Venue: NIPS_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 1.0 | GPT Label: 2.0\n",
      "Rationale: The review point criticizes the evaluation by stating that the baselines used are not designed for fair classification. However, it does not provide explicit guidance on how the authors should address this issue. The comment lacks specific suggestions for alternative baselines or methods to improve the evaluation. As a result, the authors are left with an implicit understanding that they need to improve the evaluation but without concrete steps on how to do so. This makes the comment barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The comment critiques the evaluation section, specifically pointing out that the baselines used are not designed for fair classification. However, it does not specify which part of the paper this assessment is based on, nor does it provide details on what aspects of the evaluation are considered weak or how the baselines are inappropriate. Without explicit references to sections, figures, or specific elements of the paper, the authors cannot confidently determine which parts need attention or improvement. The lack of specificity and grounding makes it difficult for the authors to address the feedback effectively. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 2 | GPT Label: 1\n",
      "Rationale: The review point claims that \"the evaluation is weak\" because \"the baselines used in the paper are not even designed for fair classification.\" However, the comment does not provide any supporting evidence, specific examples, or references to substantiate this claim. Without additional context or explanation, it is difficult for the authors to understand the basis of the critique or how to address it. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 1.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a significant weakness in the paper's evaluation, specifically pointing out that the baselines used are not designed for fair classification. This is a critical issue that could undermine the validity of the paper's findings. However, the comment lacks depth and actionable suggestions for improvement. It does not provide guidance on how to address this issue, such as recommending more appropriate baselines or suggesting ways to improve the evaluation methodology. Without specific advice or examples, the authors are informed of the problem but are not supported in making the necessary improvements. This limits the utility of the feedback, making it somewhat helpful as it highlights an important area for improvement but does not fully support the authors in addressing it.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. I am not completely convinced by the experimental strengths of this approach. To run the proposed algorithm, the authors need to run a descent procedure for 40 different networks from the training phase. In contrast, you could simply run vanilla Adam on the final network with 40 random initial points, and one of these restarts would reach the global minimum. It is not important that EACH initialization reach the global minimum, as long as AT LEAST one initialization reaches the global minimum.\n",
      "Paper ID: NIPS_2019_1346\n",
      "Venue: NIPS_2019\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 1.0 | GPT Label: 5.0\n",
      "Rationale: The review comment questions the experimental strengths of the approach and suggests an alternative method: using vanilla Adam with 40 random initial points instead of running a descent procedure for 40 different networks. This provides a clear and explicit action for the authors to consider, offering a concrete alternative to their current method. The suggestion is specific and provides a clear path for the authors to explore, making it highly actionable. Therefore, this comment is rated as a 5 on the actionability scale.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises concerns about the experimental strengths of the approach, specifically questioning the necessity of running a descent procedure for 40 different networks. It suggests an alternative method using vanilla Adam with 40 random initial points. While the comment provides a specific critique and suggestion, it does not explicitly mention which part of the paper this issue is discussed in, making it weakly grounded. However, the comment is specific in detailing the issue and providing an alternative approach. Therefore, the comment is weakly grounded but specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point questions the experimental strengths of the approach by comparing it to an alternative method using vanilla Adam with multiple random initializations. The reviewer provides a logical reasoning for their claim by explaining that reaching the global minimum with at least one initialization is sufficient, rather than requiring each one to do so. However, the comment lacks specific examples or references to support the assertion that this alternative method would be effective. While the reasoning is somewhat clear, the absence of detailed evidence or external references makes the claim somewhat verifiable. Therefore, the comment is categorized as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 4.0\n",
      "Rationale: The review comment raises a concern about the experimental strengths of the proposed approach, specifically questioning the necessity of running a descent procedure for 40 different networks. It suggests an alternative approach using vanilla Adam with 40 random initial points, implying that this could achieve the same goal more efficiently. This feedback is valuable as it challenges the authors to reconsider their methodology and provides a specific alternative for comparison. However, the comment could be more helpful if it elaborated on why the suggested alternative might be more effective or provided additional context or evidence to support the claim. Despite this, the comment is mostly helpful as it prompts the authors to critically evaluate their approach and consider potential improvements. Therefore, it aligns with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. It seems to me that a very straightforward hypothesis about these two parts would be that the trivial part is what’s very simple, either highly consistent to what’s in the training set, or the images with very typical object pose in the center of the images; and for the impossible part, it might be the images with ambiguous labels, atypical object pose or position. I think the human test results would support this hypothesis, but I wonder whether the authors could provide more evidence to either prove or disprove this hypothesis.\n",
      "Paper ID: ICLR_2022_1014\n",
      "Venue: ICLR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment presents a hypothesis regarding the trivial and impossible parts of the dataset and suggests that human test results might support this hypothesis. However, it does not explicitly instruct the authors to provide evidence or conduct additional analysis to prove or disprove the hypothesis. The action is implicit, as the reviewer expresses curiosity about whether the authors could provide more evidence. While the comment hints at a potential direction for further investigation, it lacks concrete guidance on how the authors should proceed. Therefore, the comment is somewhat actionable, as it implies an action but does not provide detailed instructions on how to implement it.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is not explicitly grounded, as it does not mention a specific section, figure, or table in the paper. However, it does provide a specific hypothesis about the \"trivial\" and \"impossible\" parts, suggesting that the authors could provide more evidence to support or refute this hypothesis. The specificity of the hypothesis and the request for additional evidence provide some guidance for the authors, but without clear grounding, it remains challenging for them to identify exactly where to address this feedback. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point presents a hypothesis regarding the nature of the \"trivial\" and \"impossible\" parts of the dataset, suggesting that the trivial part consists of simple, consistent images, while the impossible part includes images with ambiguous labels or atypical poses. The reviewer expresses a belief that human test results would support this hypothesis but requests more evidence from the authors to prove or disprove it. While the comment provides some logical reasoning, it lacks specific examples, detailed explanations, or references to substantiate the hypothesis. This makes the claim somewhat verifiable, as the authors would need to provide additional evidence or analysis to fully address the reviewer's point.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment presents a hypothesis regarding the straightforward and impossible parts of the study, suggesting that the trivial part might consist of simple, consistent images, while the impossible part might involve ambiguous labels or atypical poses. It encourages the authors to provide more evidence to support or refute this hypothesis, potentially through human test results. While the comment offers a thought-provoking perspective and suggests an area for further exploration, it lacks specific guidance on how the authors might gather or present this evidence. The feedback is somewhat helpful as it prompts the authors to consider additional analysis but does not provide detailed, actionable steps to achieve this. Therefore, it is rated as somewhat helpful, consistent with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. The analysis of the correlation between dataset size and the Frobenius norm and the singular values is underwhelming. It is not clear if this trend holds across different model architectures, and if so, no theoretical evidence is advanced for this correlation.\n",
      "Paper ID: 2RQokbn4B5\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The review comment points out that the analysis of the correlation between dataset size and the Frobenius norm and singular values is underwhelming. It also questions whether this trend holds across different model architectures and notes the lack of theoretical evidence for the correlation. However, the comment does not provide explicit guidance or suggestions on how the authors could address these issues. The action is implicit and vague, as the authors would need to infer that they should provide more analysis or theoretical evidence, but the comment does not specify how to do so. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the analysis of the correlation between dataset size, the Frobenius norm, and singular values. It highlights a specific issue by questioning whether the observed trend holds across different model architectures and notes the lack of theoretical evidence for this correlation. However, the comment does not explicitly mention which part of the paper discusses this analysis, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where this analysis is presented, the comment lacks full grounding. It is specific about the issue of the correlation analysis but does not provide detailed guidance on how to address it. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point claims that the analysis of the correlation between dataset size and the Frobenius norm and singular values is underwhelming. It questions whether this trend holds across different model architectures and notes the absence of theoretical evidence supporting the correlation. While the comment highlights a potential issue, it lacks specific examples, detailed explanations, or references to substantiate the claim. The authors would need to make a significant effort to understand and address the critique, as the reasoning is not fully articulated. Therefore, the comment is categorized as 3: Somewhat Verifiable, as it provides some support for the claim but lacks key elements needed for full verification.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific area of concern regarding the analysis of the correlation between dataset size, the Frobenius norm, and singular values. It points out that the trend's applicability across different model architectures is unclear and notes the lack of theoretical evidence supporting this correlation. While the comment highlights a potential weakness in the paper, it does not provide actionable suggestions or guidance on how the authors might address these issues or improve their analysis. This limits the comment's usefulness, making it somewhat helpful but incomplete. Therefore, the comment aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones.\n",
      "Paper ID: NIPS_2020_295\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The reviewer suggests that the experimental comparisons are insufficient and implies that the authors should test their proposed method, InvP, with wider backbones like ResNet50 (2×) and ResNet50 (4×), similar to methods like MoCo and SimCLR. While the comment implies a specific action, it does not explicitly instruct the authors to conduct these tests. However, the suggestion is concrete, as it specifies the exact backbones to be used for comparison. Therefore, the comment is mostly actionable, as the authors can infer the necessary steps to improve their draft.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests expanding the experimental comparisons by testing the proposed InvP method with wider backbones like ResNet50 (2×) and ResNet50 (4×), similar to methods like MoCo and SimCLR. This provides a specific suggestion for improvement, indicating what additional experiments could be conducted. However, the comment does not explicitly mention which part of the paper it addresses, such as specific sections or tables where these comparisons could be included. The authors can infer that it relates to the experimental results section, but this inference is not as direct as it could be. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the experimental comparisons are insufficient, suggesting that methods like MoCo and SimCLR also test results with wider backbones such as ResNet50 (2×) and ResNet50 (4×). The comment implies that it would be beneficial to see the results of the proposed InvP with these wider backbones. While the comment provides some context by mentioning specific methods and backbones, it lacks detailed reasoning or references to substantiate why these comparisons are necessary or how they would enhance the study. The claim is somewhat verifiable, as it provides a direction for improvement but requires further elaboration to be fully understood and addressed by the authors. Therefore, the comment is classified as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a gap in the experimental comparisons by noting that other methods, such as MoCo and SimCLR, have tested results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It suggests that it would be interesting to see the results of the proposed InvP with these wider backbones. This feedback is somewhat helpful as it points out an area for improvement and provides a specific suggestion for enhancing the experimental setup. However, it could be more helpful if it explained why testing with wider backbones is important or how it could impact the results, providing the authors with a clearer understanding of the potential benefits. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "1. This work uses an antiquated GNN model and method, it seriously impacts the performance of this framework. The baseline algorithms/methods are also antiquated.\n",
      "Paper ID: NIPS_2022_69\n",
      "Venue: NIPS_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The review comment criticizes the use of an outdated GNN model and method, stating that it significantly impacts the performance of the framework. It also mentions that the baseline algorithms and methods are antiquated. However, the comment does not provide any specific guidance or suggestions on how the authors could address this issue, such as recommending more modern models or methods to use. The feedback lacks actionable details, leaving the authors without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The comment critiques the use of an antiquated GNN model and method, as well as outdated baseline algorithms/methods, suggesting that these choices negatively impact the framework's performance. However, it does not specify which parts of the paper discuss these models and methods, making it difficult for the authors to pinpoint the exact sections that need revision. Additionally, the comment lacks specificity regarding what aspects of the models and methods are considered antiquated or how they could be improved. Without explicit references or detailed guidance, the authors cannot confidently address the feedback. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 2 | GPT Label: 1\n",
      "Rationale: The review point claims that the work uses an \"antiquated GNN model and method,\" which \"seriously impacts the performance of this framework,\" and that the baseline algorithms/methods are also outdated. However, the comment does not provide any supporting evidence, examples, or references to substantiate these claims. Without specific details or comparisons to more modern approaches, the authors may find it challenging to understand or address the critique. Therefore, the claim is unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The review comment points out that the work uses an outdated GNN model and method, which negatively affects the performance of the framework. It also mentions that the baseline algorithms or methods are antiquated. While the comment identifies a potential weakness in the choice of models and methods, it lacks specific suggestions or guidance on how the authors might update or improve their approach. Without actionable feedback or recommendations for more current models or methods, the authors are left with a general critique but no clear path for improvement. Therefore, the comment is barely helpful, as it highlights an issue but does not provide sufficient guidance for addressing it.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2 The experiments are not quite convincing. The authors choose the old baseline like R3D and C3D. To reduce computation complexity, many papers have been proposed in 3D CNN (X3D, SlowFast, etc). Does the proposed method also works on these 3D CNNs? Or compared to these approaches, what is the advantage of the proposed method?\n",
      "Paper ID: ICLR_2023_4741\n",
      "Venue: ICLR_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises concerns about the choice of baselines in the experiments, specifically mentioning that the authors have chosen older baselines like R3D and C3D. It suggests considering more recent 3D CNNs such as X3D and SlowFast to reduce computational complexity. The reviewer also questions whether the proposed method works on these newer 3D CNNs and what advantages it has over these approaches. While the comment implies that the authors should include comparisons with newer baselines and assess the applicability and advantages of their method, it does not provide explicit instructions or detailed guidance on how to conduct these comparisons. The action is implicit and somewhat vague, as the authors need to infer the specific steps to take. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment critiques the choice of baselines in the experiments, specifically mentioning \"R3D\" and \"C3D,\" which provides some grounding as the authors can infer that it relates to the experimental section of the paper. However, it does not explicitly mention which part of the paper discusses these baselines, making it weakly grounded. The comment is specific in suggesting that the authors should consider newer 3D CNN models like \"X3D\" and \"SlowFast\" and asks about the advantages of the proposed method compared to these approaches. This specificity provides clear guidance on what the authors need to address. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point claims that \"the experiments are not quite convincing\" due to the choice of older baselines like R3D and C3D. The reviewer suggests that more recent methods, such as X3D and SlowFast, should be considered to reduce computational complexity. However, the comment lacks specific examples or detailed reasoning to support why the current baselines are inadequate or how the proposed method compares to newer approaches. This makes the claim somewhat verifiable, as it provides a direction for improvement but lacks sufficient evidence or references to fully substantiate the critique.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a concern with the choice of baselines in the experiments, noting that the authors have used older models like R3D and C3D. It suggests that more recent models, such as X3D and SlowFast, should be considered to reduce computational complexity. The comment also raises a question about whether the proposed method works on these newer 3D CNNs and what advantages it might have over them. While the comment highlights important areas for improvement, it lacks specific guidance or suggestions on how to address these issues, such as how to incorporate newer models into the experiments or what specific comparisons should be made. This limits the utility of the feedback, making it somewhat helpful as it points out areas for improvement but does not fully support the authors in making those improvements.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2) Important reference missing. The paper is closely related to the idea of unrolling, first proposed in, âListaâ http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf While there are important similarities and differences between the proposed work and Lista, it is important that the paper talks about them and places itself in appropriate context.\n",
      "Paper ID: NIPS_2017_226\n",
      "Venue: NIPS_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment points out that an important reference, \"Lista,\" is missing and suggests that the paper should discuss the similarities and differences between the proposed work and Lista. This feedback implies that the authors should include a discussion of this reference to place their work in the appropriate context. While the action is implicit, it is relatively clear what the authors need to do—add a discussion about the Lista paper. However, the comment does not provide specific guidance on how to structure this discussion or what particular aspects to focus on, making the action somewhat vague. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The comment highlights the absence of an important reference related to the idea of unrolling, specifically mentioning the \"Lista\" paper. However, it does not specify where in the paper this reference should be included or how the authors should discuss the similarities and differences between their work and the \"Lista\" paper. While the authors can infer that this pertains to the literature review or background sections, the comment lacks specificity in detailing what needs to be addressed. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point claims that an important reference is missing, specifically mentioning the paper \"Lista\" by Gregor et al., which first proposed the idea of unrolling. The reviewer suggests that the current paper should discuss the similarities and differences between its work and \"Lista\" to provide appropriate context. This claim is supported by a specific reference to an external work, which substantiates the need for the authors to address this gap in their paper. The inclusion of a relevant reference makes the claim mostly verifiable, but it could be strengthened by providing more detailed reasoning or examples of the similarities and differences. Therefore, the comment is classified as 4: Mostly Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a significant oversight in the paper by pointing out the absence of an important reference related to the idea of unrolling, specifically the \"Lista\" paper. It emphasizes the importance of discussing the similarities and differences between the proposed work and Lista, which is crucial for situating the paper within the existing literature and providing appropriate context. This feedback is clear and actionable, as it guides the authors to address a specific gap in their references and discussion. However, the comment could be more helpful if it provided additional guidance on how to effectively integrate this reference into the paper or suggested specific aspects of the comparison to highlight. Overall, the comment is mostly helpful as it directs the authors to a critical area needing attention, but it could be more comprehensive with further guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2) In the first three paragraphs of section 2, the setting needs to be spelled out more clearly. It seems like the authors want to receive credit for doing something in greater generality than what they actually present, and this muddles the exposition.\n",
      "Paper ID: NIPS_2016_537\n",
      "Venue: NIPS_2016\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The review comment suggests that the first three paragraphs of section 2 need to be clarified, as the current presentation may give the impression that the authors are claiming more generality than is actually presented. However, the comment does not provide specific guidance on how to achieve this clarity or what aspects need to be addressed to avoid the perceived overstatement. The action is implicit and vague, as the authors need to infer that they should clarify the setting, but they lack concrete steps on how to do so. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 4.0 | GPT Label: 5.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions \"the first three paragraphs of section 2,\" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue by indicating that the setting needs to be spelled out more clearly and suggests that the authors might be overstating the generality of their work, which muddles the exposition. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 1 | GPT Label: 2\n",
      "Rationale: The review point claims that the setting in the first three paragraphs of section 2 is not clearly spelled out and suggests that the authors might be overstating the generality of their work. However, the comment lacks specific examples or detailed reasoning to support this claim, making it difficult for the authors to understand the basis of the critique. Without concrete evidence or a more detailed explanation, the claim remains vague and not fully articulated, which makes it borderline verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 2.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a clarity issue in the first three paragraphs of section 2, suggesting that the setting needs to be spelled out more clearly. It implies that the authors may be overstating the generality of their work, which confuses the exposition. This feedback is somewhat helpful as it points out a specific area where the authors can improve the clarity and accuracy of their presentation. However, the comment lacks detailed guidance or specific suggestions on how to achieve this clarity, leaving the authors with only a general sense of what needs to be addressed. Therefore, the comment provides some insight but lacks depth and specificity, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2) It seems like the model is limited to CTC loss, would it be possible to train them towards attention based enc-dec training?\n",
      "Paper ID: NIPS_2020_314\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises a question about the possibility of training the model using attention-based encoder-decoder (enc-dec) training instead of being limited to CTC loss. While the comment suggests an alternative approach, it does not explicitly instruct the authors to implement this change or provide concrete steps on how to do so. The action is implicit, as the authors can infer that they might explore attention-based training, but it remains vague due to the lack of detailed guidance on how to proceed with this suggestion. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment raises a question about the model's limitation to CTC loss and suggests the possibility of training with attention-based encoder-decoder methods. However, it does not specify which part of the paper this limitation is discussed or where the suggestion could be implemented. The authors might infer that it relates to the methodology or results sections, but the lack of explicit grounding makes it difficult to pinpoint the exact area of focus. The comment is somewhat specific in suggesting an alternative training approach but lacks grounding. Therefore, this comment is weakly grounded and not specific, aligning with the label 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 1\n",
      "Rationale: The review point raises a question about the model's limitation to CTC loss and suggests exploring the possibility of training with attention-based encoder-decoder (enc-dec) methods. However, the comment does not provide any supporting evidence, reasoning, or examples to justify why this limitation is significant or how attention-based methods could be beneficial. Without additional context or explanation, the authors may find it challenging to understand the importance of this suggestion or how to implement it. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a potential limitation of the model, noting its restriction to CTC loss, and poses a question about the possibility of training the model using attention-based encoder-decoder training. While this highlights an area for potential improvement, the comment is somewhat vague and lacks specific guidance or suggestions on how to implement such a change. The question format provides a starting point for the authors to consider alternative training methods, but without further elaboration or actionable advice, the feedback is limited in its usefulness. Therefore, the comment is somewhat helpful, as it prompts the authors to explore a different approach but does not fully support them in making that transition.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. Did the authors study numbers of bits in logits helps against a larger epsilon in the PGD attack? Because intuition suggests that having a 32 bit logit should improve robustness against a more powerful adversary. This experiment isn't absolutely necessary, but does strengthen the paper.\n",
      "Paper ID: NIPS_2019_776\n",
      "Venue: NIPS_2019\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point poses a question about whether the authors studied the impact of the number of bits in logits on robustness against a larger epsilon in the PGD attack. It suggests that conducting this experiment could strengthen the paper, but it does not explicitly instruct the authors to perform the experiment. The action is implied and somewhat vague, as it does not provide specific guidance on how to conduct the experiment or what results to look for. Therefore, the comment is somewhat actionable, as it hints at a potential area for improvement but lacks detailed instructions on execution.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises a question about whether the authors studied the impact of the number of bits in logits on robustness against a larger epsilon in the PGD attack. It suggests that a 32-bit logit might improve robustness against a more powerful adversary. However, the comment does not specify which part of the paper this question pertains to, making it weakly grounded. The comment is specific in suggesting an experiment that could strengthen the paper, but without explicit references to sections or experiments, the authors may find it challenging to determine where this suggestion fits within their work. Therefore, the comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point raises a question about whether the authors studied the impact of the number of bits in logits on robustness against a larger epsilon in the PGD attack. It suggests that intuition indicates a 32-bit logit should improve robustness against a more powerful adversary. While the comment provides a logical reasoning based on intuition, it lacks specific evidence, examples, or references to support this claim. The suggestion is somewhat verifiable, as it relies on a general expectation without concrete backing, requiring the authors to explore the reasoning further. Therefore, it is rated as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment poses a question about whether the authors have studied the impact of the number of bits in logits on robustness against a larger epsilon in the PGD attack. It suggests that a 32-bit logit might improve robustness against a more powerful adversary, providing a rationale for why this experiment could be valuable. While the comment acknowledges that this experiment isn't absolutely necessary, it implies that including it could strengthen the paper. The feedback is somewhat helpful as it offers a potential direction for further exploration that could enhance the paper's contribution. However, it lacks specific guidance on how to conduct the experiment or integrate the findings into the paper, which would make it more actionable. Therefore, the comment aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. It would be nice to include the hard prompt baseline in Table 1 to see the increase in performance of each method.\n",
      "Paper ID: gybvlVXT6z\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment suggests including the hard prompt baseline in Table 1 to compare the performance increase of each method. While the action is implied rather than explicitly stated, it is relatively clear what the authors should do: add the hard prompt baseline to the table. However, the comment does not provide specific guidance on how to implement this change or why it is important, leaving some room for interpretation. Therefore, the comment is mostly actionable, as the action is clear but not directly stated.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment suggests including a hard prompt baseline in Table 1 to assess the performance increase of each method. It is fully grounded as it explicitly mentions \"Table 1,\" allowing the authors to accurately identify the part of the paper being addressed. However, the comment lacks specificity regarding what the \"hard prompt baseline\" entails or how it should be implemented, leaving the authors without detailed guidance on how to incorporate this suggestion. Therefore, the comment is fully grounded but under-specific, aligning with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point suggests including a hard prompt baseline in Table 1 to evaluate the performance increase of each method. However, the comment does not provide any specific reasoning, examples, or references to justify why this inclusion is necessary or how it would benefit the analysis. Without additional context or explanation, the authors may find it challenging to understand the importance of this suggestion. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment suggests including a hard prompt baseline in Table 1 to compare the performance increase of each method. This feedback is clear and actionable, as it provides a specific recommendation that could enhance the comprehensiveness and clarity of the results presented in the paper. By adding this baseline, the authors can offer a more complete picture of the performance improvements achieved by their methods. However, the comment could be more helpful if it included additional context or justification for why this comparison is important. Overall, the comment is mostly helpful, as it guides the authors toward a meaningful improvement in their draft. score: 4\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. It's not clear why the freezing is used in MLS selection. If adaptive is good, why not just use adaptive method to choose the subset?\n",
      "Paper ID: FVhmnvqnsI\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The comment raises a question about the use of freezing in MLS selection and suggests considering an adaptive method instead. However, it does not provide explicit guidance or concrete steps on how the authors should address this issue. The action is implied, as the authors can infer that they should clarify the rationale for using freezing or consider an adaptive method. However, the comment lacks specific instructions on how to implement these changes, making it vague. Therefore, the comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment questions the rationale behind using freezing in MLS selection and suggests considering an adaptive method instead. However, it does not specify which part of the paper discusses MLS selection, making it weakly grounded. The authors can infer that it relates to the methodology or results section, but they cannot pinpoint the exact location. The comment is specific in its critique, as it questions the choice of method and suggests an alternative approach. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: X\n",
      "Rationale: The comment is a request for clarification regarding the methodology used in MLS selection. It does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual and seeks additional information, so \"No\" is the appropriate label.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises a question about the methodology, specifically questioning the rationale behind using freezing in MLS selection and suggesting the use of an adaptive method instead. While it identifies a potential area of confusion or improvement, the comment lacks depth and does not provide specific guidance or suggestions on how the authors might address this issue. It prompts the authors to reconsider their approach but does not offer actionable feedback or detailed advice on how to improve the draft. Therefore, the comment is somewhat helpful, as it points out a potential weakness but lacks the specificity needed to be more impactful. This aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. More emphasis should be placed on prompt design. This paper introduces several prompting methods to address issues in MenatQA. Since different prompts may result in varying performance outcomes, it is essential to discuss how to design prompts effectively.\n",
      "Paper ID: vexCLJO7vo\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests that more emphasis should be placed on prompt design, noting that the paper introduces several prompting methods to address issues in MenatQA. It highlights the importance of discussing how to design prompts effectively, given that different prompts can lead to varying performance outcomes. While the feedback explicitly states the need for a discussion on prompt design, it does not provide specific guidance on what aspects of prompt design should be covered or how to structure this discussion. The authors are left with a clear action to take but without detailed instructions on how to implement it. Therefore, the comment is somewhat actionable, as it identifies an area for improvement but lacks detailed guidance on execution.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests placing more emphasis on prompt design and acknowledges that the paper introduces several prompting methods to address issues in MenatQA. However, it does not specify which part of the paper should include this discussion or where the current discussion is lacking, making it weakly grounded. The comment does specify what is missing—an effective discussion on prompt design and its impact on performance outcomes—making it specific. Therefore, the comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point suggests that more emphasis should be placed on prompt design, highlighting that different prompts can lead to varying performance outcomes. The comment is somewhat verifiable as it identifies a potential area for improvement and provides a logical rationale for why prompt design is important. However, it lacks specific examples, detailed explanations, or references to support the claim or guide the authors on how to effectively design prompts. This incomplete articulation requires the authors to make a significant effort to understand and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies an important area for improvement by suggesting that more emphasis should be placed on prompt design. It acknowledges that the paper introduces several prompting methods to address issues in MenatQA but highlights the need for a discussion on how to design prompts effectively. This feedback is clear and actionable, as it directs the authors to focus on an aspect that could significantly impact the performance outcomes of their methods. However, the comment could be more helpful if it provided specific suggestions or examples of effective prompt design strategies. Overall, the comment is mostly helpful as it guides the authors toward a meaningful enhancement of their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. Quality of generated images by proposed method is limited. While good continuous control is achieved, the realism of generated results showed in paper and supplemental material is limited.\n",
      "Paper ID: ICLR_2021_1783\n",
      "Venue: ICLR_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The review point highlights a limitation in the quality of the generated images, noting that while good continuous control is achieved, the realism of the results is limited. However, the comment does not provide any explicit or implicit suggestions for how the authors might address this issue or improve the realism of the generated images. There is no guidance on potential modifications, techniques, or additional experiments that could be conducted to enhance the realism. As a result, the authors are left without any actionable steps to take in response to this feedback. Therefore, the comment is rated as unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the quality of generated images by the proposed method, specifically mentioning the limited realism of the results shown in the paper and supplemental material. However, it does not explicitly mention which part of the paper discusses these results, making it weakly grounded. The comment specifies the issue of limited realism but does not provide detailed guidance on how to improve it or what aspects contribute to this limitation. Therefore, the comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 3 | GPT Label: 1\n",
      "Rationale: The review point claims that the quality of generated images by the proposed method is limited, particularly in terms of realism, despite achieving good continuous control. However, the comment lacks specific examples, detailed reasoning, or references to substantiate this claim. Without additional context or evidence, it is challenging for the authors to understand the basis of the critique and address it effectively. Therefore, the claim is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The review comment identifies a specific weakness in the paper, namely the limited quality and realism of the generated images by the proposed method. It acknowledges that good continuous control is achieved but highlights that the realism of the results is lacking. However, the comment does not provide any suggestions or guidance on how the authors might improve the realism of the generated images or what specific aspects need attention. Without actionable advice or detailed feedback, the authors are left with a general observation that may not directly help them improve their draft. Therefore, the comment is rated as barely helpful, aligning with a score of 2.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. Some analyses can be more detailed. For example, in \"language/nationality\", the data includes Japanese, Chinese, English, Arabic, German... (~20 different types). Biases towards different languages/nationalities are different. I was wondering whether there would be some interesting observations comparing them.\n",
      "Paper ID: ARR_2022_67_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that some analyses, particularly those related to \"language/nationality,\" could be more detailed. It mentions that the data includes various languages and nationalities and implies that there might be interesting observations to be made by comparing them. However, the comment does not explicitly instruct the authors to conduct a specific analysis or provide guidance on how to perform such a comparison. The action is implied and somewhat vague, as the authors can infer that they should explore biases across different languages/nationalities, but they are not given concrete steps on how to do so. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is partially grounded as it refers to a specific analysis related to \"language/nationality,\" which includes various languages such as Japanese, Chinese, English, Arabic, and German. This allows the authors to identify the general area of the paper being addressed. However, it does not specify which section or part of the paper contains this analysis, making it somewhat challenging for the authors to pinpoint the exact location. The comment is specific in suggesting that biases towards different languages/nationalities could be explored further, indicating a need for more detailed analysis. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point suggests that some analyses, specifically regarding \"language/nationality,\" could be more detailed. It mentions that the data includes various languages/nationalities and implies that biases may differ among them. However, the comment does not provide specific examples or detailed reasoning to support the claim that more detailed analysis would yield interesting observations. While it hints at potential insights, the lack of concrete evidence or references makes the claim somewhat verifiable, as the authors would need to further develop the rationale themselves.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies an area where the analysis could be more detailed, specifically in the context of biases towards different languages and nationalities. It provides a concrete example by mentioning the variety of languages/nationalities included in the data and suggests that comparing them might yield interesting observations. This feedback is somewhat helpful as it points out a potential area for deeper analysis and encourages the authors to explore it further. However, the comment could be more helpful if it provided specific suggestions on how to conduct this analysis or what aspects to focus on. Overall, the comment offers a direction for improvement but lacks detailed guidance, making it somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. The adaptation capacity of the proposed visual memory to accommodate ever-growing concepts assumes that the image encoder can produce meaningful embeddings for new concepts. While for geometrically distinctive concepts, I believe this is less of a concern for DINO representations, as they are observed to contain rich geometric information, for concepts where class label correlates more with semantics rather than geometry, I wonder if the adaptation capacity still holds.\n",
      "Paper ID: HoyKFRhwMS\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The review comment raises a concern about the adaptation capacity of the proposed visual memory, particularly in relation to the image encoder's ability to produce meaningful embeddings for new concepts. It distinguishes between geometrically distinctive concepts and those where class labels correlate more with semantics. However, the comment does not provide explicit guidance or suggestions on how the authors might address this concern or test the adaptation capacity for semantically correlated concepts. The action is implicit and vague, leaving the authors without clear steps to take. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises a specific concern about the adaptation capacity of the proposed visual memory, particularly in relation to the image encoder's ability to produce meaningful embeddings for new concepts. It distinguishes between geometrically distinctive concepts and those where class labels correlate more with semantics. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it pertains to the sections discussing the visual memory and image encoder. The comment is specific in detailing the concern about the adaptation capacity for different types of concepts. However, since it does not explicitly mention the sections, it is weakly grounded but specific. Therefore, this comment aligns with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point raises a concern about the adaptation capacity of the proposed visual memory, specifically questioning whether the image encoder can produce meaningful embeddings for new concepts. The reviewer provides a logical reasoning by distinguishing between geometrically distinctive concepts and those where class labels correlate more with semantics. The mention of DINO representations containing rich geometric information adds some support to the claim. However, the comment lacks specific examples or references to substantiate the concern about semantic concepts, making it somewhat verifiable. The authors would need to further explore and develop the rationale themselves to fully address the critique.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises an insightful point about the adaptation capacity of the proposed visual memory, particularly questioning its effectiveness for concepts where class labels correlate more with semantics rather than geometry. It acknowledges the strength of DINO representations for geometrically distinctive concepts but expresses concern for semantically-driven concepts. This feedback is somewhat helpful as it identifies a potential limitation in the proposed method and encourages the authors to consider this aspect. However, it lacks specific suggestions or guidance on how the authors might address or test this concern, which would make the feedback more actionable and comprehensive. Therefore, the comment aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.\n",
      "Paper ID: 5BoXZXTJvL\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point suggests that the paper should include experiments on different large language model (LLM) families, such as OPT and BLOOM, to provide insights into the method's applicability and generalizability. While the comment implies a specific action—conducting trials with alternative models—it does not provide detailed guidance on how to implement these experiments or what specific aspects should be focused on. The suggestion is clear but lacks concrete details on execution, making it somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests that the paper lacks experiments on different LLM (Large Language Model) families and recommends conducting trials with models like OPT, BLOOM, or other alternatives. This provides a specific suggestion for improvement by identifying the need for broader experimentation to assess the method's applicability and generalizability. However, the comment does not explicitly mention which part of the paper this issue pertains to, such as specific sections or experiments that could be expanded. The authors can infer that it relates to the experimental or results sections, but this inference is not as direct as it could be. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point claims that the paper lacks experiments on different LLM families and suggests that conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights. The comment provides a logical reasoning for the claim by indicating that such experiments could enhance understanding of the method's applicability and generalizability. However, it lacks specific examples or references to support why these particular models would be beneficial or how they would improve the paper. This makes the claim somewhat verifiable, as it provides a reasonable suggestion but requires more detailed evidence or examples to fully substantiate the argument.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review point identifies a significant gap in the experimental design of the paper, specifically the lack of experiments on different LLM (Large Language Model) families. It suggests that conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability. This feedback is clear and actionable, offering a concrete direction for the authors to expand their experimental scope and enhance the robustness of their findings. However, the comment could be more helpful if it provided specific guidance on how to implement these experiments or discussed potential challenges and considerations. Despite this, the feedback is mostly helpful as it directs the authors to a critical area for improvement, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2. Why is this zero-shot? In addition, I guess the transferability might be limited by the real difficulty of the source task / target task. Walker-run is clearly harder than walker-walk, so the policy transfer here is possible. Also, for the manipulation scenario, 3-prong task with both clockwise / counter clockwise rotations, together with one 4-prong task, actually provides sufficient information about the target task. I guess it might be difficult to transfer a policy from simpler tasks to more complex tasks. This has to be made clear in the paper. Otherwise, it is quite misleading.\n",
      "Paper ID: NIPS_2021_1072\n",
      "Venue: NIPS_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises several questions and concerns about the zero-shot nature of the task and the transferability between tasks of varying difficulty. It suggests that the paper should clarify these issues to avoid being misleading. While the comment identifies specific areas that need clarification, it does not provide explicit instructions on how to address these concerns or what specific changes should be made to the paper. The action is implicit and somewhat vague, as the authors need to infer that they should provide more detailed explanations regarding the zero-shot claim and task transferability. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses specific concerns about the zero-shot nature of the tasks and the transferability between source and target tasks. It provides examples, such as the walker-run versus walker-walk tasks and the manipulation scenario with prong tasks, to illustrate the points being made. However, it does not explicitly mention which sections of the paper these issues relate to, leaving the authors to infer the relevant parts. While the comment is specific in its critique and suggestions for clarification, the lack of explicit grounding in the paper's structure makes it weakly grounded. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point questions the zero-shot nature of the task and suggests that the transferability might be limited by the difficulty of the source and target tasks. The reviewer provides logical reasoning by comparing the difficulty levels of tasks like \"walker-run\" and \"walker-walk\" and discusses the manipulation scenario with specific tasks. However, the comment lacks external references or detailed examples to fully substantiate the claim. While the reasoning is logical, it requires more concrete evidence or references to be fully verifiable. Therefore, the comment is categorized as somewhat verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises important questions about the zero-shot nature of the study and the potential limitations in transferability between tasks of varying difficulty. It provides specific examples, such as the walker-run versus walker-walk tasks, to illustrate the point that transferring policies from simpler to more complex tasks might be challenging. The comment suggests that the paper should clarify these aspects to avoid being misleading. While it identifies a critical area for improvement and provides some context, it lacks detailed suggestions or actionable steps for the authors to address these issues. Therefore, the feedback is somewhat helpful, as it prompts the authors to reconsider and clarify their claims, but it could be more beneficial with additional guidance on how to resolve the identified problems.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "2.The paper is not sound as there are many exploration methods in RL literature, such as count-based methods and intrinsic motivations(RND,ICM). But the paper does not discuss and compare these methods.\n",
      "Paper ID: ICLR_2023_2869\n",
      "Venue: ICLR_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment points out that the paper lacks a discussion and comparison of various exploration methods in reinforcement learning (RL) literature, such as count-based methods and intrinsic motivations like RND and ICM. While the comment identifies a gap in the paper, it does not provide explicit instructions on how to address this issue. The authors can infer that they should include a discussion and comparison of these methods, but the comment does not specify which aspects to focus on or how to integrate this information into the paper. As a result, the action is implicit and somewhat vague, making the comment barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment critiques the paper for not discussing and comparing various exploration methods in reinforcement learning (RL) literature, such as count-based methods and intrinsic motivations like RND and ICM. However, it does not specify which part of the paper should include this discussion or comparison, making it weakly grounded. The comment is specific in identifying what is missing—discussion and comparison of certain RL methods—but lacks grounding as it does not point to a particular section or context within the paper. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review comment claims that the paper is not sound because it fails to discuss and compare various exploration methods in reinforcement learning (RL) literature, such as count-based methods and intrinsic motivations like RND and ICM. This claim is somewhat verifiable as it points out a potential gap in the paper's literature review or comparative analysis. However, the comment lacks specific examples or detailed reasoning about how these methods relate to the paper's content or why their inclusion is critical. To fully substantiate the claim, the reviewer could provide more context or references to illustrate the importance of these methods in the context of the paper. Therefore, the comment is categorized as somewhat verifiable, justifying a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a significant gap in the paper's discussion by pointing out the absence of a comparison with existing exploration methods in reinforcement learning (RL) literature, such as count-based methods and intrinsic motivations like RND and ICM. This feedback highlights an area where the paper could be improved by situating its contributions within the broader context of RL exploration methods. However, the comment lacks specific guidance on how the authors might incorporate this comparison or what aspects of these methods should be discussed. While it provides a clear direction for improvement, it could be more helpful with additional details or suggestions on how to address the identified gap. Therefore, the comment is somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. Also, the theoretical discussions can use some improvements. Although directly related to fair representation, the current theorems follow directly from the algorithm design itself and the well-known property of mutual information to $\\Delta_{DP}$. For instance, I was expecting some sample complexity-type results for not returning NSF, e.g., given confidence levels, what is the sufficient amount of training data points that would not return NSF.\n",
      "Paper ID: fkvdewFFN6\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point suggests that the theoretical discussions could be improved by providing more depth and specificity. It mentions that the current theorems follow directly from the algorithm design and a well-known property, implying that the discussion lacks novelty or depth. The reviewer provides a specific example of what they were expecting, such as sample complexity-type results related to not returning NSF, and suggests including information about confidence levels and the sufficient amount of training data points. While the comment implies an action, it is somewhat vague in terms of how the authors should implement these improvements. The suggestion is explicit but lacks detailed guidance on how to achieve the desired improvements. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the theoretical discussions in the paper, suggesting that they could be improved by including sample complexity-type results. It specifies that the current theorems follow directly from the algorithm design and the well-known property of mutual information to $\\Delta_{DP}$. The comment provides a specific example of what is missing—sample complexity-type results with confidence levels and sufficient training data points. However, it does not explicitly mention which part of the paper these theoretical discussions are located in, making it weakly grounded. The specificity of the feedback regarding the need for additional theoretical results makes it specific. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point suggests that the theoretical discussions could be improved, specifically noting that the current theorems follow directly from the algorithm design and the known property of mutual information to $\\Delta_{DP}$. The reviewer provides a logical basis for their claim by pointing out the lack of sample complexity-type results, which would offer more depth to the theoretical analysis. However, the comment lacks specific examples or references to support the claim fully, making it somewhat verifiable. The authors would need to make a significant effort to understand and address the critique without additional guidance. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a specific area for improvement in the theoretical discussions of the paper. It points out that the current theorems are directly derived from the algorithm design and known properties, suggesting that the authors could enhance their work by including sample complexity-type results. The comment provides a clear example of what is expected, such as determining the sufficient amount of training data needed to achieve certain confidence levels. This feedback is actionable and directs the authors toward a meaningful enhancement of their theoretical analysis. However, the comment could be more helpful if it provided additional guidance or examples on how to derive these results. Overall, the comment is mostly helpful, as it effectively directs the authors toward a significant improvement in their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. Authors state that reliable PPP metrics are important for understanding PPP effects in different tasks. While this point is surely intriguing, such an explanation or understanding is not explicitly given in the article. Can the authors explicitly explain what type of understanding one reaches by looking at the PPP maps?\n",
      "Paper ID: NIPS_2022_2814\n",
      "Venue: NIPS_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point highlights a gap in the article, noting that while the authors mention the importance of reliable PPP metrics for understanding effects in different tasks, they do not provide an explicit explanation of the type of understanding that can be gained from PPP maps. The reviewer asks the authors to explicitly explain this understanding. Although the request for an explanation is clear, it lacks specific guidance on how the authors should present this information or what aspects to focus on. Therefore, the action is explicit but somewhat vague, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses a specific claim made by the authors regarding the importance of reliable PPP metrics for understanding PPP effects in different tasks. It requests an explicit explanation of the type of understanding that can be gained by examining the PPP maps. However, the comment does not specify which section of the paper this claim is made, making it weakly grounded. Despite this, the comment is specific in its request for an explanation of the understanding derived from PPP maps. Therefore, the comment is weakly grounded but specific, aligning with label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point questions the authors' claim about the importance of reliable PPP metrics for understanding PPP effects in different tasks. It highlights that the explanation or understanding of this importance is not explicitly provided in the article. While the comment points out a gap in the explanation, it does not provide specific examples or references to support why this understanding is crucial or how it could be better articulated. This makes the claim somewhat verifiable, as it identifies an area for improvement but lacks detailed evidence or reasoning to fully substantiate the critique. Therefore, the comment is categorized as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a gap in the paper regarding the explanation of how reliable PPP metrics contribute to understanding PPP effects in different tasks. It highlights the need for the authors to explicitly explain the type of understanding that can be gained from examining PPP maps. This feedback is somewhat helpful as it points out a specific area where the paper lacks clarity and suggests that the authors provide a more explicit explanation. However, the comment could be more helpful if it offered specific guidance or examples on how the authors might address this gap. Overall, the comment provides some direction for improvement but lacks comprehensive guidance, making it somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. Besides the methodology, it's also not clear how the different part of this framework performs and contribute to the final result from the experimental aspect. While in the result section, we can see that the framework can yield promising visual stimuli result, it lacks either quantitative experiments and comparison between selection of algorithms, or a more detailed explanations on the presented ones. (See questions.) Therefore, it's unclear for me what the exact performance of the whole framework and individual parts compared to other solutions.\n",
      "Paper ID: 4ltiMYgJo9\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment highlights a lack of clarity regarding the performance and contribution of different parts of the framework to the final result. It suggests that the results section lacks either quantitative experiments and comparisons between algorithm selections or more detailed explanations of the presented ones. While the comment identifies areas needing improvement, it does not provide explicit instructions or concrete steps on how to address these issues. The authors are left to infer that they should include more quantitative experiments or detailed explanations, but the comment does not specify which aspects to focus on or how to conduct these analyses. Therefore, the action is implicit and somewhat vague, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the performance and contribution of different parts of the framework, particularly from an experimental perspective. It mentions the results section, allowing the authors to identify the general area of concern, providing some level of grounding. However, it does not specify which specific parts of the results section or which algorithms need more detailed explanations or comparisons. The comment is specific in pointing out the need for quantitative experiments and comparisons, but it lacks full grounding as it does not pinpoint exact sections or elements that require attention. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point raises concerns about the clarity of the framework's performance and contribution to the final result. It suggests that the results section lacks quantitative experiments and comparisons between algorithm selections, or detailed explanations of the presented ones. While the comment identifies a gap in the experimental analysis, it does not provide specific examples or references to support the claim or suggest how the authors might address the issue. This makes the comment somewhat verifiable, as it highlights a potential area for improvement but requires significant effort from the authors to understand and act upon the feedback. Therefore, it is categorized as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a significant issue with the paper's experimental section, specifically the lack of clarity on how different parts of the framework contribute to the final results. It points out that while the framework shows promising visual stimuli results, it lacks quantitative experiments and comparisons between algorithm selections or detailed explanations of the presented ones. This feedback is clear and actionable, as it directs the authors to provide more comprehensive experimental analysis and comparisons to enhance the understanding of the framework's performance. However, the comment could be more helpful if it included specific suggestions or examples of how to conduct these comparisons or what additional details to provide. Overall, the comment is mostly helpful, as it effectively highlights a critical area for improvement, but it could be more comprehensive with additional guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. Missing in-depth analysis on experimental results. For example, why the improvements of models are limited on offense detection dataset and are significant on coarse stereotype set?\n",
      "Paper ID: ARR_2022_287_review\n",
      "Venue: ARR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review comment highlights the absence of an in-depth analysis of the experimental results, specifically questioning why the model improvements are limited on the offense detection dataset but significant on the coarse stereotype set. While the comment identifies a gap in the analysis, it does not provide explicit guidance on how the authors should address this issue or what specific aspects of the analysis should be expanded. The authors are left to infer that they need to provide a more detailed analysis, but without concrete instructions, the action remains vague. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment highlights a lack of in-depth analysis on the experimental results, specifically questioning why model improvements are limited on the offense detection dataset but significant on the coarse stereotype set. However, it does not specify which sections or parts of the paper should include this analysis, making it weakly grounded. The comment is specific in detailing what is missing—an explanation for the differing levels of improvement across datasets. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The review point claims that there is a missing in-depth analysis of the experimental results, specifically questioning why model improvements are limited on the offense detection dataset but significant on the coarse stereotype set. While the comment identifies a specific area needing further analysis, it does not provide any reasoning, examples, or references to support the claim or guide the authors on how to address it. This lack of detailed justification makes the claim somewhat verifiable, as the authors would need to conduct their own investigation to understand and address the issue. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a specific area where the paper is lacking, namely the in-depth analysis of experimental results. It highlights the need for further explanation regarding the varying improvements of models on different datasets, such as the offense detection dataset and the coarse stereotype set. This feedback is valuable as it directs the authors' attention to an important aspect of their work that requires further exploration. However, the comment could be more helpful if it provided suggestions on how to conduct this analysis or offered insights into potential reasons for the observed differences. Despite this, the feedback is somewhat helpful as it points out a critical area for improvement, but it lacks depth and actionable guidance. Therefore, it aligns with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.\n",
      "Paper ID: NIPS_2020_639\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment highlights the lack of empirical evaluation and comparison with other methods, emphasizing that the practical value of the contribution is unclear. It suggests that even theoretical papers should argue for their significance, which this submission fails to do. While the comment identifies critical shortcomings, it does not provide explicit guidance on how to address these issues, such as suggesting specific empirical evaluations or comparisons to include. The authors are left to infer that they need to add empirical evaluations and comparisons, but without concrete instructions, the action remains vague. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment highlights a significant issue with the paper, specifically the lack of empirical evaluation and comparison with other methods. It points out that the practical value of the contribution is unclear and suggests that even theoretical papers should argue for their significance. While the comment does not explicitly mention specific sections, it is clear that it addresses the overall evaluation and comparison aspects of the paper. The authors can infer that it relates to sections discussing results, methodology, or theoretical contributions. The comment is specific in detailing what is missing and why it is important, but it lacks explicit grounding in terms of pinpointing exact sections. Therefore, this comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 3\n",
      "Rationale: The review point claims that there is \"no empirical evaluation whatsoever\" and that the paper lacks comparison with other methods, except on an abstract level. It further argues that the practical value of the contribution is unclear and that even theoretical papers should justify their significance. While the comment provides a general critique of the paper's lack of empirical evaluation and practical justification, it does not offer specific examples or detailed reasoning to substantiate these claims. The absence of specific references or examples makes it challenging for the authors to fully understand and address the critique. Therefore, the comment is somewhat verifiable, as it provides a general rationale but lacks detailed support.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 2.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a significant issue with the paper, specifically the lack of empirical evaluation and comparison with other methods. It highlights the absence of practical value and argues that even theoretical papers should justify their relevance. This feedback is clear and actionable, as it points out a critical weakness that the authors need to address to make their paper suitable for publication. However, the comment could be more helpful if it provided specific suggestions on how to incorporate empirical evaluations or comparisons. Despite this, the feedback is mostly helpful as it directs the authors' attention to a crucial aspect of their work that needs improvement. Therefore, it aligns with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. Some of the details of the models are missing. In particular the grammar over kernels is not explained in any detail, making it hard to understand how this approach is applied in practice. Presumably there are also probabilities associated with the grammar that define a hypothesis space of kernels? How is inference performed?\n",
      "Paper ID: NIPS_2016_417\n",
      "Venue: NIPS_2016\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point identifies missing details about the models, specifically mentioning the lack of explanation regarding the grammar over kernels. It raises questions about the application of this approach in practice and inquires about the probabilities associated with the grammar and how inference is performed. While the comment highlights areas that need clarification, it does not provide explicit instructions on how to address these issues. The authors can infer that they need to add more detailed explanations, but the feedback lacks concrete guidance on what specific information should be included. Therefore, the comment is somewhat actionable as it implies actions but remains vague on execution.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is weakly grounded as it does not specify a particular section or figure in the paper, making it difficult for the authors to pinpoint exactly where the issues lie. However, it is specific in detailing what is missing, particularly the explanation of the grammar over kernels and the associated probabilities, as well as how inference is performed. This provides clear guidance on what needs to be addressed, even though the exact location in the paper is not identified. Therefore, the comment is weakly grounded and specific, aligning with label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 3\n",
      "Rationale: The comment claims that some details of the models are missing, specifically mentioning the lack of explanation regarding the grammar over kernels. The reviewer questions how this approach is applied in practice and raises additional questions about probabilities and inference. While the comment identifies specific areas that are unclear, it does not provide any supporting evidence or references to substantiate the claim or suggest how these details could be addressed. This makes the comment somewhat verifiable, as it highlights a gap but lacks detailed justification or examples to fully support the claim. Therefore, the comment is categorized as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific area where the paper lacks detail, particularly concerning the grammar over kernels. It highlights that this omission makes it difficult to understand the practical application of the approach. The comment also raises questions about the probabilities associated with the grammar and how inference is performed, suggesting that these are important aspects that need clarification. While the comment effectively points out areas for improvement, it does not provide specific suggestions or guidance on how to address these issues. This limits its helpfulness, as the authors are informed of the problem but not given a clear path to resolve it. Therefore, the comment is somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The analysis of experimental results is insufficient. For instance, the authors only mention that the scope prompting method shows poor performance on GPT-3.5-turbo, but they do not provide any analysis of the underlying reasons behind this outcome.\n",
      "Paper ID: vexCLJO7vo\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment points out that the analysis of experimental results is insufficient, specifically noting that the authors mention the poor performance of the scope prompting method on GPT-3.5-turbo without analyzing the reasons behind this outcome. While the comment identifies a clear area for improvement—providing an analysis of the underlying reasons—it does not offer specific guidance on how to conduct this analysis or what aspects to focus on. The action is explicit but somewhat vague, as the authors know they need to perform additional analysis but may not be entirely sure of the exact steps to take. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a specific issue with the analysis of experimental results, particularly regarding the scope prompting method's performance on GPT-3.5-turbo. However, it does not explicitly mention which section of the paper this issue is discussed in, making it weakly grounded. The comment is specific in pointing out the lack of analysis on the reasons behind the poor performance, providing clear guidance on what needs to be addressed. Therefore, the comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the analysis of experimental results is insufficient, specifically highlighting the lack of analysis regarding the poor performance of the scope prompting method on GPT-3.5-turbo. While the comment identifies a specific area where analysis is lacking, it does not provide any reasoning or evidence to support why this analysis is necessary or what potential factors could be explored. This makes the claim somewhat verifiable, as it points out a gap but does not offer detailed guidance or references to help the authors address the issue. Therefore, the comment is rated as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The comment identifies a specific weakness in the paper, namely the insufficient analysis of experimental results. It highlights that the authors mention the poor performance of the scope prompting method on GPT-3.5-turbo but fail to provide an analysis of the reasons behind this outcome. This feedback is somewhat helpful as it points out a critical area that needs improvement, prompting the authors to delve deeper into their analysis. However, the comment could be more helpful if it offered suggestions on how to conduct this analysis or what aspects to consider, providing more detailed guidance for the authors. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The approach section is missing in the main paper. The reviewer did go through the “parallelization descriptions” in the supplementary material but the supplementary should be used more like additional information and not as an extension to the paper as it is. Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In International Conference on Learning Representations, 2021. Update: Please see my comment below. I have increased the score from 3 to 5.\n",
      "Paper ID: NIPS_2021_386\n",
      "Venue: NIPS_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment points out that the approach section is missing from the main paper and suggests that the supplementary material should not serve as an extension to the paper. However, it does not provide specific guidance on how to integrate the approach section into the main paper or what specific content should be included. The comment implies that the authors should move or incorporate the approach section into the main paper, but it lacks explicit instructions or concrete details on how to do so. This makes the action implicit and somewhat vague, leaving the authors without clear steps to follow. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment is fully grounded as it explicitly mentions the \"approach section\" and the \"parallelization descriptions\" in the supplementary material, allowing the authors to accurately identify the parts of the paper being addressed. It also provides specific feedback on the use of supplementary material, suggesting that it should serve as additional information rather than an extension of the main paper. However, the comment does not specify what exactly is missing from the approach section or how it should be integrated into the main paper. Despite this, the mention of specific sections and the clear guidance on the role of supplementary material make the comment fully grounded and somewhat specific. Therefore, this comment aligns with a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the approach section is missing from the main paper and criticizes the reliance on supplementary material for essential content. The reviewer references specific authors and a paper from the International Conference on Learning Representations, which adds some credibility to their perspective. However, the comment does not provide detailed reasoning or examples of how the supplementary material is being used improperly or how it affects the paper's comprehensibility. The mention of an update and score change suggests some additional context or reasoning might be provided elsewhere, but within this comment, the claim lacks sufficient justification. Therefore, it is somewhat verifiable, aligning with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment highlights a significant structural issue with the paper, noting the absence of the approach section in the main document. It emphasizes that supplementary material should serve as additional information rather than an extension of the main paper. This feedback is crucial as it addresses a fundamental aspect of the paper's organization, guiding the authors to ensure that essential content is included in the main text. However, the comment does not provide specific suggestions on how to integrate the missing section or improve the overall structure. Despite this, the feedback is mostly helpful as it directs the authors' attention to a critical issue that needs resolution to enhance the paper's coherence and completeness. Therefore, the comment is rated as mostly helpful, consistent with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works. Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance?\n",
      "Paper ID: NIPS_2021_1743\n",
      "Venue: NIPS_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point addresses two main issues. First, it suggests that the comparison with Megatron might be overstated, noting that the performance of Megatron and COCO-LM is similar to other models like RoBERTa, ELECTRA, and DeBERTa. It implies that if COCO-LM is claimed to be parameter-efficient, this conclusion could also apply to these other models. However, this part of the comment lacks explicit guidance on what the authors should do with this information, making it somewhat vague. Second, the reviewer asks a question about the experimental setup, specifically why the authors switched the types of BPE vocabulary and whether this change could affect performance variance. This question is explicit and prompts the authors to consider and potentially address this aspect of their methodology. Overall, the comment contains an implicit suggestion regarding the comparison and an explicit question about the experimental setup, making it somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the comparison with Megatron and questions the claim of parameter efficiency for COCO-LM, suggesting that similar claims could be made for other models like RoBERTa, ELECTRA, and DeBERTa. It also raises a question about the experimental setup regarding the switch in BPE vocabulary types. However, the comment does not explicitly mention specific sections or figures, making it weakly grounded. The authors might infer that it pertains to sections discussing comparisons and experimental setups, but the lack of explicit references makes it challenging to pinpoint the exact parts needing attention. The comment is specific in detailing the issues with the comparison and the experimental setup, but the lack of grounding reduces its effectiveness. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point questions the comparison with Megatron, suggesting that the performance of Megatron and COCO-LM is similar to other models like RoBERTa, ELECTRA, and DeBERTa. The reviewer argues that if COCO-LM is claimed to be parameter-efficient, this conclusion should also apply to these other models. This claim is somewhat verifiable as it provides a logical argument based on the comparison of model sizes and performance. However, it lacks specific performance metrics or detailed evidence to fully substantiate the claim. The comment could be strengthened by providing explicit data or references to support the comparison, making it more verifiable. Therefore, it is categorized as 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment addresses two main points. First, it critiques the comparison with Megatron, suggesting that the performance of Megatron and COCO-LM is similar to other models like RoBERTa, ELECTRA, and DeBERTa, which are of similar sizes. This observation challenges the uniqueness of the claim regarding COCO-LM's parameter efficiency, prompting the authors to reconsider or clarify their claims. Second, the comment raises a question about the experimental setup, specifically the switch between uncased and cased BPE vocabulary, and whether this change could affect performance variance. This question encourages the authors to provide additional details or justification for their experimental choices. While the comment identifies areas for clarification and improvement, it could be more helpful by offering specific suggestions on how to address these issues. Overall, the feedback is somewhat helpful as it points out areas for further explanation and consideration, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The evaluation is limited, mostly relying on 4 OCR QA datasets. As the authors admit in Fig 4(5), this evaluation may be unreliable. More scenarios like the LLaVA benchmark would be expected, especially in ablation studies.\n",
      "Paper ID: tj4a1JY03u\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment points out that the evaluation is limited, primarily relying on four OCR QA datasets, and notes that this may be unreliable according to the authors' admission in Fig 4(5). It suggests that more scenarios, like the LLaVA benchmark, would be expected, particularly in ablation studies. While the comment implies that the authors should expand their evaluation to include more scenarios and benchmarks, it does not provide specific guidance on how to do so or which additional scenarios should be prioritized. The action is implicit and somewhat vague, as it lacks detailed instructions on implementation. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment mentions \"Fig 4(5)\" and the reliance on \"4 OCR QA datasets,\" which provides some grounding as it points to specific elements of the paper. However, it does not explicitly state which section or part of the evaluation is being critiqued, leaving the authors to infer that it relates to the evaluation section. The comment is specific in suggesting the inclusion of more scenarios like the LLaVA benchmark, especially in ablation studies, to improve the evaluation. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the evaluation is limited due to its reliance on only four OCR QA datasets, and it suggests that this limitation may lead to unreliable results, as admitted by the authors in Fig 4(5). The comment further recommends including more scenarios, such as the LLaVA benchmark, particularly in ablation studies. While the claim is somewhat supported by the mention of the authors' own admission of potential unreliability, it lacks specific examples or references to substantiate the need for additional scenarios or benchmarks. This makes the claim somewhat verifiable, as it provides a basis for the critique but requires more detailed evidence or examples to fully validate the argument.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a limitation in the evaluation section, noting that it primarily relies on four OCR QA datasets and acknowledges the authors' admission of potential unreliability in this evaluation. It suggests expanding the evaluation to include more scenarios, such as the LLaVA benchmark, particularly in ablation studies. This feedback is somewhat helpful as it points out a specific area for improvement and provides a concrete example of what could be included to enhance the evaluation's robustness. However, the comment could be more helpful by offering additional guidance or suggestions on how to effectively incorporate these scenarios or what specific aspects of the evaluation need strengthening. Overall, the comment is somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.\n",
      "Paper ID: Pb1DhkTVLZ\n",
      "Venue: EMNLP_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 2.0\n",
      "Rationale: The review point highlights a limitation in the evaluative framework, noting that it is restricted to three Question-Answering tasks and two language models. It raises concerns about the method's broader applicability and its potential to generalize to other reasoning or generation tasks or more advanced models like vicunna or alpaca. However, the comment does not provide explicit guidance or suggestions on how the authors could address these concerns or expand the scope of their framework. The feedback lacks concrete actions or specific recommendations, leaving the authors uncertain about how to proceed. As a result, the comment is barely actionable, as it identifies an issue but does not offer detailed guidance on how to resolve it.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the evaluative framework of the paper, noting its limited scope due to the consideration of only three Question-Answering tasks and two language models. It raises concerns about the method's broader applicability and its potential to generalize to other tasks or more advanced models. However, the comment does not explicitly mention which part of the paper this issue is discussed in, making it weakly grounded. The authors can infer that it relates to the sections discussing the evaluative framework or methodology, but this inference is not direct. The comment is specific in pointing out the limitations and suggesting areas for potential expansion, such as considering more tasks or models. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point claims that the evaluative framework is limited in scope, as it only considers three Question-Answering tasks and two language models. This raises concerns about the method's broader applicability. While the comment provides some reasoning by highlighting the limited number of tasks and models, it lacks specific examples or references to support the claim further. The authors might need to make a significant effort to understand and address the concern about generalization to other tasks or models. Therefore, the claim is somewhat verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a limitation in the evaluative framework, noting that it is restricted to only three Question-Answering tasks and two language models. It raises a valid concern about the broader applicability of the method to other reasoning or generation tasks and more advanced models. This feedback is somewhat helpful as it highlights an area for improvement and prompts the authors to consider expanding their evaluation to enhance the generalizability of their findings. However, the comment lacks specific suggestions or guidance on how to address this limitation, such as recommending additional tasks or models to include in the evaluation. Providing more detailed advice would make the feedback more actionable and valuable for the authors. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The expected counterfactual violates $\\mathcal{P}_2$ stated in Definition 1.\n",
      "Paper ID: 9TpgFnRJ1y\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 2.0 | GPT Label: 1.0\n",
      "Rationale: The review point states that \"the expected counterfactual violates $\\mathcal{P}_2$ stated in Definition 1.\" However, it does not provide any explicit or implicit action for the authors to take. It lacks guidance on whether this violation is a concern, if it needs addressing, or how the authors might investigate or rectify it. Without any actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The comment is fully grounded as it explicitly refers to \"Definition 1,\" allowing the authors to accurately identify the specific part of the paper being addressed. However, the comment lacks specificity because it does not explain how the expected counterfactual violates $\\mathcal{P}_2$ or provide any details on what needs to be addressed or corrected. Without this information, the authors may struggle to understand the exact issue or how to resolve it. Therefore, this comment is fully grounded but under-specific, corresponding to a score of 4.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 1\n",
      "Rationale: The review point claims that \"the expected counterfactual violates $\\mathcal{P}_2$ stated in Definition 1.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why this violation occurs. Without additional context or explanation, the authors may find it challenging to understand and address the issue. The lack of detailed justification or examples makes it difficult for the authors to follow the reasoning behind the claim. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 2.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific issue with the expected counterfactual violating a condition ($\\mathcal{P}_2$) stated in Definition 1. This is a critical observation that the authors need to address to ensure the validity of their theoretical framework. However, the comment lacks detailed guidance or suggestions on how the authors might resolve this violation or what implications it has for the rest of the paper. Without actionable feedback or specific advice on addressing the issue, the comment provides limited value to the authors in terms of improving their draft. Therefore, it aligns with a score of 3, indicating that the comment is somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. The model description could be improved, e.g., the generative process is in detail but presenting such process in separate steps should be better for understanding, too many symbols and a notation table could be better.\n",
      "Paper ID: NIPS_2020_309\n",
      "Venue: NIPS_2020\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment suggests that the model description could be improved by presenting the generative process in separate steps for better understanding and by providing a notation table due to the presence of too many symbols. While the comment explicitly states actions that the authors can take to enhance clarity, such as breaking down the generative process and including a notation table, it does not provide detailed guidance on how to implement these changes. The actions are explicit but somewhat vague in terms of execution, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the model description, suggesting improvements such as presenting the generative process in separate steps and including a notation table to enhance understanding. However, it does not specify which part of the paper contains the model description, leaving the authors to infer the relevant section. This makes the comment weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. Nonetheless, the comment is specific in detailing what could be improved in the model description. Therefore, this comment is weakly grounded and specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point suggests that the model description could be improved by presenting the generative process in separate steps and including a notation table. While the comment provides some guidance on how to enhance clarity, it lacks specific examples or detailed reasoning to support why these changes would be beneficial. The suggestion is somewhat verifiable, as it offers a general direction for improvement but requires additional detail for the authors to fully understand and implement the feedback effectively.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment identifies a specific area for improvement in the model description, suggesting that the generative process should be presented in separate steps for better understanding. It also notes the presence of too many symbols and recommends the inclusion of a notation table. This feedback is clear and actionable, providing the authors with concrete steps to enhance the clarity and comprehensibility of their model description. However, the comment could be more helpful if it provided examples or further guidance on how to effectively implement these suggestions. Overall, the comment is mostly helpful, as it directs the authors toward meaningful improvements in their draft.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. Token reverse as the inverse of orthogonal matrix is easy to get, just the transpose of the matrix. Step 2 can be done regardless of the weight matrix of this local window MLP is orthogonal or not. Step 3 is the vital part that only orthogonal matrix weight can perform, I believe this should be studied, which is not presented, for validating the essentialness of using orthogonal matrix rather than just following the form that connects local and connects beyond local windows.\n",
      "Paper ID: NIPS_2022_2617\n",
      "Venue: NIPS_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review point provides some observations about the process of token reversal and the role of orthogonal matrices. It suggests that Step 3, which involves the use of orthogonal matrix weights, is vital and should be studied to validate its essentialness. However, the comment does not explicitly instruct the authors to conduct this study or provide concrete guidance on how to approach it. The action is implicit and vague, leaving the authors uncertain about how to proceed with the suggested validation. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment provides specific feedback on the methodology, particularly focusing on the use of orthogonal matrices and the steps involved in the process. It highlights the importance of studying Step 3, which is vital for validating the use of orthogonal matrices. However, the comment does not explicitly mention which part of the paper these steps are discussed in, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where these topics are discussed, the comment lacks full grounding. It is specific about the issue of orthogonal matrices but does not provide detailed guidance on how to address it. Therefore, this comment is weakly grounded and specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point discusses the process of token reversal as the inverse of an orthogonal matrix and highlights the importance of Step 3, which involves the use of an orthogonal matrix weight. The reviewer suggests that this aspect should be studied further to validate the necessity of using an orthogonal matrix. While the comment provides some logical reasoning about the process and the significance of Step 3, it lacks specific examples or references to substantiate the claim fully. The explanation is somewhat detailed but could benefit from additional evidence or references to make the claim more robust. Therefore, the comment is classified as \"3: Somewhat Verifiable,\" as it provides a basis for the claim but requires more comprehensive support to be fully verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment provides a detailed critique of the methodology, specifically focusing on the use of orthogonal matrices. It identifies a key area that requires further study—validating the essentialness of using orthogonal matrices in the context of the paper's methodology. The comment explains the steps involved and highlights the importance of Step 3, which is specific to orthogonal matrices. However, while it points out a significant area for improvement, it does not offer specific suggestions or guidance on how the authors might conduct this study or what aspects should be explored. The feedback is somewhat helpful as it provides insight into a potential gap in the research, but it could be more beneficial with additional guidance on how to address the identified issue. Therefore, the comment is rated as somewhat helpful.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3. While consistency training might usually be deployed on unlabeled data, I wonder if it would be beneficial to utilize labeled data for consistency training as well. Specifically, labeled data has exact labels, which might provide effective information for consistency training the model in dealing with the taks of graph anomaly detection. [a] Graph Contrastive Learning Automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang. ICML 2021 [b] Graph Contrastive Learning with Adaptive Augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang. WWW 2021.\n",
      "Paper ID: elMKXvhhQ9\n",
      "Venue: ICLR_2024\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The comment raises a question about the potential benefits of using labeled data for consistency training in the context of graph anomaly detection. It suggests that labeled data, with its exact labels, might provide effective information for this purpose. However, the comment does not explicitly instruct the authors to explore this idea or provide concrete steps on how to implement it. The action is implicit and somewhat vague, as it leaves the authors to infer the potential benefits and how to incorporate labeled data into their consistency training. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests considering the use of labeled data for consistency training in the context of graph anomaly detection. It provides specific references to external works ([a] and [b]) that might be relevant, which adds specificity to the suggestion. However, the comment does not explicitly mention which part of the paper it addresses, such as specific sections or experiments where consistency training is discussed. The authors can infer that it relates to the methodology or results sections, but this inference is not as direct as it could be. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point suggests that labeled data might be beneficial for consistency training in graph anomaly detection, which is typically done on unlabeled data. The reviewer supports this claim by referencing two external works: \"Graph Contrastive Learning Automated\" by Yuning You et al., and \"Graph Contrastive Learning with Adaptive Augmentation\" by Yanqiao Zhu et al. These references provide a basis for the suggestion, indicating that the use of labeled data could enhance the model's performance. The inclusion of specific references makes the claim mostly verifiable, as it provides a logical rationale but could benefit from more detailed explanation or examples to be fully verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment suggests a potential improvement by questioning whether consistency training could benefit from using labeled data in addition to unlabeled data. It provides a rationale for this suggestion, noting that labeled data with exact labels might offer effective information for consistency training in the context of graph anomaly detection. Additionally, the comment references two relevant papers, which could guide the authors in exploring this idea further. While the comment is insightful and offers a clear direction for improvement, it could be more helpful by providing specific examples or methodologies for implementing this suggestion. Overall, the feedback is mostly helpful as it provides actionable insights and references, but it could be expanded with more detailed guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "3.In the experiment, the author didn’t consider Vision Transformer, which is an important SOTA model in image classification. And it is unsure if such technique is still working for larger image dataset such as ImageNet. Will the pruning strategy will be different in self attention layers?\n",
      "Paper ID: ICLR_2022_1905\n",
      "Venue: ICLR_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point suggests that the authors should consider including Vision Transformer in their experiments, as it is an important state-of-the-art model in image classification. It also raises a question about the applicability of the technique to larger datasets like ImageNet and whether the pruning strategy would differ in self-attention layers. While the comment implies actions—such as including Vision Transformer and addressing the applicability to larger datasets—it does not provide specific guidance on how to implement these actions or what specific aspects to focus on. The actions are implicit and somewhat vague, making the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the experiment section, specifically noting the omission of the Vision Transformer, a state-of-the-art model in image classification. It raises a question about the applicability of the technique to larger datasets like ImageNet and whether the pruning strategy would differ in self-attention layers. While the comment does not explicitly mention a specific section, the authors can infer that it relates to the experimental setup or results. The comment is specific in highlighting the omission and posing relevant questions, but it is weakly grounded as it does not provide precise guidance on where in the paper these issues should be addressed. Therefore, this comment is weakly grounded and specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 2\n",
      "Rationale: The review point raises a claim that the experiment did not consider the Vision Transformer, an important state-of-the-art model in image classification, and questions the applicability of the technique to larger datasets like ImageNet. While the comment highlights a potential gap in the experimental setup, it does not provide specific reasoning or evidence to support why the inclusion of Vision Transformer is crucial or how it might impact the results. Additionally, the question about the pruning strategy in self-attention layers is posed without further elaboration. The lack of detailed justification or examples makes it challenging for the authors to fully understand and address the claim, resulting in a score of 2: Borderline Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a notable omission in the experimental setup by pointing out that the Vision Transformer, a state-of-the-art model in image classification, was not considered. It raises a valid concern about the applicability of the technique to larger datasets like ImageNet and questions whether the pruning strategy would differ in self-attention layers. This feedback is somewhat helpful as it highlights areas for potential improvement and encourages the authors to consider broader applicability and robustness of their method. However, the comment lacks specific suggestions or guidance on how to incorporate Vision Transformer into the experiments or how to address the concerns raised. Providing more detailed advice or examples would enhance the comment's utility for the authors. Therefore, the comment is rated as somewhat helpful, consistent with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "34 \"to force the neural network to memorize them\" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact \"critical point\" as such in TopoNet [24]. Minor: I find the method section to be a bit wordy, it could be compressed on the essential definitions. There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l.\n",
      "Paper ID: NIPS_2022_285\n",
      "Venue: NIPS_2022\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review point contains multiple pieces of feedback. Firstly, it suggests toning down the statement \"to force the neural network to memorize them,\" as the reviewer believes that the neural network does not memorize an exact \"critical point\" as described in TopoNet [24]. This is an explicit and concrete suggestion, as it directly instructs the authors to revise the language used. Secondly, the reviewer mentions that the method section is a bit wordy and could be compressed to focus on essential definitions. This is a somewhat actionable suggestion, as it provides a general direction but lacks specific guidance on which parts to compress. Lastly, the reviewer points out the existence of several grammatical errors and advises double-checking these, focusing on plurals and articles. This is an explicit and concrete action, as it clearly instructs the authors to review and correct grammatical issues. Overall, the review point provides a mix of explicit and somewhat actionable feedback, making it mostly actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is partially grounded as it mentions specific feedback related to line 34, suggesting a revision of the statement about the neural network's memorization. This allows the authors to identify the specific part of the paper being addressed. However, the subsequent feedback about the method section being wordy and containing grammatical errors is not grounded, as it does not specify which parts of the section are wordy or contain errors. The comment is also under-specific in these areas, as it does not provide detailed guidance on how to compress the section or which grammatical errors to correct. Therefore, the comment is partially grounded and under-specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 1\n",
      "Rationale: The review point suggests that the statement \"to force the neural network to memorize them\" should be toned down, as the reviewer believes that the neural network does not memorize an exact \"critical point\" as described in TopoNet [24]. However, the comment does not provide specific reasoning or evidence to support this claim, such as explaining why the statement might be inaccurate or referencing additional sources that clarify the concept. The lack of detailed justification makes the claim unverifiable. Additionally, the comment includes minor suggestions about the method section being wordy and grammatical errors, but these do not contribute to verifying the main claim. Therefore, the review point is rated as \"1: Unverifiable.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment provides several pieces of feedback that can help the authors improve their draft. First, it suggests toning down a specific statement about the neural network's behavior, which can help refine the paper's claims. Second, it notes that the method section is wordy and could be compressed, offering a direction for improving clarity and conciseness. Lastly, it points out the presence of grammatical errors and advises a review focusing on plurals and articles. While the comment identifies multiple areas for improvement, it could be more helpful by providing specific examples or suggestions for rephrasing or compressing the text. Overall, the comment is mostly helpful as it highlights key areas for refinement and offers some guidance, aligning with a score of 4.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.\n",
      "Paper ID: ACL_2017_318_review\n",
      "Venue: ACL_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment implies that the authors should provide empirical evidence to support the hypothesis that the proposed models are useful for learning representations for low-frequency words. However, it does not explicitly instruct the authors to conduct specific experiments or analyses to test this hypothesis. The suggestion is implicit and lacks concrete details on how to implement the action, leaving the authors with a general idea but no specific guidance on how to proceed. Therefore, this comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 5.0\n",
      "Rationale: The comment is fully grounded as it explicitly refers to the argument made about the proposed models being useful for learning representations for low-frequency words. This allows the authors to accurately identify the part of the paper being addressed. The comment is also specific because it points out the lack of empirical evidence to support the hypothesis and suggests that the authors should explore this aspect further. Additionally, it provides a critique regarding the explanation of improvements in relation to word similarity datasets. Therefore, this comment is fully grounded and specific, corresponding to label 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point acknowledges a reasonable argument made by the authors regarding the usefulness of the proposed models for learning representations of low-frequency words. However, it highlights the lack of empirical evidence to support this hypothesis. The comment suggests that further investigation into this aspect would be beneficial. While the review provides logical reasoning by pointing out the absence of empirical evidence, it does not offer specific examples or references to strengthen the claim. Therefore, the comment is somewhat verifiable, as it requires additional detail or evidence to fully substantiate the claim. Hence, it aligns with a score of 3: Somewhat Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a gap in the paper regarding the lack of empirical evidence to support the claim that the proposed models are particularly useful for learning representations of low-frequency words. It suggests that the authors should investigate this hypothesis further, providing a clear direction for additional research and analysis. However, while the comment highlights an important area for improvement, it does not offer specific guidance or suggestions on how the authors might conduct this investigation or what kind of empirical evidence would be most compelling. This limits the comment's helpfulness, as it provides insight but lacks detailed, actionable advice. Therefore, it is rated as somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "4. Adding it at the very end is a bit counterintuitive. If I understand well the authors apply Alg 1 either with one iteration T=1 or with two iterations T=2? What happens for larger T? Usually iterative algorithms run until some criterion is fulfilled, with T >> 2. â¢ Line 224: no reference to Laplacian eigenmaps, and was not cited either in the introduction. â¢ Fig.\n",
      "Paper ID: NIPS_2019_1200\n",
      "Venue: NIPS_2019\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review point raises several issues but lacks explicit guidance on how to address them. It questions the placement of a section, the application of an algorithm with specific iterations, and the absence of a reference to Laplacian eigenmaps. However, it does not provide concrete suggestions or actions for the authors to take. The comment is implicit and vague, leaving the authors uncertain about how to proceed. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The comment addresses multiple issues, but it lacks clarity and specificity. It mentions \"Alg 1\" and \"Line 224,\" which provides some grounding, but it is not fully grounded because it does not clearly specify the exact parts of the paper being addressed. The comment raises questions about the application of \"Alg 1\" with different iterations and the absence of a reference to Laplacian eigenmaps, but it does not provide specific guidance or suggestions for improvement. The mention of \"Fig.\" is incomplete and does not contribute to grounding or specificity. Overall, the comment is weakly grounded and not specific, aligning with a score of 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point raises questions about the application of an algorithm with a limited number of iterations (T=1 or T=2) and suggests that iterative algorithms typically run until a criterion is met, often with T much greater than 2. However, the comment lacks specific references or examples to substantiate the claim about typical iterative algorithm practices. Additionally, it mentions a missing reference to Laplacian eigenmaps but does not provide further context or justification. The lack of detailed evidence or references makes the claim somewhat verifiable, as the authors would need to infer the reasoning and seek additional information themselves.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment raises several points, starting with a critique of the placement of a particular section, suggesting it is counterintuitive. It questions the application of Algorithm 1 with a limited number of iterations (T=1 or T=2) and inquires about the behavior for larger T, highlighting a potential gap in the explanation of the iterative process. Additionally, it points out a missing reference to Laplacian eigenmaps, which could be crucial for understanding the context or background of the work. While the comment identifies areas for improvement, it lacks specific suggestions or guidance on how to address these issues, such as how to better integrate the section or what specific information should be included about Laplacian eigenmaps. This limits the comment's helpfulness, as it provides some insight but lacks depth and actionable advice. Therefore, the comment is somewhat helpful, aligning with a score of 3.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "4. Besides norm, is there any other property of features can be used? It is necessary and helpful for your approach design.\n",
      "Paper ID: ICLR_2023_4236\n",
      "Venue: ICLR_2023\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 2.0\n",
      "Rationale: The review point raises a question about whether there are other properties of features, besides norm, that can be used, suggesting that exploring these could be necessary and helpful for the approach design. However, it does not provide explicit guidance or suggestions on what other properties to consider or how to incorporate them into the approach. The action is implicit and vague, leaving the authors uncertain about how to address the feedback effectively. Without specific recommendations or examples, the authors may struggle to determine what changes, if any, are needed to explore additional feature properties. Therefore, this comment is barely actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment raises a question about the properties of features that can be used in the approach design, specifically mentioning \"norm.\" However, it does not specify which part of the paper discusses the use of feature properties, making it difficult for the authors to pinpoint the exact section that needs attention. Additionally, the comment lacks specificity regarding what other properties could be considered or how they would be beneficial. As a result, the comment is weakly grounded because the authors cannot confidently determine the part of the paper being addressed, and it is not specific in detailing what needs to be addressed. Therefore, this comment aligns with category 2.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 3 | GPT Label: 1\n",
      "Rationale: The review point poses a question about whether there are other properties of features, besides norm, that could be used, suggesting that considering additional properties might be necessary and helpful for the approach design. However, the comment does not provide any specific examples, references, or detailed reasoning to support the implication that other properties could be beneficial. The lack of justification or evidence makes it difficult for the authors to understand the basis of the suggestion or how it could improve their work. Therefore, the claim is not verifiable due to the absence of supporting information, making it challenging for the authors to act on the feedback. This aligns with a score of 1: Unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 3.0 | GPT Label: 2.0\n",
      "Rationale: The comment raises a question about the potential use of other properties of features besides norm, suggesting that exploring these could be beneficial for the approach design. However, it lacks specificity and does not provide examples or guidance on what other properties might be relevant or how they could be integrated into the approach. While it hints at a possible area for improvement, the lack of detailed suggestions or actionable advice limits its usefulness. Therefore, the comment is rated as barely helpful, as it provides a general direction but lacks depth and clarity.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "4. I read the paper in detail. The fact that their theory does not seem to be applicable to the used model, is not honestly mentioned in the limitations. To the contrary, the vagueness of unspecified 'structural assumptions', that are only given in the appendix, makes this theoretical limitation hard to find. I think the authors underestimate the current use of graph neural networks in industry. They are used widely. As such, some more elaboration on potential negative societal impact of graph neural networks in general could be given.\n",
      "Paper ID: NIPS_2021_1731\n",
      "Venue: NIPS_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point raises several issues: the theory's applicability to the model is not clearly mentioned in the limitations, the vagueness of unspecified 'structural assumptions' in the appendix, and the need for more elaboration on the potential negative societal impact of graph neural networks. While the comment identifies areas for improvement, it does not provide explicit guidance on how to address these issues. The authors are left to infer that they should clarify the limitations, specify the structural assumptions, and discuss societal impacts. However, the lack of concrete instructions makes the comment somewhat vague. Therefore, the comment is somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment is partially grounded as it refers to the limitations section and the appendix, allowing the authors to identify the general area of concern. However, it does not specify exactly where in the limitations or appendix the issues lie, making it weakly grounded. The comment is specific in pointing out the need for more honesty about the applicability of the theory to the model, the vagueness of unspecified 'structural assumptions,' and the need for elaboration on the societal impact of graph neural networks. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 3\n",
      "Rationale: The review point raises a claim that the paper's theory is not applicable to the model used, and this is not clearly mentioned in the limitations. The reviewer supports this claim by pointing out the vagueness of unspecified 'structural assumptions' that are only detailed in the appendix, making the limitation difficult to identify. Additionally, the reviewer suggests that the authors underestimate the use of graph neural networks in the industry and recommends discussing potential negative societal impacts. While the comment provides some reasoning regarding the lack of clarity in the limitations, it lacks specific examples or references to fully substantiate the claim about the theory's applicability and the societal impact. Therefore, the claim is somewhat verifiable, as it requires more detailed evidence or examples to fully validate the argument.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a significant issue with the paper, noting that the theory presented does not seem applicable to the model used, and this is not transparently mentioned in the limitations section. It highlights the problem of vague 'structural assumptions' that are only detailed in the appendix, making it difficult for readers to grasp the theoretical limitations. Additionally, the comment suggests that the authors may underestimate the use of graph neural networks in the industry and recommends elaborating on their potential negative societal impacts. While the comment effectively points out critical areas for improvement, it lacks specific suggestions on how to address these issues or enhance the clarity and transparency of the paper. Therefore, the feedback is somewhat helpful, as it provides valuable insights but could be more actionable with detailed guidance.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "5.In experimental section, authors only compared with two baselines, there’re several works also focus on the same questions, for example [1,2,3], so it’s suggested to add more experimental to show the effectiveness of proposed method.\n",
      "Paper ID: OBUQNASaWw\n",
      "Venue: ICLR_2025\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review point suggests that the authors should include more experimental comparisons with additional works that focus on the same questions, specifically mentioning references [1,2,3]. This implies a direct action for the authors to take, which is to expand their experimental section by comparing their method with these additional baselines. However, the comment does not provide specific guidance on how to conduct these comparisons or what aspects to focus on, making the action somewhat vague. Therefore, while the action is explicit, the lack of detailed instructions on how to implement it makes the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses the experimental section, indicating that the authors have only compared their work with two baselines. It suggests adding more experiments to demonstrate the effectiveness of the proposed method, referencing other works that focus on the same questions. While the comment does not explicitly mention specific parts of the experimental section, the authors can infer that it pertains to the comparison of baselines. The suggestion is specific, as it outlines a particular improvement to enhance the paper. Therefore, the comment is weakly grounded but specific, aligning with the label 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 4\n",
      "Rationale: The review point suggests that the experimental section is limited because it only compares the proposed method with two baselines. The reviewer supports this claim by referencing several other works that focus on the same questions, specifically citing [1,2,3]. This provides a clear indication that there are additional relevant studies that could be included for a more comprehensive comparison. The suggestion to add more experiments is well-supported by these references, making the claim mostly verifiable. However, the comment could be further strengthened by providing more detailed reasoning or examples from the cited works. Therefore, the score is 4: Mostly Verifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a limitation in the experimental section, specifically noting that the authors have only compared their work with two baselines. It suggests that there are several other works focusing on the same questions and recommends adding more experiments to demonstrate the effectiveness of the proposed method. While the comment highlights an area for improvement, it lacks specific guidance on which additional baselines to include or how to structure the additional experiments. This leaves the authors with a general idea of what needs to be improved but without detailed direction on how to achieve it. Therefore, the comment is somewhat helpful, as it provides some insight but lacks depth and specificity.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "6) The authors discuss the misestimation of mu, but as it is the proportion of missing observations - it is not wholly clear how it can be estimated at all.\n",
      "Paper ID: NIPS_2016_232\n",
      "Venue: NIPS_2016\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 1.0\n",
      "Rationale: The review point highlights a potential issue with the discussion on the misestimation of mu, questioning how it can be estimated given that it represents the proportion of missing observations. However, the comment does not provide any explicit or implicit suggestions for how the authors might address this issue or improve their discussion. There is no guidance on potential modifications, clarifications, or additional explanations that could be added to make the estimation process clearer. As a result, the authors are left without any actionable steps to take in response to this feedback. Therefore, the comment is rated as unactionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 3.0 | GPT Label: 5.0\n",
      "Rationale: The comment explicitly mentions the discussion of the \"misestimation of mu,\" which allows the authors to identify the specific part of the paper being addressed, providing full grounding. It also specifies the issue by questioning the clarity of estimating mu, given that it represents the proportion of missing observations. This provides clear guidance on what aspect of the discussion is problematic and needs clarification. Therefore, the comment is fully grounded and specific, aligning with category 5.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 4 | GPT Label: 1\n",
      "Rationale: The review point questions the authors' discussion on the misestimation of \"mu,\" which is described as the proportion of missing observations. The comment implies that estimating \"mu\" is unclear, but it does not provide any supporting evidence, reasoning, or examples to justify why this estimation is problematic or unclear. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a specific issue with the discussion of the misestimation of \"mu,\" which is described as the proportion of missing observations. The reviewer points out a lack of clarity in how \"mu\" can be estimated, suggesting that the authors need to provide more information or clarification on this point. While the comment highlights an important area for improvement, it lacks detailed guidance or suggestions on how the authors might address this issue. To be more helpful, the comment could include specific questions or propose methods for estimating \"mu.\" As it stands, the feedback is somewhat helpful, as it directs the authors to a critical area needing clarification but does not provide comprehensive guidance for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "8. A section on synonym identification is missing under similarity measurement that would describe how the multiple-choice task is approached.\n",
      "Paper ID: ACL_2017_792_review\n",
      "Venue: ACL_2017\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 4.0 | GPT Label: 3.0\n",
      "Rationale: The review point identifies a missing section on synonym identification under similarity measurement, specifically mentioning that it should describe how the multiple-choice task is approached. This feedback implies that the authors should add this section to their paper. However, the comment does not provide explicit instructions or details on what the section should include or how to structure it. While the action is clear, the lack of concrete guidance on how to implement it makes the comment somewhat actionable.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment suggests that a section on synonym identification is missing under the similarity measurement, specifically related to how the multiple-choice task is approached. While it does not explicitly mention a specific section of the paper, it provides enough context for the authors to infer that it relates to the similarity measurement part of the paper. The comment is specific in its request for a section on synonym identification, providing clear guidance on what needs to be added. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: X | GPT Label: 1\n",
      "Rationale: The review point claims that a section on synonym identification is missing under similarity measurement, which would describe the approach to the multiple-choice task. However, the comment does not provide any supporting evidence, reasoning, or examples to justify why this section is necessary or how its absence affects the paper. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review comment identifies a missing section on synonym identification under similarity measurement, which is crucial for understanding how the multiple-choice task is approached. This feedback highlights a specific gap in the manuscript that the authors need to address to provide a comprehensive understanding of their methodology. However, the comment lacks detailed guidance or suggestions on what the section should include or how to effectively integrate it into the existing content. While it points out an important area for improvement, the feedback could be more actionable and detailed to be fully helpful. Therefore, it is rated as somewhat helpful, as it provides insight but not comprehensive guidance for improvement.\n",
      "\n",
      "========================\n",
      "Review Point:\n",
      "• shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 ) , since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution.\n",
      "Paper ID: ICLR_2021_1740\n",
      "Venue: ICLR_2021\n",
      "\n",
      "--- ACTIONABILITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The review point consists of two parts. The first part highlights a lack of clarity regarding why a specific shift in the shifted-MNIST dataset performs better than another, but it does not provide any actionable guidance on how to address this issue. The second part suggests that it would be useful to show the performance of the model and baselines on test samples from the observational distribution, which is a more concrete suggestion. However, the first part remains vague and unactionable, while the second part is actionable but not explicitly stated. Overall, the comment is somewhat actionable due to the implicit suggestion in the second part.\n",
      "\n",
      "--- GROUNDING_SPECIFICITY ---\n",
      "Human Label: 5.0 | GPT Label: 3.0\n",
      "Rationale: The comment addresses two distinct points. The first point questions the performance difference between \"shift=0\" and \"shift~N(0, σ²)\" in the shifted-MNIST experiment, indicating a need for clarification on why one performs better despite both involving domain shifts. The second point suggests showing model and baseline performance on test samples from the observational distribution. While the comment does not explicitly mention specific sections or figures, the authors can infer that it relates to the experimental results and analysis sections. The comment is specific in its requests for clarification and additional data presentation, but it lacks explicit grounding as it does not refer to specific sections or figures. Therefore, this comment is weakly grounded but specific, aligning with category 3.\n",
      "\n",
      "--- VERIFIABILITY ---\n",
      "Human Label: 5 | GPT Label: 2\n",
      "Rationale: The review point raises a question about the performance difference between shift=0 and shift~N(0, σ²) in the shifted-MNIST experiment, noting that both involve domain shifts. However, it does not provide any reasoning or evidence to explain why this difference is unexpected or problematic. The comment also suggests showing performance on test samples from the observational distribution but does not provide specific examples or references to support this suggestion. The lack of detailed justification or evidence makes the claim borderline verifiable, as it is not fully articulated and may be challenging for the authors to address effectively.\n",
      "\n",
      "--- HELPFULNESS ---\n",
      "Human Label: 5.0 | GPT Label: 4.0\n",
      "Rationale: The review comment addresses two distinct points. First, it questions the results of the shifted-MNIST experiment, specifically why a shift of 0 performs better than a shift drawn from a normal distribution, despite both involving domain shifts. This highlights a potential area of confusion or inconsistency in the results that the authors should clarify. Second, it suggests showing the performance of the model and baselines on test samples from the observational distribution, which is a clear and actionable recommendation for additional analysis. While the comment identifies areas for improvement and provides a specific suggestion, it could be more helpful by offering further details or context on why these points are important. Overall, the feedback is mostly helpful, as it provides actionable insights but could be expanded for greater impact.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "aspect_dfs = dataset\n",
    "aspects = list(aspect_dfs.keys())\n",
    "\n",
    "# Use one DataFrame to get shared metadata\n",
    "base_aspect = aspects[0]\n",
    "base_df = aspect_dfs[base_aspect][['review_point', 'paper_id', 'venue', 'focused_review', 'batch', 'prompt', 'id']].copy()\n",
    "\n",
    "# Get aspect-specific columns\n",
    "aspect_cols = []\n",
    "for aspect, df in aspect_dfs.items():\n",
    "    cols_to_keep = [\n",
    "        'review_point',\n",
    "        f'{aspect}_label',\n",
    "        f'chatgpt_{aspect}_score',\n",
    "        f'chatgpt_{aspect}_rationale'\n",
    "    ]\n",
    "    aspect_cols.append(df[cols_to_keep])\n",
    "\n",
    "# Merge aspect columns on review_point\n",
    "merged_aspects = reduce(lambda left, right: pd.merge(left, right, on='review_point', how='outer'), aspect_cols)\n",
    "\n",
    "# Merge with base metadata\n",
    "merged_df = pd.merge(base_df, merged_aspects, on='review_point', how='outer')\n",
    "\n",
    "# For each aspect, create a boolean mismatch column\n",
    "for aspect in aspects:\n",
    "    merged_df[f'{aspect}_mismatch'] = (\n",
    "        (merged_df[f'{aspect}_label'] != merged_df[f'chatgpt_{aspect}_score']) &\n",
    "        (merged_df[f'{aspect}_label'].notna()) &\n",
    "        (merged_df[f'chatgpt_{aspect}_score'].notna())\n",
    "    )\n",
    "\n",
    "# Keep only rows where ALL mismatches are True\n",
    "all_mismatch_condition = merged_df[[f'{aspect}_mismatch' for aspect in aspects]].all(axis=1)\n",
    "all_mismatched = merged_df[all_mismatch_condition]\n",
    "\n",
    "# Print details\n",
    "for _, row in all_mismatched.iterrows():\n",
    "    print(f\"\\n========================\")\n",
    "    print(f\"Review Point:\\n{row['review_point']}\")\n",
    "    print(f\"Paper ID: {row['paper_id']}\")\n",
    "    print(f\"Venue: {row['venue']}\")\n",
    "    for aspect in aspects:\n",
    "        print(f\"\\n--- {aspect.upper()} ---\")\n",
    "        print(f\"Human Label: {row[f'{aspect}_label']} | GPT Label: {row[f'chatgpt_{aspect}_score']}\")\n",
    "        print(f\"Rationale: {row[f'chatgpt_{aspect}_rationale']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

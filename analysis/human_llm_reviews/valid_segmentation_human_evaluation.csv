review_point,paper_id,venue,focused_review,batch,actionability,actionability_label,actionability_label_type,id,grounding_specificity,grounding_specificity_label,grounding_specificity_label_type,verifiability,verifiability_label,verifiability_label_type,helpfulness,helpfulness_label,helpfulness_label_type,valid_point
"• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.",ICLR_2021_1213,ICLR_2021,"weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under “Additional Comments” as well, since they affect my assessment and understanding of the paper; consequently my score for the paper. Summary:
• The paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex.
• The authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known “random shuffling” sampling strategy.
• Specifically, AdaGrad-window is shown to achieve O ~ ( T − 1 / 2 )
rate of convergence, whereas AdaGrad-truncation attains ( T − 1 / 2 )
convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis.
• The paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach.
• In order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition “consistency ratio” over epochs. Strengths:
• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.
• I have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments.
• Performance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition.
• Main text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new “consistency condition” is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors’ approach to proving the results. Weaknesses:
• Although numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn’t verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds.
• Theorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of r
. I couldn’t figure out how it is possible to compute the value r
ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing r
weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me.
• The related work which is listed in Table 1, within the group “Adaptive Gradient Methods” prove \emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers.
• As a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work.
• Numerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion.
• This is a minor comment that should be easy to address. For ICLR, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part.
Additional Comments:
• I haven’t seen the definition that x t , m + 1 = x t + 1 , 1
in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis?
• Second bullet point of your contributions claim that “[consistency] condition is easy to verify”. I do not agree with this as I cannot see how someone could guarantee/compute the value r
ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context?
• In Assumption A3, I understand that G t e i = g t , i and G t e = ∑ i = 1 m g t , i
. I believe the existing notation makes it complicated for the reader to understand the implications of this condition.
• In the paragraph right above Section 4.2, authors state that presence of second moments, V t , i
enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details?
• In Corollary 1, authors state that “the computational complexity is nearly O ( m 5 / 2 n d 2 ϵ − 2 ) ~
”. A similar statement exists in Corollary 2. Could you please explain what “nearly” means in this context?
• In Lemma 8 in the supplements, a a T and b b T
in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that a a T or b b T
correspond to something like g t , j 2 – g t − 1 , j 2
. I am not sure if this construction fits into Lemma 8 because, for instance, the expression g t , j 2 – g t − 1 , j 2
is difference of two rank-1 matrices, which could have rank \leq 2. Hence, there may not exist some vector a
such that a a T = g t , j 2 – g t − 1 , j 2
, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors.
• In the supplements, in section “A.1.7 PROOF OF MAIN THEOREM 1”, in the expression following the first line, I didn’t understand how you obtained the last upper bound to ∇ f ( x t , i )
. Could you please explain how this is obtained? Score:
I would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns:
I am not convinced about the importance of consistency ratio and that it is a verifiable condition.
Related work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective.
(Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio.
Overall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications.
======================================= Post-Discussions =======================================
I would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score.
Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice.
Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '5', '5'], dtype=object)}",5,silver,154,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '4', 'X'], dtype=object)}",4,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '4', '4'], dtype=object)}",4,silver,
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.,ARR_2022_98_review,ARR_2022,"1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets.
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system.
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges.
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content.
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system. 2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)?
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1280,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '4', 'X'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,
"• This statement in the abstract is unclear: “ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions”. What does this mean? The abstract should be more high-level. Such technicalities are not necessary.",ICLR_2022_2671,ICLR_2022,", starting from the most critical ones.
Unclear Threat Model. The threat model is provided in Section 4.1, and the authors (implicitly) use the general notation of goal, capabilities, knowledge (the strategy is explained later). However, the capabilities of the attacker are not well defined. Specifically, the paper says that the attacker can control a fraction ( α
) of the M clients. Hence, I ask: what can the attacker do with such clients? Can they perform any manipulation? Are some of the features not modifiable? Is such attacker subject to realistic constraints or feature dependencies? Is the perturbation magnitude bounded? I am referring to the well-known issue of ‘problem vs feature space’ attacks [B], because real attackers are subject to many real world constraints (and especially in networked systems [C, D, E]) and not all adversarial perturbations may be physically realizable ([F]). The authors should elucidate this issue, because it could differentiate between “fictional” and “practical” attacks, therefore defining whether the proposed method is applicable to solve real world problems. Moreover, the following is unclear:
• “There are α
fraction of agents that are malicious and total β
fraction of instances with backdoor trigger across all malicious agents.” I am unable to understand the relationship between α and β
. Is β
a subset of α ?
• From Equation 3 onwards, is h^{j} meant to denote h j
? Is there a need for the braces, or is it a typo?
• Please differentiate from h b e n i g n and h B
. The current notation is very confusing
Tradeoff. A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients (or when such clients behave legitimately). In this paper, I am unable to determine what is the baseline performance of the models in “non-adversarial” settings. Does such performance degrade after RVFR is applied? How does RVFR compare to previous works in these circumstances? Even if the baseline performance does not decrease, what is the overhead of the proposed RVFR with respect to past defenses? Figure 3 and 2 only show results for adversarial scenarios. In real circumstances, a defense should have some practical utility. Note that I would not reject the paper even if RVFR does have a significant “cost”. However, such cost must be known.
Very poor Introduction and Abstract. The Introduction fails to provide a concrete justification and enough context for the considered problem. Let me list all the issues I encountered while reading the introductory part of the paper:
• This statement in the abstract is vague: “However, unlike the standard horizontal federated learning, improving the robustness of robust VFL remains challenging.”. Specifically, why is it challenging? Just name a few reasons.
• This statement in the abstract is unclear: “ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions”. What does this mean? The abstract should be more high-level. Such technicalities are not necessary.
• Please be consistent. In the Introduction: “In FL, a central server coordinates with multiple agents/clients”. Either use “client” or “agent”.
• I suggest mentioning [A] for a practical, recent and useful application of FL in a real world problem.
• This example is unclear: “In VFL, different agents hold different parts of features for the same set of training data. For example, in VFL for credit score prediction, Agent 1 may have the banking data of a user and Agent 2 may have the credit history of the same user, while the server holds corresponding labels.”. To me, it appears that Agent 1 and Agent 2 have different data, and hence represents a HFL problem (and I still do not understand the necessity of the last statement involving the ‘label’). Perhaps the authors should provide a visual example that better explains the difference between HFL and VFL.
• The Introduction still suffers of the same problem as the abstract. “However, it is challenging to defend against malicious attacks in VFL.”. Why? It is very annoying to read the abstract, not get a response to such question; and then re-reading the same concept in the Introduction, and not finding an answer even there. The impression is that the authors are trying to make the problem more difficult than what it currently is: if it is challenging, it should be clear.
• In the Introduction: “The fraction of malicious agents is relatively small”. Can you define such fraction? Does it have an upper boundary?
• The term “backdoor attacks” is never mentioned in the Introduction until the “contribution” paragraph. Such term should be better contextualized: not all poisoning attacks are backdoor attacks.
• I had to reach the Related Work section to understand why poisoning attacks in VFL are “challenging”. However, the motivation provided by the authors is confusing—to say the least. According to the paper, the challenge is “Backdoor attack against VFL is challenging since in the default setting the agent does not have the label information.” First, what is challenging exactly: the attack, or the defense? Second, what is such “default setting”? Third, why does the agent not have the label information? I believe the latter is due to the (poor) explanation provided in the early example in the introduction.
Minor issues:
• Please use the term “stage” (or “phase”) and not “time” to differentiate between training and testing/inference.
• Please add some text between Section 6 and 6.1
• In Section 6.1, the authors state “We study the classification task on two datasets: NUS-WIDE and CIFAR-10. Following (Liu et al., 2020), which proposed the backdoor attack against VFL, we use NUS-WIDE dataset to evaluate our defense.”. Does it mean that the defense is only evaluated on NUS-WIDE?
• In Section 6.2, the authors state “The noise variance is 0:05 for NUS-WIDE and 0:0001 for CIFAR-10 to preserve reasonable utility of the model.”. Please define what “utility” means in this statement.
• Figure 2: please maintain the same range for the y-axis. It does not only varies among the rows, but also among the columns.
• Figure 3 and Figure 2: the range of the y-axis should be the same for both figures.
• Use the same term to refer to figures. If the figures are named “Figures”, then use “Figures” in the text, and not “Fig.”
• What is the difference between “Backdoor Accuracy” and “Clean Accuracy” in Figures 2 and 3?
EXTERNAL REFERENCES
[A]: ""Federated learning for predicting clinical outcomes in patients with COVID-19."" Nature medicine (2021): 1-9.
[B]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[F]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '2', '5'], dtype=object)}",5,silver,512,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['X', '2', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '2', '5'], dtype=object)}",5,silver,
"- The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.",NIPS_2020_1228,NIPS_2020,"- The method section looks not self-contained and lacks descriptions of some key components. In particular: * What is Eq.(9) for? Why ""the SL is the negative logarithm of a polynomial in \theta"" -- where is the ""negative logarithm"" in Eq.(9)? * Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained. - The proposed approach essentially treats the structured constraints (a logical rule) as part of the discriminator that supervises the training of the generator. This idea looks not new -- one can simply treat the constraints as an energy function and plug it into energy-based GANs (https://arxiv.org/abs/1609.03126). Modeling structured constraints as a GAN discriminator to train the generative model has also been studied in [15] (which also discussed the relation b/w the structured approach with energy-based GANs). Though the authors derive the formula from a perspective of semantic loss, it's unclear what's the exact difference from the previous work? - The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '1', '1'], dtype=object)}",1,silver,398,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', 'X'], dtype=object)}",4,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '2', '3'], dtype=object)}",4,hard,
"2. In Table 3, besides the number of queries, it would be better to compare the real search cost (e.g. in terms of GPU days).",ICLR_2022_2660,ICLR_2022,"1. There is an assumption “graphs are topological close should have also comparable performance”. Nevertheless, it may not hold for architectures. For example, by only modifying one node/edge (add or remove skip connection), the architecture may incur significant performance drop. Thus, it is questionable to use spectral distance to evaluate the similarity. 2. In Table 3, besides the number of queries, it would be better to compare the real search cost (e.g. in terms of GPU days). 3. This paper only considers small search spaces, e.g. NASBench. Can the proposed method be used in DARTS and MobileNet search spaces? It would be better to report the results on these spaces. 4. In Section 4.2, the authors claimed that “our model selection approach is very stable”. However, there seem no empirical results to support it. 5. Since this paper focuses on learning predictors, several recent related work [a,b] should be discussed and/or compared.
Reference: [a] ReNAS: Relativistic evaluation of neural architecture search. CVPR 2021. [b] Contrastive Neural Architecture Search with Neural Architecture Comparators. CVPR 2021.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1088,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['X', '3', 'X'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,
"1. This paper presents a highly effective engineering method for ReC. However, it should be noted that the proposed framework incorporates some combinatorial and heuristic aspects. In particular, the Non-Ambiguous Query Generation procedure relies on a sophisticated filtering template. It would be helpful if the author could clarify the impact of these heuristic components.",FgEM735i5M,EMNLP_2023,"1. This paper presents a highly effective engineering method for ReC. However, it should be noted that the proposed framework incorporates some combinatorial and heuristic aspects. In particular, the Non-Ambiguous Query Generation procedure relies on a sophisticated filtering template. It would be helpful if the author could clarify the impact of these heuristic components.
2. Since the linguistic expression rewriting utilizes the powerful GPT-3.5 language model, it would be interesting to understand the extent of randomness and deviation that may arise from the influence of GPT-3.5. Is there any studies or analyses on this aspect?",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,726,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['X', '5', '4'], dtype=object)}",X,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,
"• Dependence on MIA (Membership Inference Attack) Testing via Ulira: While the paper uses MIA testing as a metric for unlearning effectiveness, the effectiveness of MIA testing itself is not sufficiently robust for privacy guarantees. Additionally the use of U-LiRA [1] is recommended.",7tpMhoPXrL,ICLR_2025,"•	GDPR Compliance Concerns: The paper’s reliance on approximate unlearning without theoretical guarantees presents a significant shortfall. While approximate unlearning may be practical, it falls short in scenarios where data privacy and regulatory compliance are non-negotiable. Without provable guarantees, it is questionable whether this method can satisfy GDPR requirements for data erasure. This gap undermines the core purpose of Model Unlearning in privacy-centered contexts, where the ""right to be forgotten"" demands more than a probabilistic assurance.
•	Scalability to Other Domains: The Forget Vector approach is developed and validated primarily for image classification tasks, potentially limiting its application in NLP or other non-visual domains where input perturbations may be less effective.
•	Dependence on MIA (Membership Inference Attack) Testing via Ulira: While the paper uses MIA testing as a metric for unlearning effectiveness, the effectiveness of MIA testing itself is not sufficiently robust for privacy guarantees. Additionally the use of U-LiRA [1] is recommended.
•	Sensitivity to Data Shifts: From the paper the effectiveness of unlearning decreases under certain data shifts, which may hinder the reliability of Forget Vectors in dynamic data environments or adversarial settings.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,821,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '2'], dtype=object)}",4,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '1'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '2'], dtype=object)}",5,silver,
"- it would seem to me that in section 4, ""X"" should be a multiset (and [\cal X]**n the set of multisets of size n) instead of a set, since in order the histogram to honestly represent a graph that has repeated vertex or edge labels, you need to include the multiplicities of the labels in the graph as well.",NIPS_2016_182,NIPS_2016,"weakness of the technique in my view is that the kerne values will be dependent on the dataset that is being used. Thus, the effectiveness of the kernel will require a rich enough dataset to work well. In this respect, the method should be compared to the basic trick that is used to allos non-PSD similarity metrics to be used in kernel methods, namely defining the kernel as k(x,x') = (s(x,z_1),...,s(x,z_N))^T(s(x',z_1),...,s(x',z_N)), where s(x,z) is a possibly non-PSD similarity metric (e.g. optimal assignment score between x and z) and Z = {z_1,...,z_n} is a database of objects to compared to. The write-up is (understandably) dense and thus not the easiest to follow. However, the authors have done a good job in communicating the methods efficiently. Technical remarks: - it would seem to me that in section 4, ""X"" should be a multiset (and [\cal X]**n the set of multisets of size n) instead of a set, since in order the histogram to honestly represent a graph that has repeated vertex or edge labels, you need to include the multiplicities of the labels in the graph as well. - In the histogram intersection kernel, it think for clarity, it would be good to replace ""t"" with the size of T; there is no added value to me in allowing ""t"" to be arbitrary.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,581,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.,NIPS_2020_257,NIPS_2020,"* In terms of novelty, note that both the motivation for the model as well as the initial parts of it hold similarities to some prior works. See detailed description in the relation to prior work section. * I would be happy to see results about generalization not only for the CLEVR dataset, but also for natural images datasets where there is larger variance both in the language and visual complexity. There are multiple datasets for generalization in VQA that can be used for that such as CP-VQA and also some splits of GQA. For the CLEVR dataset, the model is basically based on using an object detector to recognize the objects and their properties and build a semantic graph that represents the image. While other approaches that are compared to for this task use object detectors as well, there are many approaches for CLEVR (such as the Neural Module Network, Relation Network, MAC and FiLM) that do not use such strong supervision and therefore the comparison between these approaches in the experimental section is not completely valid. For better comparability, I would be interested to see generalization results when these models are also being fed with at least object-based bounding-boxes representations instead of the earlier commonly used spatial features, as is very common in VQA in the last years (see bottom-up attention networks). * This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '1', '2'], dtype=object)}",2,silver,99,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '3', '3'], dtype=object)}",3,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '3', '2'], dtype=object)}",3,silver,
"1) only the SimCLR case is covered and yet, there is no analysis on a seemingly important (see SimCLR-v2 and other recent papers that show that) part of that approach, ie the projection head.",NIPS_2021_2235,NIPS_2021,"(and questions):
A) The biggest weakness I think is that the analysis happens on a very restricted scenario, with no transfer: the authors study only the case where we have a single dataset and learn the encoder without using the label that we know exist and use to learn the classifiers - this is suboptimal and would not make sense in practive. I understand that this evaluation is common practice in SSL paper, however this is only a small part of the evaulations these papers have, and transfer learning is the more important and realisting setting. The authors do discuss this in lines 121-124 but justifying their choice by only citing empirical evidence of correlation of this ""task"" with transfer tasks, but I wouldn't say there are no guarantees there. Calling the second stage of classifier learning on the same dataset as traing as a ""downstream supervised task"" is an exaggeration (I would suggest to the authors to rephrase). Although this task ""correlates"" with transfer tasks, it is not clear to me if also this analysis extends. It would be great to discuss this at least a bit further.
B) Even for this task above, there are further simplifications to facilitate the analysis: 1) only the SimCLR case is covered and yet, there is no analysis on a seemingly important (see SimCLR-v2 and other recent papers that show that) part of that approach, ie the projection head. 2) The MoCo approach which is a very popular variant with a memory queue is not discussed. How does the analysis extend to negatives from a memory queue and dual encoders with exponential moving average? 3) There is a further sumplification by the use of a mean classifier, which is not common practice . Why is that simplification there, and is it central for the analysis?
C) The (absolute) numbers in Table 1 are not so intutive, unbounded and hard to understand. It is really hard to understand what is the main message of Table 1 and some of the rows, eg colisions, could perhaps be made more informative by turning them into probabilities. It is unclear what is meant in line 269 by ""10 sampled data augmentation per sample"" and unclear what reporting the Collision bound without the alpha and beta constants offer (section 4.2 is very unclear to me).
Some more notes/questions:
The discussion on clustering based SSL methods and Sec 4.4 is very restricted to this unrealistic task, that becomes even more unrealistic for clustering based pretraining. It is uncler to me what it offers.
A missing ref (Kalantidis et al ""Hard Negative Mixing for Contrastive Learning"" NeuriPS 2020) synthesizes hard negatives for contrastive SSL. Same as MoCo, it would be interesting to discuss how this analysis extends to synthetic negatives. Rating
Although an interesting study, the paper has limitations (see ""weaknesses"" section above). I would say that the current version of the paper is marginally below the acceptance threshold, but I am looking forward to the authors addressing my concerns above in their rebuttal.
Post-rebutal thoughts
The authors provided extensive responses to my questions, answering many in a satisfactory way. I still think however that a central concern listed in the original review stand: the fact that Arora et al study the same task that first learns without labels and the with labels on the same dataset (and only that task) doesn't mean that this is what should be the only task to study for ""Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning"".
In their response, the authors claim that
The self-supervised learning setting of our analysis is practical because the setting is quite similar to a semi-supervised learning setting, where we can access massive unlabeled samples and a few labeled samples.
With all due respect, I wouldn't compare this to semi-supervised learning for one key reason: as the authors also say here, in semi-supervised learning you have few labeled examples, a key property of the task. So, I would totally understand this analysis if the proposed bound was evaluated in a semi-supervised setting. This is not the case here, ie more than few labeled examples per class are used for learning the classifiers in this case.
Similarly, wrt the answer on the usage of a mean classifier:
a few-shot learning setting uses a mean classifier, namely, Prototypical Networks [9], which has been cited more than 2700 times, according to Google Scholar.
Again, in the same way, the use of a mean classifier is indeed justified for few-shot learning, but it is well known that in the case of datasets with many labels, a logistic regression classifiers is superior.
Overall, I do see some merit in this paper, yet I think the breadth of the analysis is not enough; I will keep my score to 5.
The authors do discuss some limitations, but not potential societal impacts. Given the nature of the work, the latter is not easy to assess and in my opinion it is fine to skip for a theoretical paper on SSL.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '2'], dtype=object)}",4,hard,612,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '2'], dtype=object)}",5,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '2', 'X'], dtype=object)}",5,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '2'], dtype=object)}",5,hard,
2. The writing is difficult to follow in many places and can be simplified.,ARR_2022_187_review,ARR_2022,"1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty. 2. The writing is difficult to follow in many places and can be simplified.
1. Line 360-367 are occupying too much space than needed. 2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :) 3. Too many metrics used for evaluation. While I commend the paper’s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text. 4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM’s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant. 5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself? 6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better. 7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of “despite the weaknesses, NPRM is useful'' but it does not flesh out why it’s useful. 8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results. 10. Line 559: “lower performance on Vikidia-Fr compared to Newsela-Es …” – Why? These are different languages after all, so isn’t the performance difference in-comparable?",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', '1', '3'], dtype=object)}",2,hard,376,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '1', '1'], dtype=object)}",1,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', 'X', '1'], dtype=object)}",2,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', '1', '2'], dtype=object)}",2,silver,
"- No in-depth analysis. The authors found inverse scaling happens over compute, but why? It would make the paper much more solid if the authors can provide some analysis explaining such training dynamics.",5BWvVIa5Uz,EMNLP_2023,"- The contribution is too limited. The paper only took a pre-trained model family and evaluated them on 4 existing datasets.
- No in-depth analysis. The authors found inverse scaling happens over compute, but why? It would make the paper much more solid if the authors can provide some analysis explaining such training dynamics.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,1072,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '4', '4'], dtype=object)}",4,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '4', '4'], dtype=object)}",4,silver,
"- Line 140: here the first column of Qo is replaced by vo to form P'o, so that the first state is not reachable anymore but from a terminating state. I assume that either Ass.1 (finite length of an option) or Ass.",NIPS_2017_303,NIPS_2017,"of their approach with respect to the previous SUCRL. The provided numerical simulation is not conclusive but supports the above considerations;
- Clarity: the paper could be clearer but is sufficiently clear. The authors provide an example and a theoretical discussion which help understanding the mathematical framework;
- Originality: the work seems to be sufficiently original with respect to its predecessor (SUCRL) and with respect to other published works in NIPS;
- Significance: the motivation of the paper is clear and relevant since it addresses a significant limitation of previous methods;
Other comments:
- Line 140: here the first column of Qo is replaced by vo to form P'o, so that the first state is not reachable anymore but from a terminating state. I assume that either Ass.1 (finite length of an option) or Ass. 2 (the starting state is a terminal state) clarify this choice. In the event this is the case, the authors should mention the connection between the two;
- Line 283: ""four"" -> ""for"";
- Line 284: ""where"" s-> ""were"";",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '4', '1'], dtype=object)}",1,silver,1285,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '4', '4'], dtype=object)}",4,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '4', '2'], dtype=object)}",X,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '3', '2'], dtype=object)}",2,silver,
"2) Important reference missing. The paper is closely related to the idea of unrolling, first proposed in, âListaâ http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf While there are important similarities and differences between the proposed work and Lista, it is important that the paper talks about them and places itself in appropriate context.",NIPS_2017_226,NIPS_2017,"- An important reference is missing
- Other less important references are missing
- Bare-bones evaluation
The paper provides an approach to solve linear inverse problems by reducing training requirements. While there is some prior work in this area (notably the reference below and reference [4] of the paper), the paper has some very interesting improvements over them. In particular, the paper combines the best parts of [4] (decoupling of signal prior from the specific inverse problem being solved) and Lista (fast implementation). That said, evaluation is rather skim â almost anecdotal at times â and this needs fixing. There are other concerns as well on the specific choices made for the matrix inversion that needs clarification and justifications.
1) One of the main parts of the paper is a network learnt to invert (I + A^T A). The paper used the Woodbury identity to change it to a different form and learns the inverse of (I+AA^T) since this is a smaller matrix to invert. At test time, we need to apply not just this network but also ""A"" and ""A^T"" operators.
A competitor to this is to learn a deep network that inverts (I+A^T A). A key advantage of this is that we do not need apply A and A^T during test time. It is true that that this network learns to inverts a larger matrix ... But at test time we have increased rewards. Could we see a comparison against this ? both interms of accuracy as well as runtime (at training and testing) ?
2) Important reference missing. The paper is closely related to the idea of unrolling, first proposed in, âListaâ http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf
While there are important similarities and differences between the proposed work and Lista, it is important that the paper talks about them and places itself in appropriate context.
3) Evaluation is rather limited to a visual comparison to very basic competitors (bicubic and wiener filter). It would be good to have comparisons to
- Specialized DNN: this would provide the loss in performance due to avoiding the specialized network.
- Speed-ups over [4] given the similarity (not having this is ok given [4] is an arXiv paper)
- Quantitative numbers that capture actual improvements over vanilla priors like TV and wavelets and gap to specialized DNNs. Typos
- Figure 2: Syntehtic",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,1148,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,
"- Unclear whether bringing connections to human cognition makes sense As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"".",NIPS_2017_349,NIPS_2017,"- The paper is not self contained
Understandable given the NIPS format, but the supplementary is necessary to understand large parts of the main paper and allow reproducibility.
I also hereby request the authors to release the source code of their experiments to allow reproduction of their results.
- Use of deep-reinforcement learning is not well motivated
The problem domain seems simple enough that a linear approximation would have likely sufficed? The network is fairly small and isn't ""deep"" either.
- > We argue that such a mechanism is more realistic because it has an effect within the game itself, not just on the scores
This is probably the most unclear part. It's not clear to me why the paper considers one to be more realistic than the other rather than just modeling different incentives? Probably not enough space in the paper but actual comparison of learning dynamics when the opportunity costs are modeled as penalties instead. As economists say: incentives matter. However, if the intention was to explicitly avoid such explicit incentives, as they _would_ affect the model-free reinforcement learning algorithm, then those reasons should be clearly stated.
- Unclear whether bringing connections to human cognition makes sense
As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"".
* Minor comments
** Line 16:
> [18] found them...
Consider using \citeauthor{} ?
** Line 167:
> be the N -th agentâs
should be i-th agent?
** Figure 3:
Clarify what the `fillcolor` implies and how many runs were the results averaged over?
** Figure 4:
Is not self contained and refers to Fig. 6 which is in the supplementary. The figure is understandably large and hard to fit in the main paper, but at least consider clarifying that it's in the supplementary (as you have clarified for other figures from the supplementary mentioned in the main paper).
** Figure 5:
- Consider increasing the axes margins? Markers at 0 and 12 are cut off.
- Increase space between the main caption and sub-caption.
** Line 299:
From Fig 5b, it's not clear that |R|=7 is the maximum. To my eyes, 6 seems higher.",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', '1', '1'], dtype=object)}",1,silver,351,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '4', '2'], dtype=object)}",3,hard,
4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results.,NIPS_2017_567,NIPS_2017,"Weakness:
1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly.
Here are some examples:
(1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is ""trivial"" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)?
(2) In line 96, I do not understand the sentence ""our lower hierarchical layers zoom in time"" and the sentence following that.
2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN.
3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper.
4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '3', '4'], dtype=object)}",4,silver,1407,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '3', '5'], dtype=object)}",4,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,
4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN.,NIPS_2019_175,NIPS_2019,"1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,253,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '5', '3'], dtype=object)}",3,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '5', '4'], dtype=object)}",3,hard,
"8: s/expensive approaches2) allows/expensive approaches,2) allows/ p.8: s/estimates3) is/estimates, and3) is/ In the references: Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers. Dusenberry et al. (2020) was published in ICML 2020 Osawa et al. (2019) was published in NeurIPS 2019 Swiatkowski et al. (2020) was published in ICML 2020 p. 13, supplement, Fig.",ICLR_2021_872,ICLR_2021,"The authors push on the idea of scalable approximate inference, yet the largest experiment shown is on CIFAR-10. Given this focus on scalability, and the experiments in recent literature in this space, I think experiments on ImageNet would greatly strengthen the paper (though I sympathize with the idea that this can a high bar from a resources standpoint).
As I noted down below, the experiments currently lack results for the standard variational BNN with mean-field Gaussians. More generally, I think it would be great to include the remaining models from Ovadia et al. (2019). More recent results from ICML could also useful to include (as referenced in the related works sections). Recommendation
Overall, I believe this is a good paper, but the current lack of experiments on a dataset larger than CIFAR-10, while also focusing on scalability, make it somewhat difficult to fully recommend acceptance. Therefore, I am currently recommending marginal acceptance for this paper.
Additional comments
p. 5-7: Including tables of results for each experiment (containing NLL, ECE, accuracy, etc.) in the main text would be helpful to more easily assess
p. 7: For the MNIST experiments, in Ovadia et al. (2019) they found that variational BNNs (SVI) outperformed all other methods (including deep ensembles) on all shifted and OOD experiments. How does your proposed method compare? I think this would be an interesting experiment to include, especially since the consensus in Ovadia et al. (2019) (and other related literature) is that full variational BNNs are quite promising but generally methodologically difficult to scale to large problems, with relative performance degrading even on CIFAR-10. Minor
p. 6: In the phrase ""for 'in-between' uncertainty"", the first quotation mark on 'in-between' needs to be the forward mark rather than the backward mark (i.e., ‘ i n − b e t w e e n ′ ).
p. 7: s/out of sitribution/out of distribution/
p. 8: s/expensive approaches 2) allows/expensive approaches, 2) allows/
p. 8: s/estimates 3) is/estimates, and 3) is/
In the references:
Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers.
Dusenberry et al. (2020) was published in ICML 2020
Osawa et al. (2019) was published in NeurIPS 2019
Swiatkowski et al. (2020) was published in ICML 2020
p. 13, supplement, Fig. 5: error bar regions should be upper and lowered bounded by [0, 1] for accuracy.
p. 13, Table 2: Splitting this into two tables, one for MNIST and one for CIFAR-10, would be easier to read.",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,107,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', 'X', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,
"- For effectiveness, the performance comparison in Table 1 is unfair. VINS sets different sample weights W u i in the training process, while most compared baselines like DNS, AOBPR, SA, PRIS set all sample weights as 1.",ICLR_2022_1923,ICLR_2022,"Weakness: 1. The novelty of this paper is limited. First, the analysis of the vertex-level imbalance problem is not new, which is a reformulation of the observations in previous works [Rendle and Freudenthaler, 2014; Ding et al., 2019]. Second, the designed negative sampler uses reject sampling to increase the chance of popular items, which is similar to the proposed one in PRIS [Lian et al., 2020]. 2. The paper overclaims on its ability of debiasing sampling. The “debiased” term in the paper title is confusing. 3. The methodology detail is unclear in Sec. 4.2. The proposed design that improves sampling efficiency seems interesting but the corresponding description is hard to follow given the limited space. 4. Space complexity of the proposed VINS should also be analyzed and compared in empirical studies, given that each (u, i) corresponds to a b u f f e r u i
. 5. Experiment results are not convincing enough to demonstrate the superiority of VINS on effectiveness and efficiency. - For effectiveness, the performance comparison in Table 1 is unfair. VINS sets different sample weights W u i
in the training process, while most compared baselines like DNS, AOBPR, SA, PRIS set all sample weights as 1. - For efficiency, Table 2 should also include the theoretical analysis for contrast.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['1', '4', '2'], dtype=object)}",1,hard,308,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '5', '4'], dtype=object)}",2,hard,
"3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.",NIPS_2016_9,NIPS_2016,"Weakness: The authors do not provide any theoretical understanding of the algorithm. The paper seems to be well written. The proposed algorithm seems to work very all on the experimental setup, using both synthetic and real-world data. The contributions of the papers are enough to be considered for a poster presentation. The following concerns if addressed properly could raise to the level of oral presentation: 1. The paper does not provide an analysis on what type of data the algorithm work best and on what type of data the algorithm may not work well. 2. The first claimed contribution of the paper is that unlike other existing algorithms, the proposed algorithm does not take as many points or does not need apriori knowledge about dimensions of subspaces. It would have been better if there were some empirical justification about this. 3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,79,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '4', '4'], dtype=object)}",4,silver,
"2. The proposed method adopts a proposal generator pretrained on MSCOCO which aggregates more information. Is it fair to compared with other methods? Besides, could the proposed technique propmotes existing Class incremental semantic segmentation methods. The authors adequately addressed the limitations and potential negative societal impact of their work.",NIPS_2022_2513,NIPS_2022,"Weakness:
The claim in Line 175~176 is not validated which it is valuable to see whether the proposed method could prevents potential classes from being incorrectly classified into historical classed.
In Tab. 1, for VOC 2-2 (10 tasks) and VOC 19-1 (2 tasks) MicroSeg gets inferior performance compared with SSUL, the reason should be explained. It also appear in ADE 100-50 (2 tasks) in Tab. 2.
The proposed method adopts a proposal generator pretrained on MSCOCO which aggregates more information. Is it fair to compared with other methods? Besides, could the proposed technique propmotes existing Class incremental semantic segmentation methods.
The authors adequately addressed the limitations and potential negative societal impact of their work.",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '4', '2'], dtype=object)}",3,hard,96,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '4', 'X'], dtype=object)}",5,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '4', '2'], dtype=object)}",4,silver,
"- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.",NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.
- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.
- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.
- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '2', '1'], dtype=object)}",4,hard,417,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '4', '5'], dtype=object)}",3,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '2'], dtype=object)}",3,silver,
4. The reviewer suggests showing the smoothed GT shapes in Figure. 3 and Figure. 5 so that the readers can better understand the quality of the reconstruction. A minor concern:,NIPS_2020_832,NIPS_2020,"The reviewer has some major concerns about the experiments. 1. The paper combines many objectives (about nine loss terms in Eq. 5, Eq. 8, and Eq. 12) to optimize the reconstruction network, but has not studied these losses in the experiments section. Such a complex loss function may weaken the contribution of the data representation. Besides, it seems unfair for the compared methods. Do some of these losses can be used for other methods such as Pixel2mesh and MeshRCNN? 2. The SDF (recent SOTA 3D representation method) based approaches (e.g., DISN [1]) have not been discussed and compared in the submission. 3. While the proposed method can not perform better than existing methods such as Pixel2mesh, MeshRCNN, and DISN [1] for 3D reconstruction from images, the paper has not analyzed the reasons. The reviewer suggests presenting some qualitative results of these SOTA methods in Figure 5. 4. The reviewer suggests showing the smoothed GT shapes in Figure. 3 and Figure. 5 so that the readers can better understand the quality of the reconstruction. A minor concern: 1. For Eq. 9~11, How about directly using the last visible surface? Dose Eq. 10 really improve the performance? For example, if f_1 is partial occluded (D_a is visible), f_2 is visible. The color attribute of the pixel I should mainly depend on f_2, right? Especially in the case that f_1 and f_2 are from different parts (e.g, chair leg and chair body), then why do you directly use the color attributes of f_2. [1] Xu, Qiangeng, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. ""Disn: Deep implicit surface network for high-quality single-view 3d reconstruction."" In Advances in Neural Information Processing Systems, pp. 492-502. 2019.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '1'], dtype=object)}",5,silver,1252,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '5', '3'], dtype=object)}",X,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '1'], dtype=object)}",5,silver,
- I wonder how crucial the annealing scheme from the last paragraph in Section 4 is. Especially when $\alpha$ is not decreased to $0$ I imagine this could induce a bias which may be so large that it outweighs the bias reductions attained by using IWAE in the first place.,NIPS_2020_1391,NIPS_2020,"- I wonder how crucial the annealing scheme from the last paragraph in Section 4 is. Especially when $\alpha$ is not decreased to $0$ I imagine this could induce a bias which may be so large that it outweighs the bias reductions attained by using IWAE in the first place. - The only other weakness is related to the clarity of the exposition, especially around the ""OVIS_~"" estimator (see further details below). ==== EDIT: 2020-08-24 ===== replaced ""$\alpha$ is not increased to $1$"" by ""$\alpha$ is not decreased to $0$"" as I had had a typo in this part of my review",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '3'], dtype=object)}",3,silver,688,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '3'], dtype=object)}",4,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '3'], dtype=object)}",4,silver,
"- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.",eI6ajU2esa,ICLR_2024,"- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.
- Under noisy conditions, many TTA methods can achieve desirable results, while the improvement brought by this paper's method is relatively low.
- In appendix A.2.1, under noisy conditions, the average performance improvement brought by this paper's method is very low and can even be counterproductive under certain noise conditions. Does this indicate an issue with the approach of changing input data?
- How to verify the reliability of the long-range photometric consistency in section 3.3? Are there any ablation study results reflecting the performance gain brought by each part?
- The explanation of the formula content in Algorithm 1 in the main body is not clear enough.
[A] Temporal Coherent Test-Time Optimization for Robust Video Classification. ICLR23
[B] Video Test-Time Adaptation for Action Recognition. CVPR23",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,105,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', 'X'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,
"- The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful.",NIPS_2016_321,NIPS_2016,"======================== Positive aspects: + The paper is well written and has a clear and coherent structure. The discussion of related work is comprehensive. + Non-parametric emission distributions add flexibility to the general HMM framework and reduce bias due to wrong modeling assumptions. Progress in this area should have theoretical and practical impact. + The paper builds upon existing spectral methods for parametric HMMs but introduces novel techniques to extend those approaches to the non-parametric case. + The theoretical bounds (section 5) are interesting, even though most of the results are special cases or straightforward extensions of known results. Negative aspects: - The restriction to triplets (or a sliding window of length 3) is quite limiting. Is this a fundamental limitation of the approach or is an extension to longer subsequences (without a sliding window) straightforward? - Since the paper mentions the possibility to use Chebyshev polynomials to achieve a speed-up, it would have been interesting to see a runtime comparison at test time. - The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful. - The experimental evaluation compares the proposed approach to 4 other HMM baselines. Even though NP-SPEC-HMM outperforms those baselines, the experimental evaluation has only toy character (simple length 6 conditional distributions, only one training/test sequence in case of the real datasets). Minor points ============ * l.22: Only few parametric distributions allow for tractable exact inference in an HMM. * l.183: Much faster approximations than Chebyshev polynomials exist for the evaluation of kernel density estimates, especially in low-dimensional spaces (e.g., based on fast multipole methods). * Figure 1: There is probably a âx 10^3â missing in the plot on the bottom right. Questions ========= * Eq. (3): Why the restriction to an isotropic bandwidth and a product kernel? Especially a diagonal bandwidth matrix could have been helpful. Would the approximation with Chebyshev polynomials still work? * The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM? * Is it computationally feasible to use the proposed model in a more realistic application, e.g., action recognition from motion capture sequences? Conclusion ========== The paper is well written and, from a theoretical perspective, interesting to read. However, the experiments are weak and it remains unclear how practical the proposed model would be in real applications. Iâm tending towards accept, but the authors should comment in their rebuttal on the above points.",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,390,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', '3'], dtype=object)}",3,silver,
"5: More experiments on deeper networks (e.g., ResNet-50) and other network structures (e.g., MobileNet) are needed to further strengthen the paper. References: [1] MoBiNet: A Mobile Binary Network for Image Classification, in WACV 2020. [2] Dynamic Channel Pruning: Feature Boosting and Suppression, in ICLR2019. [3] Learning Dynamic Routing for Semantic Segmentation, in CVPR2020.",ICLR_2021_738,ICLR_2021,"---:
1: This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs, which is not significant enough.
The dynamic routing strategy (conditional on input) has been widely explored. For example, the proposed dynamic formulation in this paper has been used in several studies [2, 3].
Varying width and depth has been extensively explored in the quantization literature, especially in AutoML based approaches [Shen et al. 2019, Bulat et al. 2020], to design high capacity quantized networks.
The effectiveness of the group convolution in BNNs was initially studied in [1]. Later works also incorporate the group convolution into the search space in NAS+BNNs methods [e.g., Bulat et al. 2020a] to reduce the complexity.
2: In each layer, the paper introduces a full-precision fully-connected layer to decide which expert to use. However, for deeper networks, such as ResNet-101, it will include ~100 full-precision layers, which can be very expensive especially in BNNs. As a result, it deteriorates the benefits and practicability of the dynamic routing mechanism.
3: The actual speedup, memory usage and energy consumption on edge devices (e.g., CPU/GPU/FPGA) or IoT devices must be reported. Even though the full-precision operations only account for a small amount of computations in statistics, it can have a big influence on the efficiency on platforms like FPGA.
4: This paper proposes to learn the binary gates via gradient-based optimization while exploring the network structure via EfficientNet manner. Then the problem comes. This paper can formulate the <width, depth, groups and layer arrangement> as configuration vectors and optimize them using policy gradients and so on, with the binary gates learning unified in a gradient-based framework. So what is the advantage of the ""semi-automated"" method of EfficientNet over the gradient-based optimization? In addition, how about learning a policy agent via RL to predict the gates? I encourage the authors can add comparsions and discussions with these alternatives.
5: More experiments on deeper networks (e.g., ResNet-50) and other network structures (e.g., MobileNet) are needed to further strengthen the paper. References:
[1] MoBiNet: A Mobile Binary Network for Image Classification, in WACV 2020.
[2] Dynamic Channel Pruning: Feature Boosting and Suppression, in ICLR2019.
[3] Learning Dynamic Routing for Semantic Segmentation, in CVPR2020.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,218,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['1', '3', '3'], dtype=object)}",3,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['1', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '4', '5'], dtype=object)}",2,hard,
"2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152.",NIPS_2020_125,NIPS_2020,"- In section 3.1, the logic of extending HOGA from second order is not consistent with the extension from first order to second order; i.e., second order attention creates one more intermediate state U compared to the first order attention module. However, from the second order to higher order attention module, although intermediate states U0, U1 … are created, they are only part of the intermediate feature (Concatenating them will form U with full channel resolution). In this way, it seems we could regard the higher order attention module as a special form of second order attention module. - The paper does not clearly explain the intuition as to why different channel groups should have different attention mechanisms; i.e., in what specific way the network can benefit from the proposed channel group specific attention module. - Experiments are not solid enough: 1. There are no ablation studies on the effect of parameter numbers, so it is not clear whether the performance gain is due to the proposed approach or additional parameters. 2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152. 3. There are no results using strong object detection frameworks; the current SSD framework is relatively weak (e.g. Faster RCNN would be a stronger, more standard approach); it is not clear whether the improvements would be retained with a stronger base framework. - The proposed approach requires larger FLOPS compared to baselines; i.e., any performance gain requires large computation overhead (this is particularly pronounced in Table 3). - In Table 3 shows ResNet32/56 but L222 refers to ResNet34/50, which is confusing.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '4'], dtype=object)}",4,gold,867,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['X', '4', '1'], dtype=object)}",X,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '3'], dtype=object)}",5,hard,
"2.) I found the definition of the quantile a little confusing, an extra pair of brackets around the term ( 1 | D | ∑ ( X r , Y r ) ∈ D 1 S ( X r , Y r ) ≤ s ) might help, or maybe defining the bracketed term separately if space allows.",NIPS_2021_40,NIPS_2021,"/Questions:
I only have minor suggestions:
1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying Q ^ t and S t
, as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of α t
and also on the average lengths of the predictive intervals when we let Q ^ t and S t
vary with time?
2.) I found the definition of the quantile a little confusing, an extra pair of brackets around the term ( 1 | D | ∑ ( X r , Y r ) ∈ D 1 S ( X r , Y r ) ≤ s )
might help, or maybe defining the bracketed term separately if space allows.
3.) I think there are typos in Lines 93, 136, 181 (and maybe in the Appendix too): should it be Q ^ t ( 1 − α t ) instead? ##################################################################### Overall:
This is a very interesting extension to conformal prediction that no longer relies on exchangeability but is still general, which will hopefully lead to future work that guarantees coverage under weak assumptions. I believe the generality also makes this method useful in practice.
The authors have described the limitations of their theory, e.g. having a fixed Q ^
with time.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1305,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
- It would be helpful if you provided glosses in Figure 2.,ACL_2017_433_review,ACL_2017,"- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement. UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.
- General Discussion: I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.
- Questions for the authors: - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.
- Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?
- Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use ""discourse"" for things that are not considered ""discourse"" in other languages in UD?
- Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.
- Low-level comments: - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.
- You use the word ""grammar"" in a slightly strange way. I think replacing ""grammar"" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90) - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.
- Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with ""very high performing model"" or something like that.
- It would be helpful if you provided glosses in Figure 2.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,935,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '4', '5'], dtype=object)}",1,hard,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,
"1. The paper appears to be limited to a combination of existing techniques: adaptation to an unknown level of corruption (Lykouris et al., 2018); varying variances treated with a weighted version of OFUL (Zhou et al., 2021); variable decision sets (standard in contextual linear bandits). The fact that these results can be combined together is not surprising, and thus the contribution could be considered incremental.",NIPS_2021_2367,NIPS_2021,"1. The paper appears to be limited to a combination of existing techniques: adaptation to an unknown level of corruption (Lykouris et al., 2018); varying variances treated with a weighted version of OFUL (Zhou et al., 2021); variable decision sets (standard in contextual linear bandits). The fact that these results can be combined together is not surprising, and thus the contribution could be considered incremental. 2. The regret bounds seem sub-optimal in the level of corruption (they are of order C^2 when existing bounds seem to be of order C). The authors should discuss more this sub-optimality, is it due to the unknown corruption level? Why should we incur it?
Other comments:
The authors state that if C is known and the variances are fixed then one could directly apply OFUL with a modified variance (to add the corruption). This yields several questions: a. The regret bound would then be of order O((R+C)d\sqrt{T}), right? It would be informative to write it, so that we can compare it with the bound of Thm. 5.1. b. It is then claimed that one of the problems is the varying variances. But this problem was already solved by Weighed OFUL (Thm. 4.2 of Zhou et al 2021) for OFUL without corruption. Isn't it possible to apply the same reasoning with this algorithm? c. For the adaptation to the unknown value of C, I am wondering whether it is not possible to just apply the Corral algorithm (see Cor. 6 of [1]) to (Weighed)-OFUL with an exponential grid of possible values for C (from 1 to T). Wouldn't this imply a bound of order C \sqrt{T} log T?
The assumption that the variance is revealed by the adversary (l. 135) is not clear and should be better motivated. We understand that it was already done by Kirschner and Krause (2018) and Zhou et al (2021) but examples of practical applications would be enjoyable to make the paper more self-contained. Similarly, the assumption of varying decision sets is standard in linear bandits but a few lines to recall why this allows dealing with contexts could be helpful for a reader new to the area.
About the experiments: a.The results seem significantly different when C = 0 and 300. What is the intermediate regime? For what level of corruption does Multi-level OFUL outperform algorithms that do not consider corruption? b. I regret that the algorithm is only compared to baselines that are not designed to deal with corruption and suffer linear regrets. It would be interesting to compare Multi-level OFUL with algorithms for linear bandits with corruptions. This could be done by considering fixed variance and fixed decision sets for instance to apply existing algorithms, so that we can see the actual cost of having a more general algorithm. The algorithm could also be compared with the version of OFUL which knows C and takes into account the corruption.
Minor remarks:
How a \min in (6.3) is obtained using lemma 6.5 is not clear and should be clarified. Same for the \min in (6.8) using lemma 6.6?
How substituting (6.5) and (6.6) into (6.3) gives (6.7) should also be more detailed.
[1] Agarwal et al. Corralling a Band of Bandit Algorithms, 2017.
The authors did not mention the limitations and potential negative societal impact of their work.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '1', '1'], dtype=object)}",1,silver,194,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '2', '2'], dtype=object)}",2,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '4', '2'], dtype=object)}",4,silver,
- It would have been helpful to include additional benchmarking tasks outside of AitW.,jR6YMxVG9i,ICLR_2025,"-""a around"" should be ""an around""
-The grammar here could be improved as the phrasing is awkward: ""Apart from agent framework""
-""VLM - generated"" should be ""VLM-generated"" in Line 173
- If a trajectory fails to execute the desired command and the system tries to start over, the initial state, $s_1$, may not be the same if the system is trialing the trajectories in the real world. The paper does not seem to address the problem of undoing or resetting the system back to its initial state during the reflection and retry component. If the system used a world model, then this would not be a problem, but it doesn't seem that a world model is discussed or used.
- It would be helpful for the x-axis of Figure 2 to not have increments of 1/2 as the actual step size is 1.
- The paper could be improved by including an analysis to show how often failures in each iteration lead to more problems at subsequent iterations as opposed to getting to reset to the initial state.
- The results could be improved if there were confidence intervals on the results in the tables and some notion of temperature and how that hyperparameter might effect results.
- Table 3 is helpful to show the performance of the system improves with a 2x-3x increase in training data size; however, it would be even better if a dimension was included for the quality of the data.
- It would have been helpful to include additional benchmarking tasks outside of AitW.
- An additional analysis showing the performance of the system as a function of task complexity (e.g., number of steps required in the optimal plan) would have been useful.
- The approach is a relatively straight-forward combination of
- It would be helpful to add details about whether the reward model and the VLM policy are the same system. If so, how is the VLM trained to output the reward? Is this just intrinsic or through prompt engineering or is the VLM literally modified to add a scalar output head and trained from scratch or fine-tuned?
- The approach seems to combine some relatively simple elements to create a new system; the paper could be improved by being more clear about the novelty and intellectual merit of the approach more than just combining simple elements of prior work.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '3', '5'], dtype=object)}",3,silver,1180,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '3', '4'], dtype=object)}",2,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '3', '3'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '3', '2'], dtype=object)}",2,silver,
"- **Single Seed Experiments**: The experiments in the paper are limited to training on a single seed, making it difficult to assess the significance of performance differences and the true impact of the proposed cycle consistency loss on convergence. Multiple seed experiments would provide a more robust evaluation.",WYsLU5TEEo,ICLR_2024,"- **Limited to Binary Tasks**: A major limitation of the paper is that it only addresses binary classification tasks. It would be interesting to expand its applicability to multiclass problems to demonstrate broader utility, as mentioned in the discussion section.
- **Single Seed Experiments**: The experiments in the paper are limited to training on a single seed, making it difficult to assess the significance of performance differences and the true impact of the proposed cycle consistency loss on convergence. Multiple seed experiments would provide a more robust evaluation.
- **Experiment Clarity**: The presentation of experiments can be confusing and should be more detailed. For instance, the ""Hybrid D"" model is never introduced in the paper. The explanation of the computation of performance when using D is also presented *after* showing results. The description of Table 2 is also unclear, making it challenging for readers to understand the methodology and the comparison.
- **Misleading Introduction**: The paper introduces the approach as ""combining classifier and discriminator in a single model"" (in the abstract), which is incorrect since the generator and discriminator are fundamentally different.
- **Lack of Comparative Analysis**: The paper lacks a comparison with other counterfactual approaches, which could provide insights into the quality of the counterfactuals produced and help position the proposed method within the broader context of counterfactual research.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,1345,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,
- Figure 3 is very hard to read anything on the figure.,NIPS_2016_117,NIPS_2016,"weakness of this work is impact. The idea of ""direct feedback alignment"" follows fairly straightforwardly from the original FA alignment work. Its notable that it is useful in training very deep networks (e.g. 100 layers) but its not clear that this results in an advantage for function approximation (the error rate is higher for these deep networks). If the authors could demonstrate that DFA allows one to train and make use of such deep networks where BP and FA struggle on a larger dataset this would significantly enhance the impact of the paper. In terms of biological understanding, FA seems more supported by biological observations (which typically show reciprocal forward and backward connections between hierarchical brain areas, not direct connections back from one region to all others as might be expected in DFA). The paper doesn't provide support for their claim, in the final paragraph, that DFA is more biologically plausible than FA. Minor issues: - A few typos, there is no line numbers in the draft so I haven't itemized them. - Table 1, 2, 3 the legends should be longer and clarify whether the numbers are % errors, or % correct (MNIST and CIFAR respectively presumably). - Figure 2 right. I found it difficult to distinguish between the different curves. Maybe make use of styles (e.g. dashed lines) or add color. - Figure 3 is very hard to read anything on the figure. - I think this manuscript is not following the NIPS style. The citations are not by number and there are no line numbers or an ""Anonymous Author"" placeholder. - I might be helpful to quantify and clarify the claim ""ReLU does not work very well in very deep or in convolutional networks."" ReLUs were used in the AlexNet paper which, at the time, was considered deep and makes use of convolution (with pooling rather than ReLUs for the convolutional layers).",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '1'], dtype=object)}",5,silver,801,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', 'X', '1'], dtype=object)}",5,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '2'], dtype=object)}",5,silver,
"- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?",NIPS_2017_370,NIPS_2017,"- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?
- From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.
- The improvements on image deconvolution is minimal with CNN-X working better than ACNN when all the dataset is considered. This shows that the adaptive convolutions are not universally applicable when the side information is available. Also, there are no comparisons with state-of-the-art network architectures for digit recognition and image deconvolution. Suggestions:
- It would be good to move some visual results from supplementary to the main paper. In the main paper, there is almost no visual results on crowd density estimation which forms the main experiment of the paper. At present, there are 3 different figures for illustrating the proposed network architecture. Probably, authors can condense it to two and make use of that space for some visual results.
- It would be great if authors can address some of the above weaknesses in the revision to make this a good paper.
Review Summary:
- Despite some drawbacks in terms of experimental analysis and the general applicability of the proposed technique, the paper has several experiments and insights that would be interesting to the community. ------------------
After the Rebuttal: ------------------
My concern with this paper is insufficient analysis of 'filter manifold network' architecture and the placement of adaptive convolutions in a given CNN. Authors partially addressed these points in their rebuttal while promising to add the discussion into a revised version and deferring some other parts to future work.
With the expectation that authors would revise the paper and also since other reviewers are fairly positive about this work, I recommend this paper for acceptance.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1040,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['X', 'X', '5'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"1. It appears in Sections 6.1 and 6.2 that the tree-sliced Wasserstein distance outperforms the original optimal transport distance, which is surprising. Could you explain why this occurs?",NIPS_2019_1102,NIPS_2019,"1. It appears in Sections 6.1 and 6.2 that the tree-sliced Wasserstein distance outperforms the original optimal transport distance, which is surprising. Could you explain why this occurs? 2. The proof in the main text of Proposition 1 looks more like a proof sketch, particularly as the existence of a function f having the property you claim isn't immediately obvious. Could you include (in the supplement, at least) the full proof? --- UPDATE: I have read and I appreciate the authors' response. I will not be changing my score.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1514,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '3', 'X'], dtype=object)}",2,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '4', '5'], dtype=object)}",4,silver,
"1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.",ARR_2022_114_review,ARR_2022,"By showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials). For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.
As another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\in \mathcal{R}^{r \times r}$ is parameterized as $V U^T$ where $U,V\in \mathcal{R}^{r \times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\mathcal{R}^{r \times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here?
In regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.
2. For each HMM with rank r, add a baseline smaller HMM with state size being r.",2,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,72,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '3', 'X'], dtype=object)}",5,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,
2. The alignment of relabeled reward data with human annotator judgments remains insufficiently validated.,iamWnRpMuQ,ICLR_2025,"The results have a few issues which make evaluating the contribution difficult:
1. The paper lacks a comparison with some existing works, particularly methods involve iterative PPO/DPO method that train a reward model simultaneously and reward ensembles [1].
[1] Coste T, Anwar U, Kirk R, et al. Reward model ensembles help mitigate overoptimization.
2. The alignment of relabeled reward data with human annotator judgments remains insufficiently validated.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['1', '3', '1'], dtype=object)}",1,silver,233,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '4', '2'], dtype=object)}",2,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '2', '1'], dtype=object)}",3,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '3', '2'], dtype=object)}",2,silver,
2) the model has many components whose hyper parameters are not fully provided (someone may have to trace them in the source code),NIPS_2020_1490,NIPS_2020,"1) the authors do not compare with the model in [15]: ""Modeling long-and short-term temporal patterns with deep neural networks."". This restricts the potential impact of the model. 2) the model has many components whose hyper parameters are not fully provided (someone may have to trace them in the source code) 3) the paper doesn't propose a conceptual/computational novelty. it combines existing modules to achieves its results.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '3', '4'], dtype=object)}",2,hard,1418,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '3', '5'], dtype=object)}",1,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '3', 'X'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '3', '5'], dtype=object)}",2,hard,
"4. For Figure 1, are the figures generated by real experiments or artificially? If they are artificially generated, can authors conduct some real-world experiments to support the phenomenon occurred in these figures? This would be an important evaluation of the proposed method.",NIPS_2020_243,NIPS_2020,"My main concerns are listed as follows: 1. There are some typos in the manuscript, e.g., in Abstract, ""betwenn"". 2. It is a pity that the authors only perform experiments on positive-unlabeled learning, optimal transport techniques have been used in many applications. More results on other applications such as transfer learning, few-shot learning, or zero-shot learning may be better, with more baseline methods being compared. 3. In recent years, both optimal transport and deep learning are hot research issues. The authors are encourage to explain how to expand the proposed method to integrate with deep learning models. 4. For Figure 1, are the figures generated by real experiments or artificially? If they are artificially generated, can authors conduct some real-world experiments to support the phenomenon occurred in these figures? This would be an important evaluation of the proposed method. 5. When citing literature, the tense of sentences is inconsistent, e.g., ""Peyré et al. (2016) proposed"" and ""Chizat et al. (2018) propose"".",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1224,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '5', 'X'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"1. I am not completely convinced by the experimental strengths of this approach. To run the proposed algorithm, the authors need to run a descent procedure for 40 different networks from the training phase. In contrast, you could simply run vanilla Adam on the final network with 40 random initial points, and one of these restarts would reach the global minimum. It is not important that EACH initialization reach the global minimum, as long as AT LEAST one initialization reaches the global minimum.",NIPS_2019_1346,NIPS_2019,"below. 2. Theorem 3.1 is interesting in itself, because it applies to vectors which are not in the range of the generative model. Clarity: The paper is well written and ideas clearly expressed. I believe that others can reproduce the algorithm described. I only have a problem with the way the set $S(x,theta, tau)$ is defined in line 177, since the authors do not require the signs to strictly differ on this set. Significance: I think other researchers can build on Theorem 3.1. The conditions proposed for Theorem 3.3 are novel and could be used for future results. Weaknesses: 1. I am not completely convinced by the experimental strengths of this approach. To run the proposed algorithm, the authors need to run a descent procedure for 40 different networks from the training phase. In contrast, you could simply run vanilla Adam on the final network with 40 random initial points, and one of these restarts would reach the global minimum. It is not important that EACH initialization reach the global minimum, as long as AT LEAST one initialization reaches the global minimum. 2. While Theorem 3.3 is interesting, it does not directly influence the experiments because the authors never perform the search operation in line 3 of algorithm 2. Because of this, it provides a proof of correctness for an algorithm that is quite different from the algorithm used in practice. Although Algorithm 2 and the empirical algorithm are similar in spirit, lines 1 and 3 in algorithm 2 are crucial for proof of correctness. Clarifications: 1. For the case where $y= G(z) + noise$, where noise has sufficiently low energy, you would expect a local minimum close to $z$. Would this not contradict the result of Theorem 3.1? ---Edit after author response--- Thank you for your response. After reading your rebuttal and other reviews, I have updated my score to a 8. I think Table in the rebuttal and Theorem 3.1 are solid contributions. Regarding my criticism of the definition of S(x,tau,theta)- I only meant that defining the complement of this set may make things clearer, since you only seem to work with its complement later on (this did not influence my score).",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '1', '2'], dtype=object)}",1,silver,1092,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,
"- Eq. (3): What is $e_l$? Corollaries 1, 2 and 3 and Theorem 4: All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).",NIPS_2017_585,NIPS_2017,"weakness of the paper is in the experiments: there should be more complete comparisons in computation time, and comparisons with QMC-based methods of Yang et al (ICML2014). Without this the advantage of the proposed method remains unclear.
- The limitation of the obtained results:
The authors assume that the spectrum of a kernel is sub-gaussian. This is OK, as the popular Gaussian kernels are in this class. However, another popular class of kernels such as Matern kernels are not included, since their spectrum only decay polynomially. In this sense, the results of the paper could be restrictive.
- Eq. (3):
What is $e_l$?
Corollaries 1, 2 and 3 and Theorem 4:
All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).
- The equation in Line 170:
What is $e_i$?
- Subsampled dense grid:
This approach is what the authors used in Section 5 on experiments. However, it looks that there is no theoretical guarantee for this method. Those having theoretical guarantees seem not to be practically useful.
- Reweighted grid quadrature:
(i) It looks that there is no theoretical guarantee with this method.
(ii) The approach reminds me of Bayesian quadrature, which essentially obtains the weights by minimizing the worst case error in the unit ball of an RKHS. I would like to look at comparison with this approach.
(iii) Would it be possible to derive a time complexity?
(iv) How do you chose the regularization parameter $\lambda$ in the case of the $\ell_1$ approach?
- Experiments in Section 5:
(i) The authors reported the results of computation time very briefly (320 secs vs. 384 seconds for 28800 features in MNIST and ""The quadrature-based features ... are about twice as fast to generate, compared to random Fourier features ..."" in TIMIT). I do not they are not enough: the authors should report the results in the form of Tables, for example, varying the number of features.
(ii) There should be comparison with the QMC-based methods of Yang et al. (ICML2014, JMLR2016). It is not clear what is the advantage of the proposed method over the QMC-based methods.
(iii) There should be explanation on the settings of the MNIST and TIMIT classification tasks: what classifiers did you use, and how did you determine the hyper-parameters of these methods? At least such explantion should be included in the appendix.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,924,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
1) The RQ1 mentioned in the paper seems redundant. This adds no extra information for the audience. It is expected the performance will vary across multiple HS datasets when evaluated in cross-data setting. Another interesting point to analyse would've been how % of explicit hate information in the dataset affects implicit hate speech detection performance and vice-versa and it's corresponding effect on RQ2 & RQ3 t-sne plots. (Reference - https://aclanthology.org/2023.findings-eacl.9/),bt9Ho2FMxd,EMNLP_2023,"1) The RQ1 mentioned in the paper seems redundant. This adds no extra information for the audience. It is expected the performance will vary across multiple HS datasets when evaluated in cross-data setting. Another interesting point to analyse would've been how % of explicit hate information in the dataset affects implicit hate speech detection performance and vice-versa and it's corresponding effect on RQ2 & RQ3 t-sne plots. (Reference - https://aclanthology.org/2023.findings-eacl.9/)
2) Again it is only obvious / intuitive that employing a contrastive learning strategy would bring together the implicit and explicit hate embeddings. What would be interesting to understand is that how can these correlations be leveraged to improve the downstream classification performance ?
In it's current form the paper lacks enough significant learnings / contribution to be accepted. Incorporating the above mentioned suggestions should be sufficient",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '1', '5'], dtype=object)}",4,hard,1314,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,
"1) Since this paper focuses on biometric verification learning, the comparison against the state-of-the-art loss functions widely used in face/iris verification should be added (e.g., Center-Loss, A-Softmax, AM-Softmax, ArcFace).",ICLR_2022_3218,ICLR_2022,"Weakness: 1) Since this paper focuses on biometric verification learning, the comparison against the state-of-the-art loss functions widely used in face/iris verification should be added (e.g., Center-Loss, A-Softmax, AM-Softmax, ArcFace). 2) Cosine similarity score is more often used in biometric verification, so I wonder if it would work better than the Euclidean distance when computing the Decidability. 3) Large batch-size may be significant in the proposed loss. The authors conducted on three settings to select the best number of batch-size. However, it may be better to examine the performance with more settings. For example, what would happen if a small batch-size is used. 4) Why triplet loss cannot convergent on CASIA-V4? I guess many previous iris verification works have employed such loss. 5) Figure 5 shows the impact of the D-Loss before and after training the model. It is suggested to compare with other losses on it.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1175,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"2. However, overall, no information from 2-hop neighbors is included. Again, this method is simple, but it is highly unclear why it is effective.",oSdrJyb4UH,ICLR_2025,"W1. This paper claims that the proposed method has good expressiveness. However, I found no (theoretical) analysis regarding the expressiveness.
W2. The proposed method is actually pretty simple, and the rationale is simple, monophyly (lines 50-52), i.e., 2-hop neighbors are helpful for node classification on both homophilic and heterophilic graphs. Such a simplicity is good, but I found some defects unsolved.
W2.1 Why are 2-hop neighbors useful for node classification on graphs regardless of whether they are homophilic or heterophilic?
W2.2 It looks like the proposed method (from Eq. 1 to 3, Figure 2 c) still utilizes information from 1-hop neighbors. In my understanding, the core idea of this paper is to collect edge messages (from 1-hop neighbors) by Eq. 3 and then let the edge messages interact via a self-attention module Eq. 2. However, overall, no information from 2-hop neighbors is included. Again, this method is simple, but it is highly unclear why it is effective.
W2.3 Why previous methods cannot capture the information from 2-hop neighbors? If the 2-hop information is that useful, I think many previously proposed methods should be able to capture it. E.g., GPRGNN [1], ACM-GNN [2].
*Minor weaknesses: *
W3. Some baselines are missing [3-5]. After adding them back, the performance advantage of the proposed method is not that significant on datasets Roman Empire, A-ratings, and Tolokers.
W4 The strategies proposed in Section 4.2 are a bit heuristic-based.
W5. The code is not provided, which lowers the reproducibility of this study.
[1] Chien, Eli, et al. ""Adaptive Universal Generalized PageRank Graph Neural Network."" International Conference on Learning Representations.
[2] Luan, Sitao, et al. ""Revisiting heterophily for graph neural networks."" Advances in neural information processing systems 35 (2022): 1362-1375.
[3] Zhao, Kai, et al. ""Graph neural convection-diffusion with heterophily."" Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. 2023.
[4] Jang, Hyosoon, et al. ""Diffusion probabilistic models for structured node classification."" Advances in Neural Information Processing Systems 36 (2024).
[5] Zheng, Amber Yijia, et al. ""Graph Machine Learning through the Lens of Bilevel Optimization."" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '4', '4'], dtype=object)}",4,gold,1483,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '3', '3'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '3', '3'], dtype=object)}",3,silver,
"- The main contribution of combining attention with other linear mechanisms is not novel, and, as noted in the paper, a lot of alternatives exist.",bIlnpVM4bc,ICLR_2025,"- The main contribution of combining attention with other linear mechanisms is not novel, and, as noted in the paper, a lot of alternatives exist.
- A comprehensive benchmarking against existing alternatives is lacking. Comparisons are only made to their proposed variants and Sliding Window Attention in fair setups. A thorough comparison with other models listed in Appendix A (such as MEGA, adapted to Mamba) would strengthen the findings. Additionally, selected architectures are evaluated on a very small scale and only measured by perplexity. While some models achieve lower perplexity, this alone may not suffice to establish superiority (e.g., in H3 by Dao et al., 2022b, lower perplexity is reported against transformer baselines).
- Results on common benchmarks are somewhat misleading. The paper aims to showcase the architecture’s strengths, yet comparisons are often made against models trained on different data distributions, which weakens the robustness of the conclusions.
- Conclusions on long-context handling remain vague, although this should be a key advantage over transformers. It would be helpful to include dataset statistics (average, median, min, max lengths) to clarify context length relevance.
- The only substantial long-context experiment, the summarization task, should be included in the main paper, with clearer discussion and analysis.
- Section 4, “Analysis,” could benefit from clearer motivation. Some explored dimensions may appear intuitive (e.g., l. 444, where SWA is shown to outperform full attention on larger sequence lengths than those used in training), which might limit the novelty of the findings. Other questions seems a bit unrelated to the paper topics (see Questions).
- Length extrapolation, a key aspect of the paper, is barely motivated or discussed in relation to prior work.
- The paper overall feels somewhat unstructured and difficult to follow. Tables present different baselines inconsistently, and messages regarding architectural advantages are interleaved with comments on training data quality (l. 229). The evaluation setup lacks consistency (performance is sometimes assessed on real benchmarks, other times by perplexity), and the rationale behind baseline choices or research questions is insufficiently explained.",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '1', '1'], dtype=object)}",1,gold,385,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '2', '3'], dtype=object)}",1,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '3', '1'], dtype=object)}",1,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', '2', '1'], dtype=object)}",2,silver,
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.,ARR_2022_303_review,ARR_2022,"- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses. However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text. Table 1's revised caption mentions 2.9K paragraphs as the size.
This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR.
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column. I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work. S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base",2,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '1', '1'], dtype=object)}",1,silver,55,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '2', '5'], dtype=object)}",3,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '1', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '2', '3'], dtype=object)}",3,silver,
"3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission.",ICLR_2021_2824,ICLR_2021,"Weakness:
While the authors claimed that they challenged the hypothesis by Kang et al. that the learning of feature representation and classifier should be completely decoupled in long-tail classification, from my perspective this paper is a nature extension of Kang et al. Similar to Kang et al., this paper further demonstrates the importance of progressive learning to address long-tailed distribution, which first mainly focuses on head classes (first stage) and then on tail classes (second stage). Different from Kang et al., a) this paper uses more aggressive sampling strategy at the second stage, by replacing class-balanced sampler with class-reversed sampler, and b) this paper shows that the feature can be further fine-tuned at the second stage.
This paper mainly reported the overall performance. It would be more convincing to provide the per-class performance to show the improvement on the tail classes.
This paper mainly focused on the comparison with Kang et al. Since Kang et al. extensively conducted experiments on the long-tailed ImageNet and Places datasets, how does the proposed approach perform on these two datasets?
While the time to switch from instance-balanced sampling to class-reversed sampling is a hyper-parameter as the authors mentioned, I was wondering if there is any principle to guide this design choice. Also, if this is dataset/distribution dependent or sensitive.
Consider an even smoother transition, from instance-balanced sampling to class-balanced sampling and finally to class-reversed sampling. Will this combination outperform the strategy in the paper?
In the discussion section, the authors claimed that the near-optimality of their method and that little room left for the improvement of the successive resampling strategy. This seems a very strong argument. I was wondering if there is any formal theoretical guarantee on this.
Post Rebuttal:
I do appreciate the efforts and additional experiments and theoretical analysis that the authors made in the rebuttal. While this paper proposed an interesting approach to long-tail recognition, some connections, distinctions, and comparisons with related work and thorough experimental analysis were missing in the original manuscript, as mentioned by other reviewers as well. Some of these concerns were addressed in the rebuttal, but not fully clarified. For example, the new comparison on the more challenging ImageNet-LT dataset (Table 3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,977,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"- In SI 6.5, the authors should mention that despite the preprocessing is identical to that in Mnih et al. [7], the evaluation is slightly different as no human starts are used.",NIPS_2018_464,NIPS_2018,"of the approach is the definition of the behavior characterization, which is domain-dependent, and may be difficult to set in some environments; however, the authors make this point clear in the paper and I understand that finding methods to define good behavior characterization functions is out of the scope of the submission. The article is properly motivated, review of related work is thorough and extensive experiments are conducted. The methods are novel and their limitations are appropriately stated and justified. The manuscript is carefully written and easy to follow. Source code is provided in order to replicate the results, which is very valuable to the community. Overall, I believe this is a high quality submission and should be accepted to NIPS. Please read more detailed comments below: - The idea of training M policies in parallel is somewhat related to [Liu et al., Stein Variational Policy Gradient], a method that optimizes for a distribution of M diverse and high performing policies. Please add this reference to the related work section. - The update of w in NSRA-ES somehow resembles the adaptation of parameter noise in [Plapper et al., âParameter Space Noise for Explorationâ, ICLR 2018]. The main difference is that the adaptation in Plappert et al. is multiplicative, thus yielding more aggressive changes. Although this is not directly compatible with the proposed method, where w is initialized to 0, I wonder whether the authors tried different policies for the adaptation of w. Given its similarity to ES (where parameter noise is used for structured exploration instead of policy optimization), I believe this is a relevant reference that should be included in the paper as well. - It seems that the authors report the best policy in plots and tables (i.e. if f(theta_t) > f(theta_{t+k}), the final policy weights are theta_t). This is different to the setup by Salimans et al. (c.f. Figure 3 in their paper). I understand that this is required for methods that rely on novelty only (i.e. NS-ES), but not for all of them. Please make this difference clear in the text. - Related to the previous point, I believe that section 3.1 lacks a sentence describing how the final policy is selected (I understand that the best performing one, in terms of episodic reward, is kept). - In the equation between lines 282 and 283, authors should state how they handle comparisons between episodes with different lengths. I checked the provided code and it seems that the authors pad the shorter sequence by replicating its last state in order to compare both trajectories. Also, the lack of a normalization factor of 1/T makes this distance increase with T and favors longer trajectories (which can be a good proxy for many Atari games, but not necessarily for other domains). These decisions should be understood by readers without needing to check the code. - There is no caption for Figure 3 (right). Despite it is mentioned in the text, all figures should have a caption. - The blue and purple color in the plots are very similar. Given the small size of some of these plots, it is hard to distinguish them -- especially in the legend, where the line is quite thin. Please use different colors (e.g. use some orangish color for one of those lines). - Something similar happens with the ES plot in Figure 2. The trajectory is quite hard to distinguish in a computer screen. - In SI 6.5, the authors should mention that despite the preprocessing is identical to that in Mnih et al. [7], the evaluation is slightly different as no human starts are used. - In SI 6.6, the network description in the second paragraph is highly overlapping with that in the first paragraph. ---------------------------------------------------------------------------------------------------------------------------------------------------------- Most of my comments had to do with minor modifications that the authors will adress. As stated in my initial review, I vote for accepting this submission.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,1163,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '3', '5'], dtype=object)}",X,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,
"3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.",NIPS_2021_1222,NIPS_2021,"Claims: 1.a) I think the paper falls short of the high-level contributions claimed in the last sentence of the abstract. As the authors note in the background section, there are a number of published works that demonstrate the tradeoffs between clean accuracy, training with noise perturbations, and adversarial robustness. Many of these, especially Dapello et al., note the relevance with respect to stochasticity in the brain. I do not see how their additional analysis sheds new light on the mechanisms of robust perception or provides a better understanding of the role stochasticity plays in biological computation. To be clear - I think the paper is certainly worthy of publication and makes notable contributions. Just not all of the ones claimed in that sentence.
1.b) The authors note on lines 241-243 that “the two geometric properties show a similar dependence for the auditory (Figure 4A) and visual (Figure 4B) networks when varying the eps-sized perturbations used to construct the class manifolds.” I do not see this from the plots. I would agree that there is a shared general upward trend, but I do not agree that 4A and 4B show “similar dependence” between the variables measured. If nothing else, the authors should be more precise when describing the similarities.
Clarifications: 2.a) The authors say on lines 80-82 that the center correlation was not insightful for discriminating model defenses, but then use that metric in figure 4 A&B. I’m wondering why they found it useful here and not elsewhere? Or what they meant by the statement on lines 80-82.
2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?
2.c) The authors report mean capacity and width in figure 2. I think this is the mean across examples as well as across seeds. Is the STD also computed across examples and seeds? The figure caption says it is only computed across seeds. Is there a lot of variability across examples?
2.d) I am unsure why there would be a gap between the orange and blue/green lines at the minimum strength perturbation for the avgpool subplot in figure 2.c. At the minimum strength perturbation, by definition, the vertical axis should have a value of 1, right? And indeed in earlier layers at this same perturbation strength the capacities are equal. So why does the ResNet50 lose so much capacity for the same perturbation size from conv1 to avgpool? It would also be helpful if the authors commented on the switch in ordering for ATResNet and the stochastic networks between the middle and right subplots.
General curiosities (low priority): 3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.
3.b) There appears to be a pretty big difference in the overall trends of the networks when computing the class manifolds vs exemplar manifolds. Specifically, I think the claims made on lines 191-192 are much better supported by Figure 1 than Figure 2. I would be interested to hear what the authors think in general (i.e. at a high/discussion level) about how we should interpret the class vs exemplar manifold experiments.
Nitpick, typos (lowest priority): 4.a) The authors note on line 208 that “Unlike VOneNets, the architecture maintains the conv-relu-maxpool before the first residual block, on the grounds that the cochleagram models the ear rather than the primary auditory cortex.” I do not understand this justification. Any network transforming input signals (auditory or visual) would have to model an entire sensory pathway, from raw input signal to classification. I understand that VOneNets ignore all of the visual processing that occurs before V1. I do not see how this justifies adding the extra layer to the auditory network.
4.b) It is not clear why the authors chose a line plot in figure 4c. Is the trend as one increases depth actually linear? From the plot it appears as though the capacity was only measured at the ‘waveform’ and ‘avgpool’ depths; were there intermediate points measured as well? It would be helpful if they clarified this, or used a scatter/bar plot if there were indeed only two points measured per network type.
4.c) I am curious why there was a switch to reporting SEM instead of STD for figures 5 & 6.
4.c) I found typos on lines 104, 169, and the fig 5 caption (“10 image and”).",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '5'], dtype=object)}",4,hard,430,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '3'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '5'], dtype=object)}",4,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '5'], dtype=object)}",4,hard,
"- First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what ""error""?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as ""sub-standard"".",NIPS_2016_283,NIPS_2016,"weakness of the paper are the empirical evaluation which lacks some rigor, and the presentation thereof: - First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what ""error""?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as ""sub-standard"". - The results comparing standard- vs. evolutional dropout on shallow models should be presented as a mean over many runs (at least 10), ideally with error-bars. The plotted curves are obviously from single runs, and might be subject to significant fluctuations. Also the models are small, so there really is no excuse for not providing statistics. - I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark: - In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor: *",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,712,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', 'X', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '3'], dtype=object)}",4,hard,
1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones.,NIPS_2020_295,NIPS_2020,"1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones. 2. Some methods use epochs and pretrain epochs as 200, while the reported InvP uses 800 epochs. What are the results of InvP with epochs as 200? It would be more clear after adding these results into the tables. 3. The proposed method adopts memory bank to update vi, as detailed in the beginning of Sec.3. What the results would be when adopting momentum queue and current batch of features? As the results of SimCLR and MoCo are better than InsDis, it would be nice to have those results.",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,77,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"2) analyze the domain gap. It would be nice to add some discussions about the gap between datasets. Some datasets are closer to each other thus the adaption may not be a big issue. Also, if the method is able to finetune a pre-trained model on synthetic data, then the value of the approach would be much higher.",NIPS_2022_742,NIPS_2022,"It seems that the 6dof camera poses of panoramas are required to do the projection. Hence, precisely speaking, the method is not fully self-supervised but requires camera pose ground truth. This is usually accessible, easier compared to the ground truth layout, but may also cause error for the layout projection and thus hurts the overall finetuning performance.
The experiment could be stronger to demonstrate the effectiveness of the method from two aspects: 1) a stronger baseline. It seems SSLayout360 is in general outperforming HorizonNet. It would be convincing to show that this method is able to improve powerful backbones. 2) analyze the domain gap. It would be nice to add some discussions about the gap between datasets. Some datasets are closer to each other thus the adaption may not be a big issue. Also, if the method is able to finetune a pre-trained model on synthetic data, then the value of the approach would be much higher.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '2', '5'], dtype=object)}",5,silver,806,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '4', '5'], dtype=object)}",3,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '4'], dtype=object)}",4,gold,
"3. In the experimental section of the paper, the standard deviation after multiple experiments is not provided. The improvement brought by SoRA compared with the baseline is quite limited, which may be due to random fluctuations. The author should clarify which effects are within the range of standard deviation fluctuations and which are improvements brought by the SoRA method.",jxgz7FEqWq,EMNLP_2023,"1. The comparison experiment with the MPOP method is lacking (See Missing References [1]). MPOP method is a lightweight fine-tuning method based on matrix decomposition and low-rank approximation, and it has a strong relevance to the method proposed in the paper. However, there is no comparison with this baseline method in the paper.
2. In Table 4, only the training time of the proposed method and AdaLoRA is compared, lacking the efficiency comparison with LoRA, Bitfit, and Adapter.
3. In the experimental section of the paper, the standard deviation after multiple experiments is not provided. The improvement brought by SoRA compared with the baseline is quite limited, which may be due to random fluctuations. The author should clarify which effects are within the range of standard deviation fluctuations and which are improvements brought by the SoRA method.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,671,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', 'X'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"1. The experiment section could be improved. For example, it is better to carry significance test on the human evaluation results. It is also beneficial to compare the proposed method with some most recent LLM.",Akk5ep2gQx,EMNLP_2023,"1. The experiment section could be improved. For example, it is better to carry significance test on the human evaluation results. It is also beneficial to compare the proposed method with some most recent LLM.
2. The classifier of determining attributes using only parts of the sentence may not perform well. Specifically, I am wondering what is the performance of the attribute classifer obtained using Eq.2 and Eq.7.
3. Some of the experiment results could be explained in more details. For example, the author observes that ""Compared to CTRL, DASC has lower Sensibleness but higher Interestingness"", but why? Is that because DASC is bad for exhibiting Sensibleness? Similar results are also observed in Table1.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,810,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '4', '5'], dtype=object)}",3,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '4', '5'], dtype=object)}",3,hard,
"3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the âelementsâ equivalent to âstatesâ? or âactionsâ? More should be elaborated.",NIPS_2018_821,NIPS_2018,"1. A few parts of this paper are not very clear or not sufficiently provided, such as the model details. Section 4.3 should be addressed more to make it clearer since some concepts/statements are misleading or confusing. 2. The trace to code model is complex, which requires as many LSTMs as the number of input/output pairs, and it may be hard to be applied to other program synthesis scenarios. 3. Apart from the DSL, more experiments on dominant PL (e.g., Python) would be appreciated by people in this research field. Details are described as below: 1. For a given program, how to generate diverse execution traces that can be captured by the I/O -> Trace model? Since execution traces are generated by running the program on N I/O pairs, it is possible that some execution traces have a large overlap. For example, in the extreme case, two execution traces may be the same (or very similar) given different I/O pairs. 2. The authors involve the program interpreter in their approach, which is a good trial and it should help enhance the performance. However, I am curious about is it easy to be integrated with the neural network during training and testing? 3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the âelementsâ equivalent to âstatesâ? or âactionsâ? More should be elaborated. 4. Line 183-line 184, is it necessary to use embedding for only four conditionals (of Boolean type)? only 16 possible combinations. 5. As depicted in Figure 3, if more I/O pairs are provided, the Trace->Code should be very complex since each i/o example requires such an LSTM model. How to solve this issue? 6. In essence, the Trace->Code structure is a Sequence to Sequence model with attention, the only differences are the employment of I/O pair embedding and the max pooling on multiple LSTM. How are the I/O pair embeddings integrated into the computation? Some supplementary information should be provided. 7. It is interesting to find that model trained on gold traces perform poorly on inferred traces, the authors do not give a convincing explanation. More exploration should be conducted for this part. 8. It would be better if some synthesized program samples are introduced in an appendix or other supplementary documents.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,884,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,
"3) the artificial networks trained using ASAP (and similar methods) do not necessarily resemble biological networks (other than the weight transport problem, which is of arguable importance) more than other techniques like backprop. Again, I do not hold the authors accountable for this, and this does not affect the review I gave.",NIPS_2021_942,NIPS_2021,"The biggest real-world limitation is that the method does not perform as well as backprop. This is unfortunate, but also understandable. The authors do mention that ASAP has a lower memory footprint, but there are also other methods that can reduce memory footprints of neural networks trained using backprop. Given that this method is worse than backprop, and it is also not easy to implement, I cannot see any practical use for it.
On the theoretical side, although the ideas here are interesting, I take issue with the term ""biologically plausible"" and the appeal to biological networks. Given that cognitive neuroscience has not yet proceeded to a point where we understand how patterns and learning occur in brains, it is extremely premature to try and train networks that match biological neurons on the surface, and claim that we can expect better performance because they are biology inspired. To say that these networks behave more similarly to biological neurons is true only on the surface, and the claim that these networks should therefore be better or superior (in any metric, not just predictive performance) is completely unfounded (and in fact, we can see that the more ""inspiration"" we draw from biological neurons, the worse our artificial networks tend to be). In this particular case, humans can perform image classification nearly perfectly, and better than the best CNNs trained with backprop. And these CNNs trained with backprop do better than any networks trained using other methods (including ASAP). To clarify, I do not blame (or penalize) the authors for appealing to biological networks, since I think this is a bigger issue in the theoretical ML community as a whole, but I do implore them to soften the language and recognize the severe limitations that prevent us from claiming homology between artificial neural networks and biological neural networks (at least in this decade). I encourage the authors to explicitly clarify that: 1) biological neurons are not yet understood, so drawing inspiration from the little we know does not improve our chances at building better artificial networks; 2) the artificial networks trained using ASAP (and similar methods) do not improve our understanding of biological neurons at all; and 3) the artificial networks trained using ASAP (and similar methods) do not necessarily resemble biological networks (other than the weight transport problem, which is of arguable importance) more than other techniques like backprop. Again, I do not hold the authors accountable for this, and this does not affect the review I gave.",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '1', '1'], dtype=object)}",1,gold,465,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '1', '3'], dtype=object)}",5,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', 'X', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '1', '2'], dtype=object)}",1,silver,
"2) The chat-gpt baseline is very rudimentary. Few-shot approach isn’t tested. Also, including the discourse relation information in the prompts (probably in a Chain-of-Thought style approach) might yield good results. This will only add to the paper’s evaluation. But, it is extraneous to their line of evaluation as presented.",xbnNgqGefc,EMNLP_2023,"1) All of the empirical evaluation is performed on one dataset. This makes it hard to judge the generalizability of the approach. But, it is understandable given the difficulty in annotating data for such a task.
2) The chat-gpt baseline is very rudimentary. Few-shot approach isn’t tested. Also, including the discourse relation information in the prompts (probably in a Chain-of-Thought style approach) might yield good results. This will only add to the paper’s evaluation. But, it is extraneous to their line of evaluation as presented.
3) In addition to sentence level and token level performance, it would have been interesting to see document level evaluation of propaganda as well. It seems like a natural setting for the task which is missing in their evaluation.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '4', '5'], dtype=object)}",2,hard,1193,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '4', '3'], dtype=object)}",2,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '4', '4'], dtype=object)}",4,silver,
"3. Dynamic precision control during training might only show meaningful performance gains on bit-serial accelerators. However, most existing ML accelerators tend to use bit-parallel fixed-point numbers, this might restrict the implications of the proposed methodology.",NIPS_2020_1016,NIPS_2020,"1. The PFQ algorithm introduced many hyperparameters, and I am curious how the authors chose the parameters \epsilon and \alpha. The authors simply claimed these parameters are determined from the four-stage manual PFQ from Figure 1, and then claim that FracTrain is insensitive to hyperparameters. First, the precision choices of the four stage PFQ in Figure 1 is already arbitrary. Second, I do not think the empirical results can support the claim that FracTrain is insensitive to hyperparameters. I would encourage the authors to have an ablation study of \epsilon and \alpha. I do understand an ablation study of various precision combinations is shown in the appendix, but this might not provide enough insights for users of FracTrain that simply want to know what is the best hyperparameter combination to use. 2. I found the MACs and Energy results reported in the paper needs further explanation. For instance, in Table2, it seems to me MACs cannot be a useful measurement since SBM and FracTrain might use different precisions. Even if the MACs numbers are the same, low-precision operations will surely be more energy efficient. A more useful measurement metric might be bitwise operations. In terms of the Energy reported in this paper, the authors claim it is calculated from an RTL design. However, BitFusion is an inference accelerator, what modifications have you done to the BitFusion RTL to support this training energy estimation? What is the reuse pattern for gradients/activations? 3. Dynamic precision control during training might only show meaningful performance gains on bit-serial accelerators. However, most existing ML accelerators tend to use bit-parallel fixed-point numbers, this might restrict the implications of the proposed methodology. 4. I think this paper has missed a number of citations in the recent advances of dynamic inference methods. Dynamic channel control [1,2] and dynamic precisions [3] have recently been widely explored and these citations are not seen in this paper. [1] Gao, Xitong, et al. ""Dynamic channel pruning: Feature boosting and suppression."" ICLR 2018. [2] Hua, Weizhe, et al. ""Channel gating neural networks."" Advances in Neural Information Processing Systems. 2019. [3] Song, Zhuoran, et al. ""DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration."" ISCA 2020",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '1', '2'], dtype=object)}",2,silver,948,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '3', '3'], dtype=object)}",3,silver,
"1. The authors have not covered more on the types of activities captured in the datasets, and their importance in smart homes, particularly from the perspective of occupant comfort and energy efficiency.",AqXzHRU2cs,ICLR_2024,"1. The authors have not covered more on the types of activities captured in the datasets, and their importance in smart homes, particularly from the perspective of occupant comfort and energy efficiency.
2. The number of sensors used to collect data seems a lot. In practice, its not practical to have so many sensors in a home collecting information. The authors should try some benchmarking on a subset of sensors if the dataset permits.
3. How will a sensor fusion approach work in this scenario?
4. What are the motivations behind hierarchical approach?
5. For Milan and Cairo, the temporal method might not be effective since the number of days in the experiment is less.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '4'], dtype=object)}",4,silver,882,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', 'X'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,
"2. Table 2 includes several work but drops out Vidgen et al, 2021, which might be really similar to the dataset presented in this work though the size varies significantly here. So, why is this dataset not used as a potential benchmark for evaluation (for investigating the role of context in detection of hate) as well?",ARR_2022_342_review,ARR_2022,"1. The importance of context is well known and well-established in several prior work related to hate speech. While the paper cites works such as Gao and Huang, 2017 and Vidgen, et al., it just mentions that they don’t identify the role of context in annotation or modeling. The former definitely considers its role for modeling and the latter incorporates it in the annotation phase. Though this work performs analysis of corpus and model to study the role of the context, the claim of being the first work to establish the importance of context may be a little stretched.
2. Table 2 includes several work but drops out Vidgen et al, 2021, which might be really similar to the dataset presented in this work though the size varies significantly here. So, why is this dataset not used as a potential benchmark for evaluation (for investigating the role of context in detection of hate) as well?
3. Though MACE can be used to assess the competent annotators and eliminate redundant annotators, it could be challenging to use when it involves most ambiguous content.
4. Some of the analysis and results discussed (for eg. section 6) might be specific to the tested Roberta model. More experiments using different architectures are needed to determine if the findings and errors that arise are consistent across different models and different settings.
Claim of being the first one to recognize the importance of context might be too stretched.
More experiments with multiple runs with different random seeds for the dataset split will help report the mean score and the standard deviation. This will help us understand the sensitivity of the model to the data split or order of training etc.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1198,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '5', 'X'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.,NIPS_2017_401,NIPS_2017,"Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.
Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility:
Appears to be reproducible.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '1', '1'], dtype=object)}",1,gold,875,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['2', '2', '2'], dtype=object)}",2,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '2', '2'], dtype=object)}",2,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['1', '2', '1'], dtype=object)}",1,silver,
"- some details are missing. For example, how to design the rewards is not fully understandable.",NIPS_2018_591,NIPS_2018,"Weakness: - some details are missing. For example, how to design the rewards is not fully understandable. - some model settings are arbitrarily set and are not well tested. For example, what is the sensitivity of the model performance w.r.t. the number of layers used in GCN for both the generator and discriminator?",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '5', '2'], dtype=object)}",2,silver,959,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '3', '2'], dtype=object)}",2,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '3', '1'], dtype=object)}",1,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '3', '2'], dtype=object)}",2,silver,
- Section 3 and Section 4 are slightly redundant: maybe putting the first paragraph of sec 4 in sec 3 and putting the remainder of sec 4 before section 3 would help.,NIPS_2018_606,NIPS_2018,", I tend to vote in favor of this paper. * Detailed remarks: - The analysis in Figure 4 is very interesting. What is a possible explanation for the behaviour in Figure 4(d), showing that the number of function evaluations automatically increases with the epochs? Consequently, how is it possible to control the tradeoff between accuracy and computation time if, automatically, the computation time increases along the training? In this direction, I think it would be nice to see how classification accuracy evolves (e.g. on MNIST) with the precision required. - In Figure 6, an interpolation experiment shows that the probability distribution evolves smoothly along time, which is an indirect way to interpret it. Since this is a low (2D) dimensional case, wouldn't it be possible to directly analyze the learnt ODE function, by looking at its fixed points and their stability? - For the continuous-time time-series model, subsec 6.1 clarity should be improved. Regarding the autoencoder formulation, why is an RNN used for the encoder, and not an ODE-like layer? Indeed, the authors argue that RNNs have trouble coping with such time-series, so this might also be the case in the encoding part. - Do the authors plan to share the code of the experiments (not only of the main module)? - I think it would be better if notations in appendix A followed the notations of the main paper. - Section 3 and Section 4 are slightly redundant: maybe putting the first paragraph of sec 4 in sec 3 and putting the remainder of sec 4 before section 3 would help.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,894,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', 'X', '1'], dtype=object)}",3,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', '3'], dtype=object)}",3,silver,
"- Some questionable design choices. Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting. How are such factors controlled?",KE5QunlXcr,EMNLP_2023,"- The PLMs used (BERT) are by current standards, quite old, and quite small. As work in scaling PLMs up to sizes orders of magnitude greater, performance on syntactic tasks has shown to improve naturally (along with many other useful emergent forms of knowledge). Some comparison to larger models / application of this method to such models, is necessary to ensure that the method has any practical purpose.
- There are also no baselines from existing work. There are other forms of fine-tuning, such as adapters, which seek to add additional knowledge to PLMs with less chance of catastrophic forgetting. The authors even cite one of these papers. This is also a confusing oversight, because the many appropriate inline citations which contrast various decisions in this work to decisions in existing work demonstrate a great familiarity with the literation, so lacking any comparison to any existing methods is an odd oversight.
- Some questionable design choices. Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting. How are such factors controlled?
- Related, there is questionable motivation. Often when talking about catastrophic forgetting, both the original training and the new task are both relevant. This is clear in the context of robotics, where learning a new behavior should not result in hindering the robot from performing existing behaviors. Is this true in this case? PPL is almost always not a valuable end goal, and the entire PLM/LLM paradigm is built around this notion of pre-training in whatever way leads to learning useful linguistic representations, before fine-tuning, aligning, or few-shotting the model towards the task the user actually cares about. If users never care about both tasks in approximately equal measure, than what good is retaining the original model weights which were not pertinent to the target task?
- There's arguably too much going on here. The focus of the paper aims to be about catastrophic forgetting, but secondary to that, is also the problem of matching the right syntactic fine-tuning task to the right problem. This is not entirely known a priori, so all possible pairings are explored, but realistically a good guess can be made (as it would likely be if pursued in a more practical setting). For instance, it is no surprise that the phrase syntax task helps with key phrase identification. The disadvantage of the exhaustive approach is that it has both distracted from the main takeaway points while cutting into the space available for supporting the main hypothesis.
- No inclusion of baselines from existing work / SOTA on performance tables
- Key extraction F1 results are better than standard optimizers, but negligibly so.
- No discussion of GC vs EWC. When a priori would you choose which method? If the paper included only one such method, traditional optimizers would be the best choice in most situations.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '5', '3'], dtype=object)}",4,hard,283,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', 'X', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,
"• Minor suggestion: the average of k -means objectives with multiple seeds are used as a baseline, I think the minimal k -means objective over multiple seeds is more reasonable. [1] Jin, Chi, et al. ""Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences."" Advances in neural information processing systems 29 (2016): 4116-4124. [2] Fränti, Pasi, and Sami Sieranoja. ""K-means properties on six clustering benchmark datasets."" Applied Intelligence 48.12 (2018): 4743-4759.",ICLR_2022_74,ICLR_2022,"Weakness
• The theorem applies when the label error is small, less than 1/7. However, it might be non-trivial to obtain a predictor with that quality in the first place. For example, in the experiments, the initial solution are derived from k
-means (Lloyd's) algorithm, which might require many initial seeds to attain a good solution. Are there any guarantees can be made when the initial label error is larger?
• When k
gets larger, the k
-means algorithm (even with k
-means++) solution can be stuck at local minima, with arbitrarily worse objective [1]. How would algo+ k
-means++/predictor behave compare with k
-means++ (with multiple seeds)? Can the algorithm help escape the local minima and attain a much better solution? There is a collection of synthetic datasets [1] for k-means to understand the performance of the algorithm. I suggest the authors take these benchmark datasets into consideration for the experiments for evaluation. In the current experiment, only k = 10 and k = 25
are tested, and it is hard to see the comparison of algorithms when k
gets larger, which is a more challenging case for the k
-means problem.
• Minor suggestion: the average of k
-means objectives with multiple seeds are used as a baseline, I think the minimal k
-means objective over multiple seeds is more reasonable.
[1] Jin, Chi, et al. ""Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences."" Advances in neural information processing systems 29 (2016): 4116-4124. [2] Fränti, Pasi, and Sami Sieranoja. ""K-means properties on six clustering benchmark datasets."" Applied Intelligence 48.12 (2018): 4743-4759.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,1144,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '5', '3'], dtype=object)}",1,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '4', '3'], dtype=object)}",3,silver,
"3. The evaluation in the paper is not sufficiently comprehensive and lacks transparency regarding the experiment setup. For instance, there is no mention of the number of different sets of in-content examples used in the experiments. Additionally, the paper does not explore the effects of varying the number of In-Context Examples. Moreover, the evaluation relies solely on one dataset, which may limit the generalizability of the results.",N3a2vVk8vu,EMNLP_2023,"1. The paper lacks specific details on how SUMMARIZER effectively eliminates irrelevant information. Particularly, it does not clearly define what qualifies as irrelevant information that should be avoided when providing input to the LLM. Additionally, there is a lack of an error study on SUMMARIZER, which could help determine whether there is a possibility of crucial information being mistakenly removed by the prompt.
2. The paper's failure to provide comprehensive details about the baseline models for comparison, such as REACT, is a notable limitation. The differences between ASH and REACT are not highlighted, making it challenging to discern the most crucial components of ASH that contribute to its superiority over existing approaches.
3. The evaluation in the paper is not sufficiently comprehensive and lacks transparency regarding the experiment setup. For instance, there is no mention of the number of different sets of in-content examples used in the experiments. Additionally, the paper does not explore the effects of varying the number of In-Context Examples. Moreover, the evaluation relies solely on one dataset, which may limit the generalizability of the results.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,1522,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', 'X'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '4'], dtype=object)}",4,silver,
"-General Discussion:- The title is a bit ambiguous, it would be good to clarify that you are referring to machine comprehension of text, and not human reading comprehension, because “reading comprehension” and “readability” usually mean that.",ACL_2017_148_review,ACL_2017,"- The goal of your paper is not entirely clear. I had to read the paper 4 times and I still do not understand what you are talking about!
- The article is highly ambiguous what it talks about - machine comprehension or text readability for humans - you miss important work in the readability field - Section 2.2. has completely unrelated discussion of theoretical topics.
- I have the feeling that this paper is trying to answer too many questions in the same time, by this making itself quite weak. Questions such as “does text readability have impact on RC datasets” should be analyzed separately from all these prerequisite skills.
- General Discussion: - The title is a bit ambiguous, it would be good to clarify that you are referring to machine comprehension of text, and not human reading comprehension, because “reading comprehension” and “readability” usually mean that.
- You say that your “dataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty”, but this depends on the method/features used for answer detection, e.g. if you use POS/dependency parse features.
- You need to proofread the English of your paper, there are some important omissions, like “the question is easy to solve simply look..” on page 1.
- How do you annotate datasets with “metrics”??
- Here you are mixing machine reading comprehension of texts and human reading comprehension of texts, which, although somewhat similar, are also quite different, and also large areas.
- “readability of text” is not “difficulty of reading contents”. Check this: DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact information. - it would be good if you put more pointers distinguishing your work from readability of questions for humans, because this article is highly ambiguous.
E.g. on page 1 “These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions” you should add “for machine comprehension” - Section 3.1. - Again: are you referring to such skills for humans or for machines? If for machines, why are you citing papers for humans, and how sure are you they are referring to machines too?
- How many questions the annotators had to annotate? Were the annotators clear they annotate the questions keeping in mind machines and not people?",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,1421,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"4. But the LUQ itself is rather straightforward to design, once the goal of designing logarithmic and unbiased quantizer is clear. The approaches in Sec. 5 are also rather standard and to some extent explored in previous literature. I'd say the main contribution of this paper is showing that such a simple combination of existing techniques is sufficient to achieve (surpringly good) accuracy, rather than proposing novel techniques.",ICLR_2023_1088,ICLR_2023,"The novelty is somewhat thin: Until the second half of page 5, the paper is mostly presenting existing backgrounds. The novelty mainly falls in Sec. 4. But the LUQ itself is rather straightforward to design, once the goal of designing logarithmic and unbiased quantizer is clear. The approaches in Sec. 5 are also rather standard and to some extent explored in previous literature. I'd say the main contribution of this paper is showing that such a simple combination of existing techniques is sufficient to achieve (surpringly good) accuracy, rather than proposing novel techniques.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '1', '1'], dtype=object)}",1,gold,1429,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '2'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '1'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '5', '2'], dtype=object)}",2,silver,
"- ""D"" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion. Review summary:",NIPS_2018_76,NIPS_2018,"- A main weakness of this work is its technical novelty with respect to spatial transformer networks (STN) and also the missing comparison to the same. The proposed X-transformation seems quite similar to STN, but applied locally in a neighborhood. There are also existing works that propose to apply STN in a local pixel neighborhood. Also, PointNet uses a variant of STN in their network architecture. In this regard, the technical novelty seems limited in this work. Also, there are no empirical or conceptual comparisons to STN in this work, which is important. - There are no ablation studies on network architectures and also no ablation experiments on how the representative points are selected. - The runtime of the proposed network seems slow compared to several recent techniques. Even for just 1K-2K points, the network seem to be taking 0.2-0.3 seconds. How does the runtime scales with more points (say 100K to 1M points)? It would be good if authors also report relative runtime comparisons with existing techniques. Minor corrections: - Line 88: ""lose"" -> ""loss"". - line 135: ""where K"" -> ""and K"". Minor suggestions: - ""PointCNN"" is a very short non-informative title. It would be good to have a more informative title that represents the proposed technique. - In several places: ""firstly"" -> ""first"". - ""D"" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion. Review summary: - The proposed technique is sensible and the performance on different benchmarks is impressive. Missing comparisons to established STN technique (with both local and global transformations) makes this short of being a very good paper. After rebuttal and reviewer discussion: - I have the following minor concerns and reviewers only partially addressed them. 1. Explicit comparison with STN: Authors didn't explicitly compare their technique with STN. They compared with PointNet which uses STN. 2. No ablation studies on network architecture. 3. Runtimes are only reported for small point clouds (1024 points) but with bigger batch sizes. How does runtime scale with bigger point clouds? Authors did not provide new experiments to address the above concerns. They promised that a more comprehensive runtime comparison will be provided in the revision. Overall, the author response is not that satisfactory, but the positive aspects of this work make me recommend acceptance assuming that authors would update the paper with the changes promised in the rebuttal. Authors also agreed to change the tile to better reflect this work.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,883,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', 'X', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"1. Could we extend the protected feature A to a vector form? For instance, A represents multiple attributes.",NIPS_2022_1048,NIPS_2022,"and comments: 1. This paper mainly focused on group sufficiency as the fairness metric. Is it possible to derive similar results under criteria of demographic parity or equalized odds? What are the potential challenges for other fair metrics? Under these settings, is it still possible to achieve both fairness and accuracy for many subgroups? 2. The regularization coefficient λ
seems to have a joint optimal value in 0.1-2. Could you elaborate more on why both fairness and accuracy drop when λ
is large? 3. Is it possible to assume the general gaussian distribution rather than isotropic gaussian in the proposed algorithm? What is the difference? 4. Can the proposed theoretical analysis be extended for a regression or segmentation task? For example, could we obtain the same results as the classification task? 5. Could you explain a bit more on the intuition of group sufficiency? Is there any relation to the well-known sufficient statistics? Other comments: 1. Could we extend the protected feature A
to a vector form? For instance, A
represents multiple attributes. 2. In the Introduction part, the authors introduced a medical therapy instance to present the importance of group sufficiency. Could you explain a bit more about the difference between sufficiency and DP/EO metrics in the real-world application scenarios? 3. In line 225 and line 227, the mathematical expression of gaussian distribution is ambiguous. 4. In section 4.4, it mentions the utilization of Monte-Carlo sampling method. I am curious about the influence of different sampling numbers.
================================================ Thanks the effort from the authors, and I am satisfied with the rebuttal. I would like to raise my score to 8.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '4'], dtype=object)}",4,silver,1218,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '4', '3'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '2', 'X'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '3', '3'], dtype=object)}",3,silver,
2. ChatGPT shows a great percentage of abstention than other models. Is that fair to compare their accuracies?,AGVANImv7S,EMNLP_2023,"**Weaknesses：**
1. The proposed evaluation pipeline is similar to prior works.
2. ChatGPT shows a great percentage of abstention than other models. Is that fair to compare their accuracies?
3. Reproducibility. The author doesn't mention if the code will be available to the public.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '4', '4'], dtype=object)}",4,silver,1128,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '3', '5'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['X', '2', '5'], dtype=object)}",X,hard,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '2', '4'], dtype=object)}",2,silver,
"3. Make the captions more descriptive. It's annoying to have to search through the text for your interpretation of the figures, which is usually on a different page 4. Explain the scramble network better...",NIPS_2019_165,NIPS_2019,"of the approach and experiments or list future direction for readers. The writeup is exceptionally clear and well organized-- full marks! I have only minor feedback to improve clarity: 1. Add a few more sentences explaining the experimental setting for continual learning 2. In Fig 3, explain the correspondence between the learning curves and M-PHATE. Why do you want to want me to look at the learning curves? Does worse performing model always result in structural collapse? What is the accuracy number? For the last task? or average? 3. Make the captions more descriptive. It's annoying to have to search through the text for your interpretation of the figures, which is usually on a different page 4. Explain the scramble network better... 5. Fig 1, Are these the same plots, just colored differently? It would be nice to keep all three on the same scale (the left one seems condensed) M-PHATE results in significantly more interpretable visualization of evolution than previous work. It also preserves neighbors better (Question: why do you think t-SNE works better in two conditions? The difference is very small tho). On continual learning tasks, M-PHATE clearly distinguishes poor performing learning algorithms via a collapse. (See the question about this in 5. Improvement). The generalization vignette shows that the heterogeneity in M-PHATE output correlates with performance. I would really like to recommend a strong accept for this paper, but my major concern is that the vignettes focus on one dataset MNIST and one NN architecture MLP, which makes the experiments feel incomplete. The results and observations made by authors would be much more convincing if they could repeat these experiments for more datasets and NN architectures.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,227,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '5', '1'], dtype=object)}",4,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '2', 'X'], dtype=object)}",3,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,
"4. Authors claim that the regret bound for the proposed mini-batch method is cast to appendix. However, I didn’t find the regret bound for the mini-batch estimator in the supplementary. [1] Zalan Borsos, Andreas Krause, and Kfir Y Levy. Online Variance Reduction for Stochastic Optimization.",NIPS_2020_1316,NIPS_2020,"My concerns are as follows. 1. The regret in [1] is defined on function value while this work defines it with the norm of gradient. It is better to provide the same measurement for a fair comparison. 2. The topic that reduces variance with importance sampling is not new. Besides vanilla SGD, more baselines with variance reduction or importance sampling should be included in experiments for demonstration, especially when the theoretical improvement is not significant. 3. It lacks the experiments for the main method in this work that the gradient is computed from a single example. The only experiment is for mini-batch estimator while the additional experiment for the extreme case is necessary for evaluating the effectiveness of the theorem. 4. Authors claim that the regret bound for the proposed mini-batch method is cast to appendix. However, I didn’t find the regret bound for the mini-batch estimator in the supplementary. [1] Zalan Borsos, Andreas Krause, and Kfir Y Levy. Online Variance Reduction for Stochastic Optimization.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '1', '4'], dtype=object)}",4,silver,1297,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '4', '5'], dtype=object)}",X,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '3', '5'], dtype=object)}",4,hard,
"- The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented).",ICLR_2023_1511,ICLR_2023,"Weakness_ - The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented). - Similarly, it is lacking a ""So What"" on the bounds provided, which are often just left there as final statements, without an analysis that explains whether 1) they are (likely to be) tight and 2) what this implies for practitioners. - Although well-written, the paper felt quite dense, even compared to other pure-math ML papers. More examples such as Figure 2 would help. - As far as I understood, the assumption on the non-linearities discards the sigmoid and the softmax, which are popular non-linearities. It would be good to acknowledge this directly by name.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '2', '1'], dtype=object)}",5,hard,974,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '2', '1'], dtype=object)}",3,hard,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '1', '1'], dtype=object)}",1,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '2', '1'], dtype=object)}",3,hard,
"- lines 32 - 37: You discuss how the regret cannot be sublinear, but proceed to prove that your method achieves T^{1/2} regret. Do you mean that the prediction error over the entire horizon T cannot be sublinear?",NIPS_2018_428,NIPS_2018,"weakness which decreased my score. Some line by line comments: - lines 32 - 37: You discuss how the regret cannot be sublinear, but proceed to prove that your method achieves T^{1/2} regret. Do you mean that the prediction error over the entire horizon T cannot be sublinear? - eq after line 145: typo --- i goes from 1 to n and since M,N are W x k x n x m, the index i should go in the third position. Based on the proof, the summation over u should go from tau to t, not from 1 to T. - line 159: typo -- ""M"" --> Theta_hat - line 188: use Theta_hat for consistency. - line 200: typo -- there should no Pi in the polynomial. - line 212: typo --- ""beta^j"" --> beta_j - line 219: the vector should be indexed - lines 227 - 231: the predictions in hindsight are denoted once by y_t^* and once by hat{y}_t^* - eq after line 255: in the last two terms hat{y}_t --> y_t Comments on the Appendix: - General comment about the Appendix: the references to Theorems and equations are broken. It is not clear if a reference points to the main text or to the appendix. - line 10: Consider a noiseless LDS... - line 19: typo -- N_i ---> P_i - equation (21): same comment about the summation over u as above. - line 41: what is P? - line 46: typo --- M_omega' ---> M_ell' - eq (31): typo -- no parenthesis before N_ell - line 56: the projection formula is broken - eq (56): why did you use Holder in that fashion? By assumption the Euclidean norm of x is bounded, so Cauchy Schwartz would avoid the extra T^{1/2}. ================== In line 40 of the appendix you defined R_x to be a bound on \|x\|_2 so there is no need for the inequality you used in the rebuttal. Maybe there is a typo in line 40, \|x\|_2 maybe should be \|x\|_\infty",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,913,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['X', 'X', 'X'], dtype=object)}",X,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"1. The motivation for the choice of $\theta = \frac{\pi}{2}(1-h)$ from theorem 3, is not very straightforward and clear. The paper states that this choice is empirical, but there is very little given in terms of motivation for this exact form.",4A5D1nsdtj,ICLR_2024,"1. The motivation for the choice of $\theta = \frac{\pi}{2}(1-h)$ from theorem 3, is not very straightforward and clear. The paper states that this choice is empirical, but there is very little given in terms of motivation for this exact form.
2. For this method, the knowledge of the homophily ratio seems to be important. In many practical scenarios, this may not be possible to be estimated accurately and even approximations could be difficult. No ablation study is presented showing the sensitivity of this model to the accurate knowledge of the homophily ratio.
3. The HetFilter seems to degrade rapidly past h=0.3 whereas OrtFilter is lot more graceful to the varying homophily ratio. It is unclear whether one would consider the presented fluctuations as inferior to the presented UniBasis. For UniBasis, in the region of h >= 0.3, the choice of tau should become extremely important (as is evident from Figure 4, where lower tau values can reduce performance on Cora by about 20 percentage points).",3,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['2', '4', '4'], dtype=object)}",4,silver,132,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['1', 'X', '4'], dtype=object)}",1,hard,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['3', '5', '5'], dtype=object)}",5,silver,
"1. The tasks are somewhat standard - Figure captioning, and matching figures/sub-figures to appropriate captions. It would have been nice to see some unique tasks created from this nice dataset showcasing the diversity of images/plots. e.g. some variety of interleaved image-text tasks such as Question Answering from images could have been considered.",DEOV74Idsg,ICLR_2025,"The main weaknesses are that considering the diversity in the data, the tasks do not seem to go beyond standard tasks; and even on the figure captioning tasks, the analysis is lacking, particularly in terms of representation of strong evaluations from domain experts.
1. The tasks are somewhat standard - Figure captioning, and matching figures/sub-figures to appropriate captions. It would have been nice to see some unique tasks created from this nice dataset showcasing the diversity of images/plots. e.g. some variety of interleaved image-text tasks such as Question Answering from images could have been considered.
2. It would have been nicer to have more detailed analysis of model responses for a few images (3-5) in about 10 domains. Where experts weigh-in on model responses even for just the figure captioning task to evaluate the strengths and weaknesses of models in the different domains. Especially on the variety showcased in Figure-2, e.g. 10 examples from each category in figure-2 analyzed by domain experts.
3. Some metrics for figure captioning are missing e.g. BLEU, CIDEr, SPICE (https://github.com/tylin/coco-caption) are metrics often used in figure captioning evaluations, and it would be good to include these. ROUGE is primarily a recall based metric, while it’s relevant, in itself it’s not a sufficient signal particularly for captioning.
* Other LLM based metrics to consider using: LAVE (https://arxiv.org/pdf/2310.02567), L3Score (https://github.com/google/spiqa), PrometheusVision (https://github.com/prometheus-eval/prometheus-vision). L3Score is particularly interesting because you get the confidence from GPT-4o in addition to the generated response.
4. The results for the materials science case study is hard to interpret.
4.1 What is the baseline LLAMA-2-7B performance? (without any tuning?) Many numbers in Table 5 and Figure 6 already seem quite high so it is hard to understand what baseline you are starting from and how much room for improvement there was (and from the presented results, it doesn’t look like that much, which perhaps may not be correct)
4.2 How well do proprietary models perform on this task? Are there any proprietary models that generate reasonable responses worth evaluating?
4.3 In Table 5, the “Stable DFT” column numbers for LLAMA models (from Gruver et. al.) appear to not be consistent with numbers reported in Gruver et. al. Why is that? Minor
5. Related to point-2 Figure 4 is extremely difficult to follow. Perhaps reduce the materials science case study and include more detailed analysis of model responses and a discussion.
6. Figure 3 can be improved to clearly highlight the tasks and also the ground truth caption.
7. Other papers to consider citing in related works:
* the papers proposing different relevant metrics noted in Weakness-3.
* https://openaccess.thecvf.com/content/WACV2024/papers/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.pdf
Initial rationale for rating of 5 is primarily due to weakness 1 and 2.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,830,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', 'X'], dtype=object)}",3,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '3'], dtype=object)}",4,silver,
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.,NIPS_2017_401,NIPS_2017,"Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.
Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility:
Appears to be reproducible.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,624,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '3'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '1'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '3'], dtype=object)}",3,silver,
1)what happens if the original CAD model is already associated with spatially-varying (SV) BRDF maps?,AkL2ID5rRV,ICLR_2025,"[1] Clarification. Several details need to be clarified to better understand the model and the training strategy.
(a) What does the model estimate w.r.t. the PBR parameters? Does it only estimate albedo?
(b) With sampled metallic, roughness and lighting envmaps, do we apply the metallic and roughness globally? If yes: 1)what happens if the original CAD model is already associated with spatially-varying (SV) BRDF maps? 2) And if applied globally, does this strategy diminish the model's generation ability towards real images with complex SV materials? 3) Given a good portion of Objaverse models are assigned with PBR materials, does it benefit the training to also predict ground truth BRDF (roughness, metallic) without manually sampling and enforcing global roughness and metallic?
(c) Is the split-sum approximation only applied to synthesizing estimated image from estimated representations, or it is also used to render ground truth images? Are ground truth images rendered on-the-fly for each batch in training?
[2] Writing. Language issues are abundant and need to be fixed for a polished version. Examples:
(a) L015: for what purposes?
(b) L020: Need to introduce the full name of PBR before first use of the abbreviation.
(c) L050, L235: Need to clarify 'dependence on images rendered under fixed and simple lighting conditions' of previous methods. Mostly previous methods use PBR materials and envmap base lighting similar to this paper, so it would be important to clarify this assertion.
(d) L186: functionalities -> downstream applications of ...
(e) L283: what is 'a richer set of equations'?
[3] Additional evaluation results on images of complex lighting and materials. The paper is able to showcase the robustness towards complex lighting and materials in Fig. 9, however one scene is too few, and comparison with baselines on this setting is necessary to further justify the claim.",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1487,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '5', '3'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', 'X', 'X'], dtype=object)}",X,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,
"2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps). In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.",ICLR_2021_140,ICLR_2021,"Weakness
When discussing the difference over [Tulyakov et al. 2018], the paper states “…applies h_t as the motion code for the frame to be generated, while the content code is fixed for all frames. However, such a design requires a recurrent network to estimate the motion while preserving consistent content from the latent vector, … difficult to learn in practice”. I do not fully understand why this is the case. It would be clearer if the paper can explain why such a design causes difficulty in learning and why the proposed design could alleviate such problems.
For motion diversity, why maximizing the mutual information between the hidden vector and the noise vector can prevent mode collapse?
It seems to me that the proposed method can only handle 1) “subtle” motion, such as facial expressions and 2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps).
In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '5', '1'], dtype=object)}",1,silver,1103,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,
1. This paper lacks some very important references for domain adaptation. The authors should cite and discuss in the revised manuscript.,NIPS_2020_389,NIPS_2020,"1. This paper lacks some very important references for domain adaptation. The authors should cite and discuss in the revised manuscript. - Li et al. Bidirectional Learning for Domain Adaptation of Semantic Segmentation. In CVPR, 2019. https://arxiv.org/pdf/1904.10620.pdf - Chen et al. CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency. In CVPR, 2019. https://arxiv.org/pdf/2001.03182.pdf 2. My minor concern is that in Table 1 A and B, the proposed method seems to have inferior performance compared to MFSAN. It is a bit pity that the proposed method is not the state-of-the-art on these two datasets.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '5', '3'], dtype=object)}",2,hard,1258,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '1', '4'], dtype=object)}",1,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '5', '2'], dtype=object)}",1,hard,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '5', '2'], dtype=object)}",2,silver,
"3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.",ICLR_2023_3705,ICLR_2023,"1)The main assumption is borrowed from other works but is actually rarely used in the optimization field. Moreover, the benefits of this assumption is not well investigated. For example, a) why it is more reasonable than the previous one? B) why it can add gradient norm L_1 \nabla f(w_1) in Eqn (3) or why we do not add other term? It should be mentioned that a milder condition does not mean it is better, since it may not reflect the truth. For me, problem B) is especially important in this work, since the authors do not well explain and investigate it.
2)Results in Theorem 1 show that Adam actually does not converge, since this is a constant term O(D_0^{0.5}\delta) in Eqn. (5). This is not intuitive, the authors claim it is because the learning rate may not diminish. But many previous works, e.g. [ref 1], can prove Adam-type algorithms can converge even using a constant learning rate. Of course, they use the standard smooth condition. But (L0,L1)-smoothness condition should not cause this kind of convergence, since for nonconvex problem, in most cases, we only need the learning rate to be small but does not care whether it diminishes to zero.
[ref 1] Dongruo Zhou, Jinghui Chen, et al. On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization
3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.
4)Under the same assumption, the authors use examples to show the advantage of Adam over GD and SGD. This is good. But one issue is that is the example reasonable or does it share similar properties with practical problems, especially for networks. This is important since both SGD and ADAM are widely used in the deep learning field.
5)In the work, when comparing SGD and ADAM, the authors explain the advantage of adam comes from the cases when the local smoothness varies drastically across the domain. It is not very clear for me why Adam could better handle this case. Maybe one intuitive example could help.
6)The most important problem is that this work does not provide new insights, since it is well known that the second order moment could help the convergence of Adam. This work does not provide any insights beyond this point and also does not give any practical solution to further improve.",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,327,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '3'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', 'X'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,
"2. On line 205, it should be Fig. 1 instead of Fig. 5.1. In latex, please put '\label' after the '\caption', and the bug will be solved.",NIPS_2016_431,NIPS_2016,"1. It seems the asymptotic performance analysis (i.e., big-oh notation of the complexity) is missing. How is it improved from O(M^6)? 2. On line 205, it should be Fig. 1 instead of Fig. 5.1. In latex, please put '\label' after the '\caption', and the bug will be solved.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,657,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['X', 'X', 'X'], dtype=object)}",X,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"- L259 ""Perplexity is the probability that the model generates the current sentence"". This is not what perplexity is. Eq1 - This does not look like perplexity either, this looks like cross-entropy.",tauoKi9IWO,EMNLP_2023,"- Missing performance comparison with other approaches. Missing performance comparison on out-of-domain data. *Edit:* This concern seems to be partially addressed during the rebuttal phase and the authors provided some additional baseline results.
- L259 ""Perplexity is the probability that the model generates the current sentence"". This is not what perplexity is. Eq1 - This does not look like perplexity either, this looks like cross-entropy.
- The writing of the paper seems rushed and there are many grammatical mistakes, unfinished sentences. E.g., in the abstract L016 ""tool that can sourcing text"", L026 ""achieving x3.7 faster for recognizing text"". GPT-Zero (or GPT-zero) has footnote with link to FAQ on the first three pages. Etc.
- I don't like the including of GPT-Zero in the efficiency comparison. It is an API that you have no control over. I don't find the comparison fair.
- I don't like the mapping between the four requirements and the four RQs. Especially the first two seems to be forced. How does the fact that the true perplexity can be used for classification says anything about the specificity of your method?",10,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['1', '3', '4'], dtype=object)}",1,hard,1402,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '3', '4'], dtype=object)}",3,silver,
"1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking.",NIPS_2019_82,NIPS_2019,"1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking. 2. The language in the paper is sometimes conversational and not scientific (use of terms like massive), and there are several opinions and claims that are not substantiated (e.g. ""... facial landmarks, which are helpful for the recognition of AUs defined in small regions""), the paper could benefit from copy-editing 3. Why are two instances of the same network (resnet) are used as different views? Would using a different architecture instead be considered a more differing view? Would be great to see a justification for using two resnet networks. 4. Why is the approach limited to two views, it feels like the system should be able to generalize to more views without too much difficulty? Minor comments: - What is PCA style guarantee? - What is v in equation 2? - why are dfferent numbers of unlabeled images using in training BP4D and EmotioNet models? Trivia: massive face images -> large datasets donates -> denotes (x2) adjacent -> adjacency",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,401,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
- No empirical validation. I would have like to see some experiments where the bounds are validated.,NIPS_2018_43,NIPS_2018,"- Theoretical analyses are not particularly difficult, even if they do provide some insights. That is, the analyses are what I would expect any competent grad student to be able to come up with within the context of a homework assignment. I would consider the contributions there to be worthy of a posted note / arXiv article. - Section 4 is interesting, but does not provide any actionable advice to the practitioner, unlike Theorem 4. The conclusion I took was that the learned function f needs to achieve a compression rate of \zeta / m with a false positive rate F_p and false negative rate F_n. To know if my deep neural network (for example) can do that, I would have to actually train a fixed size network and then empirically measure its errors. But if I have to do that, the current theory on standard Bloom filters would provide me with an estimate of the equivalent Bloom filter that achieves the same error false positive as the learned Bloom filter. - To reiterate the above point, the analysis of Section 4 doesn't change how I would build, evaluate, and decide on whether to use learned Bloom filters. - The analytical approach of Section 4 gets confusing by starting with a fixed f with known \zeta, F_p, F_n, and then drawing the conclusion for an a priori fixed F_p, F_n (lines 231-233) before fixing the learned function f (lines 235-237). In practice, one typically fixes the function class (e.g. parameterized neural networks with the same architecture) *first* and measures F_p, F_n after. For such settings where \zeta and b are fixed a priori, one would be advised to minimize the learned Bloom filter's overall false positive (F_p + (1-F_p)\alpha^{b/F_n}) in the function class. An interesting analysis would then be to say whether this is feasible, and how it compares to the log loss function. Experiments can then conducted to back this up. This could constitute actionable advice to practitioners. Similarly for the sandwiched learned Bloom filter. - Claim (first para of Section 3.2) that ""this methodology requires significant additional assumptions"" seems too extreme to me. The only additional assumption is that the test set be drawn from the same distribution as the query set, which is natural for many machine learning settings where the train, validation, test sets are typically assumed to be from the same iid distribution. (If this assumption is in fact too hard to satisfy, then Theorem 4 isn't very useful too.) - Inequality on line 310 has wrong sign; compare inequality line 227 --- base \alpha < 1. - No empirical validation. I would have like to see some experiments where the bounds are validated.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,1116,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '4', '2'], dtype=object)}",3,hard,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['X', '2', '2'], dtype=object)}",2,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['3', '3', '3'], dtype=object)}",3,gold,
"5.In experimental section, authors only compared with two baselines, there’re several works also focus on the same questions, for example [1,2,3], so it’s suggested to add more experimental to show the effectiveness of proposed method.",OBUQNASaWw,ICLR_2025,"1.It’s suggested that authors give a comprehensive survey on adaptive sparse training method. Although authors claim “Previous works have only managed to solve one, or perhaps two of these challenges”, can authors give a comprehensive comparison of existing methods?
2.Considering different clients train different submodels, the server also maintains a full model. So can the sparsity of clients be different to apply for heterogeneous hardware?
3.Can authors further explain why clients should achieves consensus on the clients’ sparse model masks when server always maintain a full model.
4.What’s the definition of the model plasticity?
5.In experimental section, authors only compared with two baselines, there’re several works also focus on the same questions, for example [1,2,3], so it’s suggested to add more experimental to show the effectiveness of proposed method.
6.Considering the model architecture, authors only show the effectiveness on convolutional network, what’s the performance on other architecture, for example Transformer?
[1]Stripelis, Dimitris, et al. ""Federated progressive sparsification (purge, merge, tune)+."" arXiv preprint arXiv:2204.12430 (2022).
[2]Wang, Yangyang, et al. ""Theoretical convergence guaranteed resource-adaptive federated learning with mixed heterogeneity."" Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023.
[3]Zhou, Hanhan, et al. ""Every parameter matters: Ensuring the convergence of federated learning with dynamic heterogeneous models reduction."" Advances in Neural Information Processing Systems 36 (2024).",4,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '4', '5'], dtype=object)}",4,silver,267,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['4', '4', '5'], dtype=object)}",4,silver,
2. It would be better to compare with other self-supervised learning methods that are not based on contrastive learning.,ICLR_2023_1645,ICLR_2023,1. Can this method be used on both SEEG and EEG simultaneously? 2. It would be better to compare with other self-supervised learning methods that are not based on contrastive learning.,5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '4', '3'], dtype=object)}",3,silver,336,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '1', '2'], dtype=object)}",4,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '1', '1'], dtype=object)}",1,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '2', '2'], dtype=object)}",2,silver,
"* It would help if the form of p was described somewhere near line 135. As per my above comment, I assume it is a Gaussian distribution, but it's not explicitly stated.",NIPS_2019_663,NIPS_2019,"of their work?""] The submission is overall reasonably sound, although I have some comments and questions: * Regarding the model itself, I am confused by the GRU-Bayes component. I must be missing something, but why is it not possible to ingest observed data using the GRU itself, as in equation 2? This confusion would perhaps be clarified by an explanation in line 89 of why continuous observations are required. As it is written, I am not sure why it you couldn't just forecast (by solving the ODE defined by equation 3) the hidden state until the next measurement arrives, at which point g(t) and z(t) can be updated to define a new evolution equation for the hidden state. I am guessing the issue here is that this update only changes the derivative of the hidden state and not its value itself, but since the absolute value of the hidden state is not necessarily meaningful, the problem with this approach isn't very clear to me. I imagine the authors have considered such a model, so I would like to understand why it wouldn't be feasible here. * In lines 143-156, it is mentioned that the KL term of the loss can be computed empirically for binomial and Gaussian distributions. I understand that in the case of an Ornstein-Uhlenbeck SDE, the distribution of the observations are known to be (conditionally) Gaussian, but in the case of arbitrary data (e.g. health data), as far as I'm aware, few assumptions can be made of the underlying process. In this case, how is the KL term managed? Is a Gaussian distribution assumption made? Line 291 indicates this is the case, but it should be made clear that this is an assumption imposed on the data. For example, in the case of lab test results as in MIMIC, these values are rarely Gaussian-distributed and may not have Gaussian-distributed observation noise. On a similar note, it's mentioned in line 154 that many real-world cases have very little observation noise relative to the predicted distribution - I assume this is because the predicted distribution has high variance, but this statement could be better qualified (e.g. which real-world cases?). * It is mentioned several times (lines 203, 215) that the GRU (and by extension GRU-ODE-Bayes) excels at long-term forecasting problems, however in both experiments (sections 5.2 and 5.3) only near-term forecasting is explored - in both cases only the next 3 observations are predicted. To support this claim, longer prediction horizons should be considered. * I find it interesting that the experiments on MIMIC do not use any regularly-measured vital signs. I assume this was done to increase the ""sporadicity"" of the data, but it makes the application setting very unrealistic. It would be very unusual for values such as heart rate, respiratory rate, blood pressure and temperature not to be available in a forecasting problem in the ICU. I also think it's a missed opportunity to potentially highlight the ability of the proposed model to use the relationship between the time series to refine the hidden state. I would like to know why these variables were left out, and ideally how the model would perform in their presence. * I think the experiment in Section 5.5 is quite interesting, but I think a more direct test of the ""continuity prior"" would be to explicitly test how the model performs (in the low v. high data cases) on data which is explicitly continuous and *not* continuous (or at least, not 2-Lipschitz). The hypothesis that this continuity prior is useful *because* it encodes prior information about the data would be more directly tested by such a setup. At present, we can see that the model outperforms the discretised version in the low data regime, but I fear this discretisation process may introduce other factors which could explain this difference. It is slightly hard to evaluate because I'm not entirely sure what the discretised version consists of , however - this should be explained (perhaps in the appendix). Furthermore, at present there is no particular reason to believe that the data in MIMIC *is* Lipschitz-2 - indeed, in the case of inputs and outputs (Table 4, Appendix), many of these values can be quite non-smooth (e.g. a patient receiving aspirin). * It is mentioned (lines 240-242, section H.1.3) that this approach can handle ""non-aligned"" time series well. As mentioned, this is quite a challenging problem in the healthcare setting, so I read this with some interest. Do these statements imply that this ability is unique to GRU-ODE-Bayes, and is there a way to experimentally test this claim? My intuition is that any latent-variable model could in theory capture the unobserved ""stage"" of a patient's disease process, but if GRU-ODE-Bayes has some unique advantage in this setting it would be a valuable contribution. At present it is not clearly demonstrated - the superior performance shown in Table 1 could arise from any number of differences between this model and the baselines. 2.c Clarity: [""Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.)""] While I quite like the layout of the paper (specifically placing related work after a description of the methodology, which is somewhat unusual but makes sense here) and think it is overall well written, I have some minor comments: * Section 4 is placed quite far away from the Figure it refers to (Figure 1). I realise this is because Figure 1 is mentioned in the introduction of the paper, but it makes section 4 somewhat hard to follow. A possible solution would be to place section 4 before the related research, since the only related work it draws on is the NeuralODE-VAE, which is already mentioned in the Introduction. * I appreciate the clear description of baseline methods in Section 5.1. * The comprehensive Appendix is appreciated to provide additional detail about parts of the paper. I did not carefully read additional experiments described in the Appendix (e.g. the Brusselator) out of time consideration. * How are negative log-likelihoods computed for non-probabilistic models in this paper? * Typo on line 426 (""me"" instead of ""we""). * It would help if the form of p was described somewhere near line 135. As per my above comment, I assume it is a Gaussian distribution, but it's not explicitly stated. 2.d Significance: [""Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?""] This paper describes quite an interesting approach to the modelling of sporadically-measured time series. I think this will be of interest to the community, and appears to advance state of the art even if it is not explicitly clear where these gains come from.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,909,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', 'X', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"* L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else.",NIPS_2016_386,NIPS_2016,", however. For of all, there is a lot of sloppy writing, typos and undefined notation. See the long list of minor comments below. A larger concern is that some parts of the proof I could not understand, despite trying quite hard. The authors should focus their response to this review on these technical concerns, which I mark with ** in the minor comments below. Hopefully I am missing something silly. One also has to wonder about the practicality of such algorithms. The main algorithm relies on an estimate of the payoff for the optimal policy, which can be learnt with sufficient precision in a ""short"" initialisation period. Some synthetic experiments might shed some light on how long the horizon needs to be before any real learning occurs. A final note. The paper is over length. Up to the two pages of references it is 10 pages, but only 9 are allowed. The appendix should have been submitted as supplementary material and the reference list cut down. Despite the weaknesses I am quite positive about this paper, although it could certainly use quite a lot of polishing. I will raise my score once the ** points are addressed in the rebuttal. Minor comments: * L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0? * L177: ""(OCO )"" -> ""(OCO)"" and similar things elsewhere * L176: You might want to mention that the learner observes the whole concave function (full information setting) * L223: I would prefer to see a constant here. What does the O(.) really mean here? * L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards. * L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes? * The algorithm only stops /after/ it has exhausted its budget. Don't you need to stop just before? (the regret is only trivially affected, so this isn't too important). * L213: \tilde \mu is undefined. I guess you mean \tilde \mu_t, but that is also not defined except in Corollary 1, where it just given as some point in the confidence ellipsoid in round t. The result holds for all points in the ellipsoid uniformly with time, so maybe just write that, or at least clarify somehow. ** L435: I do not see how this follows from Corollary 2 (I guess you meant part 1, please say so). So first of all mu_t(a_t) is not defined. Did you mean tilde mu_t(a_t)? But still I don't understand. pi^*(X_t) is (possibly random) optimal static strategy while \tilde \mu_t(a_t) is the optimistic mu for action a_t, which may not be optimistic for pi^*(X_t)? I have similar concerns about the claim on the use of budget as well. * L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else. * L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee? * L200: ""for every arm a"" implies there is a single optimistic parameter, but of course it depends on a ** L303: Why not choose T_0 = m Sqrt(T)? Then the condition becomes B > Sqrt(m) T^(3/4), which improves slightly on what you give. * It would be nice to have more interpretation of theta (I hope I got it right), since this is the most novel component of the proof/algorithm.",7,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '4'], dtype=object)}",4,silver,896,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', 'X', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"1) I have understood that the integral in Equation (1) corresponds to bag observation model in [Law et al., NeurIPS'18] or spatial aggregation process in [4]. The formulation introduced by the authors assume that the observations are obtained by averaging over the corresponding support $v$. However, the data might be aggregated by another procedure, e.g., simple summation or population weighted average; actually the disease incident data are often available in count, or rate per the number of residents.",NIPS_2019_1350,NIPS_2019,". Some important related works are not discussed. Multi-task (i.e., multivariate) GPs have been widely studied in machine learning community. Although most of them assume that data values are associated with points, it would be better to mention several related multi-task GPs (e.g., [1],[2],[3]). Especially, [1] designed the dependent GP by a linear mixing of latent GPs, which is similar to this submission. Also, there is an important related work missing here: [4]. I think this paper essentially addressed a related task: Predicting the fine-grained data by using auxiliary data sets with various granularities. I would like the authors to clarify the differences and advantages of this submission. [Quality] Strengths. This paper is technically sound except for some concerns. The authors evaluate the proposed model in the simple experimental setting using synthetic and real data sets. Weaknesses. My concerns about the proposed model are as follows: 1) I have understood that the integral in Equation (1) corresponds to bag observation model in [Law et al., NeurIPS'18] or spatial aggregation process in [4]. The formulation introduced by the authors assume that the observations are obtained by averaging over the corresponding support $v$. However, the data might be aggregated by another procedure, e.g., simple summation or population weighted average; actually the disease incident data are often available in count, or rate per the number of residents. 2) In order to handle various data types (e.g., count and rate), shouldn't the corresponding aggregation processes be performed at the likelihood level? 3) I think it would be more efficient to estimate ${a_{d,q}}$ instead of $B_q$ since $b^q_{d,d'} = a_{d,q}a_{d',q}$. The major weakness of this submission is in the experiments. First, the proposed model should be compared with any typical baseline, such as regression-based model with aggregation process (e.g., Law et al., NeurIPS'18, [4]) and multi-task GP with point-referenced data (e.g., [1]). I believe the previous multi-task GP can be applied via the simplification; that is, each data value at the support $v$ is assumed to be associated with the representative point (e.g., centroid) of its support (as in the previous work [4]). Second, the extensive experiments are helpful to verify the effectiveness of the proposed model. In all the experiments, the authors consider two tasks. I would like to see the experimental results considering more tasks; then it is a good idea to discuss how to determine the number of latent GPs $Q$. Short question: I was wondering if you could give me the detail of *resolution 5 \times 5* in the experimental setting of fertility rates. [Clarity] This paper is easy to understand. Some typos: 1) In line 235, *low-cost* should be *low-accuracy*? 2) In line 239, *GP process* should be *GP*. [Significance] Aggregated data with different supports are commonplace in a wide variety of applications, so I think this is an important problem to tackle. However, the major weakness of the submission in my view is that the evaluation of the proposed model is not enough, so the effectiveness/usefulness of the model is unclear from the experimental results. I think it would be great to compare the proposed model with baseline methods. [1] Y. W. Teh et al., Semiparametric Latent Factor Models, AISTATS, 333-340, 2005. [2] P. Boyle et al., Dependent Gaussian Processes, NeurIPS, 217-224, 2005. [3] E. Bonilla et al., Multi-task Gaussian Process Prediction, NeurIPS, 153-160, 2008. [4] Y. Tanaka et al., Refining Coarse-grained Spatial Data Using Auxiliary Spatial Data Sets with Various Granularities, AAAI, 2019. https://arxiv.org/abs/1809.07952 ------------------------------ After author feedback: I appreciate the responses to my questions. The new experimental results in the rebuttal is a welcome addition. In light of this, I upgraded my score. The proposal is a combination of the coregionalization and the concept of aggregation process used in block-kriging; this is a simple but effective way. I also agree that a sensor experiment is one of the applications with the proposed model. But I'm still of the opinion that there is not enough experiments and/or discussions to support the authors' claims. The authors state that the model is a general framework and has many applications related to geostatistics (lines 14-23); the support $v$ corresponds to the 2-dimensional region, e.g., borough (line 92). As described in Related work (lines 222-229), the proposed model strongly relates to spatial downscaling and disaggregation in geostatistics. If anything, I think this application that contains spatial aggregation is a more critical one for the proposed model. In the spatial data setting, a wide variety of data sets is available at various spatial granularities (for instance, New York City publish open data in [https://opendata.cityofnewyork.us]). Naturally, one would like to handle these data sets simultaneously (as in Law et al., NeurIPS'18, [4]); namely the setting with a large number of tasks. In that case, I believe the authors should discuss several issues; for example, the sensitivity of the number of latent GPs $Q$, the approximate accuracy of integral over regions, etc. I think it would be better to clarify the scope of this study and discuss the above issues.",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '2', '3'], dtype=object)}",1,hard,1071,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '3', '4'], dtype=object)}",2,hard,
"1. The paper split the papers according to their publication years on the ACL anthology. However, many papers are posted on arxiv much earlier than ACL anthology. For example, the BERT paper is available on arxiv from Oct.",qhwYFIrSm7,EMNLP_2023,"1. The paper split the papers according to their publication years on the ACL anthology. However, many papers are posted on arxiv much earlier than ACL anthology. For example, the BERT paper is available on arxiv from Oct. 2018 (when its influence started), but is on ACL Anthology from 2019. This may more or less influence the soundness of the causal analysis.
2. The paper claims that the framework aids the literature survey, which is true. However, the listed overviews are mostly ""Global Research Trends"" that are well-known by the community. For example, it is known that BLEU and Transformers fuel machine translation. The paper didn't illustrate how to use the framework to identify ""Local Research Trends"" which are more useful for daily research: for example, what is the impact of instruction-following LLMs and in-context learning on NLP tasks? How do multi-task learning and reinforcement learning transfer to instruction learning?",8,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['1', '4', '1'], dtype=object)}",1,silver,1109,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['5', '5', '1'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['X', 'X', '4'], dtype=object)}",X,silver,"{'annotators': array(['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'],
      dtype=object), 'labels': array(['2', '4', '3'], dtype=object)}",2,hard,
"4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail.",ARR_2022_7_review,ARR_2022,"1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model.
2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail.
5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever.
Reference: * Ni et al. 2021. https://arxiv.org/abs/2112.07899
the proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring.
Line 300-304: q and d are confusing",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1236,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['3', '5', '3'], dtype=object)}",3,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '4'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['2', '5', '4'], dtype=object)}",2,hard,
2. The captions of Fig. 1 and Fig. 2 have large overlaps with your content. You can consider shrinking the captions to leave more space to your methods or related work.,NIPS_2022_431,NIPS_2022,"Lack information of comparison with related work.
Experiments on SQuAD have no results of the competitor A3.
Improvement is limited.
UPDATE: The authors' responses address my main concerns, which should be included in the revised version.
Minor suggestions: 1. The writing can be further improved. There are typos in: a) Line 121, “significant \delta” should be “significance \delta”. b) Line 162, “is a symmetric matrix contains input” should be “is a symmetric matrix containing input”. c) Line 176-177, “for encourage the attended items becomes” should be “for encouraging the attended items to become”. 2. The captions of Fig. 1 and Fig. 2 have large overlaps with your content. You can consider shrinking the captions to leave more space to your methods or related work.",9,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,1197,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['X', '5', '5'], dtype=object)}",5,silver,"{'annotators': array(['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,
"- The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data. If otherwise, why [10] cannot use these side information?",NIPS_2016_499,NIPS_2016,"- The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data. If otherwise, why [10] cannot use these side information? - The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph? - Second rule in Lemma 2, i.e., Eq (7) and the definition of minimal conditional dependence seem to be conflicting. Taking Zâ in this definition to be the empty set, we should have that x and y are independent given W, but Eq. (7) says otherwise.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '2', '3'], dtype=object)}",3,silver,549,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '2'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '2'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '2'], dtype=object)}",3,silver,
"1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying Q ^ t and S t , as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of α t and also on the average lengths of the predictive intervals when we let Q ^ t and S t vary with time?",NIPS_2021_40,NIPS_2021,"/Questions:
I only have minor suggestions:
1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying Q ^ t and S t
, as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of α t
and also on the average lengths of the predictive intervals when we let Q ^ t and S t
vary with time?
2.) I found the definition of the quantile a little confusing, an extra pair of brackets around the term ( 1 | D | ∑ ( X r , Y r ) ∈ D 1 S ( X r , Y r ) ≤ s )
might help, or maybe defining the bracketed term separately if space allows.
3.) I think there are typos in Lines 93, 136, 181 (and maybe in the Appendix too): should it be Q ^ t ( 1 − α t ) instead? ##################################################################### Overall:
This is a very interesting extension to conformal prediction that no longer relies on exchangeability but is still general, which will hopefully lead to future work that guarantees coverage under weak assumptions. I believe the generality also makes this method useful in practice.
The authors have described the limitations of their theory, e.g. having a fixed Q ^
with time.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '3', '5'], dtype=object)}",5,silver,648,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '5', '5'], dtype=object)}",5,gold,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', 'X'], dtype=object)}",3,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '4', '5'], dtype=object)}",4,silver,
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures?,ACL_2017_108_review,ACL_2017,"Clarification is needed in several places.
1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.
2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?
3. section 5.1 does not seem to provide useful info regarding why the new model is superior.
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures? - General Discussion: The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better.",2,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,71,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '2', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '2', '5'], dtype=object)}",5,silver,"{'annotators': array(['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'],
      dtype=object), 'labels': array(['5', '2', '5'], dtype=object)}",5,silver,
"- line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help?",NIPS_2018_553,NIPS_2018,"- This paper misses a few details in model design and experiments: A major issue is the ""GTA"" / ""DET"" feature representation in Table 1. As stated in section 4.1, image regions are extracted from ground-truth / detection methods. But what is the feature extractor used on top of those image regions? Comparing resnet / densenet extracted features with vgg / googlenet feature is not fair. - The presentation of this paper can be further improved. E.g. paragraph 2 in intro section is a bit verbose. Also breaking down overly-long sentences into shorter but concise ones will improve fluency. Some additional comments: - Figure 3: class semantic feature should be labeled as ""s"" instead of ""c""? - equation 1: how v_G is fused from V_I? please specify. - equation 5: s is coming from textual representations (attribute / word to vec / PCA'ed TFIDF). It might have positive / negative values? However the first term h(W_{G,S}, v_G) is post ReLU and can only be non-negative? - line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help? - line 170: class semantic information is [not directly] embedded into the network? - Equation 11: v_s and u_G are both outputs from trained-network, and they are not normalized? So minimize L-2 loss could be simply reducing the magnitude of both vectors? - Line 201: the dimensionality of each region is 512: using which feature extractor? - Section 4.2.2: comparing number of attention layers is a good experiment. Another baseline could be not using Loss_G? So attention is only guided by global feature vector. - Table 4: what are the visual / textual representations used in each method? otherwise it is unclear whether the end-to-end performance gain is due to proposed attention model.",6,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '3', '5'], dtype=object)}",3,silver,652,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['5', '4', '5'], dtype=object)}",5,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['X', '3', 'X'], dtype=object)}",X,silver,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '3', '4'], dtype=object)}",4,silver,
"* The authors combine two existing techniques to get the framework without innovation. The adversarial attack or correction method and the domain adaptation method used by the authors are proposed by prior work. And the adopted domain adaptation method here is a very old and simple method which is proposed eight years ago. Considering there were so many effective domain adaptation methods proposed in the recent few years, why don't you use other domain adaptation methods to further improve the performance?",uSiyu6CLPh,ICLR_2025,"* I suggest that the authors show a more intuitive figure to visualize the framework that includes the images and labels in the original dataset and also the corrected images. This will help the readers to gain more intuition for your method.
* The authors combine two existing techniques to get the framework without innovation. The adversarial attack or correction method and the domain adaptation method used by the authors are proposed by prior work. And the adopted domain adaptation method here is a very old and simple method which is proposed eight years ago. Considering there were so many effective domain adaptation methods proposed in the recent few years, why don't you use other domain adaptation methods to further improve the performance?
* In Section 3.3, the authors align the features of the weak classifier on the original dataset and the synthetic dataset. Considering that the difference between the original and the synthetic datasets is the corrected part, can we omit the correctly classified samples and only minimize the covariance difference for the adversarially corrected sample and the misclassified sample?
* How do you choose the hyper-parameters such as $\lambda,\epsilon$? Does your method work robustly for other choices of hyper-parameters? If not, how do you choose them?",5,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', '3'], dtype=object)}",3,silver,405,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['4', '5', '2'], dtype=object)}",4,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', 'X'], dtype=object)}",3,hard,"{'annotators': array(['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'],
      dtype=object), 'labels': array(['3', '5', '2'], dtype=object)}",3,hard,

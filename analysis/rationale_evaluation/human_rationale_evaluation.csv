paper_id,venue,focused_review,review_point,id,chatgpt_actionability_score,chatgpt_actionability_rationale,chatgpt_helpfulness_score,chatgpt_helpfulness_rationale,chatgpt_grounding_specificity_score,chatgpt_grounding_specificity_rationale,chatgpt_verifiability_score,chatgpt_verifiability_rationale,__index_level_0__,actionability_rationale_correctness,actionability_rationale_relevance,grounding_specificity_rationale_correctness,grounding_specificity_rationale_relevance,verifiability_rationale_correctness,verifiability_rationale_relevance,helpfulness_rationale_correctness,helpfulness_rationale_relevance
NIPS_2018_202,NIPS_2018,"- My major concern with the paper is that the theoretical results seem to be stated in vague terms and I don't fully understand them. In Theorem 1, what does it mean to say that Omega^k ""contains"" disjoint matrices Omega_tau? Does it mean that Omega^k is a stack of matrices Omega_tau column-wise? Also, what does it mean that it is ""possible"" to perfectly recover all columns of X^k? Does it mean that the subspaces and the missing entries can be uniquely determined from the observed data? In addition, how is this result compared with the deterministic result for the problem of subspace clustering with missing entries in [4]? Overall this is a nice paper that brings up a new problem formulation into attention. The study of this problem is still preliminary, though, as one can clearly see that there is no very successful application of the proposed method yet. The experiment on face clustering has a very unrealistic test scenario, and the experiment for background segmentation does not generate results as good as classical robust PCA. Nonetheless, addressing these challenges could be the topic of future study. My reason for not having a higher rating is that I cannot fully appreciate the theoretical studies as mentioned above. Additional comments: - It appears that the ""cluster"" step in the proposed algorithm is very complicated. I'm wondering if this step can be decomposed into the following substeps to make it easier to explain: the estimation of Omega_k is composed of two separate tasks of 1) clustering the entries in each column to different data points, and 2) the assigning data points extracted from all columns to multiple subspaces. In fact, once one have solved task 1) above then the problem reduces to subspace clustering with missing data, which could be solvedby alternating between matrix completion and subspace clustering. - Since the overall problem is nonconvex, initialization is expected to be very important for the algorithm to achieve good performance. Can the authors comment on how their algorithm is initialized on real data experiments? Response to rebuttals: The updated statement of Theorem 1 in the rebuttal is now much better in terms of clarity and it appears to be very necessary to incorporate it into the final version. Also, given that the conditions in Theorem 1 and 2 are very much similar to those in [4] for a related problem, a more detailed discussion of their connections will help understand the merits of these results. I maintain my overall rating as above and recommend a weak acceptance for this work.","1) above then the problem reduces to subspace clustering with missing data, which could be solvedby alternating between matrix completion and subspace clustering.",6284,2,"The review point makes an observation about the problem reducing to subspace clustering with missing data and suggests a potential solution involving alternating between matrix completion and subspace clustering. However, it does not provide explicit guidance or instructions on how the authors should implement this solution or whether it is a recommended course of action. The comment lacks concrete details and leaves the authors uncertain about how to proceed. Without specific steps or a clear directive, the comment is vague and implicit, making it barely actionable.",2,"The review comment suggests a potential approach to solving the problem by reducing it to subspace clustering with missing data and proposes alternating between matrix completion and subspace clustering as a solution. However, the comment lacks clarity and detail, as it does not explain how this approach should be implemented or why it would be beneficial. Without specific guidance or actionable steps, the authors may find it challenging to apply this suggestion to improve their draft. Therefore, the feedback is rated as barely helpful, as it identifies a possible direction but lacks the depth and specificity needed to be more useful.",1,"The comment suggests a potential solution to a problem by reducing it to subspace clustering with missing data and proposes an approach involving alternating between matrix completion and subspace clustering. However, it does not specify which part of the paper this suggestion pertains to, making it difficult for the authors to identify the exact section or context where this solution should be applied. Additionally, the comment lacks specificity regarding the details of the proposed approach or how it should be implemented. Therefore, the comment is not grounded and lacks specificity, aligning with a score of 1.",1,"The review point makes a claim about reducing the problem to subspace clustering with missing data and suggests a solution involving alternating between matrix completion and subspace clustering. However, it lacks any supporting evidence, reasoning, or references to justify this approach. Without further explanation or examples, the claim is difficult for the authors to verify or understand how to implement the suggestion effectively. Therefore, the comment is considered unverifiable.",4501,,,,,,,,
ICLR_2023_2171,ICLR_2023,"1. All the tasks in this paper seem based on grasping. The difficulties of these tasks are too homogeneous. Perhaps refer to these projects to design more tasks, such as [1] and [2]. 2. The State Space contains the pose and velocity of the object without any vision-based input, thus resulting in missing geometric features of the object. This may limit the out-of-domain generalization ability. Recently, lots of dexterous hand works contain visual input with a strong generalization ability, such as [3] and [4]. 3. There seem to be no ablation experiments on synergies. 4. The experiment seems to be done with only one seed, and the authors should consider doing several more seeds in order to better demonstrate the robustness of the method. 5. The previous work [5], which also presents a dexterous manipulation benchmark with musculoskeletal hands, detracts from the novelty of this work.[1] https://bi-dexhands.ai/ [2] https://openreview.net/pdf?id=k2Ml8FGtJZp [3] https://openreview.net/pdf?id=k2Ml8FGtJZp [4] https://openreview.net/pdf?id=tJE1Yyi8fUX [5] https://arxiv.org/pdf/2205.13600.pdf","1. All the tasks in this paper seem based on grasping. The difficulties of these tasks are too homogeneous. Perhaps refer to these projects to design more tasks, such as [1] and [2].",3504,3,"The comment suggests that the tasks in the paper are too homogeneous and recommends referring to specific projects ([1] and [2]) to design more diverse tasks. While the action is implicit, it provides a concrete suggestion by pointing to specific projects that could inspire the design of new tasks. However, it does not detail how the authors should incorporate these references into their work or what specific aspects of the projects should be considered. The authors have a general direction but lack detailed guidance on implementation. Therefore, the comment is somewhat actionable.",4,"The review comment identifies a potential issue with the homogeneity of the tasks presented in the paper, noting that they all seem to be based on grasping. It suggests that the tasks lack diversity in difficulty, which could limit the scope and applicability of the research. The reviewer provides a constructive suggestion by recommending the authors refer to specific projects ([1] and [2]) to design more varied tasks. This feedback is actionable and offers a clear direction for the authors to enhance the diversity and challenge of their tasks, making the comment mostly helpful. However, it could be more helpful if it included more specific guidance on how to implement the suggested changes or what aspects of the referenced projects are particularly relevant. Overall, the comment aligns with a score of 4.",3,"The comment suggests that the tasks in the paper are too homogeneous and recommends referring to specific projects ([1] and [2]) to design more diverse tasks. However, it does not specify which part of the paper discusses these tasks, making it weakly grounded. The comment is specific in identifying the issue of homogeneity and provides a suggestion for improvement by referencing other projects. Therefore, the comment is weakly grounded and specific, aligning with label 3.",3,"The review point claims that ""all the tasks in this paper seem based on grasping"" and suggests that the tasks are ""too homogeneous."" It recommends referring to other projects to design more diverse tasks, citing [1] and [2] as potential resources. While the comment provides a general critique and suggests looking at specific projects, it lacks detailed reasoning or examples to substantiate why the current tasks are considered homogeneous or how the suggested projects could help. The absence of specific examples or detailed justification makes the claim somewhat verifiable, as the authors would need more information to fully understand and address the critique.",4658,,,,,,,,
ICLR_2023_1400,ICLR_2023,"- While the paper shows improvements on CIFAR derivatives, it lacks analysis or results on other datasets (e.g., ImageNet derivatives). Verifying the effectiveness of the framework on ImageNet-1k or even ImageNet-100 is important. These results ideally can be presented in the main paper.- The authors should add some details on how to solve the optimization in the main paper. It's an important piece of information currently lacking in the paper.- Some baselines such as [1] are not considered and should be added.I feel that influence function can be replaced by other influence estimation methods such as datamodels[2] or tracin[3]. It will be beneficial to understand if the updated framework results in better pruning than the baselines. I am assuming it would result in better pruning results, however it would be beneficial to understand which influence based methods are particularly suitable for pruning.[1]. https://arxiv.org/pdf/2107.07075[2]. https://arxiv.org/abs/2202.00622[3]. https://arxiv.org/abs/2002.08484",- The authors should add some details on how to solve the optimization in the main paper. It's an important piece of information currently lacking in the paper.,3701,5,"The review comment explicitly instructs the authors to add details on how to solve the optimization in the main paper, emphasizing that this is an important piece of information currently missing. This direct request provides a clear and specific action for the authors to take, leaving no ambiguity about what needs to be done to improve the draft. The comment is explicit and concrete, making it highly actionable.",4,"The comment identifies a specific area where the paper is lacking—details on solving the optimization problem. By pointing out this omission, the reviewer provides the authors with clear and actionable feedback on where to improve their manuscript. However, while it highlights the problem, it does not offer detailed suggestions or examples on how to address this issue, which could enhance its helpfulness. Overall, the comment is mostly helpful as it directs the authors to a critical area needing clarification, but it could be more comprehensive with additional guidance.",3,"The comment suggests that the authors should add details on solving the optimization problem, indicating that this is an important piece of information currently missing from the paper. However, it does not specify which section or part of the paper this information should be added to, making it weakly grounded. The comment is specific in identifying what is missing—details on solving the optimization—but lacks guidance on where this should be addressed. Therefore, the comment is weakly grounded and specific, aligning with category 3.",1,"The review point suggests that the paper lacks important details on solving the optimization, which is a critical aspect of the work. However, the comment does not provide any specific reasoning, examples, or references to support why this information is crucial or how it might be addressed. Without additional context or justification, the authors may find it challenging to understand the importance of this suggestion or how to implement it. Therefore, the comment is considered unverifiable due to the lack of supporting evidence or detailed explanation, warranting a score of 1: Unverifiable.",2777,,,,,,,,
ICLR_2022_733,ICLR_2022,Weakness 1 The number of experts is limited to 2. Indeed it would maintain the computation load. But the power of experts may not be exhibited sufficiently. It would be great to investigate the performance by changing the number of experts. 2 I am wondering if this mechanism can be used in the vision transformers?,2 I am wondering if this mechanism can be used in the vision transformers?,2700,2,"The review comment poses a question about whether the mechanism discussed can be applied to vision transformers. While it suggests a potential area for exploration, it does not provide explicit guidance or instructions on how to investigate this possibility. The action is implicit, as the authors can infer that they might consider exploring the applicability of their mechanism to vision transformers. However, the comment lacks concrete details or steps on how to carry out this exploration, making it vague. Therefore, this comment is barely actionable.",2,"The comment raises a question about the applicability of a mechanism to vision transformers, which could prompt the authors to consider and discuss this aspect in their paper. However, the comment lacks depth and does not provide any specific suggestions or guidance on how to explore this possibility. It does not identify any weaknesses or areas for improvement in the current draft, nor does it offer actionable feedback. As a result, the comment is barely helpful, as it hints at a potential area of interest but provides insufficient guidance for the authors to act upon.",1,"The comment poses a question about the applicability of a mechanism to vision transformers. However, it does not specify which part of the paper discusses this mechanism, leaving the authors unable to confidently determine the exact section being addressed. Additionally, the comment lacks specificity regarding what aspects of the mechanism's applicability should be considered or explored. As a result, the comment is not grounded and not specific, aligning with a score of 1.",X,"The review point is a question seeking clarification about the applicability of a mechanism to vision transformers. It does not express an opinion, judgment, or suggestion that requires validation or support. Therefore, it is classified as ""No.""",5967,,,,,,,,
ICLR_2021_2180,ICLR_2021,"Weakness: 1. The authors should provide justification of choosing to use AAE in the work. In particular, why is AAE an attractive approach for MTPP? 2. The authors mention: ""The typical MTTP baselines like Reinforcement Learning, RNNs, Wasserstein GANs require a significant amount of data to train a network. Therefore, such techniques are not applicable to our proposed approach. As the baseline, we compare the proposed data generation technique with a Markov chain approach which was applied to the same dataset in (Klausen et al., 2018b)."" The authors could try to apply Reinforcement Learning, RNNs, Wasserstein GANs with data filled by simple methods, to empirically validate that these methods are not suitable for incomplete, small data. 3. In feature mapping method, the authors could provide justification for converting marked point times t_ij to days a_i. Besides, the method to convert t_ij to a_i and examples are not provided. If this is a common preprocessing method in MTPP, the authors should cite the relevant work. Overall, Step 1 of Algorithm 1 is not clear. 4. At the data approximation technique (step 3 of the Algorithm 1), the author randomly chooses a probability for the appearance of an unobservable data point but there is a lack of explanations. Can the authors explain the reason of selecting from [0, Pcj(0)]? 5. The authors should provide more details of incomplete and small dataset, and compare with their method when training with full data. This is to understand that if generated data is still good when training with small dataset=============== after rebuttal: I thank authors for the responses. After reviewing the authors' response and other reviewers' comments, I keep my original rating.","2. The authors mention: ""The typical MTTP baselines like Reinforcement Learning, RNNs, Wasserstein GANs require a significant amount of data to train a network. Therefore, such techniques are not applicable to our proposed approach. As the baseline, we compare the proposed data generation technique with a Markov chain approach which was applied to the same dataset in (Klausen et al., 2018b)."" The authors could try to apply Reinforcement Learning, RNNs, Wasserstein GANs with data filled by simple methods, to empirically validate that these methods are not suitable for incomplete, small data.",2190,5,"The review point provides a clear and explicit suggestion for the authors to empirically validate their claim that typical MTTP baselines like Reinforcement Learning, RNNs, and Wasserstein GANs are not suitable for their proposed approach due to data limitations. It suggests that the authors attempt to apply these methods using data filled by simple methods to demonstrate their unsuitability for incomplete, small data. This feedback is actionable as it provides a specific action for the authors to take, along with a rationale for why this action would be beneficial. The authors know exactly what experiment to conduct to strengthen their argument, making this comment highly actionable.",5,"The review comment provides a constructive suggestion for the authors to empirically validate their claim that certain techniques like Reinforcement Learning, RNNs, and Wasserstein GANs are not suitable for their approach due to data limitations. By recommending the application of these methods with data filled by simple techniques, the comment offers a clear and actionable way for the authors to strengthen their argument and provide empirical evidence for their claims. This feedback is highly helpful as it guides the authors towards a potential improvement in their methodology and the robustness of their conclusions.",5,"The comment is fully grounded as it directly quotes a specific statement from the paper, allowing the authors to accurately identify the part being addressed. It is also specific because it suggests a clear course of action: applying Reinforcement Learning, RNNs, and Wasserstein GANs with data filled by simple methods to empirically validate the claim that these methods are not suitable for incomplete, small data. This provides the authors with a concrete suggestion for improvement. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.",3,"The review point suggests that the authors should attempt to apply Reinforcement Learning, RNNs, and Wasserstein GANs with data filled by simple methods to empirically validate their unsuitability for incomplete, small data. The claim is somewhat verifiable as it provides a logical suggestion for empirical validation, but it lacks specific examples or references to support why these methods might be unsuitable. The comment could be strengthened by providing more detailed reasoning or evidence to substantiate the claim, making it easier for the authors to understand and act upon the suggestion. Therefore, the comment aligns with a label of 3: Somewhat Verifiable.",7016,,,,,,,,
ICLR_2022_1250,ICLR_2022,"1. The authors claimed that “we develop a unified algorithm based on maximization of a submodular function, which can combine different selection objectives” in Introduction. However, I can’t see the necessary or benefit of using “submodular function and matroid intersection” in this paper, except that some prior works about submodular show the guarantee of approximation to the original dataset in the some feature/output space. The question is that the subset with optimal approximation of a dataset in some feature space doesn’t guarantee that it is the optimal for training neural networks. It is known that random selection can also perfectly approximate the geometry of original dataset (Chen et al 2012) as long as the subset is large enough. In addition, it is also unknown that such approximation guarantee of using submodular function is better or faster to reach than other approximation methods like herding, k-center (Sener & Savarese, 2018), k-medoids etc. 2. The authors admitted that “the difference between our method and k-center is marginal” in Discussion. The performance comparison also verified it. The performance improvement of the proposed method is mainly due to balancing constraints. More detailed ablation study on each individual balancing constraint should be given. Otherwise, it is unclear which constraint works. 3. The experiments lack the comparison to state-of-the-art subset selection methods. 4. In the balancing constraints, the image number of each class is pre-defined and the underlying assumption is that the class number is known. Is this assumption realistic in the experimental setting (active learning)?(Chen et al 2012) Super-Samples from Kernel Herding",3. The experiments lack the comparison to state-of-the-art subset selection methods.,3033,3,"The comment points out that the experiments do not include a comparison to state-of-the-art subset selection methods. While this feedback is explicit in identifying a missing element, it does not provide specific guidance on which subset selection methods should be included or how to incorporate them into the experiments. The authors are given a clear direction to add comparisons but lack detailed instructions on how to execute this. Therefore, the comment is somewhat actionable, as it identifies a specific area for improvement but lacks concrete guidance on implementation.",3,"The comment highlights a specific shortcoming in the experimental section of the paper, noting the absence of comparisons to state-of-the-art subset selection methods. This is a relevant point that could help the authors improve the comprehensiveness and relevance of their experimental analysis. However, the comment does not provide specific guidance or suggestions on which state-of-the-art methods should be considered or how to incorporate them into the experiments. While it identifies an important area for improvement, the feedback lacks actionable steps or detailed direction, making it somewhat helpful. Therefore, the comment is rated as somewhat helpful, consistent with a score of 3.",3,"The comment identifies a specific issue with the experiments, namely the lack of comparison to state-of-the-art subset selection methods. However, it does not specify which part of the experiments or which specific methods should be included for comparison. While the authors can infer that this comment pertains to the experimental section, they cannot confidently determine the exact part of the paper being addressed. This makes the comment weakly grounded. Nonetheless, the comment is specific in detailing what is missing in the experiments. Therefore, this comment is weakly grounded and specific, aligning with the label 3.",3,"The review point claims that the experiments lack a comparison to state-of-the-art subset selection methods. However, it does not provide any specific examples of such methods or explain why their inclusion is necessary for the study. The comment lacks detailed justification or references, making it challenging for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is somewhat verifiable, as it highlights a potential gap but does not provide enough information to fully substantiate the claim. This aligns with a score of 3: Somewhat Verifiable.",8361,,,,,,,,
Mq5cyRMGlD,EMNLP_2023,"While the paper addresses an important problem related to rewriting responses in the counseling education domain, there are some concerns regarding its novelty and contribution.1. The literature review shows that ample research already exists in this area. Although the authors present a paraphrase-augmented training basis as their USP, the problem of topic/domain shifting in paraphrase-augmented training is a common concern. The paper fails to discuss the intricacies involved with the approach and ""when not to take action"" especially in the case of uncontrolled domain shift.2. Although a mix of expert and non-expert validation has been included, no domain-centric or expert-guided metric is discussed. Being from a very similar field, I believe it is important to involve metrics or methods in psychiatric validation that lead to computational and psychological performance comparison. The question is why authors missed presenting such domain-centric metrics even with the presence of expert evaluators.3. Despite the existence of ample work, the selection of baselines is very limited to naive/old models. I encourage authors to add more baselines to perform robust performance comparisons.4. The proposed work lacks novelty. Several methods in the past have tried to apply very similar incorporation to state-of-the-art paradigms and have reported gains in response rewriting space and, in some cases, counseling rewriting too.","3. Despite the existence of ample work, the selection of baselines is very limited to naive/old models. I encourage authors to add more baselines to perform robust performance comparisons.",1594,3,"The comment explicitly encourages the authors to add more baselines to perform robust performance comparisons, indicating a clear action that needs to be taken. However, it does not specify which baselines should be included or provide guidance on how to select appropriate baselines. This lack of specificity makes the action somewhat vague, as the authors are left to determine which additional baselines would be most relevant or beneficial for their work. Therefore, while the action is explicit, the lack of concrete details on implementation makes this comment somewhat actionable.",3,"The review comment identifies a specific weakness in the paper, namely the limited selection of baselines used for performance comparisons. It suggests that the authors include more baselines to enhance the robustness of their comparisons. While the comment provides a clear direction for improvement, it lacks specific guidance on which additional baselines to consider or how to select them. This limits the comment's helpfulness, as the authors are left to determine the appropriate baselines on their own. Nonetheless, the feedback is actionable and points out a critical area for enhancement, making it somewhat helpful. Therefore, the comment aligns with a score of 3.",3,"The comment addresses the selection of baselines in the paper, suggesting that the authors have limited their comparisons to naive or older models. It encourages the addition of more baselines for robust performance comparisons. However, the comment does not explicitly mention which part of the paper discusses the baselines, making it weakly grounded. The suggestion to add more baselines is specific, as it provides a clear direction for improvement. Therefore, the comment is weakly grounded and specific, aligning with category 3.",3,"The review point claims that the selection of baselines is limited to naive or outdated models, suggesting that more baselines should be added for robust performance comparisons. While the comment highlights a potential issue, it lacks specific examples of more contemporary or relevant baselines that could be included. This absence of detailed references or examples makes the claim somewhat verifiable, as it provides a general direction but requires significant effort from the authors to identify and incorporate the suggested improvements.",7916,,,,,,,,
NIPS_2018_544,NIPS_2018,"- the presented results do not give me the confidence to say that this approach is better than any of the other due to a lot of ad-hoc decisions in the paper (e.g. digital identity part of the code vs full code evaluation, the evaluation itself, and the choice of the knn classifier) - the results in table 1 are quite unusual - there is a big gap between the standard autoencoders and the variational methods which makes me ask whether thereâs something particular about the classifier used (knn) that is a better fit for autoencoders. the particularities of the loss or the distribution used when training. why was k-nn used? a logical choice would be a more powerful method like svm or a multilayer perceptron. there is no explanation for this big gap - there is no visual comparison of what some of the baseline methods produce as disentangled representations so itâs impossibly to compare the quality of (dis) entanglement and the semantics behind each factor of variation - the degrees of freedom among features of the code seem binary in this case, therefore it is important which version of vae and beta-vae, as well as infogan are used, but the paper does not provide those details - the method presented can easily be applied on unlabeled data only, and that should have been one point of comparison to the other methods dealing with unlabeled data only - showing whether it works on par with baselines when no labels are used, but that wasnât done. the only trace of that is in the figure 3, but that compares the dual and primary accuracy curves (for supervision of 0.0), but does not compare it with other methods - though parts of the paper are easy to understand, in whole it is difficult to get the necessary details of the model and the training procedure (without looking into the appendix which i admittedly did not do, but then, i want to understand the paper fully (without particular details like hyperparameters) from the main body of the paper). i think the paper would benefit from another writing iteration Questions: - are the features of the code binary? because i didnât find it clear from the paper. if so, then the effects of varying the single code feature is essentially a binary choice, right? did the baseline (beta-)vae and infogan methods use the appropriate distribution in that case? i cannot find this in the paper - is there a concrete reason why you decided to apply the model only in a single pass for labelled data, because you could have applied the dual-swap on labelled data too - how is this method applied at test time - one needs to supply two inputs? which ones? does it work when one supplies the same input for both? - 57 - multi dimension attribute encoding, does this essentially mean that the projection code is a matrix, instead of a vector, and thatâs it? - 60-62 - if the dimensions are not independent, then the disentanglement is not perfect - meaning there might be correlations between specific parts of the representation. did you measure/check for that? - can you explicitly say what labels for each dataset in 4.1 are - where are they coming from? from the dataset itself? from what i understand, thatâs easy for the generated datasets (and generated parts of the dataset), but what about cas-peal-r1 and mugshot? - i find the explanation in 243-245 very unclear. could you please elaborate what this exactly means - why is it 5*3 (and not 5*2, e.g. in the case of beta-vae where thereâs a mean and stdev in the code) - 247 - why was k-nn used, and not some other more elaborate classifier? what is the k, what is the distance metric used? - algorithm 1, ascending the gradient estimate? what is the training algorithm used? isnât this employing a version of gradient descent (minimising loss)? - what is the effect of the balance parameter? it is a seemingly important parameter, but there are no results showing a sweep of that parameter, just a choice between 0.5 and 1 (and why does 0.5 work better)? - did you try beta-vae with significantly higher values of beta (a sweep between 10 and 150 would do)? Other: - the notation (dash, double dash, dot, double dot over inputs is a bit unfortunate because itâs difficult to follow) - algorithm 1 is a bit too big to follow clearly, consider revising, and this is one of the points where itâs difficult to follow the notation clearly - figure 1, primary-stage I assume that f_\phi is as a matter of a fact two f_\phis with shared parameters. please split it otherwise the reader can think that f_\phi accepts 4 inputs (and in the dual-stage it accepts only 2) - figure 3 a and b are very messy / poorly designed - it is impossible to discern different lines in a because thereâs too many of them, plus itâs difficult to compare values among the ones which are visible (both in a and b). log scale might be a better choice. as for the overfitting in a, from the figure, printed version, i just cannot see that overfitting - 66 - is shared - 82-83 Also, require limited weakerâ¦sentence not clear/gramatically correct - 113 - one domain entity - 127 - is shared - 190 - conduct disentangled encodings? strange word choice - why do all citations in parentheses have a blank space between the opening parentheses and the name of the author? - 226 - despite the qualities of hybrid images are not exceptional - sentence not clear/correct - 268 - is that fig 2b or 2a? - please provide an informative caption in figure 2 (what each letter stands for) UPDATE: Iâve read the author rebuttal, as well as all the reviews again, and in light of a good reply, Iâm increasing my score. In short, I think the results look promising on dSprites and that my questions were well answered. I still feel i) the lack of clarity is apparent in the variable length issue as all reviewers pointed to that, and that the experiments cannot give a fair comparison to other methods, given that the said vector is not disentangled itself and could account for a higher accuracy, and iii) that the paper doesnât compare all the algorithms in an unsupervised setting (SR=0.0) where I wouldnât necessarily expect better performance than the other models.","- i find the explanation in 243-245 very unclear. could you please elaborate what this exactly means - why is it 5*3 (and not 5*2, e.g. in the case of beta-vae where thereâs a mean and stdev in the code) - 247 - why was k-nn used, and not some other more elaborate classifier? what is the k, what is the distance metric used?",6551,5,"The review comment identifies specific areas where the explanation is unclear and requests elaboration. It explicitly asks for clarification on the meaning of lines 243-245, the reasoning behind the choice of 5*3, and the use of k-NN instead of another classifier. Additionally, it inquires about the value of k and the distance metric used. These questions provide direct and concrete actions for the authors to take, as they clearly outline the areas that need further explanation and detail. The feedback is explicit and actionable, making it highly actionable.",4,"The review comment identifies specific areas of the paper that are unclear, particularly the explanation in lines 243-245 and the choice of k-NN as a classifier. It asks for elaboration on the reasoning behind the numbers used and the choice of classifier, including details such as the value of k and the distance metric. This feedback is clear and actionable, as it directly points out where the authors need to provide more information and justification. By addressing these points, the authors can significantly improve the clarity and comprehensiveness of their paper. However, the comment could be more helpful if it offered suggestions on how to present this information or examples of alternative approaches. Overall, the comment is mostly helpful, as it effectively directs the authors toward areas needing clarification and improvement.",5,"The comment is fully grounded as it explicitly mentions specific lines, ""243-245"" and ""247,"" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it details the issues that need clarification, such as the explanation of the 5*3 calculation, the choice of k-NN as a classifier, and the parameters used for k-NN like the value of k and the distance metric. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.",X,"The comment consists of requests for clarification and questions about specific methodological choices in the paper. It does not contain any opinions, judgments, or suggestions that require verification. The inquiries are factual and descriptive, fitting the classification of ""No.""",4081,,,,,,,,
NIPS_2018_814,NIPS_2018,"1. The key question is âDo we need to make capsule networks explicitly equivariant?â My answer is âmost likely noâ. Because the idea of capsule networks is not to push for a specific and restricted type of equivariance. It allows freedom in the pose vector and choses the routing algorithm such that the data speak for itself and the pose matrices automatically capture the ârightâ variances in the lower level features. 2. Continuing the previous comment, arenât we unnecessarily restricting the capsule networks? Unfortunately, in the experiments the group CapNet is deeper than the simple CapNet and the authors do not provide the number of parameters in the group CapNet. So, we cannot judge how much enforcing the equivariance strictly is beneficial. Because the original CapNet does capture some variations in the data. 3. Unfortunately, the authors omit a key baseline in the experiments: the group-CNN. Not only it is not clear how much extra accuracy is due to CapNets, but also, they do not validate their argument in Section 4. I certainly enjoyed reading this paper, mainly because of the insights and rigor that it brings to the capsule networks idea. Unfortunately, there are doubts about about necessity of this idea and the experiments do not provide any solid evidence either. ======================== After reading the authors' rebuttal: I still do not understand why the proposed model is not directly comparable to the original G-CNN. In the rebuttal, the authors talk about keeping interpretability. But the current version of the paper does not have any experimental validation of these claims. As I mentioned in the original review, I like this paper, but I think it is not ready for publication at the current shape.","2. Continuing the previous comment, arenât we unnecessarily restricting the capsule networks? Unfortunately, in the experiments the group CapNet is deeper than the simple CapNet and the authors do not provide the number of parameters in the group CapNet. So, we cannot judge how much enforcing the equivariance strictly is beneficial. Because the original CapNet does capture some variations in the data.",6515,2,"The review comment raises a concern about potentially restricting the capsule networks unnecessarily and notes that the group CapNet is deeper than the simple CapNet. It also points out that the authors do not provide the number of parameters in the group CapNet, making it difficult to assess the benefits of enforcing equivariance strictly. However, the comment does not provide explicit guidance or suggestions on how the authors should address these issues. It lacks actionable details, such as recommending specific analyses or comparisons to clarify the benefits of the group CapNet. As a result, the authors may be uncertain about how to respond to the feedback effectively. Therefore, this comment is barely actionable.",3,"The review comment raises a relevant concern about the potential restriction of capsule networks and questions the experimental setup, specifically the depth and parameter count of the group CapNet compared to the simple CapNet. It highlights a gap in the information provided, which prevents a thorough evaluation of the benefits of enforcing equivariance. This feedback is somewhat helpful as it points out an area that requires clarification and additional information, prompting the authors to provide more details about their experimental setup. However, the comment could be more helpful if it offered specific suggestions on how to address these concerns or improve the analysis. Overall, it provides some direction for improvement but lacks comprehensive guidance, aligning with a score of 3.",3,"The comment addresses the issue of restricting capsule networks and provides specific feedback on the experiments, particularly regarding the depth of the group CapNet compared to the simple CapNet. It also highlights the lack of information about the number of parameters in the group CapNet, which prevents a proper assessment of the benefits of enforcing equivariance. However, the comment does not explicitly mention which part of the paper discusses these experiments or the capsule networks, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where these topics are discussed, the comment lacks full grounding. It is specific about the issue of parameter information but does not provide detailed guidance on how to address it. Therefore, this comment is weakly grounded and specific, aligning with category 3.",3,"The review point questions whether capsule networks are being unnecessarily restricted and highlights a lack of information regarding the number of parameters in the group CapNet. This critique is somewhat verifiable as it raises a valid concern about the experimental setup and the ability to assess the benefits of enforcing equivariance. However, it lacks specific examples or references to support the claim that the restriction might be unnecessary or to explain the importance of parameter count in this context. Additional details or comparisons would strengthen the justification, making it easier for authors to address the concern. Therefore, the comment is categorized as 3: Somewhat Verifiable.",6482,,,,,,,,
zweyouirw7,ICLR_2025,"1. Although the hybrid architecture is described in detail, the explanations of specific processes, such as spike-driven token selection and intra- and inter-stage feature fusion, could be clearer. Including pseudo-code or flow diagrams might enhance the reader’s understanding of the model’s operation.2. While the results on the Gen1 dataset are compelling, it would strengthen the paper to evaluate the model on more diverse datasets, particularly larger or more complex event-based datasets, to demonstrate generalizability.3. The paper could benefit from a discussion on how Spike-TransCNN compares with hybrid SNN-ANN models, given their potential to balance energy efficiency and performance. This would contextualize the performance and energy efficiency gains of Spike-TransCNN more effectively.4. While there is an ablation study on some components, further exploration on the impact of various hyperparameters (e.g., number of time steps, membrane potential thresholds) could provide insights into optimizing the architecture for different applications.5. Consistent terminology, particularly around spiking mechanisms and attention mechanisms, would improve readability. Some abbreviations and terms could be clarified for non-specialist readers.6. The paper includes visualization results, but providing side-by-side comparisons with other models on challenging scenarios could offer a clearer view of the model’s strengths in handling occlusions and motion.","1. Although the hybrid architecture is described in detail, the explanations of specific processes, such as spike-driven token selection and intra- and inter-stage feature fusion, could be clearer. Including pseudo-code or flow diagrams might enhance the reader’s understanding of the model’s operation.",8600,5,"The comment suggests that the explanations of specific processes within the hybrid architecture could be clearer and offers concrete suggestions for improvement, such as including pseudo-code or flow diagrams. This provides the authors with explicit actions to take in order to enhance the clarity of their explanations. The feedback is clear and provides specific guidance on how to implement the suggested improvements, making it highly actionable.",4,"The review comment identifies specific areas within the paper where the explanations could be clearer, namely the spike-driven token selection and intra- and inter-stage feature fusion processes. It provides actionable suggestions for improvement, such as including pseudo-code or flow diagrams, which could help enhance the reader's understanding of the model's operation. This feedback is mostly helpful as it offers clear guidance on how the authors can improve the clarity and comprehensibility of their draft. However, it could be further improved by providing more detailed suggestions or examples of how to implement these enhancements. Therefore, the comment aligns with a score of 4.",5,"The comment is fully grounded as it explicitly mentions the ""hybrid architecture"" and specific processes like ""spike-driven token selection"" and ""intra- and inter-stage feature fusion,"" allowing the authors to accurately identify the parts of the paper being addressed. It also provides specific suggestions for improvement, such as including pseudo-code or flow diagrams to enhance understanding. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.",3,"The review point suggests that while the hybrid architecture is well-described, the explanations of specific processes like spike-driven token selection and feature fusion could be clearer. The suggestion to include pseudo-code or flow diagrams is based on logical reasoning to enhance understanding. However, the comment lacks specific examples or references to support the claim that these explanations are unclear, making it somewhat verifiable. The authors would need to interpret the feedback and determine how best to address it, thus justifying a score of 3: Somewhat Verifiable.",4668,,,,,,,,
Z9AZsU1Tju,ICLR_2024,"1.The inspiration from neuroscience research seems simple, and unnecessary to the main processing pipeline of the main work, making the main work distracting.2.The order of the modality in processing the data seems quite important, which highly impacts the performance. For such an importance factor, it is better to provide further theoretical analysis or practical guidance on it.3.The paper should include some recent SOTA works, at lease works in 2022, on comparing the sentiment analysis.","1. The inspiration from neuroscience research seems simple, and unnecessary to the main processing pipeline of the main work, making the main work distracting.",9004,1,"The review comment criticizes the inclusion of inspiration from neuroscience research, suggesting it is simple and unnecessary, and that it distracts from the main work. However, it does not provide any specific guidance or suggestions on how the authors could address this issue or improve the focus of their work. The comment lacks actionable details, such as recommending ways to integrate the neuroscience inspiration more effectively or suggesting alternative approaches. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",1,"The comment expresses a subjective opinion about the inspiration from neuroscience research, suggesting it is simple and unnecessary, and that it distracts from the main work. However, it lacks specificity and does not provide actionable feedback or suggestions for improvement. Without identifying particular weaknesses or offering guidance on how the authors might address the perceived distraction, the comment does not assist the authors in improving their draft. Therefore, it aligns with a score of 1, as it is not helpful at all.",1,"The comment suggests that the inspiration from neuroscience research appears simple and unnecessary, potentially distracting from the main work. However, it does not specify which part of the paper this critique applies to, leaving the authors unable to identify the exact section or content being addressed. Additionally, the comment lacks specificity in explaining why the neuroscience inspiration is considered unnecessary or how it distracts from the main work. Without clear guidance or specific references, the comment is not grounded and not specific, aligning with a score of 1.",1,"The review point claims that the inspiration from neuroscience research is ""simple and unnecessary,"" suggesting it distracts from the main work. However, the comment lacks any supporting evidence, reasoning, or references to justify this claim. There is no detailed explanation or examples provided to help the authors understand why the neuroscience inspiration is considered unnecessary or distracting. As a result, the claim is unverifiable.",5336,,,,,,,,
UmdotAAVDe,ICLR_2025,"1. As we know LLM often hallucinates. How do the authors quality check the generated dataset?2. While the authors concluded that induction and transduction approaches are complementary, I hoped to see more in-depth analysis of why induction works better for some while transduction works better for the others.3. While the authors also discussed the limitation that such method is only tested on ARC, I still think this is a weakness. I wonder the conclusion would be different for some other real-world datasets.",1. As we know LLM often hallucinates. How do the authors quality check the generated dataset?,11453,2,"The review point raises a concern about the quality checking of the generated dataset, given that large language models (LLMs) often hallucinate. It implicitly suggests that the authors should address this issue by explaining their quality control measures. However, the comment does not provide explicit instructions or concrete steps on how to improve the draft in this regard. The action is implicit and somewhat vague, as the authors need to infer that they should include a discussion on quality checking but are not given specific guidance on how to do so. Therefore, this comment is barely actionable.",3,"The review comment raises an important question about the quality control of datasets generated by large language models (LLMs), which are known to sometimes produce inaccurate or ""hallucinated"" information. This is a critical aspect that the authors should address to ensure the reliability of their dataset. However, the comment does not provide specific suggestions or methods for how the authors might implement quality checks, which limits its helpfulness. While it points out a significant area for improvement, the lack of actionable guidance means the authors may struggle to effectively address the issue. Therefore, the comment is somewhat helpful, as it highlights a key concern but lacks depth and specificity in its feedback.",1,"The comment raises a concern about the quality checking of the generated dataset, particularly in the context of large language models (LLMs) that are known to hallucinate. However, it does not specify which part of the paper this concern relates to, such as the methodology, results, or discussion sections. Without explicit references or detailed guidance, the authors cannot effectively identify the areas needing improvement or clarification. As a result, the comment lacks both grounding and specificity, making it not grounded at all. Therefore, this comment aligns with a score of 1.",X,"The review point consists of a factual statement followed by a question. The statement ""As we know LLM often hallucinates"" is a widely recognized fact about large language models and does not constitute a subjective claim or opinion requiring verification. The subsequent question does not express an opinion or judgment but seeks clarification. Therefore, the review point should be labeled as ""No.""",4706,,,,,,,,
NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?","- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.",5941,3,"The review point suggests that the model implementation needs better justification, specifically questioning the stopping rule with n consecutive identical samples. It implies that the authors should provide a rationale for this choice and discuss its sensitivity. However, the comment does not explicitly instruct the authors on how to justify the stopping rule or what aspects of sensitivity should be addressed. The action is implicit and somewhat vague, as the authors need to infer that they should provide a justification and sensitivity analysis but lack detailed guidance on how to do so. Therefore, this comment is somewhat actionable.",3,"The review comment identifies a specific area where the manuscript could be improved, namely the justification of the model implementation, particularly the stopping rule with n consecutive identical samples. It points out that this rule seems arbitrary and lacks discussion on its sensitivity, which could be crucial for understanding the model's robustness and applicability. By highlighting these issues, the comment provides the authors with clear and actionable feedback on where to improve their manuscript. However, while it identifies the problem, it does not offer detailed suggestions or examples on how to address these issues, which could enhance its helpfulness. Overall, the comment is somewhat helpful as it directs the authors to a critical area needing clarification, but it could be more comprehensive with additional guidance.",3,"The comment addresses a specific aspect of the model implementation, namely the stopping rule with ""n consecutive identical samples."" It questions the justification for this choice and suggests discussing its sensitivity. However, the comment does not specify which part of the paper this issue is discussed in, making it weakly grounded. The comment is specific in detailing what needs to be addressed regarding the stopping rule and its justification. Therefore, the comment is weakly grounded and specific, aligning with category 3.",2,"The review point raises a claim that the stopping rule using ""n consecutive identical samples"" is arbitrary and lacks justification, particularly in terms of neural or behavioral parallels. While the comment identifies a potential issue with the model implementation, it does not provide specific examples, detailed reasoning, or references to support the claim. The lack of detailed justification or evidence makes it challenging for the authors to understand and address the concern effectively. Therefore, the claim is categorized as borderline verifiable, as it provides some insight but lacks sufficient support to be fully actionable.",2141,,,,,,,,
PFdjJiZjPj,ICLR_2024,"- The experimental evaluation and the observations are nice and make sense to share with the community, but the actual contribution of the paper beyond that is relatively minor.- Using self-generated code (instead of a placeholder) is not exactly a fair comparison when compared to previous work, as it samples the model 2x for each generation. Using the place-holder code was somewhat intended in the original technique so you only generate the code itself once.","- Using self-generated code (instead of a placeholder) is not exactly a fair comparison when compared to previous work, as it samples the model 2x for each generation. Using the place-holder code was somewhat intended in the original technique so you only generate the code itself once.",11669,3,"The review comment highlights a concern about the fairness of using self-generated code instead of a placeholder, as it involves sampling the model twice for each generation. It implies that the authors should reconsider their approach to align with the original technique, which only generates the code once. However, the comment does not explicitly instruct the authors on what specific changes to make or how to address this issue. The action is implicit and lacks concrete guidance, making it somewhat actionable.",3,"The review comment identifies a potential issue with the fairness of the experimental comparison, specifically noting that using self-generated code instead of a placeholder may lead to an unfair advantage by sampling the model twice for each generation. It highlights that the original technique intended to generate the code only once, suggesting that the current approach might deviate from the intended methodology. While the comment points out a significant concern, it lacks specific guidance or suggestions on how the authors might address this issue or adjust their methodology to ensure a fair comparison. This limits the comment's utility for the authors, making it somewhat helpful as it provides a direction for improvement but lacks depth and actionable advice.",3,"The comment addresses a specific issue regarding the use of self-generated code instead of a placeholder, which affects the fairness of the comparison with previous work. However, it does not explicitly mention which part of the paper this issue is discussed in, making it weakly grounded. The comment is specific in detailing the problem with the comparison method, providing clear guidance on what needs to be addressed. Therefore, the comment is weakly grounded but specific, aligning with category 3.",4,"The review point critiques the use of self-generated code for comparison, suggesting it is unfair because it samples the model twice for each generation, unlike the original technique that uses a placeholder code to generate only once. The comment provides a logical reasoning by explaining the difference in sampling frequency between the current approach and the original technique. However, it lacks specific references or examples from previous work to fully substantiate the claim. This makes the comment mostly verifiable, as it provides a clear reasoning but could benefit from additional evidence or references to strengthen the argument. Therefore, it aligns with a score of 4: Mostly Verifiable.",3821,,,,,,,,
uvq4Nh8eZB,ICLR_2024,"1. The novelty is limited. There are many prior studies have explored semi-supervised federated learning [1-3] and knowledge distillation [4-6]. The paper does not propose a new framework of FedSSL but just naively combining differential privacy techniques. The paper does not illustrate the difference between FedCT and those previous work.2. The motivation to protect the privacy of soft predictions is weak. Comparing to raw data/ features map, the soft predictions are not likely to leak privacy.3. The paper is not well-organized and using non-standard terminology. For example, the term 'semi-honst' in section 4. Usually, we use 'honest-but-curious' to descrtibe the server which can not modify the local updates but trys to infer private information of clients.4. The baselines are not comprehensive. DP-FedAvg is using DP to the updated gradients but the proposed FedCT is using DP in the soft predictions, it is not comparable. FedCT should compare other FedSSL methods [1-3] and conduct attach on the method to verify the effect of FedCT.5. Setting for data heterogenity is not sufficient. With beta = 2 for non-i.i.d setting is not good. Most of papers set beta = 0.01 or 0.1 to create heterogeneous data partitions. Reference:[1] Diao E, Ding J, Tarokh V. SemiFL: Semi-supervised federated learning for unlabeled clients with alternate training[J]. Advances in Neural Information Processing Systems, 2022, 35: 17871-17884.[2] Lin H, Lou J, Xiong L, et al. Semifed: Semi-supervised federated learning with consistency and pseudo-labeling[J]. arXiv preprint arXiv:2108.09412, 2021.[3] Itahara S, Nishio T, Koda Y, et al. Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data[J]. IEEE Transactions on Mobile Computing, 2021, 22(1): 191-205.[4] Li D, Wang J. Fedmd: Heterogenous federated learning via model distillation[J]. arXiv preprint arXiv:1910.03581, 2019.[5] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning[C]//International conference on machine learning. PMLR, 2021: 12878-12889.[6] Chen H, Vikalo H. The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation[J]. arXiv preprint arXiv:2301.08968, 2023.","3. The paper is not well-organized and using non-standard terminology. For example, the term 'semi-honst' in section 4. Usually, we use 'honest-but-curious' to descrtibe the server which can not modify the local updates but trys to infer private information of clients.",8671,3,"The review point highlights two issues: the paper's organization and the use of non-standard terminology, specifically mentioning the term 'semi-honst' and suggesting the more commonly used term 'honest-but-curious.' While the comment identifies a specific example of non-standard terminology and provides a suggestion for improvement, it does not offer detailed guidance on how to reorganize the paper or address other instances of non-standard terminology. The action regarding terminology is explicit and concrete, but the suggestion about organization is vague. Therefore, the comment is somewhat actionable.",3,"The review comment identifies two key issues with the paper: poor organization and the use of non-standard terminology. It provides a specific example of the term 'semi-honst' used in section 4, suggesting that 'honest-but-curious' is the more commonly accepted term. This feedback is valuable as it highlights areas where the paper could be improved for clarity and adherence to standard terminology, which is crucial for effective communication in academic writing. However, the comment could be more helpful if it offered additional examples of organizational issues or provided suggestions on how to improve the paper's structure. Overall, the feedback is somewhat helpful as it points out important areas for improvement but lacks comprehensive guidance. Therefore, it aligns with a score of 3.",5,"The comment identifies a specific issue with the organization and terminology used in the paper, providing an example with the term 'semi-honst' in section 4. This allows the authors to accurately pinpoint the section being addressed, ensuring full grounding. The comment also specifies the issue by suggesting the use of the more standard term 'honest-but-curious' to describe the server, which adds specificity to the feedback. Therefore, this comment is fully grounded and specific, aligning with category 5.",4,"The review point claims that the paper is disorganized and uses non-standard terminology, providing an example with the term 'semi-honst' in section 4. The reviewer contrasts this with the more commonly used term 'honest-but-curious,' which is typically used to describe a server that cannot modify local updates but tries to infer private information from clients. This comparison provides a clear example of the non-standard terminology, making the claim mostly verifiable. However, the comment could be strengthened by providing additional examples or references to standard terminology, which would make it fully verifiable. Therefore, the comment aligns with a label of 4: Mostly Verifiable.",8425,,,,,,,,
ARR_2022_145_review,ARR_2022,"I will list the detailed weakness in the next section. In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable.1. "" a smoother loss"" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting? There is a text overlap between 0.00 and 0.03 around the origin.2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is ""our method is also the **best** among all the baseline methods"", which is a contradiction to me. Later the paper suggests that ""outperforming the **baselines** of our implementation."", what is the difference between ""baseline"" and ""your re-implemented results"". 4. Some introduction to the datasets might also be helpful, such as the size of each dataset. 5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here. 8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting? 9. Line 22, ""Bert"" --> ""BERT""","6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?",1050,1,"The comment raises a question about the availability of labels for the test sets in the GLUE datasets, indicating some confusion. However, it does not provide any explicit or implicit action for the authors to take. The authors are left without guidance on how to address the reviewer's confusion or what changes, if any, are needed in the draft. Since the comment lacks actionable advice or suggestions, it is unactionable.",3,"The comment raises a question about the availability of labels for the test set performance on GLUE datasets, indicating some confusion. While it identifies a potential area of misunderstanding, it does not provide any guidance or suggestions on how the authors might address this issue in their paper. The comment lacks depth and actionable feedback, making it only somewhat helpful. It points out a potential weakness but does not offer a clear path for the authors to improve their draft. Therefore, the comment aligns with a score of 3.",1,"The comment raises a question about the test set performance on GLUE datasets and whether the labels are publicly available. However, it does not specify which part of the paper this question pertains to, such as a particular section, table, or figure. Without explicit references, the authors cannot confidently determine which part of the paper needs clarification or additional information. The comment is also not specific in terms of providing guidance on how to address the confusion. Therefore, this comment is not grounded and not specific, aligning with a score of 1.",X,"The comment is a request for clarification regarding the availability of labels for the GLUE datasets' test set performance. It does not contain any subjective claims or judgments, making it a factual inquiry. Therefore, it fits the ""No"" label.",9639,,,,,,,,
ICLR_2023_385,ICLR_2023,"- The statement ""These methods rely on maximizing the similarity between..."" in the Abstract is inaccurate. Not all self-supervised visual representation learning methods are based on such a contrastive learning scheme. Instead, most of the early methods are based on pretext tasks. Suggest revising the corresponding claims and also the title.- The main idea of this work is based on the assumption of the self-supervised visual learning method being contrastive-based ones. Whereas there are quite a few non-contrastive learning approaches. As a result, the main contributions of this work may be a bit limited.- It is unclear how the defined u_\psi was guaranteed to represent the transformation t. From the description, t was encoded by another set of layers, which are also learnable, making the equivariance here (esp. the ""equivariant predictor"") a bit misleading. The encoding of the transformations (Sec. 2.3.2) is rather confusing. In the beginning, it looks like the authors used one-hot encoding, but later some of the augmentations directly use the coordinates or real values (i.e. 1, 2, 3). The generalization for the augmentation set, as a result, is another issue. What if new augmentations were included? How would they be encoded?- It is unclear why the final model still needs the conventional invariance loss (i.e. the first term in the final loss function), if the proposed new equivariance-based loss is as claimed to be effective. Jointly optimizing both invariance and equivariance also seems to be confusing. How does the model actually learn in this case?- It is unclear why the Barlow Twins was only evaluated on the small-scale dataset CIFAR10.- The quality of the learned representation was only evaluated on the linear evaluation setting, which is a bit insufficient to get a clear conclusion. There are quite a few other downstream tasks that could be used as reported in the literature (e.g. fine-tuning, detection, and segmentation to name a few).- The proposed method was motivated by the case where ""augmentation information is useful"" as claimed by the authors, and example cases are flowers and birds (as claimed in the Introduction). But this was not validated in the experiment. There are some fine-grained datasets for such categories (e.g. the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset) that should have been used to validate the claims.- It is unclear why not use an objective similar to Eq. (5) or (6) to constrain the equivariance model training, but instead only maximize the similarity between z_i' and \hat{z}_i'?- Would an absolute value makes more sense for Eq (5)? Otherwise, what does a negative value mean, as the second case ""H-flip"" shown in Fig. 3?- Line below Eq. (5), ""Fig. 4"" should be ""Fig. 3"".","- The proposed method was motivated by the case where ""augmentation information is useful"" as claimed by the authors, and example cases are flowers and birds (as claimed in the Introduction). But this was not validated in the experiment. There are some fine-grained datasets for such categories (e.g. the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset) that should have been used to validate the claims.",3512,5,"The review point suggests that the authors should validate their claims about the usefulness of augmentation information by using specific datasets, such as the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset. This is an explicit and concrete action, as it clearly instructs the authors on what needs to be done to improve their draft. The comment provides specific examples of datasets that should be used, making it highly actionable.",4,"The review comment is mostly helpful as it identifies a discrepancy between the motivation for the proposed method and its experimental validation. It points out that while the authors claim augmentation information is useful for cases like flowers and birds, this was not tested in the experiments. The reviewer provides specific examples of fine-grained datasets, such as the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset, that could be used to validate these claims. This feedback is actionable and provides a clear direction for the authors to strengthen their work by aligning their experiments with their stated motivations. However, the comment could be more helpful if it included suggestions on how to incorporate these datasets into the experimental design or discussed potential challenges in doing so. Overall, the comment provides valuable guidance for improving the paper, aligning with a score of 4.",5,"The comment is fully grounded as it references the introduction where the authors claim that augmentation information is useful, specifically mentioning examples like flowers and birds. This allows the authors to accurately identify the part of the paper being addressed. The comment is also specific because it points out that the claim was not validated in the experiment and suggests using specific datasets (Oxford 102 flowers and Caltech-UCSD Birds-200) to validate the claims. Therefore, this comment is fully grounded and specific, aligning with category 5.",3,"The review point critiques the lack of validation for the claim that ""augmentation information is useful"" in the context of flowers and birds, as mentioned in the introduction. The reviewer suggests using specific datasets, such as the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset, to validate this claim. While the comment provides a logical reasoning for the need for validation and mentions specific datasets, it does not provide detailed evidence or references to support the necessity of these datasets for validation. This makes the claim somewhat verifiable, as the authors can understand the critique but may need more detailed guidance to fully address it.",3448,,,,,,,,
5lokEzttBF,ICLR_2025,"1. The motivation of this paper is unclear. Existing research has widely recognized the importance of ensuring diversity in instruction-tuning data. However, this paper seems to oppose this common understanding without strong justification. The experiments do not persuade me, as they are somewhat weak: both the dataset and the LLM size are limited. The results are unconvincing and, if not thoroughly validated, could potentially mislead the community.2. Although the paper proposes ""linguistic form"" and ""semantic surprisal"" as key style elements, the definitions and measurement methods of these concepts are slightly vague in some sections, especially the concept of ""semantic surprisal"", which still needs further clarification and explanation.3. Although the paper provides a lot of experimental data and results, the design and interpretation of some experiments are a bit complicated. For example, in the comparison of different data selection methods, some performance differences are not adequately explained. For some performance degradation cases (such as the performance of LLAMA2-13B), the paper does not explore the reasons behind it in detail.","1. The motivation of this paper is unclear. Existing research has widely recognized the importance of ensuring diversity in instruction-tuning data. However, this paper seems to oppose this common understanding without strong justification. The experiments do not persuade me, as they are somewhat weak: both the dataset and the LLM size are limited. The results are unconvincing and, if not thoroughly validated, could potentially mislead the community.",9346,1,"The review point raises concerns about the paper's motivation, noting that it appears to contradict the widely recognized importance of diversity in instruction-tuning data without providing strong justification. It also criticizes the experiments for being weak due to limited dataset and LLM size, and suggests that the results are unconvincing and potentially misleading. However, the comment does not provide specific guidance or suggestions on how the authors could address these issues, improve the justification, or strengthen the experiments. The feedback lacks actionable details, leaving the authors uncertain about what steps to take to improve their draft. Therefore, this comment is unactionable.",2,"The comment identifies a critical issue with the paper's motivation, noting that it contradicts widely recognized research without providing strong justification. It also critiques the experiments for being weak due to limitations in both the dataset and the LLM size, and expresses concern that the results are unconvincing and could mislead the community if not thoroughly validated. While the comment highlights significant weaknesses, it lacks specific suggestions or guidance on how the authors could address these issues or improve their work. Without actionable advice, the feedback does not offer much value to the authors in terms of improving their draft. Therefore, it aligns with a score of 2, indicating that the comment is barely helpful.",2,"The comment addresses several aspects of the paper, including the unclear motivation, the opposition to a common understanding in existing research, and the perceived weaknesses in the experiments. However, it does not specify which sections or parts of the paper these issues pertain to, making it difficult for the authors to pinpoint the exact areas needing improvement. Additionally, while the comment mentions the limited dataset and LLM size, it lacks specific guidance on how to strengthen the experiments or provide stronger justification. This lack of detail and specificity makes the comment weakly grounded and not specific, aligning with a score of 2.",3,"The review point raises concerns about the paper's motivation, suggesting it contradicts established research on the importance of diversity in instruction-tuning data without strong justification. The reviewer supports this claim by critiquing the experiments as weak due to limited dataset and LLM size, and describes the results as unconvincing. However, the comment lacks specific examples or references to existing research that emphasize the importance of diversity, and does not provide detailed reasoning or evidence to substantiate the claim about the experiments' limitations. This makes the claim somewhat verifiable, as it requires more detailed evidence or examples to fully validate the argument.",5398,,,,,,,,
NIPS_2020_863,NIPS_2020,"- The authors claim that the proposed structure-aware graph convolution filter is helpful. However, no ablation study is done to verify the design. What is the result if we just use a simple graph convolution layer? - The effect of including the “maximum convolution result” in Equation 5 is unclear. Ablation study is needed to support such a design. - The major evaluation is done on 10 objects illuminated under a fixed lighting distribution from the DiLiGenT benchmark, which is rather limited. The authors should evaluate their method on a wide diversity of synthetic data (e.g., different shapes rendered using different BRDFs under different (biased) lighting distributions) to justify the proposed method. - It is unclear what is the input image number during training of the proposed method. In Table 1, does the training number equals to the testing number? It would be helpful to analyze the effect of training image number as well. - It would be desirable to include another baseline network using only 1x1 convolutions in Table 3 for comparison. To make a fair comparison, different networks with a similar number of parameters should be compared. - It would be desirable to include runtime comparison of different deep learning based methods.",- It would be desirable to include runtime comparison of different deep learning based methods.,7377,2,"The review point suggests that including a runtime comparison of different deep learning-based methods would be beneficial. However, it does not specify which methods should be compared or how the comparison should be conducted. The action is implicit and lacks concrete details, leaving the authors uncertain about how to implement the suggestion effectively. Therefore, the comment is barely actionable.",3,"The review comment suggests including a runtime comparison of different deep learning-based methods. This feedback identifies a specific area where the manuscript could be improved, as runtime comparisons can provide valuable insights into the efficiency and practicality of the proposed method relative to others. However, the comment lacks depth and does not provide detailed guidance or suggestions on how to effectively present or analyze this comparison. To be more helpful, the comment could include specific aspects of the runtime comparison that need attention or suggest methodologies for conducting the analysis. Therefore, the comment is rated as somewhat helpful, consistent with label 3.",3,"The comment suggests including a runtime comparison of different deep learning-based methods, which is a specific suggestion for improvement. However, it does not explicitly mention which part of the paper this suggestion pertains to, such as the results section or a specific table or figure. The authors can infer that it relates to the evaluation or results section, but this inference is not directly guided by the comment. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.",1,"The review point suggests including a runtime comparison of different deep learning-based methods. However, it does not provide any justification or reasoning for why this comparison is necessary or how it would benefit the paper. Without supporting evidence or explanation, the claim lacks verifiability, making it difficult for the authors to understand the importance of this suggestion. Therefore, the comment is rated as 1: Unverifiable.",1906,,,,,,,,
NIPS_2020_253,NIPS_2020,"The main weaknesses of this paper are perhaps in missed opportunities to make it even more insightful. - The fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way). - This could perhaps be demonstrated by seeing if those detectors also suffered the same degradation in detection accuracy under compression. It would be even more interesting to see if the accuracy of these prior methods suffered after the proposed 'spectrum synthesis' step. - It would also be good to have more discussion, and preferably also analysis, of ""why"" these generative models produce images with different spectra. Is it something in the small-kernel convolution-transpose decoder architecture that have become so common ? Or is it because of where and how the random source (i.e., noise) is injected into these architectures ? One can imagine experiments to investigate this further: for example, how does fake detection accuracy change when one trains a GAN with spatially larger kernels; and how does it change if one introduces noise in later vs earlier layers.",- The fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way).,7380,1,"The review comment raises an interesting point about the potential underlying mechanisms of 'fake image detectors,' suggesting that they might be using the same spectral cues as the simple classifier mentioned. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how this observation should influence the authors' work, nor are there suggestions for improvement or further exploration. As a result, the comment lacks actionability, leaving the authors without direction on how to apply this information to enhance their draft. Therefore, it is rated as a 1 on the actionability scale.",3,"The comment raises an interesting point about the potential underlying mechanisms of 'fake image detectors,' suggesting that they might be using the same spectral cues as the simple classifier discussed. However, it does not provide specific guidance or suggestions on how the authors could explore or address this hypothesis in their work. The comment lacks actionable feedback that would help the authors improve their draft, such as proposing experiments or analyses to investigate this claim. As a result, the feedback is somewhat helpful, as it prompts the authors to consider a broader context but does not offer concrete steps for improvement. Therefore, it aligns with a score of 3.",1,"The comment raises an interesting point about the potential underlying mechanisms of 'fake image detectors,' suggesting that they might be using similar spectral cues as the simple classifier mentioned. However, it does not specify which part of the paper this observation is based on, nor does it provide detailed guidance on how the authors should address this point. Without specific references or detailed feedback, the authors cannot effectively identify the areas needing improvement or further exploration. As a result, the comment lacks both grounding and specificity, making it not grounded at all.",1,"The review point raises a question about the underlying mechanisms of 'fake image detectors,' suggesting that they might be using the same spectral cues as the simple classifier mentioned. However, the comment does not provide any evidence, references, or detailed reasoning to support this claim. It lacks specific examples or comparisons that would help verify the assertion. As a result, the claim is not verifiable due to the absence of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.",3025,,,,,,,,
NIPS_2018_112,NIPS_2018,"I have some questions on the experiments. - Missing baseline: Performances when removing L_id, L_pd, L_r and performances when only removing one of the not share E, L_sp, L_v are not evaluated. - How the experimental comparison with DR-GAN was done is not clear enough since DR-GAN does not use the pose map for input data. - The proposed method does not require the pose estimation in the inference, so test image does not have pose maps. It is not clear if the visual analysis on Sec.4.4 is the result on training images or test images. - Why the generated images of Figure 4 (a) shows only one of the [18],[19] for each input image? - Why the [18][19] fails and the proposed method succeed? Several parts of this paper are hard to understand. -The names âIdentity discriminatorâ and âpose discriminatorâ are confusing. When reading Fig.2, it looks like these losses are to distinguish different identities and poses. However, Eq.(2) seems to be the loss to distinguish real vs. fake images (of the same person), not to distinguish between different identities. Similarly, the âpose discriminatorâ is not to distinguish between different poses. - Line29: What is LOSO regularization? - I suggest replacing some arXiv papers with actual published works. Response to Rebuttal: The rebuttal addressed my concerns about the experiments. However, the additional ablation study was only on Market-1501 dataset, and DukeMTMC-reID dataset should be included in the final version. Also, the explanation of identity/pose discriminators should be carefully revised according to the response.","- The proposed method does not require the pose estimation in the inference, so test image does not have pose maps. It is not clear if the visual analysis on Sec.4.4 is the result on training images or test images.",6373,4,"The comment points out a lack of clarity regarding whether the visual analysis in Section 4.4 pertains to training images or test images, given that the proposed method does not require pose estimation during inference. While the comment highlights an area that needs clarification, it does not explicitly instruct the authors to clarify this point in their paper. However, the action is implied and concrete, as the authors can deduce that they need to specify the source of the images used in the visual analysis. Therefore, this comment is mostly actionable.",3,"The review comment identifies a lack of clarity in the paper regarding whether the visual analysis in Section 4.4 pertains to training or test images, given that the proposed method does not require pose estimation during inference. This feedback highlights a specific area where the manuscript could be improved, prompting the authors to clarify this aspect. However, the comment does not provide suggestions on how to address this issue or why this distinction is important, which limits its helpfulness. While it points out a necessary clarification, it lacks depth and actionable guidance, making it somewhat helpful for the authors to consider revising their explanation. Therefore, the comment aligns with a score of 3.",5,"The comment explicitly mentions ""Sec.4.4,"" providing full grounding as it allows the authors to accurately identify the section being addressed. It also specifies the issue by questioning whether the visual analysis pertains to training or test images, offering a clear direction for the authors to clarify this aspect. This level of detail makes the comment fully grounded and specific, aligning with a score of 5.",X,"The comment points out a lack of clarity regarding whether the visual analysis in Section 4.4 pertains to training or test images. It does not express an opinion, judgment, or suggestion that requires verification. It is a request for clarification, making ""No"" the appropriate label.",3424,,,,,,,,
D97Zfgv4em,EMNLP_2023,"1. Model extraction is a very relevant topic in the age of ChatGPT, where several approaches have been introduced to construct instruction tuning datasets automatically [1, 2, 3, 4], in an attempt to ""extract"" closed-source ChatGPT. This paper would be much more timely and exciting if the methods introduced in this paper are applied to the instrucion tuning setting, and the sample efficiency is compared to techniques like self-instruct [1]. The clustering and filtering approach introduced in this paper could be an interesting way to create instruction diversity, and could potentially lead to better instruction tuned models. However, the paper currently focuses on BERT fine-tuning settings which was the paradigm in 2019, and not used much anymore in state-of-the-art language modeling.2. In Table 1 (appendix version), the victim models perform much better than the extracted models, and I am guessing this is because of how strict the query budget for the extracted models are. Is such a strict query budget (<1K queries) necessary? API access is quite cheap these days --- for instance, 10K queries to ChatGPT (each with 500 tokens), would cost just 10$ according to the OpenAI API [5]. The paper should justify the strict query budgets by presenting cost estimates from different APIs. This is critical since the main premise of the paper is that it is a ""query-efficient"" method.3. I have some concerns about the sequence inference classifier that is used to filter data. It's unclear to me what kind of data passes this filter --- the task of filtering data using the task description seems quite different from the original MNLI distribution. Some examples of the filtered data would be helpful to look at.4. Minor: A discussion on possible defenses against model extraction would be useful.[1] - https://arxiv.org/abs/2212.10560[2] - https://gpt4all.io/reports/GPT4All_Technical_Report_3.pdf[3] - https://crfm.stanford.edu/2023/03/13/alpaca.html[4] - https://arxiv.org/abs/2305.15717[5] - https://openai.com/pricing",3. I have some concerns about the sequence inference classifier that is used to filter data. It's unclear to me what kind of data passes this filter --- the task of filtering data using the task description seems quite different from the original MNLI distribution. Some examples of the filtered data would be helpful to look at.,1735,4,"The review comment expresses concerns about the sequence inference classifier used to filter data and mentions that it is unclear what kind of data passes this filter. The reviewer suggests that providing examples of the filtered data would be helpful. While the comment implies that examples should be included, it does not explicitly instruct the authors to do so. However, the suggestion to include examples is a concrete action that the authors can take to address the concern. The action is somewhat implicit but clear enough for the authors to understand what needs to be done. Therefore, the comment is mostly actionable.",3,"The review comment raises concerns about the sequence inference classifier used to filter data, specifically questioning the type of data that passes through this filter. It highlights a potential discrepancy between the filtering task and the original MNLI distribution. The suggestion to provide examples of the filtered data is a clear and actionable recommendation that could help clarify the filtering process and its outcomes. While the comment identifies a significant area for improvement and offers a specific suggestion, it could be more helpful by providing additional guidance or context on why this issue is important. Overall, the feedback is somewhat helpful as it directs the authors' attention to a specific aspect of their work that needs clarification, aligning with a score of 3.",3,"The comment raises concerns about the sequence inference classifier used for filtering data, indicating that the task of filtering seems different from the original MNLI distribution. It suggests providing examples of the filtered data to clarify what kind of data passes the filter. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the methodology or results section discussing data filtering. The comment is specific in detailing what is unclear and what could be added to improve understanding, namely examples of the filtered data. Therefore, the comment is weakly grounded but specific, aligning with the label 3.",3,"The review point raises concerns about the sequence inference classifier used for data filtering, suggesting that the task of filtering data using the task description differs from the original MNLI distribution. However, the comment lacks specific examples or detailed reasoning to support the claim. The suggestion to include examples of filtered data is helpful, but without further elaboration or evidence, the claim remains somewhat vague. This makes the comment somewhat verifiable, as the authors would need to further develop the rationale themselves to address the concern effectively.",7995,,,,,,,,
ICLR_2021_2759,ICLR_2021,""" section. StrengthsExcellent writing and well-structured paper.Intuitive and novel method that is really interesting to read about with good intuition and analytical examples to motivate the method.Experiments are carried out well and the results are presented in a coherent manner. Each experiment is repeated multiple times. WeaknessesAs mentioned before, I would suggest to tone down the lottery ticket comparisons. I would assume that PHEW does not perform quite as well as the classic lottery tickets derived from IMP, which would not be surprising since IMP requires numerous training iterations. But in any case, I would like to see actual comparisons in terms of the resulting final test accuracy as main comparison tool instead of the comparison metric (Jaccard index, IoU) presented in Section 6.2.The method seems to be limited to convolutional and fully-connected layers. Could the authors discuss potential generalization to a larger class of network layers? Since unstructured pruning methods (like PHEW) usually do not enable faster training or inference times, I find their main advantage to be that they are network-agnostic. This can thus enable the easy deployment of unstructured pruning to any desired layer or architecture.As a follow-up to the previous question. How are non-sequential network architectures handled? The experiments include ResNets, which contain residual connections, but to me it is not clear from the paper how this is achieved with PHEW. Also, what about recurrent architectures? Can PHEW handle those? As far as I am aware, the competing methods should not have a problem handling those architectures as well.How about (at least a few) experiments with the full ImageNet data set? It shouldn't be too computationally expensive to train a ResNet on the full ImageNet data set especially since pruning at initialization does not require multiple retrain cycles like IMP.Other Minor Feedbacktypo, page 4, below Fig. 4: ""...even more better...""I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs.Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work.","4: ""...even more better..."" I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs. Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work.",2184,4,"The review comment addresses two issues. First, it points out a grammatical error in the phrase ""...even more better...,"" which is explicit and actionable as it clearly indicates a correction that needs to be made. Second, it questions the claim that Figure 4(b) demonstrates faster training due to the proposed method, suggesting that this claim might be overstated. The reviewer implies that the authors should reconsider or clarify this claim, but does not provide specific guidance on how to do so. Lastly, the reviewer suggests that the paper would benefit from a longer discussion of related work, which is an explicit suggestion but lacks detail on what specific related work should be included. Overall, the comment contains explicit actions but lacks concrete details for some of them, making it mostly actionable.",4,"The review comment addresses multiple aspects of the paper. It begins by pointing out a grammatical error, which is a minor but helpful correction. The comment then questions the interpretation of Figure 4(b), suggesting that the claim of faster training may be overstated, especially since all networks are trained for the same number of epochs. This critique encourages the authors to reassess their conclusions and possibly provide additional evidence or clarification. Lastly, the reviewer suggests that the paper would benefit from a more extensive discussion of related work in the field of pruning, which is a constructive recommendation for broadening the paper's context and depth. While the comment provides valuable insights and actionable suggestions, it could be more helpful if it included specific examples or references for the related work discussion. Overall, the feedback is mostly helpful, offering clear directions for improvement but lacking some depth in its suggestions. Therefore, it aligns with a score of 4.",4,"The comment refers to Figure 4(b) and questions the claim that the proposed method results in faster training. This makes the comment fully grounded, as it explicitly mentions a specific figure in the paper. However, the comment lacks specificity regarding why the figure does not convincingly support the claim, and it does not provide detailed guidance on how to improve the discussion of related work. Therefore, the comment is fully grounded but under-specific, aligning with category 4.",4,"The review point questions the claim that Figure 4(b) demonstrates faster training for the proposed method. The reviewer argues that this claim is unconvincing because all networks are trained for the same number of epochs, which is a logical reasoning that challenges the validity of the claim. Additionally, the reviewer suggests that the paper could benefit from a more extensive discussion of related work in the field of pruning, which implies that the current discussion may be lacking in depth or context. However, the comment does not provide specific examples or references to support the need for a longer discussion, which leaves some gaps in the justification. Therefore, the claim is mostly verifiable, aligning with a score of 4: Mostly Verifiable.",9298,,,,,,,,
r5d8zkYizS,ICLR_2025,"1. The significance of the theoretical findings is unclear. The authors should have demonstrated more potential application of the theoretical findings, e.g., how to apply the findings to design a new defense.2. The theoretical finding is a limit behavior that only holds for extremely small perturbation sizes. The authors’ argument that such an assumption on small perturbation size is needed is not sufficient because we don’t even have a proper definition of how small is “small enough”. As the authors describe in Section 7, a formal convergence rate analysis is required.3. The experiments should be improved further.* The number of adversarial examples is too small to demonstrate some distribution.* The experiment setup is quite arbitrary. Different architectures and attacks are used for different datasets.* The choice of attack algorithms (DeepFool and FAB) also seems uncommon. It is better to use simpler and common attack algorithms.",* The number of adversarial examples is too small to demonstrate some distribution.,5500,1,"The review point notes that the number of adversarial examples is too small to demonstrate some distribution, but it does not provide any explicit or implicit actions for the authors to take. There is no guidance on whether the authors should increase the number of examples, provide additional analysis, or consider other approaches to address this issue. Without any actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.",1,"The comment identifies a potential issue with the paper, specifically that the number of adversarial examples is too small to demonstrate some distribution. However, it lacks any further explanation or suggestions on how the authors might address this issue. Without additional context or guidance, the authors are left without actionable steps to understand or resolve the problem. The comment does not provide enough information to be helpful in improving the draft, making it unhelpful. Therefore, it aligns with a score of 1, indicating that the comment is not helpful at all.",1,"The comment suggests that the number of adversarial examples is insufficient to demonstrate a distribution. However, it does not specify which part of the paper this critique pertains to, such as a particular section, table, or figure. Additionally, it lacks specificity regarding what distribution is being referred to or how the number of examples should be increased or analyzed differently. This makes it difficult for the authors to identify the exact part of the paper that needs attention or to understand the specific issue being raised. Therefore, the comment is not grounded and highly unspecific, aligning with a score of 1.",1,"The review point claims that ""the number of adversarial examples is too small to demonstrate some distribution."" However, it does not provide any supporting evidence, reasoning, or examples to justify why the number is considered too small or what distribution is being referred to. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.",438,,,,,,,,
aM7US5jKCd,ICLR_2024,"- My major concern is the limited novelty, as the explored loss functions are not new. Although JS divergence, masked CE loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions- The conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation- The paper only considers untargeted attacks, and it would be useful to extend the analysis to targeted attacks to showcase the strength of the proposed attack method.- The authors could conduct experiments to evaluate the transferability of the proposed attack to other models and compare it against the baseline PGD/CosPGD/SegPGD attacks.","- My major concern is the limited novelty, as the explored loss functions are not new. Although JS divergence, masked CE loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions - The conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation - The paper only considers untargeted attacks, and it would be useful to extend the analysis to targeted attacks to showcase the strength of the proposed attack method.",4513,3,"The review point raises several concerns about the paper, including limited novelty due to the use of existing loss functions, the lack of black-box evaluation, and the focus on untargeted attacks. While the comment identifies areas for improvement, it does not provide explicit guidance on how to address these issues. The suggestion to extend the analysis to targeted attacks is somewhat actionable, but the comment lacks concrete details on how to implement this extension or how to address the novelty and evaluation concerns. The feedback is implicit and vague, leaving the authors with a general direction but without specific steps to follow. Therefore, this comment is somewhat actionable.",4,"The review comment addresses several critical aspects of the paper, providing a comprehensive evaluation of its limitations. It highlights concerns about the novelty of the work, noting that the loss functions used are not new and suggesting that the approach appears to be a straightforward application of existing methods. The comment also points out the lack of black-box evaluation, which is a significant limitation, and suggests extending the analysis to include targeted attacks to better demonstrate the proposed method's effectiveness. While the comment identifies key weaknesses and offers some direction for improvement, it could be more helpful by providing specific suggestions or examples on how to address these issues. Overall, the feedback is mostly helpful as it covers multiple areas for improvement and provides a clear rationale for each point, aligning with a score of 4.",3,"The comment raises several concerns about the paper, including the limited novelty of the explored loss functions, the lack of black-box evaluation, and the focus on untargeted attacks. However, it does not specify which sections or parts of the paper these concerns relate to, making it difficult for the authors to pinpoint where revisions are needed. While the comment is specific in terms of the issues it raises, it lacks grounding because it does not reference specific sections, figures, or tables in the paper. Therefore, this comment is weakly grounded but specific, aligning with a score of 3.",3,"The review point raises several claims regarding the paper's limitations, including limited novelty due to the use of existing loss functions, the lack of black-box evaluation, and the focus on untargeted attacks. The claim about limited novelty is somewhat supported by the mention of specific loss functions (JS divergence, masked CE loss, and masked spherical loss) that are not new but are noted as uncommon in the specific context of segmentation attacks. However, the comment does not provide references or detailed reasoning to substantiate why these functions are considered a simple 'plug and play.' The critique about the absence of black-box evaluation and the suggestion to extend the analysis to targeted attacks are logical but lack specific examples or references to support the claims. Overall, the comment provides some support for its claims but lacks detailed justification, making it somewhat verifiable.",2194,,,,,,,,
hL5jone2Oh,ICLR_2025,"1. Some domain problem setting, namely rejoining and deciphering, is not convincing.2. Lack of the results of fine-tuned open-source LLMs which is quiet important to a domain specific benchmark.3. Among the analysis of each domain problem or each scenario, the essential reason why open LMM perform like that is lacking which is important to the community beyond a specific benchmark.",2. Lack of the results of fine-tuned open-source LLMs which is quiet important to a domain specific benchmark.,9831,3,"The review point highlights the absence of results for fine-tuned open-source LLMs, which the reviewer considers important for a domain-specific benchmark. While the comment identifies a gap in the paper, it does not explicitly instruct the authors to include these results or provide guidance on how to do so. The action is implicit, as the authors need to infer that they should add these results, and it is somewhat vague because it lacks specific details on which LLMs to include or how to present the results. Therefore, this comment is somewhat actionable.",3,"The comment identifies a missing element in the paper—the results of fine-tuned open-source LLMs—which is important for a domain-specific benchmark. While it highlights a relevant gap, the feedback lacks specificity and actionable guidance on how the authors might address this issue. It does not suggest which open-source LLMs should be considered or how their inclusion could enhance the paper. As a result, the authors are left with a general idea of what is missing but without detailed direction on how to improve their draft. Therefore, the comment is somewhat helpful, as it provides some insight but lacks depth and specificity.",3,"The comment highlights the absence of results for fine-tuned open-source LLMs, which are important for a domain-specific benchmark. However, it does not specify which part of the paper this issue pertains to, making it weakly grounded. The comment is specific in identifying what is missing—results of fine-tuned open-source LLMs—but lacks grounding as it does not guide the authors to a particular section or part of the paper. Therefore, this comment is weakly grounded and specific, aligning with category 3.",1,"The review point claims that there is a lack of results for fine-tuned open-source LLMs, which are important for a domain-specific benchmark. However, the comment does not provide any supporting evidence, reasoning, or references to justify why these results are important or how their absence impacts the paper. The lack of detailed explanation or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. As a result, the claim is considered unverifiable.",6785,,,,,,,,
ICLR_2022_912,ICLR_2022,"1. The paper in general does not read well, and more careful proofreading is needed. 2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.","1. The paper in general does not read well, and more careful proofreading is needed.",3310,4,"The review point explicitly states that the paper does not read well and suggests that more careful proofreading is needed. This is a direct and clear action for the authors to take, indicating that they should review the paper for readability and make necessary corrections. However, it does not provide specific areas or examples where the readability is lacking, which would have made the action more concrete. Despite this, the action is explicit, making the comment mostly actionable.",2,"The comment identifies a general issue with the readability of the paper, suggesting that more careful proofreading is needed. While this is a valid observation, the feedback is vague and lacks specificity. It does not pinpoint particular sections or types of errors that need attention, nor does it provide guidance on how to improve the readability. Without detailed suggestions or examples, the authors may find it challenging to address the issue effectively. Therefore, the comment is barely helpful as it provides only minimal guidance for improvement.",1,"The comment provides a general critique of the paper's readability and suggests the need for more careful proofreading. However, it does not specify which sections or parts of the paper are problematic, nor does it provide specific guidance on what needs to be improved. This lack of detail makes it challenging for the authors to identify and address the issues effectively. Therefore, the comment is not grounded and lacks specificity, aligning with a score of 1.",1,"The review point makes a claim that the paper does not read well and requires more careful proofreading. However, it does not provide specific examples of poorly written sections or errors that need correction. Without detailed reasoning or evidence, such as pointing out specific areas that need improvement, the claim remains unsupported. This makes it difficult for the authors to understand the basis of the suggestion and address it effectively. Therefore, the comment is considered unverifiable.",5455,,,,,,,,
y3dqBDnPay,ICLR_2024,"1. Overall, while the idea is interesting, the scope is relatively narrow as it combines self-supervised learning with multimodal representation learning based on hypergraph. However, hypergraph attention network for multimodal learning has been explored in https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.pdfCompared to this CVPR 2020 paper, the only novel part for the manuscript seems to be adding self-supervised learning on top of it. However, that novelty is relatively small.2. The performance gain appears to be incremental as the paper serves as an extension to recent work.","1. Overall, while the idea is interesting, the scope is relatively narrow as it combines self-supervised learning with multimodal representation learning based on hypergraph. However, hypergraph attention network for multimodal learning has been explored in https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.pdf Compared to this CVPR 2020 paper, the only novel part for the manuscript seems to be adding self-supervised learning on top of it. However, that novelty is relatively small.",9802,1,"The review comment critiques the originality of the manuscript by comparing it to a previous work, specifically noting that the only novel aspect is the addition of self-supervised learning, which is deemed relatively minor. However, the comment does not provide any actionable guidance or suggestions for how the authors could enhance the originality or scope of their work. It lacks explicit or implicit actions that the authors could take to improve their draft. As a result, the authors are left without a clear understanding of what steps to take to address the critique. Therefore, this comment is unactionable.",2,"The review comment acknowledges the interesting nature of the idea but points out that the scope is relatively narrow, as it primarily combines self-supervised learning with multimodal representation learning based on hypergraph. It references a specific paper (CVPR 2020) that has already explored hypergraph attention networks for multimodal learning, suggesting that the only novel aspect of the manuscript is the addition of self-supervised learning. However, the comment does not provide specific suggestions or guidance on how the authors might enhance the novelty or expand the scope of their work. Without actionable feedback or detailed advice, the comment is limited in its usefulness to the authors. Therefore, it is rated as barely helpful, aligning with a score of 2.",3,"The comment provides a critique of the paper's novelty by comparing it to a specific external work, the CVPR 2020 paper, which explores hypergraph attention networks for multimodal learning. It suggests that the only novel aspect of the manuscript is the addition of self-supervised learning, which is considered relatively minor. However, the comment does not specify which part of the paper this critique is based on, such as specific sections or claims that should be revised or expanded. While the authors can infer that it relates to the novelty and contribution sections, the lack of explicit grounding makes it weakly grounded. The comment is specific in its critique of the novelty but lacks grounding. Therefore, this aligns with a score of 3.",4,"The review point claims that the manuscript's novelty is limited to adding self-supervised learning to a hypergraph attention network for multimodal learning, which has been previously explored in a CVPR 2020 paper. The reviewer supports this claim by providing a specific reference to the CVPR 2020 paper, which serves as a basis for comparison. This reference helps substantiate the claim by offering a concrete example of prior work, making the comment mostly verifiable. However, the review could be further strengthened by detailing how the self-supervised learning addition impacts the overall contribution, which would provide a more comprehensive justification. Thus, the comment is rated as 4: Mostly Verifiable.",5069,,,,,,,,
ICLR_2023_2807,ICLR_2023,"W1. Depth estimation use-case is not convincingW2. The claimed contributions are weak.W3. The convergence bound is not convincing (in an adversarial sequence)W4. Weak or no comparison in related work or results DiscussionW1. In Section 1.1, the paper introduces online depth estimation as a motivational example for justifying risk controlling. Here, the paper considers (I guess) depth measurements from accurate sensors (e.g., Lidar) as the ground truth depth, and claimed the necessity for uncertainty quantification for speed-up depth sensing. I think this may be a good example, but there are many missing pieces. First, I’m not sure if there are actual benefits in speed-up by using depth estimation via RGB instead of Lidar. In close range, Lidar can collect accurate depth very quickly, but for far objects, anyway we don’t need accurate depth, for example, in self-driving scenarios. Secondly, this motivation example uses the sensor measurements from an accurate sensor as ground truth, but it is unrealistic. For example, Lidar can be very noisy or wrong in snowy weather or puddles in the ground. So, it provides wrong labels to online learning algorithms. At least these two points undermine the justification on the necessity of risk controlling in online settings. Could you refine the motivation example more?W2. The paper claims three contributions: (1) validity on the risk of constructed prediction sets, (2) compatibility with online learning methods, and (3) fast reaction to distribution shift.For the risk validity, it is novel in online settings (considering risk controlling is possible under iid), but I’m not convinced why we need risk controlling as mentioned in W1.For the second contribution, I think this holds for any conformal approaches, including ACI; I don’t think this is a unique contribution of this paper.For the fast reaction to the distribution shift, I feel that this is a bit contradictory. The paper proposes several interesting stretch functions to reduce prediction set size, while maintaining a desired coverage. But, choosing a stretch function is equivalent to hyperparameter tuning. Meaning that, a good stretch function depends on the property of a data sequence, but simultaneously online algorithms, including Rolling RC, want to work for any distributions. I think I mainly get this impression on the way of choosing hyperparameters of this paper, i.e., hold out an initial subsequence in the entire sequence and test on the later subsequence. This hold-out-first-sequence approach does not make sense in distribution shift, as the initial subsequence cannot represent the later sequence. Could you justify this tuning approach?W3. I was trying to understand the convergence bound in the last equation of Theorem 1 proof. Mainly, I was curious about why this deterministic algorithm could work in adversarial settings; recall that traditional online learning algorithms are randomized such that an adversary could not entirely fool the algorithms (or need a realisability assumption, e.g., for the perceptron algorithm).To make the analysis simple, let m=0, M=1, \gamma=1, \theta_1=0, risk is the indicator loss, and r=0.1. Then, this convergence bound means the absolute difference of the average coverage rate and a desired coverage rate r is bounded by 1/T. By setting T = 10, this means after 10 time steps, the average coverage rate is at most 0.2 for any sequences. So, an adversary can choose an odd-subsequence (i.e., data arrive in odd time) such that a prediction set makes an error. Then, the \theta changes as follows:t=1: 0 -> 0.9, t=2: 0.9 -> 0.8, t=3: 0.8 -> 1, t=4: 1 -> 0.9, t=5: 0.9 -> 1, … As can be seen, at the odd time, \theta is not one, the adversary can generate a datum to make an error. This means the average coverage rate is 0.5, which is larger than the bound by theorem. Could you correct my understanding? If not, what are additional assumptions to achieve the desired coverage rate?W4. The risk controlling aspect is new in online learning, but I’m not sure why we cannot use the risk controlling idea along with ACI? As mentioned, ACI updates the coverage rate, while Rolling RC updates the threshold directly. What is the reason that we cannot control risk in ACI? If that’s possible, what’s the benefit of using risk controlling in threshold update? This may be justified empirically.","1 -> 0.9, t=5: 0.9 -> 1, … As can be seen, at the odd time, \theta is not one, the adversary can generate a datum to make an error. This means the average coverage rate is 0.5, which is larger than the bound by theorem. Could you correct my understanding? If not, what are additional assumptions to achieve the desired coverage rate?",3400,4,"The review point presents a specific observation about the coverage rate and asks the authors to correct the reviewer's understanding or provide additional assumptions to achieve the desired coverage rate. The comment is explicit in its request for clarification and provides a concrete context for the authors to address. By asking for a correction or additional assumptions, the reviewer gives the authors a clear action to take. However, the comment does not specify what the desired coverage rate should be or how to achieve it, leaving some room for interpretation. Therefore, the comment is mostly actionable, as it provides a clear direction but lacks complete details on execution.",3,"The review comment raises a question about the understanding of a specific aspect of the paper, particularly regarding the coverage rate and its relation to the theorem. It points out a potential discrepancy in the average coverage rate and asks for clarification on whether additional assumptions are needed to achieve the desired coverage rate. While the comment identifies a potential issue and seeks clarification, it does not provide specific guidance or suggestions for improvement. The authors are left with a question to address but without actionable feedback on how to resolve the issue. Therefore, the comment is somewhat helpful, as it highlights an area for clarification but lacks depth and specific direction for improvement.",1,"The comment is not grounded as it does not specify a particular section, figure, or table in the paper. It presents a sequence of numbers and a question about understanding the coverage rate, but without context, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what needs to be corrected or clarified, as it does not provide enough detail about the assumptions or the theorem mentioned. Therefore, this comment is not grounded and not specific, aligning with a score of 1.",X,"The review point consists of a question seeking clarification on the understanding of a specific concept. It does not contain any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, it is purely factual and fits the ""No"" label.",4272,,,,,,,,
gWHQQagPbN,ICLR_2025,"1. [Novelty] I am not sure about the differences between this paper and the Venom paper [1]. Wasn't V:N:M proposed in the Venom paper [1]? How does it differ from Venom [1]?2. The Venom paper proposed a GPU kernel for speedup sparse model on GPUs. how does this paper handle processing on the GPU? Does it propose a dedicated GPU kernel? If so, what are the differences compared to the Venom [1] GPU kernel?3. The title of the paper includes ""Transformer""—what specific characteristics of Transformers does it take into account?4. Looking at Table 5, the caption lacks sufficient explanation. For instance, a clear description of ""$\Delta$"" is necessary in the caption.[1] Venom: A vectorized N:M format for unleashing the power of sparse tensor cores. SC 2023",1. [Novelty] I am not sure about the differences between this paper and the Venom paper [1]. Wasn't V:N:M proposed in the Venom paper [1]? How does it differ from Venom [1]?,10063,2,"The review point raises a question about the novelty of the paper by asking about the differences between the current paper and the Venom paper, specifically regarding the V:N:M concept. However, it does not provide any explicit guidance or suggestions on how the authors should address this concern. The comment implies that the authors should clarify the differences, but it does not offer concrete steps or details on how to do so. The action is implicit and vague, leaving the authors uncertain about how to respond effectively. Therefore, this comment is barely actionable.",2,"The review comment raises a concern about the novelty of the paper by questioning the differences between the current work and the Venom paper. It specifically asks whether the V:N:M concept was already proposed in the Venom paper and seeks clarification on how the current work differs from it. While the comment identifies a potential issue with the originality of the paper, it does not provide any constructive feedback or suggestions on how the authors might address this concern or differentiate their work. Without actionable advice or specific guidance, the comment offers limited value to the authors in terms of improving their draft. Therefore, it aligns with a score of 2, indicating that the comment is barely helpful.",3,"The comment raises questions about the novelty of the paper by comparing it to the ""Venom paper"" and specifically mentions ""V:N:M,"" which allows the authors to identify the specific aspect of their work being questioned. However, it does not specify which sections or elements of the paper should be compared to the Venom paper, making it weakly grounded. The comment is specific in its inquiry about the differences between the two papers, but without explicit references to sections or content, the authors may struggle to pinpoint where to address these concerns. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.",X,"The review point consists of questions regarding the novelty of the paper in comparison to the Venom paper. It does not present an opinion or a claim requiring verification but rather seeks clarification. As such, it fits the ""No"" label, as it does not contain claims or subjective statements.",5779,,,,,,,,
ICLR_2023_4452,ICLR_2023,"Strengths: 1. The paper is well organized and structured. 2. The advantage and usefulness of the proposed approach is well justified, and demonstrated with experiments. Weaknesses: 1. Some details of the proposed approach are not mentioned or clarified. a) Which task is used for the test in Fig. 3? This should be noted in the caption. b) In Fig. 5, are the transferred LIP winning tickets obtained using single-IMP or multi-IMP? is the test image used in finding the transferred LIP winning tickets? c) The algorithms in the Appendices are not clarified. How is the mask mu' created (the sparsity of mu' is not updated in each iteration)? The objective function for training is not given. How is the j determined? Is the algorithm intended to only find the desired subnet or it also gives the final restored image? 2. Some statements in the paper are incorrect. In section 4, the authors say “Results presented in Fig. 4 show that multi-image IMP significantly can improve the quality of the LIPs in the cross-domain setting.” However, from Fig. 4, the multi-IMP shows essentially the same performance as others. 3. Supporting results are missing for some of the statements in the paper. To enable transferability across images, the authors propose a multi-IMP method to improve on the basic single-IMP method, and show results for the multi-IMP method in Fig. 4. However, the single-IMP might also have this transferability though not as strong. A comparison is necessary. 4. Some part of the paper might not be relevant to the main topic. The paper discusses the transferability of the LIP to high-level tasks like image classification. However, image classification requires training on massive dataset and is thus not relevant to the DIP (untrained networks) topic.","4. However, the single-IMP might also have this transferability though not as strong. A comparison is necessary.",3925,3,"The review point suggests that a comparison is necessary to evaluate the transferability of the single-IMP, implying that the authors should conduct such a comparison. However, it does not provide specific guidance on how to perform this comparison or what metrics or methods should be used. The action is inferred and lacks concrete details, making it somewhat actionable. The authors understand that a comparison is needed but are left without clear instructions on how to execute it effectively.",3,"The review comment identifies a potential area for improvement by suggesting that a comparison is necessary to evaluate the transferability of the single-IMP. This feedback is somewhat helpful as it highlights a gap in the analysis that the authors should address to strengthen their work. However, the comment lacks depth and does not provide detailed guidance or suggestions on how to effectively conduct this comparison or what specific aspects should be considered. To be more helpful, the comment could include specific aspects of the comparison that need attention or suggest methodologies for conducting the analysis. Therefore, the comment is rated as somewhat helpful, consistent with label 3.",2,"The comment suggests that a comparison involving the single-IMP's transferability is necessary. However, it does not specify which part of the paper this comparison should be included in, making it weakly grounded. The comment is somewhat specific as it indicates the need for a comparison regarding the transferability of the single-IMP, but it lacks details on what aspects should be compared or how this should be done. Therefore, the comment is weakly grounded and under-specific, aligning with a score of 2.",3,"The review point suggests that the single-IMP might possess transferability, albeit not as strong, and recommends a comparison. The claim is somewhat verifiable as it implies a potential characteristic of the single-IMP but lacks specific evidence or reasoning to support this assertion. The suggestion for comparison is logical, yet the absence of detailed justification or examples leaves the authors with significant effort to understand and address the point. Consequently, the comment is rated as ""3: Somewhat Verifiable.""",8931,,,,,,,,
8POQ904HEc,EMNLP_2023,"The main result of the work presented in the paper is the dataset, but there are insufficient information regarding it:- the process of obtaining questions is described, but what are the answers? are they developed in parallel?- what are the final statistics of the dataset? Section 3.2 provides details of data obtained based on Wikipedia, but no details about the data obtained from Geektime.- the authors argue for the diversity of the data gathered, but there is no quantification, and this is not difficult to provide. Even a shallow types vs. token statistic would be useful.- it seems like a good idea to choose negative spans from the wrongly predicted answers, but they may not necessarily be good for evaluating the string matching method. How about partly overlapping spans with the positives? Or other such spans that are more difficult negatives? The purpose of the new string match method is to deal correctly with morphological variations, including misleading ones. It would also be useful to have some error analysis for the TLNLS. What situations does it get wrong?- some discussion and error analysis on the new dataset would be useful. Contrasting this to ParaShoot would also be useful, to show the benefits of using the new dataset (e.g. examples of ParaShoot that the model trained on HeQ gets right, to illustrate what phenomena/situations the new dataset helps model better)- the ParaShoot dataset is frequently referenced but not described, and not well contrasted with the dataset presented here.","- the authors argue for the diversity of the data gathered, but there is no quantification, and this is not difficult to provide. Even a shallow types vs. token statistic would be useful.",1368,4,"The review comment points out that while the authors argue for the diversity of the data gathered, they do not provide any quantification of this diversity. The reviewer suggests that providing even a simple types vs. token statistic would be useful. This feedback implies a clear action for the authors to take: quantify the data diversity. The suggestion is specific and provides a concrete example of how to implement the action, making it mostly actionable. However, it does not explicitly instruct the authors to perform this action, which slightly reduces its actionability.",4,"The comment identifies a gap in the paper regarding the lack of quantification of data diversity, which the authors claim to have achieved. It suggests a specific and actionable improvement by recommending the inclusion of a types vs. token statistic to provide evidence for the diversity claim. This feedback is clear and provides a concrete step for the authors to enhance the credibility and robustness of their work. However, it could be more helpful if it included additional guidance on how to implement this suggestion or why it is important. Overall, the comment is mostly helpful as it offers a clear direction for improvement, aligning with a score of 4.",3,"The comment critiques the authors' claim about the diversity of the data gathered, pointing out the lack of quantification. It suggests that providing even a basic types vs. token statistic would be beneficial. However, the comment does not specify which part of the paper discusses the data diversity, making it difficult for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where data diversity is mentioned, the comment lacks full grounding. It is specific about the need for quantification but does not provide detailed guidance on how to implement it. Therefore, this comment is weakly grounded and specific, aligning with category 3.",3,"The review point claims that the authors argue for the diversity of the data gathered but fail to provide quantification. It suggests that even a basic types vs. token statistic would be useful. This claim is somewhat verifiable as it highlights a gap in the paper's argument regarding data diversity. However, it lacks specific examples or references to support the need for quantification, making it necessary for the authors to infer the full reasoning behind the suggestion. Therefore, the comment is categorized as 3: Somewhat Verifiable.",4153,,,,,,,,
YDuYWjbKDA,ICLR_2025,"1. The experimental section primarily compares TreeDQN with basic methods and lacks extensive benchmarking against a wider range of state-of-the-art approaches in combinatorial optimization, which could better illustrate its relative strengths.2. The method is tailored to similar MILP tasks, potentially limiting its generalizability to significantly different combinatorial optimization problems, which the paper acknowledges but does not address experimentally.","2. The method is tailored to similar MILP tasks, potentially limiting its generalizability to significantly different combinatorial optimization problems, which the paper acknowledges but does not address experimentally.",5464,1,"The review comment points out a limitation of the method, noting that it is tailored to similar MILP tasks and may not generalize well to different combinatorial optimization problems. While the paper acknowledges this limitation, the reviewer notes that it is not addressed experimentally. However, the comment does not provide explicit or implicit suggestions for how the authors might address this limitation, such as conducting additional experiments or proposing modifications to improve generalizability. As a result, the authors are left without actionable steps to take in response to this feedback. Therefore, the comment is rated as unactionable.",3,"The comment identifies a potential limitation of the method's generalizability to different combinatorial optimization problems, which the paper acknowledges but does not address experimentally. This feedback is somewhat helpful as it points out an area that could be explored further to strengthen the paper. However, it lacks specific suggestions or guidance on how the authors might address this limitation or what kind of experiments could be conducted to test the method's generalizability. While it provides a direction for improvement, the feedback could be more actionable and detailed to be fully helpful. Therefore, it is rated as somewhat helpful.",2,"The comment highlights a concern about the method's generalizability to different combinatorial optimization problems, which the paper acknowledges but does not address experimentally. However, it does not specify which part of the paper discusses this limitation or where the experimental evidence is lacking. While the authors might have some idea of where the generalizability is mentioned, the comment lacks specificity on what additional experiments or evidence are needed. Therefore, this comment is weakly grounded and not specific, aligning with the label 2.",3,"The review point claims that the method's generalizability is potentially limited to similar MILP tasks, as acknowledged by the paper, but not addressed experimentally. This claim is somewhat verifiable because it logically follows from the paper's acknowledgment of the limitation. However, it lacks specific examples or detailed reasoning about how this limitation affects the method's applicability to different problems, making it difficult for authors to fully understand and address the issue. Therefore, the comment is categorized as somewhat verifiable, justifying a score of 3.",7193,,,,,,,,
j9dDXNffBz,ICLR_2024,"1. Self-training is a prominent and effective approach within the field of unsupervised domain adaptation (UDA). However, the proposed method solely focuses on domain-alignment methods. It is recommended to explore another UDA method based on self-training.2. The paper exclusively addresses the negative transfer resulting from excessive alignment between two domains. However, it is worth noting that negative transfer can arise from various factors.3. The authors assert that the metric should be independent of the training method; however, it appears that the methods employed utilize training losses during the evaluation process.4. Given the remarkable achievements of large-language models (LLM) in transfer learning, I am curious about the performance of the proposed metric on other LLM methods, such as CLIP or Visual Prompt, in the context of domain adaptation or transfer learning.","1. Self-training is a prominent and effective approach within the field of unsupervised domain adaptation (UDA). However, the proposed method solely focuses on domain-alignment methods. It is recommended to explore another UDA method based on self-training.",4399,3,"The review point acknowledges the effectiveness of self-training in unsupervised domain adaptation (UDA) and suggests that the authors explore this approach, as their current method focuses solely on domain-alignment methods. The comment explicitly recommends exploring another UDA method based on self-training, providing a clear direction for the authors to consider. However, it does not offer specific guidance on how to integrate self-training into their work or which aspects of self-training to focus on. While the action is explicit, the lack of concrete details on implementation makes it somewhat actionable. Therefore, this comment is rated as a 3 on the actionability scale.",4,"The review comment identifies a limitation in the paper's focus, noting that it only considers domain-alignment methods within unsupervised domain adaptation (UDA) and suggests exploring self-training as an alternative approach. This feedback is clear and actionable, as it provides a specific direction for expanding the scope of the research. By considering self-training methods, the authors could potentially enhance the comprehensiveness and impact of their work. However, the comment could be more helpful if it included specific examples or references to self-training methods that could be explored, offering more detailed guidance. Overall, the comment is mostly helpful, as it effectively highlights an area for improvement and suggests a meaningful enhancement to the draft.",3,"The comment suggests exploring self-training as an alternative method for unsupervised domain adaptation (UDA), contrasting it with the current focus on domain-alignment methods. However, it does not specify which part of the paper discusses the proposed method, making it weakly grounded. The suggestion to explore self-training is specific, as it provides a clear direction for potential improvement. Therefore, the comment is weakly grounded but specific, aligning with category 3.",3,"The review point makes a claim that the proposed method focuses only on domain-alignment methods and suggests exploring self-training as an alternative UDA method. The comment is somewhat verifiable as it identifies a gap in the approach by contrasting it with another prominent method in the field. However, it lacks specific examples or references to support the claim that self-training is effective or to illustrate how it could be integrated into the current work. This requires the authors to make a significant effort to understand and address the suggestion, thus justifying the label 3: Somewhat Verifiable.",5112,,,,,,,,
1djnGJnaiy,ICLR_2024,"1. The paper is unclear and difficult to follow.1. ""Dimension reduction"" step of the voxel-Mixer should learn time-window specific spatial components in $W_{\mbox{segment}}^{(s)}$ since logically the voxels needs to be grouped into ROIs. However $\tilde{X}^{(t)}W^{(s)}_{\mbox{segment}}$ seems to be reducing time, not space. Notation is fairly mixed up making it difficult to follow what has been done exactly.2. ""Functional connectivity encoder"" starts with a connectivity graph $\cal{G}_{F}$ where does that graph come from? This section is also unclear - the source of patches and how they are mixed is very difficult to discern.3. Evaluation tasks cite evaluation in classifying ADHD and ASD but results are not reported in Table 1.4. Experiments are not clearly explained. Specifically, how the test and train sets were split?2. The contribution to either the ML or Neuroimaging is unclear1. For ML: it appears the paper is proposing to train a model on the functional connectivity graph and another model with a comparable number of parameters on time series. The number of parameters relative to the BNT used in the benchmark roughly doubles since the Temporal Graph Mixer in this paper is doing about what BNT is doing. This parameter doubling not surprisingly leads to improved performance. This is not explained.2. For ML: since clasification accuracy is of a major concern, why not compare to Logistic Regression on the FNC data as a much simpler model with much fewer parameters?3. For Neuroimaging: The paper does not show what are the learned voxel groupings. Note, all voxel-level analysis papers with which the current submission contrast the work obtain biologically interpretable voxel maps. The current model supposedly should capture spatial maps like that since it is operating at the voxel level input from subjects. However, these maps are neither presented nor discussed.4. For Neuroimaging: What is the significance of Table 2? This problem is almost a toy problem on a synthetic task. This task is barely relevant to the neuroimaging field and the cited workshop is the only paper that uses this synthetic data generation approach for testing the method.5. For Neuroimaging: Why the ROIs in Figure 3 are so carefully delineated? The described methods would be unlikely to produce such precisely carved spatial maps. This result is unclear.3. Some relatively minor problems:1. Abstract: ""toward **revealing the understanding**"" what do you exactly mean?2. Abstract: ""we bridge this gap"", which gap? This is strangely phrased3. The first paragraph of the introduction section is awkwardly written and can gain a lot from a rewrite.4. Page 2: Limitations state that most studies focus on either voxel level or functional connectivity. However, to study functional connectivity researchers usually first extract intrinsic networks from the voxel level, by running ICA, RBM, NMF, Dictionary Learning or any other decomposition. The statement does not seem to hold.5. Page 3 lats paragraph of section 2: MEG and EEG do not readily provide localization of the time-series to parts of the brain. The sensor space recordings need to be projected to the brain volume or surface by overcoming the difficulties of an ill-posed inverse problem.6. Note, that a model that has the characteristics you describe for BrainMixer on page 3 is available:- Mahmood U, Fu Z, Calhoun V, Plis S. [Glacier: Glass-Box Transformer for Interpretable Dynamic Neuroimaging](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10231935/). In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023 Jun 4 (pp. 1-5). IEEE.- and a paper from 2022 by the same author7. On page 4 Notation: $\tilde{T}$ is not defined.8. On page 4 Notation: is $K$ the same as $|\cal{V}|$? It would be simpler to follow the paper if the notation was consistent9. Page 5: ""even after 2 **minuetes**""10. Appendix is missing","2. ""Functional connectivity encoder"" starts with a connectivity graph $\cal{G}_{F}$ where does that graph come from? This section is also unclear - the source of patches and how they are mixed is very difficult to discern.",4855,2,"The review comment highlights a lack of clarity in the section discussing the ""Functional connectivity encoder,"" specifically questioning the origin of the connectivity graph $\cal{G}_{F}$ and the process of mixing patches. While it identifies areas of confusion, it does not provide explicit guidance on how to clarify these points or suggest specific improvements. The authors can infer that they need to provide more detailed explanations regarding the source of the graph and the mixing process, but the comment lacks concrete suggestions on how to achieve this. Therefore, the action is implicit and somewhat vague, making the comment barely actionable.",3,"The comment identifies a lack of clarity in the section discussing the ""Functional connectivity encoder,"" specifically questioning the origin of the connectivity graph $\cal{G}_{F}$ and the process of sourcing and mixing patches. While it highlights areas of confusion, it does not provide specific suggestions or guidance on how the authors might clarify these points. The feedback points out important areas for improvement but lacks actionable advice, making it somewhat helpful. The authors gain insight into where their explanation may be lacking, but they are left without clear steps to address the issues. Therefore, it aligns with a score of 3.",5,"The comment is fully grounded as it explicitly mentions the section related to the ""Functional connectivity encoder"" and the connectivity graph $\cal{G}_{F}$. This allows the authors to accurately identify the part of the paper being addressed. The comment is also specific because it points out the lack of clarity regarding the origin of the connectivity graph and the source and mixing of patches. This provides clear guidance on what needs to be clarified or addressed in the paper. Therefore, the comment is fully grounded and specific, corresponding to label 5.",X,"The comment primarily consists of inquiries and observations regarding the clarity of the section on the ""Functional connectivity encoder."" It questions the origin of the connectivity graph and the process of mixing patches, indicating a lack of clarity. However, it does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of ""No.""",6331,,,,,,,,
daEqXJ0yZo,ICLR_2024,"1. How to differentiate between the content and style of motion is not a trivial work. Usually, these two content is interleaved. The proposed approach seems to implicitly to decouple these two factors. Is it possible to provide a validation of the proposed approach can decouple these two content?2. For the experiments, the metrics may not well validate the performance of the proposed approach. For example, the style accuracy is based on a style classifier, which may be very accurate to capture the styleness of the proposed algorithm. Also, for case with a styled motion input, the transfer result based on the stylized input may not be easily verified.3. In the attribute edting literature, scholars are actively involved in the style edting, for example, to interpolate two style codes. Is it possible to generalize the generative ability of the proposed approach for more editing operation based on the latent space.4. How about the performance of the proposed approach on the long motion sequence? Is it possible to maintain the style for the whole motion sequence rather than a short period.","2. For the experiments, the metrics may not well validate the performance of the proposed approach. For example, the style accuracy is based on a style classifier, which may be very accurate to capture the styleness of the proposed algorithm. Also, for case with a styled motion input, the transfer result based on the stylized input may not be easily verified.",5053,3,"The comment raises concerns about the validity of the metrics used in the experiments, specifically questioning the accuracy of the style classifier and the verification of transfer results based on stylized inputs. However, it does not provide explicit guidance on how the authors should address these concerns or suggest alternative metrics or validation methods. The feedback identifies potential issues but lacks concrete instructions on how to resolve them, leaving the authors with a general understanding of the problem but without specific actions to take. Therefore, the comment is somewhat actionable.",3,"The comment raises concerns about the validity of the metrics used in the experiments, specifically questioning the reliability of the style accuracy metric and the verification of transfer results based on stylized inputs. While it identifies potential weaknesses in the evaluation approach, it lacks specific suggestions or guidance on how to address these issues or what alternative metrics might be more appropriate. This limits the comment's utility for the authors, as it provides a direction for improvement but lacks depth and actionable advice. Therefore, the comment is somewhat helpful, aligning with a score of 3.",3,"The comment addresses concerns about the metrics used in the experiments, specifically mentioning the style accuracy and the challenges in verifying the transfer result based on stylized input. While it does not explicitly mention a specific section, the authors can infer that it relates to the experimental results or methodology section. The comment is specific in detailing the issues with the style accuracy metric and the verification of the transfer result, providing clear guidance on what needs to be addressed. However, since it does not explicitly mention the section, it is weakly grounded but specific. Therefore, this comment aligns with category 3.",2,"The review point raises concerns about the validity of the metrics used in the experiments, specifically questioning the accuracy of the style classifier and the verifiability of the transfer results based on stylized input. However, the comment lacks specific examples, detailed reasoning, or references to substantiate these claims. Without such supporting evidence, the authors may find it challenging to understand the basis of the critique and address it effectively. Therefore, the comment is considered borderline verifiable, as it provides an initial rationale but lacks comprehensive support.",6546,,,,,,,,
ICLR_2022_2933,ICLR_2022,"1. The main claim mismatches the research problem. This paper claims to propose dataset compression method and motivates their method from related dataset compression (distillation) methods. However, the work they actually do is (single) image compression, which is another research topic. The proposed method is not designed for reducing the training set size (i.e. dataset compression). Instead, the authors compress the single images individually. Unfortunately, they are not clear about what they are doing. 2. Chaotic compression measurement. We know that images are saved with specific compression algorithms/standards (like JPEG) in computers. It is neither suitable nor fair to compare the matrix/tensor element numbers of the original image and that “compressed” by this method. A fair comparison can be the storage size in a hard disk. 3. Unclear short/long-range correlation information. The paper claims that they “compress datasets by filtering long-range correlation information” and “retain short-range correlation information”. However, no correspondence between short/long-range correlation and image pixels/textures is given. It is unknown whether they really removed the “long-range correlation information”. 4. Unconvincing experiments. No repeated results, no standard deviation. As stated in the paper, “Finally, we report results on testing datasets with the best model on evaluation datasets.” In addition, the results in Sec 5.2 are unconvincing. It is unacceptable to use a pre-trained resnet and then fine-tune it on 75% size of training data of ImageNet and get only 52.10% testing accuracy. 5. No ablation study. No comparison to methods in the past decade.",5. No ablation study. No comparison to methods in the past decade.,2511,3,"The review point highlights two issues: the absence of an ablation study and the lack of comparison to methods from the past decade. While it clearly identifies areas that need attention, it does not provide explicit guidance on how to address these issues. The authors are left to infer that they should conduct an ablation study and include comparisons with recent methods, but the comment does not specify which methods to compare or how to structure the ablation study. The action is implicit and somewhat vague, making it somewhat actionable.",3,"The review point highlights two significant weaknesses in the paper: the absence of an ablation study and the lack of comparison to methods from the past decade. These are critical aspects that could enhance the paper's rigor and contextual relevance. However, the comment does not provide specific guidance or suggestions on how to conduct the ablation study or which methods should be included in the comparison. While it identifies important areas for improvement, the feedback lacks actionable details that would empower the authors to make specific enhancements to their draft. Therefore, it is somewhat helpful, as it points out key deficiencies but does not fully guide the authors on how to address them.",3,"The comment highlights the absence of an ablation study and a comparison to methods from the past decade. However, it does not specify which sections of the paper should include these elements or provide details on what specific comparisons or analyses are needed. While the authors might infer that this feedback pertains to the results or discussion sections, the comment lacks explicit grounding. It is specific in terms of what is missing (ablation study and comparisons) but does not guide the authors on how to address these issues. Therefore, this comment is weakly grounded and specific, aligning with category 3.",1,"The review point makes two claims: the absence of an ablation study and the lack of comparison to methods from the past decade. However, it does not provide any supporting evidence or reasoning to justify why these omissions are significant or how they impact the paper's contributions. Without specific examples or references to relevant methods from the past decade, the claims remain unsupported and unverifiable. As a result, the feedback is categorized as 1: Unverifiable, as it lacks the necessary detail to help authors understand and address the concerns raised.",9467,,,,,,,,
pA8oI8a00l,ICLR_2025,"1. Section 3.1 only introduces the setting of the adversary and does not cover the setting of the defender.2. Section 3.1 states that ""the attacker can poison pre-trained MCL models by fine-tuning with carefully crafted dirty datasets,"" which is a strong assumption. To my knowledge, the first article [1] in this direction implemented a data poisoning setting without a separate fine-tuning phase on the poisoned data.3. There is an overlap between the poisoning dataset and the fine-tuning dataset, which creates a simpler setting. When fine-tuning is performed on the overlapping data, the ""backdoor connection"" is naturally disrupted, even without any improvements being made.4. The first paragraph of section 3.3 contains a **repeated paragraph**.5. Where does the first term on the right side of Equation 4 come from? It hasn't been introduced before.6. The second term on the right side of Equation 4 has not been introduced either.7. How is the augmented data mentioned in the article being used? The introduction in section 3.3 is quite vague.8. Due to the overlap between the poisoned data and the fine-tuning data in the experimental setup, there is a risk of data leakage.[1] Carlini, N., & Terzis, A. (2021, October 6). Poisoning and Backdooring Contrastive Learning. International Conference on Learning Representations.",4. The first paragraph of section 3.3 contains a **repeated paragraph**.,9019,5,"The review comment explicitly points out that the first paragraph of section 3.3 contains a repeated paragraph. This is a clear and direct action for the authors to take, as they need to remove the repetition to improve the draft. The comment is specific and provides a concrete action, making it highly actionable.",5,"The review point identifies a specific and easily correctable issue in the manuscript: a repeated paragraph in the first paragraph of section 3.3. This feedback is clear and actionable, allowing the authors to quickly address the redundancy and improve the clarity and professionalism of their draft. The precision of the comment makes it highly helpful, as it directly points out an error that can be rectified to enhance the quality of the paper. Therefore, it deserves a score of 5, indicating it is highly helpful.",5,"The comment is fully grounded as it explicitly mentions ""the first paragraph of section 3.3,"" allowing the authors to accurately identify the specific part of the paper being addressed. It is also specific because it clearly identifies the issue of a ""repeated paragraph,"" providing precise guidance on what needs to be corrected. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.",X,"The comment identifies a factual issue in the paper, specifically that there is a repeated paragraph in section 3.3. This is a straightforward observation and does not contain any subjective claims, opinions, or suggestions that require justification or evidence. Therefore, it fits the criteria for a normal statement.",9136,,,,,,,,
rg0vQmkB7F,ICLR_2024,"Despite the significance of the claim, I have several major concerns:- (Novelty) Technically, it’s hard for me to find novelty, especially regarding LIMA (Zhou et a., 2023) paper, when it comes to “fine-tuning with a small number of data”. In other words, the LIMA paper shows that LLMs can be aligned with small but carefully curated instruction data. In this sense, the shadow alignment method can be regarded as one application of the LIMA approach. To find out its novel points, is the shadow alignment data in this paper outstanding data to fast and/or effectively make the LLMs deviate from the previous safe alignment? How can you conclude from the existing or supplementary results? Or you can argue why the proposed method is a technically sound and novel approach.- (Annotators and IRB) Even though there are only two annotators who participated in this work for human evaluation, this work is required to be approved by IRB, because the evaluation response contains harmful content. If approved, please mention it in the Ethical Statement section.- How the two annotators are recruited? If they are one of the authors, then they might have caused data bias. Moreover, two annotators for data evaluation are too small to validate the quality. Particularly, deciding safe or unsafe is sometimes subjective, leading to requiring more annotators. Generally, three annotators evaluate each sentence, and their inter-annotator agreements are also reported together.- Clarification- In the introduction, “The emphasis on attacking safety-aligned models arises from two main factors: 1) the foundation base models lack sufficient dialogue capabilities for following instructions, whereas aligned models possess instruction-following ability and thus can be misused more readily. 2) It’s necessary to demonstrate that existing safety protocols are inadequate for managing the potential risks.” → These sentences are, for me, hard to understand the intent and implication of authors.- On page 3,- “The temperature is set to 0.7, max tokens=1024, n=5.” → what is “n”?- “This gives us 5∗5∗50∗13 = 16250 questions” → how this number was determined?- “Then, to increase the data diversity, we perform clustering by each scenario to filter out the most similar questions and only pick *the most diverse questions*.” → The most “diverse” is vague.- On page 4, I have no idea about the following sentences:- We evaluate the refusal ratio of both the original chat model and the attacked model on 200 sensitive questions relevant to ’Illegal activity’, …. We perform decoding 3 times and average the results.- In Table 3, the definition of violation rate \gamma was not found. If I missed, please let me know.- In Figure 3, what is “MaliciousInstruction” dataset?- In table 5, please elaborate on the table caption. The numbers are significantly different?- In Figure 3, Table 3, and Figure 5, which evaluation method used between automatic and human evaluation?- Interpretation of experimental results- In Figure 3, why the shadow-aligned model could not generate unsafe response to the prompts in PhysicalSafetyUnsafe dataset? Is this because the eval dataset is out-of-domain from the Shadow Alignment Dataset?","- In Figure 3, Table 3, and Figure 5, which evaluation method used between automatic and human evaluation?",10951,4,"The review comment asks a direct question about which evaluation method was used between automatic and human evaluation in Figure 3, Table 3, and Figure 5. This question implies that the authors should clarify the evaluation method used in these sections. The action is explicit, as it directly instructs the authors to provide this clarification. However, it does not specify how the authors should present this information, making it somewhat vague. Despite this, the comment is mostly actionable because it clearly identifies the need for clarification.",2,"The review comment raises a question about the evaluation method used in Figure 3, Table 3, and Figure 5, specifically asking whether automatic or human evaluation was employed. While this question highlights a potential area of ambiguity in the paper, it does not provide any actionable feedback or suggestions for improvement. The authors are made aware of a possible oversight but are not guided on how to address it or improve their draft. The comment lacks depth and specificity, making it barely helpful in assisting the authors to enhance their work. Therefore, it aligns with a score of 2.",5,"The comment is fully grounded as it explicitly mentions specific figures and a table (Figure 3, Table 3, and Figure 5), allowing the authors to accurately identify the parts of the paper being addressed. It also specifies the issue by questioning which evaluation method was used between automatic and human evaluation. This provides clear guidance on what needs to be clarified in the paper. Therefore, this comment is fully grounded and specific, aligning with category 5.",X,"The comment is a question requesting clarification about the evaluation method used in specific figures and tables. It does not express an opinion, judgment, or suggestion that requires verification. It is purely factual, fitting the ""No"" label.",1568,,,,,,,,
u9gI4JlOSj,EMNLP_2023,"1. The paper lacks of novelty in terms of methodology. After such a detailed analysis, a better method based on the findings is encouraged.2. Some statements in this paper lack detail and analysis.- In Line 123, ""Methods that **implicitly** increase model parameters perform better using the same T5 initialization."". What does the ""implicitly"" refer to, and what will the corresponding ""explicitly"" refer to?- In Line 140, ""We find this counter-intuitive to the common conception of generative retrieval being limited by model **capacity**"". What does the model capability here refer to, and why is the capability of a larger model (11B parameters) low?3. The experiments are not very convincing.- Only one metric is compared on each dataset. Additionally, on large-scale corpora, the ""@10"" metric may be overly strict, and the author should consider adding metrics based on ""@100"".- Some related work (i.e., substring-based docids) has been omitted, which is also a major research branch in the field of generative retrieval, e.g., SEAL (Bevilacqua et al., 2022). I think a comparative analysis and discussion should also be carried out.4. In Tables, some abbreviations are used without explanation of their meanings, e.g., At., D2Q. A clear explanation is suggested to provide in summary.",2. Some statements in this paper lack detail and analysis.,8664,2,"The review point indicates that certain statements in the paper lack detail and analysis, suggesting that the authors should provide more information and examination. However, it does not specify which statements are lacking or what kind of detail and analysis are needed. This leaves the authors with an implicit action that is vague, as they must infer which parts of the paper require enhancement and how to address them. Therefore, the comment is barely actionable.",2,"The review comment points out a general issue with the paper, noting that some statements lack detail and analysis. However, it does not specify which statements are problematic or provide any guidance on how the authors might address this issue. Without specific examples or actionable suggestions, the feedback is too vague to be of significant help to the authors. As a result, the comment is barely helpful, as it identifies a potential area for improvement but fails to offer the necessary detail or direction to assist the authors in enhancing their draft.",1,"The comment indicates that some statements in the paper lack detail and analysis, but it does not specify which statements these are or where they are located in the paper. This makes it difficult for the authors to identify the specific parts of the paper that need improvement. Additionally, the comment does not provide specific guidance on what kind of detail or analysis is missing. Therefore, the comment is not grounded and not specific, aligning with a score of 1.",1,"The review point claims that ""some statements in this paper lack detail and analysis,"" but it does not provide any specific examples or reasoning to support this claim. Without identifying which statements are lacking or explaining why they need more detail and analysis, the comment is not verifiable. The authors would struggle to understand which parts of their paper need improvement based on this feedback. Therefore, the comment is considered unverifiable.",6936,,,,,,,,
X5rO5VyTgB,ICLR_2025,"- Lack of baselines: Although UnKE has demonstrated quite good results, the baselines used for comparison are not the most advanced methods in the field. It would be more convincing if UnKE could be compared with state-of-the-art methods such as WISE, GRACE, and FT-M, and show better results. In particular, UnKE should against with non-term-driven optimization method GRACE for a more fairly comparison.- Contributions of UnKEBench: First, before UnKEBench, there have already been works on long-form knowledge editing (e.g., LEME[1]), but this is not discussed in the paper. Second, UnKEBench is generated by ChatGPT and lacks steps to ensure data quality, such as human verification.- Description inconsistent with reality:- The paper hypothesizes that unstructured knowledge is distributed across multiple layers of the language model, but the experiment only edits one layer.- The paper claims that UnKE's cause-driven optimization can preserve pre-trained knowledge during the editing process. However, in the experiments, UnKE's ability to retain original knowledge is worse than the baseline (i.e., MMLU in Table 2 and Src-Acc and Tgt-Acc in Table 3).[1] Rosati D, Gonzales R, Chen J, et al. Long-form evaluation of model editing[J]. arXiv preprint arXiv:2402.09394, 2024.","- The paper claims that UnKE's cause-driven optimization can preserve pre-trained knowledge during the editing process. However, in the experiments, UnKE's ability to retain original knowledge is worse than the baseline (i.e., MMLU in Table 2 and Src-Acc and Tgt-Acc in Table 3). [1] Rosati D, Gonzales R, Chen J, et al. Long-form evaluation of model editing[J]. arXiv preprint arXiv:2402.09394, 2024.",5494,3,"The review comment highlights a discrepancy between the paper's claims and the experimental results, specifically noting that UnKE's ability to retain original knowledge is worse than the baseline. It references specific metrics (MMLU in Table 2 and Src-Acc and Tgt-Acc in Table 3) to support this observation. However, the comment does not provide explicit guidance on how the authors should address this issue or improve the results. While it points out an area of concern, it lacks concrete suggestions or actions for the authors to take, leaving them uncertain about how to respond to the feedback. Therefore, this comment is somewhat actionable.",4,"The review comment identifies a discrepancy between the paper's claims and the experimental results. It specifically points out that UnKE's ability to retain original knowledge is worse than the baseline, as evidenced by the results in Table 2 and Table 3. By referencing specific tables and providing a citation for further reading, the comment offers clear and actionable feedback. This helps the authors understand where their claims may not align with their results, prompting them to either adjust their claims or investigate the reasons behind the observed performance. However, the comment could be more helpful if it suggested potential reasons for the discrepancy or ways to address it. Despite this, the feedback is mostly helpful as it directs the authors' attention to a critical issue in their work.",5,"The comment is fully grounded as it explicitly mentions specific tables, ""Table 2"" and ""Table 3,"" allowing the authors to accurately identify the parts of the paper being addressed. It also specifies the issue by pointing out that UnKE's ability to retain original knowledge is worse than the baseline in the experiments, providing a clear critique of the results presented. Additionally, it references an external work, [1] Rosati et al., which could be relevant for further context or comparison. Therefore, this comment is categorized as fully grounded and specific, aligning with category 5.",5,"The review point critiques the claim that UnKE's cause-driven optimization preserves pre-trained knowledge, providing specific evidence from the experiments where UnKE underperforms compared to the baseline. The reviewer references specific metrics (MMLU, Src-Acc, and Tgt-Acc) from Tables 2 and 3 and cites an external source, Rosati et al. (2024), to substantiate the claim. This detailed evidence and reference make the comment fully verifiable, as it provides a clear and robust foundation for the critique.",3240,,,,,,,,
NIPS_2018_849,NIPS_2018,"Weakness: - Some training details are missing. It may be better to provide more details in order for others to reproduce the results. In particular, a. K-Means is applied to initialize the centers. However, in the beginning of training stage, the feature maps are essentially random values. Or, is K-Means applied to a pretrained model? b. KL divergence between p(Q) and p is included in the loss. May be interesting to see the final learned p(Q) compared to p. c. It is not clear to the reviewer how one could backpropagte to x, w, and \sigma in Equation (2). Also, will the training be sensitive to \sigma? - Is it possible to visualize how the graph representations evolve during training? - What is the extra cost in terms of computation resources and speed when employing the GCU? - 4 GCUs with (2, 4, 8, 32) vertices are used for semantic segmentation. It may be informative to discuss the improvement of each added GCU (e.g., which one is more helpful for segmentation). - It may be also informative to provide the learned graph representations for object detection and instance segmentation. - The proposed model attains similar performance as non-local neural network. It may be informative to provide some more discussion about this (e.g., extra computational cost for each method.). - In line 121, what does \sigma_k = sigmoid(\sigma_k) mean? \sigma_k appears on both sides. In short, the reviewer thinks the proposed method is very interesting since it is able to capture the long-range context information for several computer vision tasks. It will be more informative if the authors could elaborate on the issues listed above.","- In line 121, what does \sigma_k = sigmoid(\sigma_k) mean? \sigma_k appears on both sides. In short, the reviewer thinks the proposed method is very interesting since it is able to capture the long-range context information for several computer vision tasks. It will be more informative if the authors could elaborate on the issues listed above.",6448,5,"The review point raises a question about the meaning of the expression ""\(\sigma_k = \text{sigmoid}(\sigma_k)\)"" in line 121, pointing out a potential confusion with the notation. It also acknowledges the interest in the proposed method's ability to capture long-range context information for computer vision tasks. The reviewer explicitly requests that the authors elaborate on the issues mentioned, providing a clear and direct action for the authors to take. The comment is specific in identifying the area that needs clarification, making it highly actionable.",3,"The review comment identifies a specific point of confusion in the manuscript, namely the expression ""\sigma_k = sigmoid(\sigma_k)"" in line 121, which appears to be unclear or potentially incorrect. The reviewer expresses interest in the proposed method's ability to capture long-range context information for computer vision tasks, indicating that the method has potential. However, the comment lacks detailed guidance or suggestions on how to address the confusion or improve the explanation. While it highlights an area for clarification, it does not provide actionable feedback or specific advice on how to resolve the issue, making it somewhat helpful. Therefore, the comment aligns with a score of 3.",5,"The comment is fully grounded as it explicitly mentions ""line 121,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue with the equation ""\sigma_k = sigmoid(\sigma_k),"" questioning the appearance of \sigma_k on both sides. This provides clear guidance on what needs to be clarified. Additionally, the comment appreciates the proposed method and suggests elaborating on the issues mentioned, which adds specificity to the feedback. Therefore, this comment is fully grounded and specific, aligning with category 5.",1,"The review point raises a question about the meaning of the expression ""\sigma_k = sigmoid(\sigma_k)"" in line 121, indicating a potential issue with clarity or notation. However, the comment does not provide any supporting evidence or reasoning to substantiate why this is problematic or how it affects the understanding of the proposed method. The reviewer also expresses a positive opinion about the method's ability to capture long-range context information but does not provide specific examples or references to support this claim. Overall, the comment lacks sufficient detail and justification, making it unverifiable.",1650,,,,,,,,
ARR_2022_104_review,ARR_2022,"1. it utilizes quantization technology but does not compare with the other quantization approaches.2. there is still a softmax computation left in the presented approach.3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant.4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication.5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers).6. the maximum performance loss (- 0.78 BLEU) might be a significant loss.It's recommended to report the overall energy reduction of the full model, conduct experiments on more language pairs and with the big setting on a few of them.","5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers).",762,2,"The review point raises a question about the performance of the approach in more challenging settings, such as Transformer Big and deep Transformers, compared to the base setting used in the experiments. However, it does not explicitly instruct the authors to conduct additional experiments or provide guidance on how to address this curiosity. The action is implicit and vague, leaving the authors uncertain about what specific steps to take to improve their draft. Therefore, this comment is barely actionable.",3,"The review comment raises a question about the generalizability of the approach by asking whether it can perform well in more challenging settings, such as Transformer Big and deep Transformers. While it identifies a potential area for further exploration, the comment lacks specificity and actionable guidance on how the authors might address this concern. It does not provide suggestions for additional experiments or analyses that could be conducted to test the approach in these settings. As a result, the feedback is somewhat helpful, as it prompts the authors to consider broader applicability but does not offer detailed advice on how to proceed. Therefore, it aligns with a score of 3.",3,"The comment refers to experiments performed on three language pairs with a base setting and questions the performance of the approach in more challenging settings, such as Transformer Big and deep Transformers. However, it does not specify which part of the paper discusses these experiments, making it weakly grounded. The comment does specify the concern about the approach's performance in different settings, adding some level of specificity. Therefore, the comment is weakly grounded and specific, aligning with category 3.",X,"The comment primarily consists of a question regarding the performance of the approach in more challenging settings. It does not present any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification ""No"" is appropriate.",9566,,,,,,,,
gDcL7cgZBt,ICLR_2025,"1. Sections 3 and 4 feel like they should be the main contributions of the paper, but they squeezed into about a single page. For example, precise mathematical definitions of the how the quantum states in Eq (4) are missing from the main text, which makes it hard to follow.2. The work as a whole considers expressivity of a variational quantum circuit as a problem of the ansatz and does not give any weight to the importance of feature encoding into the quantum circuit. It is known that the spectral representation of a QNN is directly given by the feature encodings ([Schuld, 2021](https://arxiv.org/abs/2008.08605)).3. The diamond norm under parameter change seems like an interesting metric, but it remains unclear what can be done with it.4. Similarly, the bound on the channel distinguishability under small perturbations seems not to be useful. It seems natural that the QNNs will only change gradually under small parameter perturbations such as those encountered during training, so what use is an upper bound?5. The main points of the work seem somewhat incongruous: the criticism of the 2-design as an expressivity metric seems like it should give way to an alternative expressivity metric. However, the channel distinguishability metric proposed does not provide an expressivity metric.","1. Sections 3 and 4 feel like they should be the main contributions of the paper, but they squeezed into about a single page. For example, precise mathematical definitions of the how the quantum states in Eq (4) are missing from the main text, which makes it hard to follow.",9024,3,"The review comment suggests that Sections 3 and 4, which appear to be the main contributions of the paper, are not given enough space and detail, particularly noting the absence of precise mathematical definitions for the quantum states in Eq (4). The reviewer implies that these sections should be expanded and clarified to make the content easier to follow. While the comment identifies specific areas that need improvement, it does not provide explicit instructions on how to expand these sections or what specific details to include. The action is implicit and somewhat vague, as the authors know they need to provide more detail but may not be entirely sure of the exact steps to take. Therefore, the comment is somewhat actionable.",4,"The review comment identifies a structural issue with the paper, noting that Sections 3 and 4, which seem to be the main contributions, are condensed into a single page. It highlights a specific problem with the lack of precise mathematical definitions for the quantum states in Equation (4), which affects the readability and comprehension of the paper. This feedback is clear and actionable, as it directs the authors to expand these sections and provide the necessary mathematical details to improve the clarity and impact of their contributions. However, the comment could be more helpful if it offered additional guidance on how to effectively expand these sections or suggested specific areas that require more detail. Overall, the feedback is mostly helpful as it points out a critical area for improvement, but it could be more comprehensive with further suggestions. Therefore, it aligns with a score of 4.",5,"The comment is fully grounded as it explicitly mentions ""Sections 3 and 4"" and ""Eq (4),"" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it details the issue of missing precise mathematical definitions of quantum states in the main text, which makes it hard to follow. This provides clear guidance on what needs to be addressed to improve the paper. Therefore, this comment is categorized as fully grounded and specific, aligning with category 5.",3,"The review point claims that Sections 3 and 4, which should be the main contributions of the paper, are inadequately covered, being condensed into about a single page. It further supports this claim by pointing out the absence of precise mathematical definitions of quantum states in Eq (4), making it difficult to follow. While the comment highlights a specific issue with the presentation and provides an example, it lacks detailed reasoning or references to substantiate why this is a significant problem. Therefore, the claim is somewhat verifiable, as it provides a starting point for improvement but requires more detailed evidence or examples to fully validate the argument.",5109,,,,,,,,
ICLR_2023_3307,ICLR_2023,"Weakness:• The authors argue that B-cos Transformer performs at least as well as the baseline ViTs under the same number of backbone layers in Section 5.1. However, the experimental setting is unfair without comparing the computational complexity to the baseline methods, especially when the authors replace the conventional tokenization module with a pre-trained B-cos DenseNet-121. Though deriving more explainable models, the proposed model should be toned down if they are too heavy to deploy. This is the same for Fig. 6, which also lacks comparisons of model complexity vs. Localisation/perturbation.• I am also confused about where the good performance of B-cos Transformer comes from. The authors claim that the positional information design largely improves performance. However, the effectiveness of the positional information cannot be concluded from Fig. 5, with many other components modified. To fully explain the performance gain, comparisons have to be done by attaching the B-cos positional information to the conventional ViTs.• I am also curious about how the third column of Fig. 7 is generated, as the visualizations do not seem to be normalized.","• I am also curious about how the third column of Fig. 7 is generated, as the visualizations do not seem to be normalized.",3903,2,"The comment expresses curiosity about how the third column of Figure 7 is generated, noting that the visualizations do not seem to be normalized. While it raises a question about the methodology, it does not provide explicit guidance or suggest specific actions for the authors to take. The authors might infer that they need to clarify the generation process or address the normalization issue, but the comment lacks concrete instructions on how to do so. Therefore, the action is implicit and vague, making this comment barely actionable.",2,"The comment expresses curiosity about the generation of the third column in Figure 7, noting that the visualizations do not appear to be normalized. While it raises a question about a specific aspect of the figure, it does not provide any guidance or suggestions for improvement. The comment lacks depth and actionable feedback, leaving the authors with only a vague sense of what might need clarification or adjustment. As a result, the comment is barely helpful, as it identifies a potential issue but does not assist the authors in addressing it. Therefore, it aligns with a score of 2.",5,"The comment is fully grounded as it explicitly mentions ""the third column of Fig. 7,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue by questioning how the visualizations are generated and noting that they do not seem to be normalized. This provides clear guidance on what aspect of the figure needs clarification or improvement. Therefore, the comment is fully grounded and specific, aligning with category 5.",X,"The comment consists of a question expressing curiosity about the generation of the third column of Fig. 7 and mentions that the visualizations do not seem to be normalized. It does not present an opinion, judgment, or suggestion requiring verification. It is purely factual and seeks clarification, making ""No"" the correct label.",9907,,,,,,,,
NIPS_2020_826,NIPS_2020,"- My major concern is that this paper misses important comparisons with the uniform channel scaling baseline. As shown in [1] that the final architecture plays a huge role in the effectiveness of channel/filter pruning, I think pruning methods should compare to a naive way of model scaling (width-multiplier). More specifically, width-multiplier uniformly increases or decreases the channel counts across all layers with the same proportion (e.g., 50% of the original channel counts). Such a method is very simple (near zero computational cost) and can match any desired FLOPs requirement. If after all the pruning efforts, one still fail to compare with uniform channel scaling, what is the point? To perform a fair comparison, I'd suggest training such a baseline using the epoch number of training+fine-tuning, which is 248 for ResNet on ImageNet, 512 for MobileNetv2 on ImageNet, and 400 for CIFAR according to this paper's setup. - I think [7] is also very relevant to this work that is not cited. [7] proposed a regularizer that improves upon L1 to avoid shrinkage for all parameters, which is of the same motivation for this work. I think it is necessary to compare with this work. - Another concern is regarding hyperparameter tuning. While this paper makes a good argument regarding the property of polarization regularizer compared to the L1 regularizer, it is has two hyperparameters and make pruning laborious. Most notably, since both hyperparameters affect FLOPs and accuracy, there should be multiple solutions that hit the same FLOPs, which makes the tuning process hard as one cannot navigate on the simple objective (FLOPs) to decide. According to current presentation, it is not clear how the grid search is done and I'd appreciate detailed discussions regarding this issue for reproducibility. - The empirical results only focus on pruning less than ~50% FLOPs away. It seems to me that going lower might not favor the proposed method due to the huge regularization imposed in the training loss. This seems to be a problem for L1 regularizer as well. However, there are many methods for filter pruning and it is not clear why one wants to go for regularization-based methods. It would be great if the paper provides a wider spectrum of FLOPs (this at least can be done by comparing the proposed method to uniform pruning and L1 pruning). - Since I can find better pruning results (available prior to the submission deadline) [2-6], I do not think it is appropriate to claim state-of-the-art. I agree the results of this paper look good, but without proper comparisons with these work (taking into account the differences in training settings), it seems to me the presented results are ""comparable"" to the current state-of-the-art. Also, it is informative to discuss these competitive methods [2-6] in the paper. [1] Liu, Zhuang, et al. ""Rethinking the value of network pruning."" ICLR 2019. [2] Guo, Shaopeng, et al. ""DMCP: Differentiable Markov Channel Pruning for Neural Networks."" arXiv preprint arXiv:2005.03354 (2020). [3] Chin, Ting-Wu, et al. ""Towards Efficient Model Compression via Learned Global Ranking."" arXiv preprint arXiv:1904.12368 (2019). [4] You, Zhonghui, et al. ""Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks."" Advances in Neural Information Processing Systems. 2019. [5] Dong, Xuanyi, and Yi Yang. ""Network pruning via transformable architecture search."" Advances in Neural Information Processing Systems. 2019. [6] Yu, Jiahui, and Thomas Huang. ""AutoSlim: Towards One-Shot Architecture Search for Channel Numbers."" arXiv preprint arXiv:1903.11728 (2019). [7] Yang, Huanrui, Wei Wen, and Hai Li. ""Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures."" ICLR 2020.","- Another concern is regarding hyperparameter tuning. While this paper makes a good argument regarding the property of polarization regularizer compared to the L1 regularizer, it is has two hyperparameters and make pruning laborious. Most notably, since both hyperparameters affect FLOPs and accuracy, there should be multiple solutions that hit the same FLOPs, which makes the tuning process hard as one cannot navigate on the simple objective (FLOPs) to decide. According to current presentation, it is not clear how the grid search is done and I'd appreciate detailed discussions regarding this issue for reproducibility.",7814,5,"The review comment raises concerns about hyperparameter tuning, specifically regarding the polarization regularizer compared to the L1 regularizer. It points out that the presence of two hyperparameters makes pruning laborious and suggests that multiple solutions could achieve the same FLOPs, complicating the tuning process. The reviewer explicitly requests detailed discussions on how the grid search is conducted for reproducibility. This feedback provides a clear and concrete action for the authors to take, as it specifies the need for a detailed explanation of the grid search process. Therefore, the comment is highly actionable.",4,"The review comment is mostly helpful as it identifies a specific concern regarding hyperparameter tuning in the context of the polarization regularizer compared to the L1 regularizer. It highlights the complexity introduced by having two hyperparameters, which can make the pruning process laborious. The comment also points out the challenge of multiple solutions affecting FLOPs and accuracy, complicating the tuning process. Additionally, it requests a detailed discussion on how the grid search is conducted, emphasizing the importance of reproducibility. This feedback is actionable as it guides the authors to provide more clarity and detail on their hyperparameter tuning process, which can significantly improve the transparency and reproducibility of their work. However, the comment could be more helpful if it suggested specific ways to address the tuning complexity or provided examples of best practices in hyperparameter tuning. Overall, it provides valuable insights and direction for the authors to enhance their draft.",3,"The comment addresses concerns about hyperparameter tuning, specifically in relation to the polarization regularizer compared to the L1 regularizer. It highlights the complexity introduced by having two hyperparameters and the challenges in the tuning process due to their impact on FLOPs and accuracy. The comment suggests that multiple solutions could achieve the same FLOPs, complicating the tuning process. It also requests detailed discussions on how the grid search is conducted for reproducibility. While the comment does not explicitly mention a specific section of the paper, it provides enough context for the authors to infer the relevant part, likely the methodology or results section. The comment is specific in detailing what needs to be addressed regarding hyperparameter tuning and the need for detailed discussions. Therefore, this comment is weakly grounded but specific, aligning with a score of 3.",3,"The review point raises a concern about hyperparameter tuning, specifically regarding the polarization regularizer compared to the L1 regularizer. It highlights the complexity introduced by having two hyperparameters that affect both FLOPs and accuracy, making the tuning process challenging. The reviewer suggests that multiple solutions could achieve the same FLOPs, complicating the decision-making process. However, the comment lacks specific examples or references to support the claim, and it does not provide detailed reasoning or evidence about the grid search process. This makes the claim somewhat verifiable, as it points out a potential issue but does not fully substantiate it with detailed information or references. Therefore, the comment is classified as 3: Somewhat Verifiable.",866,,,,,,,,
ICLR_2023_385,ICLR_2023,"- The statement ""These methods rely on maximizing the similarity between..."" in the Abstract is inaccurate. Not all self-supervised visual representation learning methods are based on such a contrastive learning scheme. Instead, most of the early methods are based on pretext tasks. Suggest revising the corresponding claims and also the title.- The main idea of this work is based on the assumption of the self-supervised visual learning method being contrastive-based ones. Whereas there are quite a few non-contrastive learning approaches. As a result, the main contributions of this work may be a bit limited.- It is unclear how the defined u_\psi was guaranteed to represent the transformation t. From the description, t was encoded by another set of layers, which are also learnable, making the equivariance here (esp. the ""equivariant predictor"") a bit misleading. The encoding of the transformations (Sec. 2.3.2) is rather confusing. In the beginning, it looks like the authors used one-hot encoding, but later some of the augmentations directly use the coordinates or real values (i.e. 1, 2, 3). The generalization for the augmentation set, as a result, is another issue. What if new augmentations were included? How would they be encoded?- It is unclear why the final model still needs the conventional invariance loss (i.e. the first term in the final loss function), if the proposed new equivariance-based loss is as claimed to be effective. Jointly optimizing both invariance and equivariance also seems to be confusing. How does the model actually learn in this case?- It is unclear why the Barlow Twins was only evaluated on the small-scale dataset CIFAR10.- The quality of the learned representation was only evaluated on the linear evaluation setting, which is a bit insufficient to get a clear conclusion. There are quite a few other downstream tasks that could be used as reported in the literature (e.g. fine-tuning, detection, and segmentation to name a few).- The proposed method was motivated by the case where ""augmentation information is useful"" as claimed by the authors, and example cases are flowers and birds (as claimed in the Introduction). But this was not validated in the experiment. There are some fine-grained datasets for such categories (e.g. the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset) that should have been used to validate the claims.- It is unclear why not use an objective similar to Eq. (5) or (6) to constrain the equivariance model training, but instead only maximize the similarity between z_i' and \hat{z}_i'?- Would an absolute value makes more sense for Eq (5)? Otherwise, what does a negative value mean, as the second case ""H-flip"" shown in Fig. 3?- Line below Eq. (5), ""Fig. 4"" should be ""Fig. 3"".","- The quality of the learned representation was only evaluated on the linear evaluation setting, which is a bit insufficient to get a clear conclusion. There are quite a few other downstream tasks that could be used as reported in the literature (e.g. fine-tuning, detection, and segmentation to name a few).",3790,4,"The review point suggests that evaluating the quality of the learned representation solely on the linear evaluation setting is insufficient for drawing clear conclusions. It implies that the authors should consider additional downstream tasks, such as fine-tuning, detection, and segmentation, as reported in the literature. While the comment provides examples of other tasks that could be used for evaluation, it does not explicitly instruct the authors to perform these additional evaluations. The action is implicit but somewhat concrete, as the authors can infer that they should expand their evaluation to include these tasks. Therefore, the comment is mostly actionable.",4,"The review comment identifies a limitation in the evaluation of the learned representation, noting that it was only assessed using the linear evaluation setting. It suggests that this is insufficient for drawing clear conclusions and recommends considering additional downstream tasks, such as fine-tuning, detection, and segmentation, as reported in the literature. This feedback is mostly helpful as it provides a clear and actionable suggestion for broadening the evaluation scope, which could enhance the robustness and applicability of the findings. However, the comment could be more helpful if it included specific examples or references to guide the authors in implementing these additional evaluations. Overall, it effectively highlights an area for improvement and offers a constructive direction for enhancing the paper's evaluation methodology.",3,"The comment addresses the evaluation of the learned representation, specifically mentioning that it was only assessed using the linear evaluation setting. This provides some grounding, as the authors can infer that the comment pertains to the evaluation section of the paper. However, it does not explicitly mention a specific section or figure, making it weakly grounded. The comment is specific in suggesting that additional downstream tasks, such as fine-tuning, detection, and segmentation, could be used for a more comprehensive evaluation. Therefore, the comment is weakly grounded but specific, aligning with label 3.",4,"The review point suggests that evaluating the quality of the learned representation solely on the linear evaluation setting is insufficient. It supports this claim by mentioning that other downstream tasks, such as fine-tuning, detection, and segmentation, are commonly used in the literature for a more comprehensive evaluation. However, the comment does not provide specific references or examples of studies that have used these additional tasks, which would strengthen the justification. As a result, the claim is mostly verifiable, as it is based on logical reasoning and common practices in the field, but lacks detailed evidence or references. Therefore, the comment aligns with a label of 4: Mostly Verifiable.",3466,,,,,,,,
EPfGHb9Y68,ICLR_2024,"######## Weaknesses ########- Section 2 (preliminaries) is largely unclear and not stand-alone- Section 3 (approach) is also not sufficiently clear or precise, and seems to introduce both math and text that are not related to the submission- The experimental evaluation is not sufficient to assess the benefits of the proposed method######## Recommendation ########Unfortunately, I recommend that this manuscript is not accepted in its current form. The main concern I have is the limited experimental evaluation. The authors propose an approach that is a combination of existing ideas (which is fine!), but unfortunately there is not sufficient evidence to support the choice of this combination. Moreover, the draft (especially sections 2 and 3) would need to undergo major revisions to make the text clear and (most importantly) precise.######## Arguments ########- Experimental setting -- The place with the most room for improvement in this submission is the empirical evaluation. In particular, I have three main concerns:- The evaluation uses very simple MuJoCo tasks to evaluate methods. The authors should evaluate their approach on much more complex tasks, such as those from the Meta-World [1], CausalWorld [2], RLBench [3], or CompoSuite [4,5]. This is particularly true because the authors' motivation stems from the need for diffusion models to represent complex behaviors.- The continual setting uses sequences of only 4 tasks. Given that the curves in Figure 2 for CuGRO get progressively worse as more tasks are trained, it would be important to study how this detriment scales with the number of tasks. Can we expect CuGRO to handle a really long stream of tasks? We need empirical evidence to answer that question- The evaluation considers no external baselines. There are a number of existing continual learning and offline RL methods (which the authors themselves cite in their manuscript). Yet the evaluation is limited to variations of the authors' proposed method (oracle, noise, and none). Would other continual learning methods be as effective? Would other offline RL methods work well in this setting (especially considering that the tasks are quite simple).- Clarity of preliminaries- The description of advantage-weighted regression misses a key piece: the Q function is w.r.t. \mu, which is the piece that makes Eq. 4 solve the _costrained_ optimization of Eq. 2. This should be explicitly stated and explained, as otherwise the reader might think Q_\theta is the standard Q^\pi --- this is what I thought initially when reading 2.1- It's unclear if Eq. 5 corresponds to Eq. 3. I believe that it does, and if so, my understanding is that Q_\theta in Eq. 5 is not the same as Q_\theta in Eq. 4---the former is w.r.t. \pi and the latter is w.r.t. \mu. Please clarify.- The description of the diffusion models in Sec 2.2 misses mentioning the existence of a forward diffusion process that adds noise to an (observed) action. This makes it difficult to parse the last sentence before Eq. 6 where the diffusion model is predicting some un-defined noise.- The description prior to Eq. 7 is still unclear to me: is Q estimating the value of the actions w.r.t. \mu or \pi? The authors mention that the actions are sampled from \mu, but is the long-term value measured for \pi or \mu? It seems that the middle part of Eq. 7 entails that it's from \pi, but this isn't stated in text.- What does re-sampling mean toward the end of Sec 2.1? What was the first ""sampling""?- Clarity/precision of approach- Sec 3.1- The description of CORL is mostly clear, but I do have a couple of questions:- What aspects of the MDP are allowed to change from task to task? If all of them, then which ones do the authors consider in their approach/evaluation?- Is the distribution over MDPs P(M) stationary or is it allowed to change over time? Is there any implication of that for the learning process or evaluation setting in the experiments?- Sec 3.2- While I agree that diffusion models (or generally expressive generative models) are useful for expressing RL policies, the authors seem to conflate two things in their description of why that is the case: 1) some tasks require multimodal behaviors (like in footnote 1) and 2) the overall behavior expressed by the diffusion model should capture a breadth of tasks. The latter is hinted at in both the intro and here, but never actually explained or exemplified.- The argument of footnote 2 is somewhat weak. What if two visual classification tasks are ""detect if dog is in image"" and ""detect if cat is in image"" and they're both given the same image of a dog? Tasks would require opposite predictions given the same image, just like the RL model would. Plus, the conclusion would be that diffusion models are better because they could capture both the opposite actions, but how is that useful if they can't differentiate when to execute each? The only way to solve a problem like this is to let the model take as input a task indicator (or something to differentiate the tasks), and it's unclear that a Gaussian conditioned on this information would fail.- This section is very odd. The first paragraph is all motivation and no technical details. Then second paragraph contains some details about how the diffusion model for state generation works (the equivalent for actions was in the perliminaries). Then the final paragraph is all about doing classifier or classifier-free guidance, but it's unclear what for or why 6 lines of this show up in the middle of a technical section when the authors don't even actually try it. I guess the idea is that the classifier-based/free guidance could ensure that the generated states actually correspond to the conditioning task?- Sec 3.3- ""the desired importance of the new task..."" seems to suggest that Eq. 8 should be a weighted sum. But the authors more likely mean that there needs to be some weighting to ensure that the current task is learned sufficiently well while avoiding forgetting. It isn't really about the importance but about being able to optimize properly.- It's unclear why there's a test loss (Eq. 11 and 14) for specific models. Isn't performance measured as the obtained reward of the agent on the tasks?- It's also unclear what the authors mean by ""reconstruct the cumulative state space"". Doesn't the task conditioning imply that the agent is learning separate state spaces, one for each task?- Are the generated states for behavior replay drawn after updating the state generator? Before? Or are they both updated together? How was this choice made and what are the implications of it? My intuition would be that it's better to first train the behavior model on the fixed state generator and then train the state generator, because updates to the behavior model have no effect on the state generator but the converse is not true.- This seems to be clarified in Algorithm 1, but the authors should state it explicitly in text and not rely exclusively on the Appendix to transmit that point.- Sec 3.4- It's quite unclear after reading this section why the authors use the term behavior cloning, which has a very specific connotation in the context of off-line RL -- replicating the behavior that generates the data.- Instead, I think this approach is better described as a form of _functional regularization_, which is a method broadly studied in supervised continual learning research.[1] Yu et al., ""Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning."" CoRL, 2020[2] Ahmed et al., ""CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning."" ICLR, 2021[3] James et al., ""Rlbench: The robot learning benchmark & learning environment."" RAL, 2020[4] Mendez et al., ""CompoSuite: A Compositional reinforcement learning benchmark."" CoLLAs, 2022[5] Hussing et al., ""Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning."" arXiv, 2023","- Clarity of preliminaries - The description of advantage-weighted regression misses a key piece: the Q function is w.r.t. \mu, which is the piece that makes Eq. 4 solve the _costrained_ optimization of Eq.",10414,3,"The review comment highlights a missing key piece in the description of advantage-weighted regression, specifically noting that the Q function is with respect to \mu. This detail is crucial for understanding how Equation 4 solves the constrained optimization problem. However, the comment does not provide explicit guidance on how the authors should incorporate this missing information into their draft. While the action is implied and somewhat clear, the lack of specific instructions on how to address the issue makes it less actionable. Therefore, the comment is somewhat actionable.",4,"The review comment identifies a specific area where the clarity of the preliminaries could be improved, particularly in the description of advantage-weighted regression. It points out that the Q function is with respect to \mu, which is crucial for understanding how Eq. 4 solves the constrained optimization problem. This feedback is clear and actionable, as it highlights a missing piece of information that is essential for the authors to address in order to improve the clarity and completeness of their explanation. However, the comment could be more helpful if it provided suggestions on how to integrate this information into the existing text. Overall, the comment is mostly helpful, as it directs the authors toward a meaningful enhancement of their draft.",3,"The comment addresses the clarity of the preliminaries, specifically mentioning the description of advantage-weighted regression. It highlights a missing key piece regarding the Q function with respect to \mu, which is crucial for understanding the constrained optimization of Eq. 4. However, the comment does not explicitly mention where in the paper this issue is discussed, making it weakly grounded. The authors might infer that it relates to the section discussing advantage-weighted regression, but this inference is not direct. The comment is specific in identifying what is missing in the description, making it specific. Therefore, the comment is weakly grounded and specific, aligning with category 3.",3,"The review point claims that the description of advantage-weighted regression is missing a key piece, specifically that the Q function is with respect to \(\mu\), which is crucial for solving the constrained optimization of Eq. 4. This claim is somewhat verifiable as it highlights a specific missing element in the explanation, providing a logical basis for the critique. However, the comment lacks additional context or references that could further substantiate the importance of this missing piece. Without these, the authors may need to make a significant effort to fully understand and address the issue. Therefore, the feedback is categorized as 3: Somewhat Verifiable.",1001,,,,,,,,
ICLR_2022_2131,ICLR_2022,"in section 3.3, the authors proved the invertibility and concluded that the proposed method is manifold intrusion-free.The proof of invertibility assumes the graph edge’s weight must be 0 or 1. Can the proposed method be used for the graph with real-value edges [0,1]?When using a graph with real-value edge weights, does the proposed method still satisfy the invertibility?3)The proof also requires the lambda not to be 0.5. But in standard mixup, the mixed sample can be generated in the case of lambda being 0.5. Thus, the assumption in this proof may not be correct. 4)The most important question in this part is how the invertibility can induce “manifold intrusion free”? The connection between them is unclear. Assume your mixed sample can recover two original interpolation graphs, how to ensure that there is no conflict between the mixed sample’s representation and other training samples’ ones. Please explain this connection.As you used 5- and 8-layer GNNs in experiments, why not consider shallower GNNs? As seen in Figure 5, baselines show better performance when using a shallower structure.it is unclear what do you want to prove in section 4.3.1 as there are no connections to the proposed method.there are several existing related papers that all use mixup for graph node-classification tasks. As your proposed method can be used for mixing two graphs, it can also mix one graph with itself. Besides, as in your title, it should be a general mixup method for graphs, right? For these reasons, it will be good to also do the evaluation on node-classification tasks. Then SOTA graph mixup method should be [1]. I would like to see a comparison between the proposed method and baseline [1]. ([2] should be a weak baseline as it does not work significantly better than the basic baseline GNN, see Table3.)[1] 2021AAAI GraphMix: Improved Training of GNNs for Semi-Supervised Learning [2]2021 WWW MixupGrpah: Mixup for node and graph classification","3)The proof also requires the lambda not to be 0.5. But in standard mixup, the mixed sample can be generated in the case of lambda being 0.5. Thus, the assumption in this proof may not be correct.",2862,2,"The review point identifies a potential issue with the proof, specifically concerning the assumption that lambda cannot be 0.5, which contradicts the standard mixup scenario where lambda can indeed be 0.5. However, the comment does not provide explicit guidance on how the authors should address this issue or suggest any specific actions to rectify the assumption in the proof. The action is implicit, as the authors can infer that they need to revisit and possibly revise the proof, but it remains vague without concrete steps or suggestions. Therefore, this comment is barely actionable.",3,"The comment identifies a potential flaw in the proof related to the assumption that lambda cannot be 0.5, which contradicts the standard mixup practice where lambda can indeed be 0.5. This is a critical observation that could impact the validity of the proof, and it is important for the authors to address this inconsistency. However, the comment does not provide specific suggestions on how to rectify the assumption or offer alternative approaches to strengthen the proof. While it highlights an important issue, the feedback could be more actionable and detailed to be fully helpful. Therefore, it is rated as somewhat helpful, as it provides insight but not comprehensive guidance for improvement.",5,"The comment is fully grounded as it explicitly refers to a proof and the condition regarding lambda not being 0.5, allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it points out a potential issue with the assumption in the proof, particularly in the context of standard mixup where lambda can be 0.5. This provides clear guidance on what needs to be addressed or reconsidered in the paper. Therefore, the comment is fully grounded and specific, corresponding to label 5.",3,"The review point claims that the proof requires lambda not to be 0.5, which contradicts the standard mixup where lambda can be 0.5. This suggests that the assumption in the proof may be incorrect. The comment provides a logical reasoning by pointing out a specific inconsistency between the proof's assumption and the standard practice in mixup. However, it lacks additional references or detailed explanations to fully substantiate the claim. Therefore, the comment is somewhat verifiable, as it provides a basis for the claim but requires further elaboration or evidence for full verification. Hence, it is rated as 3: Somewhat Verifiable.",7802,,,,,,,,
NIPS_2022_1785,NIPS_2022,"The paper displays a collection of different results and interpretations but few seems actually necessary to explain the method. More preciselyI find the connection to causality weak overall. In this light any centered variable can be thought of as an instance of a causal power, this does not necessarily mean there is any causal estimation behind.The content of section 3 is mostly motivated by the synthetic example. I find this problematic as I believe the synthetic example is a very contrived one. While one motivation for DAE is the credit assignment problem (l22-23) this environment is specifically designed so there is no credit assignment.Baselines: while in practice DAE ends up being very close to a n-step return method, the only comparison point in the paper is with GAE. A minimum I would need to recommend this paper for acceptance would be a fair comparison with its 1. n-step return ablation (by removing the effect of A in eq 17 if I understood correctly). Then I think, to show the interest of directly estimating the advantage, another baseline could be to replace your directly learned A byQ - V where Q and V are learned like usualA = Q - V learned with a duelling architecture.would show if your estimator which incoporates the advatange is superior to the tradional TD oneand 3) would be useful to show whether learning the advantage directly is useful. Indeed while you make a case for the advantage being a quantity of interest, I find there is little evidence in the paper for why learning it directly with your method is important compared to the methods 2) and 3).Minor points:eq 4: maybe rather than the approximate symbol it would be more precise and exact to say that it is a sample?As you do not seem to be Vtrace or the Impala algorithm, I think using ""IMPALA"" just to mean the architecture can be misleading.Some limitations are addressed with Fig 3. However I think the authors need to make a stronger case for their method:First, a comparison with a more similar algorithm (n-step returns) would be better than GAE, as it would showcase that incorporating the advantage in the TD error is importantThen they would need to show why estimating directly the advantage as they do is useful/important/interesting. This could be tested experimentally by comparing with other methods for estimating the advantage.","3) would be useful to show whether learning the advantage directly is useful. Indeed while you make a case for the advantage being a quantity of interest, I find there is little evidence in the paper for why learning it directly with your method is important compared to the methods",8568,3,"The review comment suggests that it would be useful to demonstrate whether learning the advantage directly is beneficial. While it highlights a potential area for improvement, it does not provide explicit guidance on how to address this issue or what specific evidence should be included. The comment implies that the authors should provide more evidence or analysis, but it lacks concrete steps or suggestions on how to do so. As a result, the action is implicit and somewhat vague, making the comment somewhat actionable.",3,"The comment suggests that the paper could benefit from demonstrating the usefulness of directly learning the advantage, as the current evidence for its importance is insufficient. This feedback highlights a potential gap in the paper's argumentation and suggests an area for further exploration. However, the comment lacks specific guidance on how the authors might address this issue or what kind of evidence would be compelling. While it points out a potential area for improvement, it does not provide actionable advice, making it somewhat helpful but incomplete. Therefore, the comment aligns with a score of 3.",3,"The comment suggests that it would be useful to demonstrate whether learning the advantage directly is beneficial. It critiques the paper for lacking evidence on why learning the advantage directly with the proposed method is important compared to other methods. However, the comment does not specify which part of the paper this issue is discussed in, making it weakly grounded. While it does provide some specificity by identifying the need for evidence regarding the advantage, it lacks detailed guidance on how to address this concern. Therefore, the comment is weakly grounded and specific, aligning with category 3.",3,"The review point suggests that it would be useful to demonstrate the importance of directly learning the advantage, as the paper currently lacks evidence supporting this claim. While the comment identifies a potential gap in the paper's argument, it does not provide specific examples or references to substantiate why learning the advantage directly is significant. This makes the claim somewhat verifiable, as the authors would need to further develop the rationale themselves to fully understand and address the issue. Therefore, the comment is rated as ""3: Somewhat Verifiable.""",7787,,,,,,,,
NIPS_2020_195,NIPS_2020,"My main concerns are: - Feeding the actual pose of one arm (master) and the relative pose of the second arm (slave) with respect to the master and similarly other objects would have been more informative for the network to capture the relational dependencies at the pose level. A baseline comparison with this method would be useful to understand the dependency structure, especially to improve the performance for the second task. Adding other baselines with state-of-the-art methods in the related work would further improve the understanding of the work. - The authors discuss a few examples with the position in table tasks, but the effect of orientation is not explained. Does the approach generalize with randomly sampled orientation of the target object ? Are the orientations normalized to unit quaternions after prediction ? The authors are encouraged to show orientation errors and quantify the performance. - Adding visual pixel information from pixels would help to establish the true merits of graph attention mechanism. - 29 percent accuracy with table assembly tasks is rather low. The Euclidean distance error units in Table 1 seem very high. Are they normalized to per datapoint position errors ? If so, an error of 5 cm with HDR-IL seems unreasonably high. - It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate. - Experiments with real setup would have been useful to establish the merits of the proposed approach.","- 29 percent accuracy with table assembly tasks is rather low. The Euclidean distance error units in Table 1 seem very high. Are they normalized to per datapoint position errors ? If so, an error of 5 cm with HDR-IL seems unreasonably high.",7639,3,"The review comment highlights concerns about the reported accuracy and error units in the table assembly tasks. It questions whether the Euclidean distance error units in Table 1 are normalized to per datapoint position errors and suggests that an error of 5 cm with HDR-IL seems unreasonably high. While the comment raises important points, it does not provide explicit guidance or suggestions for improvement. The authors are left to infer that they should clarify the normalization of error units and potentially address the high error values. However, the lack of specific instructions or recommendations makes the action implicit and somewhat vague, resulting in a somewhat actionable comment.",3,"The review comment identifies specific concerns with the experimental results, particularly the low accuracy in table assembly tasks and the high Euclidean distance error units in Table 1. It questions whether these errors are normalized to per datapoint position errors and highlights that an error of 5 cm with HDR-IL seems unreasonably high. This feedback is somewhat helpful as it points out potential weaknesses in the results and prompts the authors to clarify or address these issues. However, the comment could be more helpful if it provided suggestions on how to improve the accuracy or reduce the error, or if it offered insights into why these results might be problematic. Overall, the comment provides some value by highlighting areas for further investigation, but it lacks actionable guidance for improvement. Therefore, it is rated as somewhat helpful.",5,"The comment is fully grounded as it explicitly mentions ""Table 1,"" allowing the authors to accurately identify the part of the paper being addressed. It is specific because it points out the low accuracy of 29 percent with table assembly tasks and questions the high Euclidean distance error units, asking whether they are normalized to per datapoint position errors. The comment further questions the reasonableness of a 5 cm error with HDR-IL, providing clear guidance on what needs to be clarified or addressed in the paper. Therefore, the comment is fully grounded and specific, corresponding to label 5.",1,"The review point raises concerns about the reported accuracy and error metrics, suggesting that they may be unreasonably high. However, the comment lacks specific reasoning or evidence to support why these values are considered low or high. It does not provide comparisons to other works or standards, nor does it offer detailed explanations or references that would help the authors understand the basis of the claim. As a result, the claim is unverifiable due to the absence of supporting evidence or detailed justification.",729,,,,,,,,
HEi70bBquo,ICLR_2025,"1. The source code is inaccessible via the anonymous link provided.2. The paper introduces numerous concepts without adequate explanation, which complicates comprehension:* The term ""GSN guidance"" is not clearly defined. Is it a concept coined by the authors? If ""GSN guidance"" refers to the optimization of latents, why isn't the proposal method considered a form of ""GSN guidance""? What is the distinction between ""Ours"" and ""Ours+""?* The phrase ""lacks inherent semantic meaning"" is used but not further elaborated upon.* It is unclear what the ""Similarity Score"" is intended to measure.3. The paper's novelty is questionable. The optimal iteration appears to be simply a result of the author's experimentation with various step iterations. But it has been widely accepted in research that detailed results are typically generated after the 8th step.","* The phrase ""lacks inherent semantic meaning"" is used but not further elaborated upon.",9631,1,"The review comment points out that the phrase ""lacks inherent semantic meaning"" is used in the draft but is not further elaborated upon. However, it does not provide any explicit guidance or suggestions on how the authors should address this issue. The comment lacks actionable details, such as recommending that the authors provide a definition or explanation of the phrase. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",3,"The comment identifies a specific phrase in the paper that is used without sufficient elaboration, suggesting a need for further explanation. This feedback highlights an area where the authors could improve clarity and depth in their writing. However, the comment does not provide guidance on how to elaborate on the phrase or why it is important, leaving the authors with only a general idea of what needs to be addressed. While it points out a potential weakness, it lacks actionable advice, making it somewhat helpful but incomplete. Therefore, the comment aligns with a score of 3.",1,"The comment highlights the use of the phrase ""lacks inherent semantic meaning"" without further elaboration. However, it does not specify where in the paper this phrase is used, making it difficult for the authors to pinpoint the exact location that needs attention. Additionally, while the comment identifies a lack of elaboration, it does not provide specific guidance on what kind of elaboration or additional information is needed. This lack of grounding and specificity makes it challenging for the authors to address the issue effectively. Therefore, the comment is not grounded and not specific, aligning with a score of 1.",X,"The comment points out that the phrase ""lacks inherent semantic meaning"" is used without further elaboration. This is a factual observation about the content of the paper and does not express a subjective claim or opinion. It does not require justification or evidence, making it a normal statement.",459,,,,,,,,
HexshmBu0P,ICLR_2024,"1. Copyright scenario is not clear. Is the copyright protection for model owner or for user who downloaded?2. Detecting generated contents is also not clear. Are the authors proposing method for detecting generated content? If so, where is the related experiments?3. Watermarking Stable Diffusion using Dreambooth has less novelty. The Dreamfusion itself is designed for training personalized concept to use it for Stable Diffusion's rich representation. In this sense, the authors change the personalized concept to watermark images.1. The manuscript could benefit from a clearer delineation of the copyright scenario. It would be helpful to specify whether the copyright protection mechanisms are designed to safeguard the interests of the model owner or the end-users who utilize the model.2. The section on detecting generated content could use further clarification. If that is the case, could you please direct me to the experiments that validate this approach?3. The approach to watermarking Stable Diffusion via Dreambooth may appear to have limited novelty since Dreamfusion is inherently capable of training personalized concepts for Stable Diffusion. It seems that this method lies in the adaptation of personalized concepts into watermark images.",1. The manuscript could benefit from a clearer delineation of the copyright scenario. It would be helpful to specify whether the copyright protection mechanisms are designed to safeguard the interests of the model owner or the end-users who utilize the model.,4805,3,"The review comment suggests that the manuscript should more clearly delineate the copyright scenario, specifically asking for clarification on whether the copyright protection mechanisms are intended to safeguard the interests of the model owner or the end-users. This feedback implies that the authors should provide a more detailed explanation of the copyright mechanisms in their manuscript. The action is explicit, as it directly states what needs to be clarified, but it lacks concrete guidance on how to implement this clarification. Therefore, while the authors know they need to address this issue, they may not be entirely sure of the exact steps to take. Given this, the comment is somewhat actionable.",4,"The review comment identifies a specific area for improvement by suggesting that the manuscript should clearly delineate the copyright scenario. It highlights the need to specify whether the copyright protection mechanisms are intended to safeguard the interests of the model owner or the end-users. This feedback is clear and actionable, as it provides a direct suggestion for enhancing the clarity and focus of the manuscript. By addressing this point, the authors could significantly improve the comprehensibility and relevance of their work. However, the comment could be more helpful if it offered additional guidance or examples on how to address this issue. Overall, the comment is mostly helpful, as it effectively directs the authors toward a meaningful enhancement of their draft.",3,"The comment suggests that the manuscript should clarify the copyright scenario, specifically whether the copyright protection mechanisms are intended to protect the model owner or the end-users. However, it does not specify which part of the manuscript this clarification should be added to, leaving the authors to infer that it relates to sections discussing copyright or legal aspects. The comment is specific in detailing what needs to be clarified regarding the copyright scenario. Nonetheless, since it does not explicitly mention a specific section, it is weakly grounded but specific. Therefore, this comment aligns with category 3.",1,"The review point suggests that the manuscript lacks clarity regarding the copyright scenario, specifically whether the protection mechanisms are intended for the model owner or the end-users. However, the comment does not provide any supporting evidence, examples, or references to justify this claim. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.",5360,,,,,,,,
HcY3fbVDqa,ICLR_2025,"The methodological component of the paper reads excessively like a pure application (without methodological novelty). First the model basically applies BiMamba twice without further modification. Second, a big emphasis of the proposed approach is its linear scaling, however, this is not discussed or analyzed in the methods, basically, because it translates directly from BiMamba.The first experiment (protein analysis) is underwhelming. First they show that increasing k does not improve the performance of neither of the approaches considering, thus defeating the need of a more expressive, but more expensive, model. Second, the model considered had 1M parameters vs. 3.5M in Notin et al. (2023b). Why not consider the larger model at least for NPSSM? Also, from Table 1, it doe not look like PNPT scales (in memory) much worse than NPSSM? One would expect the factor (~2) not to remain that similar for K=1000 and K=1500.The second experiment is not very convincing because the advantage of NPSSM over MSA Transformer is not clear. This because i) results (Tables 2 and 3) are only presented on a single dataset, ii) the variability and dependency of the main performance characteristics on hyperparameter choices is not clear (though explored via ablation), and iii) computational cost is briefly illustrated in Figure 5. However, is is not clear which context size is used for the main experiments in Tables 2 and 3 and why given the relationship between context size and performance (in Figure 2), the proposed model is not more much better than MSA Transformer in terms of r2.","2), the proposed model is not more much better than MSA Transformer in terms of r2.",5085,1,"The review comment states that the proposed model is not significantly better than the MSA Transformer in terms of r2. However, it does not provide any specific guidance or suggestions on how the authors could address this issue or improve their model's performance. The comment lacks actionable details, such as recommending ways to enhance the model or suggesting alternative approaches or metrics. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",2,"The comment points out that the proposed model does not significantly outperform the MSA Transformer in terms of the r2 metric. However, it lacks depth and does not provide any constructive feedback or suggestions on how the authors might address this issue or improve their model's performance. Without actionable advice or specific guidance, the comment offers limited value to the authors in terms of improving their draft. Therefore, it aligns with a score of 2, indicating that the comment is barely helpful.",1,"The comment lacks grounding as it does not specify which part of the paper discusses the comparison between the proposed model and the MSA Transformer in terms of r2. Without explicit references to sections, tables, or figures, the authors cannot confidently determine where this comparison is made. Additionally, the comment is not specific as it does not provide details on why the proposed model is not significantly better or what aspects of the comparison are lacking. This makes it challenging for the authors to address the feedback effectively. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.",1,"The review point claims that the proposed model is not significantly better than the MSA Transformer in terms of r2. However, the comment lacks any supporting evidence, such as specific data, comparisons, or references to substantiate this claim. Without these elements, the authors cannot verify the accuracy of the statement or understand the basis for the critique. Therefore, the comment is considered unverifiable.",6291,,,,,,,,
lrQlLqQase,ICLR_2024,- The reviewer find the paper very hard to read and understand.- Some of the assumptions are already well known in causality (“Causality necessitates time”. “ Cause happens before effect”).- The reliance of the proposed method on optimal value function corresponding to reward function seems a strong one.- will be interesting to investigate how the proposed method works as the function of number of variables.,- The reliance of the proposed method on optimal value function corresponding to reward function seems a strong one.,4761,1,"The review comment points out a concern regarding the reliance of the proposed method on the optimal value function corresponding to the reward function. However, it does not provide any specific guidance or suggestions on how the authors could address this issue or what changes they should consider. The comment lacks actionable details, such as recommending alternative approaches or providing examples of how to mitigate this reliance. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",2,"The comment points out a potential issue with the proposed method's reliance on the optimal value function corresponding to the reward function, suggesting that this reliance might be too strong. However, the comment lacks specificity and does not provide any actionable feedback or suggestions for improvement. It does not elaborate on why this reliance is problematic or how the authors might address or mitigate this issue. Without detailed guidance or constructive suggestions, the comment offers limited value to the authors in terms of improving their draft. Therefore, it aligns with a score of 2, indicating that the comment is barely helpful.",1,"The comment raises a concern about the reliance of the proposed method on the optimal value function corresponding to the reward function. However, it does not specify which part of the paper this concern pertains to, such as a particular section, figure, or table. Additionally, the comment lacks specificity in detailing what exactly is problematic about this reliance or how it could be addressed. Without clear grounding or specific guidance, the authors may struggle to identify the exact issue or how to improve it. Therefore, this comment is not grounded and not specific, aligning with a score of 1.",1,"The review point claims that the proposed method's reliance on the optimal value function corresponding to the reward function is a strong assumption. However, the comment does not provide any supporting evidence, reasoning, or examples to justify why this reliance is considered strong. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.",3484,,,,,,,,
BRoBig6ov1,ICLR_2024,"Although the analysis is about right, the results rely on a set of accurate slice-wise low-rank prior. That leads to major questions:1. How to efficiently obtain those priors? Please give examples. Based on what I read in the numerical experiment section. It seems the authors don't have a practical way to obtain the required prior {$U_{k_n}$}.2. The analysis doesn't show what could happen if the inaccurate low-rank prior was used. There is a chance of having even worse recovery results when some large noise is added to the prior.Therefore, I doubt the proposed $U_1$ norm is practically useful at all.The paper was not carefully written, many notations were used before being defined. For example,3. Page 3 under (1), $(I_{k_1}, I_{k_2})$ were not defined until page 4 Definition 1.4. Page 3 above (2), the full name of DCM were not introduced until page 7 Sec 4.1.5. Page 4 'By known result in Luet al. (2019b)', although I am familiar with the paper, the author should restate the quoted result for the completeness of the analysis.6. Page 7, 'For given $R$' -> 'For given rank parameter $R$'",1. How to efficiently obtain those priors? Please give examples. Based on what I read in the numerical experiment section. It seems the authors don't have a practical way to obtain the required prior {$U_{k_n}$}.,8800,3,"The review point raises a question about how to efficiently obtain certain priors and requests examples. It also suggests that the authors may not have a practical method for obtaining the required prior {$U_{k_n}$} based on the numerical experiment section. While the comment explicitly asks for examples, it does not provide specific guidance on what kind of examples or methods should be included. The action is explicit but somewhat vague, as it lacks detailed instructions on how to address the issue. Therefore, the comment is somewhat actionable.",3,"The review comment raises a pertinent question about the efficiency of obtaining priors, which is a critical aspect of the paper. It requests examples to clarify this point, indicating that the current explanation in the numerical experiment section is insufficient. This feedback is actionable as it guides the authors to provide practical examples or methods for obtaining the required priors, thereby enhancing the clarity and applicability of their work. However, the comment could be more helpful if it suggested specific types of examples or methods that would be beneficial. Overall, the comment is somewhat helpful, as it identifies a key area for improvement but lacks detailed guidance.",3,"The comment raises a question about obtaining priors and requests examples, which suggests a need for clarification in the numerical experiment section. However, it does not explicitly mention a specific part of the paper, leaving the authors to infer that it relates to the numerical experiment section. This makes the comment weakly grounded. The comment is specific in detailing what needs to be addressed—providing examples and practical methods for obtaining the required prior {$U_{k_n}$}. Therefore, this comment is weakly grounded and specific, aligning with category 3.",1,"The review point questions the efficiency of obtaining certain priors and suggests that the authors lack a practical method for obtaining the required prior {$U_{k_n}$}. However, the comment does not provide any supporting evidence, examples, or references to substantiate this claim. Without specific details or justification, the authors may find it challenging to understand and address the critique effectively. Therefore, the claim is considered unverifiable.",4796,,,,,,,,
NIPS_2020_476,NIPS_2020,"+ The main claim of this paper is that co-training between RGB and flow learns better representations than just using RGB. There is prior well-cited work that demonstrates this fact as well (CMC - Tian et al., 2019). The authors do not compare against this work. It would not be a serious issue, but the following two reasons make this quite serious: - The learning objective proposed in CMC is (in my opinion) simpler than the current work and it is a few line of code changes from the authors' formulation. In the spirit of good empirical understanding, it is valuable to see what simple baselines achieve. - The authors propose a positive mining framework (similar to [A]) which is more involved than a simple multi-view contrastive objective. This paper does not answer the question whether the additional complexity is warranted. To sum up, the proposed method is intuitive, the results are good, but it is not clear if the co-training, positive mining, and alternate optimization steps (main contributions of this paper over prior work) are necessary. [A] - Audio-Visual Instance Discrimination with Cross-Modal Agreement","- The authors propose a positive mining framework (similar to [A]) which is more involved than a simple multi-view contrastive objective. This paper does not answer the question whether the additional complexity is warranted. To sum up, the proposed method is intuitive, the results are good, but it is not clear if the co-training, positive mining, and alternate optimization steps (main contributions of this paper over prior work) are necessary. [A] - Audio-Visual Instance Discrimination with Cross-Modal Agreement",7411,2,"The review comment raises a concern about the necessity of the additional complexity introduced by the proposed positive mining framework, co-training, and alternate optimization steps. It suggests that the paper does not adequately justify whether these complexities are warranted compared to simpler methods. However, the comment does not provide explicit guidance on how the authors should address this issue or what specific analyses or comparisons should be conducted to demonstrate the necessity of these steps. The action is implicit and vague, as the authors are left to infer that they need to provide justification or evidence for the added complexity, but without clear instructions on how to do so. Therefore, this comment is barely actionable.",3,"The review comment raises a critical question about the necessity of the additional complexity introduced by the proposed positive mining framework compared to a simpler multi-view contrastive objective. It acknowledges the intuitiveness and good results of the proposed method but questions whether the main contributions—co-training, positive mining, and alternate optimization steps—are essential. While the comment identifies a potential area of concern, it lacks specific suggestions or guidance on how the authors might address this issue or demonstrate the necessity of their approach. The feedback prompts the authors to consider justifying their methodological choices more clearly, but without actionable steps, it remains somewhat helpful. Therefore, the comment aligns with a score of 3.",3,"The comment critiques the proposed positive mining framework, questioning whether the additional complexity is warranted compared to a simple multi-view contrastive objective. It mentions specific aspects of the method, such as co-training, positive mining, and alternate optimization steps, which are the main contributions over prior work. However, it does not specify which part of the paper should address this question, making it weakly grounded. The comment is specific in detailing the aspects of the method that are in question, but without explicit references to sections or experiments, the authors cannot confidently determine where to focus their revisions. Therefore, this comment is weakly grounded and specific, aligning with the label 3.",3,"The review point questions the necessity of the additional complexity introduced by the proposed positive mining framework, comparing it to a simpler multi-view contrastive objective. The reviewer references a similar work, [A], to highlight the complexity issue but does not provide specific evidence or detailed analysis to support the claim that the complexity may not be warranted. While the comment raises a valid point about evaluating the necessity of the proposed method's complexity, it lacks detailed justification or examples to fully substantiate the claim. Therefore, the comment is somewhat verifiable, as it provides a basis for the claim but requires further elaboration for full clarity. This aligns with a score of 3: Somewhat Verifiable.",2770,,,,,,,,
NIPS_2018_756,NIPS_2018,"It looks complicated to assess the practical impact of the paper. On the one hand, the thermodynamic limit and the Gaussianity assumption may be hard to check in practice and it is not straightforward to extrapolate what happens in the finite dimensional case. The idea of identifying the problem's phase transitions is conceptually clear but it is not explicitly specified in the paper how this can help the practitioner. The paper only compares the AMP approach to alternate least squares without mention, for example, positive results obtained in the spectral method literature. Finally, it is not easy to understand if the obtained results only regard the AMP method or generalize to any inference method. Questions: - Is the analysis restricted to the AMP inference? In other words, could a tensor that is hard to infer via AMP approach be easily identifiable by other methods (or the other way round)? - Are the easy-hard-impossible phases be related with conditions on the rank of the tensor? - In the introduction the authors mention the fact that tensor decomposition is in general harder in the symmetric than in the non-symmetric case. How is this connected with recent findings about the `nice' landscape of the objective function associated with the decomposition of symmetric (orthogonal) order-4 tensors [1]? - The Gaussian assumption looks crucial for the analysis and seems to be guaranteed in the limit r << N. Is this a typical situation in practice? Is always possible to compute the `effective' variance for non-gaussian outputs? Is there a finite-N expansion that characterize the departure from Gaussianity in the non-ideal case? - For the themodynamic limit to hold, should one require N_alpha / N = O(1) for all alpha? - Given an observed tensor, is it possible to determine the particular phase it belongs to? [1] Rong Ge and Tengyu Ma, 2017, On the Optimization Landscape of Tensor Decompositions",- The Gaussian assumption looks crucial for the analysis and seems to be guaranteed in the limit r << N. Is this a typical situation in practice? Is always possible to compute the `effective' variance for non-gaussian outputs? Is there a finite-N expansion that characterize the departure from Gaussianity in the non-ideal case?,6598,2,"The review point raises several questions about the Gaussian assumption and its implications for the analysis. However, it does not provide explicit actions or suggestions for the authors to address these questions. The authors are left to infer that they might need to discuss or clarify these aspects in their paper. The comment is implicit and lacks concrete guidance on how to address the issues raised, making it barely actionable.",3,"The review comment raises important questions about the Gaussian assumption in the analysis, questioning its applicability in practical situations and the possibility of computing the 'effective' variance for non-Gaussian outputs. It also inquires about a finite-N expansion to characterize deviations from Gaussianity. These questions highlight potential gaps in the analysis that the authors should address to strengthen their work. However, while the comment identifies relevant issues, it does not provide specific suggestions or guidance on how to address these questions or improve the analysis. This limits its helpfulness, making it somewhat helpful as it directs the authors' attention to important considerations but lacks actionable advice. Therefore, the comment aligns with a score of 3.",3,"The comment raises questions about the Gaussian assumption and its implications for the analysis, specifically asking whether the situation is typical in practice, if it's always possible to compute the 'effective' variance for non-Gaussian outputs, and if there's a finite-N expansion to characterize departures from Gaussianity. However, it does not specify which part of the paper these questions pertain to, making it weakly grounded. The questions themselves are specific, as they address particular aspects of the analysis that need clarification. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.",X,"The review point consists of questions seeking clarification about the Gaussian assumption and its practical implications. It does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, it is classified as ""No.""",2638,,,,,,,,
ICLR_2022_2956,ICLR_2022,,"2) The authors aim to address the overfitting problem, why limit the scope to dropout? What about other regularization techniques, like L2 regularization, data augmentation, adding noise? In practice, people never use a single regularization technique. What happens when ZeroLiers is used together with those techniques?",2827,4,"The review comment questions the authors' decision to focus solely on dropout as a means to address overfitting and suggests considering other regularization techniques such as L2 regularization, data augmentation, and adding noise. It also asks about the effects of using ZeroLiers in conjunction with these techniques. While the comment implies that the authors should explore and discuss these additional regularization methods, it does not explicitly instruct them to do so. The action is implicit but concrete, as it provides specific areas for the authors to consider and potentially expand upon in their work. Therefore, this comment is mostly actionable.",4,"The review comment raises a pertinent question about the scope of the paper's approach to addressing overfitting, specifically questioning why the focus is limited to dropout. It suggests considering other regularization techniques such as L2 regularization, data augmentation, and adding noise, and inquires about the combined effect of ZeroLiers with these techniques. This feedback is clear and actionable, as it encourages the authors to broaden their investigation and consider a more comprehensive approach to regularization. By addressing these points, the authors could significantly enhance the depth and applicability of their work. However, the comment could be more helpful if it provided specific examples or guidance on how to integrate these additional techniques into the study. Overall, the comment is mostly helpful, as it effectively directs the authors toward a meaningful expansion of their research scope.",3,"The comment is specific in its critique, questioning the authors' focus on dropout as the sole method for addressing overfitting and suggesting the consideration of other regularization techniques such as L2 regularization, data augmentation, and adding noise. It also raises a pertinent question about the practical application of combining ZeroLiers with other techniques. However, the comment does not explicitly mention a specific section of the paper, leaving the authors to infer that it relates to the methodology or discussion sections. This makes the comment weakly grounded but specific, aligning with label 3.",X,"The review point consists of questions and observations regarding the authors' choice to limit their scope to dropout as a regularization technique. It does not contain subjective opinions, judgments, or suggestions that require validation or support. The comment is primarily seeking clarification and exploring possibilities, fitting the classification of ""No.""",6120,,,,,,,,
4NsYCAxubi,ICLR_2025,"- The usage of math symbols is sometimes confusing. Also not required, it is suggested that the authors follow the [default notation](https://github.com/ICLR/Master-Template/raw/master/iclr2025.zip) for a clearer presentation of the equations.- The proposed method is not thoroughly explained. For example, the computation of some terms in (4) is missing, as well as its optimization algorithm, e.g., how to calculate $p(x_k|d)$. From my perspective, if one chooses to express the idea using math formulas, then every term should be clearly explained except for the cases where it is extremely obvious, which I think does not apply to (4).- A figure or algorithm may better explain the proposed method.- The authors use GPT-4 for clustering and tagging but GPT-3.5 for response generation and did not provide experimental results on other combinations. The performance of the proposed method therefore may not be universally applicable.- Potential data leakage issues (detailed in Questions).Overall, I think the approach proposed by this article is rather straightforward and could be easily described with better clarity without introducing any formulae, perhaps except for the motivation part. In addition, it seems that this article may find a broader audience in the pure NLP community rather than a mixed community of different machine learning topics. Therefore I would recommend to submitting this manuscript to ACL ARR venue instead of machine learning conferences.","- The proposed method is not thoroughly explained. For example, the computation of some terms in (4) is missing, as well as its optimization algorithm, e.g., how to calculate $p(x_k|d)$. From my perspective, if one chooses to express the idea using math formulas, then every term should be clearly explained except for the cases where it is extremely obvious, which I think does not apply to (4).",10401,5,"The review comment explicitly points out that the proposed method is not thoroughly explained, specifically highlighting missing computations and the optimization algorithm related to equation (4). The reviewer provides a clear action by stating that every term should be clearly explained, except in cases where it is extremely obvious. This feedback is explicit and concrete, as it specifies exactly what the authors need to do to improve the clarity of their explanation. The detailed nature of the suggestion makes this comment highly actionable.",4,"The review comment identifies a significant issue with the paper, specifically the lack of thorough explanation of the proposed method. It points out that the computation of certain terms in equation (4) is missing, as well as details about the optimization algorithm, such as how to calculate $p(x_k|d)$. The reviewer emphasizes the importance of clearly explaining every term in mathematical expressions unless they are extremely obvious, which is not the case here. This feedback is clear and actionable, providing the authors with specific areas to address in order to improve the clarity and comprehensiveness of their method description. By following this guidance, the authors can significantly enhance the quality and understandability of their work. Therefore, the comment is mostly helpful, aligning with a score of 4.",5,"The comment is fully grounded as it explicitly refers to a specific equation, ""(4),"" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it details the missing elements in the explanation, such as the computation of certain terms and the optimization algorithm, including how to calculate $p(x_k|d)$. This provides clear guidance on what needs to be addressed in the explanation of the proposed method. Therefore, this comment is categorized as fully grounded and specific, aligning with category 5.",4,"The review point claims that the proposed method is not thoroughly explained, specifically highlighting the missing computation of terms in equation (4) and the optimization algorithm. The reviewer provides a clear rationale by pointing out specific areas where the explanation is lacking, such as the calculation of $p(x_k|d)$. This reasoning is based on logical expectations for mathematical clarity and completeness in academic writing. However, the comment does not provide external references or detailed examples to further substantiate the claim. While the critique is logically sound, it could be strengthened with additional evidence or references. Therefore, the comment is categorized as 4: Mostly Verifiable, as it provides a solid basis for the claim but leaves room for further support.",3432,,,,,,,,
qIJenSdGbW,ICLR_2025,"1. Provide diversity metrics (e.g., FID, precision, recall [1]) across all models (SDXL, DreamShaper-xl-v2-turbo, Hunyuan-DIT). Currently, only FID on class-conditional ImageNet is reported, which seems limited. Bias toward certain noise could lower diversity, yet ImageNet’s small number of classes may skew results. Robust experiments showing that foundation text-to-image models maintain diversity would strengthen this claim. Although an aesthetic score is provided, it reflects only individual image quality.2. Related work is in the Appendix and experimental results include only two comparative methods:- 2-1. Move related work to the main paper. Repetition of text-image alignment results occupies substantial space. I recommend moving those to the Appendix and including related work in the main paper.- 2-2. The compared technique Repaint is outdated. Use more recent methods.- 2-3. Compare with Attend-and-Excite [2]. Though the Appendix states longer runtimes make it impractical, the paper specifies only a slight increase (5.6 to 9.7 seconds on Stable Diffusion), which is manageable.- 2-4. The credibility of Appendix results for InitNO is questionable. Though the paper InitNO reported improved text-image alignment on StableDiffusion-v1-4, Table 6 shows InitNO’s alignment is lower than that of StableDiffusion-v1-4.- 2-5. Details on Repaint and Inversion used in the main paper’s experiments are missing. Include these in the experiment section.3. The illustration for Stage 2 in Figure 2 is unclear. According to the method section, noise calculated by the singular value predictor and the residual predictor is combined, and this combined noise is compared with inversed noise in loss calculation. However, the figure seems to imply separate loss measurements, which hinders clarity.The two critical issues are: (1) whether the proposed method compromises diversity and (2) whether it performs better than other methods. I understand that InitNO or Attend-and-Excite can achieve higher performance due to its high computation time, therefore I would increase my score if the trade-off is acceptable. However, diversity is an important metric that should not be overlooked.[1] Kynkäänniemi et al., ""Improved Precision and Recall Metric for Assessing Generative Models.""[2] Chefer et al., ""Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Model.""","- 2-4. The credibility of Appendix results for InitNO is questionable. Though the paper InitNO reported improved text-image alignment on StableDiffusion-v1-4, Table 6 shows InitNO’s alignment is lower than that of StableDiffusion-v1-4.",11588,1,"The review point raises a concern about the credibility of the results in the Appendix for InitNO, noting a discrepancy between the reported improvements in text-image alignment and the results shown in Table 6. However, it does not provide any explicit or implicit actions for the authors to take to address this issue. The comment lacks guidance on whether the authors should re-evaluate the results, provide additional explanations, or take any corrective measures. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.",3,"The review comment raises a concern about the credibility of the results presented in the Appendix for InitNO, specifically questioning the reported improvement in text-image alignment. It highlights a discrepancy between the paper's claims and the results shown in Table 6, where InitNO's alignment is reportedly lower than that of StableDiffusion-v1-4. This is an important issue that the authors need to address to ensure the validity and reliability of their findings. However, the comment does not provide specific suggestions on how to resolve this discrepancy or improve the credibility of the results. While it identifies a critical area for improvement, the lack of actionable guidance limits its helpfulness. Therefore, the comment is somewhat helpful, aligning with a score of 3.",5,"The comment explicitly mentions ""Appendix results for InitNO"" and ""Table 6,"" allowing the authors to accurately identify the parts of the paper being addressed. It specifies the issue, which is the discrepancy between the reported improved text-image alignment for InitNO and the lower alignment shown in Table 6 compared to StableDiffusion-v1-4. This provides clear guidance on what the authors need to address regarding the credibility of the results. Therefore, the comment is fully grounded and specific, corresponding to label 5.",3,"The review point questions the credibility of the Appendix results for InitNO, specifically noting a discrepancy between the reported improvements in text-image alignment on StableDiffusion-v1-4 and the results shown in Table 6, where InitNO's alignment is lower. This claim is somewhat verifiable as it points out a specific inconsistency in the results, but it lacks further explanation or references to substantiate the claim fully. The authors would need to investigate the discrepancy themselves to understand the issue better. Therefore, the comment is rated as 3: Somewhat Verifiable.",723,,,,,,,,
ICLR_2023_4026,ICLR_2023,"My main concerns are the presentation and theoretical results.On the presentation, it is not clear what are the causal associations of the underlying system under investigation and what are the functional relationships assumed for the model that are ultimately learned, e.g. Def. 3 is presented as the underlying data generating mechanism which it isn't (I think). Moreover, it would be more standard to talk about causal graphs when referring to the underlying system, vaguely shown in Fig. 2, rather than for the label inference process. And in addition, definition 4 is hard to read because we don’t know what graph should be considered to determine independencies.The proof of Thm. 1 is straightforward once Def. 3 and its assumptions are given. Is there any particular insight in this result? There is no discussion of Thm. 2.Questions and suggestionsCan the authors better describe the reason for such a large performance gap between the proposed approach and existing algorithms. What does “min” and “avg” on table 1 refer to? By the description of the CMNIST dataset it seems agree with that used in the IRM but the performance results are very different.I don’t understand the paragraph starting “Why is our causal graph general”. It seems to omit unobserved confounding, I don’t think it is accurate that all possible world structures can be reduced to the simple model that is considered.Please write the optimization problem is Sec. 4.1 in a separate line.","3 is presented as the underlying data generating mechanism which it isn't (I think). Moreover, it would be more standard to talk about causal graphs when referring to the underlying system, vaguely shown in Fig. 2, rather than for the label inference process. And in addition, definition 4 is hard to read because we don’t know what graph should be considered to determine independencies. The proof of Thm.",3536,2,"The review comment raises several issues but lacks explicit guidance on how the authors should address them. It suggests that the presentation of ""3"" as the underlying data-generating mechanism might be incorrect and recommends using causal graphs for the underlying system rather than for the label inference process. Additionally, it notes that Definition 4 is hard to read due to unclear graph references for determining independencies. However, the comment does not provide specific actions or detailed suggestions for improvement, leaving the authors uncertain about how to proceed. The feedback is vague and requires the authors to infer the necessary actions, making it barely actionable.",3,"The review comment identifies several issues with the presentation and clarity of the paper. It questions the accuracy of presenting ""3"" as the underlying data-generating mechanism and suggests that discussing causal graphs would be more appropriate. Additionally, it points out that Definition 4 is difficult to understand due to a lack of clarity about which graph should be considered for determining independencies. While the comment highlights these weaknesses, it does not provide specific guidance or actionable suggestions for improvement, such as how to better present the data-generating mechanism or clarify Definition 4. This limits the usefulness of the feedback, making it somewhat helpful as it identifies areas for improvement but does not fully support the authors in addressing them.",3,"The comment addresses multiple issues, including the presentation of the data-generating mechanism, the use of causal graphs, and the readability of Definition 4. It references Figure 2, which provides some grounding, but it does not specify which part of the paper discusses the data-generating mechanism or the label inference process. The comment is somewhat specific in pointing out the issues with the presentation and the need for clarity in Definition 4, but it lacks explicit grounding for each point. Therefore, this comment is weakly grounded and specific, aligning with category 3.",3,"The review point raises concerns about the presentation of the data-generating mechanism and suggests that causal graphs would be more appropriate for describing the underlying system. It also mentions that Definition 4 is difficult to understand due to unclear graph references. While the comment provides some logical reasoning by suggesting the use of causal graphs and pointing out the lack of clarity in Definition 4, it lacks specific examples or references to substantiate the claims fully. The authors would need to infer the details and context to address the concerns effectively. Therefore, the comment is somewhat verifiable.",7666,,,,,,,,
ICLR_2022_1098,ICLR_2022,"Weakness: 1. It seems the experiments verifies the greedy learner hypothesis, but no proofs or theories are provided to support this hypothesis, either detailed analysis why the training speeds are different. 2. The re-balancing step method, which is used to make multi-modal learning less greedy, is not well described and analysis in the full text. The author should have a well explanation that why using the average feature of previous samples can stop the training of the uni-modal branch. 3. Lack comparation with other methods that solve imbalance problem, including [1]. The discussion in related work section is also inadequate. For example, related work [2-3] are not discussed. 4. The author set different imbalance parameter for different dataset, but did not provided the ablation study for this crucial parameter. Does it impact the model performance greatly? 5. All datasets only contain visual modality. More natural modalities, like audio and visual, should be included for better evaluation.[1] Weiyao Wang, Du Tran, and Matt Feiszli. What makes train- 1055 ing multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12695–12705, 2020. [2] Thomas Winterbottom, Sarah Xiao, Alistair McLean, and Noura Al Moubayed. On modality bias in the tvqa dataset. BMVC, 2020. [3] Ya Sun, Sijie Mai, and Haifeng Hu. Learning to balance the learning rates between various modalities via adaptive tracking factor. IEEE Signal Processing Letters, 28:1650– 1654, 2021.","2. The re-balancing step method, which is used to make multi-modal learning less greedy, is not well described and analysis in the full text. The author should have a well explanation that why using the average feature of previous samples can stop the training of the uni-modal branch.",3217,4,"The review point explicitly states that the re-balancing step method, which aims to make multi-modal learning less greedy, is not well described and analyzed in the text. It suggests that the authors should provide a thorough explanation of why using the average feature of previous samples can halt the training of the uni-modal branch. This feedback is explicit in its request for a detailed explanation and analysis, providing a clear direction for the authors to follow. However, it could be more actionable if it included specific aspects or questions that the explanation should address. Nonetheless, the comment is mostly actionable as it gives a clear indication of what needs to be improved.",3,"The review comment identifies a specific area of the paper that lacks clarity and depth: the re-balancing step method used in multi-modal learning. It points out that the method is not well described or analyzed in the text and suggests that the authors should provide a thorough explanation of why using the average feature of previous samples can halt the training of the uni-modal branch. This feedback is somewhat helpful as it highlights a critical gap in the manuscript and directs the authors to provide more detailed information. However, it could be more helpful by offering suggestions on how to improve the explanation or by indicating specific sections where the analysis could be expanded. Therefore, the comment is rated as somewhat helpful, consistent with a score of 3.",3,"The comment identifies a specific issue with the ""re-balancing step method"" used in the paper, suggesting that it is not well described or analyzed. While it does not mention a specific section, the authors can infer that it pertains to the methodology or results sections where this method is discussed, making it weakly grounded. The comment is specific as it requests a detailed explanation of why using the average feature of previous samples can stop the training of the uni-modal branch. Therefore, this comment is weakly grounded and specific, aligning with label 3.",2,"The review point claims that the ""re-balancing step method"" is not well described or analyzed in the text, and suggests that the authors should provide a detailed explanation of why using the average feature of previous samples can halt the training of the uni-modal branch. While the comment identifies a specific area that requires more clarity, it lacks supporting evidence, detailed reasoning, or references to substantiate the claim. The suggestion for improvement is clear, but the absence of a rationale or examples makes it difficult for the authors to fully understand the issue. Therefore, the comment is categorized as 2: Borderline Verifiable, as it provides some direction but lacks sufficient justification.",7339,,,,,,,,
IA3wm5vwUl,ICLR_2025,"1. The novelty/contribution of the method is weak given LEHD (Luo et al., 2023). The proposed method seems incremental compared to LEHD and DR (in the current paper) seems very similar to RRC (in LEHD).1. A better motivation for the encoder and decoder structure would have helped to better understand the value of the proposed approach.1. The rationale for the dual channel decoder is not explained clearly. Also, why only two channels and why not more?1. In fact, in terms of performance, for both TSP and CVRP, LEHD is not that far off from the proposed DEDD. This further takes away from the value of DEDD.1. In Table 1, for CVRP, HGS (Vidal, 2022) seems better than all solutions but this is not mentioned. Can you please comment on this?1. Since HGS performs the best in the CVRP experiments, why not use HGS as the baseline reference, instead of LKH3?1. Future work mentions exploring the training DEDD with RL. This seems like a straightforward approach to implement and I am curious why the paper did not do that. This might strengthen the contributions of the paper (see point 1 above).1. Some practical considerations such as training time and computational complexity are not discussed in the paper. This would allow readers to better understand the paper's contributions and results in the context of existing work.1. The paper needs to release code for all the presented results. Otherwise, how are we to know that the results in Table 1/Table 2 were computed correctly and fairly?1. There are minor errors scattered throughout the paper. Please proofread the paper carefully. One such example is: (i) The caption of Figure 2 is not appropriate.",1. A better motivation for the encoder and decoder structure would have helped to better understand the value of the proposed approach.,10965,2,"The review point suggests that the motivation for the encoder and decoder structure could be improved to enhance understanding of the proposed approach's value. However, it does not provide specific guidance on how to improve the motivation or what aspects should be addressed. The action is implicit and vague, as the authors need to infer that they should provide a better motivation but are not given concrete steps on how to do so. Therefore, this comment is barely actionable.",3,"The review comment identifies a specific area for improvement by suggesting that a better motivation for the encoder and decoder structure would enhance the understanding of the proposed approach's value. This feedback is somewhat helpful as it points out a potential weakness in the paper's explanation or justification of its methodology. However, the comment lacks depth and does not provide specific guidance or examples on how the authors might improve the motivation or what aspects need further elaboration. Without actionable steps or detailed suggestions, the authors may find it challenging to address this feedback effectively. Therefore, the comment is rated as somewhat helpful, consistent with a score of 3.",1,"The comment suggests that the motivation for the encoder and decoder structure could be improved to enhance understanding of the proposed approach's value. However, it does not specify which part of the paper this feedback pertains to, nor does it provide details on what aspects of the motivation are lacking or how they could be improved. Without specific references or detailed guidance, the authors may struggle to identify the exact areas needing enhancement. As a result, the comment lacks both grounding and specificity, making it not grounded at all.",1,"The review point suggests that the motivation for the encoder and decoder structure is insufficient, implying that a clearer explanation would enhance understanding of the proposed approach's value. However, the comment lacks specific examples, reasoning, or references to support this claim, making it difficult for the authors to fully grasp the issue or how to address it. As a result, the claim is unverifiable due to the absence of detailed justification or evidence. Therefore, it falls into Category 1: Unverifiable.",4635,,,,,,,,
LaNCeNmoHR,ICLR_2025,"Soundness- The evaluation for issue generation is not sound: (1) LLM judge can introduce noise (2) the metric is proposed by the authors instead of using standard approaches, e.g., comparing LLM results with existing github issues generated manually.- It is unclear if 16 models studied are the state-of-the-art. It will be interesting to compare the performance of this benchmark with SWE-BENCH for the models. Presentation- Section 3.1 needs concrete details. for example, what types of bugs are injected? how many of 160 bugs are injected?- Figure 3 is confusing, please improve Contribution- The contribution over SWE-Bench is low. The authors claimed that their benchmark has new additions of testing fault localization and LLM's capability of discovering issues itself. However,(1) if a model can fix a bug, it will need to do fault localization, so all the repair benchmarks implicitly test the fault localization capabilities of a model(2) When a unit test fails, we can pass it to an LLM automatically via scripts, we don't need LLM for issue generation.","- Figure 3 is confusing, please improve Contribution - The contribution over SWE-Bench is low. The authors claimed that their benchmark has new additions of testing fault localization and LLM's capability of discovering issues itself. However, (1) if a model can fix a bug, it will need to do fault localization, so all the repair benchmarks implicitly test the fault localization capabilities of a model (2) When a unit test fails, we can pass it to an LLM automatically via scripts, we don't need LLM for issue generation.",11690,4,"The review comment addresses two distinct issues. First, it explicitly states that Figure 3 is confusing and needs improvement, which is a direct and actionable suggestion. Second, it critiques the claimed contributions over SWE-Bench, arguing that the benchmark's additions are not as novel as suggested. The reviewer provides specific reasoning for this critique, such as the implicit testing of fault localization in repair benchmarks and the ability to pass failed unit tests to an LLM automatically. While the critique is clear, it does not provide explicit guidance on how the authors should address these concerns or improve their claims. Therefore, the comment is a mix of explicit and implicit actions, with the first part being highly actionable and the second part somewhat actionable. Overall, the comment is mostly actionable.",3,"The review comment addresses two distinct issues. First, it notes that Figure 3 is confusing and suggests improvement, although it does not provide specific guidance on how to make it clearer. Second, it critiques the claimed contributions over SWE-Bench, arguing that the benchmark's additions are not as novel as suggested. The reviewer provides reasoning for this by explaining that fault localization is inherently tested in repair benchmarks and that issue generation can be automated without LLMs. While the comment raises valid points and challenges the novelty of the contributions, it lacks actionable suggestions for improving Figure 3 or enhancing the benchmark's contributions. The feedback is somewhat helpful as it prompts the authors to reconsider their claims and the clarity of their figures, but it could be more beneficial with additional guidance on how to address these issues. Therefore, the comment is rated as somewhat helpful.",5,"The comment is fully grounded as it explicitly mentions ""Figure 3,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue with the contribution over SWE-Bench, detailing two specific points: the implicit testing of fault localization capabilities in repair benchmarks and the automatic passing of failed unit tests to an LLM via scripts. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.",4,"The review point critiques the contribution over SWE-Bench by questioning the novelty of the claimed additions. The reviewer provides logical reasoning to support their claim by explaining that fault localization is inherently tested when a model fixes a bug and that issue generation can be automated without LLMs. However, the comment lacks specific references or examples from existing literature or benchmarks to further substantiate these claims. While the reasoning is sound, the absence of concrete evidence or external references leaves minor gaps in the justification, making the claim mostly verifiable.",1215,,,,,,,,
NIPS_2020_1256,NIPS_2020,"1. The author should highlight the data dependent (on the covariance spectrum) nature of their approximation results (maybe in the abstract), though it is totally fine to have a data dependent bound. 2. The reader might be especially interested in the convergence, computing time, and approx ratio on real data. However, these materials are put into the appendix. I personally suggest moving these figures 3/4/5, at least partial of them, into the main paper by instead putting some settings in Table 2 into the appendix. Typos: * line 461: Theorem 5.2 -> Theorem 4.1 * line 644: Problem (A.2) -> Problem 3.1","1. The author should highlight the data dependent (on the covariance spectrum) nature of their approximation results (maybe in the abstract), though it is totally fine to have a data dependent bound.",7246,5,"The review point explicitly instructs the authors to highlight the data-dependent nature of their approximation results, specifically suggesting that this could be mentioned in the abstract. This feedback is clear and provides a direct action for the authors to take, ensuring that they know exactly what needs to be added to improve their draft. The comment is specific and actionable, making it highly actionable.",4,"The comment provides a specific suggestion for improvement by advising the authors to highlight the data-dependent nature of their approximation results, particularly in relation to the covariance spectrum. It acknowledges that having a data-dependent bound is acceptable, which helps clarify the context for the authors. This feedback is actionable and clear, guiding the authors on how to enhance the clarity and transparency of their work. However, it could be more helpful if it included specific examples or further elaboration on how to effectively incorporate this suggestion into the abstract. Overall, the comment is mostly helpful as it directs the authors to a specific area for improvement with a clear rationale.",3,"The comment suggests that the author should highlight the data-dependent nature of their approximation results, possibly in the abstract. However, it does not specify which part of the paper currently discusses this aspect or where exactly this addition should be made, aside from a general suggestion to include it in the abstract. This makes the comment weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. Nonetheless, the comment is specific in detailing what should be highlighted—the data-dependent nature of the approximation results. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.",1,"The review point suggests that the authors should emphasize the data-dependent nature of their approximation results, particularly in relation to the covariance spectrum. However, the comment does not provide any supporting evidence, reasoning, or examples to justify why this emphasis is necessary or how it would improve the paper. Without additional context or explanation, the authors may find it challenging to understand and address the suggestion. Therefore, the comment is considered unverifiable.",5186,,,,,,,,
8LSuy5nNmz,EMNLP_2023,"- Experimental results are a bit lacking. For example, are all the modules necessary?- The presentation of the paper could be improved a lot. See comments below.","- Experimental results are a bit lacking. For example, are all the modules necessary?",1436,2,"The review comment implies that the experimental results section is insufficient and raises a question about the necessity of all the modules. However, it does not explicitly instruct the authors to conduct additional experiments or provide specific guidance on how to address the concern. The action is inferred and vague, leaving the authors uncertain about the exact steps to take to improve the experimental results. Therefore, the comment is barely actionable.",2,"The review comment raises a concern about the experimental results, specifically questioning whether all the modules in the study are necessary. This is a relevant point that could lead to a more streamlined and efficient model if addressed. However, the comment lacks specificity and does not provide actionable guidance or suggestions on how the authors might explore this issue further. Without detailed feedback or examples, the authors are left with a vague sense of direction, making the comment barely helpful. Therefore, it aligns with a score of 2.",1,"The comment raises a concern about the experimental results and questions the necessity of all the modules. However, it does not specify which part of the paper or which modules are being referred to, making it difficult for the authors to pinpoint the exact area that needs attention. Additionally, the comment lacks specificity as it does not provide details on what aspects of the experimental results are lacking or how the necessity of the modules should be evaluated. This makes it challenging for the authors to address the issue effectively. Therefore, the comment is not grounded and not specific, aligning with a score of 1.",1,"The review point raises a concern about the sufficiency of the experimental results, specifically questioning the necessity of all the modules. However, it does not provide any supporting evidence, specific examples, or detailed reasoning to substantiate this claim. The comment is vague and lacks the necessary detail to guide the authors in understanding or addressing the issue. As a result, the claim is unverifiable, as it does not provide sufficient evidence or reasoning to support the assertion.",1180,,,,,,,,
aTFPO9FHL3,ICLR_2024,"1. The authors claim that Transformers demonstrate exceptional computational capacity, compared with Graph Neural Networks. However, the computational complexity of GNNs and traditional Transformers is $O(N)$ [1] and $O(N^2)$ [2] respectively, where $N$ is the number of nodes in the input graph. I suggest reporting the complexity and runtime in experiments.2. Todyformer is similar to Vision Transformer, while the authors do not provide the necessary analysis in terms of graph learning. Some suggestions are as follows.1. The authors may want to analyze the expressiveness of Todyformer in terms of sub-structure identification and the Weisfeiler-Leman (WL) test.2. The authors may want to analyze how and why Todyformer alleviates over-squashing and over-smoothing.3. Please explain why the baseline results in Table 2 are different from those from Table 3 in [3].4. The authors claim that the significantly low performance of Todyformer on the Wiki and Review datasets is due to the insufficient hyperparameter search. However, in my opinion, hyperparameter tuning is difficult to improve 3% accuracy.5. Please report the standard deviation in Table 2 following the baselines. Moreover, I suggest reporting a statistically significant difference.[1] Recipe for a General, Powerful, Scalable Graph Transformer. NeurIPS 2022.[2] NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification. NeurIPS 2022.[3] Temporal Graph Benchmark for Machine Learning on Temporal Graphs https://arxiv.org/pdf/2307.01026.pdf",3. Please explain why the baseline results in Table 2 are different from those from Table 3 in [3].,11140,5,"The review point explicitly instructs the authors to explain why the baseline results in Table 2 differ from those in Table 3 of a referenced work. This direct request provides a clear action for the authors to take, leaving no ambiguity about what is needed to improve the draft. The comment is specific and provides a concrete action for the authors to address, making it highly actionable.",3,"The comment requests clarification on a specific discrepancy between the baseline results presented in Table 2 of the current paper and those in Table 3 of a referenced work [3]. This is a clear and actionable request that prompts the authors to address a potential inconsistency in their results, which could be crucial for the credibility and understanding of their work. By asking for an explanation, the reviewer provides the authors with a direct path to improve the transparency and accuracy of their draft. However, the comment could be more helpful if it suggested possible reasons for the discrepancy or ways to resolve it. Overall, the feedback is somewhat helpful as it identifies a specific issue and requests clarification, aligning with a score of 3.",5,"The comment is fully grounded as it explicitly mentions ""Table 2"" and references ""Table 3 in [3],"" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it clearly specifies what needs to be addressed, namely the explanation for the difference in baseline results between the two tables. Therefore, this comment is labeled as fully grounded and specific.",X,"The comment is a request for clarification regarding the differences in baseline results between two tables. It does not contain any subjective claims, opinions, or suggestions that require verification. It is a factual inquiry, fitting the classification of ""No.""",8194,,,,,,,,
BViIHjzvoY,EMNLP_2023,"- The main contribution of the work is the proposed method that trains the error estimator to determine the selection ratio for sub-structure. While the idea is interesting, it is quite straightforward. My main concern for this method is the requirement for a validation set in order to train the error-estimator. It is not clear how the performance is sensitive to the size of the validation set.- There also doesn't seem to have a strong motivation for the proposed method. For instance, why do we need to use the estimated error rate, instead of simply using the current uncertainty? Is there any theoretical reason for this or empirical evidence to support?- The self-training scheme is more of a direct application here. It's great that the authors explore this direction in the paper but the contribution here is relatively limited.",- The self-training scheme is more of a direct application here. It's great that the authors explore this direction in the paper but the contribution here is relatively limited.,1334,1,"The review comment acknowledges the exploration of the self-training scheme but notes that the contribution is relatively limited. However, it does not provide any explicit or implicit suggestions for how the authors might enhance their contribution or improve their approach. There is no guidance on potential modifications, additional experiments, or alternative methods that could be explored to strengthen the contribution. As a result, the authors are left without any actionable steps to take in response to this feedback. Therefore, the comment is rated as unactionable.",1,"The comment acknowledges the exploration of a self-training scheme but critiques the contribution as relatively limited. However, it lacks specificity and does not provide actionable feedback or suggestions for improvement. Without identifying particular weaknesses or offering guidance on how the authors might enhance the novelty or impact of their work, the comment does not assist the authors in improving their draft. Therefore, it aligns with a score of 1, as it is not helpful at all.",1,"The comment critiques the self-training scheme as being more of a direct application and notes that the contribution is relatively limited. However, it does not specify which part of the paper this assessment is based on, nor does it provide details on what aspects of the self-training scheme are considered limited. Without explicit references to sections, figures, or specific elements of the paper, the authors cannot confidently determine which parts need attention or improvement. The lack of specificity and grounding makes it difficult for the authors to address the feedback effectively. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.",1,"The review point claims that the self-training scheme is a direct application and suggests that the contribution is limited. However, it does not provide specific examples, references, or detailed reasoning to support this claim. The lack of supporting evidence or explanation makes it difficult for the authors to understand the basis of the critique or how to address it. As a result, the claim is not verifiable, aligning with a score of 1: Unverifiable.",3516,,,,,,,,
gTRtDFm8Di,ICLR_2024,"1.I am confused about what is “shifted InD data”. In my understanding, “shifted + InD data” = OOD data. However, in the paper, they are different. And the concept “shifted InD data” is first introduced in the last line of page 1 with further definition. Could the authors give more clear explanation?2.In Section 3.1, the concept “benign OOD-contamination” means the OOD data that is helpful for TTA?3.In Section 4.1, for “Reduce” part, what are the differences from and advantages over EATA? Can we identify harmful OOD instances with entropy metric instead of energy one?4.In “Preliminaries and Notations”, the authors mention that irrelevant OOD data do not share the label set with training data? How to deal with these differences in label sets?5.The pipeline seems to be very complex and computationally costly. It may introduce significant latency while inferring models. Do the authors compare the latency of the proposed methods and SOTAs? Note that introducing too much computational cost will make TTA algorithms hard for practical applications.","3. In Section 4.1, for “Reduce” part, what are the differences from and advantages over EATA? Can we identify harmful OOD instances with entropy metric instead of energy one?",11009,4,"The review point raises two questions regarding Section 4.1: the differences and advantages of the ""Reduce"" part compared to EATA, and whether harmful OOD instances can be identified using an entropy metric instead of an energy one. These questions imply that the authors should provide a comparison with EATA and explore the possibility of using an entropy metric. While the intended actions are clear, the comment does not explicitly instruct the authors to make these additions or provide detailed guidance on how to approach them. The actions are implicit but concrete, making this comment mostly actionable.",3,"The review comment raises two pertinent questions regarding the ""Reduce"" part in Section 4.1. It asks for clarification on the differences and advantages over EATA, which is a valuable prompt for the authors to elaborate on their contributions and how they compare to existing methods. Additionally, the comment suggests considering an alternative metric (entropy instead of energy) for identifying harmful OOD instances. While these questions are insightful and encourage the authors to provide more detailed explanations, the comment lacks specific suggestions or guidance on how to address these questions. It could be more helpful if it offered potential directions or considerations for the authors to explore. Nonetheless, the feedback is somewhat helpful as it highlights areas for further clarification and exploration, prompting the authors to enhance their draft. Therefore, the comment aligns with a score of 3.",5,"The comment is fully grounded as it explicitly mentions ""Section 4.1"" and the ""Reduce"" part, allowing the authors to accurately identify the specific part of the paper being addressed. It also specifies the issues by questioning the differences and advantages over EATA and suggesting the use of an entropy metric instead of an energy one for identifying harmful OOD instances. This provides clear guidance on what needs to be addressed or clarified in the paper. Therefore, this comment is categorized as fully grounded and specific, aligning with category 5.",X,"The review point consists of questions asking for clarification and comparison regarding the ""Reduce"" part in Section 4.1. It does not contain any claims, opinions, or suggestions that require justification or evidence. The inquiries are factual and descriptive, fitting the classification of ""No"".",8022,,,,,,,,
ICLR_2021_1189,ICLR_2021,"weakness of the paper is its experiments section. 1. Lack of large scale experiments: The models trained in the experiments section are quite small (80 hidden neurons for the MNIST experiments and a single convolutional layer with 40 channels for the SVHN experiments). It would be nice if there were at least some experiments that varied the size of the network and showed a trend indicating that the results from the small-scale experiments will (or will not) extend to larger scale experiments. 2. Need for more robustness benchmarks: It is impressive that the Lipschitz constraints achieved by LBEN appear to be tight. Given this, it would be interesting to see how LBEN’s accuracy-robustness tradeoff compare with other architectures designed to have tight Lipschitz constraints, such as [1]. 3. Possibly limited applicability to more structured layers like convolutions: Although it can be counted as a strength that LBEN can be applied to convnets without much modification, the fact that its performance considerably trails that of MON raises questions about whether the methods presented here are ready to be extended to non-fully connected architectures. 4. Lack of description of how the Lipschitz bounds of the networks are computed: This critique is self-explanatory.Decision: I think this paper is well worthy of acceptance just based on the quality and richness of its theoretical development and analysis of LBEN. I’d encourage the authors to, if possible, strengthen the experimental results in directions including (but certainly not limited to) the ones listed above.Other questions to authors: 1. I was wondering why you didn’t include experiments involving larger neural networks. What are the limitations (if any) that kept you from trying out larger networks? 2. Could you describe how you computed the Lipschitz constant? Given how notoriously difficult it is to compute bounds on the Lipschitz constants of neural networks, I think this section requires more elaboration.Possible typos and minor glitches in writing: 1. Section 3.2, fist paragraph, first sentence: Should the phrase “equilibrium network” be plural? 2. D^{+} used in Condition 1 is used before it’s defined in Condition 2. 3. Just below equation (7): I think there’s a typo in “On the other size, […]”. 4. In Section 4.1, \epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \epsilon when (11) is discussed. 5. Section 4.2, in paragraph “Computing an equilibrium”, first sentence: Do you think there’s a grammar error in this sentence? I might also have mis-parsed the sentence. 6. Section 5, second sentence: There are two “the”s in a row.[1] Anil, Cem, James Lucas, and Roger Grosse. ""Sorting out lipschitz function approximation."" International Conference on Machine Learning. 2019.",1. Lack of large scale experiments: The models trained in the experiments section are quite small (80 hidden neurons for the MNIST experiments and a single convolutional layer with 40 channels for the SVHN experiments). It would be nice if there were at least some experiments that varied the size of the network and showed a trend indicating that the results from the small-scale experiments will (or will not) extend to larger scale experiments.,2477,5,"The comment explicitly points out the lack of large-scale experiments and provides a specific suggestion to vary the size of the network in order to demonstrate whether the results from small-scale experiments extend to larger-scale experiments. This feedback gives the authors a clear action to take and provides concrete guidance on how to implement it, making the comment highly actionable. The authors know exactly what needs to be done to improve their draft, as the reviewer has specified the need for experiments with varied network sizes to explore trends. Therefore, this comment is rated as a 5 on the actionability scale.",4,"The review comment identifies a limitation in the experimental setup, specifically the small scale of the models used in the experiments. It suggests that including experiments with larger models could provide insights into whether the results from small-scale experiments extend to larger scales. This feedback is clear and actionable, as it provides a specific direction for improving the draft by expanding the scope of the experiments. However, it could be more helpful if it offered specific guidance on how to implement larger scale experiments or what specific trends to look for. Despite this, the comment is mostly helpful as it directs the authors' attention to an important aspect of their work that could be enhanced.",5,"The comment is fully grounded as it explicitly refers to the ""experiments section"" and provides specific details about the models used, such as ""80 hidden neurons for the MNIST experiments"" and ""a single convolutional layer with 40 channels for the SVHN experiments."" This allows the authors to accurately identify the parts of the paper being addressed. The comment is also specific, as it suggests conducting experiments with varying network sizes to demonstrate whether the results from small-scale experiments extend to larger scales. This provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with a score of 5.",3,"The review point highlights a concern about the lack of large-scale experiments, noting that the models used are relatively small. The reviewer suggests that including experiments with varying network sizes could demonstrate whether the results from small-scale experiments extend to larger scales. While the comment identifies a potential limitation and suggests a way to address it, it lacks specific examples or references to support the claim that larger-scale experiments are necessary or beneficial. This makes the claim somewhat verifiable, as it provides a logical suggestion but requires additional evidence or examples to fully substantiate the need for larger-scale experiments. Therefore, the comment is rated as 3: Somewhat Verifiable.",4995,,,,,,,,
j2bP0STpw7,EMNLP_2023,"1. For the code summarization task and the code search task, the dataset size of Vault and CSN should be the same in order to demonstrate that Vault is better in terms of data quality than CSN.2. The compared dataset, CSN, is relatively old. It would be more convincing if the authors could include more recent datasets such as XLCoST for comparison.3. The authors run experiments on three tasks only: Code Search, Code Summarization, and Code Generation. To show the general performance improvement, it would require to include more tasks such as code translation or completion.4. In the experiments, only one baseline model is considered for each task. It would be better if there are more baselines to show the generality of Valut. Without it, one might suspect that only the well-performing results are reported.","1. For the code summarization task and the code search task, the dataset size of Vault and CSN should be the same in order to demonstrate that Vault is better in terms of data quality than CSN.",1441,3,"The comment suggests that for a fair comparison between Vault and CSN in terms of data quality, the dataset sizes for the code summarization and code search tasks should be the same. This is an explicit suggestion that provides a clear action for the authors to take. However, it does not provide detailed guidance on how to achieve this, such as specific steps to adjust the dataset sizes or any potential challenges in doing so. While the action is explicit, the lack of concrete details on implementation makes it somewhat actionable.",4,"The review comment provides a specific and actionable suggestion for improving the paper by recommending that the dataset sizes for Vault and CSN be the same in the code summarization and code search tasks. This would allow for a fair comparison to demonstrate that Vault is superior in terms of data quality. The feedback is clear and offers a concrete step for the authors to enhance the validity and impact of their experimental results. However, it could be more helpful if it explained why this comparison is important or how it might impact the paper's conclusions, which would elevate it to a highly helpful comment. Therefore, it aligns with a score of 4, indicating that the comment is mostly helpful.",5,"The comment is fully grounded as it explicitly mentions the ""code summarization task"" and ""code search task,"" allowing the authors to accurately identify the parts of the paper being addressed. It also specifies the issue by suggesting that the dataset size of Vault and CSN should be the same to demonstrate Vault's superiority in data quality. This provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.",1,"The review point suggests that the dataset sizes for Vault and CSN should be the same to demonstrate Vault's superiority in data quality. However, the comment lacks specific reasoning or evidence to support why equal dataset sizes are necessary for this comparison. It does not provide any references or detailed explanations to substantiate the claim, making it difficult for the authors to understand the basis of the suggestion. As a result, the comment is considered unverifiable.",4774,,,,,,,,
ezzmWTm8r6,ICLR_2025,"1. The proposed sparse-CL and k-NL mask the loss for some classes, controlled by the hyperparameters $s$ and $k$. Therefore, the choice of $s$ and $k$ is crucial for the effectiveness of the proposed method. However, in Supplementary Material A.3, it is only stated that $s$ and $k$ are chosen as $5$ based on cross-validation. This empirical selection reduces the generalizability of the proposed loss function.2. Additionally, the paper lacks an explanation and ablation study for the value of the hyperparameter $\alpha$ in Equation 17.3. The derivation from Equation 7 to Equation 8 appears to be problematic. Assuming the denominator in Equation 7 is approximately equal to 1 to derive Equation 8 is incorrect.4. Typo: line 334 in the main text, “if ∈ {1, 2, · · · , k}” seems to be redundant.","1. The proposed sparse-CL and k-NL mask the loss for some classes, controlled by the hyperparameters $s$ and $k$. Therefore, the choice of $s$ and $k$ is crucial for the effectiveness of the proposed method. However, in Supplementary Material A.3, it is only stated that $s$ and $k$ are chosen as $5$ based on cross-validation. This empirical selection reduces the generalizability of the proposed loss function.",5727,3,"The review point highlights the importance of the hyperparameters $s$ and $k$ in the proposed method and notes that their choice is crucial for its effectiveness. It points out that the empirical selection of these values based on cross-validation, as mentioned in Supplementary Material A.3, reduces the generalizability of the proposed loss function. However, the comment does not provide explicit guidance on what the authors should do to address this issue, such as suggesting alternative methods for selecting these hyperparameters or providing more detailed justification for their chosen values. The action is implicit and somewhat vague, leaving the authors with a general understanding of the issue but without specific steps to take. Therefore, the comment is somewhat actionable.",3,"The review comment identifies a critical aspect of the proposed method, specifically the importance of the hyperparameters $s$ and $k$ in the effectiveness of the sparse-CL and k-NL approaches. It points out that the empirical selection of these hyperparameters based on cross-validation, as mentioned in the Supplementary Material, may limit the generalizability of the loss function. This feedback is somewhat helpful as it highlights a potential weakness in the methodology and suggests that more attention should be given to the choice of these hyperparameters. However, the comment could be more helpful if it provided specific suggestions on how to address this issue, such as recommending alternative methods for selecting or validating these hyperparameters. Therefore, the comment is rated as somewhat helpful.",5,"The comment is fully grounded as it explicitly mentions ""Supplementary Material A.3,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue with the empirical selection of hyperparameters $s$ and $k$, highlighting that this choice reduces the generalizability of the proposed loss function. The comment provides clear guidance on what aspect of the methodology is problematic and why it might be an issue. Therefore, the comment is fully grounded and specific, corresponding to a score of 5.",4,"The review point claims that the empirical selection of hyperparameters $s$ and $k$ reduces the generalizability of the proposed loss function. This claim is supported by the explanation that $s$ and $k$ are crucial for the method's effectiveness and are chosen based on cross-validation, as stated in the Supplementary Material A.3. While the comment provides a logical reasoning about the importance of these hyperparameters, it lacks specific examples or references to substantiate the impact of this empirical selection on generalizability. Therefore, the claim is mostly verifiable, as it is based on sound reasoning but could benefit from additional evidence or examples to fully support the assertion. Thus, the feedback is rated as 4: Mostly Verifiable.",5556,,,,,,,,
Zy2XgaGpDw,ICLR_2025,"My major concerns lie in the experimental settings and evaluation setups, which makes the results less convincing to me.- Backbone choices: I found the models used in this paper are somewhat arbitrary: the reward model is trained upon PaliGemma; human evaluation is conducted with captions generated by MiniCPM; GPT-4V is used to perform self-correction experiments. Are there any specific reasons to choose different models in these experiments?- Insufficient experiments regarding model selection:- The reward model is only trained with a PaliGemma-3B model, which as demonstrated in Table 3, performs the worst in the hallucination. Wouldn't a stronger backbone lead to stronger Token-level reward models? An ablation study comparing TLDR models trained on different backbones, including stronger ones like phi and llama-3.2, is recommended.- The correlation between MMMU scores is misleading. As pointed by Cambrian-1 and MMMU-Pro, MMMU score does not faithfully reflect the visual understanding and reasoning performance but more about the LLM capability. This renders the correlation analysis less convincing and I recommend the authors incorporate visual-oriented benchmarks to justify this claim.- Why LLaVA-series and Qwen-VL are not included in the evaluation? These are commonly adopted LVLMs. Including these models would provide a more comprehensive comparison across different LVLM architectures and training approaches.- In Table 5, why are only two tasks (Counting and spatial relation) of BLINK adopted as in-domain tasks? There are also in-domain tasks such as Visual correspondence & Object localization. Adding the comprehensive results on BLINK could better illustrate the full picture.",- Why LLaVA-series and Qwen-VL are not included in the evaluation? These are commonly adopted LVLMs. Including these models would provide a more comprehensive comparison across different LVLM architectures and training approaches.,11171,4,"The reviewer questions the exclusion of LLaVA-series and Qwen-VL from the evaluation and suggests that including these commonly adopted LVLMs would offer a more comprehensive comparison across different architectures and training approaches. While the comment implies that the authors should include these models in their evaluation, it does not explicitly instruct them to do so. However, the action is clear and concrete, as it specifies which models should be added to enhance the comparison. Therefore, this comment is mostly actionable.",4,"The review comment raises a pertinent question about the exclusion of LLaVA-series and Qwen-VL from the evaluation, noting that these are commonly adopted LVLMs. It suggests that including these models would offer a more comprehensive comparison across different LVLM architectures and training approaches. This feedback is clear and actionable, as it provides a specific recommendation that could enhance the breadth and relevance of the evaluation in the paper. By addressing this point, the authors can improve the comprehensiveness and robustness of their comparative analysis. However, the comment could be more helpful if it included additional context or justification for why these specific models are important to include. Overall, the feedback is mostly helpful, as it effectively directs the authors toward a meaningful enhancement of their draft.",3,"The comment raises a question about the exclusion of LLaVA-series and Qwen-VL from the evaluation, suggesting that their inclusion would provide a more comprehensive comparison. However, it does not specify which part of the paper this issue pertains to, such as the evaluation section or a specific table or figure. This makes the comment weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. The comment is specific in its suggestion to include these models for a more comprehensive comparison, providing clear guidance on what could be improved. Therefore, the comment is weakly grounded and specific, aligning with category 3.",1,"The review point questions the exclusion of LLaVA-series and Qwen-VL from the evaluation, suggesting that their inclusion would offer a more comprehensive comparison. However, the comment does not provide specific reasoning or evidence to support why these models are essential for the evaluation or how their inclusion would impact the study's findings. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.",3981,,,,,,,,
NIPS_2020_866,NIPS_2020,"- My major concern is why the problem is difficult. Assumption 1 literally enforces that the adversary cannot pick arbitrary S, but only those such that a constant alpha-fraction of the observations are hidden/removed. Thus, suppose before removal we have a total of m samples (a, y). After removal it reduces to alpha * m pairs (a, y), which still suffices for accurate recovery provided that m = O(k log n). - It is not convincing to me that the sample complexity in Theorem 3.1 is near-optimal. I know that O(k log n) is near-optimal, but does your result really imply such bound? Observe that your success probability is 1 - 1/log n, which has a very slow convergence to 1. In contrast, almost all existing works provide a strong probability of 1 - n^{-100}, or even 1 - exp(-n). If you rephrase Theorem 3.1 in a way that with probability 1- delta, || x_bar - x^* || < f(m, k, n, delta) What is the expression of the function f? - In the rest of the paper, the success probability is superceded with ""high probability"", which I do not believe comforms common practice. Again, note that 1-1/log n does not suffice for a high probability argument since it carries out serious issue on sample complexity. - I am disappointed that the proof of Theorem 3.1 is nowhere found, and the organization of the supplement is quite a mess. I was not able to track the true sample complexity under a real high probability event. - The term ""adversarial noise"" is misleading. You are not handling arbitrary noise, e.g. you cannot slightly tailor your algorithm to gross outliers. - PDW in Wainwright 2009 requires a lower bound on |xhat - x|, but this paper asks for an upper bound. Is there any intuition behind the essential difference? - I find Lemmas 5.7 and 5.8 are hard to follow. How can you ensure/test for any x, f(x) - f(xhat) < log n / m^3 ? If the purpose of Section 5.2 is to provide faster rate of SGD, why don't you show that the Hessian matrix (Lemma 5.1) has a lower bound on its smallest singular value when restricted on sparse directions? My overall feeling is that the paper presents a dense set of results to extend a previous work on truncated linear regression to the high-dimensional regime. Yet, most of the key techniques, e.g. log-likelihood of truncated linear regression, projected SGD, have been established in Daskalakis 2019. The main difference lies in an application of primal-dual witness, but was also known due to Wainwright 2009. The discussion in the main body, and the proof in the supplement, are hard to follow. I believe many of the results have been proved in Daskalakis 2019. So references to lemmas thereof (maybe with a short paragraph explaining the difference) should serve the purpose. ---updates after rebuttal--- Thank you for addressing my concerns. I suggest incorporating them into your revision. I still feel Section 5 is dense and hard to follow. The notation and terminology can also be improved. Overall this is a good submission, but it turns out the computational efficiency heavily relies on Assumption II.","- My major concern is why the problem is difficult. Assumption 1 literally enforces that the adversary cannot pick arbitrary S, but only those such that a constant alpha-fraction of the observations are hidden/removed. Thus, suppose before removal we have a total of m samples (a, y). After removal it reduces to alpha * m pairs (a, y), which still suffices for accurate recovery provided that m = O(k log n).",7073,1,"The review point raises a concern about the difficulty of the problem, specifically questioning Assumption 1 and its implications for the adversary's ability to pick arbitrary sets. It provides a detailed explanation of the assumption and its impact on the number of samples required for accurate recovery. However, the comment does not offer any explicit or implicit suggestions for how the authors should address this concern or improve their draft. There is no guidance on potential modifications, clarifications, or additional analyses that could be conducted to address the issue raised. As a result, the authors are left without any actionable steps to take in response to this feedback. Therefore, the comment is rated as unactionable.",4,"The review comment raises a critical concern about the difficulty of the problem being addressed, specifically questioning the assumptions made about the adversary's capabilities. It provides a detailed explanation of why the assumption might not make the problem as challenging as suggested, using a mathematical argument to support the point. This feedback is valuable as it prompts the authors to reconsider or justify their assumptions, potentially leading to a more robust and convincing argument in their paper. However, the comment could be more helpful if it offered suggestions on how to address this concern or alternative assumptions that could be explored. Despite this, the feedback is mostly helpful as it directs the authors' attention to a fundamental aspect of their work that may need revision or clarification.",3,"The comment raises a major concern about the difficulty of the problem, specifically questioning Assumption 1 and its implications on the adversary's ability to pick arbitrary sets. However, it does not specify which part of the paper this assumption is discussed in, making it weakly grounded. The comment does provide some specificity by explaining the issue with the assumption and its impact on the sample size required for accurate recovery. Therefore, the comment is weakly grounded and specific, aligning with category 3.",4,"The review point raises a concern about the difficulty of the problem, specifically questioning Assumption 1. The reviewer provides a logical reasoning by explaining that even after the removal of samples, the remaining data should still suffice for accurate recovery, given a certain condition (m = O(k log n)). This reasoning is based on a mathematical argument, which supports the claim that the problem may not be as difficult as suggested. However, the comment could be strengthened by providing more detailed examples or references to similar scenarios in the literature. As it stands, the claim is mostly verifiable due to the logical reasoning provided, but it has minor gaps that could be addressed with additional evidence or references. Therefore, the comment aligns with a score of 4: Mostly Verifiable.",2195,,,,,,,,
wVMzK2Feuu,ICLR_2025,"- The paper is hard to follow, and the writing could be improved. For example, the definition of $w_k$ lacks clarity and could be more explicitly explained.- The update process of $\alpha_k$ is somewhat confusing. Since it is connected to class $c$, it raises the question: if $\alpha_k$ is defined for each class $c$, then does each class have a unique $\alpha_k$? However, the earlier sections suggest it is shared across all classes. Additional clarification on this would be helpful.- The way the dynamic weight adjustment mechanism strengthens the protection of parameters for tail classes is not entirely clear. If $\alpha_k$ is indeed shared across all classes, it is unclear why adjusting the weights of different scoring criteria would selectively protect parameters for tail classes. Could there be a scoring criterion that more specifically targets the protection of parameters for tail classes?- From the experimental results on the ImageNet-LT dataset, compared to ATO and RReg, it appears that LTAP achieves a larger performance improvement for head classes than for tail classes. This seems to contradict the stated goal of strengthening parameter protection for tail classes in LTAP.- There are also some typos, such as in line 700, where ""equation eqution"" should be ""equation.""","- From the experimental results on the ImageNet-LT dataset, compared to ATO and RReg, it appears that LTAP achieves a larger performance improvement for head classes than for tail classes. This seems to contradict the stated goal of strengthening parameter protection for tail classes in LTAP.",10100,1,"The review point highlights a potential contradiction in the experimental results on the ImageNet-LT dataset, where LTAP appears to achieve a larger performance improvement for head classes than for tail classes, despite the stated goal of strengthening parameter protection for tail classes. However, the comment does not provide explicit guidance or suggestions on how the authors should address this contradiction. It lacks specific actions or recommendations for the authors to follow, leaving them uncertain about how to resolve the issue. Without clear instructions or concrete steps, the comment is unactionable.",3,"The review comment identifies a potential contradiction between the stated goal of the LTAP method and the experimental results on the ImageNet-LT dataset. It points out that LTAP appears to achieve a larger performance improvement for head classes than for tail classes, which contradicts the goal of strengthening parameter protection for tail classes. This observation is valuable as it highlights a discrepancy that the authors need to address to ensure the coherence and validity of their claims. However, the comment does not provide specific suggestions or guidance on how the authors might resolve this contradiction or further investigate the issue. While it identifies an important area for improvement, it lacks actionable feedback that would empower the authors to make significant changes to their draft. Therefore, the comment is somewhat helpful, aligning with a score of 3.",5,"The comment is fully grounded as it explicitly refers to the experimental results on the ""ImageNet-LT dataset,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue by pointing out a contradiction between the observed performance improvement for head classes and the stated goal of strengthening parameter protection for tail classes in LTAP. This provides clear guidance on what the authors need to address. Therefore, the comment is fully grounded and specific, corresponding to label 5.",3,"The review point claims that the experimental results on the ImageNet-LT dataset show that LTAP achieves a larger performance improvement for head classes than for tail classes, which contradicts the stated goal of strengthening parameter protection for tail classes. This claim is somewhat verifiable as it provides a specific observation from the experimental results, indicating a potential contradiction with the stated goal. However, the comment lacks detailed evidence or examples from the results to fully substantiate the claim, making it challenging for the authors to understand the basis of the observation. Therefore, the comment is rated as ""3: Somewhat Verifiable.""",1307,,,,,,,,
SDG0EBoqpp,ICLR_2025,"- A primary concern is the lack of baseline comparisons in this study. Although the authors claim that this is the only work addressing the fMRI forecasting problem, prior fMRI foundation models, such as BrainLM (Ortega Caro et al., 2023), which employs masked-prediction training, can also perform forecasting by placing the mask over the prediction segment. Additionally, the forecasting horizon in this study (up to 30 TR) aligns with the sequence length used in BrainLM (200 TR, if recalled correctly). The authors' rationale for not including a comparison with BrainLM is unclear.- This absence of baselines raises a second concern: the model architecture used in this paper appears somewhat antiquated techniques. Without comparisons to other models, there is no evidence that the current methodology is more effective or novel for fMRI forecasting compared to contemporary time-series forecasting architectures. Beyond the MAE architecture previously applied to fMRI data, numerous advanced time-series forecasting models could be potential baselines that could be evaluated on fMRI data, such as those available in the “Time-Series Library” repository (https://github.com/thuml/Time-Series-Library/tree/main).- The study does not address the influence of fMRI auto-correlation, a significant factor in fMRI forecasting. Typically, fMRI signals exhibit auto-correlation across a time lag of approximately -10 to 10 seconds, potentially making short-term forecasting within this range easier. In this study, the forecasting horizon for Table 1 is set to 15 TR, which largely falls within this auto-correlation window. While this does not imply an issue with the methodology itself, it would be valuable to examine prediction performance when introducing a gap between the input and forecasting time windows to reduce the effects of auto-correlation. At a minimum, the authors should discuss this factor, as it plays an important role in fMRI forecasting task.","- This absence of baselines raises a second concern: the model architecture used in this paper appears somewhat antiquated techniques. Without comparisons to other models, there is no evidence that the current methodology is more effective or novel for fMRI forecasting compared to contemporary time-series forecasting architectures. Beyond the MAE architecture previously applied to fMRI data, numerous advanced time-series forecasting models could be potential baselines that could be evaluated on fMRI data, such as those available in the “Time-Series Library” repository (https://github.com/thuml/Time-Series-Library/tree/main).",11936,5,"The review point highlights the absence of baselines and suggests that the model architecture used in the paper may be outdated. It explicitly recommends comparing the current methodology to other models, particularly advanced time-series forecasting models, to demonstrate its effectiveness or novelty. The comment even provides a specific resource, the ""Time-Series Library"" repository, where potential baselines can be found. This guidance is explicit and concrete, offering clear actions for the authors to take to improve their draft. Therefore, the comment is highly actionable.",5,"The review comment identifies a significant issue with the paper: the absence of baseline comparisons, which raises concerns about the effectiveness and novelty of the proposed model architecture for fMRI forecasting. It suggests that the techniques used in the paper may be outdated and emphasizes the need for comparisons with contemporary time-series forecasting architectures. The comment provides a specific resource, the ""Time-Series Library"" repository, as a potential source for advanced models that could serve as baselines. This feedback is highly helpful as it offers clear, actionable guidance for the authors to improve their work by incorporating relevant baseline comparisons and situating their methodology within the context of current advancements in the field.",3,"The comment addresses the absence of baselines and critiques the model architecture as outdated, specifically in the context of fMRI forecasting. It suggests comparing the current methodology with contemporary time-series forecasting architectures and provides a specific resource, the ""Time-Series Library"" repository, for potential baselines. However, the comment does not specify which part of the paper discusses the model architecture or where the comparisons should be included, making it weakly grounded. The suggestion is specific, as it points to particular models and a resource for potential baselines. Therefore, the comment is weakly grounded but specific, aligning with the label 3.",5,"The review point raises a concern about the absence of baselines, suggesting that the model architecture used in the paper may be outdated. It emphasizes the need for comparisons with contemporary time-series forecasting architectures to establish the effectiveness or novelty of the current methodology. The comment is well-supported by referencing the ""Time-Series Library"" repository as a source for potential baselines. This specific reference provides a robust foundation for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.",3719,,,,,,,,
TjTe2sEOMP,ICLR_2025,"1. There is limited discussion on the diversity and quality of the dataset, especially regarding rare conditions or imaging variations. And how the model might behave on noisy or imbalanced real-world medical datasets, where labels may be incomplete, inaccurate, or skewed.2. While the paper claims to introduce a novel mixture of expert (MoE) architecture, the core idea of MoE is not new. The mixture of experts has been explored extensively in both natural language processing and computer vision. The author should clarify the differences and how it can work on the specific problem well.3. The natural language prompts used in the experiments are relatively straightforward (e.g., “A chest X-ray image to evaluate the lungs”). The paper doesn’t address how the model would handle more complex or nuanced prompts.4. While the hallucination-aware experts are designed to minimize false positives, the paper lacks a clear explanation of why and how these experts work effectively. The mechanism for detecting and quantifying hallucination anomalies is somewhat opaque, and there are no visual or empirical explanations of how these experts influence anomaly maps. Providing more visual examples of the hallucination-aware experts in action, along with a more intuitive explanation of their impact.","1. There is limited discussion on the diversity and quality of the dataset, especially regarding rare conditions or imaging variations. And how the model might behave on noisy or imbalanced real-world medical datasets, where labels may be incomplete, inaccurate, or skewed.",11851,3,"The review comment highlights a lack of discussion on the diversity and quality of the dataset, particularly concerning rare conditions or imaging variations. It also raises concerns about how the model might perform on noisy or imbalanced real-world medical datasets with incomplete, inaccurate, or skewed labels. While the comment identifies areas that need more attention, it does not provide explicit guidance or suggestions on how to address these issues. The authors can infer that they need to expand their discussion on these topics, but the lack of specific instructions makes the action somewhat vague. Therefore, the comment is somewhat actionable.",3,"The review comment identifies a significant gap in the paper regarding the discussion of dataset diversity and quality, particularly in the context of rare conditions or imaging variations. It also raises concerns about the model's performance on noisy or imbalanced real-world medical datasets, where labels might be incomplete, inaccurate, or skewed. This feedback is valuable as it highlights critical areas that need attention to ensure the robustness and applicability of the model in practical scenarios. However, the comment could be more helpful if it provided specific suggestions or guidance on how to address these issues, such as proposing methods for evaluating dataset quality or strategies for handling imbalanced data. Despite this, the comment is somewhat helpful as it directs the authors' attention to important considerations that could enhance the paper's impact and relevance.",3,"The comment highlights a concern about the limited discussion on the diversity and quality of the dataset, particularly in relation to rare conditions or imaging variations. It also raises questions about the model's performance on noisy or imbalanced real-world medical datasets. However, the comment does not specify which part of the paper should address these issues, making it weakly grounded. The comment is specific in identifying the aspects of the dataset and model performance that need further discussion. Therefore, this comment is weakly grounded and specific, aligning with the label 3.",3,"The review point raises concerns about the limited discussion on the dataset's diversity and quality, particularly in relation to rare conditions or imaging variations. It also questions how the model might perform on noisy or imbalanced real-world medical datasets. While the comment highlights important considerations, it lacks specific examples or references to support the claim. The absence of detailed reasoning or evidence makes it somewhat verifiable, as the authors would need to further explore these aspects themselves to address the critique. Therefore, the comment is rated as 3: Somewhat Verifiable.",5655,,,,,,,,
ICLR_2023_1498,ICLR_2023,"1. All the proposed manipulation tasks are based on robotic arms and lack support for other actuators (e.g. Dexterous hands). 2. Since different simulators have different parameters that may drastically affect the overall performance, please include the details of parameters for each simulator (e.g. Did you use the GPU pipeline in IsaacGym or just used the CPU pipeline, the substeps for physics simulation, the number of the total vertex in the simulation). 3. The performance comparison is carried out on RGBD input settings. The performance of pure state input and point-cloud input is not reported. 4. The effectiveness and accuracy of demonstration conversion are neither discussed in detail nor measured quantitatively. 5. Point-cloud observations in some tasks contain ground truth segmentation, which cannot be easily obtained in the real world. This barrier for sim-to-real transfer could be avoided with a better observation design.","2. Since different simulators have different parameters that may drastically affect the overall performance, please include the details of parameters for each simulator (e.g. Did you use the GPU pipeline in IsaacGym or just used the CPU pipeline, the substeps for physics simulation, the number of the total vertex in the simulation).",3786,5,"The review comment explicitly instructs the authors to include details of parameters for each simulator, providing specific examples such as whether the GPU or CPU pipeline was used in IsaacGym, the substeps for physics simulation, and the total number of vertices in the simulation. This direct request leaves no ambiguity about what the authors need to do to improve their draft. The comment is specific and provides a clear action for the authors to take, making it highly actionable.",4,"The comment provides a clear and actionable suggestion for improvement by requesting the inclusion of detailed parameters for each simulator used in the study. It highlights the importance of these details, as different parameters can significantly impact overall performance. By specifying examples such as the use of GPU or CPU pipelines in IsaacGym, substeps for physics simulation, and the total number of vertices in the simulation, the reviewer offers concrete guidance that can help the authors enhance the transparency and reproducibility of their work. This feedback is mostly helpful, as it directs the authors to provide essential information that could improve the clarity and robustness of their paper. However, it could be further improved by explaining how these details might affect the study's conclusions or interpretations, which would elevate it to a highly helpful comment. Therefore, it aligns with a score of 4.",5,"The comment is fully grounded as it explicitly refers to the need for details on parameters for each simulator, allowing the authors to accurately identify the part of the paper being addressed. It specifies the issue by asking for details such as whether the GPU or CPU pipeline was used in IsaacGym, the substeps for physics simulation, and the number of total vertices in the simulation. This provides clear guidance on what information is missing and needs to be included. Therefore, the comment is fully grounded and specific, aligning with category 5.",X,"The review point requests additional details about the parameters used for each simulator, which is a factual observation and a request for clarification. It does not contain a subjective claim or opinion that requires verification. Therefore, it is classified as ""No.""",6909,,,,,,,,
fMzO6vcmhy,ICLR_2024,"- Their presentation looks not ready to be published yet in many aspects.- Ambiguous expression or not introduced terms.- In the introduction, the terms ""generalization"" (the model's ability to make effective decisions when exposed to novel inputs), ""interpretability"" (how easily its learned parameters can be inspected and understood by a human), and ""robustness"" (the predictability of its behavior on arbitrary inputs) need clarification. Explain the distinction between ""novel"" inputs (previously unseen) and ""arbitrary"" inputs (inputs that may not follow a specific pattern or structure) to avoid confusion.- On page 4, when discussing the player's ability to swap its color with the ""new change-color action,"" it's important to introduce the ""new change-color action"" before using it in the sentence. Provide a brief explanation or definition of this action to ensure readers understand its context and purpose.- On page 5, when referring to the ""best"" candidate, explicitly define what ""best"" means within the context of the paper. Clarify whether ""best"" refers to candidates that are most relevant for prediction, those with the highest confidence scores, or some other criteria.- Regarding the explanation of boosting and the working set in the sentence on page 5, consider whether this explanation is necessary. If it adds value to the reader's understanding of how the working set is updated, provide a concise summary of how boosting is related to the working set, ensuring that it enhances clarity.- In section 3.2, provide a clear definition or explanation of the ""learnable module"" within the context of Quantified Function Learning. Specify its purpose and functionality to ensure the reader understands its role in the paper.- Equation 2 needs clarification regarding the conditions for movement to the right and the meaning of first and second coordinates.- The explanation of the relation group is insufficient, and details about its design or training architecture are lacking.- The meaning of ""c"" is not provided in Section 3.3.- In Section 4, different notations for different meanings should be used to avoid reader confusion.- The abbreviation ""EMD"" is used before it is introduced.- The analysis of experimental results needs improvement- The use of $m=1$ for Neural Networks and larger $m$ for DOORMAX and QORA should be justified more, as it may raise fairness concerns.- More analysis is needed to explain the errors that occur when $m=1000"" for QORA, beyond the fact that a single layout is used.- The explanation for DOORMAX's inability to resolve the effect of the change-color action is insufficient.- There are several literatures studied the object-centric or object-oriented representation for the Reinforcement Learning tasks, but they are missed. For example, in [1], the unsupervised object-centric representation for model-free reinforcement learning agent is investigated, and in [2], the world model learning of given object attributes is studied.- The evaluation is limited to a synthetic environment, and the paper should discuss the potential for extending the model to more realistic environments and the expected limitations in such cases.- The choice of baselines lacks diversity, and the paper could benefit from considering relevant studies in the object-centric representation field [3,4,5].[1] Yoon, Jaesik, et al. ""An investigation into pre-training object-centric representations for reinforcement learning."" arXiv preprint arXiv:2302.04419 (2023).[2] Sancaktar, Cansu, Sebastian Blaes, and Georg Martius. ""Curious exploration via structured world models yields zero-shot object manipulation."" Advances in Neural Information Processing Systems 35 (2022): 24170-24183.[3] Locatello, Francesco, et al. ""Object-centric learning with slot attention."" Advances in Neural Information Processing Systems 33 (2020): 11525-11538.[4] Singh, Gautam, Yeongbin Kim, and Sungjin Ahn. ""Neural systematic binder."" The Eleventh International Conference on Learning Representations. 2022.[5] Jiang, Jindong, et al. ""Object-centric slot diffusion."" arXiv preprint arXiv:2303.10834 (2023).","- On page 5, when referring to the ""best"" candidate, explicitly define what ""best"" means within the context of the paper. Clarify whether ""best"" refers to candidates that are most relevant for prediction, those with the highest confidence scores, or some other criteria.",9026,5,"The review point explicitly instructs the authors to define what ""best"" means in the context of their paper when referring to the ""best"" candidate on page 5. It provides specific guidance by suggesting that the authors clarify whether ""best"" refers to candidates that are most relevant for prediction, those with the highest confidence scores, or some other criteria. This clear and direct instruction leaves no ambiguity about what the authors need to do to improve their draft. The comment is specific and provides a concrete action for the authors to take, making it highly actionable.",5,"The review comment is highly helpful as it provides specific and actionable feedback regarding the use of the term ""best"" in the paper. It suggests that the authors explicitly define what ""best"" means in the context of their work, offering potential criteria such as relevance for prediction or confidence scores. This guidance is clear and constructive, enabling the authors to improve the clarity and precision of their language, which is crucial for effective communication of their findings. By addressing this feedback, the authors can enhance the reader's understanding and interpretation of their results, making the comment highly beneficial for improving the draft.",5,"The comment is fully grounded as it explicitly mentions ""page 5,"" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies what needs to be clarified, namely the definition of ""best"" in the context of the paper and the criteria used to determine it. This provides clear guidance on what the authors need to address. Therefore, this comment is fully grounded and specific, corresponding to label 5.",X,"The comment is a request for clarification regarding the definition of ""best"" in the context of the paper. It does not make a subjective claim or judgment but seeks additional information. Therefore, it fits the ""No"" label.",2243,,,,,,,,
MKjGklW9TP,EMNLP_2023,"1. The novelty of the paper is somewhat vague and needs to be more concise. The primary components seem to integrate effective elements of previous works, including SCL, Aligned, and CLNN, among others.2. The authors' claims of solving ""in-domain over-fitting"" and ""meaningless new intent problems"" require more in-depth analysis and visualization.","1. The novelty of the paper is somewhat vague and needs to be more concise. The primary components seem to integrate effective elements of previous works, including SCL, Aligned, and CLNN, among others.",2061,2,"The review comment points out that the novelty of the paper is vague and suggests that it needs to be more concise. However, it does not provide specific guidance on how the authors should achieve this conciseness or which aspects of the novelty should be clarified. The comment mentions the integration of elements from previous works but does not offer actionable steps for improvement. As a result, the action is implicit and vague, leaving the authors uncertain about how to address the feedback. Therefore, this comment is barely actionable.",2,"The comment identifies a concern with the paper's novelty, noting that it appears to integrate elements from previous works such as SCL, Aligned, and CLNN. However, it lacks specific suggestions or guidance on how the authors might clarify or enhance the novelty of their work. Without actionable feedback or detailed advice on how to address this issue, the comment provides limited value to the authors in terms of improving their draft. Therefore, it is rated as barely helpful, consistent with a score of 2.",1,"The comment critiques the novelty of the paper, suggesting that it is vague and needs to be more concise. It mentions the integration of elements from previous works, such as SCL, Aligned, and CLNN. However, it does not specify which part of the paper this assessment is based on, nor does it provide details on how the novelty could be improved or which specific elements are being integrated. Without explicit references to sections, figures, or specific elements of the paper, the authors cannot confidently determine which parts need attention or improvement. The lack of specificity and grounding makes it difficult for the authors to address the feedback effectively. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.",2,"The review point claims that the novelty of the paper is vague and suggests that it integrates elements from previous works such as SCL, Aligned, and CLNN. However, it does not provide specific details or references to these previous works to substantiate the claim. The lack of explicit examples or detailed comparisons makes it difficult for the authors to fully understand and address the critique. Therefore, the comment is considered borderline verifiable, as it provides some context but lacks sufficient evidence or reasoning to be fully verifiable.",5415,,,,,,,,
ICLR_2021_1189,ICLR_2021,"weakness of the paper is its experiments section. 1. Lack of large scale experiments: The models trained in the experiments section are quite small (80 hidden neurons for the MNIST experiments and a single convolutional layer with 40 channels for the SVHN experiments). It would be nice if there were at least some experiments that varied the size of the network and showed a trend indicating that the results from the small-scale experiments will (or will not) extend to larger scale experiments. 2. Need for more robustness benchmarks: It is impressive that the Lipschitz constraints achieved by LBEN appear to be tight. Given this, it would be interesting to see how LBEN’s accuracy-robustness tradeoff compare with other architectures designed to have tight Lipschitz constraints, such as [1]. 3. Possibly limited applicability to more structured layers like convolutions: Although it can be counted as a strength that LBEN can be applied to convnets without much modification, the fact that its performance considerably trails that of MON raises questions about whether the methods presented here are ready to be extended to non-fully connected architectures. 4. Lack of description of how the Lipschitz bounds of the networks are computed: This critique is self-explanatory.Decision: I think this paper is well worthy of acceptance just based on the quality and richness of its theoretical development and analysis of LBEN. I’d encourage the authors to, if possible, strengthen the experimental results in directions including (but certainly not limited to) the ones listed above.Other questions to authors: 1. I was wondering why you didn’t include experiments involving larger neural networks. What are the limitations (if any) that kept you from trying out larger networks? 2. Could you describe how you computed the Lipschitz constant? Given how notoriously difficult it is to compute bounds on the Lipschitz constants of neural networks, I think this section requires more elaboration.Possible typos and minor glitches in writing: 1. Section 3.2, fist paragraph, first sentence: Should the phrase “equilibrium network” be plural? 2. D^{+} used in Condition 1 is used before it’s defined in Condition 2. 3. Just below equation (7): I think there’s a typo in “On the other size, […]”. 4. In Section 4.1, \epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \epsilon when (11) is discussed. 5. Section 4.2, in paragraph “Computing an equilibrium”, first sentence: Do you think there’s a grammar error in this sentence? I might also have mis-parsed the sentence. 6. Section 5, second sentence: There are two “the”s in a row.[1] Anil, Cem, James Lucas, and Roger Grosse. ""Sorting out lipschitz function approximation."" International Conference on Machine Learning. 2019.","3. Possibly limited applicability to more structured layers like convolutions: Although it can be counted as a strength that LBEN can be applied to convnets without much modification, the fact that its performance considerably trails that of MON raises questions about whether the methods presented here are ready to be extended to non-fully connected architectures.",2146,1,"The review point raises a concern about the applicability of LBEN to more structured layers like convolutions, noting that its performance lags behind MON. However, it does not provide any explicit guidance or suggestions on how the authors might address this issue or improve the applicability of their methods. The comment lacks specific actions or recommendations, leaving the authors uncertain about how to proceed. As a result, the comment is unactionable.",3,"The comment identifies a potential limitation in the applicability of the methods to more structured layers like convolutions. It acknowledges a strength in the ability of LBEN to be applied to convnets with minimal modification but raises concerns about its performance compared to MON. This feedback is somewhat helpful as it highlights an area of concern that the authors should address, potentially by providing further justification or exploring improvements. However, the comment lacks specific suggestions or guidance on how to address the performance gap or enhance the applicability of the methods, limiting its usefulness. Therefore, the comment aligns with a score of 3.",3,"The comment raises a concern about the applicability of LBEN to more structured layers like convolutions and compares its performance to MON. It questions whether the methods are ready to be extended to non-fully connected architectures. However, the comment does not specify which part of the paper discusses LBEN's application to convnets or where the performance comparison with MON is presented. This makes it challenging for the authors to pinpoint the exact sections that need attention. While the comment is specific about the issue of applicability and performance, it lacks grounding as it does not explicitly mention the sections or figures involved. Therefore, this comment is weakly grounded and specific, aligning with category 3.",3,"The review point raises a concern about the limited applicability of LBEN to more structured layers like convolutions, noting that its performance lags behind MON. This claim is somewhat verifiable as it highlights a potential limitation based on performance comparisons. However, the comment lacks specific examples, data, or references to substantiate the claim fully. Providing detailed comparisons or citing studies where MON outperforms LBEN would strengthen the argument. Therefore, the comment is classified as 3: Somewhat Verifiable.",8198,,,,,,,,
VhQUwxIHER,ICLR_2024,"•The paper only considers accuracy disparities as fairness metrics. At the beginning of the paper, the authors mention fairness metrics such as equalized odds; however, such metrics are not considered. The authors focus on accuracy disparities and do not provide results for other fairness metrics of major interest in the literature. This reviewer strongly suggests the authors replace two of the metrics in Table 1 with other fairness metrics (e.g., equal odds and statistical parity).•Table 1 provides results for the performance of different fairness improvement techniques. However, the reported values are very close. Therefore, it is hard to get any conclusion out of the reported values. Can the authors please add confidence intervals?","• Table 1 provides results for the performance of different fairness improvement techniques. However, the reported values are very close. Therefore, it is hard to get any conclusion out of the reported values. Can the authors please add confidence intervals?",4580,5,"The review point explicitly suggests that the authors should add confidence intervals to the results in Table 1, where the performance of different fairness improvement techniques is reported. This suggestion is clear and provides a concrete action for the authors to take, as it directly addresses the issue of the reported values being too close to draw meaningful conclusions. By specifying the addition of confidence intervals, the comment gives the authors a precise way to improve the clarity and interpretability of their results. Therefore, this comment is highly actionable.",4,"The review comment identifies a specific issue with the presentation of results in Table 1, noting that the reported values for different fairness improvement techniques are very close, making it difficult to draw conclusions. The comment provides a clear and actionable suggestion by asking the authors to add confidence intervals to the results. This feedback is valuable as it guides the authors on how to enhance the clarity and interpretability of their results, making it mostly helpful. However, it could be even more helpful if it provided additional context or suggestions on how to interpret the results with confidence intervals. Overall, the comment is rated as mostly helpful, aligning with a score of 4.",5,"The comment is fully grounded as it explicitly mentions ""Table 1,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue with the reported values being very close, making it difficult to draw conclusions. The comment provides a specific suggestion to add confidence intervals to address this issue. This gives clear guidance on what needs to be improved in the paper. Therefore, the comment is fully grounded and specific, corresponding to label 5.",3,"The review point suggests that the reported values in Table 1 are very close, making it difficult to draw conclusions. The reviewer recommends adding confidence intervals to provide more clarity. This claim is somewhat verifiable as it logically suggests a method (adding confidence intervals) to enhance the interpretability of the results. However, the comment lacks specific examples or references to support why confidence intervals would be beneficial in this context, leaving the authors to infer the full rationale. Therefore, the comment aligns with a label of 3: Somewhat Verifiable.",9962,,,,,,,,
NiNIthntx7,ICLR_2025,"This paper has several weaknesses regarding the evaluation methodologies.1. The tasks are relatively easy:- For most of the tasks given in the anonymous GitHub, I've learned that LLMs cannot even move some code snippets to another file or add additional parameters to one function. Is that correct?- In `problems/ansible_refactor/sort-groups-to-group-sort-task.txt`: Rename `sort_groups` to `group_sort` and update all references in the repo.- In `problems/ansible_refactor/add-log-parameter-get-group-vars-task.txt`: Add a log parameter to `get_group_vars,` keep the base case as true, and update the rest of the repository calls to have `log=False.`- $\ldots$- From my experience using Cursor and GitHub Copilot, the observation in this paper is not true. Most refactoring like these can be solved with one click of the mouse.- For example, I can modify the original function using the Cursor and open all the relevant files (i.e., Ctrl+F for that specific function). The Cursor will devise an auto-completion suggestion to refactor all usage of this function.I am pretty sure that if our language agent is equipped with some retrieval tools and employs a hierarchical structure (i.e., multi-agent) in which each agent updates one file separately, it would largely improve performance and consistency.2. Bad definition of refactoring:- Martin Fowler's ""Repetition is the root of all software evil"" is a good point. However, I did not see any prompt examples of reducing repetition within the refactor tasks. From my daily experience, a good refactor should reduce the complexity and repetition of a large codebase and increase the portability of modules and functions. It is not about adding a logger to a function or moving some code to `utils.py.`- The refactor should not be evaluated using unit tests only. Unlike the bug-fixes scenario in SWE-Bench, the function before refactoring should also pass the unit test. The objective should be, for example, to reduce the length/loops of the initial code after refactoring while still passing the unit test.- I don't have a clear idea of how to evaluate the refactoring process, but it is obvious that using a pass rate for unit tests is insufficient. Below are some potential metrics you can choose from (maybe for your next benchmark).- *Test Execution Time*: After refactoring, measure if the test suite executes faster.- *Linting and Style Compliance*: Assess adherence to coding standards. (This might not be helpful because most code generated by LLM should follow linting compliance).- *Lines of Code (LOC)*: While not always directly indicative of quality, a reduction in LOC often reflects simplification.- *Function Size*: Track average function length; shorter, well-named functions are generally easier to understand and test.- *Coupling Metrics*: Evaluate coupling using Coupling Between Objects (CBO) measures. (I hate the coupling of my code. So, if the LLMs can solve that, it will be really helpful.)- *Dependency Analysis*: Check the number and nature of dependencies between modules. Reducing unnecessary dependencies enhances modularity and flexibility in the code.3. Other frameworks worthy to test on [1, 2, 3]- Aider [1] also has a good way of code edits and refactoring using `git diff` [4].- Papers such as [2, 5] evaluate their performance on multiple agent frameworks.[1] P. Gauthier. How aider scored sota 26.3. https://aider.chat/2024/05/22/swe-bench-lite.html. Accessed: 2024-06-05.[2] Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., ... & Neubig, G. (2024). OpenHands: An Open Platform for AI Software Developers as Generalist Agents. arXiv preprint arXiv:2407.16741.[3] Xia, C. S., Deng, Y., Dunn, S., & Zhang, L. (2024). Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489.[4] P. Gauthier. Unified diffs make GPT-4 Turbo 3X less lazy. https://aider.chat/2023/12/21/unified-diffs.html. Accessed: 2024-10-31.[5] Liu, Y., Tang, X., Cai, Z., Lu, J., Zhang, Y., Shao, Y., ... & Gerstein, M. (2023). ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks. arXiv e-prints, arXiv-2311.","- I don't have a clear idea of how to evaluate the refactoring process, but it is obvious that using a pass rate for unit tests is insufficient. Below are some potential metrics you can choose from (maybe for your next benchmark).",10058,3,"The comment acknowledges a lack of clarity on how to evaluate the refactoring process and suggests that using a pass rate for unit tests is insufficient. It implies that the authors should consider alternative metrics for evaluation, but it does not provide specific guidance on which metrics to use or how to implement them. The mention of potential metrics for a future benchmark suggests an action, but it remains vague and lacks concrete details. Therefore, the comment is somewhat actionable, as it points out a direction for improvement but does not offer detailed steps for execution.",2,"The review comment acknowledges a gap in the evaluation of the refactoring process, specifically pointing out the insufficiency of using a pass rate for unit tests. However, it does not provide any specific metrics or suggestions for improvement, leaving the authors without actionable guidance. While it hints at potential metrics, it does not elaborate on them or offer concrete examples, making it difficult for the authors to implement changes based on this feedback. Therefore, the comment is barely helpful, as it identifies an issue but lacks the depth and specificity needed to guide the authors effectively.",1,"The comment suggests that using a pass rate for unit tests is insufficient for evaluating the refactoring process and implies that alternative metrics should be considered. However, it does not specify which part of the paper this feedback pertains to, nor does it provide detailed guidance on what specific metrics could be used or how they should be implemented. Without clear references or detailed suggestions, the authors cannot effectively identify the areas needing improvement. As a result, the comment lacks both grounding and specificity, making it not grounded at all. Therefore, this comment aligns with a score of 1.",1,"The review point claims that ""using a pass rate for unit tests is insufficient"" for evaluating the refactoring process. However, it does not provide any supporting evidence, reasoning, or examples to justify why the pass rate is inadequate. While the reviewer suggests considering other potential metrics, the lack of specific examples or detailed justification makes the claim difficult for authors to verify and understand the basis of the critique. Therefore, the comment is considered unverifiable.",1449,,,,,,,,
ARR_2022_247_review,ARR_2022,"- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text. - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains.Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?Typos: - 166: Space after footnote 5.- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""?",- The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).,1133,5,"The review point suggests that the paper is dense and difficult to read in parts, and it provides a specific suggestion to improve readability by converting numbered lists within paragraphs into actual lists where appropriate. This feedback is explicit and provides a concrete action for the authors to take, giving them clear guidance on how to enhance the readability of their paper. Therefore, this comment is highly actionable.",4,"The comment identifies a potential issue with the paper's readability, noting that the dense nature of the text makes it difficult to navigate. It offers a specific and actionable suggestion to improve readability by converting numbered lists within paragraphs into actual lists where appropriate. This feedback is clear and provides the authors with a concrete way to enhance the presentation of their work, making it easier for readers to follow. However, the comment could be more helpful if it provided examples of specific sections where this change would be most beneficial. Overall, the feedback is mostly helpful as it offers a practical suggestion for improving the draft's readability, aligning with a score of 4.",2,"The comment provides a general observation about the paper's density and suggests a specific improvement regarding the use of numbered lists. However, it does not specify which sections or paragraphs are particularly dense or difficult to read, nor does it identify where the numbered lists could be better formatted. This lack of specific references makes it challenging for the authors to pinpoint the exact areas needing revision. Therefore, the comment is weakly grounded because it does not specify the exact parts of the paper being addressed, but it is somewhat specific in suggesting a potential improvement. This aligns with a score of 2.",1,"The review point claims that the paper is ""very dense"" and suggests that using actual lists instead of numbered lists within paragraphs could improve readability. However, the comment does not provide specific examples or detailed reasoning to support the claim that the paper's density is problematic or that the suggested change would significantly enhance readability. The lack of specific evidence or examples makes it difficult for the authors to understand the basis of the suggestion and how to implement it effectively. Therefore, the claim is not verifiable due to the absence of supporting information, making it challenging for the authors to address the concern. Consequently, the feedback is categorized as 1: Unverifiable.",3277,,,,,,,,
KZZbdJ4wff,ICLR_2024,"- Just combining existing methods, by trying everything and finding a good combination, might not be considered novel.- While this paper empirically validates which setups/parameters are useful for test time training, it does not provide theoretical understanding on why they do not collapse.- Details of algorithm (PRO) are missing. It is unclear how the loss functions are combined.","- While this paper empirically validates which setups/parameters are useful for test time training, it does not provide theoretical understanding on why they do not collapse.",4981,1,"The review comment points out a lack of theoretical understanding in the paper regarding why certain setups or parameters do not collapse during test time training. However, it does not provide specific guidance or suggestions on how the authors could address this issue or improve the theoretical aspect of their work. The comment lacks actionable details, such as recommending ways to enhance the theoretical understanding or suggesting specific areas to explore. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",3,"The comment identifies a significant gap in the paper, noting that while empirical validation is provided for certain setups and parameters in test time training, there is a lack of theoretical understanding explaining why these setups do not collapse. This feedback highlights an important area for improvement, encouraging the authors to delve deeper into the theoretical aspects of their work. However, the comment does not offer specific guidance or suggestions on how to address this issue, such as recommending particular theoretical frameworks or analyses that could be explored. As a result, the comment is somewhat helpful, as it points out a critical area for enhancement but lacks detailed, actionable advice. Therefore, it aligns with a score of 3.",3,"The comment highlights a lack of theoretical understanding regarding why certain setups or parameters do not collapse during test time training. However, it does not specify which part of the paper this issue pertains to, making it difficult for the authors to pinpoint where improvements are needed. The comment is specific in identifying the need for theoretical understanding but lacks grounding as it does not reference a particular section or aspect of the paper. Therefore, this comment is weakly grounded and specific, aligning with the label 3.",1,"The review point claims that the paper lacks a theoretical understanding of why certain setups/parameters do not collapse during test time training, despite providing empirical validation. However, the comment does not offer any supporting evidence, reasoning, or examples to substantiate this claim. Without additional context or references to theoretical frameworks or studies that could explain the phenomenon, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.",3977,,,,,,,,
ICLR_2022_353,ICLR_2022,"Weakness: 1. It is an interesting phenomenon that ViT supernet training suffers from the gradient conflict issue. But I am curious about the performance of other supernet training methods that don’t utilize KD from supernet, like once-for-all (ICLR20), DNANAS (CVPR20) and Cream (NeurIPS20). 2. I am not clear why the authors introduce the switchable scaling. I don’t see the relationship between it and the gradient conflict issue, also the improvement in Table 7 looks marginal. Also Figure 3 is not very clear, e.g., what does c1 and c2 means? 3. There is no experiment to support the statement “the supernet and the sub-networks are more likely to conflict with each other in the presence of stronger data augmentations and strong regularization.”. There is performance improvement shown in Table 7, but it is not clear whether this is caused by the conflict between supernet and sub-networks. And if so, I am wondering is there still an improvement for weak DA & Reg if the gradient projection is already applied. 4. In Table 8, it seems that AlphaNets trained with alpha-divergence doesn’t benefit from your method. Maybe you should show the gradient conflicts ratio for it to give some insights. 5. Since the gradient projection leads to slow convergence, the authors should provide the training time cost before and after using the proposed gradient projection, as the training time cost is also important. 6. How about the training time consuming after using the other two techniques. Does it reduce the training time caused by gradient projection for its slow convergence? Detailed information should be presented. 7. Why do the authors use EfficientNet-B5 as the teacher? Why not B6，B7 or other networks? The reasons are not mentioned in this paper. It is confusing. 8. To be more general and convincing, though the evaluations of the smallest and the largest networks are provided, I think the results of the middle-size network in needed experiments are also critical. For example, in Fig 2, Tab 3, and so on. 9. The authors only adopt gradient projection on CNN-based supernet while conducting ablation studies. The generalizability of the other two techniques in CNN should be verified as well like Tab.7.","4. In Table 8, it seems that AlphaNets trained with alpha-divergence doesn’t benefit from your method. Maybe you should show the gradient conflicts ratio for it to give some insights.",2601,5,"The comment suggests that the authors should show the gradient conflicts ratio for AlphaNets trained with alpha-divergence, as it seems these do not benefit from the method discussed. This suggestion is explicit and provides a concrete action for the authors to take, namely to include the gradient conflicts ratio to provide insights. The feedback is clear and directly actionable, as it specifies what additional information should be included to enhance the understanding of the results. Therefore, this comment is highly actionable.",4,"The comment identifies a specific observation in Table 8, noting that AlphaNets trained with alpha-divergence do not seem to benefit from the proposed method. It suggests a potential way to gain insights by showing the gradient conflicts ratio. This feedback is clear and actionable, as it provides a concrete suggestion for additional analysis that could help the authors understand and address the issue. However, it could be more helpful if it explained why showing the gradient conflicts ratio might be beneficial or provided further guidance on interpreting the results. Overall, the comment is mostly helpful as it directs the authors towards a specific area for further investigation, aligning with a score of 4.",5,"The comment is fully grounded as it explicitly mentions ""Table 8,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue by pointing out that AlphaNets trained with alpha-divergence do not seem to benefit from the method and suggests showing the gradient conflicts ratio to provide insights. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with a score of 5.",3,"The review point suggests that AlphaNets trained with alpha-divergence do not benefit from the proposed method, as indicated by the results in Table 8. The reviewer proposes showing the gradient conflicts ratio to provide insights. While the comment identifies a potential issue and suggests a way to gain further understanding, it lacks detailed reasoning or evidence to support the claim that the method does not benefit AlphaNets. The suggestion to show the gradient conflicts ratio is a logical step, but without further explanation or references, the claim remains somewhat verifiable. The authors would benefit from more detailed justification to fully understand and address the comment.",8963,,,,,,,,
Pd3jVGTacT,ICLR_2025,"1. The technical contributions are sound but limited. The authors propose a reference-free optimization framework. However, optimization with a reference-free objective is not novel and has already been proposed by SimPO. Moreover, the length-normalized reward and reward margin parameter in SimNPO are also derived from SimPO.2. SimNPO introduces two additional hyperparameters that must be manually adjusted to achieve optimal performance, significantly increasing the complexity and time cost of experiments. This greatly limits the potential application of this method for large language models in real-world settings.3. Is there any theoretical analysis or intuition? What is the divergence rate of SimNPO compared to NPO? The current version of this paper only provides an experimental report without solid analysis.4. A potential issue is over-reliance on the reference model; would adjusting the weight of the reference model help mitigate this problem? While removing the reference model entirely might solve the over-reliance issue, could it lead to insufficient performance for certain cases, such as math and coding datasets?","2. SimNPO introduces two additional hyperparameters that must be manually adjusted to achieve optimal performance, significantly increasing the complexity and time cost of experiments. This greatly limits the potential application of this method for large language models in real-world settings.",9969,1,"The review point highlights a limitation of the SimNPO method, noting that it introduces two additional hyperparameters that increase complexity and time cost, thus limiting its applicability to large language models in real-world settings. However, the comment does not provide any explicit or implicit suggestions for how the authors might address this limitation or improve their approach. There is no guidance on potential modifications, alternative methods, or strategies to mitigate the complexity and time cost. As a result, the authors are left without any actionable steps to take in response to this feedback. Therefore, the comment is rated as unactionable.",3,"The review comment identifies a specific issue with the SimNPO method, noting that it introduces two additional hyperparameters that require manual adjustment. This increases the complexity and time cost of experiments, which could limit the method's applicability to large language models in real-world settings. While the comment highlights a valid concern, it lacks actionable suggestions or guidance on how the authors might address this issue, such as proposing ways to automate the hyperparameter tuning or suggesting alternative approaches to reduce complexity. Therefore, the comment is somewhat helpful as it points out a potential limitation but does not provide comprehensive guidance for improvement.",3,"The comment specifically addresses the introduction of two additional hyperparameters in SimNPO, highlighting the increased complexity and time cost associated with manually adjusting them. While it does not explicitly mention a specific section of the paper, the authors can infer that it pertains to the methodology or results sections where these hyperparameters are discussed. The comment is specific in detailing the issue of increased complexity and its impact on real-world applications. Therefore, the comment is weakly grounded but specific, aligning with category 3.",3,"The review point claims that the introduction of two additional hyperparameters in SimNPO increases complexity and time cost, limiting its applicability to large language models. The reasoning is based on the logical expectation that more hyperparameters can complicate optimization and increase computational demands. However, the comment lacks specific examples, data, or references to support the claim, making it somewhat verifiable. The authors would need to infer the implications without detailed evidence, thus justifying a score of 3: Somewhat Verifiable.",6905,,,,,,,,
NIPS_2020_300,NIPS_2020,"- The method has been tested on a single architecture even if, as the authors states at line 243, new architecture like PSPNet might perform even better. - Some more recent works that might be highly correlated needs to be cited and compared against, for example this work “Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation - Wang et al. cvpr2020” might share some similarity with the proposed method that need to be discussed in context. - The method has been tested only among synthetic and real datasets that share quite a lot of similarities. It’s a little bit unclear how well the proposed feature matching strategy generalizes to datasets with bigger domain discrepancies. - Some implementation details can be made more clear in the paper","- Some more recent works that might be highly correlated needs to be cited and compared against, for example this work “Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation - Wang et al. cvpr2020” might share some similarity with the proposed method that need to be discussed in context.",11241,3,"The review comment suggests that more recent works, such as ""Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation - Wang et al. cvpr2020,"" should be cited and compared against the proposed method. The reviewer implies that there may be similarities that need to be discussed in context. While the action of citing and comparing is explicit, the comment does not provide detailed guidance on how to conduct the comparison or what specific aspects should be discussed. This leaves the authors with a clear action to take but without concrete instructions on how to implement it. Therefore, the comment is somewhat actionable, as it identifies an area for improvement but lacks detailed guidance on execution.",4,"The review comment identifies a gap in the literature review by suggesting that more recent works, such as the one by Wang et al. (CVPR 2020), should be cited and compared against the proposed method. It highlights the potential similarity between the cited work and the proposed method, indicating that a discussion of this similarity is necessary. This feedback is clear and actionable, as it provides a specific reference and suggests a direction for expanding the literature review and contextualizing the proposed method. However, it could be more helpful if it offered additional guidance on how to integrate this comparison into the paper. Overall, the comment is mostly helpful as it directs the authors to enhance their draft by acknowledging relevant recent work.",3,"The comment suggests that more recent works should be cited and compared against, specifically mentioning the work by Wang et al. from CVPR 2020. This provides a clear example of what the authors should consider, making the comment specific. However, it does not explicitly mention which part of the paper should include these citations or comparisons, leaving the authors to infer where this might be relevant. This lack of explicit grounding makes it weakly grounded. Therefore, the comment is weakly grounded and specific, aligning with the label 3.",3,"The review point suggests that more recent works, such as ""Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation"" by Wang et al. (CVPR 2020), should be cited and compared against the proposed method. The claim is somewhat verifiable as it identifies a specific work that might share similarities with the proposed method, indicating a need for discussion. However, it lacks detailed reasoning or examples of how the mentioned work is similar or relevant, leaving the authors to deduce the connections themselves. This makes the comment somewhat verifiable, meriting a score of 3: Somewhat Verifiable.",2538,,,,,,,,
NIPS_2022_2292,NIPS_2022,"Some possible discrepancy between the proposed algorithm and how it is actually implemented: the average for the variance reduction is a rolling average in practice, and this is not covered by the theory.Rate for SOBA is not optimal and it is not clear why.Experiments are quite small in scale and can be slightly improved. No experiment under the PL-condition. No plots showing how the performance varies w.r.t. the number of single-sample gradients and hessian-vector products.Some of the points are expanded in the Questions section.[1] Li, Junyi, Bin Gu, and Heng Huang. ""A fully single loop algorithm for bilevel optimization without hessian inverse."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 7. 2022.Post authors’ response.The authors have thoroughly addressed my concerns in their revision and response. In particular, concerning weakness 2, they now achieve optimal rates for SOBA under additional smoothness assumptions which are still realistic. Therefore I accordingly increased the score from 7 to 8.Limitations are only partially addressed. The non standard finite sum assumption is properly discussed in the theoretical analysis section but could be anticipated in the abstract. The discrepancy between the theoretical and practical updates of SABA is mentioned briefly and not properly addressed (See Question section).","36. No.7. 2022. Post authors’ response. The authors have thoroughly addressed my concerns in their revision and response. In particular, concerning weakness 2, they now achieve optimal rates for SOBA under additional smoothness assumptions which are still realistic. Therefore I accordingly increased the score from 7 to 8. Limitations are only partially addressed. The non standard finite sum assumption is properly discussed in the theoretical analysis section but could be anticipated in the abstract. The discrepancy between the theoretical and practical updates of SABA is mentioned briefly and not properly addressed (See Question section).",8280,3,"The review point acknowledges that the authors have addressed some concerns, particularly achieving optimal rates for SOBA under additional assumptions. However, it also notes that some limitations are only partially addressed, such as the non-standard finite sum assumption and the discrepancy between theoretical and practical updates of SABA. While the reviewer suggests that the non-standard assumption could be anticipated in the abstract, it does not provide specific guidance on how to address the discrepancy between theoretical and practical updates. The comment lacks explicit instructions or concrete suggestions for improvement, leaving the authors with only a vague understanding of what further actions to take. Therefore, this comment is somewhat actionable.",3,"The review point provides feedback on the authors' response to previous concerns, noting that the authors have addressed certain weaknesses, such as achieving optimal rates for SOBA under additional smoothness assumptions. This acknowledgment is helpful as it confirms that the authors have made improvements. However, the comment also points out that some limitations are only partially addressed, specifically mentioning the non-standard finite sum assumption and the discrepancy between theoretical and practical updates of SABA. While it identifies areas that still need attention, the comment lacks specific suggestions or guidance on how to address these remaining issues. The feedback is somewhat helpful as it provides insight into what has been improved and what still needs work, but it could be more actionable with detailed recommendations. Therefore, it is rated as somewhat helpful.",2,"The review comment acknowledges that the authors have addressed certain concerns, specifically mentioning ""weakness 2"" and the achievement of optimal rates for SOBA under additional assumptions. However, it does not clearly specify which parts of the paper these changes pertain to, making it difficult for the authors to pinpoint the exact sections. The comment also mentions limitations that are only partially addressed, but it lacks specific guidance on how to improve these areas. While it references the ""theoretical analysis section"" and suggests anticipating certain discussions in the abstract, the overall feedback remains vague. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.",3,"The review point acknowledges that the authors have addressed some concerns, particularly regarding achieving optimal rates for SOBA under additional smoothness assumptions. However, it also notes that some limitations are only partially addressed, such as the non-standard finite sum assumption and the discrepancy between theoretical and practical updates of SABA. While the comment provides some reasoning for the increased score, it lacks specific examples or detailed evidence to fully verify the claims about the limitations and discrepancies. Therefore, the review point is somewhat verifiable, as it provides partial support but could benefit from more detailed justification.",8716,,,,,,,,
NIPS_2021_2082,NIPS_2021,"weakness of the existing works. 3. In Table 1 we can see that, besides the CAMELYON16 dataset, the baseline MIL-based methods showed much lower performances than max-pooling. Please give some discussions about the reason. 4. For ablation study, The Table 2 and Fig. 5 were not mentioned in the manuscript. What do the values stand for in Table 2 and Fig. 5? Why giving the detailed discussion in Appendix? I suggest move the discussion to the main manuscript. 5. For Fig.6, What is the purpose to show the zoom-in view of heatmap? I cannot see anything special in this area. 6. For Fig. 7, the initial accuracy of MIL-based baseline model were higher than the converged models, especially for the NSCLC dataset, why?","4. For ablation study, The Table 2 and Fig. 5 were not mentioned in the manuscript. What do the values stand for in Table 2 and Fig. 5? Why giving the detailed discussion in Appendix? I suggest move the discussion to the main manuscript.",7881,5,"The review comment raises several points regarding the ablation study, specifically noting that Table 2 and Fig. 5 are not mentioned in the manuscript and questioning what the values in these figures represent. The reviewer also questions why the detailed discussion is placed in the Appendix and suggests moving it to the main manuscript. This comment provides explicit actions for the authors to take, such as mentioning the figures in the manuscript and relocating the discussion from the Appendix to the main text. The guidance is clear and specific, making the comment highly actionable.",4,"The comment identifies a specific issue with the manuscript, noting that Table 2 and Figure 5, which are part of the ablation study, are not mentioned in the main text. It questions the meaning of the values presented in these figures and suggests that the detailed discussion currently in the Appendix should be moved to the main manuscript. This feedback is clear and actionable, as it provides the authors with a concrete suggestion to improve the clarity and accessibility of their findings. By addressing this comment, the authors can enhance the comprehensiveness and coherence of their manuscript. However, the comment could be more helpful if it provided additional guidance on how to integrate the discussion into the main text effectively. Overall, it is mostly helpful.",5,"The comment is fully grounded as it explicitly mentions ""Table 2"" and ""Fig. 5,"" allowing the authors to accurately identify the parts of the paper being addressed. It also specifies the issue, which is the lack of mention and explanation of these elements in the main manuscript, and suggests moving the detailed discussion from the Appendix to the main manuscript. This provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with a score of 5.",3,"The review point raises a concern about the ablation study, specifically noting that Table 2 and Fig. 5 were not mentioned in the manuscript. It questions what the values in these figures represent and why the detailed discussion is relegated to the Appendix, suggesting that it should be moved to the main manuscript. While the comment identifies a potential issue with the presentation of the study, it lacks specific reasoning or evidence to support why this change is necessary or how it would improve the manuscript. The absence of detailed justification makes the claim somewhat verifiable, as the authors would need to infer the underlying reasons for the suggestion. Therefore, the comment is rated as 3: Somewhat Verifiable.",8913,,,,,,,,
ICLR_2021_425,ICLR_2021,"It seems like a time complex scheme for decoding based on the formulationLacks generated samples in the main paper for qualitative assessment.Human evaluation missing. Questions:Current neural text generation models are trained to optimize the cross-entropy loss. In an ideal scenario lower the cross-entropy (hence perplexity), the better should be the samples. Since this work (as well as its corresponding related works) show that humans don't form sentences that have the lowest perplexity (when measured using the trained model), does this work help in highlighting that cross-entropy loss might not be the correct objective for optimization?In Algo 1: Do you first perform top-m for obtaining s and then perform top-k later based on the value obtained? If so, what is the time complexity of this process, as a function of m?From Fig 4(c), it seems that humans prefer sentences having cross-entropy rates ~ 5.25 on avg. Why do you suggest 3 as the target average surprise?The point about ad-hoc settings being applied for top-p and not for mirostat seems weird. This paper suggests 3.0 as the setting for target average surprise (based on empirical data) while top-p suggests ~0.9 as the setting for p (again based on empirical data - and shown in this paper that CE remains more or less constant wrt the number of generated tokens at p=0.9). In the end, a certain bit of ad-hoc tuning needs to be made for both cases. Why would this algorithm be better than nucleus sampling (Given it is more time consuming)? Overall:a) The theoretical analysis helps in understanding why top-k and top-p perform the way they do.b) Based on the underlying assumptions:Zipfian distributioncross-entropy loss optimizationLowest perplexity does not always mean good quality generation.(I somehow feel a lot of disconnect between assumption 2 and 3), this work proposes an algorithm for adaptive top-k decoding to produce arguably ""better"" samples.Please let me know if I am misunderstanding something. I am open to revising my assessment based on the answers to the questions raised. Update:Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m.","3), this work proposes an algorithm for adaptive top-k decoding to produce arguably ""better"" samples. Please let me know if I am misunderstanding something. I am open to revising my assessment based on the answers to the questions raised. Update: Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m.",2116,5,"The review point initially raises a question about the proposed algorithm for adaptive top-k decoding, indicating a willingness to revise the assessment based on the authors' responses. The update suggests that the reviewer has adjusted their score after receiving feedback from the authors. Additionally, the reviewer explicitly requests a plot of the time complexity of the decoding scheme as a function of m. This request is clear and provides a specific action for the authors to take, making it highly actionable. The authors know exactly what additional information to provide to address the reviewer's comment.",3,"The review comment initially raises a question about the proposed algorithm for adaptive top-k decoding, indicating a willingness to revise the assessment based on the authors' responses. This openness to dialogue is positive, but the comment lacks specific feedback or suggestions for improvement. However, the update provides a constructive suggestion to include a plot of time complexity as a function of m, which is actionable and could enhance the paper's analysis. While the initial part of the comment is vague, the update adds value by offering a clear direction for improvement. Overall, the comment is somewhat helpful, as it provides some actionable feedback but could be more comprehensive in its initial critique.",4,"The comment refers to ""adaptive top-k decoding,"" which allows the authors to identify the specific part of the paper being discussed, providing full grounding. It also suggests a specific improvement by requesting a plot of the time complexity of the decoding scheme as a function of m. However, the initial part of the comment is somewhat vague, as it asks for clarification on whether the understanding of the algorithm is correct, without specifying what aspects are unclear. The update indicates that the reviewer has revised their assessment based on the author's response, but it does not provide additional specificity on what was initially misunderstood. Overall, the comment is fully grounded due to the mention of ""adaptive top-k decoding"" and the request for a plot, but it is under-specific regarding the initial misunderstanding. Therefore, this aligns with a score of 4.",1,"The review point makes a claim about the proposed algorithm for adaptive top-k decoding producing ""better"" samples. However, the comment lacks specific evidence, reasoning, or examples to support this claim. The reviewer also expresses uncertainty and openness to revising their assessment, indicating that the claim is not fully substantiated. Additionally, the request for a plot of time complexity suggests a need for further evidence to support the claim. Due to the lack of detailed justification or examples, the comment is considered unverifiable.",7788,,,,,,,,
JU9oHs7ivN,ICLR_2025,"1. While CCKT-Det++ performs well by leveraging stronger teacher models, it is heavily reliant on the quality and alignment of these teacher models. If the teacher model has limitations or biases, these could propagate to CCKT-Det++, affecting performance and potentially introducing unintended biases.2. Although the student model maintains moderate parameters, using stronger teacher models during training and inference can introduce significant computational costs. Scaling the teacher models effectively requires access to substantial computational resources, which could limit practical applications for researchers without high-performance infrastructure.3. The approach relies on pseudo annotations, which are less accurate than human annotations. If the pseudo labels are of low quality, they could introduce noise into the training process, possibly harming model performance on more challenging or nuanced object classes.4. While semantic priors improve performance on novel objects, there is a risk that the model might over-rely on these priors, leading to reduced flexibility in identifying objects that fall outside its learned semantic scope. This dependency may affect its generalization to truly unseen classes in new environments.5. The method heavily relies on CLIP for both semantic priors and feature extraction, making it vulnerable to limitations inherent in the CLIP model. For instance, CLIP's biases in language and visual associations could impact detection accuracy and lead to incorrect classifications in culturally or contextually sensitive settings.","1. While CCKT-Det++ performs well by leveraging stronger teacher models, it is heavily reliant on the quality and alignment of these teacher models. If the teacher model has limitations or biases, these could propagate to CCKT-Det++, affecting performance and potentially introducing unintended biases.",9935,1,"The review point highlights a potential issue with the reliance of CCKT-Det++ on the quality and alignment of teacher models, noting that limitations or biases in the teacher models could affect performance and introduce unintended biases. However, it does not provide specific guidance or suggestions on how the authors could address this issue or mitigate the potential biases. The comment lacks actionable details, such as recommending ways to evaluate or improve the alignment of teacher models or suggesting methods to identify and mitigate biases. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",3,"The review comment identifies a potential issue with the CCKT-Det++ model, specifically its reliance on the quality and alignment of teacher models. It highlights the risk of propagating limitations or biases from the teacher models to CCKT-Det++, which could affect performance and introduce unintended biases. This feedback is valuable as it points out a critical aspect of the model that the authors should consider addressing. However, the comment lacks specific suggestions or guidance on how the authors might mitigate these risks or improve the robustness of their model. While it provides insight into a potential weakness, it does not offer actionable advice, making it somewhat helpful. Therefore, the comment aligns with a score of 3.",3,"The comment discusses the performance of CCKT-Det++ and its reliance on teacher models, highlighting potential issues with limitations or biases in these models. However, it does not specify which part of the paper this observation pertains to, such as specific sections, experiments, or results. This makes it weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. The comment is specific in detailing the potential issue with reliance on teacher models, but without grounding, it is less actionable. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.",3,"The review point claims that the performance of CCKT-Det++ is heavily reliant on the quality and alignment of the teacher models, and that limitations or biases in these models could affect CCKT-Det++'s performance. The comment provides a logical reasoning for the claim by explaining how the reliance on teacher models could lead to the propagation of biases. However, it lacks specific examples, detailed explanations, or references to external sources that would fully substantiate the claim. Therefore, the comment is somewhat verifiable, as it provides a reasonable argument but requires more detailed evidence or examples to fully support the claim.",5736,,,,,,,,
NIPS_2019_933,NIPS_2019,"+ I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms. Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.","- Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret?",6810,3,"The review comment raises a question about the exploration parameter gamma in Algorithm 1 and Theorem 3.1, suggesting that it might be unnecessary since only upper bounds on the pseudo-regret are provided. The reviewer proposes that the choice of gamma=0 might be optimal and suggests that a remark on high-probability upper bounds and the role of gamma could be interesting. Additionally, the reviewer asks whether the analysis, which is based on expectations, can be extended to high-probability bounds on the regret. While the comment provides some direction by questioning the role of gamma and suggesting a potential area for further exploration, it does not explicitly instruct the authors on what actions to take or how to address these points. The action is implicit and somewhat vague, leaving the authors to infer the necessary steps. Therefore, the comment is somewhat actionable.",5,"The review comment provides a thoughtful critique of Algorithm 1 and Theorem 3.1, questioning the utility of the exploration parameter gamma given the context of upper-bounds on pseudo-regret. It suggests that gamma=0 might be optimal and proposes adding a remark on high-probability upper-bounds and the role of gamma. Additionally, it inquires whether the current analysis, which relies on expectations, could be extended to high-probability bounds on the regret. This feedback is highly helpful as it not only identifies a potential issue with the current analysis but also offers specific suggestions and questions that could lead to a deeper understanding and improvement of the work. The comment is detailed and constructive, providing the authors with clear directions for enhancing their draft.",5,"The comment is fully grounded as it explicitly mentions ""Alg 1"" and ""Thm 3.1,"" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it raises a detailed question about the exploration parameter gamma, suggesting that it might be useless given the context of upper-bounds on pseudo-regret. The comment further suggests adding a remark on high-probability upper-bounds and questions whether the analysis can be extended to these bounds. This provides clear guidance on what needs to be addressed or clarified in the paper. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.",3,"The review point questions the usefulness of the exploration parameter gamma in the context of Alg 1 and Thm 3.1, suggesting that the choice of gamma=0 might be optimal given that only upper-bounds on the pseudo-regret are provided. The reviewer further suggests that a remark on high-probability upper-bounds and the role of gamma could be interesting and inquires whether the analysis could be extended to high-probability bounds on the regret. While the comment raises a valid point, it lacks specific evidence or references to support the claim about the exploration parameter's usefulness or the potential for extending the analysis. The reasoning is logical but not fully substantiated, making the claim somewhat verifiable. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.",803,,,,,,,,
ICLR_2021_2506,ICLR_2021,"Weakness== Exposition ==The exposition of the proposed method can be improved. For examples, - it’s unclear how the “Semantic Kernel Generation” is implemented. I can probably guess this step is essentially a 1 x 1 convolution, but it would be better to fill in the details. - In the introduction, the paper mentioned that the translated images often contain artifacts due to the nearest-neighbor upsampling. Yet, in the Feature Spatial Expansion step it also used nearest-neighbor interpolation. This needs some more justification. - For Figure 3, it’s unclear what the learned semantically adaptive kernel visualization means. At each upsampling layer, the kernel is only K x K. - At the end of Section 2, I do not understand what it means by “the semantics of the upsampled feature map can be stronger than the original one”. - The proposed upsampling layer is called “semantically aware”. However, I do not see anything that’s related to the semantics other than the fact that the input is a semantic map. I would suggest that this should be called “content aware” instead.== Technical novelty == - My main concern about the paper lies in its technical novelty. There have been multiple papers that proposed content aware filter. As far as I know, the first one is [Xu et al. 2016]. Jia, Xu, et al. ""Dynamic filter networks."" Advances in neural information processing systems. 2016.The work most relevant to this paper is the CARAFE [Wang et al. ICCV 2019], which can be viewed as a special case of the dynamic filter network for feature upsampling. By comparing Figure 2 in this paper and Figure 2 in the CARAFE paper [Wang et al. ICCV 2019], it seems to me that the two methods are * the same*. The only difference is the application of the layout-to-image task. The paper in the related work section suggests, “the settings of these tasks are significantly different from ours, making their methods cannot be used directly.” I respectfully disagree with this statement because both methods take in a feature tensor and produce an upsampled feature tensor. I believe that the application would be straightforward without any modification. Given the similarity to prior work, it would be great for the authors to (1) describe in detail the differences (if any) and (2) compare with prior work that also uses spatially varying upsampling kernel.Minor: - CARAFE: Content-Aware ReAssembly of Features. The paper is in ICCV 2019, not CVPR 2019In sum, I think the idea of the spatially adaptive upsampling kernel is technically sound. I also like the extensive evaluation in this paper. However, I have concerns about the high degree of similarity with the prior method and the lack of comparison with CARAFE.** After discussions **I have read other reviewers' comments. Many of the reviewers share similar concerns regarding the technical novelty of this work. I don't find sufficient ground to recommend acceptance of this paper.","- At the end of Section 2, I do not understand what it means by “the semantics of the upsampled feature map can be stronger than the original one”.",2457,1,"The review point highlights a lack of clarity in a specific statement at the end of Section 2, indicating that the reviewer does not understand what is meant by ""the semantics of the upsampled feature map can be stronger than the original one."" However, the comment does not provide any explicit or implicit action for the authors to take to address this lack of understanding. It lacks guidance on whether the authors should clarify the statement, provide additional context, or rephrase it for better comprehension. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.",3,"The comment highlights a specific point of confusion regarding a statement made at the end of Section 2 about the semantics of the upsampled feature map being stronger than the original one. However, it does not provide any suggestions or guidance on how the authors might clarify or elaborate on this statement. While it identifies an area that may need further explanation, the lack of actionable feedback or specific advice limits its usefulness to the authors. Therefore, the comment is somewhat helpful, as it points out a potential issue but does not offer detailed guidance for improvement.",5,"The comment is fully grounded as it explicitly mentions ""the end of Section 2,"" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it points out a particular phrase that is unclear, indicating that the authors need to clarify what is meant by ""the semantics of the upsampled feature map can be stronger than the original one."" Therefore, this comment is categorized as fully grounded and specific, aligning with category 5.",X,"The review point is a request for clarification about a specific statement in the paper. It does not contain an opinion, claim, or suggestion but is purely a factual question. Therefore, it fits the criteria for a normal statement and should be labeled as ""No.""",941,,,,,,,,
CpiOUOaqh3,ICLR_2024,"1. The proposed SEPAI3R3O model introduces over 10 parameters, which may be difficult to reliably estimate given limited real-world observations. Moreover, modeling these parameters as time-varying further exacerbates the challenge of estimation from scarce data. The authors should discuss the uncertainty in fitted parameter values and provide sensitivity analysis.2. Limited Novelty. The authors apply a standard genetic algorithm approach for parameter estimation. While computationally fitting an extended SEIR model is non-trivial, the optimization methodology itself does not represent a significant novel contribution. More justification is needed on the novelty of the parameter estimation technique.3. Unvalidated Estimates. The authors estimate under-reporting rates for infections, but it is unclear how these proposed values were validated. Further analysis should be provided to support the assumed levels of under-reporting across different model compartments.","1. The proposed SEPAI3R3O model introduces over 10 parameters, which may be difficult to reliably estimate given limited real-world observations. Moreover, modeling these parameters as time-varying further exacerbates the challenge of estimation from scarce data. The authors should discuss the uncertainty in fitted parameter values and provide sensitivity analysis.",8998,5,"The review comment highlights a concern regarding the difficulty of estimating over 10 parameters in the proposed SEPAI3R3O model, especially when modeled as time-varying and given limited real-world observations. The reviewer explicitly suggests that the authors should discuss the uncertainty in the fitted parameter values and provide a sensitivity analysis. This feedback provides clear and concrete actions for the authors to take, including specific areas to address (uncertainty and sensitivity analysis). The explicit nature of the suggestions and the detailed guidance on how to implement them make this comment highly actionable.",5,"The review comment identifies a potential issue with the proposed SEPAI3R3O model, specifically the challenge of estimating over 10 parameters from limited real-world observations. It further highlights the complexity added by modeling these parameters as time-varying. The comment provides a clear and actionable suggestion for the authors to discuss the uncertainty in the fitted parameter values and to conduct a sensitivity analysis. This feedback is valuable as it directs the authors to address a critical aspect of their model's robustness and reliability. By following this suggestion, the authors can significantly enhance the rigor and credibility of their work. Therefore, the comment is rated as highly helpful, consistent with a score of 5.",5,"The comment is fully grounded as it explicitly mentions the ""SEPAI3R3O model"" and the introduction of over 10 parameters, allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies what needs to be addressed, including discussing the uncertainty in fitted parameter values and providing a sensitivity analysis. This provides clear guidance on what the authors need to address in their paper. Therefore, the comment is fully grounded and specific, corresponding to label 5.",4,"The review point raises a concern about the difficulty of estimating over 10 parameters in the proposed SEPAI3R3O model due to limited real-world observations. It further argues that modeling these parameters as time-varying increases the challenge. The comment suggests that the authors should discuss the uncertainty in the fitted parameter values and provide a sensitivity analysis. This claim is mostly verifiable as it provides logical reasoning regarding the challenges of parameter estimation and the need for sensitivity analysis. However, it could be strengthened by referencing specific examples or studies that highlight similar challenges or by providing more detailed reasoning about the impact of these challenges on the model's reliability. Therefore, the comment aligns with a label of 4: Mostly Verifiable.",5529,,,,,,,,
G4P1q2G0XK,ICLR_2025,"- The paper generally compares its method with earlier methods, it lacks comparisons to some recent methods (e.g. FINER [1], Incode [2], SL2A-INR [3]) which propose new activation (and in some cases adaptive activation) functions for INRs.- Another weakness of this paper is that they do not show if the proposed method has less spectral bias than SIREN and other methods, which is the core deficiency of INRs.- I was hoping to provide more theoretical or experimental analysis on the spectral bias, such as using Neural Tangent Kernel tools for their approach.- How does your method compare with others in terms of complexity? Does it add any complexities to the network?[1]Liu Z, Zhu H, Zhang Q, Fu J, Deng W, Ma Z, Guo Y, Cao X. FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024 (pp. 2713-2722).[2]Kazerouni A, Azad R, Hosseini A, Merhof D, Bagci U. INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 2024 (pp. 1298-1307).[3]Heidari M, Rezaeian R, Azad R, Merhof D, Soltanian-Zadeh H, Hacihaliloglu I. Single-Layer Learnable Activation for Implicit Neural Representation (SL $^{2} $ A-INR). arXiv preprint arXiv:2409.10836. 2024 Sep 17.","- The paper generally compares its method with earlier methods, it lacks comparisons to some recent methods (e.g. FINER [1], Incode [2], SL2A-INR [3]) which propose new activation (and in some cases adaptive activation) functions for INRs.",9185,5,"The review point suggests that the paper should include comparisons with recent methods such as FINER, Incode, and SL2A-INR, which propose new activation functions for INRs. This feedback provides a clear and explicit action for the authors to take, namely to compare their method with these specific recent methods. The suggestion is concrete, as it specifies exactly which methods should be included in the comparison. Therefore, the comment is highly actionable.",3,"The review comment identifies a gap in the paper's comparative analysis, specifically noting the absence of comparisons with recent methods such as FINER, Incode, and SL2A-INR, which introduce new activation functions for INRs. This feedback is somewhat helpful as it highlights an area where the paper could be improved by including comparisons with these recent methods to provide a more comprehensive evaluation of the proposed method. However, the comment could be more helpful if it provided specific guidance on how to incorporate these comparisons or discussed the relevance of these methods in more detail. Overall, the comment offers some direction for improvement but lacks depth and specificity, aligning with a score of 3.",3,"The comment is weakly grounded as it does not specify which part of the paper lacks comparisons to recent methods. However, it does provide specific examples of recent methods (FINER, Incode, SL2A-INR) that should be included in the comparison, making it specific in terms of what needs to be addressed. The authors can infer that this comment pertains to the section of the paper where comparisons with other methods are discussed, but the lack of explicit grounding makes it challenging to pinpoint the exact location. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.",3,"The review point claims that the paper lacks comparisons to recent methods, specifically mentioning FINER, Incode, and SL2A-INR, which propose new activation functions for INRs. While the comment identifies specific works that are missing from the comparison, it does not provide further details or reasoning on how these methods are relevant or why their inclusion would be beneficial. The claim is somewhat verifiable as it points out a potential gap in the literature review, but it lacks detailed justification or explanation, requiring the authors to make an effort to understand the significance of these omissions. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.",3272,,,,,,,,
Ob0UafH2YI,ICLR_2025,"1. Table 1 has many errors. ComFedL and FedNest communicate in every iteration. Their communication complexity is $O(\epsilon^{-2})$. The communication complexity of Local-SCGDM is $O(\epsilon^{-1.5})$, not $O(\epsilon^{-1})$. For FedBiO, its computation complexity is $O(\epsilon^{-1.5})$, not $O(\epsilon^{-2.5})$, and its communication complexity is $O(\epsilon^{-1})$. In addition, FedNest needs to take into account the inner loop length.2. This paper states that DS-FedDRO requires stronger assumptions than FedDRO. However, it seems they use the same assumptions, as Assumption 5.1 is same as Assumption 3.4. Why does DS-FedDRO have a better communication complexity? It would be good to provide detailed discussions about this improvement.3. To estimate $g$, why do DS-FedDRO and FedDRO use different methods?4. Some claims in this paper are incorrect. For example, this paper states that this is the first work that ensures linear speed up in a federated CO setting. Actually, existing federated bilevel optimization algorithms, e.g., FedBiO and FedMBO in Table 1, have achieved linear speedup. In addition, existing federated compositional optimization algorithms [1,2] also have achieved linear speedup.5. The novelty is incremental. A key contribution of this paper is to communicate $y$. However, existing federated compositional methods, such as Local-SCGDM, have already proposed to communicate it. Moreover, as a special bilevel optimization problem, existing federated bilevel optimization algorithms, such as FedBiO, also communicate the inner level function. Due to these existing methods, I don't think communicating $y$ is the contribution of this paper. For the convergence analysis, I didn't see new challenges in this paper compared to those existing federated compositional and bilevel optimization problems. It might be good if the authors could point out why those methods do not work for FedDRO problems.6. It seems that Algorithm 2 requires to communicate $y$ in every iteration. Then, the communication complexity in Corollary 4.4 is incorrect. It should be same as the number of iterations.[1] Wu, Xidong, et al. ""Federated conditional stochastic optimization."" Advances in Neural Information Processing Systems 36 (2024).[2] Gao, Hongchang. ""A Doubly Recursive Stochastic Compositional Gradient Descent Method for Federated Multi-Level Compositional Optimization."" Forty-first International Conference on Machine Learning. 2024.","3. To estimate $g$, why do DS-FedDRO and FedDRO use different methods?",5426,2,"The review point poses a question about why DS-FedDRO and FedDRO use different methods to estimate $g$. While it highlights a potential area of confusion or inconsistency, it does not provide explicit guidance or suggest specific actions for the authors to take. The authors might infer that they need to clarify or justify the use of different methods, but the comment does not concretely state what should be done or how to address the issue. As a result, the action is implicit and vague, making the comment barely actionable.",2,"The review comment raises a question about the methodology used in the paper, specifically why DS-FedDRO and FedDRO employ different methods to estimate $g$. While it identifies a potential area of confusion or inconsistency, the comment does not provide any suggestions or guidance on how the authors might address this issue. It lacks depth and actionable feedback, which limits its usefulness to the authors. The comment could be more helpful if it explained why this difference is significant or suggested ways to clarify or justify the methodological choices. As it stands, the comment is barely helpful, aligning with a score of 2.",3,"The comment raises a question about the estimation of $g$ and why DS-FedDRO and FedDRO use different methods. However, it does not specify which part of the paper discusses these methods, making it difficult for the authors to pinpoint the exact section that needs attention. While the authors might have an idea of where these methods are mentioned, the comment lacks full grounding. It is specific in questioning the difference in methods but does not provide detailed guidance on how to address the issue. Therefore, this comment is weakly grounded and specific, aligning with category 3.",X,"The review point is a question asking for clarification about the methods used by DS-FedDRO and FedDRO to estimate $g$. It does not contain any subjective claims, opinions, or suggestions that require verification. It is purely a request for information, fitting the classification of ""No.""",8572,,,,,,,,
yJdj2QQCUB,ICLR_2024,"1. **Comparison with related work:** While the paper does touch upon the LSPE (Dwivedi et al. 2021) – which shares similarities in learning positional representations along with the prediction task – a more detailed comparison is crucial to identify the unique contributions of the present work.2. **Clarity on computational complexity:** A major limitation of most hand-crafted PSEs is their high complexity on large graphs. The paper could benefit from a deeper dive into GPSE's computational complexity, particularly when compared against the often complex hand-crafted PSEs and other encoding strategies.3. **Missing baseline method:** Throughout the experiments, GPSE is compared against individual PSEs. Considering GPSE is designed to capture multiple PSE representations, a comparison against a combined baseline which concatenates various PSEs – akin to those used in GPSE's pre-training – would offer a more fair evaluation.**Minor points**- **Assumptions about Audience:** The introduction might be challenging for those new to the field, given the use of specific terms like ""1-WL bounded expressiveness"" and ""over-squashing"" without much elucidation. _Reference:_Dwivedi, Vijay Prakash, et al. ""Graph Neural Networks with Learnable Structural and Positional Representations."" ICLR 2021.","2. **Clarity on computational complexity:** A major limitation of most hand-crafted PSEs is their high complexity on large graphs. The paper could benefit from a deeper dive into GPSE's computational complexity, particularly when compared against the often complex hand-crafted PSEs and other encoding strategies.",4864,4,"The review point suggests that the paper should include a more detailed discussion on the computational complexity of GPSE, especially in comparison to hand-crafted PSEs and other encoding strategies. While the comment implies that the authors should expand on this aspect, it does not explicitly instruct them to do so. The action is clear and concrete but stated implicitly, making this comment mostly actionable. The authors can infer that they need to provide a deeper analysis of computational complexity, but the comment does not specify exactly how to approach this analysis or what specific comparisons to make.",4,"The review comment identifies a significant area for improvement by suggesting that the paper should delve deeper into the computational complexity of GPSE, especially in comparison to hand-crafted PSEs and other encoding strategies. This feedback is clear and actionable, as it points out a specific aspect of the paper that could be expanded upon to enhance the reader's understanding of the proposed method's efficiency and practicality. However, the comment could be more helpful by providing specific suggestions or examples of how to address this issue, such as recommending particular analyses or comparisons to include. Overall, the comment is mostly helpful as it highlights a key area for improvement and provides some direction, aligning with a score of 4.",3,"The comment suggests that the paper could benefit from a deeper analysis of GPSE's computational complexity, especially in comparison to hand-crafted PSEs and other encoding strategies. However, it does not specify which part of the paper should include this analysis, making it weakly grounded. The comment is specific in identifying the need for a detailed discussion on computational complexity, but without explicit references to sections or figures, the authors may struggle to pinpoint where this analysis should be incorporated. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.",3,"The review point suggests that the paper could benefit from a deeper analysis of GPSE's computational complexity, especially in comparison to hand-crafted PSEs and other encoding strategies. The claim is based on the logical reasoning that high complexity is a known limitation of hand-crafted PSEs, and understanding how GPSE compares could provide valuable insights. However, the comment lacks specific examples or references to support the claim, making it somewhat verifiable. The authors may need to put in additional effort to fully understand and address the suggestion, thus justifying a score of 3: Somewhat Verifiable.",6339,,,,,,,,
ICLR_2023_1608,ICLR_2023,"The baseline methods seem too naïve and lacks more ablation study experiments. 1. The authors claim their dataset could be applied into other ML task, e.g. data studying, anomaly detection. Wish to conduct related experiments to verify this statement. 2. I wonder whether there exist additional data from other zones, which could be implement the cross evaluation of their baseline models. 3. There seems lack more details related works about this topic. 4. The evaluation metric just uses MSE in Table 2, which cannot reflect the accuracy of nowcasting. 5. The results of the previous methods on the proposed dataset also should be listed.","4. The evaluation metric just uses MSE in Table 2, which cannot reflect the accuracy of nowcasting.",3441,1,"The review point highlights a limitation in the evaluation metric used in Table 2, noting that MSE cannot reflect the accuracy of nowcasting. However, it does not provide any explicit or implicit suggestions for alternative metrics that could better capture the accuracy of nowcasting. Without guidance on what other metrics might be more appropriate or how to implement them, the authors are left without a clear action to take. The comment identifies a problem but lacks actionable advice, making it unactionable.",3,"The review comment identifies a potential issue with the evaluation metric used in the paper, specifically that using Mean Squared Error (MSE) in Table 2 may not adequately reflect the accuracy of nowcasting. This is a relevant observation that could impact the interpretation of the results. However, the comment lacks specific suggestions or guidance on alternative metrics that might better capture the accuracy of nowcasting. Without actionable advice or examples of more suitable metrics, the feedback provides limited direction for the authors to improve their evaluation approach. Therefore, the comment is somewhat helpful, as it points out a potential weakness but does not offer comprehensive guidance for improvement.",5,"The comment is fully grounded as it explicitly mentions ""Table 2,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue by pointing out that the evaluation metric used, MSE, cannot reflect the accuracy of nowcasting. This provides clear guidance on what aspect of the evaluation needs reconsideration or improvement. Therefore, this comment is categorized as fully grounded and specific, aligning with category 5.",1,"The review point claims that using MSE as an evaluation metric in Table 2 is insufficient for reflecting the accuracy of nowcasting. However, the comment does not provide any supporting evidence, reasoning, or alternative metrics that could better reflect accuracy. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.",9114,,,,,,,,

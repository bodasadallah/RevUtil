paper_id,venue,focused_review,review_point,id,chatgpt_actionability_score,chatgpt_actionability_rationale,chatgpt_helpfulness_score,chatgpt_helpfulness_rationale,chatgpt_grounding_specificity_score,chatgpt_grounding_specificity_rationale,chatgpt_verifiability_score,chatgpt_verifiability_rationale,__index_level_0__,norm_review_point,actionability_label,grounding_specificity_label,verifiability_label,helpfulness_label,actionability_rationale,grounding_specificity_rationale,verifiability_rationale,helpfulness_rationale
NIPS_2018_202,NIPS_2018,"- My major concern with the paper is that the theoretical results seem to be stated in vague terms and I don't fully understand them. In Theorem 1, what does it mean to say that Omega^k ""contains"" disjoint matrices Omega_tau? Does it mean that Omega^k is a stack of matrices Omega_tau column-wise? Also, what does it mean that it is ""possible"" to perfectly recover all columns of X^k? Does it mean that the subspaces and the missing entries can be uniquely determined from the observed data? In addition, how is this result compared with the deterministic result for the problem of subspace clustering with missing entries in [4]? Overall this is a nice paper that brings up a new problem formulation into attention. The study of this problem is still preliminary, though, as one can clearly see that there is no very successful application of the proposed method yet. The experiment on face clustering has a very unrealistic test scenario, and the experiment for background segmentation does not generate results as good as classical robust PCA. Nonetheless, addressing these challenges could be the topic of future study. My reason for not having a higher rating is that I cannot fully appreciate the theoretical studies as mentioned above. Additional comments: - It appears that the ""cluster"" step in the proposed algorithm is very complicated. I'm wondering if this step can be decomposed into the following substeps to make it easier to explain: the estimation of Omega_k is composed of two separate tasks of 1) clustering the entries in each column to different data points, and 2) the assigning data points extracted from all columns to multiple subspaces. In fact, once one have solved task 1) above then the problem reduces to subspace clustering with missing data, which could be solvedby alternating between matrix completion and subspace clustering. - Since the overall problem is nonconvex, initialization is expected to be very important for the algorithm to achieve good performance. Can the authors comment on how their algorithm is initialized on real data experiments? Response to rebuttals: The updated statement of Theorem 1 in the rebuttal is now much better in terms of clarity and it appears to be very necessary to incorporate it into the final version. Also, given that the conditions in Theorem 1 and 2 are very much similar to those in [4] for a related problem, a more detailed discussion of their connections will help understand the merits of these results. I maintain my overall rating as above and recommend a weak acceptance for this work.","1) above then the problem reduces to subspace clustering with missing data, which could be solvedby alternating between matrix completion and subspace clustering.",6284,2,"The review point makes an observation about the problem reducing to subspace clustering with missing data and suggests a potential solution involving alternating between matrix completion and subspace clustering. However, it does not provide explicit guidance or instructions on how the authors should implement this solution or whether it is a recommended course of action. The comment lacks concrete details and leaves the authors uncertain about how to proceed. Without specific steps or a clear directive, the comment is vague and implicit, making it barely actionable.",2,"The review comment suggests a potential approach to solving the problem by reducing it to subspace clustering with missing data and proposes alternating between matrix completion and subspace clustering as a solution. However, the comment lacks clarity and detail, as it does not explain how this approach should be implemented or why it would be beneficial. Without specific guidance or actionable steps, the authors may find it challenging to apply this suggestion to improve their draft. Therefore, the feedback is rated as barely helpful, as it identifies a possible direction but lacks the depth and specificity needed to be more useful.",1,"The comment suggests a potential solution to a problem by reducing it to subspace clustering with missing data and proposes an approach involving alternating between matrix completion and subspace clustering. However, it does not specify which part of the paper this suggestion pertains to, making it difficult for the authors to identify the exact section or context where this solution should be applied. Additionally, the comment lacks specificity regarding the details of the proposed approach or how it should be implemented. Therefore, the comment is not grounded and lacks specificity, aligning with a score of 1.",1,"The review point makes a claim about reducing the problem to subspace clustering with missing data and suggests a solution involving alternating between matrix completion and subspace clustering. However, it lacks any supporting evidence, reasoning, or references to justify this approach. Without further explanation or examples, the claim is difficult for the authors to verify or understand how to implement the suggestion effectively. Therefore, the comment is considered unverifiable.",4501,"1) above then the problem reduces to subspace clustering with missing data, which could be solvedby alternating between matrix completion and subspace clustering.",3,2,1,3,"The review point suggests that the problem can be reduced to subspace clustering with missing data, which could be solved by alternating between matrix completion and subspace clustering. However, it does not provide explicit instructions or concrete steps on how to implement this approach. The authors are left to infer that they should explore this method, but without detailed guidance, the action remains vague. Therefore, this comment is 3.","The comment refers to a specific part of the paper, ""1) above,"" which suggests that it is addressing the first point or section of the paper. However, it does not specify which part of the first point is being discussed, leaving the authors to make an educated guess. The comment provides a suggestion for solving the problem by alternating between matrix completion and subspace clustering, but it does not specify what aspects of the problem need to be addressed or how this suggestion would improve the paper. Therefore, the comment is 2, aligning with a score of 2.","The review point suggests that the problem can be reduced to subspace clustering with missing data, which could be solved by alternating between matrix completion and subspace clustering. However, the comment lacks specific reasoning, examples, or references to support this claim. Without detailed justification or evidence, the authors may find it challenging to understand the basis of the suggestion or how to implement it. Therefore, the comment is considered 1.","The review comment suggests a potential approach to solving the problem by reducing it to subspace clustering with missing data, which could be addressed by alternating between matrix completion and subspace clustering. While this provides a direction for improvement, the comment lacks specificity and does not offer detailed guidance on how to implement this approach or what specific aspects of the problem need to be addressed. The feedback is 3 as it points out a possible solution but does not fully support the authors in making significant improvements to their draft. Therefore, it aligns with a score of 3."
ICLR_2023_2171,ICLR_2023,"1. All the tasks in this paper seem based on grasping. The difficulties of these tasks are too homogeneous. Perhaps refer to these projects to design more tasks, such as [1] and [2]. 2. The State Space contains the pose and velocity of the object without any vision-based input, thus resulting in missing geometric features of the object. This may limit the out-of-domain generalization ability. Recently, lots of dexterous hand works contain visual input with a strong generalization ability, such as [3] and [4]. 3. There seem to be no ablation experiments on synergies. 4. The experiment seems to be done with only one seed, and the authors should consider doing several more seeds in order to better demonstrate the robustness of the method. 5. The previous work [5], which also presents a dexterous manipulation benchmark with musculoskeletal hands, detracts from the novelty of this work.[1] https://bi-dexhands.ai/ [2] https://openreview.net/pdf?id=k2Ml8FGtJZp [3] https://openreview.net/pdf?id=k2Ml8FGtJZp [4] https://openreview.net/pdf?id=tJE1Yyi8fUX [5] https://arxiv.org/pdf/2205.13600.pdf","1. All the tasks in this paper seem based on grasping. The difficulties of these tasks are too homogeneous. Perhaps refer to these projects to design more tasks, such as [1] and [2].",3504,3,"The comment suggests that the tasks in the paper are too homogeneous and recommends referring to specific projects ([1] and [2]) to design more diverse tasks. While the action is implicit, it provides a concrete suggestion by pointing to specific projects that could inspire the design of new tasks. However, it does not detail how the authors should incorporate these references into their work or what specific aspects of the projects should be considered. The authors have a general direction but lack detailed guidance on implementation. Therefore, the comment is somewhat actionable.",4,"The review comment identifies a potential issue with the homogeneity of the tasks presented in the paper, noting that they all seem to be based on grasping. It suggests that the tasks lack diversity in difficulty, which could limit the scope and applicability of the research. The reviewer provides a constructive suggestion by recommending the authors refer to specific projects ([1] and [2]) to design more varied tasks. This feedback is actionable and offers a clear direction for the authors to enhance the diversity and challenge of their tasks, making the comment mostly helpful. However, it could be more helpful if it included more specific guidance on how to implement the suggested changes or what aspects of the referenced projects are particularly relevant. Overall, the comment aligns with a score of 4.",3,"The comment suggests that the tasks in the paper are too homogeneous and recommends referring to specific projects ([1] and [2]) to design more diverse tasks. However, it does not specify which part of the paper discusses these tasks, making it weakly grounded. The comment is specific in identifying the issue of homogeneity and provides a suggestion for improvement by referencing other projects. Therefore, the comment is weakly grounded and specific, aligning with label 3.",3,"The review point claims that ""all the tasks in this paper seem based on grasping"" and suggests that the tasks are ""too homogeneous."" It recommends referring to other projects to design more diverse tasks, citing [1] and [2] as potential resources. While the comment provides a general critique and suggests looking at specific projects, it lacks detailed reasoning or examples to substantiate why the current tasks are considered homogeneous or how the suggested projects could help. The absence of specific examples or detailed justification makes the claim somewhat verifiable, as the authors would need more information to fully understand and address the critique.",4658,"1. all the tasks in this paper seem based on grasping. the difficulties of these tasks are too homogeneous. perhaps refer to these projects to design more tasks, such as [1] and [2].",3,3,2,3,"The review point suggests that the tasks in the paper are based on grasping and that their difficulties are too homogeneous. It implies that the authors should consider designing more tasks, referencing specific projects like 1 and 2. However, the comment does not explicitly instruct the authors to include these references or to design new tasks, leaving the action somewhat vague. The authors can infer that they need to diversify their tasks and consider external projects, but the comment lacks concrete guidance on how to implement this suggestion. Therefore, the comment is 3.","The comment suggests that the tasks in the paper are based on grasping and that their difficulties are too homogeneous. It implies that the authors should consider designing more tasks, referencing specific projects like 1 and 2. However, the comment does not specify which part of the paper these tasks are discussed in, making it weakly grounded. The suggestion to refer to other projects is specific, as it provides a clear direction for potential improvements. Therefore, the comment is 3, aligning with category 3.","The review point claims that all tasks in the paper are based on grasping and that their difficulties are too homogeneous. It suggests referring to specific projects (1 and 2) to design more tasks. However, the comment lacks detailed reasoning or examples to support why the tasks are too homogeneous or how the referenced projects could provide a solution. Without specific evidence or examples, the claim is difficult for the authors to address effectively. Therefore, the comment is considered 2, as it provides some direction but lacks sufficient detail to fully substantiate the claim.","The review comment identifies a potential limitation in the paper, noting that all tasks are based on grasping and that their difficulties are too homogeneous. It suggests that the authors consider designing more tasks by referencing specific projects, such as 1 and 2. This feedback is 3 as it points out a specific area for improvement and provides a direction for expanding the scope of the paper. However, the comment could be more helpful if it included specific examples or detailed guidance on how to incorporate these new tasks or projects. The suggestion to refer to other projects is a good starting point, but the authors may need additional guidance to fully understand and implement the recommendation. Therefore, the comment is rated as 3."
ICLR_2023_1400,ICLR_2023,"- While the paper shows improvements on CIFAR derivatives, it lacks analysis or results on other datasets (e.g., ImageNet derivatives). Verifying the effectiveness of the framework on ImageNet-1k or even ImageNet-100 is important. These results ideally can be presented in the main paper.- The authors should add some details on how to solve the optimization in the main paper. It's an important piece of information currently lacking in the paper.- Some baselines such as [1] are not considered and should be added.I feel that influence function can be replaced by other influence estimation methods such as datamodels[2] or tracin[3]. It will be beneficial to understand if the updated framework results in better pruning than the baselines. I am assuming it would result in better pruning results, however it would be beneficial to understand which influence based methods are particularly suitable for pruning.[1]. https://arxiv.org/pdf/2107.07075[2]. https://arxiv.org/abs/2202.00622[3]. https://arxiv.org/abs/2002.08484",- The authors should add some details on how to solve the optimization in the main paper. It's an important piece of information currently lacking in the paper.,3701,5,"The review comment explicitly instructs the authors to add details on how to solve the optimization in the main paper, emphasizing that this is an important piece of information currently missing. This direct request provides a clear and specific action for the authors to take, leaving no ambiguity about what needs to be done to improve the draft. The comment is explicit and concrete, making it highly actionable.",4,"The comment identifies a specific area where the paper is lacking—details on solving the optimization problem. By pointing out this omission, the reviewer provides the authors with clear and actionable feedback on where to improve their manuscript. However, while it highlights the problem, it does not offer detailed suggestions or examples on how to address this issue, which could enhance its helpfulness. Overall, the comment is mostly helpful as it directs the authors to a critical area needing clarification, but it could be more comprehensive with additional guidance.",3,"The comment suggests that the authors should add details on solving the optimization problem, indicating that this is an important piece of information currently missing from the paper. However, it does not specify which section or part of the paper this information should be added to, making it weakly grounded. The comment is specific in identifying what is missing—details on solving the optimization—but lacks guidance on where this should be addressed. Therefore, the comment is weakly grounded and specific, aligning with category 3.",1,"The review point suggests that the paper lacks important details on solving the optimization, which is a critical aspect of the work. However, the comment does not provide any specific reasoning, examples, or references to support why this information is crucial or how it might be addressed. Without additional context or justification, the authors may find it challenging to understand the importance of this suggestion or how to implement it. Therefore, the comment is considered unverifiable due to the lack of supporting evidence or detailed explanation, warranting a score of 1: Unverifiable.",2777,- the authors should add some details on how to solve the optimization in the main paper. it's an important piece of information currently lacking in the paper.,5,3,1,4,"The review point explicitly instructs the authors to add details on how to solve the optimization in the main paper, indicating that this information is currently lacking. The comment provides a clear and direct action for the authors to take, which is to include additional details on optimization. This feedback is concrete and actionable, as it specifies exactly what needs to be addressed in the draft. Therefore, the comment is 5.","The comment suggests adding details on how to solve the optimization in the main paper, indicating that this information is currently lacking. However, it does not specify which part of the paper this information should be added to, making it weakly grounded. The comment is specific in identifying the need for additional details on optimization, but without explicit references to sections or figures, the authors may find it challenging to pinpoint the exact location for improvement. Therefore, this comment is 3, aligning with category 3.","The review point suggests that the authors should add details on how to solve the optimization in the main paper, as this information is currently lacking. However, the comment does not provide any specific reasoning, examples, or references to support why this information is crucial or how it would enhance the paper. Without additional context or justification, the claim remains vague and difficult for the authors to address effectively. Therefore, the comment is considered 1.","The review comment identifies a specific area where the paper lacks detail, namely the optimization process. It suggests that the authors should include additional information on how to solve the optimization in the main paper, which is an important piece of information currently missing. This feedback is clear and actionable, as it provides a direct suggestion for improvement that can enhance the comprehensiveness and clarity of the paper. However, the comment could be more helpful if it offered specific guidance on what aspects of the optimization process should be detailed or how to present this information effectively. Overall, the comment is 4, as it directs the authors to a critical area for improvement, but it could be more comprehensive with additional guidance."
ICLR_2022_733,ICLR_2022,Weakness 1 The number of experts is limited to 2. Indeed it would maintain the computation load. But the power of experts may not be exhibited sufficiently. It would be great to investigate the performance by changing the number of experts. 2 I am wondering if this mechanism can be used in the vision transformers?,2 I am wondering if this mechanism can be used in the vision transformers?,2700,2,"The review comment poses a question about whether the mechanism discussed can be applied to vision transformers. While it suggests a potential area for exploration, it does not provide explicit guidance or instructions on how to investigate this possibility. The action is implicit, as the authors can infer that they might consider exploring the applicability of their mechanism to vision transformers. However, the comment lacks concrete details or steps on how to carry out this exploration, making it vague. Therefore, this comment is barely actionable.",2,"The comment raises a question about the applicability of a mechanism to vision transformers, which could prompt the authors to consider and discuss this aspect in their paper. However, the comment lacks depth and does not provide any specific suggestions or guidance on how to explore this possibility. It does not identify any weaknesses or areas for improvement in the current draft, nor does it offer actionable feedback. As a result, the comment is barely helpful, as it hints at a potential area of interest but provides insufficient guidance for the authors to act upon.",1,"The comment poses a question about the applicability of a mechanism to vision transformers. However, it does not specify which part of the paper discusses this mechanism, leaving the authors unable to confidently determine the exact section being addressed. Additionally, the comment lacks specificity regarding what aspects of the mechanism's applicability should be considered or explored. As a result, the comment is not grounded and not specific, aligning with a score of 1.",X,"The review point is a question seeking clarification about the applicability of a mechanism to vision transformers. It does not express an opinion, judgment, or suggestion that requires validation or support. Therefore, it is classified as ""No.""",5967,2 i am wondering if this mechanism can be used in the vision transformers?,1,2,X,2,"The review point raises a question about the applicability of the mechanism to vision transformers, but it does not provide any explicit or implicit actions for the authors to take. It lacks guidance on whether the authors should explore this possibility, conduct experiments, or discuss it in the paper. Without any actionable steps or suggestions, the authors are left without a clear direction on how to address this question. Therefore, this comment is 1.","The comment raises a question about the applicability of the mechanism to vision transformers but does not specify which part of the paper it addresses. The authors cannot confidently determine the context or section being referred to, making it weakly grounded. Additionally, the comment lacks specificity as it does not provide any details or suggestions on how to address the question or what aspects of the mechanism might be relevant to vision transformers. Therefore, this comment is 2, aligning with a score of 2.","The review point is a question seeking clarification on the applicability of the mechanism to vision transformers. It does not contain any claims, opinions, or suggestions that require verification. It is purely a request for information, making it a normal statement. Therefore, it should be labeled as ""No.""","The review comment raises a question about the applicability of the mechanism to vision transformers, which could be an interesting area for exploration or discussion in the paper. However, it lacks specificity and does not provide any guidance or suggestions on how the authors might address this question or what aspects of the mechanism might be relevant to vision transformers. Without actionable feedback or detailed insights, the comment offers limited value to the authors in terms of improving their draft. Therefore, it is 2, aligning with a score of 2."
ICLR_2021_2180,ICLR_2021,"Weakness: 1. The authors should provide justification of choosing to use AAE in the work. In particular, why is AAE an attractive approach for MTPP? 2. The authors mention: ""The typical MTTP baselines like Reinforcement Learning, RNNs, Wasserstein GANs require a significant amount of data to train a network. Therefore, such techniques are not applicable to our proposed approach. As the baseline, we compare the proposed data generation technique with a Markov chain approach which was applied to the same dataset in (Klausen et al., 2018b)."" The authors could try to apply Reinforcement Learning, RNNs, Wasserstein GANs with data filled by simple methods, to empirically validate that these methods are not suitable for incomplete, small data. 3. In feature mapping method, the authors could provide justification for converting marked point times t_ij to days a_i. Besides, the method to convert t_ij to a_i and examples are not provided. If this is a common preprocessing method in MTPP, the authors should cite the relevant work. Overall, Step 1 of Algorithm 1 is not clear. 4. At the data approximation technique (step 3 of the Algorithm 1), the author randomly chooses a probability for the appearance of an unobservable data point but there is a lack of explanations. Can the authors explain the reason of selecting from [0, Pcj(0)]? 5. The authors should provide more details of incomplete and small dataset, and compare with their method when training with full data. This is to understand that if generated data is still good when training with small dataset=============== after rebuttal: I thank authors for the responses. After reviewing the authors' response and other reviewers' comments, I keep my original rating.","2. The authors mention: ""The typical MTTP baselines like Reinforcement Learning, RNNs, Wasserstein GANs require a significant amount of data to train a network. Therefore, such techniques are not applicable to our proposed approach. As the baseline, we compare the proposed data generation technique with a Markov chain approach which was applied to the same dataset in (Klausen et al., 2018b)."" The authors could try to apply Reinforcement Learning, RNNs, Wasserstein GANs with data filled by simple methods, to empirically validate that these methods are not suitable for incomplete, small data.",2190,5,"The review point provides a clear and explicit suggestion for the authors to empirically validate their claim that typical MTTP baselines like Reinforcement Learning, RNNs, and Wasserstein GANs are not suitable for their proposed approach due to data limitations. It suggests that the authors attempt to apply these methods using data filled by simple methods to demonstrate their unsuitability for incomplete, small data. This feedback is actionable as it provides a specific action for the authors to take, along with a rationale for why this action would be beneficial. The authors know exactly what experiment to conduct to strengthen their argument, making this comment highly actionable.",5,"The review comment provides a constructive suggestion for the authors to empirically validate their claim that certain techniques like Reinforcement Learning, RNNs, and Wasserstein GANs are not suitable for their approach due to data limitations. By recommending the application of these methods with data filled by simple techniques, the comment offers a clear and actionable way for the authors to strengthen their argument and provide empirical evidence for their claims. This feedback is highly helpful as it guides the authors towards a potential improvement in their methodology and the robustness of their conclusions.",5,"The comment is fully grounded as it directly quotes a specific statement from the paper, allowing the authors to accurately identify the part being addressed. It is also specific because it suggests a clear course of action: applying Reinforcement Learning, RNNs, and Wasserstein GANs with data filled by simple methods to empirically validate the claim that these methods are not suitable for incomplete, small data. This provides the authors with a concrete suggestion for improvement. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.",3,"The review point suggests that the authors should attempt to apply Reinforcement Learning, RNNs, and Wasserstein GANs with data filled by simple methods to empirically validate their unsuitability for incomplete, small data. The claim is somewhat verifiable as it provides a logical suggestion for empirical validation, but it lacks specific examples or references to support why these methods might be unsuitable. The comment could be strengthened by providing more detailed reasoning or evidence to substantiate the claim, making it easier for the authors to understand and act upon the suggestion. Therefore, the comment aligns with a label of 3: Somewhat Verifiable.",7016,"2. the authors mention: ""the typical mttp baselines like reinforcement learning, rnns, wasserstein gans require a significant amount of data to train a network. therefore, such techniques are not applicable to our proposed approach. as the baseline, we compare the proposed data generation technique with a markov chain approach which was applied to the same dataset in (klausen et al., 2018b)."" the authors could try to apply reinforcement learning, rnns, wasserstein gans with data filled by simple methods, to empirically validate that these methods are not suitable for incomplete, small data.",5,5,4,4,"The review point suggests that the authors should empirically validate the claim that Reinforcement Learning, RNNs, and Wasserstein GANs are not suitable for incomplete or small data by applying these methods with data filled by simple methods. This is an explicit action with concrete details on how to implement it, as it provides a specific suggestion for experimentation. The authors know exactly what they need to do to address the comment, making it 5.","The comment is fully grounded as it explicitly mentions a specific part of the paper, ""The typical MTTP baselines like Reinforcement Learning, RNNs, Wasserstein GANs,"" allowing the authors to accurately identify the section being addressed. It is also specific because it suggests a way to empirically validate the claim that these methods are not suitable for incomplete or small data by applying them with data filled by simple methods. This provides clear guidance on what needs to be addressed, making the comment 5. Therefore, this comment aligns with category 5.","The review point suggests that the authors should empirically validate the claim that Reinforcement Learning, RNNs, and Wasserstein GANs are not suitable for incomplete or small data. The reviewer provides a logical reasoning by suggesting that the authors apply these methods with data filled by simple methods to test their applicability. This suggestion is based on a clear and logical argument, making the claim 4. However, the comment could be strengthened by providing specific examples or references to support the claim, which would make it 5. Therefore, the comment is rated as 4.","The review comment identifies a potential weakness in the paper by questioning the authors"" claim that certain MTTP baselines, such as Reinforcement Learning, RNNs, and Wasserstein GANs, are not applicable due to the need for significant data. The reviewer suggests a way to empirically validate this claim by applying these methods with data filled by simple methods. This feedback is actionable and provides a clear direction for the authors to strengthen their argument by testing the applicability of these baselines under different conditions. However, the comment could be more helpful if it offered additional guidance on how to implement this suggestion or what specific aspects to focus on. Overall, the comment is 4 as it offers a constructive way to improve the paper""s validation and argumentation."
ICLR_2022_1250,ICLR_2022,"1. The authors claimed that “we develop a unified algorithm based on maximization of a submodular function, which can combine different selection objectives” in Introduction. However, I can’t see the necessary or benefit of using “submodular function and matroid intersection” in this paper, except that some prior works about submodular show the guarantee of approximation to the original dataset in the some feature/output space. The question is that the subset with optimal approximation of a dataset in some feature space doesn’t guarantee that it is the optimal for training neural networks. It is known that random selection can also perfectly approximate the geometry of original dataset (Chen et al 2012) as long as the subset is large enough. In addition, it is also unknown that such approximation guarantee of using submodular function is better or faster to reach than other approximation methods like herding, k-center (Sener & Savarese, 2018), k-medoids etc. 2. The authors admitted that “the difference between our method and k-center is marginal” in Discussion. The performance comparison also verified it. The performance improvement of the proposed method is mainly due to balancing constraints. More detailed ablation study on each individual balancing constraint should be given. Otherwise, it is unclear which constraint works. 3. The experiments lack the comparison to state-of-the-art subset selection methods. 4. In the balancing constraints, the image number of each class is pre-defined and the underlying assumption is that the class number is known. Is this assumption realistic in the experimental setting (active learning)?(Chen et al 2012) Super-Samples from Kernel Herding",3. The experiments lack the comparison to state-of-the-art subset selection methods.,3033,3,"The comment points out that the experiments do not include a comparison to state-of-the-art subset selection methods. While this feedback is explicit in identifying a missing element, it does not provide specific guidance on which subset selection methods should be included or how to incorporate them into the experiments. The authors are given a clear direction to add comparisons but lack detailed instructions on how to execute this. Therefore, the comment is somewhat actionable, as it identifies a specific area for improvement but lacks concrete guidance on implementation.",3,"The comment highlights a specific shortcoming in the experimental section of the paper, noting the absence of comparisons to state-of-the-art subset selection methods. This is a relevant point that could help the authors improve the comprehensiveness and relevance of their experimental analysis. However, the comment does not provide specific guidance or suggestions on which state-of-the-art methods should be considered or how to incorporate them into the experiments. While it identifies an important area for improvement, the feedback lacks actionable steps or detailed direction, making it somewhat helpful. Therefore, the comment is rated as somewhat helpful, consistent with a score of 3.",3,"The comment identifies a specific issue with the experiments, namely the lack of comparison to state-of-the-art subset selection methods. However, it does not specify which part of the experiments or which specific methods should be included for comparison. While the authors can infer that this comment pertains to the experimental section, they cannot confidently determine the exact part of the paper being addressed. This makes the comment weakly grounded. Nonetheless, the comment is specific in detailing what is missing in the experiments. Therefore, this comment is weakly grounded and specific, aligning with the label 3.",3,"The review point claims that the experiments lack a comparison to state-of-the-art subset selection methods. However, it does not provide any specific examples of such methods or explain why their inclusion is necessary for the study. The comment lacks detailed justification or references, making it challenging for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is somewhat verifiable, as it highlights a potential gap but does not provide enough information to fully substantiate the claim. This aligns with a score of 3: Somewhat Verifiable.",8361,3. the experiments lack the comparison to state-of-the-art subset selection methods.,3,4,1,3,"The review point highlights a lack of comparison to stateoftheart subset selection methods in the experiments. While it identifies an area for improvement, it does not provide specific guidance on which methods should be compared or how to conduct the comparison. The action is implicit and somewhat vague, as the authors can infer that they need to include comparisons but are not given concrete steps on how to do so. Therefore, this comment is 3.","The comment addresses the experiments section, specifically mentioning the lack of comparison to stateoftheart subset selection methods. This provides full grounding as it clearly identifies the part of the paper being addressed. However, the comment lacks specificity because it does not provide details on which stateoftheart methods should be compared or how this comparison should be conducted. Therefore, this comment is fully grounded but underspecific, aligning with category 4.","The review point claims that the experiments lack a comparison to stateoftheart subset selection methods. However, it does not provide any specific examples or references to these stateoftheart methods, nor does it explain why such a comparison is necessary or how it would improve the paper. Without detailed justification or evidence, the claim remains vague and difficult for the authors to address effectively. Therefore, the comment is considered 1.","The review comment identifies a significant gap in the paper by pointing out the lack of comparison to stateoftheart subset selection methods in the experiments. This is a critical observation that could impact the validity and relevance of the results presented. However, the comment does not provide specific suggestions on which stateoftheart methods should be considered or how to conduct the comparison. While it highlights an important area for improvement, the lack of detailed guidance limits its helpfulness. Therefore, the comment is 3, as it directs the authors to a specific area for enhancement but lacks depth in its suggestions."
Mq5cyRMGlD,EMNLP_2023,"While the paper addresses an important problem related to rewriting responses in the counseling education domain, there are some concerns regarding its novelty and contribution.1. The literature review shows that ample research already exists in this area. Although the authors present a paraphrase-augmented training basis as their USP, the problem of topic/domain shifting in paraphrase-augmented training is a common concern. The paper fails to discuss the intricacies involved with the approach and ""when not to take action"" especially in the case of uncontrolled domain shift.2. Although a mix of expert and non-expert validation has been included, no domain-centric or expert-guided metric is discussed. Being from a very similar field, I believe it is important to involve metrics or methods in psychiatric validation that lead to computational and psychological performance comparison. The question is why authors missed presenting such domain-centric metrics even with the presence of expert evaluators.3. Despite the existence of ample work, the selection of baselines is very limited to naive/old models. I encourage authors to add more baselines to perform robust performance comparisons.4. The proposed work lacks novelty. Several methods in the past have tried to apply very similar incorporation to state-of-the-art paradigms and have reported gains in response rewriting space and, in some cases, counseling rewriting too.","3. Despite the existence of ample work, the selection of baselines is very limited to naive/old models. I encourage authors to add more baselines to perform robust performance comparisons.",1594,3,"The comment explicitly encourages the authors to add more baselines to perform robust performance comparisons, indicating a clear action that needs to be taken. However, it does not specify which baselines should be included or provide guidance on how to select appropriate baselines. This lack of specificity makes the action somewhat vague, as the authors are left to determine which additional baselines would be most relevant or beneficial for their work. Therefore, while the action is explicit, the lack of concrete details on implementation makes this comment somewhat actionable.",3,"The review comment identifies a specific weakness in the paper, namely the limited selection of baselines used for performance comparisons. It suggests that the authors include more baselines to enhance the robustness of their comparisons. While the comment provides a clear direction for improvement, it lacks specific guidance on which additional baselines to consider or how to select them. This limits the comment's helpfulness, as the authors are left to determine the appropriate baselines on their own. Nonetheless, the feedback is actionable and points out a critical area for enhancement, making it somewhat helpful. Therefore, the comment aligns with a score of 3.",3,"The comment addresses the selection of baselines in the paper, suggesting that the authors have limited their comparisons to naive or older models. It encourages the addition of more baselines for robust performance comparisons. However, the comment does not explicitly mention which part of the paper discusses the baselines, making it weakly grounded. The suggestion to add more baselines is specific, as it provides a clear direction for improvement. Therefore, the comment is weakly grounded and specific, aligning with category 3.",3,"The review point claims that the selection of baselines is limited to naive or outdated models, suggesting that more baselines should be added for robust performance comparisons. While the comment highlights a potential issue, it lacks specific examples of more contemporary or relevant baselines that could be included. This absence of detailed references or examples makes the claim somewhat verifiable, as it provides a general direction but requires significant effort from the authors to identify and incorporate the suggested improvements.",7916,"3. despite the existence of ample work, the selection of baselines is very limited to naive/old models. i encourage authors to add more baselines to perform robust performance comparisons.",5,3,2,4,"The review point explicitly encourages the authors to add more baselines to their selection, specifically suggesting the inclusion of more robust models beyond the current naive/old models. This provides a clear and direct action for the authors to take, ensuring that their performance comparisons are more comprehensive. The comment is explicit and concrete, offering a specific suggestion on how to improve the draft. Therefore, this comment is 5.","The comment addresses the selection of baselines in the paper, suggesting that it is limited to naive or old models. It encourages the authors to add more baselines for robust performance comparisons. However, the comment does not specify which part of the paper discusses the baselines or where the authors should add these new baselines. This makes it difficult for the authors to pinpoint the exact section that needs revision. While the comment is specific in its suggestion, it lacks grounding, as it does not clearly identify the part of the paper being addressed. Therefore, this comment is 3, aligning with a score of 3.","The review point claims that the selection of baselines is limited to naive or old models, suggesting that more robust models should be included for performance comparisons. However, the comment does not provide specific examples of these robust models or explain why the current selection is inadequate. This lack of detailed justification or references makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered 2, as it provides some context but lacks sufficient evidence or examples to fully support the claim.","The review comment identifies a limitation in the paper""s selection of baselines, noting that it is limited to naive or old models. It provides a clear and actionable suggestion for improvement by encouraging the authors to add more robust baselines for performance comparisons. This feedback is valuable as it highlights an area where the paper could be strengthened and offers a specific direction for enhancement. However, the comment could be more helpful if it provided examples of additional baselines that could be considered or explained why the current selection is insufficient. Overall, the comment is 4 as it offers a clear path for improvement, but it could be more comprehensive with additional guidance."
NIPS_2018_544,NIPS_2018,"- the presented results do not give me the confidence to say that this approach is better than any of the other due to a lot of ad-hoc decisions in the paper (e.g. digital identity part of the code vs full code evaluation, the evaluation itself, and the choice of the knn classifier) - the results in table 1 are quite unusual - there is a big gap between the standard autoencoders and the variational methods which makes me ask whether thereâs something particular about the classifier used (knn) that is a better fit for autoencoders. the particularities of the loss or the distribution used when training. why was k-nn used? a logical choice would be a more powerful method like svm or a multilayer perceptron. there is no explanation for this big gap - there is no visual comparison of what some of the baseline methods produce as disentangled representations so itâs impossibly to compare the quality of (dis) entanglement and the semantics behind each factor of variation - the degrees of freedom among features of the code seem binary in this case, therefore it is important which version of vae and beta-vae, as well as infogan are used, but the paper does not provide those details - the method presented can easily be applied on unlabeled data only, and that should have been one point of comparison to the other methods dealing with unlabeled data only - showing whether it works on par with baselines when no labels are used, but that wasnât done. the only trace of that is in the figure 3, but that compares the dual and primary accuracy curves (for supervision of 0.0), but does not compare it with other methods - though parts of the paper are easy to understand, in whole it is difficult to get the necessary details of the model and the training procedure (without looking into the appendix which i admittedly did not do, but then, i want to understand the paper fully (without particular details like hyperparameters) from the main body of the paper). i think the paper would benefit from another writing iteration Questions: - are the features of the code binary? because i didnât find it clear from the paper. if so, then the effects of varying the single code feature is essentially a binary choice, right? did the baseline (beta-)vae and infogan methods use the appropriate distribution in that case? i cannot find this in the paper - is there a concrete reason why you decided to apply the model only in a single pass for labelled data, because you could have applied the dual-swap on labelled data too - how is this method applied at test time - one needs to supply two inputs? which ones? does it work when one supplies the same input for both? - 57 - multi dimension attribute encoding, does this essentially mean that the projection code is a matrix, instead of a vector, and thatâs it? - 60-62 - if the dimensions are not independent, then the disentanglement is not perfect - meaning there might be correlations between specific parts of the representation. did you measure/check for that? - can you explicitly say what labels for each dataset in 4.1 are - where are they coming from? from the dataset itself? from what i understand, thatâs easy for the generated datasets (and generated parts of the dataset), but what about cas-peal-r1 and mugshot? - i find the explanation in 243-245 very unclear. could you please elaborate what this exactly means - why is it 5*3 (and not 5*2, e.g. in the case of beta-vae where thereâs a mean and stdev in the code) - 247 - why was k-nn used, and not some other more elaborate classifier? what is the k, what is the distance metric used? - algorithm 1, ascending the gradient estimate? what is the training algorithm used? isnât this employing a version of gradient descent (minimising loss)? - what is the effect of the balance parameter? it is a seemingly important parameter, but there are no results showing a sweep of that parameter, just a choice between 0.5 and 1 (and why does 0.5 work better)? - did you try beta-vae with significantly higher values of beta (a sweep between 10 and 150 would do)? Other: - the notation (dash, double dash, dot, double dot over inputs is a bit unfortunate because itâs difficult to follow) - algorithm 1 is a bit too big to follow clearly, consider revising, and this is one of the points where itâs difficult to follow the notation clearly - figure 1, primary-stage I assume that f_\phi is as a matter of a fact two f_\phis with shared parameters. please split it otherwise the reader can think that f_\phi accepts 4 inputs (and in the dual-stage it accepts only 2) - figure 3 a and b are very messy / poorly designed - it is impossible to discern different lines in a because thereâs too many of them, plus itâs difficult to compare values among the ones which are visible (both in a and b). log scale might be a better choice. as for the overfitting in a, from the figure, printed version, i just cannot see that overfitting - 66 - is shared - 82-83 Also, require limited weakerâ¦sentence not clear/gramatically correct - 113 - one domain entity - 127 - is shared - 190 - conduct disentangled encodings? strange word choice - why do all citations in parentheses have a blank space between the opening parentheses and the name of the author? - 226 - despite the qualities of hybrid images are not exceptional - sentence not clear/correct - 268 - is that fig 2b or 2a? - please provide an informative caption in figure 2 (what each letter stands for) UPDATE: Iâve read the author rebuttal, as well as all the reviews again, and in light of a good reply, Iâm increasing my score. In short, I think the results look promising on dSprites and that my questions were well answered. I still feel i) the lack of clarity is apparent in the variable length issue as all reviewers pointed to that, and that the experiments cannot give a fair comparison to other methods, given that the said vector is not disentangled itself and could account for a higher accuracy, and iii) that the paper doesnât compare all the algorithms in an unsupervised setting (SR=0.0) where I wouldnât necessarily expect better performance than the other models.","- i find the explanation in 243-245 very unclear. could you please elaborate what this exactly means - why is it 5*3 (and not 5*2, e.g. in the case of beta-vae where thereâs a mean and stdev in the code) - 247 - why was k-nn used, and not some other more elaborate classifier? what is the k, what is the distance metric used?",6551,5,"The review comment identifies specific areas where the explanation is unclear and requests elaboration. It explicitly asks for clarification on the meaning of lines 243-245, the reasoning behind the choice of 5*3, and the use of k-NN instead of another classifier. Additionally, it inquires about the value of k and the distance metric used. These questions provide direct and concrete actions for the authors to take, as they clearly outline the areas that need further explanation and detail. The feedback is explicit and actionable, making it highly actionable.",4,"The review comment identifies specific areas of the paper that are unclear, particularly the explanation in lines 243-245 and the choice of k-NN as a classifier. It asks for elaboration on the reasoning behind the numbers used and the choice of classifier, including details such as the value of k and the distance metric. This feedback is clear and actionable, as it directly points out where the authors need to provide more information and justification. By addressing these points, the authors can significantly improve the clarity and comprehensiveness of their paper. However, the comment could be more helpful if it offered suggestions on how to present this information or examples of alternative approaches. Overall, the comment is mostly helpful, as it effectively directs the authors toward areas needing clarification and improvement.",5,"The comment is fully grounded as it explicitly mentions specific lines, ""243-245"" and ""247,"" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it details the issues that need clarification, such as the explanation of the 5*3 calculation, the choice of k-NN as a classifier, and the parameters used for k-NN like the value of k and the distance metric. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.",X,"The comment consists of requests for clarification and questions about specific methodological choices in the paper. It does not contain any opinions, judgments, or suggestions that require verification. The inquiries are factual and descriptive, fitting the classification of ""No.""",4081,"- i find the explanation in 243-245 very unclear. could you please elaborate what this exactly means - why is it 5*3 (and not 5*2, e.g. in the case of beta-vae where thereâs a mean and stdev in the code) - 247 - why was k-nn used, and not some other more elaborate classifier? what is the k, what is the distance metric used?",5,5,X,5,"The review point explicitly requests clarification on specific parts of the paper, namely the explanation in sections 243245 and the choice of knn over other classifiers. It asks for elaboration on why the calculation is 5*3 and not 5*2, and it questions the use of knn and the choice of k and distance metric. These requests are direct and provide clear guidance on what the authors need to address in their draft. The feedback is explicit and concrete, giving the authors a clear understanding of what actions to take to improve their paper. Therefore, this comment is 5.","The comment is fully grounded as it explicitly mentions specific sections of the paper (243245 and 247), allowing the authors to accurately identify the parts being addressed. It is also specific because it clearly specifies what is unclear and requests elaboration on the explanation, the choice of knn over other classifiers, and the specifics of k and the distance metric used. This level of detail provides clear guidance on what needs to be addressed, making the comment 5. Therefore, this comment aligns with category 5.","The review point consists of a series of questions seeking clarification on specific aspects of the paper, such as the explanation in sections 243245 and the choice of knn over other classifiers. These questions are factual and do not contain subjective claims or opinions that require verification. They are requests for clarification and do not present any claims that need to be substantiated. Therefore, the comment is classified as ""No.""","The review comment identifies specific sections of the paper (243245 and 247) where the explanation is unclear. It requests elaboration on the calculation of 5*3 and the choice of knn over other classifiers, including the specifics of k and the distance metric used. This feedback is clear and actionable, as it directs the authors to clarify these points, which could significantly improve the clarity and comprehensibility of their work. By addressing these questions, the authors can enhance the reader""s understanding of their methodology and results. Therefore, the comment is rated as 5, as it provides specific guidance for improvement."
NIPS_2018_814,NIPS_2018,"1. The key question is âDo we need to make capsule networks explicitly equivariant?â My answer is âmost likely noâ. Because the idea of capsule networks is not to push for a specific and restricted type of equivariance. It allows freedom in the pose vector and choses the routing algorithm such that the data speak for itself and the pose matrices automatically capture the ârightâ variances in the lower level features. 2. Continuing the previous comment, arenât we unnecessarily restricting the capsule networks? Unfortunately, in the experiments the group CapNet is deeper than the simple CapNet and the authors do not provide the number of parameters in the group CapNet. So, we cannot judge how much enforcing the equivariance strictly is beneficial. Because the original CapNet does capture some variations in the data. 3. Unfortunately, the authors omit a key baseline in the experiments: the group-CNN. Not only it is not clear how much extra accuracy is due to CapNets, but also, they do not validate their argument in Section 4. I certainly enjoyed reading this paper, mainly because of the insights and rigor that it brings to the capsule networks idea. Unfortunately, there are doubts about about necessity of this idea and the experiments do not provide any solid evidence either. ======================== After reading the authors' rebuttal: I still do not understand why the proposed model is not directly comparable to the original G-CNN. In the rebuttal, the authors talk about keeping interpretability. But the current version of the paper does not have any experimental validation of these claims. As I mentioned in the original review, I like this paper, but I think it is not ready for publication at the current shape.","2. Continuing the previous comment, arenât we unnecessarily restricting the capsule networks? Unfortunately, in the experiments the group CapNet is deeper than the simple CapNet and the authors do not provide the number of parameters in the group CapNet. So, we cannot judge how much enforcing the equivariance strictly is beneficial. Because the original CapNet does capture some variations in the data.",6515,2,"The review comment raises a concern about potentially restricting the capsule networks unnecessarily and notes that the group CapNet is deeper than the simple CapNet. It also points out that the authors do not provide the number of parameters in the group CapNet, making it difficult to assess the benefits of enforcing equivariance strictly. However, the comment does not provide explicit guidance or suggestions on how the authors should address these issues. It lacks actionable details, such as recommending specific analyses or comparisons to clarify the benefits of the group CapNet. As a result, the authors may be uncertain about how to respond to the feedback effectively. Therefore, this comment is barely actionable.",3,"The review comment raises a relevant concern about the potential restriction of capsule networks and questions the experimental setup, specifically the depth and parameter count of the group CapNet compared to the simple CapNet. It highlights a gap in the information provided, which prevents a thorough evaluation of the benefits of enforcing equivariance. This feedback is somewhat helpful as it points out an area that requires clarification and additional information, prompting the authors to provide more details about their experimental setup. However, the comment could be more helpful if it offered specific suggestions on how to address these concerns or improve the analysis. Overall, it provides some direction for improvement but lacks comprehensive guidance, aligning with a score of 3.",3,"The comment addresses the issue of restricting capsule networks and provides specific feedback on the experiments, particularly regarding the depth of the group CapNet compared to the simple CapNet. It also highlights the lack of information about the number of parameters in the group CapNet, which prevents a proper assessment of the benefits of enforcing equivariance. However, the comment does not explicitly mention which part of the paper discusses these experiments or the capsule networks, making it challenging for the authors to pinpoint the exact section that needs revision. While the authors might have an idea of where these topics are discussed, the comment lacks full grounding. It is specific about the issue of parameter information but does not provide detailed guidance on how to address it. Therefore, this comment is weakly grounded and specific, aligning with category 3.",3,"The review point questions whether capsule networks are being unnecessarily restricted and highlights a lack of information regarding the number of parameters in the group CapNet. This critique is somewhat verifiable as it raises a valid concern about the experimental setup and the ability to assess the benefits of enforcing equivariance. However, it lacks specific examples or references to support the claim that the restriction might be unnecessary or to explain the importance of parameter count in this context. Additional details or comparisons would strengthen the justification, making it easier for authors to address the concern. Therefore, the comment is categorized as 3: Somewhat Verifiable.",6482,"2. continuing the previous comment, arenât we unnecessarily restricting the capsule networks? unfortunately, in the experiments the group capnet is deeper than the simple capnet and the authors do not provide the number of parameters in the group capnet. so, we cannot judge how much enforcing the equivariance strictly is beneficial. because the original capnet does capture some variations in the data.",3,5,3,3,"The review point raises a concern about the restriction imposed by the capsule networks and questions the necessity of this restriction. It also points out that the authors do not provide the number of parameters in the group CapNet, which makes it difficult to assess the benefits of enforcing equivariance. While the comment highlights an issue and suggests that the authors should provide more information, it does not explicitly instruct the authors to include the number of parameters or to address the restriction. The action is implicit and somewhat vague, as the authors can infer that they need to provide more details but are not given specific guidance on how to do so. Therefore, this comment is 3.","The comment is fully grounded as it explicitly mentions ""the previous comment,"" which allows the authors to identify the part of the paper being addressed. It also specifies the issue by questioning the necessity of restricting capsule networks and points out the lack of information about the number of parameters in the group CapNet. This provides clear guidance on what needs to be addressed, making the comment 5. Therefore, this comment aligns with category 5.","The review point raises a concern about the restriction imposed by the capsule networks and questions the necessity of this restriction. It points out that the group CapNet is deeper than the simple CapNet and that the authors do not provide the number of parameters in the group CapNet, which makes it difficult to assess the benefits of enforcing equivariance. The comment also notes that the original CapNet captures some variations in the data. While the comment provides some reasoning, it lacks specific examples or references to support the claim that the restriction is unnecessary or that the lack of parameter information hinders evaluation. The reasoning is 3, as it highlights a potential issue but requires more detailed evidence or examples to fully substantiate the claim. Therefore, the comment is rated as 3.","The review comment continues the discussion from the previous point, questioning the necessity of restricting the capsule networks. It highlights a potential issue with the experimental setup, noting that the group CapNet is deeper than the simple CapNet and that the authors do not provide the number of parameters in the group CapNet. This lack of information makes it difficult to assess the benefits of enforcing equivariance. The comment also points out that the original CapNet captures some variations in the data, which further complicates the evaluation. While the comment identifies a specific issue and provides some context, it lacks detailed guidance or suggestions on how the authors might address this concern or improve their experimental setup. The feedback is 3 as it prompts the authors to consider these aspects, but it could be more actionable with additional guidance. Therefore, the comment is rated as 3."
zweyouirw7,ICLR_2025,"1. Although the hybrid architecture is described in detail, the explanations of specific processes, such as spike-driven token selection and intra- and inter-stage feature fusion, could be clearer. Including pseudo-code or flow diagrams might enhance the reader’s understanding of the model’s operation.2. While the results on the Gen1 dataset are compelling, it would strengthen the paper to evaluate the model on more diverse datasets, particularly larger or more complex event-based datasets, to demonstrate generalizability.3. The paper could benefit from a discussion on how Spike-TransCNN compares with hybrid SNN-ANN models, given their potential to balance energy efficiency and performance. This would contextualize the performance and energy efficiency gains of Spike-TransCNN more effectively.4. While there is an ablation study on some components, further exploration on the impact of various hyperparameters (e.g., number of time steps, membrane potential thresholds) could provide insights into optimizing the architecture for different applications.5. Consistent terminology, particularly around spiking mechanisms and attention mechanisms, would improve readability. Some abbreviations and terms could be clarified for non-specialist readers.6. The paper includes visualization results, but providing side-by-side comparisons with other models on challenging scenarios could offer a clearer view of the model’s strengths in handling occlusions and motion.","1. Although the hybrid architecture is described in detail, the explanations of specific processes, such as spike-driven token selection and intra- and inter-stage feature fusion, could be clearer. Including pseudo-code or flow diagrams might enhance the reader’s understanding of the model’s operation.",8600,5,"The comment suggests that the explanations of specific processes within the hybrid architecture could be clearer and offers concrete suggestions for improvement, such as including pseudo-code or flow diagrams. This provides the authors with explicit actions to take in order to enhance the clarity of their explanations. The feedback is clear and provides specific guidance on how to implement the suggested improvements, making it highly actionable.",4,"The review comment identifies specific areas within the paper where the explanations could be clearer, namely the spike-driven token selection and intra- and inter-stage feature fusion processes. It provides actionable suggestions for improvement, such as including pseudo-code or flow diagrams, which could help enhance the reader's understanding of the model's operation. This feedback is mostly helpful as it offers clear guidance on how the authors can improve the clarity and comprehensibility of their draft. However, it could be further improved by providing more detailed suggestions or examples of how to implement these enhancements. Therefore, the comment aligns with a score of 4.",5,"The comment is fully grounded as it explicitly mentions the ""hybrid architecture"" and specific processes like ""spike-driven token selection"" and ""intra- and inter-stage feature fusion,"" allowing the authors to accurately identify the parts of the paper being addressed. It also provides specific suggestions for improvement, such as including pseudo-code or flow diagrams to enhance understanding. This level of detail provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.",3,"The review point suggests that while the hybrid architecture is well-described, the explanations of specific processes like spike-driven token selection and feature fusion could be clearer. The suggestion to include pseudo-code or flow diagrams is based on logical reasoning to enhance understanding. However, the comment lacks specific examples or references to support the claim that these explanations are unclear, making it somewhat verifiable. The authors would need to interpret the feedback and determine how best to address it, thus justifying a score of 3: Somewhat Verifiable.",4668,"1. although the hybrid architecture is described in detail, the explanations of specific processes, such as spike-driven token selection and intra- and inter-stage feature fusion, could be clearer. including pseudo-code or flow diagrams might enhance the reader’s understanding of the model’s operation.",5,5,3,4,"The review comment suggests that the explanations of specific processes, such as spikedriven token selection and intra and interstage feature fusion, could be clearer. It recommends including pseudocode or flow diagrams to enhance the reader’s understanding of the model’s operation. This feedback is explicit and provides a concrete suggestion on how to improve the clarity of the paper. The authors know exactly what needs to be done to enhance the reader’s understanding, making this comment 5.","The comment is fully grounded as it explicitly mentions the ""hybrid architecture"" and specific processes like ""spikedriven token selection"" and ""intra and interstage feature fusion,"" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it suggests ways to improve the clarity of these processes, such as including pseudocode or flow diagrams. This provides clear guidance on what needs to be addressed to enhance the reader’s understanding. Therefore, the comment is 5, aligning with category 5.","The review point suggests that the explanations of specific processes, such as spikedriven token selection and intra and interstage feature fusion, could be clearer. It recommends including pseudocode or flow diagrams to enhance understanding. However, the comment does not provide specific examples or detailed reasoning to support why these processes are unclear or how the suggested enhancements would improve the paper. This lack of detailed justification makes the claim 3, as the authors would need to infer the exact issues and potential improvements based on the general suggestion. Therefore, the comment is rated as 3.","The review comment identifies a specific area where the paper could be improved: the clarity of explanations for certain processes within the hybrid architecture. It suggests that including pseudocode or flow diagrams could enhance the reader’s understanding of the model’s operation. This feedback is actionable and provides a clear direction for the authors to improve the clarity and comprehensibility of their work. However, the comment could be more helpful if it offered specific examples of where these diagrams or code snippets could be most beneficial or provided additional guidance on how to effectively integrate them into the paper. Overall, the comment is 4 as it directs the authors toward a meaningful improvement, but it could be more comprehensive with further details."
Z9AZsU1Tju,ICLR_2024,"1.The inspiration from neuroscience research seems simple, and unnecessary to the main processing pipeline of the main work, making the main work distracting.2.The order of the modality in processing the data seems quite important, which highly impacts the performance. For such an importance factor, it is better to provide further theoretical analysis or practical guidance on it.3.The paper should include some recent SOTA works, at lease works in 2022, on comparing the sentiment analysis.","1. The inspiration from neuroscience research seems simple, and unnecessary to the main processing pipeline of the main work, making the main work distracting.",9004,1,"The review comment criticizes the inclusion of inspiration from neuroscience research, suggesting it is simple and unnecessary, and that it distracts from the main work. However, it does not provide any specific guidance or suggestions on how the authors could address this issue or improve the focus of their work. The comment lacks actionable details, such as recommending ways to integrate the neuroscience inspiration more effectively or suggesting alternative approaches. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.",1,"The comment expresses a subjective opinion about the inspiration from neuroscience research, suggesting it is simple and unnecessary, and that it distracts from the main work. However, it lacks specificity and does not provide actionable feedback or suggestions for improvement. Without identifying particular weaknesses or offering guidance on how the authors might address the perceived distraction, the comment does not assist the authors in improving their draft. Therefore, it aligns with a score of 1, as it is not helpful at all.",1,"The comment suggests that the inspiration from neuroscience research appears simple and unnecessary, potentially distracting from the main work. However, it does not specify which part of the paper this critique applies to, leaving the authors unable to identify the exact section or content being addressed. Additionally, the comment lacks specificity in explaining why the neuroscience inspiration is considered unnecessary or how it distracts from the main work. Without clear guidance or specific references, the comment is not grounded and not specific, aligning with a score of 1.",1,"The review point claims that the inspiration from neuroscience research is ""simple and unnecessary,"" suggesting it distracts from the main work. However, the comment lacks any supporting evidence, reasoning, or references to justify this claim. There is no detailed explanation or examples provided to help the authors understand why the neuroscience inspiration is considered unnecessary or distracting. As a result, the claim is unverifiable.",5336,"1. the inspiration from neuroscience research seems simple, and unnecessary to the main processing pipeline of the main work, making the main work distracting.",1,1,1,1,"The review point claims that the inspiration from neuroscience research is unnecessary and distracting to the main work. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this issue, such as suggesting ways to integrate the neuroscience inspiration more effectively or explaining why it is necessary. Without any actionable steps or suggestions, the authors are left without a clear path forward. Therefore, this comment is 1.","The comment critiques the inspiration from neuroscience research, suggesting it is unnecessary and distracting to the main work. However, it does not specify which part of the paper this critique pertains to, such as a particular section or figure. Without explicit references or clear indicators, the authors cannot confidently determine the exact part of the paper being addressed. Additionally, the comment lacks specificity regarding what aspects of the neuroscience inspiration are considered unnecessary or distracting. Therefore, the comment is 1 and not specific, aligning with a score of 1.","The review point claims that the inspiration from neuroscience research is unnecessary and distracting to the main work. However, it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without specific details or references to the neuroscience research or its relevance to the main work, the authors may find it challenging to understand and address the critique. Therefore, the comment is considered 1.","The review comment critiques the inspiration from neuroscience research, suggesting that it is unnecessary and distracting to the main work. However, it does not provide any specific examples or detailed reasoning to support this claim, nor does it offer suggestions on how the authors might address this issue or integrate the neuroscience inspiration more effectively. Without actionable feedback or constructive guidance, the comment lacks value for the authors in improving their draft. Therefore, it is rated as 1."
UmdotAAVDe,ICLR_2025,"1. As we know LLM often hallucinates. How do the authors quality check the generated dataset?2. While the authors concluded that induction and transduction approaches are complementary, I hoped to see more in-depth analysis of why induction works better for some while transduction works better for the others.3. While the authors also discussed the limitation that such method is only tested on ARC, I still think this is a weakness. I wonder the conclusion would be different for some other real-world datasets.",1. As we know LLM often hallucinates. How do the authors quality check the generated dataset?,11453,2,"The review point raises a concern about the quality checking of the generated dataset, given that large language models (LLMs) often hallucinate. It implicitly suggests that the authors should address this issue by explaining their quality control measures. However, the comment does not provide explicit instructions or concrete steps on how to improve the draft in this regard. The action is implicit and somewhat vague, as the authors need to infer that they should include a discussion on quality checking but are not given specific guidance on how to do so. Therefore, this comment is barely actionable.",3,"The review comment raises an important question about the quality control of datasets generated by large language models (LLMs), which are known to sometimes produce inaccurate or ""hallucinated"" information. This is a critical aspect that the authors should address to ensure the reliability of their dataset. However, the comment does not provide specific suggestions or methods for how the authors might implement quality checks, which limits its helpfulness. While it points out a significant area for improvement, the lack of actionable guidance means the authors may struggle to effectively address the issue. Therefore, the comment is somewhat helpful, as it highlights a key concern but lacks depth and specificity in its feedback.",1,"The comment raises a concern about the quality checking of the generated dataset, particularly in the context of large language models (LLMs) that are known to hallucinate. However, it does not specify which part of the paper this concern relates to, such as the methodology, results, or discussion sections. Without explicit references or detailed guidance, the authors cannot effectively identify the areas needing improvement or clarification. As a result, the comment lacks both grounding and specificity, making it not grounded at all. Therefore, this comment aligns with a score of 1.",X,"The review point consists of a factual statement followed by a question. The statement ""As we know LLM often hallucinates"" is a widely recognized fact about large language models and does not constitute a subjective claim or opinion requiring verification. The subsequent question does not express an opinion or judgment but seeks clarification. Therefore, the review point should be labeled as ""No.""",4706,1. as we know llm often hallucinates. how do the authors quality check the generated dataset?,1,1,1,2,"The review point raises a concern about the potential for LLMs to hallucinate and questions how the authors quality check the generated dataset. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on what specific steps should be taken to address this concern or how to improve the quality checking process. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is 1.","The comment raises a concern about the potential for LLMs to hallucinate and questions how the authors quality check the generated dataset. However, it does not specify which part of the paper discusses the dataset or the quality checking process, making it difficult for the authors to identify the exact section that needs attention. The comment lacks grounding as it does not reference a specific section or part of the paper, and it is also not specific about what needs to be addressed regarding the quality checking process. Therefore, this comment is 1 and not specific, aligning with a score of 1.","The review point raises a concern about the potential for LLMs to hallucinate and questions how the authors quality check the generated dataset. However, it does not provide any supporting evidence, reasoning, or references to substantiate the claim. The lack of detailed explanation or examples makes it difficult for the authors to understand the basis of the concern or how to address it. Therefore, the comment is considered 1.","The review comment raises a valid concern about the potential for LLMs to hallucinate and questions how the authors quality check the generated dataset. This is a relevant issue that could impact the reliability and validity of the results. However, the comment lacks specificity and does not provide any guidance or suggestions on how the authors might address this concern. Without actionable advice or examples, the authors are left without a clear path to improve their draft. Therefore, the comment is 2, as it identifies a potential issue but does not offer any constructive feedback for improvement."
NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?","- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.",5941,3,"The review point suggests that the model implementation needs better justification, specifically questioning the stopping rule with n consecutive identical samples. It implies that the authors should provide a rationale for this choice and discuss its sensitivity. However, the comment does not explicitly instruct the authors on how to justify the stopping rule or what aspects of sensitivity should be addressed. The action is implicit and somewhat vague, as the authors need to infer that they should provide a justification and sensitivity analysis but lack detailed guidance on how to do so. Therefore, this comment is somewhat actionable.",3,"The review comment identifies a specific area where the manuscript could be improved, namely the justification of the model implementation, particularly the stopping rule with n consecutive identical samples. It points out that this rule seems arbitrary and lacks discussion on its sensitivity, which could be crucial for understanding the model's robustness and applicability. By highlighting these issues, the comment provides the authors with clear and actionable feedback on where to improve their manuscript. However, while it identifies the problem, it does not offer detailed suggestions or examples on how to address these issues, which could enhance its helpfulness. Overall, the comment is somewhat helpful as it directs the authors to a critical area needing clarification, but it could be more comprehensive with additional guidance.",3,"The comment addresses a specific aspect of the model implementation, namely the stopping rule with ""n consecutive identical samples."" It questions the justification for this choice and suggests discussing its sensitivity. However, the comment does not specify which part of the paper this issue is discussed in, making it weakly grounded. The comment is specific in detailing what needs to be addressed regarding the stopping rule and its justification. Therefore, the comment is weakly grounded and specific, aligning with category 3.",2,"The review point raises a claim that the stopping rule using ""n consecutive identical samples"" is arbitrary and lacks justification, particularly in terms of neural or behavioral parallels. While the comment identifies a potential issue with the model implementation, it does not provide specific examples, detailed reasoning, or references to support the claim. The lack of detailed justification or evidence makes it challenging for the authors to understand and address the concern effectively. Therefore, the claim is categorized as borderline verifiable, as it provides some insight but lacks sufficient support to be fully actionable.",2141,"- model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.",3,5,3,4,"The review point suggests that the model implementation should be better justified, specifically questioning the stopping rule with n consecutive identical samples. It points out that this rule seems arbitrary and lacks discussion on its sensitivity to the parameter n. While the comment identifies areas for improvement, it does not provide explicit guidance on how to address these issues or suggest specific changes to the model implementation. The authors are left to infer that they need to provide a more detailed justification for the stopping rule and discuss its sensitivity to n. However, the lack of concrete instructions or examples makes the action somewhat vague. Therefore, this comment is 3.","The comment is fully grounded as it explicitly mentions ""Model implementation,"" allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue with the stopping rule, particularly the use of ""n consecutive identical samples,"" and notes the lack of discussion on its sensitivity to ""n."" This provides clear guidance on what needs to be addressed, making the comment 5. Therefore, this comment aligns with category 5.","The review point claims that the model implementation should be better justified, specifically questioning the stopping rule with n consecutive identical samples. It suggests that this rule seems arbitrary and lacks discussion on its sensitivity to the parameter n. While the comment provides a logical critique, it does not offer specific examples or references to support the claim that the stopping rule is arbitrary or to substantiate the need for further discussion on sensitivity. This makes the claim 3, as it lacks detailed evidence or examples to fully substantiate the critique. Therefore, the comment is rated as 3.","The review comment identifies a specific area for improvement in the model implementation, namely the stopping rule with n consecutive identical samples. It questions the arbitrariness of this rule and suggests that the sensitivity to the parameter n is not discussed. This feedback is clear and actionable, as it directs the authors to provide a more detailed justification for the stopping rule and to explore its sensitivity to n. By addressing these points, the authors can enhance the robustness and clarity of their model implementation. However, the comment could be more helpful if it offered specific suggestions or examples on how to improve the justification or sensitivity analysis. Overall, the comment is 4, as it provides valuable guidance for improving the draft."
PFdjJiZjPj,ICLR_2024,"- The experimental evaluation and the observations are nice and make sense to share with the community, but the actual contribution of the paper beyond that is relatively minor.- Using self-generated code (instead of a placeholder) is not exactly a fair comparison when compared to previous work, as it samples the model 2x for each generation. Using the place-holder code was somewhat intended in the original technique so you only generate the code itself once.","- Using self-generated code (instead of a placeholder) is not exactly a fair comparison when compared to previous work, as it samples the model 2x for each generation. Using the place-holder code was somewhat intended in the original technique so you only generate the code itself once.",11669,3,"The review comment highlights a concern about the fairness of using self-generated code instead of a placeholder, as it involves sampling the model twice for each generation. It implies that the authors should reconsider their approach to align with the original technique, which only generates the code once. However, the comment does not explicitly instruct the authors on what specific changes to make or how to address this issue. The action is implicit and lacks concrete guidance, making it somewhat actionable.",3,"The review comment identifies a potential issue with the fairness of the experimental comparison, specifically noting that using self-generated code instead of a placeholder may lead to an unfair advantage by sampling the model twice for each generation. It highlights that the original technique intended to generate the code only once, suggesting that the current approach might deviate from the intended methodology. While the comment points out a significant concern, it lacks specific guidance or suggestions on how the authors might address this issue or adjust their methodology to ensure a fair comparison. This limits the comment's utility for the authors, making it somewhat helpful as it provides a direction for improvement but lacks depth and actionable advice.",3,"The comment addresses a specific issue regarding the use of self-generated code instead of a placeholder, which affects the fairness of the comparison with previous work. However, it does not explicitly mention which part of the paper this issue is discussed in, making it weakly grounded. The comment is specific in detailing the problem with the comparison method, providing clear guidance on what needs to be addressed. Therefore, the comment is weakly grounded but specific, aligning with category 3.",4,"The review point critiques the use of self-generated code for comparison, suggesting it is unfair because it samples the model twice for each generation, unlike the original technique that uses a placeholder code to generate only once. The comment provides a logical reasoning by explaining the difference in sampling frequency between the current approach and the original technique. However, it lacks specific references or examples from previous work to fully substantiate the claim. This makes the comment mostly verifiable, as it provides a clear reasoning but could benefit from additional evidence or references to strengthen the argument. Therefore, it aligns with a score of 4: Mostly Verifiable.",3821,"- using self-generated code (instead of a placeholder) is not exactly a fair comparison when compared to previous work, as it samples the model 2x for each generation. using the place-holder code was somewhat intended in the original technique so you only generate the code itself once.",2,3,3,3,"The review point highlights a potential issue with the comparison between the current work and previous studies, specifically regarding the use of selfgenerated code versus a placeholder. It suggests that using selfgenerated code may not be a fair comparison due to the model being sampled 2x for each generation, whereas the placeholder code was only generated once. However, the comment does not provide explicit guidance on how the authors should address this issue or suggest alternative methods for comparison. The action is implicit and somewhat vague, as the authors are left to infer that they should reconsider their comparison methodology but are not given concrete steps to follow. Therefore, this comment is barely actionable.","The comment addresses a specific issue related to the comparison between the current work and previous studies, specifically regarding the use of selfgenerated code versus a placeholder. It provides a clear explanation of the potential unfairness in the comparison due to the model being sampled 2x for each generation. However, it does not specify which part of the paper this issue is discussed in, making it weakly grounded. The comment is specific in detailing the issue with the comparison, but without explicit references to sections or figures, the authors may find it challenging to pinpoint the exact location for revision. Therefore, this comment is 3, aligning with category 3.","The review point claims that using selfgenerated code instead of a placeholder is not a fair comparison to previous work, as it samples the model 2x for each generation. The reviewer provides a logical explanation for why this might be unfair, noting that the original technique intended to generate the code only once. However, the comment lacks specific references or examples to support the claim, making it 3. The authors would need to consider the reasoning and potentially look into the original technique to fully understand the issue. Therefore, the comment is rated as 3.","The review comment identifies a potential issue with the comparison between the current work and previous studies, specifically regarding the use of selfgenerated code versus a placeholder. It highlights that using selfgenerated code may not be a fair comparison due to the model being sampled 2x for each generation, whereas the placeholder code was only generated once. This feedback is 3 as it points out a potential flaw in the methodology, prompting the authors to reconsider their comparison approach. However, the comment could be more helpful if it provided suggestions on how to address this issue or alternative methods for comparison. Therefore, it aligns with a score of 3, indicating that the comment is 3."
uvq4Nh8eZB,ICLR_2024,"1. The novelty is limited. There are many prior studies have explored semi-supervised federated learning [1-3] and knowledge distillation [4-6]. The paper does not propose a new framework of FedSSL but just naively combining differential privacy techniques. The paper does not illustrate the difference between FedCT and those previous work.2. The motivation to protect the privacy of soft predictions is weak. Comparing to raw data/ features map, the soft predictions are not likely to leak privacy.3. The paper is not well-organized and using non-standard terminology. For example, the term 'semi-honst' in section 4. Usually, we use 'honest-but-curious' to descrtibe the server which can not modify the local updates but trys to infer private information of clients.4. The baselines are not comprehensive. DP-FedAvg is using DP to the updated gradients but the proposed FedCT is using DP in the soft predictions, it is not comparable. FedCT should compare other FedSSL methods [1-3] and conduct attach on the method to verify the effect of FedCT.5. Setting for data heterogenity is not sufficient. With beta = 2 for non-i.i.d setting is not good. Most of papers set beta = 0.01 or 0.1 to create heterogeneous data partitions. Reference:[1] Diao E, Ding J, Tarokh V. SemiFL: Semi-supervised federated learning for unlabeled clients with alternate training[J]. Advances in Neural Information Processing Systems, 2022, 35: 17871-17884.[2] Lin H, Lou J, Xiong L, et al. Semifed: Semi-supervised federated learning with consistency and pseudo-labeling[J]. arXiv preprint arXiv:2108.09412, 2021.[3] Itahara S, Nishio T, Koda Y, et al. Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data[J]. IEEE Transactions on Mobile Computing, 2021, 22(1): 191-205.[4] Li D, Wang J. Fedmd: Heterogenous federated learning via model distillation[J]. arXiv preprint arXiv:1910.03581, 2019.[5] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning[C]//International conference on machine learning. PMLR, 2021: 12878-12889.[6] Chen H, Vikalo H. The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation[J]. arXiv preprint arXiv:2301.08968, 2023.","3. The paper is not well-organized and using non-standard terminology. For example, the term 'semi-honst' in section 4. Usually, we use 'honest-but-curious' to descrtibe the server which can not modify the local updates but trys to infer private information of clients.",8671,3,"The review point highlights two issues: the paper's organization and the use of non-standard terminology, specifically mentioning the term 'semi-honst' and suggesting the more commonly used term 'honest-but-curious.' While the comment identifies a specific example of non-standard terminology and provides a suggestion for improvement, it does not offer detailed guidance on how to reorganize the paper or address other instances of non-standard terminology. The action regarding terminology is explicit and concrete, but the suggestion about organization is vague. Therefore, the comment is somewhat actionable.",3,"The review comment identifies two key issues with the paper: poor organization and the use of non-standard terminology. It provides a specific example of the term 'semi-honst' used in section 4, suggesting that 'honest-but-curious' is the more commonly accepted term. This feedback is valuable as it highlights areas where the paper could be improved for clarity and adherence to standard terminology, which is crucial for effective communication in academic writing. However, the comment could be more helpful if it offered additional examples of organizational issues or provided suggestions on how to improve the paper's structure. Overall, the feedback is somewhat helpful as it points out important areas for improvement but lacks comprehensive guidance. Therefore, it aligns with a score of 3.",5,"The comment identifies a specific issue with the organization and terminology used in the paper, providing an example with the term 'semi-honst' in section 4. This allows the authors to accurately pinpoint the section being addressed, ensuring full grounding. The comment also specifies the issue by suggesting the use of the more standard term 'honest-but-curious' to describe the server, which adds specificity to the feedback. Therefore, this comment is fully grounded and specific, aligning with category 5.",4,"The review point claims that the paper is disorganized and uses non-standard terminology, providing an example with the term 'semi-honst' in section 4. The reviewer contrasts this with the more commonly used term 'honest-but-curious,' which is typically used to describe a server that cannot modify local updates but tries to infer private information from clients. This comparison provides a clear example of the non-standard terminology, making the claim mostly verifiable. However, the comment could be strengthened by providing additional examples or references to standard terminology, which would make it fully verifiable. Therefore, the comment aligns with a label of 4: Mostly Verifiable.",8425,"3. the paper is not well-organized and using non-standard terminology. for example, the term 'semi-honst' in section 4. usually, we use 'honest-but-curious' to descrtibe the server which can not modify the local updates but trys to infer private information of clients.",3,5,3,3,"The review point highlights two issues: the paper""s lack of organization and the use of nonstandard terminology. It specifically mentions the term ""semihonst"" in section 4, suggesting that the correct term should be ""honestbutcurious."" However, the comment does not provide explicit guidance on how to improve the organization of the paper or suggest alternative terminology. The action is implicit and somewhat vague, as the authors are left to infer that they need to reorganize their paper and use standard terminology. While the comment identifies specific issues, it lacks concrete steps for the authors to take, making it 3.","The comment is fully grounded as it explicitly mentions ""section 4"" and the term ""semihonst,"" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it points out the use of nonstandard terminology and suggests a correction, providing a clear direction for improvement. Therefore, this comment is 5, aligning with category 5.","The review point claims that the paper is not wellorganized and uses nonstandard terminology, specifically pointing out the term ""semihonst"" in section 4. The reviewer suggests that the correct term should be ""honestbutcurious,"" which is a common term in the field. This claim is 3 as it provides a specific example of the nonstandard terminology and suggests a correction. However, the comment lacks detailed reasoning or references to support the claim about the paper""s organization, making it 3. Therefore, the comment aligns with a score of 3.","The review comment identifies two issues with the paper: its lack of organization and the use of nonstandard terminology. It specifically points out the term ""semihonst"" in section 4, suggesting that the correct term should be ""honestbutcurious."" This feedback is 3 as it highlights areas for improvement, such as ensuring the paper is wellorganized and using standard terminology. However, the comment could be more helpful if it provided additional guidance on how to improve the organization or suggested alternative terminology for other nonstandard terms used in the paper. Overall, the comment offers some direction but lacks depth and specificity, making it 3."
ARR_2022_145_review,ARR_2022,"I will list the detailed weakness in the next section. In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable.1. "" a smoother loss"" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting? There is a text overlap between 0.00 and 0.03 around the origin.2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is ""our method is also the **best** among all the baseline methods"", which is a contradiction to me. Later the paper suggests that ""outperforming the **baselines** of our implementation."", what is the difference between ""baseline"" and ""your re-implemented results"". 4. Some introduction to the datasets might also be helpful, such as the size of each dataset. 5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here. 8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting? 9. Line 22, ""Bert"" --> ""BERT""","6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?",1050,1,"The comment raises a question about the availability of labels for the test sets in the GLUE datasets, indicating some confusion. However, it does not provide any explicit or implicit action for the authors to take. The authors are left without guidance on how to address the reviewer's confusion or what changes, if any, are needed in the draft. Since the comment lacks actionable advice or suggestions, it is unactionable.",3,"The comment raises a question about the availability of labels for the test set performance on GLUE datasets, indicating some confusion. While it identifies a potential area of misunderstanding, it does not provide any guidance or suggestions on how the authors might address this issue in their paper. The comment lacks depth and actionable feedback, making it only somewhat helpful. It points out a potential weakness but does not offer a clear path for the authors to improve their draft. Therefore, the comment aligns with a score of 3.",1,"The comment raises a question about the test set performance on GLUE datasets and whether the labels are publicly available. However, it does not specify which part of the paper this question pertains to, such as a particular section, table, or figure. Without explicit references, the authors cannot confidently determine which part of the paper needs clarification or additional information. The comment is also not specific in terms of providing guidance on how to address the confusion. Therefore, this comment is not grounded and not specific, aligning with a score of 1.",X,"The comment is a request for clarification regarding the availability of labels for the GLUE datasets' test set performance. It does not contain any subjective claims or judgments, making it a factual inquiry. Therefore, it fits the ""No"" label.",9639,"6. i am a little confused about the test set performance on these glue datasets, are the labels to these sets not available to the public?",1,1,1,1,"The review point expresses confusion about the test set performance on the GLUE datasets and questions whether the labels to these sets are available to the public. However, it does not provide any explicit or implicit actions for the authors to take. The comment lacks guidance on how the authors might address the confusion or clarify the availability of labels, making it 1. The authors are left without a clear understanding of what steps to take to improve their draft based on this feedback.","The comment expresses confusion about the test set performance on the GLUE datasets and questions the availability of labels to the public. However, it does not specify which part of the paper this confusion pertains to, such as a particular section or figure discussing the GLUE datasets. Without explicit references or context, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what needs to be clarified or addressed regarding the test set performance or label availability. Therefore, this comment is 1 and highly unspecific, aligning with a score of 1.","The review point expresses confusion about the test set performance on the GLUE datasets and questions the availability of labels to the public. However, it does not provide any supporting evidence, reasoning, or references to justify the confusion or the need for clarification. The comment lacks specificity and does not offer any actionable steps for the authors to address the issue. As a result, the claim is considered 1, as it does not provide sufficient information for the authors to understand or address the concern. Therefore, the comment is rated as 1.","The review comment expresses confusion about the test set performance on the GLUE datasets and questions whether the labels to these sets are available to the public. However, it does not provide any specific suggestions or guidance on how the authors might clarify this issue or address the confusion. The comment lacks actionable feedback, leaving the authors without a clear understanding of what steps to take to improve their draft. As a result, the comment is 1, aligning with a score of 1."
ICLR_2023_385,ICLR_2023,"- The statement ""These methods rely on maximizing the similarity between..."" in the Abstract is inaccurate. Not all self-supervised visual representation learning methods are based on such a contrastive learning scheme. Instead, most of the early methods are based on pretext tasks. Suggest revising the corresponding claims and also the title.- The main idea of this work is based on the assumption of the self-supervised visual learning method being contrastive-based ones. Whereas there are quite a few non-contrastive learning approaches. As a result, the main contributions of this work may be a bit limited.- It is unclear how the defined u_\psi was guaranteed to represent the transformation t. From the description, t was encoded by another set of layers, which are also learnable, making the equivariance here (esp. the ""equivariant predictor"") a bit misleading. The encoding of the transformations (Sec. 2.3.2) is rather confusing. In the beginning, it looks like the authors used one-hot encoding, but later some of the augmentations directly use the coordinates or real values (i.e. 1, 2, 3). The generalization for the augmentation set, as a result, is another issue. What if new augmentations were included? How would they be encoded?- It is unclear why the final model still needs the conventional invariance loss (i.e. the first term in the final loss function), if the proposed new equivariance-based loss is as claimed to be effective. Jointly optimizing both invariance and equivariance also seems to be confusing. How does the model actually learn in this case?- It is unclear why the Barlow Twins was only evaluated on the small-scale dataset CIFAR10.- The quality of the learned representation was only evaluated on the linear evaluation setting, which is a bit insufficient to get a clear conclusion. There are quite a few other downstream tasks that could be used as reported in the literature (e.g. fine-tuning, detection, and segmentation to name a few).- The proposed method was motivated by the case where ""augmentation information is useful"" as claimed by the authors, and example cases are flowers and birds (as claimed in the Introduction). But this was not validated in the experiment. There are some fine-grained datasets for such categories (e.g. the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset) that should have been used to validate the claims.- It is unclear why not use an objective similar to Eq. (5) or (6) to constrain the equivariance model training, but instead only maximize the similarity between z_i' and \hat{z}_i'?- Would an absolute value makes more sense for Eq (5)? Otherwise, what does a negative value mean, as the second case ""H-flip"" shown in Fig. 3?- Line below Eq. (5), ""Fig. 4"" should be ""Fig. 3"".","- The proposed method was motivated by the case where ""augmentation information is useful"" as claimed by the authors, and example cases are flowers and birds (as claimed in the Introduction). But this was not validated in the experiment. There are some fine-grained datasets for such categories (e.g. the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset) that should have been used to validate the claims.",3512,5,"The review point suggests that the authors should validate their claims about the usefulness of augmentation information by using specific datasets, such as the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset. This is an explicit and concrete action, as it clearly instructs the authors on what needs to be done to improve their draft. The comment provides specific examples of datasets that should be used, making it highly actionable.",4,"The review comment is mostly helpful as it identifies a discrepancy between the motivation for the proposed method and its experimental validation. It points out that while the authors claim augmentation information is useful for cases like flowers and birds, this was not tested in the experiments. The reviewer provides specific examples of fine-grained datasets, such as the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset, that could be used to validate these claims. This feedback is actionable and provides a clear direction for the authors to strengthen their work by aligning their experiments with their stated motivations. However, the comment could be more helpful if it included suggestions on how to incorporate these datasets into the experimental design or discussed potential challenges in doing so. Overall, the comment provides valuable guidance for improving the paper, aligning with a score of 4.",5,"The comment is fully grounded as it references the introduction where the authors claim that augmentation information is useful, specifically mentioning examples like flowers and birds. This allows the authors to accurately identify the part of the paper being addressed. The comment is also specific because it points out that the claim was not validated in the experiment and suggests using specific datasets (Oxford 102 flowers and Caltech-UCSD Birds-200) to validate the claims. Therefore, this comment is fully grounded and specific, aligning with category 5.",3,"The review point critiques the lack of validation for the claim that ""augmentation information is useful"" in the context of flowers and birds, as mentioned in the introduction. The reviewer suggests using specific datasets, such as the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset, to validate this claim. While the comment provides a logical reasoning for the need for validation and mentions specific datasets, it does not provide detailed evidence or references to support the necessity of these datasets for validation. This makes the claim somewhat verifiable, as the authors can understand the critique but may need more detailed guidance to fully address it.",3448,"- the proposed method was motivated by the case where ""augmentation information is useful"" as claimed by the authors, and example cases are flowers and birds (as claimed in the introduction). but this was not validated in the experiment. there are some fine-grained datasets for such categories (e.g. the oxford 102 flowers dataset and the caltech-ucsd birds-200 dataset) that should have been used to validate the claims.",3,5,4,4,"The review point highlights a gap in the validation of the proposed method, specifically noting that the authors claim augmentation information is useful but do not provide experimental validation. It suggests using finegrained datasets like the Oxford 102 flowers dataset and the CaltechUCSD Birds200 dataset to validate the claims. While the comment implies that the authors should conduct additional experiments to validate their claims, it does not explicitly instruct them to do so. The action is implicit and somewhat vague, as the authors need to infer that they should conduct experiments on these datasets. Therefore, this comment is 3.","The comment is fully grounded as it explicitly mentions the ""Introduction"" section, allowing the authors to accurately identify the part of the paper being addressed. It also specifies the issue by pointing out that the proposed method""s motivation, based on the claim that ""augmentation information is useful,"" was not validated in the experiment. The comment suggests using specific finegrained datasets, such as the Oxford 102 flowers dataset and the CaltechUCSD Birds200 dataset, to validate the claims. This provides clear guidance on what needs to be addressed, making the comment 5. Therefore, this comment aligns with category 5.","The review point claims that the proposed method""s motivation, based on the usefulness of augmentation information, was not validated in the experiment. It suggests using specific finegrained datasets like the Oxford 102 flowers dataset and the CaltechUCSD Birds200 dataset to validate the claims. The comment provides a logical reasoning by pointing out the lack of validation and offers a specific suggestion for improvement. However, it could be strengthened by providing more detailed reasoning or examples of how these datasets could be used to validate the claims. Overall, the comment is 4, as it provides a clear direction for improvement but lacks detailed evidence or examples to fully substantiate the claim. Therefore, it aligns with a score of 4.","The review comment identifies a critical gap in the paper, specifically that the proposed method""s motivation, based on the claim that ""augmentation information is useful,"" was not validated in the experiment. It suggests using specific finegrained datasets, such as the Oxford 102 flowers dataset and the CaltechUCSD Birds200 dataset, to validate the claims. This feedback is clear and actionable, providing the authors with a concrete suggestion for improving the validation of their method. By addressing this gap, the authors can strengthen the credibility and robustness of their work. Therefore, the comment is 4, as it offers a specific and actionable suggestion for enhancing the paper""s validity and impact."
5lokEzttBF,ICLR_2025,"1. The motivation of this paper is unclear. Existing research has widely recognized the importance of ensuring diversity in instruction-tuning data. However, this paper seems to oppose this common understanding without strong justification. The experiments do not persuade me, as they are somewhat weak: both the dataset and the LLM size are limited. The results are unconvincing and, if not thoroughly validated, could potentially mislead the community.2. Although the paper proposes ""linguistic form"" and ""semantic surprisal"" as key style elements, the definitions and measurement methods of these concepts are slightly vague in some sections, especially the concept of ""semantic surprisal"", which still needs further clarification and explanation.3. Although the paper provides a lot of experimental data and results, the design and interpretation of some experiments are a bit complicated. For example, in the comparison of different data selection methods, some performance differences are not adequately explained. For some performance degradation cases (such as the performance of LLAMA2-13B), the paper does not explore the reasons behind it in detail.","1. The motivation of this paper is unclear. Existing research has widely recognized the importance of ensuring diversity in instruction-tuning data. However, this paper seems to oppose this common understanding without strong justification. The experiments do not persuade me, as they are somewhat weak: both the dataset and the LLM size are limited. The results are unconvincing and, if not thoroughly validated, could potentially mislead the community.",9346,1,"The review point raises concerns about the paper's motivation, noting that it appears to contradict the widely recognized importance of diversity in instruction-tuning data without providing strong justification. It also criticizes the experiments for being weak due to limited dataset and LLM size, and suggests that the results are unconvincing and potentially misleading. However, the comment does not provide specific guidance or suggestions on how the authors could address these issues, improve the justification, or strengthen the experiments. The feedback lacks actionable details, leaving the authors uncertain about what steps to take to improve their draft. Therefore, this comment is unactionable.",2,"The comment identifies a critical issue with the paper's motivation, noting that it contradicts widely recognized research without providing strong justification. It also critiques the experiments for being weak due to limitations in both the dataset and the LLM size, and expresses concern that the results are unconvincing and could mislead the community if not thoroughly validated. While the comment highlights significant weaknesses, it lacks specific suggestions or guidance on how the authors could address these issues or improve their work. Without actionable advice, the feedback does not offer much value to the authors in terms of improving their draft. Therefore, it aligns with a score of 2, indicating that the comment is barely helpful.",2,"The comment addresses several aspects of the paper, including the unclear motivation, the opposition to a common understanding in existing research, and the perceived weaknesses in the experiments. However, it does not specify which sections or parts of the paper these issues pertain to, making it difficult for the authors to pinpoint the exact areas needing improvement. Additionally, while the comment mentions the limited dataset and LLM size, it lacks specific guidance on how to strengthen the experiments or provide stronger justification. This lack of detail and specificity makes the comment weakly grounded and not specific, aligning with a score of 2.",3,"The review point raises concerns about the paper's motivation, suggesting it contradicts established research on the importance of diversity in instruction-tuning data without strong justification. The reviewer supports this claim by critiquing the experiments as weak due to limited dataset and LLM size, and describes the results as unconvincing. However, the comment lacks specific examples or references to existing research that emphasize the importance of diversity, and does not provide detailed reasoning or evidence to substantiate the claim about the experiments' limitations. This makes the claim somewhat verifiable, as it requires more detailed evidence or examples to fully validate the argument.",5398,"1. the motivation of this paper is unclear. existing research has widely recognized the importance of ensuring diversity in instruction-tuning data. however, this paper seems to oppose this common understanding without strong justification. the experiments do not persuade me, as they are somewhat weak: both the dataset and the llm size are limited. the results are unconvincing and, if not thoroughly validated, could potentially mislead the community.",1,3,3,3,"The review point raises concerns about the motivation of the paper, suggesting that it opposes a widely recognized understanding without sufficient justification. It also criticizes the experiments as weak due to limited dataset and LLM size, questioning the persuasiveness and potential misleading nature of the results. However, the comment does not provide explicit guidance or suggestions on how the authors might address these concerns or improve their draft. The feedback lacks actionable details, leaving the authors uncertain about what specific changes or additions are needed to strengthen their paper. Therefore, this comment is 1.","The comment addresses the motivation of the paper, questioning its alignment with existing research on the importance of diversity in instructiontuning data. It also critiques the experiments, noting their limitations in dataset and LLM size, which affects the persuasiveness of the results. However, the comment does not specify which sections of the paper discuss the motivation or experiments, making it difficult for the authors to pinpoint the exact parts that need revision. While the authors can infer that the motivation and experimental sections are the focus, the lack of explicit references to these sections limits the comment""s grounding. The comment is specific in detailing the issues with the motivation and experiments, but without clear grounding, it aligns with a score of 3.","The review point claims that the paper""s motivation is unclear and that it opposes a widely recognized understanding without strong justification. It also criticizes the experiments as weak due to limited dataset and LLM size, questioning the persuasiveness and potential misleading nature of the results. The comment provides some logical reasoning by referencing the importance of diversity in instructiontuning data and the limitations of the experiments. However, it lacks specific examples or references to support the claim that the results could mislead the community. This makes the claim 3, as it provides a general critique but requires more detailed evidence to fully substantiate the concerns. Therefore, the comment is rated as 3.","The review comment identifies a significant issue with the paper""s motivation, suggesting that it contradicts a widely recognized understanding without providing strong justification. It also critiques the experiments, noting their limitations in dataset and LLM size, which affects the persuasiveness of the results. The comment highlights a potential concern that the results could mislead the community if not thoroughly validated. However, it lacks specific suggestions or guidance on how the authors might address these concerns or improve their draft. While it points out important areas for improvement, the feedback is incomplete and does not provide actionable steps for the authors to take. Therefore, the comment is 3, aligning with a score of 3."
NIPS_2020_863,NIPS_2020,"- The authors claim that the proposed structure-aware graph convolution filter is helpful. However, no ablation study is done to verify the design. What is the result if we just use a simple graph convolution layer? - The effect of including the “maximum convolution result” in Equation 5 is unclear. Ablation study is needed to support such a design. - The major evaluation is done on 10 objects illuminated under a fixed lighting distribution from the DiLiGenT benchmark, which is rather limited. The authors should evaluate their method on a wide diversity of synthetic data (e.g., different shapes rendered using different BRDFs under different (biased) lighting distributions) to justify the proposed method. - It is unclear what is the input image number during training of the proposed method. In Table 1, does the training number equals to the testing number? It would be helpful to analyze the effect of training image number as well. - It would be desirable to include another baseline network using only 1x1 convolutions in Table 3 for comparison. To make a fair comparison, different networks with a similar number of parameters should be compared. - It would be desirable to include runtime comparison of different deep learning based methods.",- It would be desirable to include runtime comparison of different deep learning based methods.,7377,2,"The review point suggests that including a runtime comparison of different deep learning-based methods would be beneficial. However, it does not specify which methods should be compared or how the comparison should be conducted. The action is implicit and lacks concrete details, leaving the authors uncertain about how to implement the suggestion effectively. Therefore, the comment is barely actionable.",3,"The review comment suggests including a runtime comparison of different deep learning-based methods. This feedback identifies a specific area where the manuscript could be improved, as runtime comparisons can provide valuable insights into the efficiency and practicality of the proposed method relative to others. However, the comment lacks depth and does not provide detailed guidance or suggestions on how to effectively present or analyze this comparison. To be more helpful, the comment could include specific aspects of the runtime comparison that need attention or suggest methodologies for conducting the analysis. Therefore, the comment is rated as somewhat helpful, consistent with label 3.",3,"The comment suggests including a runtime comparison of different deep learning-based methods, which is a specific suggestion for improvement. However, it does not explicitly mention which part of the paper this suggestion pertains to, such as the results section or a specific table or figure. The authors can infer that it relates to the evaluation or results section, but this inference is not directly guided by the comment. Therefore, the comment is weakly grounded because it does not specify the exact part of the paper being addressed, but it is specific in suggesting a potential improvement. This aligns with a score of 3.",1,"The review point suggests including a runtime comparison of different deep learning-based methods. However, it does not provide any justification or reasoning for why this comparison is necessary or how it would benefit the paper. Without supporting evidence or explanation, the claim lacks verifiability, making it difficult for the authors to understand the importance of this suggestion. Therefore, the comment is rated as 1: Unverifiable.",1906,- it would be desirable to include runtime comparison of different deep learning based methods.,3,3,1,2,"The review point suggests that it would be desirable to include a runtime comparison of different deep learningbased methods. While the comment implies that the authors should conduct this comparison, it does not provide explicit instructions or guidance on how to perform the comparison or what specific aspects should be considered. The action is implicit and somewhat vague, as the authors can infer that they need to include a runtime comparison but may not know exactly how to execute it. Therefore, this comment is 3.","The comment suggests including a runtime comparison of different deep learningbased methods but does not specify which part of the paper this suggestion pertains to. The authors may infer that it relates to the experimental or results sections, but without explicit guidance, it is difficult to pinpoint the exact area needing attention. The comment is specific in suggesting a particular type of comparison but lacks grounding as it does not reference a specific section or part of the paper. Therefore, this comment is 3, aligning with a score of 3.","The review point suggests that including a runtime comparison of different deep learningbased methods would be desirable. However, it does not provide any reasoning, evidence, or examples to support why this comparison is necessary or beneficial. Without additional context or justification, the claim lacks verifiability, making it difficult for the authors to understand the rationale behind the suggestion. Therefore, the comment is classified as ""1.""","The review comment suggests that including a runtime comparison of different deep learningbased methods would be desirable. While this feedback identifies a potential area for improvement, it lacks specificity and does not provide guidance on how to conduct the comparison or what aspects should be considered. The comment does not offer actionable steps or detailed suggestions, leaving the authors with a general idea of what could be improved but without a clear path forward. Therefore, the comment is 2, aligning with a score of 2."
NIPS_2020_253,NIPS_2020,"The main weaknesses of this paper are perhaps in missed opportunities to make it even more insightful. - The fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way). - This could perhaps be demonstrated by seeing if those detectors also suffered the same degradation in detection accuracy under compression. It would be even more interesting to see if the accuracy of these prior methods suffered after the proposed 'spectrum synthesis' step. - It would also be good to have more discussion, and preferably also analysis, of ""why"" these generative models produce images with different spectra. Is it something in the small-kernel convolution-transpose decoder architecture that have become so common ? Or is it because of where and how the random source (i.e., noise) is injected into these architectures ? One can imagine experiments to investigate this further: for example, how does fake detection accuracy change when one trains a GAN with spatially larger kernels; and how does it change if one introduces noise in later vs earlier layers.",- The fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way).,7380,1,"The review comment raises an interesting point about the potential underlying mechanisms of 'fake image detectors,' suggesting that they might be using the same spectral cues as the simple classifier mentioned. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how this observation should influence the authors' work, nor are there suggestions for improvement or further exploration. As a result, the comment lacks actionability, leaving the authors without direction on how to apply this information to enhance their draft. Therefore, it is rated as a 1 on the actionability scale.",3,"The comment raises an interesting point about the potential underlying mechanisms of 'fake image detectors,' suggesting that they might be using the same spectral cues as the simple classifier discussed. However, it does not provide specific guidance or suggestions on how the authors could explore or address this hypothesis in their work. The comment lacks actionable feedback that would help the authors improve their draft, such as proposing experiments or analyses to investigate this claim. As a result, the feedback is somewhat helpful, as it prompts the authors to consider a broader context but does not offer concrete steps for improvement. Therefore, it aligns with a score of 3.",1,"The comment raises an interesting point about the potential underlying mechanisms of 'fake image detectors,' suggesting that they might be using similar spectral cues as the simple classifier mentioned. However, it does not specify which part of the paper this observation is based on, nor does it provide detailed guidance on how the authors should address this point. Without specific references or detailed feedback, the authors cannot effectively identify the areas needing improvement or further exploration. As a result, the comment lacks both grounding and specificity, making it not grounded at all.",1,"The review point raises a question about the underlying mechanisms of 'fake image detectors,' suggesting that they might be using the same spectral cues as the simple classifier mentioned. However, the comment does not provide any evidence, references, or detailed reasoning to support this claim. It lacks specific examples or comparisons that would help verify the assertion. As a result, the claim is not verifiable due to the absence of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.",3025,- the fact that such a simple classifier works begs the question whether other 'fake image detectors' are fundamentally using the same spectral cues (just in a more opaque way).,1,1,1,1,"The review point raises a question about the simplicity of the classifier and whether other ""fake image detectors"" might be using similar spectral cues in a more opaque manner. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this question or what steps to consider in their analysis. As a result, the comment lacks actionability, leaving the authors without a clear path forward. Therefore, this comment is rated as 1.","The comment raises a question about the simplicity of the classifier and whether other ""fake image detectors"" might be using similar spectral cues in a more opaque way. However, it does not specify which part of the paper this question pertains to, nor does it provide details on what needs to be addressed or improved. The authors cannot confidently determine which section of the paper this comment relates to, and it lacks specificity regarding what aspects need clarification or further discussion. Therefore, the comment is 1 and not specific, aligning with a score of 1.","The review point raises a question about the simplicity of the classifier and whether other ""fake image detectors"" might be using similar spectral cues in a more opaque way. However, it does not provide any supporting evidence, reasoning, or references to substantiate this claim. The comment lacks specific examples or detailed explanations that would help the authors understand the basis of the question or how it relates to their work. As a result, the claim is 1, as it does not provide sufficient information for the authors to address or improve their work. Therefore, the comment is rated as 1.","The review comment raises a question about the simplicity of the classifier and whether other ""fake image detectors"" might be using similar spectral cues in a more opaque way. While it prompts the authors to consider the broader implications of their findings, it does not provide specific guidance or suggestions on how to address this question or improve the draft. The comment lacks actionable feedback, leaving the authors without a clear path to enhance their work. Therefore, it is rated as 1."
NIPS_2018_112,NIPS_2018,"I have some questions on the experiments. - Missing baseline: Performances when removing L_id, L_pd, L_r and performances when only removing one of the not share E, L_sp, L_v are not evaluated. - How the experimental comparison with DR-GAN was done is not clear enough since DR-GAN does not use the pose map for input data. - The proposed method does not require the pose estimation in the inference, so test image does not have pose maps. It is not clear if the visual analysis on Sec.4.4 is the result on training images or test images. - Why the generated images of Figure 4 (a) shows only one of the [18],[19] for each input image? - Why the [18][19] fails and the proposed method succeed? Several parts of this paper are hard to understand. -The names âIdentity discriminatorâ and âpose discriminatorâ are confusing. When reading Fig.2, it looks like these losses are to distinguish different identities and poses. However, Eq.(2) seems to be the loss to distinguish real vs. fake images (of the same person), not to distinguish between different identities. Similarly, the âpose discriminatorâ is not to distinguish between different poses. - Line29: What is LOSO regularization? - I suggest replacing some arXiv papers with actual published works. Response to Rebuttal: The rebuttal addressed my concerns about the experiments. However, the additional ablation study was only on Market-1501 dataset, and DukeMTMC-reID dataset should be included in the final version. Also, the explanation of identity/pose discriminators should be carefully revised according to the response.","- The proposed method does not require the pose estimation in the inference, so test image does not have pose maps. It is not clear if the visual analysis on Sec.4.4 is the result on training images or test images.",6373,4,"The comment points out a lack of clarity regarding whether the visual analysis in Section 4.4 pertains to training images or test images, given that the proposed method does not require pose estimation during inference. While the comment highlights an area that needs clarification, it does not explicitly instruct the authors to clarify this point in their paper. However, the action is implied and concrete, as the authors can deduce that they need to specify the source of the images used in the visual analysis. Therefore, this comment is mostly actionable.",3,"The review comment identifies a lack of clarity in the paper regarding whether the visual analysis in Section 4.4 pertains to training or test images, given that the proposed method does not require pose estimation during inference. This feedback highlights a specific area where the manuscript could be improved, prompting the authors to clarify this aspect. However, the comment does not provide suggestions on how to address this issue or why this distinction is important, which limits its helpfulness. While it points out a necessary clarification, it lacks depth and actionable guidance, making it somewhat helpful for the authors to consider revising their explanation. Therefore, the comment aligns with a score of 3.",5,"The comment explicitly mentions ""Sec.4.4,"" providing full grounding as it allows the authors to accurately identify the section being addressed. It also specifies the issue by questioning whether the visual analysis pertains to training or test images, offering a clear direction for the authors to clarify this aspect. This level of detail makes the comment fully grounded and specific, aligning with a score of 5.",X,"The comment points out a lack of clarity regarding whether the visual analysis in Section 4.4 pertains to training or test images. It does not express an opinion, judgment, or suggestion that requires verification. It is a request for clarification, making ""No"" the appropriate label.",3424,"- the proposed method does not require the pose estimation in the inference, so test image does not have pose maps. it is not clear if the visual analysis on sec.4.4 is the result on training images or test images.",1,5,1,3,"The review point raises a question about the clarity of the visual analysis in Section 4.4, specifically whether it is based on training or test images. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to clarify this point or what specific information should be added to the paper. As a result, the authors are left without a clear understanding of what steps to take to address the issue. Therefore, this comment is 1.","The comment raises a question about the clarity of the visual analysis in Section 4.4, specifically whether it is based on training or test images. It explicitly mentions ""Sec.4.4,"" which allows the authors to accurately identify the part of the paper being addressed, providing full grounding. The comment is also specific as it clearly specifies the issue of unclear visual analysis and the need for clarification on whether it pertains to training or test images. Therefore, this comment is 5, aligning with category 5.","The review point raises a question about the clarity of the visual analysis in Section 4.4, specifically whether it is based on training or test images. However, it does not provide any supporting evidence, reasoning, or references to justify the claim. The comment lacks specific details or examples that would help the authors understand the issue or how to address it. As a result, the claim is 1, as it does not provide sufficient information for the authors to make improvements. Therefore, the comment is rated as 1.","The review comment raises a question about the clarity of the visual analysis in Section 4.4, specifically whether it is based on training or test images. This is a relevant observation that could help the authors clarify their methodology and results. However, the comment does not provide any specific suggestions or guidance on how the authors might address this issue or improve the clarity of their analysis. While it identifies a potential area for improvement, it lacks actionable feedback, making it 3. Therefore, the comment aligns with a score of 3."
D97Zfgv4em,EMNLP_2023,"1. Model extraction is a very relevant topic in the age of ChatGPT, where several approaches have been introduced to construct instruction tuning datasets automatically [1, 2, 3, 4], in an attempt to ""extract"" closed-source ChatGPT. This paper would be much more timely and exciting if the methods introduced in this paper are applied to the instrucion tuning setting, and the sample efficiency is compared to techniques like self-instruct [1]. The clustering and filtering approach introduced in this paper could be an interesting way to create instruction diversity, and could potentially lead to better instruction tuned models. However, the paper currently focuses on BERT fine-tuning settings which was the paradigm in 2019, and not used much anymore in state-of-the-art language modeling.2. In Table 1 (appendix version), the victim models perform much better than the extracted models, and I am guessing this is because of how strict the query budget for the extracted models are. Is such a strict query budget (<1K queries) necessary? API access is quite cheap these days --- for instance, 10K queries to ChatGPT (each with 500 tokens), would cost just 10$ according to the OpenAI API [5]. The paper should justify the strict query budgets by presenting cost estimates from different APIs. This is critical since the main premise of the paper is that it is a ""query-efficient"" method.3. I have some concerns about the sequence inference classifier that is used to filter data. It's unclear to me what kind of data passes this filter --- the task of filtering data using the task description seems quite different from the original MNLI distribution. Some examples of the filtered data would be helpful to look at.4. Minor: A discussion on possible defenses against model extraction would be useful.[1] - https://arxiv.org/abs/2212.10560[2] - https://gpt4all.io/reports/GPT4All_Technical_Report_3.pdf[3] - https://crfm.stanford.edu/2023/03/13/alpaca.html[4] - https://arxiv.org/abs/2305.15717[5] - https://openai.com/pricing",3. I have some concerns about the sequence inference classifier that is used to filter data. It's unclear to me what kind of data passes this filter --- the task of filtering data using the task description seems quite different from the original MNLI distribution. Some examples of the filtered data would be helpful to look at.,1735,4,"The review comment expresses concerns about the sequence inference classifier used to filter data and mentions that it is unclear what kind of data passes this filter. The reviewer suggests that providing examples of the filtered data would be helpful. While the comment implies that examples should be included, it does not explicitly instruct the authors to do so. However, the suggestion to include examples is a concrete action that the authors can take to address the concern. The action is somewhat implicit but clear enough for the authors to understand what needs to be done. Therefore, the comment is mostly actionable.",3,"The review comment raises concerns about the sequence inference classifier used to filter data, specifically questioning the type of data that passes through this filter. It highlights a potential discrepancy between the filtering task and the original MNLI distribution. The suggestion to provide examples of the filtered data is a clear and actionable recommendation that could help clarify the filtering process and its outcomes. While the comment identifies a significant area for improvement and offers a specific suggestion, it could be more helpful by providing additional guidance or context on why this issue is important. Overall, the feedback is somewhat helpful as it directs the authors' attention to a specific aspect of their work that needs clarification, aligning with a score of 3.",3,"The comment raises concerns about the sequence inference classifier used for filtering data, indicating that the task of filtering seems different from the original MNLI distribution. It suggests providing examples of the filtered data to clarify what kind of data passes the filter. While the comment does not explicitly mention a specific section of the paper, the authors can infer that it relates to the methodology or results section discussing data filtering. The comment is specific in detailing what is unclear and what could be added to improve understanding, namely examples of the filtered data. Therefore, the comment is weakly grounded but specific, aligning with the label 3.",3,"The review point raises concerns about the sequence inference classifier used for data filtering, suggesting that the task of filtering data using the task description differs from the original MNLI distribution. However, the comment lacks specific examples or detailed reasoning to support the claim. The suggestion to include examples of filtered data is helpful, but without further elaboration or evidence, the claim remains somewhat vague. This makes the comment somewhat verifiable, as the authors would need to further develop the rationale themselves to address the concern effectively.",7995,3. i have some concerns about the sequence inference classifier that is used to filter data. it's unclear to me what kind of data passes this filter --- the task of filtering data using the task description seems quite different from the original mnli distribution. some examples of the filtered data would be helpful to look at.,4,3,3,3,"The review comment expresses concerns about the sequence inference classifier used for filtering data and questions the type of data that passes this filter. It suggests that the task of filtering data using the task description is different from the original MNLI distribution and recommends providing examples of the filtered data. While the comment implies that the authors should provide examples of filtered data, it does not explicitly instruct them to do so. The action is concrete in terms of what is needed, but it is implicit in the request for examples. Therefore, the comment is 4.","The comment addresses concerns about the sequence inference classifier used for filtering data, specifically questioning what kind of data passes this filter and how it differs from the original MNLI distribution. It suggests that providing examples of the filtered data would be helpful. While the comment does not explicitly mention a specific section or part of the paper, the authors can infer that it relates to the methodology or results sections where data filtering is discussed. The comment is specific in detailing what needs to be addressed, namely the need for examples of filtered data. Therefore, the comment is weakly grounded but specific, aligning with category 3.","The review point expresses concerns about the sequence inference classifier used for filtering data, questioning the type of data that passes this filter and suggesting that it differs from the original MNLI distribution. The comment requests examples of the filtered data to better understand the issue. While the comment highlights a potential concern, it lacks specific examples or detailed reasoning to fully substantiate the claim. The authors would need to infer the exact nature of the issue and how it affects the paper. Therefore, the comment is 3, as it provides a basis for further investigation but requires more detailed justification.","The review comment raises a concern about the sequence inference classifier used for filtering data, questioning the type of data that passes this filter and how it differs from the original MNLI distribution. It suggests that providing examples of the filtered data would be helpful to better understand the issue. This feedback is 3 as it identifies a potential area of confusion and provides a clear suggestion for improvement. However, it could be more helpful if it offered specific guidance on how to address the issue or provided examples of what kind of data might be filtered. Overall, the comment provides a useful direction for the authors to clarify their methodology, but it could be more comprehensive with additional details or examples."
ICLR_2021_2759,ICLR_2021,""" section. StrengthsExcellent writing and well-structured paper.Intuitive and novel method that is really interesting to read about with good intuition and analytical examples to motivate the method.Experiments are carried out well and the results are presented in a coherent manner. Each experiment is repeated multiple times. WeaknessesAs mentioned before, I would suggest to tone down the lottery ticket comparisons. I would assume that PHEW does not perform quite as well as the classic lottery tickets derived from IMP, which would not be surprising since IMP requires numerous training iterations. But in any case, I would like to see actual comparisons in terms of the resulting final test accuracy as main comparison tool instead of the comparison metric (Jaccard index, IoU) presented in Section 6.2.The method seems to be limited to convolutional and fully-connected layers. Could the authors discuss potential generalization to a larger class of network layers? Since unstructured pruning methods (like PHEW) usually do not enable faster training or inference times, I find their main advantage to be that they are network-agnostic. This can thus enable the easy deployment of unstructured pruning to any desired layer or architecture.As a follow-up to the previous question. How are non-sequential network architectures handled? The experiments include ResNets, which contain residual connections, but to me it is not clear from the paper how this is achieved with PHEW. Also, what about recurrent architectures? Can PHEW handle those? As far as I am aware, the competing methods should not have a problem handling those architectures as well.How about (at least a few) experiments with the full ImageNet data set? It shouldn't be too computationally expensive to train a ResNet on the full ImageNet data set especially since pruning at initialization does not require multiple retrain cycles like IMP.Other Minor Feedbacktypo, page 4, below Fig. 4: ""...even more better...""I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs.Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work.","4: ""...even more better..."" I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs. Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work.",2184,4,"The review comment addresses two issues. First, it points out a grammatical error in the phrase ""...even more better...,"" which is explicit and actionable as it clearly indicates a correction that needs to be made. Second, it questions the claim that Figure 4(b) demonstrates faster training due to the proposed method, suggesting that this claim might be overstated. The reviewer implies that the authors should reconsider or clarify this claim, but does not provide specific guidance on how to do so. Lastly, the reviewer suggests that the paper would benefit from a longer discussion of related work, which is an explicit suggestion but lacks detail on what specific related work should be included. Overall, the comment contains explicit actions but lacks concrete details for some of them, making it mostly actionable.",4,"The review comment addresses multiple aspects of the paper. It begins by pointing out a grammatical error, which is a minor but helpful correction. The comment then questions the interpretation of Figure 4(b), suggesting that the claim of faster training may be overstated, especially since all networks are trained for the same number of epochs. This critique encourages the authors to reassess their conclusions and possibly provide additional evidence or clarification. Lastly, the reviewer suggests that the paper would benefit from a more extensive discussion of related work in the field of pruning, which is a constructive recommendation for broadening the paper's context and depth. While the comment provides valuable insights and actionable suggestions, it could be more helpful if it included specific examples or references for the related work discussion. Overall, the feedback is mostly helpful, offering clear directions for improvement but lacking some depth in its suggestions. Therefore, it aligns with a score of 4.",4,"The comment refers to Figure 4(b) and questions the claim that the proposed method results in faster training. This makes the comment fully grounded, as it explicitly mentions a specific figure in the paper. However, the comment lacks specificity regarding why the figure does not convincingly support the claim, and it does not provide detailed guidance on how to improve the discussion of related work. Therefore, the comment is fully grounded but under-specific, aligning with category 4.",4,"The review point questions the claim that Figure 4(b) demonstrates faster training for the proposed method. The reviewer argues that this claim is unconvincing because all networks are trained for the same number of epochs, which is a logical reasoning that challenges the validity of the claim. Additionally, the reviewer suggests that the paper could benefit from a more extensive discussion of related work in the field of pruning, which implies that the current discussion may be lacking in depth or context. However, the comment does not provide specific examples or references to support the need for a longer discussion, which leaves some gaps in the justification. Therefore, the claim is mostly verifiable, aligning with a score of 4: Mostly Verifiable.",9298,"4: ""...even more better..."" i am not convinced that figure 4(b) indicates that the proposed method results in faster training. it seems a bit far-fetched especially since all networks are trained for the same amount of epochs. pruning is quite a big field. i believe the paper would benefit from a longer discussion of related work.",3,5,3,3,"The review point raises concerns about the interpretation of Figure 4(b) and suggests that the proposed method does not necessarily result in faster training, given that all networks are trained for the same number of epochs. It also implies that the paper could benefit from a more indepth discussion of related work in the field of pruning. While the comment identifies areas for improvement, it does not provide explicit instructions or concrete suggestions on how to address these issues. The authors are left to infer that they should clarify the interpretation of Figure 4(b) and expand the discussion of related work. However, the lack of specific guidance makes the action somewhat vague. Therefore, this comment is 3.","The comment is fully grounded as it explicitly mentions ""Figure 4(b),"" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it questions the interpretation of the figure, specifically regarding the claim that the proposed method results in faster training, and suggests that the paper would benefit from a longer discussion of related work in the field of pruning. This provides clear guidance on what needs to be addressed in the paper. Therefore, this comment is 5, aligning with category 5.","The review point questions the claim that Figure 4(b) indicates the proposed method results in faster training, given that all networks are trained for the same number of epochs. The reviewer suggests that this interpretation is ""a bit farfetched"" and implies that the paper lacks a thorough discussion of related work in the field of pruning. While the comment raises a valid concern about the interpretation of the figure, it does not provide specific examples or references to support the claim that the paper lacks a comprehensive discussion of related work. The reasoning is 3, as it highlights a potential issue but lacks detailed evidence or examples to fully substantiate the claim. Therefore, the comment is rated as 3.","The review comment identifies a potential issue with the interpretation of Figure 4(b), questioning whether it accurately indicates that the proposed method results in faster training. It also suggests that the paper could benefit from a longer discussion of related work in the field of pruning. While the comment highlights a specific area for improvement, it lacks detailed guidance on how to address the issue or expand the discussion of related work. The feedback is 3 as it points out a potential weakness and a direction for improvement, but it could be more actionable with specific suggestions or examples. Therefore, the comment is rated as 3."
r5d8zkYizS,ICLR_2025,"1. The significance of the theoretical findings is unclear. The authors should have demonstrated more potential application of the theoretical findings, e.g., how to apply the findings to design a new defense.2. The theoretical finding is a limit behavior that only holds for extremely small perturbation sizes. The authors’ argument that such an assumption on small perturbation size is needed is not sufficient because we don’t even have a proper definition of how small is “small enough”. As the authors describe in Section 7, a formal convergence rate analysis is required.3. The experiments should be improved further.* The number of adversarial examples is too small to demonstrate some distribution.* The experiment setup is quite arbitrary. Different architectures and attacks are used for different datasets.* The choice of attack algorithms (DeepFool and FAB) also seems uncommon. It is better to use simpler and common attack algorithms.",* The number of adversarial examples is too small to demonstrate some distribution.,5500,1,"The review point notes that the number of adversarial examples is too small to demonstrate some distribution, but it does not provide any explicit or implicit actions for the authors to take. There is no guidance on whether the authors should increase the number of examples, provide additional analysis, or consider other approaches to address this issue. Without any actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.",1,"The comment identifies a potential issue with the paper, specifically that the number of adversarial examples is too small to demonstrate some distribution. However, it lacks any further explanation or suggestions on how the authors might address this issue. Without additional context or guidance, the authors are left without actionable steps to understand or resolve the problem. The comment does not provide enough information to be helpful in improving the draft, making it unhelpful. Therefore, it aligns with a score of 1, indicating that the comment is not helpful at all.",1,"The comment suggests that the number of adversarial examples is insufficient to demonstrate a distribution. However, it does not specify which part of the paper this critique pertains to, such as a particular section, table, or figure. Additionally, it lacks specificity regarding what distribution is being referred to or how the number of examples should be increased or analyzed differently. This makes it difficult for the authors to identify the exact part of the paper that needs attention or to understand the specific issue being raised. Therefore, the comment is not grounded and highly unspecific, aligning with a score of 1.",1,"The review point claims that ""the number of adversarial examples is too small to demonstrate some distribution."" However, it does not provide any supporting evidence, reasoning, or examples to justify why the number is considered too small or what distribution is being referred to. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.",438,* the number of adversarial examples is too small to demonstrate some distribution.,1,1,1,2,"The review point indicates that the number of adversarial examples is insufficient to demonstrate a distribution, but it does not provide any explicit or implicit guidance on how to address this issue. There is no suggestion on how to increase the number of examples, what specific distribution should be demonstrated, or how to improve the demonstration. Without any actionable advice or concrete steps, the authors are left without a clear path to improve their draft. Therefore, this comment is 1.","The comment mentions that the ""number of adversarial examples is too small to demonstrate some distribution,"" but it does not specify which part of the paper this issue pertains to, such as a particular section or figure. This lack of explicit reference makes it difficult for the authors to identify the exact area needing attention. Additionally, the comment does not provide specific details on what distribution should be demonstrated or how the authors could improve the demonstration. Therefore, the comment is 1 and lacks specificity, aligning with a score of 1.","The review point claims that the number of adversarial examples is too small to demonstrate a distribution. However, it does not provide any supporting evidence, reasoning, or references to substantiate this claim. Without additional context or explanation, the authors may find it challenging to understand the basis of the critique or how to address it. Therefore, the comment is considered 1.","The review comment points out a potential issue with the number of adversarial examples being too small to demonstrate a distribution. However, it lacks specificity and does not provide any guidance or suggestions on how the authors might address this issue. Without actionable advice or examples, the comment does not offer much value to the authors in terms of improving their draft. Therefore, it is rated as 2, as it identifies a potential weakness but does not provide sufficient guidance for improvement."
aM7US5jKCd,ICLR_2024,"- My major concern is the limited novelty, as the explored loss functions are not new. Although JS divergence, masked CE loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions- The conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation- The paper only considers untargeted attacks, and it would be useful to extend the analysis to targeted attacks to showcase the strength of the proposed attack method.- The authors could conduct experiments to evaluate the transferability of the proposed attack to other models and compare it against the baseline PGD/CosPGD/SegPGD attacks.","- My major concern is the limited novelty, as the explored loss functions are not new. Although JS divergence, masked CE loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions - The conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation - The paper only considers untargeted attacks, and it would be useful to extend the analysis to targeted attacks to showcase the strength of the proposed attack method.",4513,3,"The review point raises several concerns about the paper, including limited novelty due to the use of existing loss functions, the lack of black-box evaluation, and the focus on untargeted attacks. While the comment identifies areas for improvement, it does not provide explicit guidance on how to address these issues. The suggestion to extend the analysis to targeted attacks is somewhat actionable, but the comment lacks concrete details on how to implement this extension or how to address the novelty and evaluation concerns. The feedback is implicit and vague, leaving the authors with a general direction but without specific steps to follow. Therefore, this comment is somewhat actionable.",4,"The review comment addresses several critical aspects of the paper, providing a comprehensive evaluation of its limitations. It highlights concerns about the novelty of the work, noting that the loss functions used are not new and suggesting that the approach appears to be a straightforward application of existing methods. The comment also points out the lack of black-box evaluation, which is a significant limitation, and suggests extending the analysis to include targeted attacks to better demonstrate the proposed method's effectiveness. While the comment identifies key weaknesses and offers some direction for improvement, it could be more helpful by providing specific suggestions or examples on how to address these issues. Overall, the feedback is mostly helpful as it covers multiple areas for improvement and provides a clear rationale for each point, aligning with a score of 4.",3,"The comment raises several concerns about the paper, including the limited novelty of the explored loss functions, the lack of black-box evaluation, and the focus on untargeted attacks. However, it does not specify which sections or parts of the paper these concerns relate to, making it difficult for the authors to pinpoint where revisions are needed. While the comment is specific in terms of the issues it raises, it lacks grounding because it does not reference specific sections, figures, or tables in the paper. Therefore, this comment is weakly grounded but specific, aligning with a score of 3.",3,"The review point raises several claims regarding the paper's limitations, including limited novelty due to the use of existing loss functions, the lack of black-box evaluation, and the focus on untargeted attacks. The claim about limited novelty is somewhat supported by the mention of specific loss functions (JS divergence, masked CE loss, and masked spherical loss) that are not new but are noted as uncommon in the specific context of segmentation attacks. However, the comment does not provide references or detailed reasoning to substantiate why these functions are considered a simple 'plug and play.' The critique about the absence of black-box evaluation and the suggestion to extend the analysis to targeted attacks are logical but lack specific examples or references to support the claims. Overall, the comment provides some support for its claims but lacks detailed justification, making it somewhat verifiable.",2194,"- my major concern is the limited novelty, as the explored loss functions are not new. although js divergence, masked ce loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions - the conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation - the paper only considers untargeted attacks, and it would be useful to extend the analysis to targeted attacks to showcase the strength of the proposed attack method.",2,3,3,3,"The review point raises several concerns about the novelty of the loss functions explored, the absence of blackbox evaluation, and the limitation to untargeted attacks. While it identifies areas for improvement, it does not provide explicit or concrete actions for the authors to take. The comment suggests that the loss functions are not new and that the attacks are limited to whitebox scenarios, but it does not offer specific guidance on how to address these issues or improve the novelty of the work. The suggestion to extend the analysis to targeted attacks is vague and lacks detail on how to implement this. Therefore, the comment is barely actionable, as it highlights areas for improvement but does not provide clear steps for the authors to follow.","The comment raises concerns about the novelty of the loss functions explored, the absence of blackbox evaluation, and the limitation to untargeted attacks. While it does not explicitly mention specific sections of the paper, the authors can infer that these issues relate to the methodology and results sections. The comment is specific in detailing what is lacking, such as the absence of blackbox evaluation and the need to extend the analysis to targeted attacks. However, it does not provide explicit references to specific parts of the paper, making it weakly grounded. Therefore, this comment is 3, aligning with a score of 3.","The review point raises several claims about the novelty of the loss functions, the absence of blackbox evaluation, and the limitation to untargeted attacks. The claim about the loss functions being ""not new"" is subjective and lacks specific evidence or references to support this assertion. The comment also suggests that the attacks are limited to whitebox scenarios and that extending the analysis to targeted attacks would be beneficial. While the reviewer provides some logical reasoning, the lack of specific examples or references makes the claims 3. The authors would need to consider these points and potentially conduct additional research to fully address the concerns. Therefore, the comment is rated as 3.","The review comment identifies several areas of concern regarding the novelty of the loss functions, the absence of blackbox evaluation, and the limitation to untargeted attacks. It provides a clear critique of the paper""s methodology and suggests that the loss functions are not new, which could limit the novelty of the work. The comment also highlights the importance of evaluating blackbox attacks and suggests extending the analysis to targeted attacks to showcase the strength of the proposed method. While the feedback is clear and identifies specific weaknesses, it could be more helpful by providing suggestions on how to address these issues or offering examples of how to incorporate blackbox evaluation or targeted attacks. Overall, the comment is 3 as it directs the authors"" attention to important areas for improvement but lacks detailed guidance on implementation."

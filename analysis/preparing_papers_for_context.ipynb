{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "* Choose 100 random entries from the gold+silver data\n",
    "* get the raw papers for them\n",
    "* extract the papers using the library\n",
    "* store them in a new dataframe\n",
    "* adapt the inf prompt to take in the paper context\n",
    "* adapt the chatgpt fn to take an optional parm for the context\n",
    "* generate labels for this data\n",
    "* do evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "aspects = [ 'actionability', 'grounding_specificity','verifiability', 'helpfulness']\n",
    "random_state = 77\n",
    "ds = datasets.load_dataset(\"boda/review_evaluation_human_annotation\", \"combined_main_aspects\", split=\"full\")\n",
    "\n",
    "ds = ds.map(lambda x: {k: str(v) for k, v in x.items()})\n",
    "ds = ds.to_pandas()\n",
    "\n",
    "\n",
    "############# take the samples where we have majority labels for all aspects\n",
    "filtered_rows = ds[\n",
    "    (ds['actionability_label_type'].isin(['gold', 'silver'])) &\n",
    "    (ds['grounding_specificity_label_type'].isin(['gold', 'silver'])) &\n",
    "    (ds['verifiability_label_type'].isin(['gold', 'silver'])) &\n",
    "    (ds['helpfulness_label_type'].isin(['gold', 'silver']))\n",
    "]\n",
    "\n",
    "\n",
    "# List of OpenReview-supported venues\n",
    "openreview_venues = ['ICLR_2024', 'ICLR_2025', 'EMNLP_2023']\n",
    "############# exclude rows with OpenReview-supported venues\n",
    "filtered_rows = filtered_rows[~filtered_rows['venue'].isin(openreview_venues)]\n",
    "\n",
    "############### take 100 random samples\n",
    "sampled_ds = filtered_rows.sample(n=100, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['3', '3', '5']}\n"
     ]
    }
   ],
   "source": [
    "print(ds['actionability'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "venue\n",
       "NIPS_2018    12\n",
       "NIPS_2019    12\n",
       "NIPS_2020    11\n",
       "ICLR_2022    11\n",
       "ARR_2022     10\n",
       "NIPS_2022     8\n",
       "NIPS_2021     8\n",
       "NIPS_2017     7\n",
       "ICLR_2023     7\n",
       "NIPS_2016     6\n",
       "ACL_2017      4\n",
       "ICLR_2021     4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sampled_ds['venue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# PDF download function\n",
    "def download_openreview_pdf(pdf_url: str, output_path: str):\n",
    "\n",
    "    # pdf_url = f\"https://openreview.net{pdf_url}\"\n",
    "    pdf_url = f\"https://openreview.net/pdf?id={pdf_url}\"\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"[+] Downloaded paper to {output_path}\")\n",
    "    else:\n",
    "        print(f\"[!] Failed to download PDF from {pdf_url}. Status code: {response.status_code}\")\n",
    "\n",
    "# Paths\n",
    "base_path = \"/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/raw_papers\"\n",
    "json_paths = {\n",
    "    \"EMNLP_2023\": os.path.join(base_path, \"emnlp2023_papers.json\"),\n",
    "    \"ICLR_2024\": os.path.join(base_path, \"iclr2024_papers.json\"),\n",
    "    \"ICLR_2025\": os.path.join(base_path, \"iclr2025_papers.json\"),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Load all JSON files into a dictionary\n",
    "paper_data = {}\n",
    "for venue, path in json_paths.items():\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        paper_data[venue] = {entry['id']: entry for entry in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################################################### Download Reviewer2 Data ####################################################\n",
    "\n",
    "# import os\n",
    "# import zipfile\n",
    "# from huggingface_hub import list_repo_files, hf_hub_download\n",
    "\n",
    "# # Configuration\n",
    "# repo_id = \"GitBag/Reviewer2_PGE_raw\"\n",
    "# target_dir = \"/l/users/abdelrahman.sadallah/reviewer2_raw_data\"\n",
    "# os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# # Step 1: List all .zip files in the dataset repo\n",
    "# zip_files = [f for f in list_repo_files(repo_id=repo_id, repo_type=\"dataset\") if f.endswith(\".zip\")]\n",
    "# print(f\"[INFO] Found {len(zip_files)} zip files.\")\n",
    "\n",
    "# # Step 2 & 3: Download and unzip each .zip file\n",
    "# for zip_file in zip_files:\n",
    "#     print(f\"[INFO] Downloading {zip_file}...\")\n",
    "    \n",
    "#     # Download ZIP\n",
    "#     local_zip_path = hf_hub_download(\n",
    "#         repo_id=repo_id,\n",
    "#         filename=zip_file,\n",
    "#         repo_type=\"dataset\",\n",
    "#         local_dir=target_dir,\n",
    "#         local_dir_use_symlinks=False  # make a real copy\n",
    "#     )\n",
    "\n",
    "#     print(f\"[INFO] Downloaded to {local_zip_path}\")\n",
    "\n",
    "#     # Step 4: Unzip\n",
    "#     with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "#         extract_path = os.path.join(target_dir, os.path.splitext(os.path.basename(zip_file))[0])\n",
    "#         os.makedirs(extract_path, exist_ok=True)\n",
    "#         zip_ref.extractall(extract_path)\n",
    "#         print(f\"[INFO] Extracted to {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NIPS_2017_382', 'NIPS_2018_76', 'NIPS_2016_238', 'NIPS_2020_350',\n",
       "       'ICLR_2023_4741', 'NIPS_2019_1350', 'NIPS_2022_2635',\n",
       "       'NIPS_2019_772', 'NIPS_2018_857', 'ICLR_2022_445', 'NIPS_2018_134',\n",
       "       'NIPS_2017_631', 'NIPS_2021_40', 'ACL_2017_494_review',\n",
       "       'NIPS_2020_295', 'NIPS_2019_1089', 'NIPS_2016_395',\n",
       "       'ICLR_2022_3058', 'NIPS_2019_1246', 'NIPS_2018_66',\n",
       "       'ARR_2022_285_review', 'NIPS_2021_1917', 'NIPS_2018_15',\n",
       "       'ACL_2017_31_review', 'ARR_2022_93_review', 'NIPS_2016_115',\n",
       "       'NIPS_2021_2163', 'NIPS_2017_53', 'NIPS_2022_670', 'NIPS_2017_217',\n",
       "       'ICLR_2022_234', 'NIPS_2022_2523', 'NIPS_2020_1274',\n",
       "       'NIPS_2019_962', 'ICLR_2023_802', 'NIPS_2017_356', 'NIPS_2020_696',\n",
       "       'ICLR_2022_562', 'NIPS_2022_948', 'ARR_2022_286_review',\n",
       "       'NIPS_2019_1145', 'NIPS_2018_38', 'NIPS_2020_1371',\n",
       "       'NIPS_2022_1572', 'ARR_2022_23_review', 'ARR_2022_162_review',\n",
       "       'ARR_2022_64_review', 'ICLR_2023_1980', 'NIPS_2022_1913',\n",
       "       'NIPS_2019_737', 'ICLR_2023_598', 'ARR_2022_219_review',\n",
       "       'ICLR_2022_212', 'ICLR_2022_537', 'NIPS_2019_629',\n",
       "       'ICLR_2021_1213', 'NIPS_2018_296', 'NIPS_2021_1788',\n",
       "       'NIPS_2018_288', 'NIPS_2020_1080', 'ICLR_2023_4411',\n",
       "       'NIPS_2020_232', 'NIPS_2019_1397', 'NIPS_2017_370',\n",
       "       'ICLR_2021_1310', 'NIPS_2022_285', 'ICLR_2023_3449',\n",
       "       'ICLR_2022_2213', 'NIPS_2021_2304', 'ARR_2022_130_review',\n",
       "       'NIPS_2020_314', 'NIPS_2019_1366', 'ICLR_2022_3183',\n",
       "       'NIPS_2019_1338', 'ICLR_2022_2671', 'ACL_2017_699_review',\n",
       "       'ACL_2017_108_review', 'ICLR_2022_2660', 'NIPS_2018_482',\n",
       "       'ICLR_2021_872', 'NIPS_2021_138', 'ARR_2022_356_review',\n",
       "       'NIPS_2017_351', 'NIPS_2018_330', 'ARR_2022_295_review',\n",
       "       'NIPS_2020_1391', 'NIPS_2018_43', 'ICLR_2021_2674',\n",
       "       'NIPS_2020_335', 'NIPS_2018_265', 'NIPS_2022_789', 'NIPS_2019_651',\n",
       "       'NIPS_2016_9', 'NIPS_2021_304', 'ICLR_2022_1267', 'NIPS_2020_1384',\n",
       "       'NIPS_2016_386', 'NIPS_2016_283', 'NIPS_2021_1604',\n",
       "       'ICLR_2023_4659'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_ds['paper_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 10669.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_382_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_76_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2016/NIPS_2016_paper/NIPS_2016_238_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_350_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_4741_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1350_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_2635_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_772_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_857_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_445_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_134_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_631_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_40_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ACL/ACL/ACL_2017/ACL_2017_paper/ACL_2017_494_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_295_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1089_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2016/NIPS_2016_paper/NIPS_2016_395_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_3058_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1246_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_66_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_285_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_1917_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_15_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ACL/ACL/ACL_2017/ACL_2017_paper/ACL_2017_31_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_93_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2016/NIPS_2016_paper/NIPS_2016_115_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_2163_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_53_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_670_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_217_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_234_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_2523_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_1274_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_962_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_802_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_356_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_696_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_562_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_948_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_286_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1145_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_38_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_1371_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_1572_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_23_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_162_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_64_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_1980_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_1913_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_737_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_598_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_219_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_212_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_537_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_629_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2021/ICLR_2021_paper/ICLR_2021_1213_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_296_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_1788_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_288_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_1080_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_4411_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_232_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1397_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_370_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2021/ICLR_2021_paper/ICLR_2021_1310_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_285_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_3449_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_2213_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_2304_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_130_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_314_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1366_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_3183_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_1338_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_2671_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ACL/ACL/ACL_2017/ACL_2017_paper/ACL_2017_699_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ACL/ACL/ACL_2017/ACL_2017_paper/ACL_2017_108_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_2660_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_482_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2021/ICLR_2021_paper/ICLR_2021_872_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_138_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_356_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2017/NIPS_2017_paper/NIPS_2017_351_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_330_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ARR/ARR/ARR_2022/ARR_2022_paper/ARR_2022_295_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_1391_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_43_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2021/ICLR_2021_paper/ICLR_2021_2674_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_335_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2018/NIPS_2018_paper/NIPS_2018_265_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2022/NIPS_2022_paper/NIPS_2022_789_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2019/NIPS_2019_paper/NIPS_2019_651_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2016/NIPS_2016_paper/NIPS_2016_9_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_304_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2022/ICLR_2022_paper/ICLR_2022_1267_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2020/NIPS_2020_paper/NIPS_2020_1384_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2016/NIPS_2016_paper/NIPS_2016_386_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2016/NIPS_2016_paper/NIPS_2016_283_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/NIPS/NIPS/NIPS_2021/NIPS_2021_paper/NIPS_2021_1604_paper.json\n",
      "/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_4659_paper.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your base storage path\n",
    "base_path = \"/l/users/abdelrahman.sadallah/reviewer2_raw_data\"\n",
    "\n",
    "# List of OpenReview-supported venues\n",
    "openreview_venues = ['ICLR_2024', 'ICLR_2025', 'EMNLP_2023']\n",
    "\n",
    "# Define the function that constructs the file path\n",
    "def get_paper_path(venue: str, paper_id: str) -> str:\n",
    "    search_id = paper_id.replace('_review','')\n",
    "    if not search_id.endswith(\"_paper\"):\n",
    "        search_id += \"_paper\"\n",
    "\n",
    "    if venue in openreview_venues:\n",
    "        return os.path.join(base_path, venue, f\"{venue}_{paper_id}.pdf\")\n",
    "    else:\n",
    "        # e.g., venue = NIPS_2017 -> prefix = NIPS\n",
    "        prefix = venue.split('_')[0]\n",
    "        cur_path = os.path.join(base_path, prefix, prefix, venue,f\"{venue}_paper\",  f\"{search_id}.json\")\n",
    "        print(cur_path)\n",
    "        return cur_path\n",
    "# Define the check-and-download logic\n",
    "def ensure_pdf_and_get_path(venue: str, paper_id: str) -> str:\n",
    "    path = get_paper_path(venue, paper_id)\n",
    "    if os.path.isfile(path):\n",
    "        return path  # Already exists\n",
    "\n",
    "    # Try to download for OpenReview venues\n",
    "    if venue in openreview_venues:\n",
    "        if venue in paper_data and paper_id in paper_data[venue]:\n",
    "            paper_info = paper_data[venue][paper_id]\n",
    "            pdf_url = paper_info.get('content', {}).get('pdf', {}).get('value', None)\n",
    "            if pdf_url:\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                download_openreview_pdf(paper_id, path)\n",
    "                return path if os.path.isfile(path) else \"\"\n",
    "            else:\n",
    "                print(f\"[!] No PDF URL for {venue}/{paper_id}\")\n",
    "                return \"\"\n",
    "        else:\n",
    "            print(f\"[!] Paper {paper_id} not found in paper_data[{venue}]\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        # Not OpenReview, no download attempt\n",
    "        print(f\"[i] Skipping download for {venue}/{paper_id}; check local path.\")\n",
    "        return path if os.path.isfile(path) else \"\"\n",
    "\n",
    "# ==== MAIN EXECUTION ====\n",
    "\n",
    "# Assume `sampled_ds` and `paper_data` are already loaded\n",
    "# sampled_ds = pd.read_csv(...) etc.\n",
    "\n",
    "paper_paths = []\n",
    "\n",
    "for _, row in tqdm(sampled_ds.iterrows(), total=len(sampled_ds)):\n",
    "    venue = row['venue']\n",
    "    paper_id = row['paper_id']\n",
    "    path = ensure_pdf_and_get_path(venue, paper_id)\n",
    "    paper_paths.append(path)\n",
    "\n",
    "# Add the path column and save\n",
    "sampled_ds['paper_path'] = paper_paths\n",
    "sampled_ds.to_csv(\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/context_experiment.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ICLR_2023_4659_pdf.pdf',\n",
       " 'metadata': {'source': 'CRF',\n",
       "  'title': None,\n",
       "  'authors': [],\n",
       "  'emails': [],\n",
       "  'sections': [{'heading': '1 INTRODUCTION',\n",
       "    'text': 'The object recognition problem remains in an unclear state. Despite compelling performance of state-of-the-art object recognition methods, several questions such as out of distribution generalization (Recht et al., 2019; Barbu et al., 2019; Shankar et al., 2020; Taori et al., 2020; Koh et al., 2020), “superhuman performance” (He et al., 2015; Geirhos et al., 2018), adversarial vulnerability (Goodfellow et al., 2014), and invariance to image transformations and distortions (Hendrycks & Dietterich, 2019) still persist. Raw performance on test sets has been the main indicator of the progress and the major feedback about the state of the field. Few test sets have been proposed for evaluating object recognition models. Some follow the footsteps of ImageNet (Recht et al., 2019). Some filter images based on failures of models (Hendrycks et al., 2021). Researchers have also used controlled settings to collect data (Barbu et al., 2019; Borji et al., 2016). While being invaluable, these datasets suffer from few shortcomings. For example, datasets that only include examples for which the best models fail give the worst case scenario accuracy. While being useful, they underestimate model performance. Datasets that are biased towards certain environments (e.g. indoor scenes in ObjectNet (Barbu et al., 2019)), may not capture the full spectrum of visual stimuli. Most of the new datasets for object recognition have been centered on ImageNet (e.g. ImageNet-V2, ImageNet-A, ImageNet-O) and thus may have inherited its biases. This may in turn give us a biased assessment of visual recognition capability of models. We argue that a good test set should strike the right balance between sample difficulty and diversity and should reflect the average-case performance of models. We also believe that new test sets that are sufficiently different from existing ones can provide new insights into the object recognition problem. To this end, we include images that contain rich semantics and require cognitive processing (e.g. artistic scenes). Existing datasets lack enough samples of such cases.\\nHere, we emphasize image diversity and difficulty over scale. While scaling up test sets has a clear advantage (e.g. covering rare cases), it comes with some shortcomings. It is hard to ensure privacy,\\nsecurity, quality, and spurious correlations1 in datasets containing millions of images. These problems are easier to tackle in small scale expert-made datasets. Nonetheless, both small and large datasets are needed and are complementary.\\nOur dataset includes 8,060 images across 36 categories (Fig. 1). Images are carefully collected, verified, and labeled. We do not limit ourselves to object recognition models proposed in academia, and also consider prominent vision APIs in industry. This allows us to test models over a wider range of categories than those available in ImageNet and obtain a broader sense of image understanding by models. State-of-the-art models show a 30% absolute drop in Top-1 acc on D2O test set compared to the best ImageNet accuracy (around 20% drop using Top-5 acc). Further, over categories for which we know humans are very good at (e.g. faces, cars), current APIs fail drastically.\\nD2O test set is intentionally not paired with a training set. It comes with a license that disallows researchers to update the parameters of any model on it. This helps avoid over-fitting on the dataset. Additionally, to mitigate the danger of leaking our data to other datasets, we mark every image by a one pixel green border which must be removed on the fly before using.'},\n",
       "   {'heading': '2 OBJECT RECOGNITION TEST SETS',\n",
       "    'text': 'A plethora of datasets have been proposed for image classification2. Here, we are concerned with datasets that focus on core object recognition. ImageNet (Deng et al., 2009) is one of the most used datasets in computer vision and deep learning. It contains 1000 classes of common objects, with more than a million training images. Its test set contains 50,000 images. ImageNet test examples tend to be simple (by today’s standards), clear, and close-up images of objects. As such, they may not represent harder images encountered in the real world. Further, ImageNet annotations are limited to a single label per image. To remedy the problems with ImageNet, new test sets have been proposed, which have been instrumental in gauging performance of models and measuring the gap between models and humans. The major ones are reviewed below. In addition to these, several other datasets such as CIFAR-100 (Krizhevsky et al., 2009), SUN (Xiao et al., 2010), Places (Zhou et al., 2017), ImageNet-Sketch (Wang et al., 2019), and iLab20M (Borji et al., 2016) have also been introduced.\\nImageNet-V2. Recht et al. (2019) built this test by closely following the ImageNet creation process. They reported a performance gap of about 11% (Top-1 acc.) between the performance of the best models on this dataset and their performance on the original test set. Engstrom et al. (2020) estimated that the accuracy drop from ImageNet to ImageNet-V2 is less than 3.6%. Some other works have also evaluated and analyzed models on this dataset (Shankar et al., 2020; Taori et al., 2020).\\nImageNet-A, ImageNet-O, ImageNet-C, and ImageNet-P. These datasets are built to measure the robustness of image classifiers against out of distribution examples and image distortions (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Hendrycks et al., 2020). Specifically, ImageNet-A dataset contains images for which a pre-trained ResNet-50 model fails to predict the correct label (Fig. 2). It has 7,500 images scrapped from iNaturalist, Flickr, and DuckDuckGo websites. Following the approach in Hendrycks et al. (2021), researchers have gathered “natural adversarial examples” for other problems such as object detection (Lau et al., 2021) and microscopy analysis (Pedraza et al., 2022). In contrast to these datasets which benchmark the worst case performance of models, here we are interested in the average case performance. To this end, instead of filtering images to fool a\\n1Deep models take advantage of correlations between testing and training sets (a.k.a “spurious cues” or “shortcuts”). These correlations are easily accessible to models but are not to humans (Geirhos et al., 2020).\\n2https://paperswithcode.com/datasets?task=image-classification&page=2\\n.\\nclassifier, we include a mix of easy and hard examples to get a better sense of accuracy. Notice that a model that can only solve a very hard test set is not guaranteed to also solve an easy one.\\nReassessed Labels (ReaL). Beyer et al. (2020) collected new human annotations over the ImageNet validation set and used them to reassess the accuracy of ImageNet classifiers. They showed that model gains are substantially smaller than those reported using the original ImageNet labels. Further, they found that ReaL labels eliminate more than half of the ImageNet labeling mistakes. This implies that they provide a superior estimate of the model accuracy.\\nObjectNet. To remove the biases of the ImageNet, Barbu et al. (2019) introduced the ObjectNet dataset. Images are pictured by Mechanical Turk workers using a mobile app in a variety of backgrounds, rotations, and imaging viewpoints. ObjectNet contains 50,000 images across 313 categories, out of which 113 are in common with ImageNet categories. Astonishingly, Barbu et al. found that the state-of-the-art object recognition models perform drastically lower on ObjectNet compared to their performance on ImageNet (about 40-45% drop). Later on, Borji (2021) revisited the Barbu et al. ’s results and found that applying deep models to the isolated objects, rather than the entire scene as is done in the original paper, leads to 20-30% performance improvement.'},\n",
       "   {'heading': '3 D2O TEST SET',\n",
       "    'text': 'We followed two approaches to collect the data. In the first one, we used publicly-available and free-to-distribute sources. We crawled images from the Flicker and Google image search engine using different search queries. The queries contained terms specifying countries, locations, materials, different times (e.g. 80s), odd appearances (e.g. odd helmet), etc. We also included images from various categorized panels in the search results (e.g. drawing, sketch, clip art, icon, neon, clay, etc.). In the second approach, we used image generation tools such as DALL-E 2 (Ramesh et al., 2022), Midjourney, and Stable Diffusion (Rombach et al., 2021) to generate some image or search for some images generated these tools. We only selected the images that had good quality. Some sample generated images are shown in Appendix I. We did our best to ensure that images do contain sensitive material, have poor resolution, or violate copyright law3. The gathered encompass a wide variety of visual concepts over both RGB images, paintings, drawings, cartoons, and clip arts. To reduce ambiguity, most of the images contain one main object. Categories were selected based on the following two criteria: b) they should be possible to collect a variety of instances for them with different levels of difficulty, and c) one would consider model errors on them egregious (i.e. confusing a cat with a dog is more troublesome than confusing a beaver with a marmot). During data collection, we emphasized choosing the odd items.\\nThree annotators, who were also computer vision researchers, were presented with an image as well as its corresponding label. They were asked to verify the label by checking the correct or the incorrect box. We found that the three annotators agreed with the correct label over all images.\\n3We choose images that were public domain, did not have copyright, or were released by the government.\\nWe did not incorporate any bias towards gender, age, or race during data collection, and tried to be as inclusive as possible. Most of the categories are about objects. Few classes such as bicyclebuilt-for-two, face, helmet, person, sunglass, and umbrella contain humans and faces. We include and balance the number of images containing different ages and genders. The age groups are (child, 22), (teenager, 30), (adult, 51), and (elderly, 43). The gender groups include (woman, 66) and (man, 80). Notice that these issues are more important to address over large training sets. This is because sometimes models trained on such datasets are directly deployed in the real-world.\\nOur dataset contains 8,060 images spread across 36 categories, out of which 29 are in common with ImageNet. Six categories including {car, cat, cow, face, giraffe, person} do not appear in ImageNet, and are included mainly because they are very common and easily recognizable by humans. Seven of the categories correspond to multiple ImageNet categories, as shown in the mapping in Table 1. For a certain class, if a model predicted any of its corresponding ImageNet classes, we considered the prediction a hit. Sample images from our dataset are shown in Fig. 1. Distribution of object frequencies is shown in the left panel of Fig. 2. The most frequent class is the person followed by car and cat classes. Interestingly, there is a large variation of person in images as this topic has fascinated many artists over time (e.g. person made of wire, clay, matches).\\nThe miscellaneous category. This category includes images that do not simply fall under a specific category and may cover multiple concepts (e.g. hybrid animals or strange objects). Thus, this category is suitable for testing image tagging algorithms. It contains 576 images covering a wide variety of concepts and topics including hybrid animals, hybrid objects, art, illusions, camouflage objects, out of context objects, shadow, animals, fruits, drawings, paintings, objects made from different materials (e.g. glass, metal, clay, cloud, tattoos, or Lego), impersonating objects, strange daily objects, and objects from odd viewpoints. Sample images alongside their tags are shown in Fig. 3. This figure also presents the tag frequencies. The top 10 most frequent tags include (camouflage,60), (person,57), (Lego,41), (fish,40), (dog,33), (art,32), (house,20), (chair,18), (bird,16), (hand,16), and (duck,14).\\nTo put our dataset in perspective with other datasets and for cross-dataset comparison, we also evaluate models over 11 classes of the ImageNet-A dataset that also exist in our dataset. Sample images from three of these categories are shown in the right panel of Fig. 2.\\nThe D2O dataset is substantially different from ImageNet and ImageNet-A validation sets measured in terms of the Fréchet Inception Distance (FID) (Heusel et al., 2017). The FID between D2O and these sets in order are 45.2 and 51.3 indicating a large distribution shift. To put these numbers in\\nperspective, the FID between ImageNet’s validation and the test set is approximately 0.99. Notice that the lower the FID, the more similar the two distributions.'},\n",
       "   {'heading': '4 RESULTS AND ANALYSES', 'text': ''},\n",
       "   {'heading': '4.1 GENERIC OBJECT RECOGNITION',\n",
       "    'text': 'We tested 10 state-of-the-art object recognition models4, pre-trained on ImageNet, on our dataset. These models have been published over the past several years and have been immensely successful over the ImageNet benchmarks. They include AlexNet (Krizhevsky et al., 2012), MobileNetV2 (Sandler et al., 2018), GoogleNet (Szegedy et al., 2015), DenseNet (Huang et al., 2017), ResNext (Xie et al., 2017), ResNet101 and ResNet152 (He et al., 2016), Inception_V3 (Szegedy et al., 2016), Deit (Touvron et al., 2021), and ResNext_WSL (Mahajan et al., 2018). Details on accuracy computation are given in Appendix A.\\nModels are trained on ImageNet and tested only on the classes shared with ImageNet. Performance per D2O category, averaged over models, is shown in Fig. 4. The average performance, over all models and categories, is around 30% using Top-1 acc and around 50% using Top-5 acc. The corresponding numbers over the ImageNet-A dataset are about 5% and 15%, respectively. Therefore, ImageNet-A images are on average harder than D2O images for models, perhaps because they contain a lot of clutter. Prior research has shown that clutter and crowding severely hinder deep models (e.g. Volokitin et al. (2017)). It is not clear which object is the main one in most of the ImageNet-A images (See Fig. 2). To pinpoint whether and how much clutter contributes to low performance on this dataset, we manually cropped the object of interest in images (the blue bounding boxes in Fig. 2). The cropped objects have low resolution, but they are still recognizable by humans. Results on this dataset, called ImageNet-A-Isolated, will be discussed in the following.\\nAmong the models, resnext101_32x8d_ws5 ranks the best over both datasets, as shown in Fig. 5. It achieves around 60% Top-1 accuracy, which is much higher than its Top-1 acc over the\\n4Models are available in PyTorch hub: https://pytorch.org/hub/. We used a 12 GB NVIDIA Tesla K80 GPU to conduct the experiments.\\n5This model scores 85.4% (97.6% top-5) over ImageNet-1k validation set (single-crop).\\nImageNet-A dataset (∼40%). The Top-5 acc of this model on our dataset is about 80% compared to its 60% over ImageNet-A. The success of this model can be attributed to the fact that it is trained to predict hashtags on billions of social media images in a weakly supervised manner. The best performance on our dataset is much lower than the best performance on the ImageNet validation set. The best Top-1 and Top-5 performance over the latter are about 91% and 99%, respectively6. We find that better performance on ImageNet translates to better performance on D2O.\\nPerformance per model, averaged over D2O categories, is shown in the left panel of Fig. 6. We observe a big difference between accuracy of resnext101_32x8d_ws model vs. other models. This model is about 20% better than the second best model deit_bas_patch16_224, using Top-1 accuracy. The former model is based on weakly supervised learning whereas the latter is a transformer-based model. See Appendix for performance of individual models.\\nAs shown in Fig. 6, models perform much better over the Isolated ImageNet-A dataset than the original ImageNet-A, even though the former has low resolution images due to region cropping. This supports our argument that lower performance on ImageNet-A dataset is partly due to its scenes being cluttered. All categories enjoy an improvement in accuracy (See Appendix C).\\nWe also test the SwinTransformer (Liu et al., 2021)7 on D2O. This model has improved the state-ofthe-art over several computer vision problems including object detection, semantic segmentation, and action recognition. It scores 58.2% (Top-1) and 76.1% (Top-5). It performs much better than ResNet, Inception and Deit models, but is slightly below the resnext101_32x8d_ws model.\\n6https://paperswithcode.com/sota/image-classification-on-imagenet 7https://github.com/microsoft/Swin-Transformer\\nIllustrative Failure Modes. According to Fig. 4, the top five most difficult D2O categories for all models in order are kite, rabbit, squirrel, turtle, and mushroom which happen to be the most difficult categories for the best model as well (Fig. 5). The kite class is often confused with parachute, balloon, and umbrella classes. Sample failure cases from the categories along with the predictions of the resnext101_32x8d_ws model are shown in Fig. 7. Models often fail on drawings, unusual objects, or images where the object of interest is not unique. We also computed the fraction of images, per category, over which all models succeed, or they all fail, as shown in the right panel of Fig. 6. For some categories models consistently fail (e.g. kite, rabbit, turtle, squirrel), while for some others they all do very well (e.g. toaster, tractor, pretzel). When all models succeed, they are correct at best over 30% of the images (toaster). This result indicates that models share similar weaknesses and strengths.'},\n",
       "   {'heading': '4.2 PERFORMANCE OF VISION APIS',\n",
       "    'text': 'We also tested several APIs from Microsoft8, Google9, and MEGVII10 over the D2O categories that do not exist in ImageNet: {face, person, car, cat, cow, giraffe}). These APIs are very popular and highly accurate. The goal here is to see how models behave beyond ImageNet.\\nFace Detection. D2O face category has 289 images and includes a lot of odd and difficult faces, some are shown in Fig. 8. We are mainly interested in finding whether a model is close enough in detecting faces. To this end, we refrain from using mAP to evaluate the APIs and use the accuracy score, which is easier to understand and interpret. An image is considered as a hit if the API is able to generate a bounding box with IOU greater than or equal to 0.5 with a face in the image. Otherwise, the image is considered a mistake. We also manually verified all the predicted boxes. Our evaluation is an overestimation of performance rather than being a strict benchmark. Over the images for which the APIs failed, most often the predicted boxes did not overlap with any face in the image. In the majority of the mistakes, though, the face was missed.\\nEven with the above relaxed evaluation, APIs did not do well. Microsoft Azure face detection API achieves 45.3% accuracy in detecting D2O faces. The MEGVII Face++ API achieves 23.9% accuracy, slightly above the 23.2% by OpenCV face detector. Google MediaPipe face detector achieves 50.5% accuracy. Sample face images and predictions of the APIs are shown in Fig. 8.\\nPerson Detection. MEGVII API person detector obtains about 16% accuracy over the 1,217 person images in the person category. OpenCV person detector achieves 5% accuracy. Microsoft object detection API achieves 27.4% accuracy. If this API predicted a correct bounding box with any of the following classes {person, snowman, bronze sculpture, sculpture, doll}, we counted it as a hit. Evaluation is done the same way as in face detection.\\nCat Detection. Over the cat category (407 images), Microsoft object detection API, predicted 95 images as cat (95/407=0.23), 9 images as Persian cat (9/407=0.022), 26 images as animal (6/407 =\\n8https://azure.microsoft.com/en-us/services/cognitive-services/ 9https://google.github.io/mediapipe/\\n10https://www.faceplusplus.com/face-detection/\\n0.064), and 95 images as mammal (95/407= 0.23). Considering all of these images as hits, this API achieves 54.8% accuracy.\\nCow Detection. Over the cat category (407 images), Microsoft object detection API, predicted 95 images as cat (95/407=0.23), 9 images as Persian cat (9/407=0.022), 26 images as animal (6/407 = 0.064), and 95 images as mammal (95/407= 0.23). Considering all of these images as hits, this API achieves 54.8% accuracy.\\nGiraffe Detection. Over the giraffe category (138 images), Microsoft object detection API predicted 30 images as giraffe (30/138=0.217), 15 images as animal (15/138=0.108), and 46 images as mammal (46/138=0.333). Considering all of these images as hits, this API achieves 65.9% accuracy.\\nCar Detection. Microsoft object detection API achieves 25.8% accuracy (139 out of 539) on this category. An image was considered a hit if the API predicted a correct bounding box with any of these labels {car, land vehicle, all terrain vehicle, taxi, vehicle, race car, limousine, Van, station wagon}.\\nOn the one hand, our investigation shows that APIs perform very poorly, even considering overestimated accuracy, over categories that are very easy for humans. On the other hand, it reveals that our test set is challenging for a large array of models trained on a variety of datasets. Sample images from the above categories and predictions of the APIs on them are shown in Fig. 9.\\n4.3 TAGGING RESULTS API overlap no overlap no fractional (% ) (% ) prediction overlap\\nMSFT Tagger 46.4 53.6 0 21.8 \" Detector 8.5 36.3 55.2 17.1 \" Recognizer 0 91.3 8.7 0 \" Captioning 30.9 69.1 0 13.8 Google Tagger 39.9 60.1 0 17.1\\nTable 2: Image tagging performance of APIs over the miscellaneous category of our dataset. “No prediction” column shows the percent of images for which there was no prediction.\\nWe used the Microsoft tagging API to annotate the 576 images in the miscellaneous category. For 46.4% of the images, there is a common tag between predicted tags and ground truth tags (calculated for each image and then averaged over images). For the\\nremaining 53.6% of images, there is no overlap between the two sets. The fractional overlap between predicted and GT tags per image, computed as the number of common tags over the number of GT tags, is 21.8%. The Google Vision API11 performed slightly worse than the Microsoft API. Results are shown in Table 2 and Fig. 10.\\n11https://cloud.google.com/vision\\nWe also used Microsoft detection, recognition, and captioning APIs as image taggers by considering their generated labels or words as tags. Using the detection API, for 8.5% of the images, there was at least one tag in common between the detected label set and the ground truth tags. For 36.3% of the images, there was no overlap at all. For the remaining 55.2% of images, the API did not detect anything. The fractional overlap between the predicted and ground-truth tags is 17.1%. Using the recognition API, 91.3% of images had no overlap and the remaining 8.7% had no prediction at all. Using the Microsoft captioning API, for 30.9% of the images there was an overlap between predicted and ground-truth tags (69.1% had no overlap). The fractional overlap between the two sets is 13.8%. Overall, the tagging APIs perform better in tagging images than APIs that are not tailored for this task, but all of them still perform poorly in tagging images. Please see Table 2.'},\n",
       "   {'heading': '5 DISCUSSION AND CONCLUSION',\n",
       "    'text': 'We introduced a new test for object recognition and evaluated several models and APIs on it. We find that, despite years of research and significant progress in this field, there is still a large gap in performance of models over our dataset vs. ImageNet.\\nSome datasets rooted in ImageNet are biased towards borrowing its images and classes, or its data collection strategy. Here, we intended to deviate from these biases For example, unlike ImageNet-A, our dataset is model independent. ImageNet-A contains images from ImageNet for which models fail. Our dataset also includes categories, such as cows or cats, that perhaps everyone can easily recognize. These categories are missing in the ImageNet-based datasets. Further, our work encourages researchers and small teams to build carefully-curated, small-scale and versatile test sets frugally. Presently, the mindset is that datasets can only be collected by large institutions since data collection and annotation is difficult and expensive.\\nWe share the dataset and code to facilitate future work, and will organize an annual challenge and associated workshop to discuss state-of-the-art methods and best practices. To stop or limit the misuse of our D2O by bad actors, we have made a dataset request form12. We review the requests that we receive and allow access for a legitimate use. The dataset we share contains images and questions is a zip file. The package also contains the detailed documentation with all relevant metadata specified to users. Our dataset is licensed under Creative Commons Attribution 4.0 (AppendixH).\\n12https://bit.ly/3JxIKh3'},\n",
       "   {'heading': 'A MEASURING ACCURACY',\n",
       "    'text': 'Since some of our classes cover multiple ImageNet classes13, we had to make some adjustments for computing accuracy. For example, ImageNet has three types of clocks including ‘digital clock’, ‘wall clock’, and ‘analog clock’. Here, we only have the ‘clock’ class, containing mostly analog clocks. We chose to give the benefit of the doubt to models. A prediction is correct if the ground-truth label is in the set of the words predicted by the model. In the mentioned scenario, if a model predicts ‘wall clock’, then a hit is counted. If the model predicts ‘wall’ or anything else, then the prediction would be considered a mistake. The same is true for the top-5 accuracy computation. For example, if the top five model predictions are ‘bib’, ‘necklace’, ‘toilet seat’, ‘pick’, ‘wall clock’, then the prediction is counted as a hit. In practice, first all words in the predicted labels are extracted, and then the prediction is counted as a hit if the ground-truth is in this set. In case of ground-truth having two words (e.g. ‘toilet seat’), then it should happen in the set of words exactly as it is.\\nNotice that this way of accuracy measurement gives an overestimation of the model performance, but it is good enough for our purposes here. Even with this overestimation, as we will show, models still perform poorly.\\n13https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/'},\n",
       "   {'heading': 'B SAMPLES IMAGES FROM THE IMAGENET-A AND ISOLATED IMAGENET-A DATASETS',\n",
       "    'text': 'The rows correspond to acorn, banana, basketball, spatula, teddy, and toaster categories. The blue bounding box shows the region that we cropped to construct the Isolated ImageNet-A dataset.'},\n",
       "   {'heading': 'C CLASSIFICATION ACCURACY OF INDIVIDUAL MODELS', 'text': ''},\n",
       "   {'heading': 'D FREQUENCY OF GROUND-TRUTH TAGS AND MODEL PREDICTED TAGS',\n",
       "    'text': 'E SAMPLE FAILURE CASES OF BEST MODEL (RESNEXT101_32X8D_WS) OVER D2O DATASET\\nF SAMPLE FAILURE CASES OF BEST MODEL (RESNEXT101_32X8D_WS) OVER IMAGENET-A DATASET\\nG SAMPLE FAILURE CASES OF BEST MODEL (RESNEXT101_32X8D_WS) OVER SISOLATED IMAGENET-A DATASET'},\n",
       "   {'heading': 'H DATASET LICENSE',\n",
       "    'text': 'D2O dataset is free to use only for research and academic purposes (not commercial). It is licensed under Creative Commons Attribution 4.0 with three additional clauses:\\n1. D2O may never be used to tune the parameters of any model. 2. The images containing people should not to be posted anywhere unless the people in the\\nimages are appropriately de-identified. Even in this case, written agreement from dataset creators is required. This is to check whether all the clauses are properly followed.\\nTo stop or limit the misuse of our D2O by bad actors, we have made a dataset request form14. We review the requests that we receive and allow access for a legitimate use. The dataset we share contains images and questions is a zip file. The package also contains the detailed documentation with all relevant metadata specified to users.\\n14https://bit.ly/3bDY0MS'},\n",
       "   {'heading': 'I SAMPLE GENERATED IMAGES', 'text': ''}],\n",
       "  'references': [{'title': 'Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models',\n",
       "    'author': ['Andrei Barbu',\n",
       "     'David Mayo',\n",
       "     'Julian Alverio',\n",
       "     'William Luo',\n",
       "     'Christopher Wang',\n",
       "     'Dan Gutfreund',\n",
       "     'Josh Tenenbaum',\n",
       "     'Boris Katz'],\n",
       "    'venue': 'In Advances in Neural Information Processing Systems,',\n",
       "    'citeRegEx': 'Barbu et al\\\\.,? \\\\Q2019\\\\E',\n",
       "    'shortCiteRegEx': 'Barbu et al\\\\.',\n",
       "    'year': 2019},\n",
       "   {'title': 'Are we done with imagenet',\n",
       "    'author': ['Lucas Beyer',\n",
       "     'Olivier J Hénaff',\n",
       "     'Alexander Kolesnikov',\n",
       "     'Xiaohua Zhai',\n",
       "     'Aäron van den Oord'],\n",
       "    'venue': 'arXiv preprint arXiv:2006.07159,',\n",
       "    'citeRegEx': 'Beyer et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Beyer et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Contemplating real-world object classification',\n",
       "    'author': ['Ali Borji'],\n",
       "    'venue': 'arXiv preprint arXiv:2103.05137,',\n",
       "    'citeRegEx': 'Borji.,? \\\\Q2021\\\\E',\n",
       "    'shortCiteRegEx': 'Borji.',\n",
       "    'year': 2021},\n",
       "   {'title': 'ilab-20m: A large-scale controlled object dataset to investigate deep learning',\n",
       "    'author': ['Ali Borji', 'Saeed Izadi', 'Laurent Itti'],\n",
       "    'venue': 'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,',\n",
       "    'citeRegEx': 'Borji et al\\\\.,? \\\\Q2016\\\\E',\n",
       "    'shortCiteRegEx': 'Borji et al\\\\.',\n",
       "    'year': 2016},\n",
       "   {'title': 'Imagenet: A large-scale hierarchical image database',\n",
       "    'author': ['Jia Deng',\n",
       "     'Wei Dong',\n",
       "     'Richard Socher',\n",
       "     'Li-Jia Li',\n",
       "     'Kai Li',\n",
       "     'Li Fei-Fei'],\n",
       "    'venue': 'IEEE conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Deng et al\\\\.,? \\\\Q2009\\\\E',\n",
       "    'shortCiteRegEx': 'Deng et al\\\\.',\n",
       "    'year': 2009},\n",
       "   {'title': 'Identifying statistical bias in dataset replication',\n",
       "    'author': ['Logan Engstrom',\n",
       "     'Andrew Ilyas',\n",
       "     'Shibani Santurkar',\n",
       "     'Dimitris Tsipras',\n",
       "     'Jacob Steinhardt',\n",
       "     'Aleksander Madry'],\n",
       "    'venue': 'In International Conference on Machine Learning,',\n",
       "    'citeRegEx': 'Engstrom et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Engstrom et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Generalisation in humans and deep neural networks',\n",
       "    'author': ['Robert Geirhos',\n",
       "     'Carlos RM Temme',\n",
       "     'Jonas Rauber',\n",
       "     'Heiko H Schütt',\n",
       "     'Matthias Bethge',\n",
       "     'Felix A Wichmann'],\n",
       "    'venue': 'In Advances in neural information processing systems,',\n",
       "    'citeRegEx': 'Geirhos et al\\\\.,? \\\\Q2018\\\\E',\n",
       "    'shortCiteRegEx': 'Geirhos et al\\\\.',\n",
       "    'year': 2018},\n",
       "   {'title': 'Shortcut learning in deep neural networks',\n",
       "    'author': ['Robert Geirhos',\n",
       "     'Jörn-Henrik Jacobsen',\n",
       "     'Claudio Michaelis',\n",
       "     'Richard Zemel',\n",
       "     'Wieland Brendel',\n",
       "     'Matthias Bethge',\n",
       "     'Felix A Wichmann'],\n",
       "    'venue': 'Nature Machine Intelligence,',\n",
       "    'citeRegEx': 'Geirhos et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Geirhos et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Explaining and harnessing adversarial examples',\n",
       "    'author': ['Ian J Goodfellow', 'Jonathon Shlens', 'Christian Szegedy'],\n",
       "    'venue': 'arXiv preprint arXiv:1412.6572,',\n",
       "    'citeRegEx': 'Goodfellow et al\\\\.,? \\\\Q2014\\\\E',\n",
       "    'shortCiteRegEx': 'Goodfellow et al\\\\.',\n",
       "    'year': 2014},\n",
       "   {'title': 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classification',\n",
       "    'author': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],\n",
       "    'venue': 'In Proceedings of the IEEE international conference on computer vision,',\n",
       "    'citeRegEx': 'He et al\\\\.,? \\\\Q2015\\\\E',\n",
       "    'shortCiteRegEx': 'He et al\\\\.',\n",
       "    'year': 2015},\n",
       "   {'title': 'Deep residual learning for image recognition',\n",
       "    'author': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],\n",
       "    'venue': 'In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,',\n",
       "    'citeRegEx': 'He et al\\\\.,? \\\\Q2016\\\\E',\n",
       "    'shortCiteRegEx': 'He et al\\\\.',\n",
       "    'year': 2016},\n",
       "   {'title': 'Benchmarking neural network robustness to common corruptions and perturbations',\n",
       "    'author': ['Dan Hendrycks', 'Thomas Dietterich'],\n",
       "    'venue': 'arXiv preprint arXiv:1903.12261,',\n",
       "    'citeRegEx': 'Hendrycks and Dietterich.,? \\\\Q2019\\\\E',\n",
       "    'shortCiteRegEx': 'Hendrycks and Dietterich.',\n",
       "    'year': 2019},\n",
       "   {'title': 'The many faces of robustness: A critical analysis of out-of-distribution generalization',\n",
       "    'author': ['Dan Hendrycks',\n",
       "     'Steven Basart',\n",
       "     'Norman Mu',\n",
       "     'Saurav Kadavath',\n",
       "     'Frank Wang',\n",
       "     'Evan Dorundo',\n",
       "     'Rahul Desai',\n",
       "     'Tyler Zhu',\n",
       "     'Samyak Parajuli',\n",
       "     'Mike Guo'],\n",
       "    'venue': 'arXiv preprint arXiv:2006.16241,',\n",
       "    'citeRegEx': 'Hendrycks et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Hendrycks et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Natural adversarial examples',\n",
       "    'author': ['Dan Hendrycks',\n",
       "     'Kevin Zhao',\n",
       "     'Steven Basart',\n",
       "     'Jacob Steinhardt',\n",
       "     'Dawn Song'],\n",
       "    'venue': 'In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,',\n",
       "    'citeRegEx': 'Hendrycks et al\\\\.,? \\\\Q2021\\\\E',\n",
       "    'shortCiteRegEx': 'Hendrycks et al\\\\.',\n",
       "    'year': 2021},\n",
       "   {'title': 'Gans trained by a two time-scale update rule converge to a local nash equilibrium',\n",
       "    'author': ['Martin Heusel',\n",
       "     'Hubert Ramsauer',\n",
       "     'Thomas Unterthiner',\n",
       "     'Bernhard Nessler',\n",
       "     'Sepp Hochreiter'],\n",
       "    'venue': 'Advances in neural information processing systems,',\n",
       "    'citeRegEx': 'Heusel et al\\\\.,? \\\\Q2017\\\\E',\n",
       "    'shortCiteRegEx': 'Heusel et al\\\\.',\n",
       "    'year': 2017},\n",
       "   {'title': 'Densely connected convolutional networks',\n",
       "    'author': ['Gao Huang',\n",
       "     'Zhuang Liu',\n",
       "     'Laurens Van Der Maaten',\n",
       "     'Kilian Q Weinberger'],\n",
       "    'venue': 'In Proceedings of the IEEE conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Huang et al\\\\.,? \\\\Q2017\\\\E',\n",
       "    'shortCiteRegEx': 'Huang et al\\\\.',\n",
       "    'year': 2017},\n",
       "   {'title': 'Wilds: A benchmark of in-the-wild distribution shifts',\n",
       "    'author': ['Pang Wei Koh',\n",
       "     'Shiori Sagawa',\n",
       "     'Henrik Marklund',\n",
       "     'Sang Michael Xie',\n",
       "     'Marvin Zhang',\n",
       "     'Akshay Balsubramani',\n",
       "     'Weihua Hu',\n",
       "     'Michihiro Yasunaga',\n",
       "     'Richard Lanas Phillips',\n",
       "     'Sara Beery'],\n",
       "    'venue': 'arXiv preprint arXiv:2012.07421,',\n",
       "    'citeRegEx': 'Koh et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Koh et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Learning multiple layers of features from tiny images',\n",
       "    'author': ['Alex Krizhevsky', 'Geoffrey Hinton'],\n",
       "    'venue': None,\n",
       "    'citeRegEx': 'Krizhevsky and Hinton,? \\\\Q2009\\\\E',\n",
       "    'shortCiteRegEx': 'Krizhevsky and Hinton',\n",
       "    'year': 2009},\n",
       "   {'title': 'Imagenet classification with deep convolutional neural networks. In Advances in neural information processing',\n",
       "    'author': ['Alex Krizhevsky', 'Ilya Sutskever', 'Geoffrey E Hinton'],\n",
       "    'venue': None,\n",
       "    'citeRegEx': 'Krizhevsky et al\\\\.,? \\\\Q2012\\\\E',\n",
       "    'shortCiteRegEx': 'Krizhevsky et al\\\\.',\n",
       "    'year': 2012},\n",
       "   {'title': 'Natural adversarial objects',\n",
       "    'author': ['Felix Lau',\n",
       "     'Nishant Subramani',\n",
       "     'Sasha Harrison',\n",
       "     'Aerin Kim',\n",
       "     'Elliot Branson',\n",
       "     'Rosanne Liu'],\n",
       "    'venue': 'arXiv preprint arXiv:2111.04204,',\n",
       "    'citeRegEx': 'Lau et al\\\\.,? \\\\Q2021\\\\E',\n",
       "    'shortCiteRegEx': 'Lau et al\\\\.',\n",
       "    'year': 2021},\n",
       "   {'title': 'Swin transformer: Hierarchical vision transformer using shifted windows',\n",
       "    'author': ['Ze Liu',\n",
       "     'Yutong Lin',\n",
       "     'Yue Cao',\n",
       "     'Han Hu',\n",
       "     'Yixuan Wei',\n",
       "     'Zheng Zhang',\n",
       "     'Stephen Lin',\n",
       "     'Baining Guo'],\n",
       "    'venue': 'In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),',\n",
       "    'citeRegEx': 'Liu et al\\\\.,? \\\\Q2021\\\\E',\n",
       "    'shortCiteRegEx': 'Liu et al\\\\.',\n",
       "    'year': 2021},\n",
       "   {'title': 'Exploring the limits of weakly supervised pretraining',\n",
       "    'author': ['Dhruv Mahajan',\n",
       "     'Ross Girshick',\n",
       "     'Vignesh Ramanathan',\n",
       "     'Kaiming He',\n",
       "     'Manohar Paluri',\n",
       "     'Yixuan Li',\n",
       "     'Ashwin Bharambe',\n",
       "     'Laurens Van Der Maaten'],\n",
       "    'venue': 'In Proceedings of the European conference on computer vision (ECCV),',\n",
       "    'citeRegEx': 'Mahajan et al\\\\.,? \\\\Q2018\\\\E',\n",
       "    'shortCiteRegEx': 'Mahajan et al\\\\.',\n",
       "    'year': 2018},\n",
       "   {'title': 'Really natural adversarial examples',\n",
       "    'author': ['Anibal Pedraza', 'Oscar Deniz', 'Gloria Bueno'],\n",
       "    'venue': 'International Journal of Machine Learning and Cybernetics,',\n",
       "    'citeRegEx': 'Pedraza et al\\\\.,? \\\\Q2022\\\\E',\n",
       "    'shortCiteRegEx': 'Pedraza et al\\\\.',\n",
       "    'year': 2022},\n",
       "   {'title': 'Hierarchical text-conditional image generation with clip latents',\n",
       "    'author': ['Aditya Ramesh',\n",
       "     'Prafulla Dhariwal',\n",
       "     'Alex Nichol',\n",
       "     'Casey Chu',\n",
       "     'Mark Chen'],\n",
       "    'venue': 'arXiv preprint arXiv:2204.06125,',\n",
       "    'citeRegEx': 'Ramesh et al\\\\.,? \\\\Q2022\\\\E',\n",
       "    'shortCiteRegEx': 'Ramesh et al\\\\.',\n",
       "    'year': 2022},\n",
       "   {'title': 'Do imagenet classifiers generalize to imagenet',\n",
       "    'author': ['Benjamin Recht',\n",
       "     'Rebecca Roelofs',\n",
       "     'Ludwig Schmidt',\n",
       "     'Vaishaal Shankar'],\n",
       "    'venue': 'arXiv preprint arXiv:1902.10811,',\n",
       "    'citeRegEx': 'Recht et al\\\\.,? \\\\Q2019\\\\E',\n",
       "    'shortCiteRegEx': 'Recht et al\\\\.',\n",
       "    'year': 2019},\n",
       "   {'title': 'High-resolution image synthesis with latent diffusion models, 2021',\n",
       "    'author': ['Robin Rombach',\n",
       "     'Andreas Blattmann',\n",
       "     'Dominik Lorenz',\n",
       "     'Patrick Esser',\n",
       "     'Björn Ommer'],\n",
       "    'venue': None,\n",
       "    'citeRegEx': 'Rombach et al\\\\.,? \\\\Q2021\\\\E',\n",
       "    'shortCiteRegEx': 'Rombach et al\\\\.',\n",
       "    'year': 2021},\n",
       "   {'title': 'Mobilenetv2: Inverted residuals and linear bottlenecks',\n",
       "    'author': ['Mark Sandler',\n",
       "     'Andrew Howard',\n",
       "     'Menglong Zhu',\n",
       "     'Andrey Zhmoginov',\n",
       "     'Liang-Chieh Chen'],\n",
       "    'venue': 'In Proceedings of the IEEE conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Sandler et al\\\\.,? \\\\Q2018\\\\E',\n",
       "    'shortCiteRegEx': 'Sandler et al\\\\.',\n",
       "    'year': 2018},\n",
       "   {'title': 'Evaluating machine accuracy on imagenet',\n",
       "    'author': ['Vaishaal Shankar',\n",
       "     'Rebecca Roelofs',\n",
       "     'Horia Mania',\n",
       "     'Alex Fang',\n",
       "     'Benjamin Recht',\n",
       "     'Ludwig Schmidt'],\n",
       "    'venue': 'In International Conference on Machine Learning (ICML),',\n",
       "    'citeRegEx': 'Shankar et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Shankar et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Going deeper with convolutions',\n",
       "    'author': ['Christian Szegedy',\n",
       "     'Wei Liu',\n",
       "     'Yangqing Jia',\n",
       "     'Pierre Sermanet',\n",
       "     'Scott Reed',\n",
       "     'Dragomir Anguelov',\n",
       "     'Dumitru Erhan',\n",
       "     'Vincent Vanhoucke',\n",
       "     'Andrew Rabinovich'],\n",
       "    'venue': 'In Proceedings of the IEEE conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Szegedy et al\\\\.,? \\\\Q2015\\\\E',\n",
       "    'shortCiteRegEx': 'Szegedy et al\\\\.',\n",
       "    'year': 2015},\n",
       "   {'title': 'Rethinking the inception architecture for computer vision',\n",
       "    'author': ['Christian Szegedy',\n",
       "     'Vincent Vanhoucke',\n",
       "     'Sergey Ioffe',\n",
       "     'Jon Shlens',\n",
       "     'Zbigniew Wojna'],\n",
       "    'venue': 'In Proceedings of the IEEE conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Szegedy et al\\\\.,? \\\\Q2016\\\\E',\n",
       "    'shortCiteRegEx': 'Szegedy et al\\\\.',\n",
       "    'year': 2016},\n",
       "   {'title': 'Measuring robustness to natural distribution shifts in image classification',\n",
       "    'author': ['Rohan Taori',\n",
       "     'Achal Dave',\n",
       "     'Vaishaal Shankar',\n",
       "     'Nicholas Carlini',\n",
       "     'Benjamin Recht',\n",
       "     'Ludwig Schmidt'],\n",
       "    'venue': 'arXiv preprint arXiv:2007.00644,',\n",
       "    'citeRegEx': 'Taori et al\\\\.,? \\\\Q2020\\\\E',\n",
       "    'shortCiteRegEx': 'Taori et al\\\\.',\n",
       "    'year': 2020},\n",
       "   {'title': 'Training data-efficient image transformers & distillation through attention',\n",
       "    'author': ['Hugo Touvron',\n",
       "     'Matthieu Cord',\n",
       "     'Matthijs Douze',\n",
       "     'Francisco Massa',\n",
       "     'Alexandre Sablayrolles',\n",
       "     'Hervé Jégou'],\n",
       "    'venue': 'In International Conference on Machine Learning,',\n",
       "    'citeRegEx': 'Touvron et al\\\\.,? \\\\Q2021\\\\E',\n",
       "    'shortCiteRegEx': 'Touvron et al\\\\.',\n",
       "    'year': 2021},\n",
       "   {'title': 'Do deep neural networks suffer from crowding',\n",
       "    'author': ['Anna Volokitin', 'Gemma Roig', 'Tomaso A Poggio'],\n",
       "    'venue': 'In Advances in Neural Information Processing Systems,',\n",
       "    'citeRegEx': 'Volokitin et al\\\\.,? \\\\Q2017\\\\E',\n",
       "    'shortCiteRegEx': 'Volokitin et al\\\\.',\n",
       "    'year': 2017},\n",
       "   {'title': 'Learning robust global representations by penalizing local predictive power',\n",
       "    'author': ['Haohan Wang', 'Songwei Ge', 'Zachary Lipton', 'Eric P Xing'],\n",
       "    'venue': 'Advances in Neural Information Processing Systems,',\n",
       "    'citeRegEx': 'Wang et al\\\\.,? \\\\Q2019\\\\E',\n",
       "    'shortCiteRegEx': 'Wang et al\\\\.',\n",
       "    'year': 2019},\n",
       "   {'title': 'Sun database: Large-scale scene recognition from abbey to zoo',\n",
       "    'author': ['Jianxiong Xiao',\n",
       "     'James Hays',\n",
       "     'Krista A Ehinger',\n",
       "     'Aude Oliva',\n",
       "     'Antonio Torralba'],\n",
       "    'venue': 'In 2010 IEEE computer society conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Xiao et al\\\\.,? \\\\Q2010\\\\E',\n",
       "    'shortCiteRegEx': 'Xiao et al\\\\.',\n",
       "    'year': 2010},\n",
       "   {'title': 'Aggregated residual transformations for deep neural networks',\n",
       "    'author': ['Saining Xie',\n",
       "     'Ross Girshick',\n",
       "     'Piotr Dollár',\n",
       "     'Zhuowen Tu',\n",
       "     'Kaiming He'],\n",
       "    'venue': 'In Proceedings of the IEEE conference on computer vision and pattern recognition,',\n",
       "    'citeRegEx': 'Xie et al\\\\.,? \\\\Q2017\\\\E',\n",
       "    'shortCiteRegEx': 'Xie et al\\\\.',\n",
       "    'year': 2017},\n",
       "   {'title': 'Places: A 10 million image database for scene recognition',\n",
       "    'author': ['Bolei Zhou',\n",
       "     'Agata Lapedriza',\n",
       "     'Aditya Khosla',\n",
       "     'Aude Oliva',\n",
       "     'Antonio Torralba'],\n",
       "    'venue': 'IEEE transactions on pattern analysis and machine intelligence,',\n",
       "    'citeRegEx': 'Zhou et al\\\\.,? \\\\Q2017\\\\E',\n",
       "    'shortCiteRegEx': 'Zhou et al\\\\.',\n",
       "    'year': 2017}],\n",
       "  'referenceMentions': [{'referenceID': 24,\n",
       "    'context': 'Despite compelling performance of state-of-the-art object recognition methods, several questions such as out of distribution generalization (Recht et al., 2019; Barbu et al., 2019; Shankar et al., 2020; Taori et al., 2020; Koh et al., 2020), “superhuman performance” (He et al.',\n",
       "    'startOffset': 140,\n",
       "    'endOffset': 240},\n",
       "   {'referenceID': 0,\n",
       "    'context': 'Despite compelling performance of state-of-the-art object recognition methods, several questions such as out of distribution generalization (Recht et al., 2019; Barbu et al., 2019; Shankar et al., 2020; Taori et al., 2020; Koh et al., 2020), “superhuman performance” (He et al.',\n",
       "    'startOffset': 140,\n",
       "    'endOffset': 240},\n",
       "   {'referenceID': 27,\n",
       "    'context': 'Despite compelling performance of state-of-the-art object recognition methods, several questions such as out of distribution generalization (Recht et al., 2019; Barbu et al., 2019; Shankar et al., 2020; Taori et al., 2020; Koh et al., 2020), “superhuman performance” (He et al.',\n",
       "    'startOffset': 140,\n",
       "    'endOffset': 240},\n",
       "   {'referenceID': 30,\n",
       "    'context': 'Despite compelling performance of state-of-the-art object recognition methods, several questions such as out of distribution generalization (Recht et al., 2019; Barbu et al., 2019; Shankar et al., 2020; Taori et al., 2020; Koh et al., 2020), “superhuman performance” (He et al.',\n",
       "    'startOffset': 140,\n",
       "    'endOffset': 240},\n",
       "   {'referenceID': 16,\n",
       "    'context': 'Despite compelling performance of state-of-the-art object recognition methods, several questions such as out of distribution generalization (Recht et al., 2019; Barbu et al., 2019; Shankar et al., 2020; Taori et al., 2020; Koh et al., 2020), “superhuman performance” (He et al.',\n",
       "    'startOffset': 140,\n",
       "    'endOffset': 240},\n",
       "   {'referenceID': 9,\n",
       "    'context': ', 2020), “superhuman performance” (He et al., 2015; Geirhos et al., 2018), adversarial vulnerability (Goodfellow et al.',\n",
       "    'startOffset': 34,\n",
       "    'endOffset': 73},\n",
       "   {'referenceID': 6,\n",
       "    'context': ', 2020), “superhuman performance” (He et al., 2015; Geirhos et al., 2018), adversarial vulnerability (Goodfellow et al.',\n",
       "    'startOffset': 34,\n",
       "    'endOffset': 73},\n",
       "   {'referenceID': 8,\n",
       "    'context': ', 2018), adversarial vulnerability (Goodfellow et al., 2014), and invariance to image transformations and distortions (Hendrycks & Dietterich, 2019) still persist.',\n",
       "    'startOffset': 35,\n",
       "    'endOffset': 60},\n",
       "   {'referenceID': 24,\n",
       "    'context': 'Some follow the footsteps of ImageNet (Recht et al., 2019).',\n",
       "    'startOffset': 38,\n",
       "    'endOffset': 58},\n",
       "   {'referenceID': 13,\n",
       "    'context': 'Some filter images based on failures of models (Hendrycks et al., 2021).',\n",
       "    'startOffset': 47,\n",
       "    'endOffset': 71},\n",
       "   {'referenceID': 0,\n",
       "    'context': 'Researchers have also used controlled settings to collect data (Barbu et al., 2019; Borji et al., 2016).',\n",
       "    'startOffset': 63,\n",
       "    'endOffset': 103},\n",
       "   {'referenceID': 3,\n",
       "    'context': 'Researchers have also used controlled settings to collect data (Barbu et al., 2019; Borji et al., 2016).',\n",
       "    'startOffset': 63,\n",
       "    'endOffset': 103},\n",
       "   {'referenceID': 0,\n",
       "    'context': 'indoor scenes in ObjectNet (Barbu et al., 2019)), may not capture the full spectrum of visual stimuli.',\n",
       "    'startOffset': 27,\n",
       "    'endOffset': 47},\n",
       "   {'referenceID': 4,\n",
       "    'context': 'ImageNet (Deng et al., 2009) is one of the most used datasets in computer vision and deep learning.',\n",
       "    'startOffset': 9,\n",
       "    'endOffset': 28},\n",
       "   {'referenceID': 36,\n",
       "    'context': ', 2010), Places (Zhou et al., 2017), ImageNet-Sketch (Wang et al.',\n",
       "    'startOffset': 16,\n",
       "    'endOffset': 35},\n",
       "   {'referenceID': 33,\n",
       "    'context': ', 2017), ImageNet-Sketch (Wang et al., 2019), and iLab20M (Borji et al.',\n",
       "    'startOffset': 25,\n",
       "    'endOffset': 44},\n",
       "   {'referenceID': 3,\n",
       "    'context': ', 2019), and iLab20M (Borji et al., 2016) have also been introduced.',\n",
       "    'startOffset': 21,\n",
       "    'endOffset': 41},\n",
       "   {'referenceID': 27,\n",
       "    'context': 'Some other works have also evaluated and analyzed models on this dataset (Shankar et al., 2020; Taori et al., 2020).',\n",
       "    'startOffset': 73,\n",
       "    'endOffset': 115},\n",
       "   {'referenceID': 30,\n",
       "    'context': 'Some other works have also evaluated and analyzed models on this dataset (Shankar et al., 2020; Taori et al., 2020).',\n",
       "    'startOffset': 73,\n",
       "    'endOffset': 115},\n",
       "   {'referenceID': 13,\n",
       "    'context': 'These datasets are built to measure the robustness of image classifiers against out of distribution examples and image distortions (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Hendrycks et al., 2020).',\n",
       "    'startOffset': 131,\n",
       "    'endOffset': 209},\n",
       "   {'referenceID': 12,\n",
       "    'context': 'These datasets are built to measure the robustness of image classifiers against out of distribution examples and image distortions (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Hendrycks et al., 2020).',\n",
       "    'startOffset': 131,\n",
       "    'endOffset': 209},\n",
       "   {'referenceID': 19,\n",
       "    'context': '(2021), researchers have gathered “natural adversarial examples” for other problems such as object detection (Lau et al., 2021) and microscopy analysis (Pedraza et al.',\n",
       "    'startOffset': 109,\n",
       "    'endOffset': 127},\n",
       "   {'referenceID': 7,\n",
       "    'context': 'These correlations are easily accessible to models but are not to humans (Geirhos et al., 2020).',\n",
       "    'startOffset': 73,\n",
       "    'endOffset': 95},\n",
       "   {'referenceID': 23,\n",
       "    'context': 'In the second approach, we used image generation tools such as DALL-E 2 (Ramesh et al., 2022), Midjourney, and Stable Diffusion (Rombach et al.',\n",
       "    'startOffset': 72,\n",
       "    'endOffset': 93},\n",
       "   {'referenceID': 25,\n",
       "    'context': ', 2022), Midjourney, and Stable Diffusion (Rombach et al., 2021) to generate some image or search for some images generated these tools.',\n",
       "    'startOffset': 42,\n",
       "    'endOffset': 64},\n",
       "   {'referenceID': 14,\n",
       "    'context': 'The D2O dataset is substantially different from ImageNet and ImageNet-A validation sets measured in terms of the Fréchet Inception Distance (FID) (Heusel et al., 2017).',\n",
       "    'startOffset': 146,\n",
       "    'endOffset': 167},\n",
       "   {'referenceID': 18,\n",
       "    'context': 'They include AlexNet (Krizhevsky et al., 2012), MobileNetV2 (Sandler et al.',\n",
       "    'startOffset': 21,\n",
       "    'endOffset': 46},\n",
       "   {'referenceID': 26,\n",
       "    'context': ', 2012), MobileNetV2 (Sandler et al., 2018), GoogleNet (Szegedy et al.',\n",
       "    'startOffset': 21,\n",
       "    'endOffset': 43},\n",
       "   {'referenceID': 28,\n",
       "    'context': ', 2018), GoogleNet (Szegedy et al., 2015), DenseNet (Huang et al.',\n",
       "    'startOffset': 19,\n",
       "    'endOffset': 41},\n",
       "   {'referenceID': 15,\n",
       "    'context': ', 2015), DenseNet (Huang et al., 2017), ResNext (Xie et al.',\n",
       "    'startOffset': 18,\n",
       "    'endOffset': 38},\n",
       "   {'referenceID': 35,\n",
       "    'context': ', 2017), ResNext (Xie et al., 2017), ResNet101 and ResNet152 (He et al.',\n",
       "    'startOffset': 17,\n",
       "    'endOffset': 35},\n",
       "   {'referenceID': 10,\n",
       "    'context': ', 2017), ResNet101 and ResNet152 (He et al., 2016), Inception_V3 (Szegedy et al.',\n",
       "    'startOffset': 33,\n",
       "    'endOffset': 50},\n",
       "   {'referenceID': 29,\n",
       "    'context': ', 2016), Inception_V3 (Szegedy et al., 2016), Deit (Touvron et al.',\n",
       "    'startOffset': 22,\n",
       "    'endOffset': 44},\n",
       "   {'referenceID': 31,\n",
       "    'context': ', 2016), Deit (Touvron et al., 2021), and ResNext_WSL (Mahajan et al.',\n",
       "    'startOffset': 14,\n",
       "    'endOffset': 36},\n",
       "   {'referenceID': 20,\n",
       "    'context': 'We also test the SwinTransformer (Liu et al., 2021)7 on D2O.',\n",
       "    'startOffset': 33,\n",
       "    'endOffset': 51}],\n",
       "  'year': 2022,\n",
       "  'abstractText': 'Test sets are an integral part of evaluating models and gauging progress in object recognition, and more broadly in computer vision and AI. Existing test sets for object recognition, however, suffer from shortcomings such as bias towards the ImageNet characteristics and idiosyncrasies (e.g. ImageNet-V2), being limited to certain types of stimuli (e.g. indoor scenes in ObjectNet), and underestimating the model performance (e.g. ImageNet-A). To mitigate these problems, here we introduce a new test set, called D2O, which is sufficiently different from existing test sets. Images are diverse, unmodified, and representative of real-world scenarios and cause state-of-the-art models to misclassify them with high confidence. To emphasize generalization, our dataset by design does not come paired with a training set. It contains 8,060 images spread across 36 categories, out of which 29 appear in ImageNet. The best Top-1 accuracy on our dataset is around 60% which is much lower than 91% best Top-1 accuracy on ImageNet. We find that popular vision APIs perform very poorly in detecting objects over D2O categories such as “faces”, “cars”, and “cats”. Our dataset also comes with a “miscellaneous” category, over which we test the image tagging models. Overall, our investigations demonstrate that the D2O test set contain a mix of images with varied levels of difficulty and is predictive of the average-case performance of models. It can challenge object recognition models for years to come and can spur more research in this fundamental area. Data and code are publicly available at [Masked].',\n",
       "  'creator': 'LaTeX with hyperref'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/l/users/abdelrahman.sadallah/reviewer2_raw_data/ICLR/ICLR/ICLR_2023/ICLR_2023_paper/ICLR_2023_4659_paper.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 980.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 100 papers. Total with non-empty text: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "csv_path = \"/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/context_experiment.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def extract_paper_text(json_path):\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to load JSON {json_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "    title = metadata.get(\"title\", \"\")\n",
    "    sections = metadata.get(\"sections\", [])\n",
    "\n",
    "    text_parts = [title.strip()] if isinstance(title, str) and title.strip() else []\n",
    "\n",
    "    if isinstance(sections, list):\n",
    "        for section in sections:\n",
    "            heading = section.get(\"heading\", \"\")\n",
    "            section_text = section.get(\"text\", \"\")\n",
    "            if isinstance(heading, str) and heading.strip():\n",
    "                text_parts.append(heading.strip())\n",
    "            if isinstance(section_text, str) and section_text.strip():\n",
    "                text_parts.append(section_text.strip())\n",
    "\n",
    "    return \"\\n\\n\".join(text_parts)\n",
    "\n",
    "# Process each row\n",
    "paper_texts = []\n",
    "word_counts = []\n",
    "\n",
    "for paper_path in tqdm(df['paper_path']):\n",
    "    if not isinstance(paper_path, str) or not paper_path.strip():\n",
    "        paper_texts.append(\"\")\n",
    "        word_counts.append(0)\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.splitext(paper_path)[0] + \".json\"\n",
    "\n",
    "    if not os.path.isfile(json_path):\n",
    "        paper_texts.append(\"\")\n",
    "        word_counts.append(0)\n",
    "        continue\n",
    "\n",
    "    paper_text = extract_paper_text(json_path)\n",
    "    paper_texts.append(paper_text)\n",
    "    word_counts.append(len(paper_text.split()))\n",
    "\n",
    "# Add the columns and save\n",
    "df['paper_text'] = paper_texts\n",
    "df['paper_word_count'] = word_counts\n",
    "\n",
    "df.to_csv(\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/context_experiment_with_paper_text.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Processed {len(df)} papers. Total with non-empty text: {(df['paper_word_count'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 43695.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Missing 0 files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the DataFrame with paper_path\n",
    "df = pd.read_csv(\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/context_experiment.csv\")\n",
    "\n",
    "# Check if file exists for each path\n",
    "exist_flags = []\n",
    "missing_paths = []\n",
    "\n",
    "for path in tqdm(df['paper_path']):\n",
    "    exists = os.path.isfile(path) if isinstance(path, str) and path.strip() else False\n",
    "    exist_flags.append(exists)\n",
    "    if not exists:\n",
    "        missing_paths.append(path)\n",
    "\n",
    "\n",
    "# Optional: Print missing file paths\n",
    "print(f\"[!] Missing {len(missing_paths)} files:\")\n",
    "for path in missing_paths:\n",
    "    print(f\" - {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      100.000000\n",
       "mean      5242.910000\n",
       "std       2009.879704\n",
       "min       2369.000000\n",
       "25%       4222.000000\n",
       "50%       4874.500000\n",
       "75%       5627.750000\n",
       "max      18873.000000\n",
       "Name: paper_word_count, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/context_experiment_with_paper_text.csv\")\n",
    "\n",
    "df['paper_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['4', '5', '5']}\n"
     ]
    }
   ],
   "source": [
    "print(df['actionability'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09e97a6c8284f979e1675583f1d3d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b3c9d11c7e4450b893d63c82665e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset saved to Hugging Face format.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df = df.astype(str)\n",
    "# \n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Save the dataset to the Hugging Face format\n",
    "hf_dataset.push_to_hub(\n",
    "    repo_id=\"boda/review_evaluation_human_annotation\",\n",
    "    config_name=\"context_experiment_with_paper_text\",\n",
    "    split=\"full\",\n",
    ")\n",
    "print(\"✅ Dataset saved to Hugging Face format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ece1004c569498eae50288aa63f49d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd66f198df9451184b685000f0f22f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full-00000-of-00001.parquet:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504a0bc9f1d14c1db332869da9e69d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import datasets\n",
    "hf_dataset = datasets.load_dataset(\n",
    "    \"boda/review_evaluation_human_annotation\",\n",
    "    \"context_experiment_with_paper_text\",\n",
    "    split=\"full\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['4', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['4', '4', '3']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['4', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['3', '4', '4']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '2']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '3', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '3', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['1', '3', '1']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '4']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '4']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '3', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['2', '1', '2']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['3', '5', '3']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['4', '4', '1']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '3', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['3', '2', '2']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['1', '1', '1']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '4']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['4', '4', '2']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['1', '1', '1']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['2', '2', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '3', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['2', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['1', '3', '1']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['3', '2', '2']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '5', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['1', '2', '1']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['1', '1', '1']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '3']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '4', '4']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['1', '2', '1']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '2', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '4', '4']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '5', '3']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '5', '1']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['3', '3', '3']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['4', '3', '3']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '4', '1']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['2', '2', '4']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['1', '1', '1']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['4', '3', '4']}\",\n",
       " \"{'annotators': ['boda', '6686ebe474531e4a1975636f', '6740484e188a64793529ee77'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6740484e188a64793529ee77', '6686ebe474531e4a1975636f', 'boda'], 'labels': ['4', '5', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['5', '3', '3']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '3']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '5', '5']}\",\n",
       " \"{'annotators': ['boda', '6740484e188a64793529ee77', '6686ebe474531e4a1975636f'], 'labels': ['5', '4', '5']}\",\n",
       " \"{'annotators': ['6686ebe474531e4a1975636f', '6740484e188a64793529ee77', 'boda'], 'labels': ['2', '2', '1']}\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "hf_dataset['actionability']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

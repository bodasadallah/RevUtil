{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score,cohen_kappa_score, accuracy_score\n",
    "import pandas as pd\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "annotation_path = \"/home/abdelrahman.sadallah/mbzuai/review_rewrite/inference/evalute_outputs/adapters/Llama-3.1-8B-Instruct/score_only/instruction/all/step-0/review_evaluation_human_annotation\"\n",
    "\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for aspect in aspects:\n",
    "\n",
    "    gold_labels_file = f\"{annotation_path}/predictions_{aspect}_gold.jsonl\"\n",
    "    silver_labels_file = f\"{annotation_path}/predictions_{aspect}_silver.jsonl\"\n",
    "    gold_labels = []\n",
    "    silver_labels = []\n",
    "    with open(gold_labels_file, 'r') as f:\n",
    "        for line in f:\n",
    "            gold_labels.append(json.loads(line)[f'{aspect}_label'])\n",
    "    with open(silver_labels_file, 'r') as f:\n",
    "        for line in f:\n",
    "            silver_labels.append(json.loads(line)[f'{aspect}_label'])\n",
    "\n",
    "    ds1 = datasets.load_dataset('boda/review_evaluation_human_annotation', name=aspect, split='gold').to_pandas()\n",
    "    ds2 = datasets.load_dataset('boda/review_evaluation_human_annotation', name=aspect, split='silver').to_pandas()\n",
    "    results[aspect] = pd.concat([ds1, ds2], ignore_index=True)\n",
    "\n",
    "    results[aspect]['model_label'] = gold_labels + silver_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels order: ['1', '2', '3', '4', '5']\n",
      "Labels order: ['1', '2', '3', '4', '5']\n",
      "Labels order: ['1', '2', '3', '4', '5', 'X']\n",
      "Labels order: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "confusion_matrices = {}\n",
    "with open(f'outputs/human_best_model_agreement_results.txt', 'w') as f:\n",
    "    for aspect, df in results.items():\n",
    "        f.write(f'Agreement Statistics for {aspect}\\n')\n",
    "        f.write(f' Total number of samples: {len(df)}\\n')\n",
    "        ## get the prompt type\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        gold_labels = []\n",
    "        chatgpt_labels = []\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            gold_label = row[f'{aspect}_label']\n",
    "            chatgpt_label = row[f'model_label']\n",
    "            chatgpt_label = str(chatgpt_label)\n",
    "            if chatgpt_label.lower() in ['na','nan']:\n",
    "                continue\n",
    "\n",
    "            ## if the labels are numeric, convert them to string\n",
    "            if str(gold_label).lower() not in ['x', 'no claim']:\n",
    "                gold_label = str(int(float(gold_label)))\n",
    "            else:\n",
    "                gold_label = 'X'\n",
    "            if str(chatgpt_label).lower() not in ['x', 'no claim']:\n",
    "                ## extract the number from the string\n",
    "                chatgpt_label = ''.join(filter(str.isdigit, chatgpt_label))\n",
    "\n",
    "\n",
    "                chatgpt_label = str(int(float(chatgpt_label)))\n",
    "            else:\n",
    "                chatgpt_label = 'X'\n",
    "\n",
    "            gold_labels.append(gold_label)\n",
    "            chatgpt_labels.append(chatgpt_label)\n",
    "\n",
    "        f.write(f' Agreement Statistics for {type}\\n')\n",
    "        ## for verifiability we have one mroe measure\n",
    "        results = get_stats(gold=gold_labels, pred=chatgpt_labels, aspect=aspect)\n",
    "        f.write(json.dumps(results, indent=4) + '\\n')\n",
    "        \n",
    "\n",
    "        ## print confusion matrix\n",
    "        cm = confusion_matrix(gold_labels, chatgpt_labels)\n",
    "        labels_order = sorted(set(gold_labels) | set(chatgpt_labels))\n",
    "        print(f'Labels order: {labels_order}')\n",
    "        confusion_matrices[aspect] = cm\n",
    "\n",
    "        f.write('-' * 50 + '\\n')\n",
    "            \n",
    "    f.write('=' * 50 + '\\n')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

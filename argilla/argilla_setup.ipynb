{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mbzuai/review_rewrite\n"
     ]
    }
   ],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = Path(os.path.abspath(\"\")).parent\n",
    "print(module_path)\n",
    "sys.path.append(str(module_path))\n",
    "import utils as utils\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "argilla_api_url = os.getenv(\"argilla_api_url\")\n",
    "argilla_api_key = os.getenv(\"argilla_api_key\")\n",
    "\n",
    "client = rg.Argilla(\n",
    "  api_url=argilla_api_url,\n",
    "  api_key=argilla_api_key\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.me.username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create Admin user\n",
    "owner_user = rg.User(\n",
    "    username=\"tim\",\n",
    "    first_name=\"Tim\",\n",
    "    last_name=\"Baumgärtner\",\n",
    "    role=\"owner\",\n",
    "    password=\"123456789\",\n",
    "    client=client\n",
    ")\n",
    "\n",
    "if not client.users(owner_user.username):\n",
    "    owner_user.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 10\n",
    "\n",
    "NUM_ANNOTATORS = 5\n",
    "credentials_file = f'../data/human_annotation_{BATCH}/annotators_credentials.txt'\n",
    "\n",
    "## Create a list of uesrnames and passwords with \n",
    "user_data = utils.generate_username_password_list(NUM_ANNOTATORS)\n",
    "utils.save_to_file( user_data, credentials_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'qhBkNx6n' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for qhBkNx6n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'cYVX6CPX' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for cYVX6CPX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'whnmAGJS' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for whnmAGJS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'd1clt3oE' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for d1clt3oE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'RRgk02HS' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for RRgk02HS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'tim' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for tim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:253: UserWarning: Workspace with name 'boda' not found.\n",
      "  warnings.warn(f\"Workspace with name {name!r} not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workspace for boda\n"
     ]
    }
   ],
   "source": [
    "### Create annotator Users\n",
    "\n",
    "annotators = []\n",
    "\n",
    "annotators_credentials = utils.read_from_file(credentials_file)\n",
    "\n",
    "assert len(annotators_credentials) == NUM_ANNOTATORS\n",
    "\n",
    "\n",
    "# delete all users and workspaces\n",
    "for user in client.users:\n",
    "    if user.role == 'annotator':\n",
    "        user.delete()\n",
    "        \n",
    "# Delete all datasets\n",
    "for ds in client.datasets:\n",
    "    ds.delete()\n",
    "for workspace in client.workspaces:\n",
    "    workspace.delete()\n",
    "\n",
    "for i in range(NUM_ANNOTATORS):\n",
    "    \n",
    "    ## Create user\n",
    "\n",
    "    annotator_user = rg.User(\n",
    "        username=annotators_credentials[i]['username'],\n",
    "        password=annotators_credentials[i]['password'],\n",
    "        role=\"annotator\",\n",
    "        client=client\n",
    "    )\n",
    "\n",
    "    annotator_user.create()\n",
    "\n",
    "    ## create workspace per uesr\n",
    "    if not client.workspaces(annotator_user.username):\n",
    "        rg.Workspace(\n",
    "            name=annotator_user.username,\n",
    "            client=client\n",
    "        ).create()\n",
    "        print(f\"Created workspace for {annotator_user.username}\")\n",
    "\n",
    "    ## add user to workspace\n",
    "    client.workspaces(annotator_user.username).add_user(annotator_user.username)\n",
    "\n",
    "    ##\n",
    "    annotators.append({'user': annotator_user, 'workspace': client.workspaces(annotator_user.username)})\n",
    "\n",
    "## Create a workspace for the owner\n",
    "if not client.workspaces(owner_user.username):\n",
    "    rg.Workspace(\n",
    "        name=owner_user.username,\n",
    "        client=client\n",
    "    ).create()\n",
    "    print(f\"Created workspace for {owner_user.username}\")\n",
    "\n",
    "## Create a workspace for the Boda\n",
    "if not client.workspaces('boda'):\n",
    "    rg.Workspace(\n",
    "        name='boda',\n",
    "        client=client\n",
    "    ).create()\n",
    "    print(f\"Created workspace for boda\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Fields \n",
    "fields = [\n",
    "rg.TextField(\n",
    "    name=\"review_point\",\n",
    "    title=\"Review Point\",\n",
    "    use_markdown=False,\n",
    "    required=True,\n",
    "    description=\"Review point extracted from the full review. You are only required to evalutae this specific review poin, and not the full review.\",\n",
    "),\n",
    "# rg.TextField(\n",
    "#     name=\"review\",\n",
    "#     title=\"Review\",\n",
    "#     use_markdown=False,\n",
    "#     required=True,\n",
    "#     description=\"Full review text. Only use this if you needed context to evaluate the review point.\",\n",
    "# ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "rg.LabelQuestion(\n",
    "    name=\"actionability\",\n",
    "    labels={\n",
    "    \"1\": \"Unactionable\", \n",
    "    \"2\": \"Borderline Actionable\", \n",
    "    \"3\": \"Somewhat Actionable\", \n",
    "    \"4\": \"Mostly Actionable\", \n",
    "    \"5\": \"Highly Actionable\"\n",
    "    },\n",
    "    title=\"What is the Actiobability of the Review Point?\",\n",
    "    description='''\n",
    "Actionability refers to the clarity and specificity of guidance provided in review comments, ensuring authors know what actions to take to improve their work. Highly actionable feedback is both explicit and concrete, offering direct suggestions or clearly defined steps that authors can easily implement, whereas less actionable feedback may be vague, implicit, or leave authors uncertain about how to proceed. \n",
    "    ''',\n",
    "    required=True,\n",
    "    visible_labels=None\n",
    "),\n",
    "\n",
    "\n",
    "rg.LabelQuestion(\n",
    "    name=\"grounding_specificity\",\n",
    "    labels={\n",
    "        \"1\": \"Not Grounded\",\n",
    "        \"2\": \"Weakly grounded and not specific\",\n",
    "        \"3\": \"Weakly grounded and specific\",\n",
    "        \"4\": \"Fully grounded and not specific\",\n",
    "        \"5\": \"Fully grounded and specific\"},\n",
    "    title=\"How Grounded and Specific is the Review Point?\",\n",
    "    description='''Grounding and specificity refer to how clearly a review comment identifies the specific part of a paper being addressed (grounding) and how detailed the comment is in explaining the issue or suggestion (specificity). These aspects are crucial for ensuring authors can accurately pinpoint and address the feedback, improving the quality and relevance of revisions.''',\n",
    "    required=True,\n",
    "    visible_labels=5),\n",
    "\n",
    "rg.LabelQuestion(\n",
    "    name=\"verifiability\",\n",
    "    labels={\n",
    "        \"1\": \"Unverifiable\",\n",
    "        \"2\": \"Borderline verifiable\",\n",
    "        \"3\": \"Somewhat verifiable\",\n",
    "        \"4\": \"Mostly verifiable\",\n",
    "        \"5\": \"Fully verifiable\",\n",
    "        \"X\": \"No Claim\"},\n",
    "    title=\"How Verifiable is the Review Point?\",\n",
    "    description='''Verifiability refers to assessing whether a comment contains a claim or subjective opinion and, if so, how well that claim is supported with justification. Justification can include logical reasoning, common-sense arguments, or references, ensuring the claim is clear, substantiated, and helpful for authors to act upon.''',\n",
    "    required=True,\n",
    "    visible_labels=6),\n",
    "\n",
    "rg.LabelQuestion(\n",
    "    name=\"helpfulness\",\n",
    "    labels={\n",
    "        \"1\": \"The comment is not helpful at all\",\n",
    "        \"2\": \"The comment is barely helpful\",\n",
    "        \"3\": \"The comment is somewhat helpful\",\n",
    "        \"4\": \"The comment is mostly helpful\",\n",
    "        \"5\": \"The comment is highly helpful\"},\n",
    "    title=\"How Helpful is the Review Point?\",\n",
    "    description='''\n",
    "Helpfulness measures the value of a review comment to authors in improving their work, rated on a scale from 1 (not helpful at all) to 5 (highly helpful). Higher scores correspond to comments that identify weaknesses with clear, actionable suggestions, while lower scores reflect vague or unconstructive feedback that offers little guidance.''',\n",
    "    required=True,\n",
    "    visible_labels=5),\n",
    "\n",
    "rg.LabelQuestion(\n",
    "    name=\"professional_tone\",\n",
    "    labels={\n",
    "        \"0\": \"No\",\n",
    "        \"1\": \"Yes\",},\n",
    "    title=\"Is the review conducted in a Professional Tone?\",\n",
    "    description='''\n",
    "Professional Tone assesses the formality, respect, and clarity of language in peer reviews, ensuring constructive, respectful, and impartial feedback that fosters collaboration and focuses on the work rather than the individual.''',\n",
    "    required=False,\n",
    "    # visible_labels=2\n",
    "    ),\n",
    "\n",
    "rg.LabelQuestion(\n",
    "    name=\"valid_point\",\n",
    "    labels={\n",
    "        \"1\": \"Yes\",\n",
    "        \"0\": \"No\",},\n",
    "    title=\"Is this review point not complete (it can not be evaluated)?\",\n",
    "    description=\"If the review point is not complete, please select 'Yes'. Otherwise, select 'No'.\",\n",
    "    required=False,\n",
    "),\n",
    "rg.LabelQuestion(\n",
    "    name=\"addressed_to_author\",\n",
    "    labels={\n",
    "        \"authors\": \"Authors\",\n",
    "        \"meta_reviewer\": \"Meta Reviewer\",},\n",
    "    title=\"Is this comment addressed to the authors or the meta-reviewer?\",\n",
    "    description=\"Who do you think the comment is addressed to?\",\n",
    "    required=False,\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read a markdown file and store it into string\n",
    "with open('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/guidelines_short.md', 'r') as file:\n",
    "    guidelines =  file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "settings = rg.Settings(\n",
    "    fields=fields,\n",
    "    questions=questions,\n",
    "    guidelines=guidelines,\n",
    "    allow_extra_metadata=True,\n",
    "    distribution=rg.TaskDistribution(min_submitted=1),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba95dbb800244f7fa9739f4c169dbb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65798fa1e794a2d99a21753bc3e8695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/254k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f926bdb4240464a844eb717f6798cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"boda/review_evaluation_main_8\", split=\"train\")\n",
    "\n",
    "## take only first 25 entries \n",
    "# hf_dataset = hf_dataset.select(range(50))\n",
    "\n",
    "## save the chosen ids to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hf_dataset = hf_dataset.remove_columns(column_names=['paper_id', 'venue','actionability',\n",
    "#  'grounding_specificity',\n",
    "#  'verifiability',\n",
    "#  'helpfulness',\n",
    "#  'appropriateness',\n",
    "#  'Do you need full review?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/client.py:354: UserWarning: Dataset with name 'review_evaluation' not found in workspace 'qhBkNx6n'\n",
      "  warnings.warn(f\"Dataset with name {name!r} not found in workspace {workspace.name!r}\")\n",
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/records/_io/_datasets.py:265: UserWarning: Record id column not found in Hugging Face dataset. Using row index and split for record ids.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbdbd737a9b4d3e80b8c2d12d3f4e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/argilla/records/_mapping/_mapper.py:89: UserWarning: Keys ['paper_id', 'venue', 'focused_review', 'point'] in data are not present in the mapping and will be ignored.\n",
      "  warnings.warn(f\"Keys {unknown_keys} in data are not present in the mapping and will be ignored.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:13<00:00, 13.31s/batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:13<00:00, 13.70s/batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:13<00:00, 13.13s/batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:13<00:00, 13.76s/batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:11<00:00, 11.12s/batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:11<00:00, 11.27s/batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m200\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 100%|██████████| 1/1 [00:11<00:00, 11.87s/batch]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetRecords(Dataset(id=UUID('ab76ff0d-cc81-45a7-8034-426fe507e68a') inserted_at=datetime.datetime(2025, 3, 19, 11, 21, 37, 939735) updated_at=datetime.datetime(2025, 3, 19, 11, 21, 41, 337130) name='review_evaluation_boda' status='ready' guidelines=\"### General instructions:  \\n- For each aspect, classify the review point to the most suitable category it belongs to.  \\n- The review point you are evaluating is addressed to the draft’s authors, who have the best knowledge of the draft's content.  \\n- The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: *“Will this review point help authors improve their draft?”*  \\n- If you don’t have enough context to judge the review point or the review point is not complete, then choose the most suitable labels and mark the review as not complete.  \\n- Ignore statements that are mainly mentioning strengths points. Do this by clicking the **discard** button.  \\n- If you think a review point belongs to multiple categories, give it the **lower** label.\\n- You should evaluate each aspect independently ( a review point can have the highest score on one aspect but the lowest for the other)  \\n\\n\\n# Aspects\\n\\n# Actionability  \\n\\n## Why is it important?  \\nThis aspect is essential because one of the main goals of a review is to provide specific guidance for reviewers on how they can improve the current draft.  \\nAlso, it’s important to note that just giving an action comment is not enough, as sometimes this action is ambiguous and hard to follow. Hence, we should evaluate actionability according to two points:  \\n- **Is the action stated directly, or do you need to infer it? (Explicit vs. Implicit)**  \\n- **After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)**  \\n\\n \\nIt’s more important for actions to be **concrete** so the authors know how to apply them. It’s also preferred that actions be stated **directly** rather than inferred.  \\n\\n\\n\\n## Definitions  \\n- **Explicit:** direct, explicit, or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\\n- **Implicit:** actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.  \\n- **Concrete:** After Identifying the action, the authors know exactly what needs to be done and how to apply the action.  \\n- **Vague:** After Identifying the action, the authors still don’t know how to carry out this action.  \\n\\n---\\n\\n## Categories:  \\n\\n### 1: Unactionable  \\nThe comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.  \\n\\n\\n### 2: Borderline Actionable  \\nThe comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\\n\\n\\n\\n### 3: Somewhat Actionable  \\nThe comment explicitly states an action but is vague on how to execute it. \\n\\n\\n### 4: Mostly Actionable  \\nThe comment implicitly states an action but concretely states how to implement the inferred action.\\n\\n\\n### 5: Highly Actionable  \\nThe comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\\n\\n# Grounding and Specificity  \\n\\n## Why is this important?  \\nThis aspect measures how explicitly a review comment is based on a part of the paper. This is important so the authors know which part of their paper causes the issue and needs to be revised. Further, it measures how specifically the comment identifies what is the issue with this part of the paper. This aspect has two dimensions: (1) what part of the paper does this comment address, and (2) what is wrong with this part?  \\n\\n### Definitions:  \\n**Grounding:** measures how well the authors can identify what is being addressed by the comment. (This can be no grounding, weak grounding, or full grounding).  \\n- Weak grounding means that the author can’t precisely identify the part of the paper being addressed by the point, but they have some hint or guess about it.  \\n- Full grounding means the authors can accurately identify which part is being addressed. This can be done by:  \\n  - Making literal mentions of sections, tables, figures, etc.  \\n  - The point discusses something unique to the paper that the authors can identify.  \\n  - General comments that do not need to mention specific parts of the paper, but the authors can easily infer which parts are addressed.  \\n\\n**Specificity:** measures how much the reviewer detailed what is wrong/missing in this area. If the comment mentions some external work, it also measures whether it mentions specific examples.  \\n\\n### Categories:  \\n#### 1: Not Grounded  \\nThis comment is not grounded at all. It does not identify a specific area in the paper. The comment is highly unspecific.  \\n\\n\\n#### 2: Weakly grounded and not specific  \\nThe authors can not confidently determine which part the comment addresses. Further, the comment does not specify what needs to be addressed in this part.  \\n\\n#### 3: Weakly grounded and specific  \\nThe authors can not confidently determine which part the comment addresses. However, the comment clearly specifies what needs to be addressed in this part.  \\n\\n#### 4: Fully grounded and under-specific  \\nThe comment explicitly mentions which part of that paper it addresses, or it should be obvious to the authors. However, this comment does not specify what needs to be addressed in this part.  \\n\\n#### 5: Fully grounded and specific  \\nThe comment explicitly mentions which part of that paper it addresses, it is obvious to the authors. The comment specifies what needs to be addressed in this part.  \\n\\n---\\n\\n# Verifiability:\\n\\n## Why is this important?\\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don’t need to be stated directly; they can also be inferred.\\n\\n## Definitions:\\n\\n### Opinion & Claims\\n- Subjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\\n- Any suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\\n- Any comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\\n- Any deductions or inferred observations that go beyond just stating facts or results from the paper.\\n- Generally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\\n  **Ex:** “Important methods like X are not discussed”. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\\n\\n### Verification\\n- The claim is verified by providing logical reasoning.\\n- The claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\\n- The claim is verified by providing external references.\\n\\n### Normal Statements\\n- General statements about the paper, that don’t include an opinion.\\n- Objective and factual statements that don’t need any kind of verification.\\n- Asking for clarifications and general questions.\\n- Logical statements, or things that can be inferred directly.\\n- We treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\\n  **Example:** This paper is well written, and the experimentation methods are well designed.\\n\\n---\\n\\n## Categories\\n\\n### 1: Unverifiable\\nThe comment contains a claim without any supporting evidence or justification.  \\n\\n\\n\\n### 2: Borderline Verifiable\\nThe comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.  \\n\\n\\n\\n### 3: Somewhat Verifiable\\nThe comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.  \\n\\n\\n### 4: Mostly Verifiable\\nThe comment’s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.  \\n\\n\\n### 5: Fully Verifiable\\nThe claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\\n- Clear and precise reasoning or explanation.\\n- References to external works/data, when applicable, are specific and relevant.\\n- Common-sense arguments are logically unassailable.  \\n\\n\\n---\\n\\n### X - No Claim\\nThe comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.  \\n\\n--- \\n# Helpfulness:  \\nAssign a subjective score (1–5) to reflect the value of the review comment to the authors.  \\n\\n---   \\n\\n# Professional Tone\\nThis evaluates the professionalism of peer review feedback, focusing on formality, respect, and clarity. It ensures constructive, respectful, and impartial feedback that avoids personal attacks and centers on the work, fostering collaboration and improvement.\" allow_extra_metadata=True distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('72e26036-d320-4593-bf6d-132904cd4bfb') last_activity_at=datetime.datetime(2025, 3, 19, 11, 21, 41, 337130)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = client.datasets(name=\"review_evaluation\")\n",
    "\n",
    "\n",
    "## Delete all datasets\n",
    "# for ds in client.datasets:\n",
    "#     ds.delete()\n",
    "\n",
    "### Create dataset for each user\n",
    "for i in range(NUM_ANNOTATORS):\n",
    "    dataset = rg.Dataset(\n",
    "        name=f\"review_evaluation_{annotators[i]['user'].username}\",\n",
    "        workspace=annotators[i]['workspace'],\n",
    "        settings=settings,\n",
    "    )\n",
    "    dataset.create()\n",
    "    dataset.records.log(records=hf_dataset, mapping={\"point\": \"review_point\"})\n",
    "    # dataset.records.log(records=hf_dataset, mapping={\"focused_review\": \"review\", \"point\": \"review_point\"})\n",
    "\n",
    "## Create dataset for Boda, and Tim\n",
    "dataset = rg.Dataset(\n",
    "    name=f\"review_evaluation_tim\",\n",
    "    workspace=client.workspaces('tim'),\n",
    "    settings=settings,\n",
    ")\n",
    "dataset.create()\n",
    "dataset.records.log(records=hf_dataset, mapping={\"point\": \"review_point\"})\n",
    "dataset = rg.Dataset(\n",
    "    name=f\"review_evaluation_boda\",\n",
    "    workspace=client.workspaces('boda'),\n",
    "    settings=settings,\n",
    ")\n",
    "dataset.create()\n",
    "dataset.records.log(records=hf_dataset, mapping={\"point\": \"review_point\"})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Datasets</h3><table><tr><th>name</th><th>id</th><th>workspace_id</th><th>updated_at</th></tr><tr><td>review_evaluation_qhBkNx6n</td><td>939b572d-f4a4-4404-bdbc-e4d585cbaaea</td><td>b752e2ae-6637-446e-840e-0355a1a05091</td><td>2025-03-19T11:19:48.933865</td></tr><tr><td>review_evaluation_cYVX6CPX</td><td>0572f958-a23d-45e8-aed3-cf61a2b6f60a</td><td>bf9821d3-6163-44a9-9faf-085e2f1a112f</td><td>2025-03-19T11:20:08.632003</td></tr><tr><td>review_evaluation_whnmAGJS</td><td>4d3a9990-55c0-4612-92ea-ecfe42d345e0</td><td>c3f6ad99-14e6-4d51-95ca-eee206f8206d</td><td>2025-03-19T11:20:28.405854</td></tr><tr><td>review_evaluation_d1clt3oE</td><td>f54ddde3-3e30-4e6d-afe6-d70a714af7dc</td><td>a1cea484-5df1-45dd-b518-878f0ee0db49</td><td>2025-03-19T11:20:47.560159</td></tr><tr><td>review_evaluation_RRgk02HS</td><td>c1ad7622-a2aa-4715-b911-9785b02be411</td><td>2d6e54b0-af5c-44fa-aa78-5b8c73782120</td><td>2025-03-19T11:21:07.192435</td></tr><tr><td>review_evaluation_tim</td><td>383f13fc-62b6-4320-840e-5270e8a024dc</td><td>3a5a73d8-c718-4d42-b515-072c6d551c38</td><td>2025-03-19T11:21:24.218432</td></tr><tr><td>review_evaluation_boda</td><td>ab76ff0d-cc81-45a7-8034-426fe507e68a</td><td>72e26036-d320-4593-bf6d-132904cd4bfb</td><td>2025-03-19T11:21:41.337130</td></tr></table>"
      ],
      "text/plain": [
       "<argilla.client.Datasets at 0x7f5161957940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "from typing import List, Dict, Any, Callable, Union\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(file_path: str) -> Dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def by_procedure(p_values: List[float], q: float) -> List[int]:\n",
    "    p_values = np.array(p_values, dtype=float)\n",
    "    m = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_pvals = p_values[sorted_indices]\n",
    "    # Compute the harmonic sum H_m = 1 + 1/2 + ... + 1/m\n",
    "    H_m = np.sum(1.0 / np.arange(1, m + 1))\n",
    "    # Compute the BY thresholds for each rank i\n",
    "    by_thresholds = (np.arange(1, m + 1) / m) * (q / H_m)\n",
    "    max_i = -1\n",
    "    for i in range(m):\n",
    "        if sorted_pvals[i] <= by_thresholds[i]:\n",
    "            max_i = i\n",
    "    if max_i == -1:\n",
    "        return []\n",
    "    rejected_sorted_indices = sorted_indices[:max_i + 1]\n",
    "    return list(rejected_sorted_indices)\n",
    "\n",
    "\n",
    "def accuracy(pred: Any, annotations: List[Any]) -> float:\n",
    "    return float(np.mean([pred == ann for ann in annotations]))\n",
    "\n",
    "\n",
    "def neg_rmse(pred: Union[int, float], annotations: List[Union[int, float]]) -> float:\n",
    "    return -1 * float(np.sqrt(np.mean([(pred - ann) ** 2 for ann in annotations])))\n",
    "\n",
    "\n",
    "def sim(pred: str, annotations: List[str], similarity_func: Callable) -> float:\n",
    "    return float(np.mean([similarity_func(pred, ann) for ann in annotations]))\n",
    "\n",
    "\n",
    "def ttest(indicators, epsilon: float) -> float:\n",
    "    return ttest_1samp(indicators, epsilon, alternative='less').pvalue\n",
    "\n",
    "\n",
    "def alt_test(llm_annotations: Dict[Union[int, str], Any],\n",
    "             humans_annotations: Dict[Union[int, str], Dict[Union[int, str], Any]],\n",
    "             scoring_function: Union[str, Callable] = 'accuracy',\n",
    "             epsilon: float = 0.2,\n",
    "             q_fdr: float = 0.05,\n",
    "             min_humans_per_instance: int = 2,\n",
    "             min_instances_per_human: int = 30):\n",
    "    # prepare alignment scoring function\n",
    "    if isinstance(scoring_function, str):\n",
    "        if scoring_function == 'accuracy':\n",
    "            scoring_function = accuracy\n",
    "        elif scoring_function == 'neg_rmse':\n",
    "            scoring_function = neg_rmse\n",
    "        else:\n",
    "            raise ValueError(\"Unknown scoring function\")\n",
    "    else:\n",
    "        scoring_function = scoring_function\n",
    "\n",
    "    # prepare sets - i_set has humans as keys, h_set has instances as keys\n",
    "    i_set, h_set = {}, {}\n",
    "    for h, anns in humans_annotations.items():\n",
    "        i_set[h] = list(anns.keys())\n",
    "        for i, ann in anns.items():\n",
    "            if i not in h_set:\n",
    "                h_set[i] = []\n",
    "            h_set[i].append(h)\n",
    "\n",
    "    # remove instances with less than min_humans_per_instance\n",
    "    instances_to_keep = {i for i in h_set if len(h_set[i]) >= min_humans_per_instance and i in llm_annotations}\n",
    "    if len(instances_to_keep) < len(h_set):\n",
    "        print(f\"Dropped {len(h_set) - len(instances_to_keep)} instances with less than {min_humans_per_instance} annotators.\")\n",
    "    i_set = {h: [i for i in i_set[h] if i in instances_to_keep] for h in i_set}\n",
    "    h_set = {i: h_set[i] for i in h_set if i in instances_to_keep}\n",
    "\n",
    "    p_values, advantage_probs, humans = [], [], []\n",
    "    for excluded_h in humans_annotations:\n",
    "        llm_indicators = []\n",
    "        excluded_indicators = []\n",
    "        instances = [i for i in i_set[excluded_h] if i in llm_annotations]\n",
    "        if len(instances) < min_instances_per_human:\n",
    "            print(f\"Skipping annotator {excluded_h} with only {len(instances)} instances < {min_instances_per_human}.\")\n",
    "            continue\n",
    "\n",
    "        for i in instances:\n",
    "            human_ann = humans_annotations[excluded_h][i]\n",
    "            llm_ann = llm_annotations[i]\n",
    "            remaining_anns = [humans_annotations[h][i] for h in h_set[i] if h != excluded_h]\n",
    "            human_score = scoring_function(human_ann, remaining_anns)\n",
    "            llm_score = scoring_function(llm_ann, remaining_anns)\n",
    "            llm_indicators.append(1 if llm_score >= human_score else 0)\n",
    "            excluded_indicators.append(1 if human_score >= llm_score else 0)\n",
    "\n",
    "        diff_indicators = [exc_ind - llm_ind for exc_ind, llm_ind in zip(excluded_indicators, llm_indicators)]\n",
    "        p_values.append(ttest(diff_indicators, epsilon))\n",
    "        advantage_probs.append(float(np.mean(llm_indicators)))\n",
    "        humans.append(excluded_h)\n",
    "\n",
    "    rejected_indices = by_procedure(p_values, q_fdr)\n",
    "    advantage_prob = float(np.mean(advantage_probs))\n",
    "    winning_rate = len(rejected_indices) / len(humans)\n",
    "    return winning_rate, advantage_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = \"/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/outputs\"\n",
    "datasets_scoring_functions = {\n",
    "    'actionability': 'accuracy',\n",
    "    'grounding_specificity': 'accuracy',\n",
    "    'helpfulness': 'accuracy',\n",
    "    'verifiability': 'accuracy',\n",
    "}\n",
    "datasets_epsilons = {\n",
    "    'actionability': 0.3,\n",
    "    'grounding_specificity': 0.3,\n",
    "    'helpfulness': 0.3,\n",
    "    'verifiability': 0.3,\n",
    "}\n",
    "\n",
    "all_human_annotations = pd.read_csv('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/human_annotation_gathered/all_human_annotations_processed.csv')\n",
    "\n",
    "chat_gpt_annotations = {}\n",
    "for aspect in datasets_scoring_functions.keys():\n",
    "    aspect_raw_annotations = pd.read_csv(os.path.join(annotations_folder, f\"{aspect}_random_samples_results.csv\"))\n",
    "    chat_gpt_annotations[aspect] =  aspect_raw_annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'actionability',\n",
       "       'actionability_label', 'actionability_label_type', 'batch',\n",
       "       'grounding_specificity', 'grounding_specificity_label',\n",
       "       'grounding_specificity_label_type', 'verifiability',\n",
       "       'verifiability_label', 'verifiability_label_type', 'helpfulness',\n",
       "       'helpfulness_label', 'helpfulness_label_type', 'professional_tone',\n",
       "       'professional_tone_label', 'professional_tone_label_type',\n",
       "       'valid_point', 'valid_point_label', 'valid_point_label_type',\n",
       "       'chatgpt_verifiability_definitions_score',\n",
       "       'chatgpt_verifiability_definitions_rationale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_annotations[aspect].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "alt_test_data = {}\n",
    "\n",
    "for aspect in datasets_scoring_functions.keys():\n",
    "    ## take only rows from all_human_annotations that have the same review_point as the chat_gpt_annotations\n",
    "    aspect_human_annotaion = all_human_annotations[all_human_annotations['review_point'].isin(chat_gpt_annotations[aspect]['review_point'])].reset_index()\n",
    "    chat_gpt_annotations[aspect] = chat_gpt_annotations[aspect].set_index('review_point').reindex(aspect_human_annotaion['review_point']).reset_index()\n",
    "\n",
    "    aspect_chat_gpt_alt_labels = {}\n",
    "    aspect_human_alt_labels = {}\n",
    "\n",
    "    for i,row in aspect_human_annotaion.iterrows():\n",
    "        review_point = row['review_point']\n",
    "        assert review_point == chat_gpt_annotations[aspect]['review_point'][i]\n",
    "\n",
    "        chatgpt_label = chat_gpt_annotations[aspect][f'chatgpt_{aspect}_definitions_score'][i]\n",
    "        human_labels = row[aspect]\n",
    "        human_labels  = ast.literal_eval(human_labels)\n",
    "\n",
    "        aspect_chat_gpt_alt_labels[f'instance_{i}'] = chatgpt_label\n",
    "        for j, human_label in enumerate(human_labels):\n",
    "            human_id = f'annotator_{j}'\n",
    "            if human_id  not in aspect_human_alt_labels.keys():\n",
    "                aspect_human_alt_labels[human_id] = {}\n",
    "            aspect_human_alt_labels[human_id][f'instance_{i}'] = human_label\n",
    "    \n",
    "    alt_test_data[aspect] = {\n",
    "        'llm_annotations': aspect_chat_gpt_alt_labels,\n",
    "        'humans_annotations': aspect_human_alt_labels\n",
    "    }\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing actionability\n",
      "Skipping annotator annotator_4 with only 11 instances < 30.\n",
      "Skipping annotator annotator_5 with only 11 instances < 30.\n",
      "Skipping annotator annotator_6 with only 11 instances < 30.\n",
      "Skipping annotator annotator_7 with only 11 instances < 30.\n",
      "actionability gpt-4o [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.33\n",
      "\n",
      "Computing grounding_specificity\n",
      "Skipping annotator annotator_4 with only 11 instances < 30.\n",
      "Skipping annotator annotator_5 with only 11 instances < 30.\n",
      "Skipping annotator annotator_6 with only 11 instances < 30.\n",
      "Skipping annotator annotator_7 with only 11 instances < 30.\n",
      "grounding_specificity gpt-4o [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.32\n",
      "\n",
      "Computing helpfulness\n",
      "Skipping annotator annotator_4 with only 11 instances < 30.\n",
      "Skipping annotator annotator_5 with only 11 instances < 30.\n",
      "Skipping annotator annotator_6 with only 11 instances < 30.\n",
      "Skipping annotator annotator_7 with only 11 instances < 30.\n",
      "helpfulness gpt-4o [FAILED]:\tWinning Rate=0.00\tAdvantage Probability=0.42\n",
      "\n",
      "Computing verifiability\n",
      "Skipping annotator annotator_4 with only 11 instances < 30.\n",
      "Skipping annotator annotator_5 with only 11 instances < 30.\n",
      "Skipping annotator annotator_6 with only 11 instances < 30.\n",
      "Skipping annotator annotator_7 with only 11 instances < 30.\n",
      "verifiability gpt-4o [PASSED]:\tWinning Rate=0.50\tAdvantage Probability=0.69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for aspect in alt_test_data.keys():\n",
    "    epsilon = datasets_epsilons[aspect]\n",
    "    metric = datasets_scoring_functions[aspect]\n",
    "    print(f\"Computing {aspect}\")\n",
    "    humans_annotations = alt_test_data[aspect]['humans_annotations']\n",
    "    llm_annotations = alt_test_data[aspect]['llm_annotations']\n",
    "    llm_annotations  = {'gpt-4o': llm_annotations}\n",
    "    for llm_name, llm_annotations in llm_annotations.items():\n",
    "        wr, ap = alt_test(llm_annotations, humans_annotations, metric, epsilon=epsilon)\n",
    "        print(f\"{aspect} {llm_name} [{'PASSED' if wr >= 0.5 else 'FAILED'}]:\\tWinning Rate={wr:.2f}\\tAdvantage Probability={ap:.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

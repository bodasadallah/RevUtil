{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from inference_utils import *\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "import utils\n",
    "from utils import  get_stats\n",
    "\n",
    "### read jsonl file\n",
    "\n",
    "raw_predcitons = []\n",
    "\n",
    "with open('evalute_outputs/all/raw_outputs.jsonl') as f:\n",
    "    for line in f:\n",
    "        raw_predcitons.append(json.loads(line))\n",
    "        \n",
    "\n",
    "raw_predcitons = [str(i['generated_text']) for i in raw_predcitons]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = extract_predictions(raw_predcitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_dataset('boda/review_evaluation_automatic_labels', 'all')['test']\n",
    "\n",
    "gold_data = get_gold_labels(data, 'all','chatgpt_ASPECT_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_outputs) == len(gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actionability': 3,\n",
       " 'grounding_specificity': 5,\n",
       " 'verifiability': '3',\n",
       " 'helpfulness': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [ 'actionability', 'grounding_specificity','verifiability', 'helpfulness']\n",
    "## convert the list of dictionaries to dictionary of lists\n",
    "\n",
    "predctions = {aspect:[] for aspect in aspects}\n",
    "labels = {aspect:[] for aspect in aspects}\n",
    "\n",
    "for i in range(len(model_outputs)):\n",
    "    for aspect in aspects:\n",
    "\n",
    "        l = gold_data[i][aspect]\n",
    "        p = model_outputs[i][f'{aspect}_label']\n",
    "        l = str(l)\n",
    "        p = str(p)\n",
    "        predctions[aspect].append(p)\n",
    "        labels[aspect].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write the stats into file\n",
    "with open('evalute_outputs/all/evalute_outputs.txt', 'w') as f:\n",
    "    for aspect in aspects:\n",
    "        stat_dict = get_stats(predctions[aspect], labels[aspect], aspect)\n",
    "        f.write('*' * 20 + f'{aspect}'+ '*' * 20 + '\\n')\n",
    "\n",
    "        for k,v in stat_dict.items():\n",
    "            ## round the values\n",
    "            if isinstance(v, float):\n",
    "                v = round(v, 3)\n",
    "            f.write(f'{k}: {v}\\n')\n",
    "        f.write('-'*50+'\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from inference_utils import *\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "import utils\n",
    "from utils import  get_stats\n",
    "\n",
    "### read jsonl file\n",
    "\n",
    "model_outputs = []\n",
    "parent_dir = 'evalute_outputs/full_model/sciLitLLM/all/'\n",
    "data_path =   parent_dir + 'predictions.jsonl'\n",
    "aspect = 'all'\n",
    "\n",
    "\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        model_outputs.append(json.loads(line))\n",
    "        \n",
    "\n",
    "# raw_predcitons = [str(i['generated_text']) for i in raw_predcitons]\n",
    "# model_outputs = extract_predictions(raw_predcitons)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_dataset('boda/review_evaluation_automatic_labels', aspect)['test']\n",
    "\n",
    "gold_data = get_gold_labels(data, aspect,'chatgpt_ASPECT_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_outputs) == len(gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actionability': 3,\n",
       " 'grounding_specificity': 5,\n",
       " 'verifiability': '3',\n",
       " 'helpfulness': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [ 'actionability', 'grounding_specificity','verifiability', 'helpfulness'] if aspect == 'all' else [aspect]\n",
    "## convert the list of dictionaries to dictionary of lists\n",
    "\n",
    "\n",
    "predctions = {aspect:[] for aspect in aspects}\n",
    "labels = {aspect:[] for aspect in aspects}\n",
    "\n",
    "for i in range(len(model_outputs)):\n",
    "    for aspect in aspects:\n",
    "\n",
    "        l = gold_data[i][aspect]\n",
    "        p = model_outputs[i][f'{aspect}_label']\n",
    "        l = str(l)\n",
    "        p = str(p)\n",
    "        predctions[aspect].append(p)\n",
    "        labels[aspect].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write the stats into file\n",
    "with open(f'{parent_dir}evalute_outputs.txt', 'w') as f:\n",
    "    for aspect in aspects:\n",
    "        stat_dict = get_stats(predctions[aspect], labels[aspect], aspect)\n",
    "        f.write('*' * 20 + f'{aspect}'+ '*' * 20 + '\\n')\n",
    "\n",
    "        for k,v in stat_dict.items():\n",
    "            ## round the values\n",
    "            if isinstance(v, float):\n",
    "                v = round(v, 3)\n",
    "            f.write(f'{k}: {v}\\n')\n",
    "        f.write('-'*50+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = datasets.load_dataset('boda/review_evaluation_automatic_labels', 'all')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert score values to numeric before computing value counts\n",
    "# value_counts = pd.DataFrame({\n",
    "#     aspect: pd.to_numeric(data['train'].to_pandas()[f'chatgpt_{aspect}_score'], errors='coerce').value_counts()\n",
    "#     for aspect in aspects\n",
    "# })\n",
    "\n",
    "# # Sort index to ensure correct order\n",
    "# value_counts = value_counts.sort_index()\n",
    "\n",
    "# # Plot the combined value counts\n",
    "# value_counts.plot(kind='bar', title='Aspect Distribution')\n",
    "# plt.xlabel('Scores')\n",
    "# plt.ylabel('Counts')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from inference_utils import *\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "import utils\n",
    "from utils import  get_stats\n",
    "\n",
    "### read jsonl file\n",
    "aspect = 'all'\n",
    "\n",
    "data = datasets.load_dataset('boda/review_evaluation_automatic_labels', aspect)['test']\n",
    "\n",
    "gold_data = get_gold_labels(data, aspect,'chatgpt_ASPECT_score')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_outputs = []\n",
    "parent_dir = 'evalute_outputs/full_model/scilitllm/score_only/instruction/all/label_loss/'\n",
    "data_path =   parent_dir + 'predictions.jsonl'\n",
    "\n",
    "\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        model_outputs.append(json.loads(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_outputs) == len(gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [ 'actionability', 'grounding_specificity','verifiability', 'helpfulness'] if aspect == 'all' else [aspect]\n",
    "## convert the list of dictionaries to dictionary of lists\n",
    "\n",
    "\n",
    "predctions = {aspect:[] for aspect in aspects}\n",
    "labels = {aspect:[] for aspect in aspects}\n",
    "\n",
    "for i in range(len(model_outputs)):\n",
    "    for aspect in aspects:\n",
    "\n",
    "        l = gold_data[i][aspect]\n",
    "        p = model_outputs[i][f'{aspect}_label']\n",
    "        l = str(l)\n",
    "        p = str(p)\n",
    "\n",
    "        if p == 'None':\n",
    "            continue\n",
    "        predctions[aspect].append(p)\n",
    "        labels[aspect].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write the stats into file\n",
    "with open(f'{parent_dir}evalute_outputs.txt', 'w') as f:\n",
    "\n",
    "    f.write('number of samples: ' + str(len(labels[aspect])) + '\\n')\n",
    "    for a in aspects:\n",
    "        stat_dict = get_stats(predctions[a], labels[a], a)\n",
    "        f.write('*' * 20 + f'{a}'+ '*' * 20 + '\\n')\n",
    "\n",
    "        ## remove the acc from the dict, and for spearman, only include the correlation\n",
    "\n",
    "        for k,v in stat_dict.items():\n",
    "            if 'accuracy' in k:\n",
    "                continue\n",
    "            if 'spearman' in k:\n",
    "                v = v[0]\n",
    "            ## round the values\n",
    "            if isinstance(v, float):\n",
    "                v = round(v, 3)\n",
    "            f.write(f'{k}: {v}\\n')\n",
    "        f.write('-'*50+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = datasets.load_dataset('boda/review_evaluation_automatic_labels', 'all')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert score values to numeric before computing value counts\n",
    "# value_counts = pd.DataFrame({\n",
    "#     aspect: pd.to_numeric(data['train'].to_pandas()[f'chatgpt_{aspect}_score'], errors='coerce').value_counts()\n",
    "#     for aspect in aspects\n",
    "# })\n",
    "\n",
    "# # Sort index to ensure correct order\n",
    "# value_counts = value_counts.sort_index()\n",
    "\n",
    "# # Plot the combined value counts\n",
    "# value_counts.plot(kind='bar', title='Aspect Distribution')\n",
    "# plt.xlabel('Scores')\n",
    "# plt.ylabel('Counts')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁###', 835), ('Re', 1123), ('view', 1493), ('▁Point', 8984), (':', 29901), ('<0x0A>', 13), ('3', 29941), ('.', 29889), ('▁The', 450), ('▁theoretical', 23399), ('▁results', 2582), ('▁are', 526), ('▁based', 2729), ('▁on', 373), ('▁certain', 3058), ('▁assumptions', 20813), ('▁that', 393), ('▁may', 1122), ('▁not', 451), ('▁hold', 4808), ('▁in', 297), ('▁all', 599), ('▁practical', 15031), ('▁scenarios', 21846), ('.', 29889), ('▁', 29871), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('Output', 6466), (':', 29901), ('<0x0A>', 13), (\"{'\", 10998), ('action', 2467), ('ability', 3097), ('_', 29918), ('label', 1643), (\"':\", 2396)]\n",
      "[('▁', 29871), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('Output', 6466), (':', 29901), ('<0x0A>', 13)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13, 13, 2277, 29937, 6466, 29901, 13]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scitulu-7b\")\n",
    "\n",
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids))[0:])\n",
    "\n",
    "prompt = \"\"\"###Review Point:\n",
    "3. The theoretical results are based on certain assumptions that may not hold in all practical scenarios. \n",
    "###Output:\n",
    "{'actionability_label':\"\"\"\n",
    "print_tokens_with_ids(prompt)  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n",
    "\n",
    "response_template = \"\\n\\n###Output:\\n\"\n",
    "print_tokens_with_ids(response_template)  # [('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]\n",
    "\n",
    "tokenizer(response_template)['input_ids'][2:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

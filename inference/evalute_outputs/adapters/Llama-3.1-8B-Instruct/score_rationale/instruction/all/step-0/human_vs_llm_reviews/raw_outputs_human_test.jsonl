{"generated_text": "{'actionability_rationale': 'The review point suggests that the authors should conduct experiments to demonstrate the necessity of HTML structure analysis or understanding in the tasks they claim require it. The reviewer provides a specific example of how to conduct such an experiment by mapping all HTML tags except the salient ones to the same token, effectively removing the need for HTML structure analysis. This feedback is explicit and provides a concrete action for the authors to take, making it highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly addresses the claim about the need for understanding both structure and content of the web-page in the context of three specific tasks. It provides a detailed analysis of the role of structure in these tasks, particularly in the example of Semantic Classification, where the value of attributes rather than the structure is crucial. The suggestion to conduct experiments to demonstrate the need for HTML structure analysis is also specific, offering a concrete approach to address the issue. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the tasks require understanding of both structure and content of the web-page, but argues that the role of structure is not clear. It provides a specific example from Semantic Classification, where the value of attributes rather than the structure is crucial for classification. This example supports the claim by demonstrating that structure analysis may not be necessary for the task. The suggestion to conduct experiments to demonstrate the need for HTML structure analysis is also logical and provides a clear direction for further investigation. Therefore, the comment is mostly verifiable, as it provides a solid foundation for the claim with specific examples and reasoning.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment provides a detailed critique of the claim that the three tasks require understanding of both structure and content of the web-page. It challenges this assertion by pointing out that the role of structure is not clear and suggests that the tasks may not require significant HTML structure analysis. The reviewer offers a specific example from Semantic Classification, where the value of attributes rather than the structure is crucial for classification. This example is accompanied by a suggestion to conduct experiments that demonstrate the need for HTML structure analysis or understanding in these tasks. The feedback is clear, actionable, and provides a constructive approach for the authors to address the critique, making it highly helpful for improving the draft.', 'helpfulness_label': '5'}"}
{"generated_text": "{'actionability_rationale': 'The review point claims that there is \"not much novelty\" in the methodological aspects of the work. However, it does not provide any specific guidance or suggestions on how the authors could enhance the novelty or improve the methodological aspects. Without explicit or implicit actions, the authors are left without a clear understanding of what changes they should make to address this feedback. As a result, the comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment claims that there is \"not much novelty\" in the methodological aspects of the work. However, it does not specify which methodological aspects are lacking in novelty or provide any examples or references to support this claim. Without specific details or references, the authors cannot confidently determine which parts of the paper are being addressed or what needs to be improved. The comment is not grounded as it does not identify a specific section or aspect of the paper, and it is also not specific in its critique. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that there is \"not much novelty\" in the methodological aspects of the work. However, it lacks any supporting evidence, reasoning, or examples to substantiate this claim. Without specific details or references to what aspects are lacking in novelty, the authors may find it challenging to understand and address the feedback. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment claims that there is \"not much novelty\" in the methodological aspects of the work. However, it does not provide any specific examples, details, or suggestions on what aspects could be improved or how the authors might enhance the novelty of their methodological approach. Without actionable feedback or guidance, the authors are left without a clear understanding of how to address this critique. As a result, the comment is not helpful at all, as it does not offer any meaningful insights or suggestions for improvement.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point simply states that the paper uses n-gram overlapping metrics for automatic evaluation, without providing any explicit or implicit actions for the authors to take. It does not offer suggestions for improvement, clarification, or additional information that the authors might need to consider. As a result, the comment lacks any actionable guidance, leaving the authors without a clear understanding of what they should do to enhance their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment mentions the use of \"n-gram overlapping metrics for automatic evaluation,\" but it does not specify which part of the paper this is discussed in, nor does it provide any details on what this means or why it is relevant. Without explicit references or context, the authors cannot confidently determine which section of the paper is being addressed, making the comment not grounded. Additionally, the comment lacks specificity as it does not provide any guidance on what needs to be addressed or improved regarding the use of these metrics. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point is a factual statement describing the use of n-gram overlapping metrics for automatic evaluation. It does not express an opinion, suggestion, or claim that requires verification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The comment simply states that the paper uses n-gram overlapping metrics for automatic evaluation, without providing any context, critique, or suggestions for improvement. It lacks depth and does not offer any actionable feedback or insights that could help the authors enhance their draft. As a result, the comment is not helpful at all, as it does not provide any value or guidance for the authors to address or improve their work. Therefore, it aligns with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point explicitly suggests that the paper should include newer metrics such as BERTscore and BARTScore to check semantic similarity. It provides specific examples of the metrics to be included, along with references to the relevant studies (Zhang et al., 2019 and Yuan et al., 2021). This provides clear and concrete guidance on what the authors need to do to improve their draft. The explicit mention of specific metrics and references ensures that the authors know exactly how to implement the suggested action. Therefore, this comment is highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment suggests the inclusion of newer metrics such as BERTscore and BARTScore to check semantic similarity. However, it does not specify which part of the paper should include these metrics or how they should be integrated. The authors can infer that it relates to the evaluation or results section, but the comment lacks full grounding as it does not explicitly mention these sections. The suggestion is specific in recommending the use of these metrics, but without explicit grounding, the comment is weakly grounded and specific. Therefore, this comment aligns with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests the inclusion of newer metrics such as BERTscore and BARTScore to check semantic similarity. It provides references to the studies by Zhang et al. (2019) and Yuan et al. (2021), which supports the claim by offering specific examples of metrics that could be used. This level of detail provides a clear rationale for the suggestion, making the claim mostly verifiable. However, the comment could be strengthened by explaining why these metrics are particularly relevant or how they would enhance the paper. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment suggests the inclusion of newer metrics such as BERTscore and BARTScore to evaluate semantic similarity. This is a specific and actionable suggestion that could enhance the paper by providing more comprehensive evaluation methods. However, the comment could be more helpful if it explained why these metrics are particularly relevant or how they would improve the paper beyond existing methods. Additionally, it does not provide guidance on how to integrate these metrics into the existing evaluation framework. While the suggestion is clear, it lacks depth and could be more beneficial with further elaboration. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point states that most of the experiment analyses are in a quantitative way, but it does not provide any explicit or implicit actions for the authors to take. It lacks guidance on whether this is a positive or negative observation, or if there are any specific aspects that need to be addressed. Without any actionable advice or suggestions, the authors are left without a clear understanding of how to improve their draft based on this comment. Therefore, the comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment mentions that \"most of the experiment analyses are in a quantitative way,\" but it does not specify which experiments or sections of the paper this refers to. This lack of explicit reference makes it difficult for the authors to pinpoint the exact parts of the paper being addressed, resulting in weak grounding. Additionally, the comment does not provide specific details on what aspects of the quantitative analysis are being referred to or how they could be improved. Therefore, the comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point claims that \"most of the experiment analyses are in a quantitative way,\" but it does not provide any supporting evidence, reasoning, or examples to justify this claim. Without additional context or explanation, the authors may find it challenging to understand the basis of this assertion or how it could be addressed. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment notes that most of the experiment analyses are in a quantitative way, but it does not provide any further context, explanation, or suggestions for improvement. Without additional details or guidance, the authors are left without a clear understanding of whether this observation is positive, negative, or if it suggests any areas for enhancement. The lack of actionable feedback or specific suggestions makes the comment unhelpful, as it does not offer any meaningful insights or directions for the authors to improve their draft. Therefore, it aligns with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the authors should include more qualitative analysis, but it does not provide any specific guidance on what aspects of the qualitative analysis are missing or how to incorporate them into the draft. The action is implicit and vague, as the authors are left to infer what kind of qualitative analysis is needed and how to implement it. Without concrete details or examples, the authors may struggle to determine the exact steps to take to address the comment. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment suggests that the authors should include more qualitative analysis, but it does not specify which part of the paper lacks this analysis or what aspects of the qualitative analysis are missing. Without explicit references to sections, tables, or figures, the authors cannot confidently determine which part of the paper the comment addresses. Additionally, the comment lacks specificity regarding what kind of qualitative analysis is needed or how it should be integrated. Therefore, the comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point suggests that the paper should include more qualitative analysis. However, it does not provide any specific examples, reasoning, or references to support why this is necessary or how it would enhance the paper. Without detailed justification or evidence, the claim remains vague and unverifiable. Therefore, the comment is classified as \"1: Unverifiable.\"', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment suggests that the paper should include more qualitative analysis, which is a reasonable request for enhancing the depth and breadth of the study. However, it lacks specificity and does not provide guidance on what aspects of the qualitative analysis are missing or how the authors might incorporate them. Without detailed suggestions or examples, the authors are left with a general idea of what to improve but without a clear path forward. Therefore, the comment is somewhat helpful, as it identifies an area for improvement but does not offer actionable guidance. The authors would need to infer the specific changes needed, making the feedback incomplete. Hence, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review comment expresses a desire to see significant test results to ensure that the improvements are not random. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on what specific tests should be conducted, how to analyze the results, or what metrics to focus on. The comment lacks concrete details or suggestions, leaving the authors without a clear understanding of how to address the concern. As a result, the comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment expresses a desire to see significant test results to ensure that the improvements are not random. However, it does not specify which part of the paper this concern relates to, such as specific sections, experiments, or results. Without explicit references or detailed guidance, the authors cannot confidently determine which aspects of the paper need attention. The comment is not grounded and lacks specificity, making it difficult for the authors to address the issue effectively. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point expresses a desire to see significant test results to ensure that the improvements are not random. However, it does not provide any supporting evidence, reasoning, or examples to justify why significant test results are necessary or how they would address the concern about randomness. Without additional context or explanation, the claim remains vague and unverifiable. Therefore, the comment is classified as \"1: Unverifiable.\"', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment expresses a desire to see significant test results to ensure that the improvements are not random. However, it lacks specificity and does not provide any actionable guidance or suggestions on how the authors might achieve this goal. Without detailed feedback or examples, the authors are left without a clear understanding of what changes or analyses are needed to address the concern. As a result, the comment is barely helpful, as it identifies a potential area for improvement but does not offer sufficient guidance for the authors to make meaningful changes.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point claims that the approach is not new, but it does not provide any specific guidance or suggestions on how the authors should address this issue. There is no explicit or implicit action for the authors to take, such as revising the introduction to better contextualize the work or providing a more detailed explanation of the novelty. Without any actionable advice, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment claims that the approach is not new, but it does not specify which part of the paper this claim pertains to, such as the introduction, methodology, or results section. Without explicit references or context, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what aspects of the approach are not new or how they could be improved. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the approach is \"not new,\" but it lacks any supporting evidence, reasoning, or references to substantiate this claim. Without further explanation or context, the authors may find it challenging to understand the basis of this assertion or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment claims that the approach is not new, but it does not provide any specific context, examples, or suggestions on how the authors could address this issue or improve the novelty of their work. Without actionable feedback or guidance, the authors are left without a clear understanding of what needs to be revised or how to enhance the originality of their approach. This lack of specificity and actionable advice makes the comment unhelpful, as it does not contribute to the authors\\' understanding of how to improve their draft. Therefore, it aligns with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the differentiation between the authors\\' suggested prompting and \"prompt engineering,\" suggesting that the distinction may not be as clear-cut as the authors claim. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how the authors might address this concern or clarify the distinction in their work. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment addresses the authors\\' differentiation between their suggested prompting and \"prompt engineering,\" specifically questioning whether these are fundamentally different. However, it does not specify which part of the paper this differentiation is discussed in, making it weakly grounded. The comment is specific in its critique, as it challenges the authors\\' definition and suggests that their work could be included in the broader domain of prompt engineering. Therefore, this comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point questions the differentiation between the authors\\' suggested prompting and \"prompt engineering,\" suggesting that the distinction may not be as clear-cut as the authors claim. However, the comment does not provide specific examples, references, or detailed reasoning to support this claim. The lack of evidence or detailed justification makes it difficult for the authors to understand and address the concern. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a concern about the differentiation between the authors\\' suggested prompting and \"prompt engineering,\" questioning whether the distinction is as clear-cut as the authors claim. However, it does not provide specific suggestions or guidance on how the authors might address this issue or clarify the distinction in their work. The comment highlights a potential area of confusion but lacks actionable feedback, making it somewhat helpful. The authors are given some insight into a potential weakness in their differentiation, but the comment does not fully support them in improving their draft. Therefore, it aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the authors provide numbers on whether stacked representations improve performance in downstream tasks like classification, specifically mentioning the example of using ImageNet + text. However, the reviewer acknowledges that this could be a significant undertaking and does not want to base their review on it. The comment implies that the authors should consider this as a potential future direction, but it does not provide explicit instructions or concrete steps for the authors to follow. The action is implicit and somewhat vague, as the authors are left to infer that they should explore this direction but are not given specific guidance on how to do so. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment suggests that the authors provide numbers on whether stacked representations improve performance in downstream tasks like classification, specifically mentioning the example of using ImageNet + text. However, it does not specify which part of the paper this suggestion pertains to, such as a particular section or experiment. The authors can infer that it relates to the experimental results or methodology sections, but this inference is not direct. The comment is specific in suggesting a potential area for exploration but lacks grounding in terms of identifying the exact part of the paper being addressed. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point is a request for additional information or experimentation, specifically asking for numbers on whether stacked representations improve performance in downstream tasks like classification. The reviewer acknowledges that this could be a significant undertaking and does not base their review on it, but suggests it as a potential future direction. This comment does not contain a claim or opinion that requires verification, as it is a request for additional information rather than a statement that needs justification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment suggests a potential area for exploration by asking for numbers on whether stacked representations improve performance in downstream tasks like classification. It provides a specific example, such as using ImageNet + text, which could be a valuable addition to the paper. However, the comment acknowledges that this could be a significant undertaking and does not want to base the review on it. While the suggestion is clear and actionable, it could be more helpful if it offered guidance on how to approach this additional analysis or provided more context on why this exploration is important. Overall, the comment is mostly helpful as it identifies a potential area for improvement and provides a specific direction for the authors to consider, but it could be more comprehensive with additional guidance.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point expresses a lack of surprise regarding the experiment results, specifically the occurrence of forgetting issues in pre-trained language models when fine-tuned on downstream tasks and the performance of rehearsal-based methods. However, it does not provide any explicit or implicit actions for the authors to take. There is no suggestion for improvement, additional experiments, or changes to the methodology. As a result, the comment lacks any actionable guidance for the authors, leaving them without a clear path for enhancing their draft. Therefore, this comment is rated as unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment does not specify which experiments or results are being referred to, making it difficult for the authors to identify the exact parts of the paper being addressed. It also lacks specificity regarding what aspects of the experiment results are not surprising, such as the forgetting issues or the performance of rehearsal-based methods. Without clear grounding or specificity, the authors are left without a clear understanding of what needs to be addressed or improved. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point expresses a subjective opinion that the phenomena observed in the experiment results are not surprising. It provides a logical explanation for this opinion by stating that pre-trained language models would naturally have forgetting issues when fine-tuned on downstream tasks and that rehearsal-based methods would perform well for pre-trained models. However, the comment lacks specific examples or references to support this claim, making it somewhat verifiable. The authors would need to infer the reasoning and potentially conduct additional research to fully understand the basis of the reviewer\\'s opinion. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment expresses a lack of surprise regarding the experiment results, specifically the occurrence of forgetting issues in pre-trained language models when fine-tuned on downstream tasks and the performance of rehearsal-based methods. However, it does not provide any actionable feedback or suggestions for improvement. The comment lacks depth and does not offer any guidance on how the authors might address the issues or enhance their work. As a result, it is not helpful to the authors in terms of improving their draft, aligning with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point questions the conclusion that BERT is the most robust option for continual learning, suggesting that the authors should provide more evidence or analysis to support this claim. However, it does not explicitly instruct the authors to revise or expand their analysis, nor does it provide specific guidance on how to address this issue. The action is implicit and somewhat vague, as the authors can infer that they need to provide more evidence but are not given concrete steps to follow. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses the conclusion that BERT is the most robust option for continual learning and the analysis provided by the authors. However, it does not specify which part of the paper this conclusion is drawn from, making it weakly grounded. The comment is specific in questioning the robustness of BERT compared to other pre-trained models, but it lacks detailed guidance on how the authors might address this issue. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point questions the conclusion that BERT is the most robust option for continual learning, suggesting that the authors have not provided sufficient evidence to support this claim. The reviewer points out that, based on the figures and tables, BERT does not appear to be significantly better than other pre-trained models. However, the comment lacks specific examples or references to support the claim that BERT is not the most robust option. This makes the claim somewhat verifiable, as the authors would need to provide additional evidence or analysis to address the critique. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment questions the conclusion that BERT is the most robust option for continual learning, suggesting that the authors have not provided sufficient evidence to support this claim. It points out that, based on the figures and tables, BERT does not appear to be significantly better than other pre-trained models. This feedback is somewhat helpful as it identifies a potential weakness in the paper's conclusion and prompts the authors to reconsider their analysis. However, the comment could be more helpful if it provided specific suggestions on how to strengthen the evidence or comparison with other models. Therefore, it aligns with a score of 3, indicating that the comment is somewhat helpful.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review comment points out that the figures and tables suggest BERT and other models appear similar, but the authors have not provided a comprehensive explanation or quantitative comparison to support this claim. While the comment identifies a gap in the paper, it does not explicitly instruct the authors to provide such an explanation or comparison. The action is implicit and somewhat vague, as the authors can infer that they need to address this issue but are not given specific guidance on how to do so. Therefore, the comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"figures and tables,\" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it points out that the authors did not provide a comprehensive explanation or a concrete quantitative comparison to support the claim that BERT and other models look similar. This provides clear guidance on what needs to be addressed to improve the paper. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the figures and tables suggest BERT and other models appear similar, but the authors have not provided a comprehensive explanation or quantitative comparison to support this claim. The comment highlights a gap in the paper, noting the absence of detailed analysis or comparison. However, it does not provide specific examples or references to substantiate the claim, making it somewhat verifiable. The authors would need to infer the exact nature of the similarity and the need for a more detailed analysis. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a potential issue with the figures and tables, suggesting that BERT and other models appear similar but lacks a comprehensive explanation or quantitative comparison to support this claim. This feedback is somewhat helpful as it points out a gap in the paper that needs to be addressed. However, it does not provide specific guidance on how the authors might improve the explanation or comparison, such as suggesting particular metrics or analyses to include. While it highlights an area for improvement, the comment could be more helpful with additional details or actionable suggestions. Therefore, it aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a potential issue with the experimental results, noting that the model size and compression ratio are impressive but may not be entirely fair since the embedding layer is included in the deployment. The reviewer suggests that the authors should clarify this aspect, which implies that the authors should provide a more detailed explanation or discussion about the model's size and compression ratio in the context of the embedding layer. While the comment identifies an area for clarification, it does not provide explicit instructions on how to address the issue or what specific details should be included. The action is implicit and somewhat vague, making the comment barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment addresses the issue of model size and compression ratio, specifically mentioning the use of the whole model including the embedding layer during deployment. However, it does not explicitly mention which part of the paper this discussion is located in, such as a specific section or figure. This makes it weakly grounded, as the authors can infer the general area but not pinpoint the exact location. The comment is specific in detailing the issue with the experimental results and the need for clarification regarding the embedding layer. Therefore, the comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the experimental results are not entirely fair because the whole model, including the embedding layer, is used during deployment, which affects the model size and compression ratio. The reviewer provides specific examples of the model size and compression ratio, which supports the claim. However, the comment lacks detailed reasoning or references to substantiate why the inclusion of the embedding layer impacts the results. While the examples provide some evidence, the lack of further explanation or references makes the claim somewhat verifiable. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a potential issue with the experimental results, specifically noting that the impressive model size and compression ratio may not be entirely fair since the whole model, including the embedding layer, is used during deployment. The reviewer points out that adding the embedding layer would significantly increase the model size and decrease the compression ratio, which could make the results less surprising. This feedback is somewhat helpful as it highlights a critical aspect of the experimental setup that needs clarification. However, it lacks specific suggestions on how the authors should address this issue or provide additional context to make the results more meaningful. To be more helpful, the comment could include recommendations for how to present the results or discuss the implications of the embedding layer. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a discrepancy between the number of related works mentioned and the limited comparison of these works in the experiments. It also mentions that some other methods are missing in the related works. However, the comment does not provide specific guidance on which methods are missing or how the authors should address this issue. The action is implicit and vague, as the authors are left to infer that they need to include more comparisons or add missing methods, but without clear instructions on how to do so. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment highlights a discrepancy between the number of related works mentioned and the limited comparison of these works in the experiments. It also mentions that some other methods are missing in the related works, providing a specific example with \"[1].\" However, the comment does not specify which part of the paper this issue pertains to, such as the related works section or the experimental results section. This makes it weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. The comment is specific in detailing the issue of missing comparisons and the need to include more methods, but without clear grounding, it aligns with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the authors mention many related works but only compare a few of them in the experiments, and that some other methods are missing in the related works. The comment provides a specific example with \"[1],\" which suggests that the reviewer has identified a particular method that is missing. However, the comment lacks detailed reasoning or examples to fully substantiate the claim, making it somewhat verifiable. The authors would need to infer the specific methods missing and their relevance to the work. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a specific issue with the paper, noting that while the authors mention many related works, they only compare a few of them in the experiments. It also points out that some other methods are missing in the related works, providing an example with \"[1].\" This feedback is somewhat helpful as it highlights a gap in the comparison of related works and suggests that the authors should include more methods in their analysis. However, the comment could be more helpful if it provided specific suggestions on which methods to include or how to improve the comparison. The authors gain some insight into an area for improvement but need more detailed guidance to fully address the issue. Therefore, the comment is rated as somewhat helpful.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the paper would be strengthened by including experiments with specific models, such as GPT3, Instruct-GPT3, and larger language models. However, it does not provide explicit instructions on how to conduct these experiments or what specific aspects of the models should be tested. The action is implicit, as the authors need to infer that they should conduct additional experiments, and it is vague regarding the details of how to implement these experiments. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment suggests that the paper would be strengthened by including experiments with specific models, such as GPT3, Instruct-GPT3, and larger language models. However, it does not specify which part of the paper this suggestion pertains to, making it weakly grounded. The comment is specific in suggesting the inclusion of these models, but it lacks detailed guidance on how to implement these experiments or which aspects of the models should be tested. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests that the paper would be strengthened by including experiments with specific models, such as GPT3, Instruct-GPT3, and larger language models. However, the comment does not provide any reasoning or evidence to support why these models would be beneficial or how they would enhance the paper. Without specific examples or justification, the claim remains vague and unverifiable. Therefore, the comment is rated as 1: Unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests that the paper would be strengthened by including experiments with specific models, such as GPT3, Instruct-GPT3, and larger language models. This feedback is clear and actionable, as it provides a specific direction for the authors to enhance their work by exploring these models. However, the comment could be more helpful if it offered guidance on how to integrate these models into the existing experimental framework or what specific aspects of the models should be tested. Despite this, the suggestion is valuable and provides a clear path for improvement, making the comment mostly helpful.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the clarity of the paper regarding whether recitation helps with larger scales, which is important for the generalizability of the method. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this issue or what specific information should be added to clarify the paper. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a concern about the clarity of the paper regarding whether recitation helps with larger scales, which is important for the generalizability of the method. However, it does not specify which part of the paper this issue is addressed in, making it difficult for the authors to pinpoint the exact section that needs clarification. The comment lacks grounding as it does not provide a clear reference to a specific section, figure, or table. Additionally, it is not specific about what aspects of the paper are unclear or how the authors might address this issue. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point raises a concern about the clarity of the paper regarding whether recitation helps with larger scales, which is important for the generalizability of the method. However, the comment does not provide any supporting evidence, examples, or references to substantiate this claim. Without additional context or justification, the authors may find it challenging to understand the basis of the concern or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a concern about the clarity of the paper regarding whether recitation helps with larger scales, which is important for the generalizability of the method. This is a valid point that could impact the understanding and applicability of the results. However, the comment does not provide specific suggestions or guidance on how the authors might address this issue or clarify the paper. Without actionable feedback or detailed advice, the authors are left with a general idea of what might be lacking but without a clear path to improvement. Therefore, the comment is somewhat helpful, as it identifies a potential weakness but lacks depth and specificity in its guidance.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that larger language models (LMs) might perform better in both recitation and direct question-answering (QA) tasks. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this observation or what specific steps to consider in their work. The comment lacks actionable details, leaving the authors without a clear understanding of how to proceed. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment suggests that larger language models (LMs) might perform better in both recitation and direct question-answering (QA) tasks. However, it does not specify which part of the paper this observation is based on, nor does it provide any guidance on how the authors might address this issue. Without explicit references to sections, tables, or figures, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what needs to be addressed or how the authors might explore this observation further. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point suggests that larger language models (LMs) might perform better in both recitation and direct question-answering (QA) tasks. However, it does not provide any supporting evidence, reasoning, or references to substantiate this claim. The statement is based on a speculative observation and lacks detailed justification or examples, making it difficult for the authors to understand the basis of the suggestion. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests a potential direction for improvement by speculating that larger language models might perform better in both recitation and direct question-answering tasks. However, it lacks specificity and does not provide actionable guidance or suggestions for the authors to explore this idea further. Without detailed feedback or examples, the comment does not offer much value in helping the authors improve their draft. Therefore, it is rated as barely helpful, aligning with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that experiments on InstructGPT, T0, or FLAN would be particularly interesting, given their fine-tuning on instructions, examples, or human preferences. However, it does not provide explicit instructions or guidance on how the authors should conduct these experiments or what specific aspects they should focus on. The action is implicit, as the authors need to infer that they should consider these models for their experiments. Additionally, the comment lacks concrete details on how to implement these suggestions, making it vague. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment suggests conducting experiments on specific models like InstructGPT, T0, or FLAN, which are mentioned by their references. However, it does not specify which part of the paper these suggestions should be incorporated into, such as the methodology or results sections. The authors can infer that these suggestions relate to the experimental design or analysis, but the comment lacks full grounding as it does not explicitly mention these sections. The suggestion is specific, as it identifies particular models that could be of interest, but the lack of explicit grounding makes it weakly grounded and specific. Therefore, this comment aligns with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests that experiments on InstructGPT, T0, or FLAN would be interesting due to their fine-tuning on instructions, examples, or human preferences. However, the comment does not provide any supporting evidence, reasoning, or references to justify why these models are particularly relevant or why their fine-tuning would be beneficial. Without additional context or explanation, the claim remains vague and unverifiable. Therefore, the comment is rated as 1: Unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests that experiments on specific models like InstructGPT, T0, or FLAN could be particularly interesting due to their fine-tuning on instructions, examples, or human preferences. This feedback is somewhat helpful as it provides a direction for potential experimentation that could enhance the paper. However, it lacks depth and does not offer specific guidance on how to conduct these experiments or what aspects to focus on. The comment could be more helpful if it included detailed suggestions or examples of how these models could be integrated into the existing experimental framework. Therefore, it aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a limitation of the proposed method, specifically the lack of discussion on the applicability of retrieval augmented systems to tail distribution information. It also mentions that generalization to information not present in the training set is only briefly mentioned in the conclusion. While the comment identifies an important limitation, it does not provide explicit guidance on how the authors should address this issue or suggest specific changes to improve the discussion. The action is implicit and somewhat vague, as the authors are left to infer that they should expand on these points but are not given concrete steps on how to do so. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment highlights a limitation of the proposed method, specifically the lack of discussion on the applicability of retrieval augmented systems to tail distribution information and generalization to information not present in the model\\'s training set. It mentions that the first point is not discussed at all, while the second is only mentioned in the conclusion. However, the comment does not specify which part of the paper should include this discussion or provide guidance on how to address these limitations. The authors can infer that the discussion should be expanded, but the comment lacks explicit grounding and specificity. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point claims that the proposed method has limitations in its applicability to tail distribution information and generalization to information not present in the training set. It provides specific examples, such as COVID for BERT, to support the claim. However, the comment lacks detailed reasoning or references to substantiate the claim fully. While the examples given are relevant, the lack of additional evidence or detailed explanation makes the claim somewhat verifiable. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': \"The review comment identifies a significant limitation of the proposed method, specifically its lack of discussion on the applicability of retrieval augmented systems to tail distribution information and generalization to information not present in the model's training set. It highlights that while the generalization aspect is mentioned in the conclusion, the discussion on tail distribution information is entirely absent. This feedback is valuable as it points out a critical gap in the paper's discussion, which the authors can address to enhance the comprehensiveness and validity of their work. However, the comment could be more helpful if it provided specific suggestions on how to incorporate these discussions or examples to support the authors' claims. Overall, the comment is mostly helpful as it directs the authors' attention to important areas for improvement, but it could be more actionable with additional guidance.\", 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point poses a question about whether most of the correct recitation cases were seen often during training. However, it does not provide any explicit or implicit actions for the authors to take. The question lacks guidance on what the authors should do with this information or how it might impact their work. Without any actionable steps or suggestions, the comment leaves the authors without a clear path for improvement. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a question about whether most of the correct recitation cases were seen often during training. However, it does not specify which part of the paper this question pertains to, such as a specific section or analysis. Without explicit references or context, the authors cannot confidently determine which part of the paper is being addressed, making the comment weakly grounded. Additionally, the comment lacks specificity as it does not provide any details or suggestions on how to address the question. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point is a question seeking clarification about the nature of correct recitation cases seen during training. It does not contain any claims, opinions, or suggestions that require verification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review point raises a question about whether most of the correct recitation cases were seen often during training. While it identifies a potential area of interest, it lacks specificity and does not provide any actionable feedback or suggestions for improvement. The authors are left without clear guidance on how to address this question or what implications it might have for their work. As a result, the comment is barely helpful, as it offers minimal insight or direction for the authors to enhance their draft.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a significant gap in the performance of closed-book models compared to recent few-shot retrieval-augmented systems, specifically mentioning ATLAS [5] as a benchmark. It provides a specific example of ATLAS\\'s performance on NQ with 64 examples and a smaller model, contrasting it with the best performance in the paper, which is 32% with a larger model. However, the comment does not offer any explicit guidance or suggestions on how the authors might address this gap or improve their model\\'s performance. The action is implicit, as the authors can infer that they need to improve their model\\'s performance, but it lacks concrete details on how to do so. Therefore, the comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions the performance of closed-book models in the paper and compares it to recent few-shot retrieval-augmented systems, such as ATLAS [5]. This allows the authors to accurately identify the part of the paper being addressed. The comment is also specific because it provides a clear comparison of performance metrics, specifically mentioning the results of ATLAS on NQ with 64 examples and a smaller model, and contrasting it with the best number in the paper. This detailed comparison gives the authors a clear understanding of what needs to be addressed in terms of improving their model\\'s performance. Therefore, the comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the performance of closed-book models in the paper lags behind recent few-shot retrieval-augmented systems, specifically mentioning ATLAS [5] as a benchmark. It provides a specific example of ATLAS\\'s performance on NQ with 64 examples and a smaller model, contrasting it with the best performance in the paper, which is 32% with a larger model. This comparison is based on logical reasoning and provides a clear reference to external work, making the claim mostly verifiable. However, the comment could be strengthened by including more detailed comparisons or references to other relevant studies. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment highlights a significant gap in the performance of closed-book models compared to recent few-shot retrieval-augmented systems, specifically mentioning ATLAS [5] as a benchmark. It provides a specific example of ATLAS\\'s performance on NQ with 64 examples and a smaller model, contrasting it with the best performance in the paper, which is 32% with a larger model. This feedback is clear and actionable, as it identifies a critical area for improvement and provides a reference point for the authors to consider. However, the comment could be more helpful if it offered suggestions on how the authors might address this gap or improve their model\\'s performance. Overall, the comment is mostly helpful, as it directs the authors\\' attention to a key area for enhancement but could be more comprehensive with additional guidance.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review comment acknowledges the technical complexity of setting up retrieval but suggests that good retrieval APIs are available, as demonstrated in reference [6] without the need for extra language model fine-tuning. However, the comment does not provide explicit guidance or suggestions for the authors on how to utilize these APIs or integrate them into their work. The action is implicit, as the authors need to infer that they should explore using these APIs, but it lacks concrete details on how to do so. Therefore, the comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment refers to the use of retrieval APIs, which suggests that it is related to the discussion of retrieval methods in the paper. However, it does not specify which part of the paper this comment addresses, making it weakly grounded. The comment is specific in suggesting the use of retrieval APIs, as demonstrated in reference [6], and implies that extra language model fine-tuning may not be necessary. Therefore, this comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests that while setting up retrieval may be technically cumbersome, good retrieval APIs are available, as demonstrated in reference [6] without the need for extra language model fine-tuning. The claim is supported by the reference to [6], which provides evidence that such APIs exist and can be used effectively. However, the comment could be strengthened by providing more detailed examples or explanations of how these APIs were utilized in [6]. Overall, the claim is mostly verifiable, as it provides a basis for the suggestion but could benefit from additional details. Therefore, it aligns with a score of 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment acknowledges the technical complexity of setting up retrieval but suggests that good retrieval APIs are available, as demonstrated in reference [6] without the need for extra language model fine-tuning. This feedback is somewhat helpful as it provides an alternative approach to the authors, suggesting that they might consider using these APIs to simplify their setup. However, the comment lacks depth and does not offer specific guidance on how to integrate these APIs into their work or address the technical complexities mentioned. While it provides a useful insight, it could be more helpful with additional details or examples. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the performance of closed-book language models (LMs) and suggests that retrieving text from the web might be more practical and efficient. It also mentions that retrieval-augmented LMs often have fewer parameters, which could be a better method from an efficiency perspective. However, the comment does not provide explicit guidance or suggestions on how the authors should address this issue or improve their draft. The action is implicit and somewhat vague, as the authors are left to infer that they should consider the efficiency and practicality of their approach. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment discusses the performance of closed-book language models (LMs) and suggests that retrieving text from the web might be more practical and efficient. It also mentions that retrieval-augmented LMs often have fewer parameters, which could be a better method from an efficiency perspective. However, the comment does not specify which part of the paper this discussion is based on, making it difficult for the authors to identify the exact section that needs attention. The lack of explicit references to sections, figures, or specific elements of the paper results in weak grounding. Additionally, while the comment provides some specific points about the efficiency and practicality of different methods, it does not specify what needs to be addressed or improved in the paper. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point raises a claim about the performance of closed-book language models (LMs) and suggests that retrieving text from the web might be more practical and efficient. It also mentions that retrieval-augmented LMs often have fewer parameters, which could be a better method from an efficiency perspective. However, the comment lacks specific examples, data, or references to support these claims, making it difficult for the authors to fully understand and address the issues. The reasoning is somewhat logical but lacks detailed evidence or examples, making the claim somewhat verifiable. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment raises a valid point about the performance of closed-book language models (LMs) and suggests that retrieving text from the web might be more practical and efficient. It also mentions that retrieval-augmented LMs often have fewer parameters, which could be a better method from an efficiency perspective. This feedback is somewhat helpful as it prompts the authors to consider alternative approaches and their efficiency implications. However, the comment could be more helpful if it provided specific examples or detailed suggestions on how to implement or evaluate these alternatives. Overall, the feedback is somewhat helpful, as it identifies a potential area for improvement but lacks depth and actionable guidance.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the passage hints fine-tuning experiments, suggesting that the performance gains might be due to the dedicated fine-tuning on Wikipedia data, which could lead to overfitting. The reviewer asks a specific question about whether the passages were removed from the test set questions during fine-tuning. While the comment implies that the authors should clarify or address this concern, it does not explicitly instruct them to do so. The action is implicit and somewhat vague, as the authors need to infer that they should provide clarification or evidence regarding the fine-tuning process. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly addresses the \"passage hints fine-tuning experiments\" and raises a specific concern about the potential overfitting due to fine-tuning on Wikipedia data. It also asks a question about whether passages were removed from the test set questions during fine-tuning. This provides clear guidance on what needs to be clarified or addressed in the paper. Therefore, the comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a concern about the passage hints fine-tuning experiments, suggesting that the performance gains might be due to overfitting to Wikipedia data. The reviewer questions whether the passages were removed from the test set questions during fine-tuning, which is a logical and relevant inquiry. However, the comment lacks specific evidence or references to support the claim of overfitting or the impact of fine-tuning on Wikipedia data. While the question is reasonable, the lack of detailed justification or examples makes the claim somewhat verifiable. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment raises a concern about the passage hints fine-tuning experiments, suggesting that the performance gains might be due to overfitting to Wikipedia data. It questions whether the passages were removed from the test set questions during fine-tuning, which is a critical point for understanding the validity of the results. This feedback is somewhat helpful as it identifies a potential issue with the experimental setup and prompts the authors to clarify their methodology. However, it could be more helpful if it provided specific suggestions on how to address the concern or offered alternative approaches to mitigate overfitting. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the paper lacks sufficient experiments to justify its added complexity over vanilla LM-Recitation. It provides two explicit actions for the authors to consider: moving the experiments to the appendix or performing experiments on all three datasets/models to demonstrate the benefit. These suggestions are clear and concrete, giving the authors a direct path to address the issue. Therefore, the comment is highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment suggests that the paper lacks sufficient experiments to justify its added complexity over vanilla LM-Recitation. It implies that the experiments should be moved to the appendix or expanded to include all three datasets/models to demonstrate the benefit. However, the comment does not specify which part of the paper this issue pertains to, such as a particular section or experiment. This makes it weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. The comment is specific in suggesting what needs to be addressed, namely the need for more experiments. Therefore, this comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the paper lacks sufficient experiments to justify its added complexity over vanilla LM-Recitation. The reviewer suggests moving the experiments to the appendix or performing experiments on all three datasets/models to demonstrate the benefit. However, the comment does not provide specific examples or detailed reasoning to support the claim that the current experiments are insufficient. This lack of detailed justification makes the claim somewhat verifiable, as the authors would need to infer the necessity of additional experiments based on the general suggestion. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a potential weakness in the paper, specifically the lack of sufficient experiments to justify the added complexity of the proposed method over vanilla LM-Recitation. It provides two actionable suggestions: moving the experiments to the appendix or performing experiments on all three datasets/models to demonstrate the benefit. This feedback is clear and offers concrete steps for the authors to enhance the paper by providing more comprehensive experimental evidence. However, the comment could be more helpful if it included specific examples or detailed guidance on how to conduct these experiments. Overall, the comment is mostly helpful as it directs the authors toward a meaningful improvement in their draft.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point identifies a main weakness in the work related to the overall goal of fact extraction from language models. However, it does not provide any specific guidance or suggestions on how the authors might address this weakness or improve their draft. The comment lacks explicit or implicit actions, leaving the authors without a clear understanding of what steps to take to enhance their work. As a result, the comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment identifies a main weakness in the work related to the overall goal of fact extraction from language models. However, it does not specify which part of the paper this weakness is located in, nor does it provide details on what aspects of fact extraction are problematic. This makes it difficult for the authors to pinpoint the exact areas needing improvement. The comment lacks both grounding and specificity, leaving the authors without clear guidance on how to address the issue. Therefore, this comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the main weakness in the work relates to the overall goal of fact extraction from language models. However, it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without specific details or references, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a main weakness in the work related to the overall goal of fact extraction from language models. However, it does not provide any specific details or suggestions on how this weakness could be addressed or improved. Without actionable feedback or guidance, the authors are left without a clear understanding of what changes or enhancements are needed to strengthen their work. As a result, the comment is not helpful at all, as it lacks the necessary depth and specificity to guide the authors in improving their draft.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a discrepancy between the \"Oracle\" results from Table 1, where the LLM achieves only ~50% correct responses despite having perfect knowledge and a 100% consistent response. The comment acknowledges that the evaluation is limited, with only 41 \"head\" predicates, mostly involving well-known entities. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address the issue, improve the evaluation, or enhance the results. As a result, the authors are left without any actionable steps to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Table 1\" and \"Oracle\" results, allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it details the issue with the results, noting that despite perfect knowledge and a 100% consistent response, the LLM achieves only ~50% correct responses. The comment further specifies the limitation of the evaluation, mentioning the number of \"head\" predicates and the entities involved. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point highlights a discrepancy between the \"Oracle\" results from Table 1, where the LLM achieves only ~50% correct responses despite having perfect knowledge and a 100% consistent response. The comment acknowledges the limited evaluation, noting that it involves only 41 \"head\" predicates, mostly of well-known entities. This observation is based on logical reasoning and provides a clear explanation of the issue, making the claim mostly verifiable. However, the comment could be strengthened by providing more detailed analysis or references to similar evaluations, which would further support the claim. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment highlights a discrepancy in the results from Table 1, specifically the \"Oracle\" results, where the LLM achieves only ~50% correct responses despite having perfect knowledge and a 100% consistent response. This observation is thought-provoking and prompts the authors to consider the limitations of their evaluation, particularly the small number of \"head\" predicates (41) and the focus on well-known entities. However, the comment does not provide specific suggestions or guidance on how the authors might address this issue or improve their evaluation. While it identifies a potential weakness, it lacks actionable feedback, making it somewhat helpful. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the value of fact extraction from language models (LMs), suggesting that the work may not be worthwhile due to its focus on consistency rather than correctness. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this concern or what changes could be made to improve the perceived value of the work. As a result, the comment lacks actionability, leaving the authors without a clear path forward. Therefore, this comment is rated as a 1 on the actionability scale.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a question about the value of fact extraction from language models, suggesting that the work may not be worthwhile due to its focus on consistency rather than correctness. However, it does not specify which part of the paper this critique is based on, nor does it provide detailed guidance on how the authors might address this concern. The lack of specific references to sections or elements of the paper makes it difficult for the authors to pinpoint where changes are needed. As a result, the comment is not grounded and lacks specificity, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point questions the value of fact extraction from language models, suggesting that the work may not be worthwhile due to its focus on consistency rather than correctness. However, the comment does not provide any supporting evidence, reasoning, or references to substantiate this claim. Without additional context or justification, the authors may find it challenging to understand the basis of the critique or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': \"The review comment raises a question about the value of fact extraction from language models, suggesting that the work may not be worthwhile due to its focus on consistency rather than correctness. However, it does not provide any specific feedback or suggestions on how the authors might address this concern or improve their work. The comment lacks actionable guidance, leaving the authors without a clear understanding of what changes could be made to enhance the value of their research. As a result, the comment is not helpful at all, aligning with a score of 1.\", 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review comment suggests that the paper should delve deeper into the \"headroom question\" mentioned in a previous point. However, it does not provide any explicit guidance or suggestions on how the authors should approach this task. The action is implicit and vague, as the authors are left to infer that they need to explore the topic further without knowing what specific aspects to focus on or how to conduct this exploration. Without concrete details, the authors may struggle to determine how to implement this suggestion effectively. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment suggests that the paper should delve deeper into the \"headroom question\" mentioned in a previous point. However, it does not specify which previous point or section of the paper this refers to, making it difficult for the authors to identify the exact part that needs further exploration. Additionally, the comment lacks specificity regarding what aspects of the headroom question should be explored or how the authors might address it. Without clear grounding or specificity, the authors are left without a clear direction for improvement. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point suggests that the paper should delve deeper into the \"headroom question\" mentioned in a previous point. However, it does not provide any specific details or references to support this claim, nor does it explain why a deeper exploration of this question is necessary. Without additional context or justification, the authors may find it challenging to understand the basis of the suggestion or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests that the paper should explore the \"headroom question\" mentioned in a previous point more deeply. However, it lacks specificity and does not provide any guidance on how the authors might address this issue or what aspects of the headroom question need further exploration. Without detailed suggestions or examples, the comment offers limited value to the authors in terms of actionable feedback. Therefore, it is barely helpful, as it identifies a potential area for improvement but does not provide enough detail to be fully beneficial.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests conducting a sampled qualitative evaluation of errors of the Oracle model in the ID cases. While it implies an action, it does not provide explicit guidance on how to conduct this evaluation or what specific aspects should be considered. The authors can infer that they need to explore the errors of the Oracle model in ID cases, but the comment lacks concrete details on how to implement this suggestion. Therefore, the comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment suggests conducting a sampled qualitative evaluation of errors of the Oracle model in the ID cases. However, it does not specify which part of the paper this suggestion pertains to, such as a specific section or analysis. This lack of explicit reference makes it difficult for the authors to pinpoint the exact area needing attention. Additionally, the comment does not provide detailed guidance on what aspects of the Oracle model\\'s errors should be evaluated or how to conduct the qualitative evaluation. Therefore, the comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point suggests conducting a sampled qualitative evaluation of errors of the Oracle model in the ID cases. However, it does not provide any reasoning, evidence, or references to support why such an evaluation is necessary or beneficial. The comment lacks specific details or justification for the suggestion, making it difficult for the authors to understand the rationale behind the recommendation. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests conducting a sampled qualitative evaluation of errors of the Oracle model in the ID cases. While it identifies a potential area for improvement, it lacks specificity and does not provide detailed guidance on how to conduct such an evaluation or what aspects should be focused on. The comment does not offer actionable steps or suggestions for the authors to enhance their draft, leaving them with limited insight into how to address the issue. Therefore, the comment is barely helpful, as it identifies a potential area for improvement but lacks depth and clarity.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the source of errors in the model, specifically whether they are due to unseen triples during training or issues with model capacity. It suggests that a larger model version (10x) might be able to recall prompted facts. While the comment implies that the authors should investigate these potential sources of error, it does not provide explicit instructions or concrete steps on how to address the issue. The action is implicit and somewhat vague, as the authors need to infer that they should explore these possibilities and consider scaling up the model. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises a question about the source of errors in the model, specifically whether they are due to unseen triples during training or issues with model capacity. It suggests that a larger model version (10x) might be able to recall prompted facts. However, the comment does not specify which part of the paper this question pertains to, such as a particular section or experiment. Without explicit references or context, the authors cannot confidently determine where to address this issue. The comment is specific in its questioning of the error sources and potential solutions but lacks grounding, making it weakly grounded and specific. Therefore, this comment aligns with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises a question about the source of errors in the model, specifically whether they are due to unseen triples during training or issues with model capacity. It suggests that a larger model version (10x) might be able to recall prompted facts. However, the comment does not provide any evidence, reasoning, or references to support the claim that a larger model version would be effective. The lack of supporting information makes it difficult for the authors to understand the basis of the suggestion, rendering the claim unverifiable. Therefore, the comment is rated as 1: Unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a question about the source of errors in the model, specifically whether they are due to unseen triples during training or issues with model capacity. It suggests that a larger model version (10x) might be able to recall prompted facts, which could help address the errors. While the comment identifies a potential area for investigation, it lacks specific guidance or actionable steps for the authors to take. It does not provide detailed suggestions on how to test the hypothesis or what specific experiments might be conducted to validate the idea. As a result, the comment offers some insight but is incomplete, making it somewhat helpful. Therefore, it aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point explicitly identifies Section 4.1 as the most confusing part of the paper and highlights specific issues with the section, such as the lack of clarity regarding the data used for training and evaluation. It also mentions specific references and examples that are mentioned in the section, suggesting that these need to be more precisely described. The comment provides clear guidance on what needs to be addressed, which is the precise description of the training, evaluation, and test datasets. This feedback is explicit and concrete, giving the authors a direct and actionable path to improve their draft. Therefore, the comment is highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Section 4.1,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies the issues with the section, such as the lack of clarity regarding the data used for training and evaluation, and suggests that the section needs to be rewritten with more precise descriptions of the datasets. The comment provides detailed guidance on what needs to be addressed, making it fully grounded and specific. Therefore, this comment aligns with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that Section 4.1 is confusing and lacks clarity regarding the data used for training and evaluation. The reviewer provides specific examples of references mentioned in the section, such as LAMA\u2019s T-REX, LPAQA, ParaRel, and BERT lexical replacements, as well as data from \"Shin et al, 2020.\" This provides a clear basis for the claim, as it highlights the specific sources and references that contribute to the confusion. However, the comment could be strengthened by providing more detailed reasoning or examples of how these references are unclear or insufficient. Overall, the claim is mostly verifiable, as it offers a solid foundation for the critique but could benefit from additional explanation or examples. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment identifies a specific section of the paper, Section 4.1, as confusing and highlights two main issues: the lack of clarity regarding the data used for training and evaluation, and the need for more precise descriptions of the datasets. The reviewer provides specific examples of references mentioned in the section, such as LAMA\u2019s T-REX, LPAQA, ParaRel, and BERT lexical replacements, as well as data from \"Shin et al, 2020.\" This feedback is actionable as it directs the authors to clarify and improve the section by providing more detailed information about the datasets used. However, the comment could be more helpful if it offered suggestions on how to reorganize or present the information to enhance clarity. Overall, the comment is mostly helpful as it provides clear guidance on areas needing improvement, but it could be more comprehensive with additional suggestions. Therefore, it aligns with a score of 4.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point explicitly encourages the authors to release the exact datasets and splits to facilitate reproducibility and improvement of the work. It also emphasizes the importance of providing a precise description of how the data was constructed. This feedback is clear and direct, providing the authors with specific actions to take to enhance the transparency and reproducibility of their work. The comment is highly actionable as it provides concrete steps for the authors to follow, ensuring they know exactly what needs to be done to improve their draft.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment suggests releasing the exact datasets and splits to allow others to reproduce or improve upon the work. It also emphasizes the importance of a precise description of how the data was constructed. However, the comment does not specify which part of the paper this suggestion pertains to, such as the methodology or results sections. This lack of explicit reference makes it difficult for the authors to pinpoint the exact part of the paper that needs revision. While the comment is specific in its suggestions, it is not grounded, as it does not clearly identify the section being addressed. Therefore, this comment is not grounded but specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests that releasing the exact datasets and splits would allow others to reproduce or improve upon the work. It also emphasizes the importance of a precise description of how the data was constructed. However, the comment does not provide specific examples or references to support the claim that a detailed description of the data construction is necessary. While the suggestion is logical, the lack of detailed justification or examples makes the claim somewhat verifiable. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment provides a clear and actionable suggestion for improving the paper by encouraging the authors to release the exact datasets and splits. This would allow others to reproduce and potentially improve upon the work, enhancing its reproducibility and impact. Additionally, the comment emphasizes the importance of a precise description of how the data was constructed, which is crucial for understanding the methodology and results. While the comment is specific and actionable, it could be more helpful if it provided additional guidance on how to present this information or what specific details should be included in the description. Overall, the comment is mostly helpful as it offers valuable feedback for improving the paper\\'s transparency and reproducibility.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a lack of sufficiency in the description of the MoE and Oracle layers, specifically regarding the outputs presented to the first layer of the model. However, it does not provide explicit guidance or suggestions on how the authors should address this issue. The comment implies that the authors need to clarify or expand their description, but it lacks concrete details on what specific information should be added or how it should be presented. As a result, the action is implicit and vague, making it barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"the MoE and Oracle layers,\" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it clearly specifies the issue with the description, namely that it is insufficient to determine the outputs presented to the first layer of the model. This provides clear guidance on what needs to be addressed to improve the paper. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the description of the MoE and Oracle layers is insufficient to determine the outputs presented to the first layer of the model. However, the comment does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a specific issue with the description of the MoE and Oracle layers, noting that it is insufficient to determine the outputs presented to the first layer of the model. This feedback is clear and actionable, as it directs the authors to clarify or expand their description to ensure that the outputs are adequately explained. However, the comment could be more helpful if it provided suggestions on how to improve the description or what specific details should be included. Despite this, the feedback is mostly helpful as it guides the authors toward a necessary improvement in their draft.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the depiction in Figure 2, suggesting that the entire sequence is rewritten using fixed-length learned embeddings and possibly preserving subject or MASK embeddings. However, it does not provide any explicit or implicit action for the authors to take. The comment lacks guidance on how the authors should address this observation or what changes they should make to their draft. As a result, the authors are left without a clear understanding of what steps to take to improve their work. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Figure 2,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it raises a question about the depiction in Figure 2, specifically regarding the use of fixed-length learned embeddings and the preservation of subject or MASK embeddings. This provides clear guidance on what aspect of the figure needs further clarification or explanation. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point is a question about the interpretation of Figure 2, specifically regarding the depiction of sequence rewriting using fixed-length learned embeddings. It does not contain any claims, opinions, or suggestions that require verification. It is purely descriptive and seeks clarification, fitting the classification of \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment raises a question about the interpretation of Figure 2, specifically regarding the depiction of sequence rewriting using fixed-length learned embeddings and the potential preservation of subject or MASK embeddings. While it identifies a potential area of confusion, it does not provide any actionable feedback or suggestions for how the authors might clarify or address this issue in their draft. The comment lacks depth and does not offer any guidance on how the authors might improve their work based on this observation. Therefore, it is barely helpful, aligning with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the use of embeddings in creating continuous prompts, specifically asking how they are used in sub-section 4.2. It does not provide explicit instructions or suggestions for the authors to address this issue. The action is implicit, as the authors need to infer that they should clarify the method of using embeddings in their draft. However, the comment lacks concrete guidance on how to provide this clarification, making it somewhat actionable. The authors know they need to address the question but may not be entirely sure how to do so without further guidance.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"sub-section 4.2,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies the issue by asking about the method of using embeddings to create continuous prompts, including questions about whether they are prepended, appended, or used to rewrite the original inputs, and whether MASK or subject tokens are copied. This provides clear guidance on what needs to be clarified or addressed in the paper. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a question about the use of embeddings in creating continuous prompts, specifically asking how they are used in sub-section 4.2. It does not make a claim or provide an opinion but rather seeks clarification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment identifies a specific gap in the paper regarding the description of how embeddings are used to create continuous prompts. It raises questions about whether the embeddings are prepended, appended, or used to rewrite the original inputs, and whether MASK or subject tokens are copied. This feedback is clear and actionable, as it prompts the authors to clarify this critical aspect of their methodology. By addressing these questions, the authors can provide a more comprehensive understanding of their approach, which is essential for the readers to fully grasp the paper\\'s contributions. However, the comment could be more helpful if it suggested specific ways to address these questions or provided examples of how similar approaches have been described in other works. Overall, the comment is mostly helpful as it directs the authors to a specific area needing clarification, but it could be more comprehensive with additional guidance.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a limitation of the LAMA benchmarks, noting that they were constructed for BERT-style single token prediction and have stripped down the original datasets. The reviewer suggests evaluating on a larger sequence-to-sequence model like BART or T5, which are available as HuggingFace models. This comment provides a clear and explicit action for the authors to consider, suggesting a specific direction for improving their work. The suggestion is concrete, as it specifies the models to evaluate and provides references for further information. Therefore, this comment is highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions the \"LAMA benchmarks\" and references specific datasets and models, allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it details the issue with the LAMA benchmarks, noting that they were constructed for BERT-style single token prediction and have stripped down the original datasets. The comment further suggests evaluating on a larger sequence-to-sequence model like BART or T5, providing specific references for these models. This level of detail and specificity makes the comment fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the LAMA benchmarks have a limitation due to their construction for BERT-style single token prediction, which has resulted in the stripping down of original datasets. The reviewer supports this claim by referencing the original version of T-Rex, which contains over 600 unique predicates compared to the 41 in LAMA. Additionally, the reviewer suggests evaluating on a larger sequence-to-sequence model like BART or T5, providing references to these models. This provides a clear and logical reasoning for the claim, making it mostly verifiable. However, the comment could be strengthened by providing more detailed comparisons or examples of how the current benchmarks are insufficient. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment identifies a limitation in the LAMA benchmarks, noting that they were constructed for BERT-style single token prediction and have stripped down the original datasets. This observation is valuable as it highlights a potential weakness in the current evaluation framework. The reviewer suggests a potential solution by recommending the use of a more comprehensive version of the benchmarks, such as evaluating on a larger sequence-to-sequence model like BART or T5. This feedback is actionable and provides a clear direction for the authors to consider, which is highly beneficial for improving the draft. However, the comment could be more helpful if it included specific guidance on how to implement this suggestion or why it would be beneficial. Overall, the comment is mostly helpful, aligning with a score of 4.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that training and evaluation could be done relatively cheaply for larger models with proper decoders, given that the work leverages frozen LLMs. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to implement this suggestion or what specific steps should be considered to make the training and evaluation more cost-effective. As a result, the authors are left without a clear understanding of what changes to make or how to address this point. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment suggests that training and evaluation could be done relatively cheaply for larger models with proper decoders, given that the work leverages frozen LLMs. However, it does not specify which part of the paper this observation pertains to, nor does it provide detailed guidance on how to implement this suggestion. The authors cannot confidently determine which section of the paper this comment addresses, making it weakly grounded. Additionally, the comment lacks specificity regarding what needs to be addressed or improved. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point suggests that training and evaluation could be done relatively cheaply for larger models with proper decoders, given the use of frozen LLMs. However, the comment lacks specific reasoning, examples, or references to support this claim. Without detailed justification or evidence, the authors may find it challenging to understand the basis of this suggestion or how to implement it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': \"The review comment suggests that training and evaluation could be done relatively cheaply for larger models with proper decoders, given the use of frozen LLMs. However, it does not provide any specific guidance or suggestions on how the authors might implement this cost-effective approach or what benefits it could bring to their work. The comment lacks actionable feedback and does not offer any insights into potential improvements or enhancements. As a result, it is not helpful to the authors in terms of improving their draft.\", 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the use of a weighted combination in the MoE solution, suggesting that the authors might benefit from exploring this approach. It provides a logical reasoning for why a weighted combination could be beneficial, noting that the relation classifier trained with cross-entropy softmax loss would result in most weights being close to one-hot, similar to the top-1 prediction. However, the comment does not explicitly instruct the authors to conduct this experiment or provide specific guidance on how to implement it. While the suggestion is clear, the lack of explicit direction makes the action somewhat vague. Therefore, the comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly addresses the \"MoE solution\" and the use of a weighted combination versus the top-1 predicted relation. This allows the authors to accurately identify the part of the paper being discussed. The comment is also specific because it suggests an alternative approach (using a weighted combination) and provides a logical reasoning for why it might be beneficial. The authors are given clear guidance on what to consider and why, making this comment fully grounded and specific. Therefore, it aligns with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a question about the use of a weighted combination in the MoE solution, suggesting that it might be beneficial over the current top-1 approach. The reviewer provides a logical reasoning by explaining that when the relation classifier is trained with cross-entropy softmax loss, most weights will be close to one-hot, similar to the top-1 prediction, except when the model is uncertain. This reasoning is clear and provides a basis for the suggestion. However, the comment does not include specific examples or references to support the claim fully, making it mostly verifiable. The authors would need to consider this suggestion and potentially conduct experiments to validate it, which aligns with a score of 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment raises a question about the use of a weighted combination in the MoE solution, suggesting that it might be beneficial over the current top-1 approach. The reviewer provides a logical reasoning for this suggestion, noting that when the relation classifier is trained with cross-entropy softmax loss, most weights will be close to one-hot, similar to the top-1 prediction, except when the model is uncertain. This feedback is clear and actionable, as it prompts the authors to consider an alternative approach that could potentially improve their model. However, the comment could be more helpful if it included specific guidance on how to implement or test this suggestion. Overall, the comment is mostly helpful, as it provides a valuable insight that could lead to improvements in the draft, but it could be more comprehensive with additional guidance.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review comment raises a question about the feasibility of decoding continuous prompt embeddings into the existing vocabulary, suggesting a potential approach using a greedy decoding strategy. It also asks if the authors have tried inspecting the continuous prompts in this manner. While the comment provides a specific idea for exploration, it does not explicitly instruct the authors to conduct this analysis or provide detailed guidance on how to implement it. The action is implicit and somewhat vague, as the authors need to infer that they should consider this approach and potentially conduct the analysis. Therefore, the comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises a question about the feasibility of decoding continuous prompt embeddings into the existing vocabulary and suggests a potential approach using a greedy decoding strategy. It also asks if the authors have tried inspecting the continuous prompts in this way. However, the comment does not explicitly mention which part of the paper discusses the continuous prompt embeddings, making it weakly grounded. The authors can infer that it relates to sections discussing the LLM or prompt embeddings, but this is not precise. The comment is specific in suggesting a method for decoding and questioning the informativeness of the prompts, but it lacks grounding. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises a question about the feasibility of decoding continuous prompt embeddings into the existing vocabulary, suggesting a potential approach using a greedy decoding strategy. However, it does not provide any evidence, reasoning, or references to support the claim that this approach would be beneficial or informative. The comment lacks specific examples or detailed justification, making it difficult for the authors to understand the basis of the suggestion. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a question about the feasibility of decoding continuous prompt embeddings into the existing vocabulary, suggesting a potential approach using a greedy decoding strategy. It also asks if the authors have tried inspecting the continuous prompts in this way and whether the output is informative or if the prompts are modeling purely latent variables. This feedback is somewhat helpful as it provides a new perspective on the work and encourages the authors to explore a specific aspect of their methodology. However, it lacks depth and does not offer detailed guidance on how to implement the suggested approach or what specific insights might be gained from it. Therefore, the comment is somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the lack of clarity regarding the practical implications of the work. However, it does not provide any explicit or implicit actions for the authors to take to address this issue. There is no guidance on how to clarify the practical implications or what specific aspects of the work need to be elaborated upon. As a result, the authors are left without a clear understanding of what steps to take to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a concern about the lack of clarity regarding the practical implications of the work. However, it does not specify which part of the paper this issue pertains to, nor does it provide details on what aspects of the practical implications are unclear. This makes it difficult for the authors to identify the exact sections that need attention or improvement. As a result, the comment is not grounded and lacks specificity, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the practical implications of the work are unclear. However, it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without additional context or explanation, the authors may find it challenging to understand the basis of the concern or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment identifies a lack of clarity regarding the practical implications of the work, which is an important aspect for readers to understand the significance and relevance of the research. However, it does not provide specific guidance or suggestions on how the authors might address this issue or clarify the practical implications. Without actionable feedback or examples, the comment offers limited value to the authors in terms of improving their draft. Therefore, it is rated as barely helpful, aligning with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point provides a statement about the current state of large language models and their relationship with dataset growth. However, it does not offer any explicit or implicit actions for the authors to take in response to this information. There is no guidance on how the authors might address this issue or incorporate it into their work. As a result, the comment lacks actionability, leaving the authors without a clear path forward. Therefore, this comment is rated as unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment provides a general statement about the current state of large language models and their relationship with dataset growth. However, it does not specify which part of the paper this statement pertains to, nor does it offer specific guidance on how the authors might address this issue. The lack of grounding makes it difficult for the authors to identify the exact section that needs attention. Additionally, the comment is not specific about what needs to be addressed or how the authors might improve their work in response to this information. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point makes a claim about the current state of large language models and their relationship with dataset growth. However, it lacks specific evidence, examples, or references to support this claim. The statement is based on a general observation and does not provide detailed reasoning or justification, making it difficult for the authors to understand the basis of the claim. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review point provides a general observation about the current state of large language models and their relationship with dataset growth. However, it does not offer any specific suggestions or guidance for the authors to improve their work. The comment lacks depth and does not provide actionable feedback, leaving the authors without a clear understanding of how to address the issue or incorporate this information into their draft. As a result, the comment is not helpful at all, aligning with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests a simple remedy to address the issue of pretraining loss not being a reliable indicator of downstream performance. It recommends evaluating models on downstream tasks during pretraining and comparing them accordingly. However, the comment does not provide explicit guidance on how to implement this suggestion, such as which specific downstream tasks to evaluate or how to compare the models. The action is implicit and somewhat vague, as the authors can infer the need to evaluate and compare models but may not know the exact steps to take. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses the issue of pretraining loss not being a reliable indicator of downstream performance and suggests a remedy by evaluating models on downstream tasks during pretraining. However, it does not specify which part of the paper this critique is based on, making it difficult for the authors to pinpoint the exact section that needs attention. The suggestion to evaluate on downstream tasks is specific, but without explicit grounding, the authors may struggle to identify where to make these changes. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests that evaluating models on downstream tasks during pretraining could be a simple remedy for the issue of pretraining loss not being a reliable indicator of downstream performance. However, the comment lacks specific examples or references to support this claim, making it somewhat verifiable. The suggestion is logical and based on common knowledge in the field, but the absence of detailed evidence or examples limits the reader\\'s ability to fully understand and verify the claim. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment acknowledges the paper\\'s point about pretraining loss not being a reliable indicator of downstream performance. It suggests a potential remedy by recommending that models be evaluated on downstream tasks during pretraining and compared accordingly. This feedback is somewhat helpful as it provides a specific suggestion for improvement, which could be beneficial for the authors. However, the comment lacks depth and does not offer detailed guidance on how to implement this suggestion or why it would be effective. Additionally, it does not address other potential issues or areas for improvement in the paper. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the paper\\'s ability to demonstrate the potential of the insights gained to lead to better downstream performance on real datasets through \"flatness regularization.\" However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this concern or what specific steps should be taken to demonstrate this potential. As a result, the authors are left without a clear understanding of what changes or additions are needed to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a concern about the paper\\'s ability to demonstrate the potential of the insights gained to lead to better downstream performance on real datasets through \"flatness regularization.\" However, it does not specify which part of the paper this concern relates to, such as a particular section or experiment. Without explicit references or clear indications of where this issue is discussed, the authors cannot confidently determine which part of the paper needs attention. Additionally, the comment lacks specificity regarding what aspects of \"flatness regularization\" or downstream performance are not adequately addressed. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the paper does not demonstrate whether the insights gained can lead to additional \"flatness regularization\" that induces better downstream performance on real datasets. However, the comment lacks specific examples, references, or detailed reasoning to support this claim. Without additional context or evidence, the authors may find it challenging to understand the basis of the claim or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a concern about the paper\\'s ability to demonstrate the potential of the insights gained to lead to better downstream performance on real datasets through \"flatness regularization.\" However, it does not provide any specific suggestions or guidance on how the authors might address this concern or what additional experiments or analyses could be conducted to demonstrate this potential. Without actionable feedback or detailed advice, the comment offers limited value to the authors in terms of improving their draft. Therefore, it is rated as barely helpful.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review comment suggests that, given the empirical motivation of the paper, some validation on real data would be desirable. However, it does not explicitly instruct the authors to conduct such validation or provide specific guidance on how to implement it. The action is implicit and somewhat vague, as the authors can infer that they should consider validating their work on real data but are not given concrete steps on how to do so. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment suggests that some validation on real data is desirable, given the empirical motivation of the paper. However, it does not specify which part of the paper this suggestion pertains to, such as specific sections or experiments. This lack of explicit reference makes it difficult for the authors to pinpoint the exact area needing attention. Additionally, the comment does not provide detailed guidance on what aspects of the paper should be validated or how to conduct the validation. Therefore, the comment is not grounded and lacks specificity, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point suggests that, given the empirical motivation of the paper, some validation on real data is desirable. However, it does not provide specific reasoning or evidence to support why this validation is necessary or how it would enhance the paper. The comment lacks detailed justification or references to substantiate the claim, making it difficult for the authors to understand the basis of the suggestion. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment acknowledges the challenges of conducting large-scale experiments but suggests that some validation on real data would be desirable, given the empirical motivation of the paper. While the comment identifies a potential area for improvement, it lacks specificity and does not provide detailed guidance on how the authors might address this issue. It does not offer actionable suggestions or examples of how to validate the work on real data, leaving the authors with a general idea but no clear path forward. Therefore, the comment is barely helpful, as it provides some insight but lacks depth and actionable feedback.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point identifies a weakness in the paper, specifically the lack of computational comparison with prior multitask prompted fine-tuning approaches. However, it does not provide any explicit or implicit guidance on how the authors should address this issue. There is no suggestion on what specific comparisons should be made, how to conduct them, or what metrics to use. Without actionable advice, the authors are left without a clear path to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment identifies a specific weakness in the paper, namely the lack of computational comparison with prior multitask prompted fine-tuning approaches. However, it does not specify which part of the paper this issue pertains to, such as a particular section or experiment. Without explicit references to specific sections or elements, the authors may find it challenging to pinpoint the exact area needing improvement. Additionally, the comment lacks specificity regarding what aspects of the computational comparison are missing or how it could be improved. Therefore, the comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the paper lacks a computational comparison with prior multitask prompted fine-tuning approaches. However, it does not provide any supporting evidence, examples, or references to substantiate this claim. Without specific details or comparisons, the authors may find it challenging to understand the basis of the critique or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a specific weakness in the paper, namely the lack of computational comparison with prior multitask prompted fine-tuning approaches. This is a clear and actionable feedback that highlights an area where the authors could improve their draft by providing a more comprehensive comparison. However, the comment does not offer suggestions on how to conduct such a comparison or what specific metrics or approaches should be used. While it points out a significant gap in the paper, it lacks depth and guidance, making it somewhat helpful. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point expresses a concern about the computational requirements of the proposed method, specifically mentioning the fine-tuning stage and the need to train the MoE layer. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address this concern or suggestions for potential solutions. As a result, the authors are left without any actionable steps to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a concern about the computational requirements of the proposed method, specifically mentioning the fine-tuning stage and the need to train the MoE layer. However, it does not specify which part of the paper this concern is based on, such as a particular section or figure. This makes it difficult for the authors to pinpoint the exact part of the paper that needs attention. Additionally, the comment lacks specificity regarding what aspects of the computation are problematic or how they could be addressed. Therefore, the comment is not grounded and lacks specificity, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point expresses a concern about the computational requirements of the proposed method, specifically mentioning the fine-tuning stage and the need to train the MoE layer. However, it does not provide any supporting evidence, examples, or references to substantiate this claim. The comment lacks detailed reasoning or specific instances to justify why the method might require more computation. As a result, the claim is unverifiable, as it does not offer sufficient information for the authors to understand or address the concern. Therefore, the comment is rated as 1: Unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a concern about the computational requirements of the proposed method, specifically highlighting the need for fine-tuning the MoE layer and the underlying language model. However, it does not provide any specific suggestions or guidance on how the authors might address this issue or mitigate the computational complexity. The comment identifies a potential weakness but lacks actionable feedback, leaving the authors without a clear path for improvement. Therefore, the comment is barely helpful, aligning with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises questions about the limitations of the RoT system, specifically whether it can learn to backtrack and how it handles problems that require modifying earlier parts of the answer based on new information. While the comment identifies areas of concern, it does not provide explicit instructions or suggestions for the authors to address these issues. The authors are left to infer that they should explore these limitations and provide more information about the capabilities of RoT. However, the lack of concrete guidance or specific actions makes the comment somewhat actionable, as the authors know they need to investigate these aspects but are not provided with detailed steps on how to do so.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises questions about the limitations of the RoT system, specifically regarding its ability to learn backtrack and handle problems that require modifying earlier parts of the answer based on new information. However, it does not explicitly mention which part of the paper these questions relate to, making it weakly grounded. The comment is specific in detailing the areas of concern, such as the need for human inputs to design subproblems and the limitations of RoT in solving certain types of problems. Therefore, the comment is weakly grounded but specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises questions about the limitations of the RoT system, specifically regarding its ability to learn backtrack and handle problems that require modifying earlier parts of the answer based on new information. However, the comment does not provide any supporting evidence, reasoning, or references to substantiate these claims. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the questions and how to address them. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises important questions about the limitations of the RoT system, specifically regarding its ability to learn backtrack and handle problems that require modifying earlier parts of the answer based on new information. It prompts the authors to consider the need for human inputs to design proper subproblems and to explore the limitations of the problems that RoT can solve. While the comment identifies areas for further investigation, it lacks specific suggestions or guidance on how the authors might address these questions or improve their work. The feedback is somewhat helpful as it points out potential areas of improvement, but it could be more actionable with additional details or examples. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point poses a question about the ability of RoT to learn problems with subproblems of different structures, which is a critical aspect of recursion. However, it does not provide any explicit or implicit actions for the authors to take. The question lacks guidance on how the authors might address this concern or what specific aspects of their draft need revision. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a question about the ability of RoT to learn problems with subproblems of different structures, which is relevant to the paper\\'s discussion on recursion. However, it does not specify which part of the paper this question pertains to, making it weakly grounded. The comment is specific in its inquiry about the structure of subproblems, but without a clear reference to a specific section or part of the paper, the authors may find it challenging to address the issue directly. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point is a question seeking clarification about the capabilities of RoT in learning problems with subproblems of different structures. It does not contain any claims, opinions, or suggestions that require verification. It is purely a request for information, making it a normal statement. Therefore, the correct label is \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment raises a question about the ability of RoT to learn problems with subproblems of different structures, which is a critical aspect of recursion. However, it does not provide any specific guidance or suggestions for the authors to address this concern or improve their draft. The comment lacks actionable feedback, leaving the authors without a clear understanding of how to enhance their work in response to this question. As a result, the comment is not helpful at all, aligning with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the performance of RoT on specific types of problems, such as NP-hard algorithmic problems without a divide-and-conquer structure, like the Traveling Salesman Problem (TSP). While it implies that the authors should consider evaluating RoT on these types of problems, the comment does not explicitly instruct them to do so. The action is implicit and somewhat vague, as it lacks specific guidance on how to conduct these evaluations or what aspects to focus on. Therefore, the comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment raises a question about the performance of RoT on specific types of problems, such as NP-hard algorithmic problems without a divide-and-conquer structure, like the Traveling Salesman Problem (TSP). However, it does not specify which part of the paper this question pertains to, making it difficult for the authors to identify the exact section that needs attention. The comment is specific in its request for information about performance on these types of problems, but it lacks grounding as it does not reference a specific section or part of the paper. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point is a question seeking clarification on the performance of RoT on specific types of problems, such as NP-hard algorithmic problems without a divide-and-conquer structure, like the Traveling Salesman Problem (TSP). It does not contain any claims, opinions, or suggestions that require verification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment raises a question about the performance of RoT on specific types of problems, such as NP-hard algorithmic problems without a divide-and-conquer structure, like the Traveling Salesman Problem (TSP). While it identifies a potential area for evaluation, it lacks specific guidance or suggestions on how the authors might address this issue or what aspects of performance should be considered. The comment provides some insight into a potential gap in the evaluation of RoT but does not offer actionable advice or detailed feedback for improvement. Therefore, it is somewhat helpful, as it prompts the authors to consider additional evaluations but does not fully support them in making those evaluations.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about how to design proper subproblems for using RoT (Reinforcement of Training) to train models. While it implies that the authors should consider this aspect, it does not provide explicit guidance or suggestions on how to address this issue. The action is implicit and somewhat vague, as the authors are left to infer that they need to explore ways to design subproblems for RoT training. However, the comment does not offer concrete steps or examples, making it difficult for the authors to know exactly how to proceed. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises a question about how to design proper subproblems for using RoT to train models. However, it does not specify which part of the paper this question pertains to, such as a specific section or methodology discussed. Without explicit references or context, the authors cannot confidently determine which part of the paper the comment addresses. Additionally, the comment lacks specificity regarding what aspects of subproblem design are being questioned or how they relate to the use of RoT. Therefore, this comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point is a question seeking guidance on how to design proper subproblems for using RoT to train models. It does not contain any claims, opinions, or suggestions that require verification. It is purely a request for clarification or advice, fitting the classification of \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment raises a question about how to design proper subproblems for using RoT (Reinforcement of Training) to train models. While it identifies a potential area for improvement, it lacks specificity and does not provide any guidance or suggestions on how the authors might address this issue. Without actionable advice or examples, the comment offers limited value to the authors in terms of improving their draft. Therefore, it is barely helpful, aligning with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises questions about the criteria and structures of the subproblems for RoT, implying that the authors should clarify these aspects in their draft. However, the comment does not provide explicit guidance on how to address these questions or what specific information should be included. The action is implicit and somewhat vague, as the authors can infer that they need to provide more details but are not given concrete steps on how to do so. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises questions about the criteria and structures of the subproblems for RoT, indicating that the authors should clarify these aspects. However, it does not specify which part of the paper these questions relate to, making it difficult for the authors to pinpoint the exact section that needs attention. The comment is specific in its request for clarification but lacks grounding, as it does not provide a clear reference to the part of the paper being addressed. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point consists of questions seeking clarification about the criteria and structures of the subproblems for RoT. It does not contain any claims, opinions, or suggestions that require verification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment raises important questions about the criteria and structures of the subproblems for RoT, which are crucial for understanding the methodology and its application. By asking for clarification on these aspects, the comment prompts the authors to provide more detailed information that could enhance the comprehensibility and rigor of their work. However, the comment lacks specific suggestions or guidance on how the authors might address these questions or what additional details could be included. While it identifies a potential area for improvement, it does not provide actionable feedback, making it somewhat helpful. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point poses a question about the existence of systematic approaches to designing subproblems for reasoning tasks. However, it does not provide any explicit or implicit actions for the authors to take. It lacks guidance on what the authors should do with this information, such as whether they should investigate existing approaches or develop new ones. Without any actionable steps or suggestions, the comment leaves the authors without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a question about the existence of systematic approaches to designing subproblems for reasoning tasks. However, it does not specify which part of the paper this question pertains to, nor does it provide any guidance on what aspects of the paper might be affected by this inquiry. Without explicit references to sections, tables, or figures, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what needs to be addressed or improved. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point is a question seeking clarification on the existence of systematic approaches to designing subproblems for reasoning tasks. It does not contain any claims, opinions, or suggestions that require verification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review point raises a question about the existence of systematic approaches to designing subproblems for reasoning tasks. While it highlights an area that could be explored or clarified in the paper, it does not provide any specific guidance or suggestions for improvement. The authors are left without actionable feedback on how to address this question or incorporate it into their work. As a result, the comment is not particularly helpful, as it lacks depth and does not offer a clear path for the authors to enhance their draft. Therefore, it aligns with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights that the description of the work is difficult to follow due to the lack of formal definitions and sloppy writing. It specifically mentions the issue with the term \"casting a set of instances (the training data) into the representation space\" and the use of \"c_0\" before it is defined. However, the comment does not provide explicit guidance on how to improve the clarity or suggest specific actions for the authors to take. The feedback is implicit and somewhat vague, as it identifies areas for improvement but does not offer concrete steps for addressing them. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions the issue with the description being \"very hard to follow\" and provides specific examples, such as the lack of formal definitions and the use of \"c_0\" before it is defined. This allows the authors to accurately identify the parts of the paper that need attention. The comment is also specific because it details what is wrong with the description, providing clear guidance on how to improve it. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the description of the work is difficult to follow due to the lack of formal definitions and sloppy writing. It provides specific examples, such as the use of \"c_0\" before it is defined and the phrase \"casting a set of instances (the training data) into the representation space,\" which supports the claim. However, the comment could be strengthened by providing more detailed examples or references to specific sections where these issues occur. Overall, the claim is mostly verifiable, as it offers some justification but could benefit from additional evidence or examples to fully substantiate the critique. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment acknowledges the novelty of the work but highlights significant issues with the description, making it difficult to follow. It points out the lack of formal definitions, which hinders understanding, particularly in the context of computing thrust. The comment also critiques the writing as \"sloppy,\" citing specific examples such as the use of \"c_0\" before it is defined. This feedback is valuable as it identifies specific areas where the authors need to improve the clarity and precision of their writing. However, the comment could be more helpful if it provided suggestions on how to address these issues, such as recommending the inclusion of formal definitions or offering guidance on rephrasing the text. Overall, the comment is mostly helpful as it directs the authors to key areas for improvement, but it could be more comprehensive with additional guidance.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the experiment results need more explanations, specifically questioning why only the result of UnifiedQA is shown and why not also compare to the case using knowledge 100% of the time. While the comment implies that the authors should provide additional explanations and comparisons, it does not explicitly instruct them to do so. The action is implicit and somewhat vague, as the authors can infer what needs to be done but may not be entirely sure of the exact steps to take. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Table 3,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it provides a clear request for additional explanations regarding the experiment results, specifically questioning the absence of results for using knowledge 100% of the time and why only UnifiedQA is compared. This level of detail guides the authors on what specific aspects need further clarification. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point questions the lack of explanation in the experiment results, specifically noting that the results for using knowledge 25% of the time are as good as using it 75% of the time for many tasks. It also questions why only the result of UnifiedQA is shown and suggests comparing it to the case using knowledge 100% of the time. While the comment raises valid points about the need for additional explanations and comparisons, it does not provide specific reasoning or evidence to support these claims. The lack of detailed justification or references makes the comment somewhat verifiable, as the authors would need to infer the importance of these points themselves. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': \"The review comment identifies a specific area for improvement in the experiment results, suggesting that more explanations are needed. It provides a concrete example by questioning why only the result of UnifiedQA is shown and why not also compare to the case using knowledge 100% of the time. This feedback is actionable as it directs the authors to consider additional comparisons and explanations that could enhance the clarity and comprehensiveness of their results. However, the comment could be more helpful if it offered suggestions on how to present these additional comparisons or explanations. Overall, the comment is mostly helpful as it provides clear guidance on improving the draft, aligning with a score of 4.\", 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the scenario presented in the paper may seem derivative and only marginally interesting compared to similar studies conducted on a single node with 8 GPUs. However, it does not provide any explicit or implicit actions for the authors to take in response to this observation. There is no guidance on how the authors might address the perceived lack of originality or interest in their work. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment discusses the similarity of the study presented in the paper to other studies conducted on a single node with 8 GPUs, noting that the computational resources used are still a fraction of those used by large research institutions. However, it does not specify which part of the paper this comparison is based on, making it difficult for the authors to identify the exact section being addressed. The comment also lacks specificity regarding what aspects of the paper are considered derivative or only marginally interesting. Therefore, the comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the scenario presented in the paper may seem derivative and only marginally interesting compared to similar studies conducted on a single node with 8 GPUs. The reviewer supports this claim by noting that the computational resources used in the paper are still a fraction of those used by large research institutions. However, the comment lacks specific examples or references to these similar studies, which would strengthen the claim. Without detailed evidence or examples, the authors may find it challenging to fully understand and address the critique. Therefore, the comment is considered borderline verifiable, as it provides some justification but lacks key elements to fully substantiate the claim.', 'verifiability_label': '2', 'helpfulness_rationale': \"The review comment raises a concern about the originality and interest of the scenario presented in the paper, suggesting that it may seem derivative compared to similar studies conducted on a single node with 8 GPUs. The reviewer notes that the computational resources used in the paper are still a fraction of those used by large research institutions, which could imply that the work may not be significantly innovative. However, the comment does not provide specific suggestions or guidance on how the authors might address this concern or enhance the originality of their work. Without actionable feedback or constructive advice, the comment offers limited value to the authors in terms of improving their draft. Therefore, it is rated as barely helpful.\", 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the clarity of how observations from the cramming regime can inform decisions in the normal training setting. However, it does not provide explicit guidance or suggestions on how the authors might address this issue. The comment implies that the authors should clarify or explore this connection, but it lacks concrete details on how to do so. As a result, the action is implicit and vague, making it barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment raises a concern about the clarity of how observations from the \"cramming regime\" can inform decisions in the \"normal training setting.\" However, it does not specify which part of the paper discusses these regimes, making it weakly grounded. The comment is specific in identifying the issue of unclear application of observations, but without explicit references to sections or specific elements, the authors may struggle to pinpoint the exact areas needing revision. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises a concern about the clarity of how observations from the \"cramming regime\" can inform decisions in the \"normal training setting.\" However, it does not provide any specific examples, reasoning, or references to support this claim. The lack of detailed justification or evidence makes it difficult for the authors to understand and address the issue effectively. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a potential gap in the paper regarding the application of observations from the \"cramming regime\" to inform decisions in the \"normal training setting.\" It highlights a lack of clarity in how these observations might be used to improve the training process. However, the comment does not provide specific suggestions or guidance on how the authors might address this issue or clarify the connection between the two regimes. While it points out an area for improvement, the feedback lacks depth and actionable advice, making it somewhat helpful. The authors are given some insight into a potential weakness but are not fully empowered to make significant improvements. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the significance of the top-3 accuracy for the task, particularly in the context of spatial and cardinal problems where the concept space is small, and for GPT-3, which is known to output in-domain words only. However, it does not provide any explicit or implicit actions for the authors to take. The comment lacks guidance on how the authors might address this concern or what specific changes they should make to their draft. As a result, the authors are left without any actionable steps to improve their work based on this feedback. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a question about the significance of the top-3 accuracy for the task, particularly in the context of spatial and cardinal problems where the concept space is small, and for GPT-3, which is known to output in-domain words only. However, it does not specify which part of the paper this question pertains to, such as a specific section or experiment. Without explicit references or clear connections to the paper, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what aspects of the top-3 accuracy are being questioned or how it relates to the task. Therefore, this comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point questions the significance of the top-3 accuracy for the task, particularly in the context of spatial and cardinal problems where the concept space is small, and for GPT-3, which is known to output in-domain words only. However, the comment does not provide any supporting evidence, reasoning, or references to justify why the top-3 accuracy might not be meaningful in these contexts. Without additional context or explanation, the claim remains vague and unverifiable. Therefore, the comment is classified as \"1: Unverifiable.\"', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a question about the significance of the top-3 accuracy for the task, particularly in the context of spatial and cardinal problems where the concept space is small, and for GPT-3, which is known to output in-domain words only. This question prompts the authors to consider the relevance and meaning of their results in specific contexts, which is a valuable insight. However, the comment does not provide any specific suggestions or guidance on how the authors might address this concern or improve their draft. While it identifies a potential area for consideration, it lacks actionable feedback, making it somewhat helpful. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the suitability of the substring metric for the color problem, particularly in the \"unseen concept\" setup. It provides an example scenario where the metric might not effectively test generalization to unseen concepts, as it could result in perfect accuracy for seen concepts in the unseen setup. However, the comment does not explicitly instruct the authors to address this issue or suggest how to improve the metric. The action is implicit and somewhat vague, as the authors can infer that they need to consider the suitability of the metric but are not provided with specific guidance on how to do so. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises a question about the suitability of the substring metric for the color problem, specifically in the \"unseen concept\" setup. It provides an example scenario where \"light blue\" is a seen concept and \"dark blue\" is the test-time unseen concept, and questions whether this setup would defeat the purpose of testing generalization to unseen concepts, as seen in Table 2. This comment is fully grounded as it explicitly mentions the \"unseen concept\" setup and Table 2, allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it provides a clear example and question about the metric\\'s suitability, making it clear what needs to be addressed. Therefore, this comment is labeled as fully grounded and specific, corresponding to a score of 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a question about the suitability of the substring metric for the color problem, particularly in the \"unseen concept\" setup. It provides an example scenario where the metric might not effectively test generalization to unseen concepts, as it could result in perfect accuracy for seen concepts in the unseen setup. This claim is somewhat verifiable as it presents a logical reasoning and an example to support the concern. However, the comment lacks specific references or detailed analysis to fully substantiate the claim, making it somewhat verifiable. Therefore, the comment aligns with a score of 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment raises a pertinent question about the suitability of the substring metric for the color problem, particularly in the context of the \"unseen concept\" setup. It provides a specific example where the metric might not effectively test generalization to unseen concepts, as it could result in perfect accuracy for seen concepts in the unseen setup. This feedback is valuable as it highlights a potential issue with the evaluation methodology and encourages the authors to consider whether their approach is adequately testing the generalization to unseen concepts. However, the comment could be more helpful if it offered suggestions or guidance on how to address this issue or improve the evaluation setup. Overall, the comment is somewhat helpful as it identifies a potential weakness and prompts the authors to consider a critical aspect of their methodology.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a concern about the authors\\' argument that the model successfully generates the correct concept based on the grounded representation. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a concrete example of this strategy and proposes several experiments to test this hypothesis, including testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are explicit and provide clear guidance on how to address the concern, making the comment highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions specific parts of the paper, such as \"Table 1,\" \"Table 2, Table 9, 10, 11 (in the appendix),\" and \"Figure 6 in the appendix,\" allowing the authors to accurately identify the sections being addressed. It is also specific because it details the alternative explanation for the model\\'s performance and suggests experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. This provides clear guidance on what needs to be addressed and how to improve the draft. Therefore, the comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a claim that the model may not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a detailed explanation of this alternative explanation, including a specific example of how the model might use a simple strategy to copy the concept of similar representations in the in-context prompts. The comment also suggests several experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are well-supported and provide a clear path for the authors to verify the claim. Therefore, the comment is fully verifiable, aligning with a score of 5.', 'verifiability_label': '5', 'helpfulness_rationale': 'The review comment raises a critical point about the authors\\' argument regarding the model\\'s ability to generate correct concepts based on grounded representations. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a detailed example of this strategy and proposes several experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are clear and actionable, offering the authors a comprehensive approach to address the concern and potentially improve their draft. The comment is highly helpful as it provides specific guidance and insights that can significantly enhance the paper\\'s quality and validity. Therefore, it aligns with a score of 5.', 'helpfulness_label': '5'} ###Output: {'actionability_rationale': 'The review point raises a concern about the authors\\\\\\' argument that the model successfully generates the correct concept based on the grounded representation. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a concrete example of this strategy and proposes several experiments to test this hypothesis, including testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are explicit and provide clear guidance on how to address the concern, making the comment highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions specific parts of the paper, such as \"Table 1,\" \"Table 2, Table 9, 10, 11 (in the appendix),\" and \"Figure 6 in the appendix,\" allowing the authors to accurately identify the sections being addressed. It is also specific because it details the alternative explanation for the model\\\\\\'s performance and suggests experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. This provides clear guidance on what needs to be addressed and how to improve the draft. Therefore, the comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a claim that the model may not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a detailed explanation of this alternative explanation, including a specific example of how the model might use a simple strategy to copy the concept of similar representations in the in-context prompts. The comment also suggests several experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are well-supported and provide a clear path for the authors to verify the claim. Therefore, the comment is fully verifiable, aligning with a score of 5.', 'verifiability_label': '5', 'helpfulness_rationale': 'The review comment raises a critical point about the authors\\\\\\' argument regarding the model\\\\\\'s ability to generate correct concepts based on grounded representations. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a detailed example of this strategy and proposes several experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are clear and actionable, offering the authors a comprehensive approach to address the concern and potentially improve their draft. The comment is highly helpful as it provides specific guidance and insights that can significantly enhance the paper\\\\\\'s quality and validity. Therefore, it aligns with a score of 5.', 'helpfulness_label': '5'} ###Output: {'actionability_rationale': 'The review point raises a concern about the authors\\\\\\' argument that the model successfully generates the correct concept based on the grounded representation. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a concrete example of this strategy and proposes several experiments to test this hypothesis, including testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are explicit and provide clear guidance on how to address the concern, making the comment highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions specific parts of the paper, such as \"Table 1,\" \"Table 2, Table 9, 10, 11 (in the appendix),\" and \"Figure 6 in the appendix,\" allowing the authors to accurately identify the sections being addressed. It is also specific because it details the alternative explanation for the model\\\\\\'s performance and suggests experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. This provides clear guidance on what needs to be addressed and how to improve the draft. Therefore, the comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a claim that the model may not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a detailed explanation of this alternative explanation, including a specific example of how the model might use a simple strategy to copy the concept of similar representations in the in-context prompts. The comment also suggests several experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are well-supported and provide a clear path for the authors to verify the claim. Therefore, the comment is fully verifiable, aligning with a score of 5.', 'verifiability_label': '5', 'helpfulness_rationale': 'The review comment raises a critical point about the authors\\\\\\' argument regarding the model\\\\\\'s ability to generate correct concepts based on grounded representations. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a detailed example of this strategy and proposes several experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are clear and actionable, offering the authors a comprehensive approach to address the concern and potentially improve their draft. The comment is highly helpful as it provides specific guidance and insights that can significantly enhance the paper\\\\\\'s quality and validity. Therefore, it aligns with a score of 5.', 'helpfulness_label': '5'} ###Output: {'actionability_rationale': 'The review point raises a concern about the authors\\\\\\' argument that the model successfully generates the correct concept based on the grounded representation. It suggests an alternative explanation that the model might not understand the relationship between concepts and grounded representations but instead uses a similarity-based strategy. The reviewer provides a concrete example of this strategy and proposes several experiments to test this hypothesis, including testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. These suggestions are explicit and provide clear guidance on how to address the concern, making the comment highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions specific parts of the paper, such as \"Table 1,\" \"Table 2, Table 9, 10, 11 (in the appendix),\" and \"Figure 6 in the appendix,\" allowing the authors to accurately identify the sections being addressed. It is also specific because it details the alternative explanation for the model\\\\\\'s performance and suggests experiments to test this hypothesis, such as testing on real unseen concepts, using fewer prompts, and replacing concept names with unrelated concepts. This provides clear guidance"}
{"generated_text": "{'actionability_rationale': 'The review comment raises a concern about the clarity of the paper\\'s overall takeaway and its usefulness for future work. However, it does not provide any explicit or implicit actions for the authors to take to address this issue. There is no guidance on how to clarify the paper\\'s contribution or make it more relevant to future work. Without specific suggestions or directions, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment raises a concern about the clarity of the paper\\'s overall takeaway and its usefulness for future work. However, it does not specify which part of the paper is unclear or how the analysis could be more useful for future work. The authors may have an idea of where the issue lies, but the comment lacks specificity and grounding, making it difficult for them to pinpoint the exact areas needing improvement. Therefore, this comment is not grounded and not specific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point expresses a subjective opinion about the clarity of the paper\\'s overall takeaway and its usefulness for future work. However, it does not provide any specific examples, reasoning, or references to support this claim. The lack of detailed justification or evidence makes it difficult for the authors to understand and address the concern. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a concern about the clarity of the paper\\'s overall takeaway and its usefulness for future work. However, it does not provide specific suggestions or guidance on how the authors might address this issue. Without actionable feedback or detailed insights, the authors are left without a clear path to improve their draft. The comment identifies a potential weakness but lacks depth and specificity, making it barely helpful. Therefore, it aligns with a score of 2.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that some details in the paper need to be checked, specifically mentioning discrepancies in the pretraining of models and the number of prompts used in color experiments. It provides explicit examples of what needs to be verified, such as the pretraining of GPT-3 and the number of prompts used in the color experiments. This feedback is clear and actionable, as it directs the authors to specific sections of the paper where these discrepancies need to be addressed. The authors know exactly what to check and how to correct the inaccuracies, making this comment highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Section 2.1\" and \"the appendix,\" allowing the authors to accurately identify the parts of the paper being addressed. It is also specific because it points out two issues: the claim about all models being pretrained on the 40GB OPENAI-WT dataset, which is disputed for GPT-3, and the ambiguity in the number of prompts used in the color experiments. This provides clear guidance on what needs to be clarified or corrected in the paper. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises concerns about the accuracy of certain details in the paper, specifically questioning the claim that all models are pretrained on the 40GB OPENAI-WT dataset, as it contradicts the information provided for GPT-3. It also points out an inconsistency in the number of prompts used in the color experiments. While the comment identifies potential inaccuracies, it does not provide specific evidence or references to support these claims. The lack of detailed justification or examples makes it difficult for the authors to fully understand and address the issues. Therefore, the comment is considered borderline verifiable, as it provides some basis for the claims but lacks sufficient detail for full verification.', 'verifiability_label': '2', 'helpfulness_rationale': 'The review comment identifies specific inaccuracies in the paper, such as the claim that all models are pretrained on the 40GB OPENAI-WT dataset, which is disputed for GPT-3. It also points out an ambiguity in the number of prompts used in the color experiments, as mentioned in the appendix. By highlighting these discrepancies, the comment provides clear and actionable feedback that directs the authors to verify and correct these inaccuracies. This feedback is valuable as it helps the authors ensure the accuracy and clarity of their work, making it mostly helpful. However, it could be more helpful if it suggested ways to clarify or correct these inaccuracies, which would align with a score of 5.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a potential weakness in the paper, noting that the model\\'s applicability is limited to specific datasets where structural properties can be defined. It suggests that the authors should consider how their method generalizes across different tasks and datasets, beyond the two datasets mentioned. While the comment identifies a gap in the paper, it does not provide explicit guidance on how the authors should address this issue or suggest specific steps to take. The action is implicit and somewhat vague, as the authors are left to infer that they need to explore generalizability and possibly provide more examples or analysis. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses a potential weakness in the paper regarding the generalizability of the method across different datasets and tasks. It mentions that the model works well for source codes and Wikipedia due to the availability of structural information, but it is unclear how it generalizes to other datasets. The comment suggests that the authors should consider whether their work will be limited to a specific audience or if it can be applied more broadly. However, the comment does not specify which part of the paper this issue is discussed in, making it weakly grounded. It is specific in identifying the need for generalizability and the potential limitations of the work. Therefore, this comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises a concern about the generalizability of the method across different datasets and tasks, beyond the two datasets mentioned. It suggests that the model\\'s applicability might be limited to specific datasets where structural properties can be defined. The comment provides a logical reasoning by pointing out the potential limitation of the method and questioning its broader applicability. However, it lacks specific examples or references to support the claim, making it somewhat verifiable. The authors would need to consider this critique and provide additional evidence or analysis to fully address the concern. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a potential weakness in the paper, specifically the lack of generalizability of the method across different datasets and tasks. It highlights that the model\\'s applicability is limited to specific datasets where structural properties can be defined, such as source codes and Wikipedia. The comment suggests that the authors should consider how their method generalizes beyond these datasets and whether it can be applied to a broader range of tasks and domains. While the comment points out a critical area for improvement, it does not provide specific suggestions or guidance on how the authors might address this issue. The feedback is somewhat helpful as it directs the authors to a specific area needing further exploration, but it lacks detailed actionable advice. Therefore, the comment is rated as somewhat helpful.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests a possible way to improve the argument by exploiting entity detection and linking, including automatically learning relation (vector) information in an unsupervised manner. While the comment provides a specific direction for improvement, it does not offer concrete steps or detailed guidance on how to implement this suggestion. The authors are left with a general idea of what to do but without specific instructions on how to achieve it. Therefore, the comment is somewhat actionable, as it provides a clear direction but lacks detailed execution guidance.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment suggests a way to improve the argument by exploiting entity detection and linking, including automatically learning relation (vector) information in an unsupervised manner. However, it does not specify which part of the paper this suggestion pertains to, such as a particular section or methodology. This lack of grounding makes it difficult for the authors to identify the exact area needing improvement. The comment is specific in suggesting a method for improvement but lacks grounding, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point suggests a possible improvement to the argument by exploiting entity detection and linking, including automatically learning relation (vector) information in an unsupervised way. However, the comment does not provide any supporting evidence, examples, or references to substantiate why this approach would be beneficial or how it could lead to a Wikidata-type structure for most datasets. The lack of detailed reasoning or evidence makes the claim difficult for the authors to verify or understand, rendering it unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests a potential improvement to the argument by proposing the use of entity detection and linking, including automatically learning relation (vector) information in an unsupervised manner. This is a specific and actionable suggestion that could help the authors enhance their work by providing a structured approach to obtaining a Wikidata-type structure for most datasets. However, the comment could be more helpful if it included additional details or examples of how this approach could be implemented or integrated into the existing framework. Overall, the feedback is mostly helpful as it offers a clear direction for improvement, but it could be more comprehensive with further elaboration.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the experiments could be improved by conducting downstream application tasks, specifically mentioning document classification tasks. While it implies that the authors should conduct these tasks, the comment does not provide explicit instructions or detailed guidance on how to implement this suggestion. The action is implicit and somewhat vague, as the authors are left to infer the specific tasks and methods to use. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses the experimental section of the paper, suggesting that the experiments could be improved by conducting downstream application tasks, specifically mentioning document classification tasks. However, it does not specify which part of the experimental section this suggestion pertains to, such as a particular experiment or section of the results. This makes it weakly grounded, as the authors cannot confidently determine the exact part of the paper being addressed. The comment is specific in suggesting a particular area for improvement, namely conducting downstream application tasks, but it lacks detailed guidance on how to implement this suggestion. Therefore, the comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point suggests that the experiments could be improved by conducting downstream application tasks, specifically mentioning document classification tasks. However, it does not provide any supporting evidence, reasoning, or references to justify why these tasks would be beneficial or how they would enhance the paper. The comment lacks specific examples or detailed explanations, making it difficult for the authors to understand the basis of the suggestion. Therefore, the claim is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment suggests that the experiments could be improved by conducting downstream application tasks, specifically mentioning document classification tasks. This feedback is somewhat helpful as it identifies a potential area for enhancement and provides a specific suggestion for further experimentation. However, the comment lacks depth and does not offer detailed guidance on how to conduct these tasks or what specific aspects of document classification should be explored. While it provides some direction, it could be more helpful with additional details or examples. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights the difficulty in assessing the usefulness of the model due to the limited experiments presented in the paper, which primarily focus on perplexity analysis. It mentions that table 5 in the appendix provides additional token prediction results. However, the comment does not provide any explicit or implicit actions for the authors to take. It lacks guidance on how to expand or improve the experiments to better demonstrate the model's usefulness. Without specific suggestions or recommendations, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"table 5 in the appendix,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it highlights the limited experiments presented, specifically mentioning perplexity analysis and the additional token prediction results in table 5. This provides clear guidance on what aspects of the paper need improvement or expansion. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that it is difficult to gauge the usefulness of the model due to the limited experiments presented in the paper. It specifically mentions that the experiments mainly revolve around perplexity analysis and references table 5 in the appendix for additional token prediction results. However, the comment lacks detailed reasoning or examples to support why the experiments are insufficient or how they could be improved. Without specific evidence or suggestions, the claim remains somewhat verifiable, as it provides a general critique but lacks the depth needed for full verification. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a limitation in the paper, specifically the difficulty in assessing the usefulness of the model due to the limited experiments presented. It highlights that the experiments mainly focus on perplexity analysis and mentions that table 5 in the appendix provides additional token prediction results. While the comment points out a weakness, it does not offer specific suggestions or guidance on how the authors might expand or improve the experiments to better demonstrate the model\\'s usefulness. This lack of actionable feedback limits the comment\\'s helpfulness, making it somewhat helpful as it provides insight into an area for improvement but does not fully support the authors in addressing it. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point expresses a doubt about the results presented in \"Section 5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES\". However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on what specific aspects of the results are questionable or how the authors might address these doubts. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment expresses a doubt about the results presented in \"Section 5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES.\" This provides full grounding as it explicitly mentions the specific section of the paper being addressed, allowing the authors to accurately identify the part of the paper that needs attention. However, the comment lacks specificity as it does not detail what exactly is causing the doubt or what aspects of the results are questionable. Therefore, this comment is fully grounded but under-specific, aligning with a score of 4.', 'grounding_specificity_label': '4', 'verifiability_rationale': 'The review point expresses a doubt about the results presented in \"Section 5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES.\" However, it does not provide any specific reasoning, evidence, or examples to support this doubt. Without additional context or justification, the authors may find it challenging to understand the basis of the reviewer\\'s concern or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment expresses a doubt about the results presented in \"Section 5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES.\" However, it does not provide any specific details or context about what aspects of the results are questionable or why the reviewer has doubts. Without additional information or guidance, the authors are left without a clear understanding of what needs to be addressed or improved. This lack of specificity and actionable feedback makes the comment unhelpful, as it does not offer any meaningful insights or suggestions for enhancing the draft. Therefore, it aligns with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a discrepancy in the use of metrics across different parts of the paper, specifically noting the use of CA@5 scores in one place and BLEU/CodeBLEU/EM in another. The reviewer questions the intuition behind using different metrics and suggests that the authors include results for BLEU, which can handle multi-reference evaluation. While the comment implies that the authors should clarify or provide additional results, it does not explicitly instruct them to do so. The action is implicit and somewhat vague, as the authors need to infer that they should address the metric inconsistency and potentially include additional results. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses a specific issue regarding the use of different metrics in the paper, mentioning the use of CA@5 scores in one place and BLEU/CodeBLEU/EM in another. It questions the intuition behind this choice and suggests adding results for BLEU, which can handle multi-reference evaluation. This provides clear guidance on what needs to be addressed, making the comment specific. However, it does not explicitly mention which sections of the paper this issue pertains to, leaving the authors to infer the relevant parts. Therefore, the comment is weakly grounded but specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point questions the use of different metrics in the paper, specifically noting the use of CA@5 scores in one place and BLEU/CodeBLEU/EM in another. The reviewer suggests that BLEU can handle multi-reference evaluation and implies that the choice of metrics might not be consistent. However, the comment does not provide specific examples or detailed reasoning to support why the use of different metrics is problematic or how it affects the paper\\'s results. This lack of detailed justification makes the claim somewhat verifiable, as the authors would need to infer the potential issues and address them in their response. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a potential inconsistency in the paper regarding the use of different metrics, specifically CA@5 scores and BLEU/CodeBLEU/EM. It questions the rationale behind this choice and suggests that the authors include results for BLEU, which can handle multi-reference evaluation. This feedback is clear and actionable, as it points out a specific area for improvement and provides a concrete suggestion for enhancing the paper. By addressing this issue, the authors can provide a more comprehensive understanding of their results and improve the clarity of their work. Therefore, the comment is rated as mostly helpful, aligning with a score of 4.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the research is superficial because it only demonstrates the models\\' tolerance to certain noises without proposing any simple heuristics to mitigate their impact. The reviewer provides a specific example of a potential heuristic, \"penalizing the noises (i.e., the targets not belonging to the source) during model training (fine-tuning),\" and references a potential source of inspiration, \"[1].\" This feedback is explicit and provides a concrete suggestion for improvement, making it highly actionable. The authors know exactly what action to take to enhance their draft by incorporating the proposed heuristic and exploring other potential solutions.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment critiques the research for being superficial, noting that it only demonstrates the models\\' tolerance to certain noises without proposing any simple heuristics to alleviate their impact. The reviewer suggests a potential heuristic, \"penalizing the noises (i.e., the targets not belonging to the source) during model training (fine-tuning),\" and references a potential source of inspiration, \"[1].\" However, the comment does not specify which part of the paper this critique pertains to, making it weakly grounded. The suggestion for a potential heuristic is specific, but without clear grounding, the authors may struggle to identify the exact section that needs revision. Therefore, this comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the research is \"somewhat superficial\" because it only demonstrates the models\\' tolerance to certain noises without proposing any simple heuristics to alleviate their impact. The reviewer suggests a potential heuristic, \"penalizing the noises (i.e., the targets not belonging to the source) during model training (fine-tuning),\" and references a potential source of inspiration, \"[1].\" This provides some support for the claim by offering a specific example of a potential solution. However, the comment could be strengthened by providing more detailed reasoning or examples of how the current approach is superficial or how the suggested heuristic could be implemented. Therefore, the claim is mostly verifiable, as it provides a basis for the critique but lacks comprehensive evidence or detailed explanation.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment identifies a limitation in the research, noting that it only demonstrates the models\\' tolerance to certain noises without proposing any simple heuristics to alleviate their impact. The reviewer suggests a potential heuristic, \"penalizing the noises (i.e., the targets not belonging to the source) during model training (fine-tuning),\" and references a potential source of inspiration, \"[1].\" This feedback is clear and actionable, as it provides a specific suggestion for improvement and points the authors in the direction of a potential solution. However, the comment could be more helpful if it offered additional guidance on how to implement the suggested heuristic or provided more context on why this approach might be beneficial. Overall, the comment is mostly helpful, as it offers valuable insights and direction for enhancing the draft.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights that the novelty of the paper is limited because MD-based OOD metrics have been widely studied and used, even though the paper applies them in a different setting (non-classification). However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address the issue of limited novelty or suggestions for enhancing the originality of the work. As a result, the authors are left without any actionable steps to improve their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment addresses the novelty of the paper by mentioning that MD-based OOD metrics have been widely studied and used, even though the paper applies them in a different setting (non-classification). However, it does not specify which part of the paper discusses these metrics or how they are used in the different setting. This makes it difficult for the authors to pinpoint the exact section that needs attention. The comment is specific in its critique of novelty but lacks grounding, as it does not clearly identify the part of the paper being addressed. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the novelty of the paper is limited because MD-based OOD metrics have been widely studied and used. However, it acknowledges that the paper applies these metrics in a different setting (non-classification), which could potentially offer some novelty. The comment provides a logical reasoning by pointing out the existing body of work on MD-based OOD metrics, but it does not elaborate on how the different setting contributes to novelty or provide specific examples. This makes the claim somewhat verifiable, as it highlights a potential limitation but lacks detailed justification or references to support the claim fully. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment points out that the novelty of the paper is limited because MD-based OOD metrics have been widely studied and used, even though the paper applies them in a different setting (non-classification). This feedback is somewhat helpful as it identifies a potential weakness in the paper's originality. However, it lacks specific suggestions or guidance on how the authors might address this issue or enhance the novelty of their work. Without actionable advice or examples, the authors are left with a general understanding of the critique but no clear path for improvement. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights two areas for improvement: the lack of comparison with other common OOD detection strategies and the need for clarity on how the proposed method generalizes to other tasks and model structures. While the comment identifies these gaps, it does not provide explicit instructions or concrete suggestions on how the authors should address these issues. The authors are left to infer that they should conduct comparisons with other methods and provide more detailed explanations of generalizability, but the comment lacks specific guidance on how to implement these actions. Therefore, the comment is somewhat actionable, as it points out areas for improvement but does not provide detailed instructions on how to address them.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses two separate issues: the lack of comparison with other common OOD detection strategies and the need for clarity on how the proposed method generalizes to other tasks and model structures. However, it does not specify which parts of the paper these issues relate to, making it difficult for the authors to pinpoint the exact sections that need attention. The comment is specific in its critique, but it lacks grounding as it does not provide explicit references to sections or figures. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the paper lacks comparison with other common OOD detection strategies and that it is unclear how the proposed method generalizes to other tasks and model structures. However, the comment does not provide specific examples of the strategies that should be compared or detailed reasoning on why the generalizability is unclear. This lack of supporting evidence or detailed justification makes the claim somewhat verifiable, as the authors would need to infer the specific comparisons or generalizability issues that need to be addressed. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies two areas for improvement in the paper. First, it points out the lack of comparison with other common OOD detection strategies, such as threshold-based, energy-based, and contrastive-based approaches. This feedback is clear and actionable, as it suggests that the authors should include comparisons with these methods to provide a more comprehensive evaluation of their approach. Second, the comment raises a concern about the generalizability of the proposed method to other tasks and model structures, which is an important aspect for the authors to address. However, the comment could be more helpful if it provided specific suggestions or examples on how to demonstrate generalizability. Overall, the feedback is mostly helpful as it highlights critical areas for improvement, but it could be more comprehensive with additional guidance. Therefore, it aligns with a score of 4.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the choice of embeddings from the final layers, indicating a lack of clarity in the paper. However, it does not provide any explicit guidance or suggestions on how the authors might address this issue or clarify their choice. The action is implicit and vague, as the authors are left to infer that they need to provide more explanation or justification for their selection. Without specific instructions or examples, the authors may find it challenging to know exactly how to improve their draft. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment raises a question about the choice of embeddings from the final layers, indicating a lack of clarity in the paper. However, it does not specify which part of the paper discusses these embeddings, making it difficult for the authors to pinpoint the exact section that needs clarification. The comment is specific in its question about the choice of embeddings but lacks grounding as it does not identify the specific section or part of the paper being addressed. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point questions the choice of embeddings from the final layers, indicating a lack of clarity in the paper. However, it does not provide any supporting evidence, reasoning, or references to justify why this choice is unclear or problematic. Without additional context or explanation, the authors may find it challenging to understand the basis of the question or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a question about the choice of embeddings from the final layers, indicating a lack of clarity in the paper. While it identifies a potential area for improvement, it does not provide any specific suggestions or guidance on how the authors might address this issue or clarify their choice. The comment lacks depth and actionable feedback, leaving the authors with a vague understanding of what needs to be improved. Therefore, it is barely helpful, as it identifies a weakness but does not offer sufficient guidance for improvement.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review comment raises concerns about the selection of background data and the generalizability of the background Gaussian. It suggests that the proposed method may not be as lightweight as claimed and questions the effectiveness of the background Gaussian when there is no good definition of background data. However, the comment does not provide explicit guidance or suggestions on how the authors should address these concerns. The authors are left to infer that they need to clarify the selection process and provide evidence of the method\\'s generalizability, but the comment lacks concrete steps or examples to follow. Therefore, the comment is somewhat actionable, as it identifies areas for improvement but does not provide detailed guidance on how to address them.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises concerns about the selection of background data and the generalizability of the background Gaussian, questioning the claim of the method being lightweight. It also raises a concern about the method\\'s performance when there is no good definition of background data. However, the comment does not specify which part of the paper discusses the background data or the background Gaussian, making it difficult for the authors to pinpoint the exact sections that need revision. While the comment is specific in its critique, it lacks grounding, as it does not provide clear references to the paper. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises concerns about the selection of background data and the generalizability of the background Gaussian, questioning the claim of the method being lightweight. It also raises a concern about the method\\'s performance when there is no good definition of background data. The comment provides logical reasoning by questioning the assumption of the background Gaussian and its applicability in scenarios where background data is not well-defined. However, it lacks specific examples or references to support the claim, making it somewhat verifiable. The authors would need to provide additional evidence or clarification to fully substantiate the concerns raised. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment raises important concerns about the selection of background data and the generalizability of the background Gaussian. It questions the claim of the method being lightweight and highlights potential issues with the background Gaussian when there is no good definition of background data. This feedback is valuable as it prompts the authors to clarify their methodology and provide evidence of the method\\'s effectiveness in various scenarios. However, the comment could be more helpful if it offered specific suggestions or examples on how to address these concerns. Overall, the comment is somewhat helpful as it identifies areas for improvement but lacks detailed guidance, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point states that \"Related works have limited coverage,\" but it does not provide any explicit or implicit actions for the authors to take. It lacks guidance on what specific aspects of the related works should be expanded upon or how the coverage could be improved. Without any actionable suggestions or details, the authors are left without a clear understanding of how to address the issue. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment \"Related works have limited coverage\" is not grounded as it does not specify which part of the paper is being addressed, leaving the authors uncertain about where the issue of limited coverage is located. Additionally, the comment lacks specificity because it does not provide details on what aspects of the related works are missing or how the coverage could be improved. Without clear guidance, the authors cannot effectively address the issue. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that \"Related works have limited coverage,\" but it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without specific details or references to what aspects of related works are missing or how the coverage could be improved, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment points out that the related works section has limited coverage, which is a valid observation. However, it lacks specificity and does not provide any guidance or suggestions on how the authors might improve the coverage or what specific aspects of related works should be included. Without actionable feedback or detailed advice, the authors are left without a clear understanding of how to address the issue. Therefore, the comment is barely helpful, as it identifies a potential weakness but does not offer any meaningful direction for improvement.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point states that the \"explanation generation part\" is vague, but it does not provide any specific guidance or suggestions on how to clarify or improve this part of the paper. The comment lacks explicit or implicit actions, leaving the authors without a clear understanding of what needs to be done to address the issue. Without actionable advice or examples, the authors are unable to make meaningful improvements to their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment states that the \"explanation generation part\" is vague, but it does not specify which part of the paper this refers to, such as a section, figure, or table. Without explicit references or clear indicators, the authors cannot confidently determine which part of the paper is being addressed. Additionally, the comment lacks specificity regarding what aspects of the explanation generation part are vague or need clarification. Therefore, the comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the \"explanation generation part\" is vague, but it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without additional context or explanation, the authors may find it challenging to understand what specific aspects of the explanation generation part are unclear or need clarification. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment identifies a specific area of the paper, namely the \"explanation generation part,\" as being vague. However, it does not provide any further details or suggestions on how to clarify or improve this part of the paper. Without actionable guidance or examples, the authors are left without a clear understanding of what needs to be addressed or how to enhance their draft. This lack of specificity and actionable feedback makes the comment barely helpful, as it does not offer much value in terms of improving the paper.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point states that the \"experiment section is weak,\" but it does not provide any specific details or suggestions on how to improve it. There is no explicit or implicit action for the authors to take, as the comment lacks concrete guidance on what aspects of the experiment section need strengthening. Without any actionable advice or examples, the authors are left without a clear path for improvement. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment states that the \"experiment section is weak,\" but it does not specify which part of the experiment section is weak or what aspects need improvement. This lack of detail makes it difficult for the authors to identify the exact areas that need attention. The comment is not grounded as it does not refer to a specific section or part of the paper, and it is also not specific about what needs to be addressed. Therefore, this comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the \"experiment section is weak,\" but it does not provide any supporting evidence, reasoning, or examples to justify this claim. Without specific details or context, the authors may find it challenging to understand what aspects of the experiment section are weak or how they could be improved. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment identifies a weakness in the experiment section, indicating that it is weak. However, it lacks specificity and does not provide any guidance or suggestions on how the authors might improve this section. Without actionable feedback or detailed insights, the authors are left without a clear understanding of what needs to be addressed or how to enhance their experiment section. This lack of detail makes the comment unhelpful, as it does not offer any meaningful direction for improvement. Therefore, it aligns with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point states that the analysis on generated explanation is weak, but it does not provide any specific guidance or suggestions on how to improve this aspect of the paper. There is no explicit or implicit action for the authors to take, such as suggesting additional analysis techniques, providing examples, or explaining what specific aspects of the analysis are weak. Without any actionable advice, the authors are left without a clear path for improvement. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment mentions that \"Analysis on generated explanation is also weak,\" but it does not specify which part of the paper this analysis is discussed in. The authors cannot confidently determine whether it refers to a specific section, figure, or table, making it weakly grounded. Additionally, the comment lacks specificity as it does not provide details on what aspects of the analysis are weak or how they could be improved. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point claims that \"Analysis on generated explanation is also weak,\" but it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without additional context or justification, the authors may find it challenging to understand the basis of the critique or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a weakness in the analysis of generated explanations, indicating that it is weak. However, it does not provide any specific guidance or suggestions on how the authors might improve this aspect of their work. Without actionable feedback or detailed advice, the authors are left without a clear path for enhancing their analysis. This lack of direction makes the comment barely helpful, as it highlights a potential issue but does not offer any meaningful steps for improvement.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point acknowledges the discussion of prototypes in the paper but does not provide any explicit or implicit actions for the authors to take. It lacks any guidance or suggestions for improvement, leaving the authors without a clear understanding of what they should do with this information. As a result, the comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment mentions that the architecture relies on using prototypes, which are discussed in the paper. However, it does not specify which part of the paper discusses these prototypes, making it weakly grounded. The comment is specific in acknowledging the discussion of prototypes but does not provide further details or suggestions for improvement. Therefore, this comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point acknowledges the discussion of prototypes in the paper, but it does not contain any claims, opinions, or suggestions that require verification. It is purely descriptive, making it a factual statement. Therefore, it should be labeled as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The comment acknowledges the discussion of prototypes in the paper, which is a positive observation. However, it lacks any specific feedback, suggestions, or guidance for the authors to improve their work. Without actionable advice or critique, the comment does not provide the authors with a clear path for enhancing their draft. Therefore, it is rated as not helpful at all, aligning with a score of 1.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review comment highlights the interest in improving interpretability without compromising downstream task F1, noting that this trade-off is common in prior works. It also mentions that the paper has limited coverage of various explanation methods, such as prompt engineering, information bottleneck, and purely generative approaches. The comment suggests that touching on different approaches is important, as the way ProtoTrex handles explanation might not easily extend to all other cases. While the comment identifies areas for improvement, it does not provide explicit instructions or concrete suggestions on how the authors should address these issues. The action is implicit and somewhat vague, as the authors need to infer that they should expand their coverage of different explanation methods. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses the issue of improving interpretability without compromising downstream task F1, noting that this trade-off is common among prior works. It mentions the need for the paper to cover various explanation methods, such as prompt engineering, information bottleneck, and purely generative approaches, as the way ProtoTrex handles explanation might not easily extend to all other cases. However, the comment does not specify which part of the paper should include these additional explanations or approaches, making it weakly grounded. The authors can infer that it relates to the sections discussing interpretability or explanation methods, but the lack of explicit reference makes it difficult to pinpoint. The comment is specific in suggesting the need for broader coverage of explanation methods, but it lacks grounding. Therefore, this comment is weakly grounded and specific, aligning with a score of 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the paper has limited coverage of various explanation methods, such as prompt engineering, information bottleneck, and purely generative approaches. It suggests that touching on different approaches is important because the way ProtoTrex handles explanation might not easily extend to all other cases. However, the comment lacks specific examples or references to support the claim about the limited coverage of these methods. Without detailed evidence or examples, the claim remains somewhat vague and difficult for the authors to address effectively. Therefore, the comment is rated as 3: Somewhat Verifiable, as it provides some justification but lacks key elements to fully substantiate the claim.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies an interesting aspect of the paper, which is improving interpretability without compromising downstream task F1, a common trade-off in prior works. It highlights the need for the paper to cover various explanation methods, such as prompt engineering, information bottleneck, and purely generative approaches, as the current coverage is limited. The comment suggests that touching on different approaches is important, especially since the way ProtoTrex handles explanation might not easily extend to all other cases. This feedback is somewhat helpful as it points out a gap in the paper\\'s coverage and suggests a direction for expansion. However, it could be more helpful if it provided specific examples or guidance on how to incorporate these additional approaches. Overall, the comment offers a clear direction for improvement but lacks detailed actionable steps, making it somewhat helpful.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the comparison between prototype embeddings and label-wise weights in the final classification layer, particularly in the context of NLP tasks. It suggests exploring how simply treating label-wise embeddings as prototypes might perform without the use of the complicated loss in Eq 1. While the comment implies that the authors should conduct this comparison, it does not explicitly instruct them to do so. The action is implicit and somewhat vague, as it lacks specific guidance on how to conduct the comparison or what aspects to focus on. Therefore, the comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment raises a question about the comparison between prototype embeddings and label-wise weights in the final classification layer, specifically in the context of NLP tasks. It suggests exploring how simply treating label-wise embeddings as prototypes might perform without the use of the complicated loss in Eq 1. However, the comment does not specify which part of the paper this question pertains to, such as a specific section or figure, making it weakly grounded. The comment is specific in detailing what needs to be addressed, namely the comparison between prototype embeddings and label-wise weights. Therefore, the comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point raises a question about the comparison between prototype embeddings and label-wise weights in the final classification layer, specifically in the context of NLP tasks. It suggests exploring how simply treating label-wise embeddings as prototypes might perform without the use of the complicated loss in Eq 1. However, the comment does not provide any supporting evidence, reasoning, or references to justify why this comparison is necessary or how it might impact the paper. Without additional context or explanation, the claim remains vague and unverifiable. Therefore, the comment is rated as 1: Unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment raises a pertinent question about the comparison between prototype embeddings and label-wise weights in the final classification layer, particularly in the context of NLP tasks. It suggests exploring how simply treating label-wise embeddings as prototypes might perform without the use of the complicated loss in Eq 1. This feedback is valuable as it prompts the authors to consider an alternative approach and potentially improve their design. However, the comment could be more helpful if it provided specific guidance on how to conduct this comparison or what aspects to focus on. Overall, the comment is somewhat helpful as it identifies a potential area for exploration, but it lacks depth and actionable steps, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The comment suggests that the explanation generation section (section 3.5) requires elaboration. However, it does not provide specific guidance on what aspects need elaboration or how the authors should expand their explanation. The action is implicit and vague, as the authors are left to infer what needs to be added or clarified without concrete instructions. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment explicitly mentions \"explanation generation (sec 3.5),\" which allows the authors to accurately identify the part of the paper being addressed. This provides full grounding. However, the comment lacks specificity as it does not detail what aspects of the explanation generation need elaboration. The authors are left without clear guidance on how to improve this section. Therefore, the comment is fully grounded but under-specific, aligning with category 4.', 'grounding_specificity_label': '4', 'verifiability_rationale': 'The review point suggests that the explanation generation section (sec 3.5) needs elaboration. However, it does not provide any specific reasons or examples to support why this elaboration is necessary or what aspects of the explanation generation require more detail. Without additional context or justification, the authors may find it challenging to understand and address the feedback effectively. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The comment identifies a specific area of the paper that requires further elaboration, namely the explanation generation section in section 3.5. However, it lacks specificity and does not provide guidance on what aspects of the explanation generation need more detail or how the authors might improve this section. Without actionable suggestions or examples, the authors are left with a general direction but no clear path for improvement. Therefore, the comment is somewhat helpful, as it points out an area for improvement but does not fully support the authors in making those improvements.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point identifies a limitation in the paper\\'s design, specifically that it uses prototype embedding to find a training example as its nearest neighbor and then uses this data point as an explanation for its prediction. The reviewer highlights two limitations: a) the method is not context/example dependent, and b) it is more aligned with salience-based explanation works rather than generation. However, the comment does not provide explicit guidance or suggestions on how the authors might address these limitations or improve their design. The feedback is implicit and lacks concrete details on what changes could be made to enhance the paper. As a result, the authors are left without a clear understanding of how to apply the feedback to improve their draft. Therefore, this comment is barely actionable.', 'actionability_label': '2', 'grounding_specificity_rationale': 'The comment addresses a specific aspect of the paper\\'s methodology, namely the use of prototype embedding to find a training example as its nearest neighbor and then using this data point as an explanation for its prediction. However, it does not specify which part of the paper discusses this methodology, making it weakly grounded. The comment is specific in detailing the limitations of this design, such as it being not context/example dependent and more aligned with salience-based explanation works. Therefore, the comment is weakly grounded and specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that the paper uses prototype embedding to find a training example as its nearest neighbor and then uses this data point as an explanation for its prediction. It further argues that this design has limitations, specifically that it is not context/example dependent and is more aligned with salience-based explanation works. The comment provides a logical reasoning for the claim by highlighting the potential limitations of the approach, but it lacks specific examples or references to support the assertion. While the reasoning is clear, the lack of detailed evidence or references makes the claim somewhat verifiable. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a specific aspect of the paper\\'s methodology, namely the use of prototype embedding to find a training example as its nearest neighbor and then using this data point as an explanation for its prediction. It highlights two limitations: the method is not context/example dependent and is more aligned with salience-based explanation works rather than generation. This feedback is somewhat helpful as it points out a potential weakness in the paper\\'s design, prompting the authors to consider these limitations and potentially explore alternative approaches. However, the comment could be more helpful if it provided suggestions or examples of how the authors might address these limitations or improve their methodology. Therefore, the comment is rated as somewhat helpful, aligning with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a concern about the examples in Table 2, noting that they are partially dependent on the input example but lacks an explanation of how they were generated. However, it does not provide any explicit or implicit actions for the authors to take. There is no guidance on how to address the issue or improve the clarity of the examples. As a result, the authors are left without a clear understanding of what steps to take to enhance their draft. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Tab2,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it points out that the examples in Table 2 depend on the input example but lacks an explanation of how they were generated. This provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the examples in Table 2 are partially dependent on the input example but lacks an explanation of how they were generated. However, the comment does not provide any supporting evidence, reasoning, or references to substantiate this claim. Without additional context or explanation, the authors may find it challenging to understand the basis of the critique or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a specific issue with the examples presented in Table 2, noting that they are partially dependent on the input example but lacks an explanation of how they were generated. This feedback highlights a potential weakness in the paper, specifically regarding the transparency and clarity of the examples provided. However, the comment does not offer any suggestions or guidance on how the authors might address this issue or improve the clarity of their examples. While it points out a problem, it lacks actionable advice, making it somewhat helpful but incomplete. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point explicitly states that \"Tab 1b is confusing\" and that the reviewer does not understand what each number means. This provides a clear and direct action for the authors to take: clarify the meaning of the numbers in Table 1b. The comment is explicit and concrete, as it specifies exactly what needs to be addressed and how to improve the draft. Therefore, this comment is highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Tab 1b,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies the issue, which is the confusion regarding the meaning of each number in the table. This provides clear guidance on what needs to be clarified or improved in the paper. Therefore, this comment is labeled as fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that \"Tab 1b is confusing\" and that the reviewer does not understand what each number means. However, the comment lacks any supporting evidence, reasoning, or examples to justify why the table is confusing or how it could be clarified. Without additional context or explanation, the authors may find it challenging to understand the basis of the claim or how to address it. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment identifies a specific issue with Table 1b, stating that it is confusing and that the reviewer does not understand what each number means. This feedback is clear and actionable, as it directs the authors to clarify the meaning of the numbers in the table. However, the comment could be more helpful if it provided suggestions on how to improve the clarity or offered examples of what information should be included to make the table more understandable. Despite this, the comment still provides valuable guidance for the authors to enhance the clarity of their work. Therefore, it is rated as mostly helpful.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point indicates that the paper briefly covers some statements without providing details, but it does not offer any specific guidance or suggestions on how the authors might address this issue. There is no explicit or implicit action for the authors to take, such as adding more details or expanding on certain points. Without any actionable advice, the authors are left without a clear path for improvement. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment suggests that the paper briefly covers some statements without providing details, but it does not specify which statements or parts of the paper are being referred to. This lack of specificity makes it difficult for the authors to identify the exact areas that need improvement. Additionally, the comment does not provide any guidance on how to address the issue or what details should be included. As a result, the comment is not grounded and highly unspecific, aligning with a score of 1.', 'grounding_specificity_label': '1', 'verifiability_rationale': 'The review point claims that the paper \"very briefly went over some statements without getting into details.\" However, it does not provide any specific examples or context to support this claim, making it difficult for the authors to understand which statements are being referred to or why they are considered brief. Without detailed justification or examples, the claim lacks verifiability, as the authors cannot effectively address the feedback. Therefore, the comment is rated as 1: Unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment points out that the paper briefly covers some statements without providing details. However, it does not specify which statements are being referred to or suggest ways in which the authors could expand on these points. Without specific guidance or examples, the comment lacks actionable feedback that could help the authors improve their draft. As a result, it is barely helpful, as it identifies a potential issue but does not provide enough detail for the authors to effectively address it.', 'helpfulness_label': '2'}"}
{"generated_text": "{'actionability_rationale': 'The review point indicates that the paper lacks a solid explanation evaluation, but it does not provide any specific guidance or suggestions on how the authors might address this issue. There is no explicit or implicit action for the authors to take, such as suggesting what aspects of the explanation evaluation should be improved or how to conduct it. Without any actionable advice, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment indicates that there is a lack of solid explanation evaluation in the paper, but it does not specify which part of the paper this issue pertains to. The authors cannot confidently determine which sections or aspects of the paper are being addressed, making it weakly grounded. Additionally, the comment lacks specificity as it does not provide details on what aspects of the explanation evaluation are missing or how they could be improved. Therefore, this comment is weakly grounded and not specific, aligning with a score of 2.', 'grounding_specificity_label': '2', 'verifiability_rationale': 'The review point claims that there is a lack of solid explanation evaluation in the paper. However, it does not provide any supporting evidence, reasoning, or examples to substantiate this claim. Without specific details or references to what aspects of the explanation evaluation are lacking, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.', 'verifiability_label': '1', 'helpfulness_rationale': 'The review comment points out a lack of solid explanation evaluation in the paper, which is a critical aspect of many research papers. However, it does not provide any specific guidance or suggestions on how the authors might address this issue or improve the explanation evaluation. Without actionable feedback or examples, the authors are left without a clear understanding of what changes are needed to enhance their draft. Therefore, the comment is not helpful at all, as it lacks depth and does not offer any constructive advice for improvement.', 'helpfulness_label': '1'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises several concerns about the evaluation of rationale performances in Table 1c, noting that the numbers are low compared to prior works and questioning whether the evaluation is based on token or sentence selection. It also points out that the evaluation has nothing to do with generation and suggests that human-based evaluation over a subset of testing data would be more appropriate for generative explanations. While the comment identifies areas for improvement, it does not provide explicit instructions or concrete steps for the authors to take. The actions are implicit and somewhat vague, as the authors need to infer that they should address the issues raised by revising the evaluation methodology and including human-based evaluation. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Tab 1c,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it details the issues with the rationale performances, noting that the numbers are low compared to prior works and questioning whether the evaluation is based on token or sentence selection. Additionally, it points out the lack of relevance to generation and suggests the need for human-based evaluation. This provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the rationale performances in Table 1c are low compared to prior works, such as Paranjape\\'s work at EMNLP 20. It also questions the basis of the evaluation, whether it is based on token or sentence selection, and notes that the evaluation has nothing to do with generation. The comment suggests that human-based evaluation over a subset of testing data would be more appropriate for generative explanations. While the comment provides some context and references a specific prior work, it lacks detailed comparisons or specific examples to fully substantiate the claim. The reasoning is somewhat verifiable, as it provides a logical basis for the critique but could be strengthened with more detailed evidence or references. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies several issues with the evaluation of rationale performances in Table 1c, noting that the numbers are low compared to prior works and questioning whether the evaluation is based on token or sentence selection. It also points out that the evaluation has no relevance to generation and suggests that human-based evaluation over a subset of testing data would be more appropriate for generative explanations. This feedback is clear and actionable, as it highlights specific areas for improvement and provides a constructive suggestion for enhancing the evaluation methodology. However, the comment could be more helpful if it offered specific examples or references to support the claim about prior works or provided more detailed guidance on implementing human-based evaluation. Overall, the comment is mostly helpful, as it effectively guides the authors toward improving their draft by addressing these critical evaluation aspects.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point explicitly instructs the authors to clarify which MoE model is used in the paper, as the current information is insufficient. It also suggests adding citations to support the choice of model. This feedback is clear and provides a direct action for the authors to take, making it highly actionable. The authors know exactly what needs to be done to improve their draft, which is to specify the MoE model used and include relevant citations.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Figure 1\" and \"MoE,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it clearly specifies the issue of not knowing which MoE model is used and suggests that the authors should announce which model they used and add citations. This provides clear guidance on what needs to be addressed, making the comment fully grounded and specific. Therefore, this comment aligns with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the paper does not specify which MoE model is used, which could lead to different conclusions. The reviewer provides a list of examples of MoE models, such as Gshard, Switch Transformer, Base Layers, and HASH Layers, to support the claim that different models may lead to different outcomes. This provides a clear and logical reasoning for the claim, making it mostly verifiable. However, the comment could be strengthened by including specific references or examples from the paper that demonstrate the lack of model specification, which would make it fully verifiable. Therefore, the comment is rated as 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment identifies a critical issue with the paper, specifically that it does not specify which MoE model is used for analysis. This lack of clarity could lead to different conclusions depending on the model chosen. The reviewer provides a list of examples of MoE models, such as Gshard, Switch Transformer, Base Layers, and HASH Layers, which helps the authors understand the importance of specifying the model used. The comment also suggests that the authors should announce which model they used and add relevant citations. This feedback is clear and actionable, offering specific guidance on how to improve the paper by addressing a critical gap in the methodology. Therefore, the comment is rated as highly helpful, aligning with a score of 5.', 'helpfulness_label': '5'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests that the authors should conduct experiments with at least two representative MoE models to draw analysis conclusions, as different models may have different performances. This feedback is explicit, as it clearly states the action needed to improve the draft. However, it lacks concrete details on which specific models should be used or how to conduct the experiments. While the authors know they need to conduct additional experiments, the comment does not provide enough guidance on how to implement this action. Therefore, the comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment addresses the use of only one MoE model in the analysis, which the reviewer disagrees with. However, it does not specify which part of the paper this issue is discussed in, making it weakly grounded. The comment is specific in suggesting that experiments should be conducted with at least two representative MoE models to draw analysis conclusions, as different models may have different performances. This provides clear guidance on what needs to be addressed. Therefore, the comment is weakly grounded but specific, aligning with category 3.', 'grounding_specificity_label': '3', 'verifiability_rationale': 'The review point claims that using only one MoE model is insufficient for drawing analysis conclusions, as different models may have different performances. The reviewer suggests conducting experiments with at least two representative MoE models to address common challenges with MoE models. While the comment provides a logical reasoning for the need for multiple models, it lacks specific examples or references to support the claim. This makes the claim somewhat verifiable, as the authors would need to infer the importance of using multiple models based on the reviewer\\'s reasoning. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a limitation in the paper\\'s analysis, specifically the use of only one MoE model to draw conclusions. It suggests that the authors should conduct experiments with at least two representative MoE models to address common challenges with MoE models. This feedback is clear and actionable, as it provides a specific direction for improving the analysis by considering multiple models. However, the comment could be more helpful if it offered examples of specific models or detailed guidance on how to conduct these experiments. Overall, the comment is mostly helpful, as it directs the authors to a meaningful area for improvement, but it could be more comprehensive with additional details.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point expresses skepticism about the expert pool method proposed in the article, questioning how to choose the size of the expert pool. It speculates that the amount of experts required by a MoE model may be related to the diversity of the dataset and references a table from another paper to support this speculation. However, the comment does not provide explicit guidance or suggestions on how the authors should address this concern or improve their method. The authors are left to infer that they should consider the diversity of the dataset and potentially test the required expert pool size using multiple datasets, but the comment lacks concrete details on how to implement this. Therefore, the comment is somewhat actionable, as it identifies an area for improvement but does not provide specific guidance on how to address it.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions the \"expert pool method\" and \"Table 1 in BASE layers paper [1],\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it raises a concern about the choice of the expert pool size and suggests that it might be related to the diversity of the dataset. The comment further provides a reference to another paper for context, which adds to its specificity. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a concern about the expert pool method proposed in the article, questioning how to choose the size of the expert pool. It speculates that the amount of experts required by a MoE model may be related to the diversity of the dataset, referencing a table from another paper to support this claim. However, the comment lacks specific examples or detailed reasoning to fully substantiate the claim, making it somewhat verifiable. The reference to another paper provides some context, but the authors may need additional information to fully understand and address the concern. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment raises a concern about the expert pool method proposed in the article, questioning how to choose the size of the expert pool. It speculates that the amount of experts required by a MoE model may be related to the diversity of the dataset, referencing a table from another paper for context. However, the comment does not provide specific suggestions or guidance on how the authors might address this concern or improve their method. While it identifies a potential issue, it lacks actionable feedback, making it somewhat helpful. The authors are left to infer that they should consider the diversity of the dataset and potentially test the required expert pool size using multiple datasets, but without explicit guidance, the comment does not fully support the authors in improving their draft. Therefore, it aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a concern about the use of the Pile dataset, which is full of duplicate documents, and notes that the paper does not perform de-duplication processing. It suggests that the dataset\\'s repetition and lack of diversity in tokens may not require a large expert pool, and that the conclusion could change with a different pre-training dataset. While the comment identifies a potential issue with the dataset, it does not provide explicit guidance on how the authors should address this concern or suggest alternative datasets. The action is implicit and somewhat vague, as the authors are left to infer that they should consider using a different dataset or performing de-duplication processing. Therefore, this comment is somewhat actionable.', 'actionability_label': '3', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions the \"third problem\" and the use of the Pile dataset, allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it details the issue with the Pile dataset, noting that it contains duplicate documents and lacks diversity in tokens. The comment further suggests that this could impact the size of the expert pool and the conclusion of the paper. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the Pile dataset is full of duplicate documents and that the paper does not perform de-duplication processing. It suggests that this could affect the size of the expert pool and the conclusion of the paper. The claim is supported by a reference to [2] page 2, which provides evidence for the duplication issue in the Pile dataset. However, the comment could be strengthened by providing more detailed reasoning or examples of how the duplication affects the results or conclusions. Overall, the claim is mostly verifiable, as it provides a basis for the critique but could benefit from additional explanation or evidence.', 'verifiability_label': '4', 'helpfulness_rationale': \"The review comment identifies a specific issue with the use of the Pile dataset, noting that it contains duplicate documents and lacks diversity in tokens. It suggests that this could impact the size of the expert pool and the conclusion of the paper, potentially leading to different results when using a different pre-training dataset. This feedback is clear and actionable, as it highlights a potential flaw in the dataset selection and suggests that the authors should consider using a more diverse dataset. However, the comment could be more helpful if it provided specific recommendations for alternative datasets or methods to address the issue. Overall, the comment is mostly helpful, as it directs the authors to a critical area for improvement but could be more comprehensive with additional guidance.\", 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point highlights a specific issue with the paper, noting that as a MoE model, it is necessary to control the number of FLOPs and compare it with dense and sparse models with the same number of FLOPs. The comment explicitly states that the paper lacks a comparison with a dense model with the same amount of FLOPs in Table 1 and does not report the total training FLOPs number and total train computer (PF-days). This provides clear guidance on what the authors need to address: they should include these comparisons and metrics in their paper. The action is explicit, and the authors know exactly what needs to be done to improve their draft. Therefore, this comment is highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"Table 1,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it details what is missing: the comparison with a dense model with the same number of FLOPs and the lack of reporting on total training FLOPs and total train computer (PF-days). This provides clear guidance on what needs to be addressed in the paper. Therefore, the comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point claims that the paper should compare the MoE model with dense and sparse models having the same number of FLOPs, and it should report the total training FLOPs number and total train computer (PF-days). The comment provides a logical reasoning by stating that this comparison is necessary for a MoE model, which is a common practice in the field. However, it lacks specific examples or references to support the claim that this comparison is essential or how it would impact the paper. The lack of detailed justification or examples makes the claim somewhat verifiable, as the authors would need to infer the importance of these comparisons and metrics themselves. Therefore, the comment is rated as 3: Somewhat Verifiable.', 'verifiability_label': '3', 'helpfulness_rationale': 'The review comment identifies a specific area for improvement in the paper, which is the lack of comparison with dense and sparse models having the same number of FLOPs. It also points out the absence of reporting total training FLOPs and total train computer (PF-days). This feedback is clear and actionable, as it provides the authors with a specific direction to enhance their paper by including these comparisons and metrics. However, the comment could be more helpful if it suggested how to conduct these comparisons or provided examples of similar studies that have successfully implemented such comparisons. Despite this, the comment is mostly helpful as it directs the authors to a critical area for improvement, aligning with a score of 4.', 'helpfulness_label': '4'}"}
{"generated_text": "{'actionability_rationale': 'The review point raises a question about the experimental results in Table 2, specifically comparing the SaMoE model with a dense model. It points out that SaMoE should have more floating-point operations (flops) due to additional all2all communication costs, yet it achieves a lower score than the dense model. However, the comment does not provide any explicit or implicit actions for the authors to take. It lacks guidance on how the authors should address this observation or what changes they should make to their experimental setup or analysis. Without actionable advice or suggestions, the authors are left without a clear path forward. Therefore, this comment is unactionable.', 'actionability_label': '1', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"table 2,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it raises a question about the experimental results, specifically comparing the SaMoE model with a dense model and questioning the expected increase in flops due to additional communication costs. The comment provides a clear observation about the scores achieved by the models, which helps the authors understand the issue. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point raises a question about the experimental results in Table 2, specifically comparing the SaMoE model with a dense model. It points out that SaMoE should have more floating-point operations (flops) due to additional all2all communication costs, yet it achieves a lower score than the dense model. The comment provides a logical observation and a specific comparison, which is verifiable through the data presented in the table. However, it lacks detailed reasoning or references to support the claim fully, making it mostly verifiable. Therefore, the comment aligns with a score of 4: Mostly Verifiable.', 'verifiability_label': '4', 'helpfulness_rationale': 'The review comment raises a question about the experimental results presented in Table 2, specifically regarding the comparison between the SaMoE model and a dense model. It points out that the SaMoE model should have more floating-point operations (flops) due to additional all2all communication costs, yet it achieves a lower score than the dense model. This observation highlights a potential inconsistency or issue in the experimental results that the authors should address. However, the comment does not provide specific suggestions or guidance on how the authors might resolve this discrepancy or improve their experimental setup. While it identifies a potential problem, it lacks actionable feedback, making it somewhat helpful. Therefore, the comment aligns with a score of 3.', 'helpfulness_label': '3'}"}
{"generated_text": "{'actionability_rationale': 'The review point suggests a minor change, recommending that the authors report pretraining perplexity instead of validation loss in Figure 3. This is an explicit suggestion that provides a clear and concrete action for the authors to take. The comment is specific and provides a direct instruction on how to improve the draft, making it highly actionable.', 'actionability_label': '5', 'grounding_specificity_rationale': 'The comment is fully grounded as it explicitly mentions \"figure 3,\" allowing the authors to accurately identify the part of the paper being addressed. It is also specific because it suggests a minor change in the reporting of pretraining perplexity instead of validation loss, providing clear guidance on what needs to be addressed. Therefore, this comment is fully grounded and specific, aligning with category 5.', 'grounding_specificity_label': '5', 'verifiability_rationale': 'The review point suggests a minor change in reporting, recommending the use of pretraining perplexity instead of validation loss in Figure 3. This is a factual statement that does not express an opinion or require verification. It is purely descriptive and does not contain any subjective claims or suggestions that need justification. Therefore, it is classified as \"No.\"', 'verifiability_label': 'X', 'helpfulness_rationale': 'The review comment provides a minor suggestion for improvement, recommending that the authors report pretraining perplexity instead of validation loss in Figure 3. While this feedback is specific and actionable, it is limited in scope and does not offer a comprehensive analysis or detailed guidance on why this change might be beneficial or how it could impact the overall understanding of the results. The comment is clear in its suggestion but lacks depth, making it somewhat helpful. Therefore, it aligns with a score of 3.', 'helpfulness_label': '3'}"}
